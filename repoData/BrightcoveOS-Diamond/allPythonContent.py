__FILENAME__ = build_doc
#!/usr/bin/env python
# coding=utf-8
################################################################################

import os
import sys
import optparse
import configobj
import traceback
import tempfile

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))


def getIncludePaths(path):
    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))

        if os.path.isfile(cPath) and len(f) > 3 and f.endswith('.py'):
            sys.path.append(os.path.dirname(cPath))

        elif os.path.isdir(cPath):
            getIncludePaths(cPath)

collectors = {}


def getCollectors(path):
    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))

        if os.path.isfile(cPath) and len(f) > 3 and f.endswith('.py'):
            modname = f[:-3]

            if modname.startswith('Test'):
                continue
            if modname.startswith('test'):
                continue

            try:
                # Import the module
                module = __import__(modname, globals(), locals(), ['*'])

                # Find the name
                for attr in dir(module):
                    if not attr.endswith('Collector'):
                        continue

                    cls = getattr(module, attr)

                    if cls.__name__ not in collectors:
                        collectors[cls.__name__] = module
            except Exception:
                print "Failed to import module: %s. %s" % (
                    modname, traceback.format_exc())
                collectors[modname] = False

        elif os.path.isdir(cPath):
            getCollectors(cPath)

handlers = {}


def getHandlers(path):
    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))

        if os.path.isfile(cPath) and len(f) > 3 and f.endswith('.py'):
            modname = f[:-3]

            try:
                # Import the module
                module = __import__(modname, globals(), locals(), ['*'])

                # Find the name
                for attr in dir(module):
                    if (not attr.endswith('Handler')
                            or attr.startswith('Handler')):
                        continue

                    cls = getattr(module, attr)

                    if cls.__name__ not in handlers:
                        handlers[cls.__name__] = module
            except Exception:
                print "Failed to import module: %s. %s" % (
                    modname, traceback.format_exc())
                handlers[modname] = False

        elif os.path.isdir(cPath):
            getHandlers(cPath)

################################################################################

if __name__ == "__main__":

    # Initialize Options
    parser = optparse.OptionParser()
    parser.add_option("-c", "--configfile",
                      dest="configfile",
                      default="/etc/diamond/diamond.conf",
                      help="Path to the config file")
    parser.add_option("-C", "--collector",
                      dest="collector",
                      default=None,
                      help="Configure a single collector")
    parser.add_option("-p", "--print",
                      action="store_true",
                      dest="dump",
                      default=False,
                      help="Just print the defaults")

    # Parse Command Line Args
    (options, args) = parser.parse_args()

    # Initialize Config
    if os.path.exists(options.configfile):
        config = configobj.ConfigObj(os.path.abspath(options.configfile))
        config['configfile'] = options.configfile
    else:
        print >> sys.stderr, "ERROR: Config file: %s does not exist." % (
            options.configfile)
        print >> sys.stderr, ("Please run python config.py -c "
                              + "/path/to/diamond.conf")
        parser.print_help(sys.stderr)
        sys.exit(1)

    collector_path = config['server']['collectors_path']
    docs_path = os.path.abspath(os.path.join(os.path.dirname(__file__), 'docs'))
    handler_path = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                                'src', 'diamond', 'handler'))

    getIncludePaths(collector_path)

    # Ugly hack for snmp collector overrides
    getCollectors(os.path.join(collector_path, 'snmp'))
    getCollectors(collector_path)

    collectorIndexFile = open(os.path.join(docs_path, "Collectors.md"), 'w')
    collectorIndexFile.write("## Collectors\n")
    collectorIndexFile.write("\n")
    collectorIndexFile.write("Note that the default collectors are noted via "
                             + "the super-script symbol <sup>♦</sup>.\n")
    collectorIndexFile.write("\n")

    for collector in sorted(collectors.iterkeys()):

        # Skip configuring the basic collector object
        if collector == "Collector":
            continue
        if collector.startswith('Test'):
            continue

        print "Processing %s..." % (collector)

        if not hasattr(collectors[collector], collector):
            continue

        cls = getattr(collectors[collector], collector)

        obj = cls(config=config, handlers={})

        options = obj.get_default_config_help()

        defaultOptions = obj.get_default_config()

        docFile = open(os.path.join(docs_path,
                                    "collectors-" + collector + ".md"), 'w')

        enabled = ''
        if defaultOptions['enabled']:
            enabled = ' <sup>♦</sup>'

        collectorIndexFile.write(" - [%s](collectors-%s)%s\n" % (collector,
                                                                 collector,
                                                                 enabled))

        docFile.write("%s\n" % (collector))
        docFile.write("=====\n")
        if collectors[collector].__doc__ is None:
            print collectors[collector]
            print collectors[collector].__doc__
        docFile.write("%s\n" % (collectors[collector].__doc__))
        docFile.write("#### Options - [Generic Options](Configuration)\n")
        docFile.write("\n")

        docFile.write("<table>")

        docFile.write("<tr>")
        docFile.write("<th>Setting</th>")
        docFile.write("<th>Default</th>")
        docFile.write("<th>Description</th>")
        docFile.write("<th>Type</th>")
        docFile.write("</tr>\n")

        for option in sorted(options.keys()):
            defaultOption = ''
            defaultOptionType = ''
            if option in defaultOptions:
                defaultOptionType = defaultOptions[option].__class__.__name__
                if isinstance(defaultOptions[option], list):
                    defaultOption = ', '.join(map(str, defaultOptions[option]))
                    defaultOption += ','
                else:
                    defaultOption = str(defaultOptions[option])

            docFile.write("<tr>")
            docFile.write("<td>%s</td>" % (option))
            docFile.write("<td>%s</td>" % (defaultOption))
            docFile.write("<td>%s</td>" % (options[option].replace(
                "\n", '<br>\n')))
            docFile.write("<td>%s</td>" % (defaultOptionType))
            docFile.write("</tr>\n")

        docFile.write("</table>\n")

        docFile.write("\n")
        docFile.write("#### Example Output\n")
        docFile.write("\n")
        docFile.write("```\n")
        docFile.write("__EXAMPLESHERE__\n")
        docFile.write("```\n")
        docFile.write("\n")
        docFile.write("### This file was generated from the python source\n")
        docFile.write("### Please edit the source to make changes\n")
        docFile.write("\n")

        docFile.close()

    collectorIndexFile.close()

    getIncludePaths(handler_path)
    getHandlers(handler_path)

    handlerIndexFile = open(os.path.join(docs_path, "Handlers.md"), 'w')
    handlerIndexFile.write("## Handlers\n")
    handlerIndexFile.write("\n")

    for handler in sorted(handlers.iterkeys()):

        # Skip configuring the basic handler object
        if handler == "Handler":
            continue

        if handler[0:4] == "Test":
            continue

        print "Processing %s..." % (handler)

        if not hasattr(handlers[handler], handler):
            continue

        cls = getattr(handlers[handler], handler)

        tmpfile = tempfile.mkstemp()

        obj = cls({
            'log_file': tmpfile[1],
            })

        options = obj.get_default_config_help()

        defaultOptions = obj.get_default_config()

        os.remove(tmpfile[1])

        docFile = open(os.path.join(docs_path,
                                    "handler-" + handler + ".md"), 'w')

        handlerIndexFile.write(" - [%s](handler-%s)\n" % (handler, handler))

        docFile.write("%s\n" % (handler))
        docFile.write("====\n")
        docFile.write("%s" % (handlers[handler].__doc__))

        docFile.write("#### Options - [Generic Options](Configuration)\n")
        docFile.write("\n")

        docFile.write("<table>")

        docFile.write("<tr>")
        docFile.write("<th>Setting</th>")
        docFile.write("<th>Default</th>")
        docFile.write("<th>Description</th>")
        docFile.write("<th>Type</th>")
        docFile.write("</tr>\n")

        if options:
            for option in sorted(options.keys()):
                defaultOption = ''
                defaultOptionType = ''
                if option in defaultOptions:
                    defaultOptionType = defaultOptions[
                        option].__class__.__name__
                    if isinstance(defaultOptions[option], list):
                        defaultOption = ', '.join(map(str,
                                                      defaultOptions[option]))
                        defaultOption += ','
                    else:
                        defaultOption = str(defaultOptions[option])

                docFile.write("<tr>")
                docFile.write("<td>%s</td>" % (option))
                docFile.write("<td>%s</td>" % (defaultOption))
                docFile.write("<td>%s</td>" % (options[option].replace(
                    "\n", '<br>\n')))
                docFile.write("<td>%s</td>" % (defaultOptionType))
                docFile.write("</tr>\n")

        docFile.write("</table>\n")

        docFile.write("\n")
        docFile.write("### This file was generated from the python source\n")
        docFile.write("### Please edit the source to make changes\n")
        docFile.write("\n")

        docFile.close()

    handlerIndexFile.close()

########NEW FILE########
__FILENAME__ = apcupsd
# coding=utf-8

"""
Collects the complete status of most American Power Conversion Corp. (APC) UPSes
provided you have the apcupsd daemon installed, properly configured and
running. It can access status information from any APC UPS attached to the
localhost or attached to any computer on the network which is running
apcuspd in NIS mode.

#### Dependencies

 * apcuspd in NIS mode

"""

import diamond.collector
import socket
from struct import pack
import re
import time


class ApcupsdCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(ApcupsdCollector, self).get_default_config_help()
        config_help.update({
            'hostname': 'Hostname to collect from',
            'port': 'port to collect from. defaults to 3551',
            'metrics': 'List of metrics. Valid metric keys can be found [here]'
            + '(http://www.apcupsd.com/manual/manual.html#status-report-fields)'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ApcupsdCollector, self).get_default_config()
        config.update({
            'path':     'apcupsd',
            'hostname': 'localhost',
            'port': 3551,
            'metrics': ['LINEV', 'LOADPCT', 'BCHARGE', 'TIMELEFT', 'BATTV',
                        'NUMXFERS', 'TONBATT', 'MAXLINEV', 'MINLINEV',
                        'OUTPUTV', 'ITEMP', 'LINEFREQ', 'CUMONBATT', ],
        })
        return config

    def getData(self):
        # Get the data via TCP stream
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((self.config['hostname'], int(self.config['port'])))

        # Packet is pad byte, size byte, and command
        s.send(pack('xb6s', 6, 'status'))

        # Ditch the header
        s.recv(1024)
        time.sleep(.25)
        data = s.recv(4096)

        # We're done. Close the socket
        s.close()
        return data

    def collect(self):
        metrics = {}
        raw = {}

        data = self.getData()

        data = data.split('\n\x00')
        for d in data:
            matches = re.search("([A-Z]+)\s+:\s+(.*)$", d)
            if matches:
                value = matches.group(2).strip()
                raw[matches.group(1)] = matches.group(2).strip()
                vmatch = re.search("([0-9.]+)", value)
                if not vmatch:
                    continue
                try:
                    value = float(vmatch.group(1))
                except ValueError:
                    continue
                metrics[matches.group(1)] = value

        for metric in self.config['metrics']:
            if metric not in metrics:
                continue

            metric_name = "%s.%s" % (raw['UPSNAME'], metric)

            value = metrics[metric]

            if metric in ['TONBATT', 'CUMONBATT', 'NUMXFERS']:
                value = self.derivative(metric_name, metrics[metric])

            self.publish(metric_name, value)

        return True

########NEW FILE########
__FILENAME__ = testapcupsd
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from apcupsd import ApcupsdCollector

################################################################################


class TestApcupsdCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('ApcupsdCollector', {
            'interval': 10
        })

        self.collector = ApcupsdCollector(config, None)

    def test_import(self):
        self.assertTrue(ApcupsdCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        patch_getdata = patch.object(ApcupsdCollector, 'getData', Mock(
            return_value='APC      : 001,039,1056\n\x00\'DATE     : 2012-07-16 '
            + '12:53:58 -0700  \n\x00 HOSTNAME : localhost\n\x00+VERSION  : 3.1'
            + '4.8 (16 January 2010) redhat\n\x00 UPSNAME  : localhost\n\x00'
            + '\x15CABLE    : USB Cable\n\x00\x1dMODEL    : Back-UPS BX1300G '
            + '\n\x00\x17UPSMODE  : Stand Alone\n\x00\'STARTTIME: 2011-12-07 '
            + '10:28:24 -0800  \n\x00\x13STATUS   : ONLINE \n\x00\x17LINEV    '
            + ': 124.0 Volts\n\x00\'LOADPCT  :   5.0 Percent Load Capacity\n'
            + '\x00\x19BCHARGE  : 100.0 Percent\n\x00\x19TIMELEFT :  73.9'
            + ' Minutes\n\x00\x15MBATTCHG : 5 Percent\n\x00\x15MINTIMEL : 3'
            + ' Minutes\n\x00\x15MAXTIME  : 0 Seconds\n\x00\x12SENSE    :'
            + ' Medium\n\x00\x17LOTRANS  : 088.0 Volts\n\x00\x17HITRANS  :'
            + ' 139.0 Volts\n\x00\x12ALARMDEL : Always\n\x00\x16BATTV    :'
            + ' 27.3 Volts\n\x00+LASTXFER : Automatic or explicit self test'
            + '\n\x00\x0eNUMXFERS : 19\n\x00\'XONBATT  : 2012-07-13 09:11:52'
            + ' -0700  \n\x00\x15TONBATT  : 0 seconds\n\x00\x17CUMONBATT: 130'
            + ' seconds\n\x00\'XOFFBATT : 2012-07-13 09:12:01 -0700  \n\x00\''
            + 'LASTSTEST: 2012-07-13 09:11:52 -0700  \n\x00\x0eSELFTEST : NO\n'
            + '\x00"STATFLAG : 0x07000008 Status Flag\n\x00\x16MANDATE  : 2009'
            + '-10-08\n\x00\x1aSERIALNO : 3B0941X40219  \n\x00\x16BATTDATE :'
            + ' 2009-10-08\n\x00\x15NOMINV   : 120 Volts\n\x00\x17NOMBATTV :'
            + '  24.0 '))

        patch_getdata.start()
        self.collector.collect()
        patch_getdata.stop()

        metrics = {
            'localhost.LINEV': 124.000000,
            'localhost.LOADPCT': 5.000000,
            'localhost.BCHARGE': 100.000000,
            'localhost.TIMELEFT': 73.900000,
            'localhost.BATTV': 27.300000,
            'localhost.NUMXFERS': 0.000000,
            'localhost.TONBATT': 0.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = beanstalkd
# coding=utf-8

"""
Collects the following from beanstalkd:
    - Server statistics via the 'stats' command
    - Per tube statistics via the 'stats-tube' command

#### Dependencies

 * beanstalkc

"""

import re
import diamond.collector

try:
    import beanstalkc
    beanstalkc  # workaround for pyflakes issue #13
except ImportError:
    beanstalkc = None


class BeanstalkdCollector(diamond.collector.Collector):
    COUNTERS_REGEX = re.compile(
        r'^(cmd-.*|job-timeouts|total-jobs|total-connections)$')

    def get_default_config_help(self):
        config_help = super(BeanstalkdCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': 'Hostname',
            'port': 'Port',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(BeanstalkdCollector, self).get_default_config()
        config.update({
            'path':     'beanstalkd',
            'host':     'localhost',
            'port':     11300,
        })
        return config

    def _get_stats(self):
        stats = {}
        try:
            connection = beanstalkc.Connection(self.config['host'],
                                               int(self.config['port']))
        except beanstalkc.BeanstalkcException, e:
            self.log.error("Couldn't connect to beanstalkd: %s", e)
            return {}

        stats['instance'] = connection.stats()
        stats['tubes'] = []

        for tube in connection.tubes():
            tube_stats = connection.stats_tube(tube)
            stats['tubes'].append(tube_stats)

        return stats

    def collect(self):
        if beanstalkc is None:
            self.log.error('Unable to import beanstalkc')
            return {}

        info = self._get_stats()

        for stat, value in info['instance'].items():
            if stat != 'version':
                self.publish(stat, value,
                             metric_type=self.get_metric_type(stat))

        for tube_stats in info['tubes']:
            tube = tube_stats['name']
            for stat, value in tube_stats.items():
                if stat != 'name':
                    self.publish('tubes.%s.%s' % (tube, stat), value,
                                 metric_type=self.get_metric_type(stat))

    def get_metric_type(self, stat):
        if self.COUNTERS_REGEX.match(stat):
            return 'COUNTER'
        return 'GAUGE'

########NEW FILE########
__FILENAME__ = testbeanstalkd
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from beanstalkd import BeanstalkdCollector

################################################################################


def run_only_if_beanstalkc_is_available(func):
    try:
        import beanstalkc
        beanstalkc  # workaround for pyflakes issue #13
    except ImportError:
        beanstalkc = None
    pred = lambda: beanstalkc is not None
    return run_only(func, pred)


class TestBeanstalkdCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('BeanstalkdCollector', {
            'host': 'localhost',
            'port': 11300,
        })

        self.collector = BeanstalkdCollector(config, None)

    def test_import(self):
        self.assertTrue(BeanstalkdCollector)

    @run_only_if_beanstalkc_is_available
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        stats = {
            'instance': {
                'current-connections': 10,
                'max-job-size': 65535,
                'cmd-release': 0,
                'cmd-reserve': 4386,
                'pid': 23703,
                'cmd-bury': 0,
                'current-producers': 0,
                'total-jobs': 4331,
                'current-jobs-ready': 0,
                'cmd-peek-buried': 0,
                'current-tubes': 7,
                'current-jobs-delayed': 0,
                'uptime': 182954,
                'cmd-watch': 55,
                'job-timeouts': 0,
                'cmd-stats': 1,
                'rusage-stime': 295.970497,
                'current-jobs-reserved': 0,
                'current-jobs-buried': 0,
                'cmd-reserve-with-timeout': 0,
                'cmd-put': 4331,
                'cmd-pause-tube': 0,
                'cmd-list-tubes-watched': 0,
                'cmd-list-tubes': 0,
                'current-workers': 9,
                'cmd-list-tube-used': 0,
                'cmd-ignore': 0,
                'binlog-records-migrated': 0,
                'current-waiting': 9,
                'cmd-peek': 0,
                'cmd-peek-ready': 0,
                'cmd-peek-delayed': 0,
                'cmd-touch': 0,
                'binlog-oldest-index': 0,
                'binlog-current-index': 0,
                'cmd-use': 4321,
                'total-connections': 4387,
                'cmd-delete': 4331,
                'binlog-max-size': 10485760,
                'cmd-stats-job': 0,
                'rusage-utime': 125.92787,
                'cmd-stats-tube': 0,
                'binlog-records-written': 0,
                'cmd-kick': 0,
                'current-jobs-urgent': 0,
            },
            'tubes': [
                {
                    'current-jobs-delayed': 0,
                    'pause': 0,
                    'name': 'default',
                    'cmd-pause-tube': 0,
                    'current-jobs-buried': 0,
                    'cmd-delete': 10,
                    'pause-time-left': 0,
                    'current-waiting': 9,
                    'current-jobs-ready': 0,
                    'total-jobs': 10,
                    'current-watching': 10,
                    'current-jobs-reserved': 0,
                    'current-using': 10,
                    'current-jobs-urgent': 0,
                    }
                ]
        }

        patch_get_stats = patch.object(BeanstalkdCollector,
                                       '_get_stats',
                                       Mock(return_value=stats))

        patch_get_stats.start()
        self.collector.collect()
        patch_get_stats.stop()

        metrics = {
            'current-connections': 10,
            'max-job-size': 65535,
            'cmd-release': 0,
            'cmd-reserve': 4386,
            'pid': 23703,
            'cmd-bury': 0,
            'current-producers': 0,
            'total-jobs': 4331,
            'current-jobs-ready': 0,
            'cmd-peek-buried': 0,
            'current-tubes': 7,
            'current-jobs-delayed': 0,
            'uptime': 182954,
            'cmd-watch': 55,
            'job-timeouts': 0,
            'cmd-stats': 1,
            'rusage-stime': 295.970497,
            'current-jobs-reserved': 0,
            'current-jobs-buried': 0,
            'cmd-reserve-with-timeout': 0,
            'cmd-put': 4331,
            'cmd-pause-tube': 0,
            'cmd-list-tubes-watched': 0,
            'cmd-list-tubes': 0,
            'current-workers': 9,
            'cmd-list-tube-used': 0,
            'cmd-ignore': 0,
            'binlog-records-migrated': 0,
            'current-waiting': 9,
            'cmd-peek': 0,
            'cmd-peek-ready': 0,
            'cmd-peek-delayed': 0,
            'cmd-touch': 0,
            'binlog-oldest-index': 0,
            'binlog-current-index': 0,
            'cmd-use': 4321,
            'total-connections': 4387,
            'cmd-delete': 4331,
            'binlog-max-size': 10485760,
            'cmd-stats-job': 0,
            'rusage-utime': 125.92787,
            'cmd-stats-tube': 0,
            'binlog-records-written': 0,
            'cmd-kick': 0,
            'current-jobs-urgent': 0,
            'tubes.default.current-jobs-delayed': 0,
            'tubes.default.pause': 0,
            'tubes.default.cmd-pause-tube': 0,
            'tubes.default.current-jobs-buried': 0,
            'tubes.default.cmd-delete': 10,
            'tubes.default.pause-time-left': 0,
            'tubes.default.current-waiting': 9,
            'tubes.default.current-jobs-ready': 0,
            'tubes.default.total-jobs': 10,
            'tubes.default.current-watching': 10,
            'tubes.default.current-jobs-reserved': 0,
            'tubes.default.current-using': 10,
            'tubes.default.current-jobs-urgent': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = bind
# coding=utf-8

"""
Collects stats from bind 9.5's statistics server

#### Dependencies

 * [bind 9.5](http://www.isc.org/software/bind/new-features/9.5)
    configured with libxml2 and statistics-channels

"""

import diamond.collector
import sys
import urllib2

if sys.version_info >= (2, 5):
    import xml.etree.cElementTree as ElementTree
    ElementTree  # workaround for pyflakes issue #13
else:
    import cElementTree as ElementTree


class BindCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(BindCollector, self).get_default_config_help()
        config_help.update({
            'host': "",
            'port': "",
            'publish': "Available stats: \n"
            + " - resolver (Per-view resolver and cache statistics) \n"
            + " - server (Incoming requests and their answers) \n"
            + " - zonemgmt (Zone management requests/responses)\n"
            + " - sockets (Socket statistics) \n"
            + " - memory (Global memory usage) \n",
            'publish_view_bind': "",
            'publish_view_meta': "",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(BindCollector, self).get_default_config()
        config.update({
            'host': 'localhost',
            'port': 8080,
            'path': 'bind',
            # Available stats:
            # - resolver (Per-view resolver and cache statistics)
            # - server (Incoming requests and their answers)
            # - zonemgmt (Requests/responses related to zone management)
            # - sockets (Socket statistics)
            # - memory (Global memory usage)
            'publish': [
                'resolver',
                'server',
                'zonemgmt',
                'sockets',
                'memory',
            ],
            # By default we don't publish these special views
            'publish_view_bind': False,
            'publish_view_meta': False,
        })
        return config

    def clean_counter(self, name, value):
        value = self.derivative(name, value)
        if value < 0:
            value = 0
        self.publish(name, value)

    def collect(self):
        try:
            req = urllib2.urlopen('http://%s:%d/' % (
                self.config['host'], int(self.config['port'])))
        except Exception, e:
            self.log.error('Couldnt connect to bind: %s', e)
            return {}

        tree = ElementTree.parse(req)

        if not tree:
            raise ValueError("Corrupt XML file, no statistics found")

        root = tree.find('bind/statistics')

        if 'resolver' in self.config['publish']:
            for view in root.findall('views/view'):
                name = view.find('name').text
                if name == '_bind' and not self.config['publish_view_bind']:
                    continue
                if name == '_meta' and not self.config['publish_view_meta']:
                    continue
                nzones = len(view.findall('zones/zone'))
                self.publish('view.%s.zones' % name, nzones)
                for counter in view.findall('rdtype'):
                    self.clean_counter(
                        'view.%s.query.%s' % (name,
                                              counter.find('name').text),
                        int(counter.find('counter').text)
                    )
                for counter in view.findall('resstat'):
                    self.clean_counter(
                        'view.%s.resstat.%s' % (name,
                                                counter.find('name').text),
                        int(counter.find('counter').text)
                    )
                for counter in view.findall('cache/rrset'):
                    self.clean_counter(
                        'view.%s.cache.%s' % (
                            name, counter.find('name').text.replace('!',
                                                                    'NOT_')),
                        int(counter.find('counter').text)
                    )

        if 'server' in self.config['publish']:
            for counter in root.findall('server/requests/opcode'):
                self.clean_counter(
                    'requests.%s' % counter.find('name').text,
                    int(counter.find('counter').text)
                )
            for counter in root.findall('server/queries-in/rdtype'):
                self.clean_counter(
                    'queries.%s' % counter.find('name').text,
                    int(counter.find('counter').text)
                )
            for counter in root.findall('server/nsstat'):
                self.clean_counter(
                    'nsstat.%s' % counter.find('name').text,
                    int(counter.find('counter').text)
                )

        if 'zonemgmt' in self.config['publish']:
            for counter in root.findall('server/zonestat'):
                self.clean_counter(
                    'zonestat.%s' % counter.find('name').text,
                    int(counter.find('counter').text)
                )

        if 'sockets' in self.config['publish']:
            for counter in root.findall('server/sockstat'):
                self.clean_counter(
                    'sockstat.%s' % counter.find('name').text,
                    int(counter.find('counter').text)
                )

        if 'memory' in self.config['publish']:
            for counter in root.find('memory/summary').getchildren():
                self.publish(
                    'memory.%s' % counter.tag,
                    int(counter.text)
                )

########NEW FILE########
__FILENAME__ = testbind
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from bind import BindCollector

################################################################################


class TestBindCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('BindCollector', {
            'interval': 10,
        })

        self.collector = BindCollector(config, None)

    def test_import(self):
        self.assertTrue(BindCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('bind.xml')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'view._default.resstat.Queryv4': 0.000000,
            'view._default.resstat.Queryv6': 0.000000,
            'view._default.resstat.Responsev4': 0.000000,
            'view._default.resstat.Responsev6': 0.000000,
            'view._default.resstat.NXDOMAIN': 0.000000,
            'view._default.resstat.SERVFAIL': 0.000000,
            'view._default.resstat.FORMERR': 0.000000,
            'view._default.resstat.OtherError': 0.000000,
            'view._default.resstat.EDNS0Fail': 0.000000,
            'view._default.resstat.Mismatch': 0.000000,
            'view._default.resstat.Truncated': 0.000000,
            'view._default.resstat.Lame': 0.000000,
            'view._default.resstat.Retry': 0.000000,
            'view._default.resstat.QueryAbort': 0.000000,
            'view._default.resstat.QuerySockFail': 0.000000,
            'view._default.resstat.QueryTimeout': 0.000000,
            'view._default.resstat.GlueFetchv4': 0.000000,
            'view._default.resstat.GlueFetchv6': 0.000000,
            'view._default.resstat.GlueFetchv4Fail': 0.000000,
            'view._default.resstat.GlueFetchv6Fail': 0.000000,
            'view._default.resstat.ValAttempt': 0.000000,
            'view._default.resstat.ValOk': 0.000000,
            'view._default.resstat.ValNegOk': 0.000000,
            'view._default.resstat.ValFail': 0.000000,
            'view._default.resstat.QryRTT10': 0.000000,
            'view._default.resstat.QryRTT100': 0.000000,
            'view._default.resstat.QryRTT500': 0.000000,
            'view._default.resstat.QryRTT800': 0.000000,
            'view._default.resstat.QryRTT1600': 0.000000,
            'view._default.resstat.QryRTT1600+': 0.000000,
            'requests.QUERY': 0.000000,
            'queries.A': 0.000000,
            'nsstat.Requestv4': 0.000000,
            'nsstat.Requestv6': 0.000000,
            'nsstat.ReqEdns0': 0.000000,
            'nsstat.ReqBadEDNSVer': 0.000000,
            'nsstat.ReqTSIG': 0.000000,
            'nsstat.ReqSIG0': 0.000000,
            'nsstat.ReqBadSIG': 0.000000,
            'nsstat.ReqTCP': 0.000000,
            'nsstat.AuthQryRej': 0.000000,
            'nsstat.RecQryRej': 0.000000,
            'nsstat.XfrRej': 0.000000,
            'nsstat.UpdateRej': 0.000000,
            'nsstat.Response': 0.000000,
            'nsstat.TruncatedResp': 0.000000,
            'nsstat.RespEDNS0': 0.000000,
            'nsstat.RespTSIG': 0.000000,
            'nsstat.RespSIG0': 0.000000,
            'nsstat.QrySuccess': 0.000000,
            'nsstat.QryAuthAns': 0.000000,
            'nsstat.QryNoauthAns': 0.000000,
            'nsstat.QryReferral': 0.000000,
            'nsstat.QryNxrrset': 0.000000,
            'nsstat.QrySERVFAIL': 0.000000,
            'nsstat.QryFORMERR': 0.000000,
            'nsstat.QryNXDOMAIN': 0.000000,
            'nsstat.QryRecursion': 0.000000,
            'nsstat.QryDuplicate': 0.000000,
            'nsstat.QryDropped': 0.000000,
            'nsstat.QryFailure': 0.000000,
            'nsstat.XfrReqDone': 0.000000,
            'nsstat.UpdateReqFwd': 0.000000,
            'nsstat.UpdateRespFwd': 0.000000,
            'nsstat.UpdateFwdFail': 0.000000,
            'nsstat.UpdateDone': 0.000000,
            'nsstat.UpdateFail': 0.000000,
            'nsstat.UpdateBadPrereq': 0.000000,
            'zonestat.NotifyOutv4': 0.000000,
            'zonestat.NotifyOutv6': 0.000000,
            'zonestat.NotifyInv4': 0.000000,
            'zonestat.NotifyInv6': 0.000000,
            'zonestat.NotifyRej': 0.000000,
            'zonestat.SOAOutv4': 0.000000,
            'zonestat.SOAOutv6': 0.000000,
            'zonestat.AXFRReqv4': 0.000000,
            'zonestat.AXFRReqv6': 0.000000,
            'zonestat.IXFRReqv4': 0.000000,
            'zonestat.IXFRReqv6': 0.000000,
            'zonestat.XfrSuccess': 0.000000,
            'zonestat.XfrFail': 0.000000,
            'sockstat.UDP4Open': 0.000000,
            'sockstat.UDP6Open': 0.000000,
            'sockstat.TCP4Open': 0.000000,
            'sockstat.TCP6Open': 0.000000,
            'sockstat.UnixOpen': 0.000000,
            'sockstat.UDP4OpenFail': 0.000000,
            'sockstat.UDP6OpenFail': 0.000000,
            'sockstat.TCP4OpenFail': 0.000000,
            'sockstat.TCP6OpenFail': 0.000000,
            'sockstat.UnixOpenFail': 0.000000,
            'sockstat.UDP4Close': 0.000000,
            'sockstat.UDP6Close': 0.000000,
            'sockstat.TCP4Close': 0.000000,
            'sockstat.TCP6Close': 0.000000,
            'sockstat.UnixClose': 0.000000,
            'sockstat.FDWatchClose': 0.000000,
            'sockstat.UDP4BindFail': 0.000000,
            'sockstat.UDP6BindFail': 0.000000,
            'sockstat.TCP4BindFail': 0.000000,
            'sockstat.TCP6BindFail': 0.000000,
            'sockstat.UnixBindFail': 0.000000,
            'sockstat.FdwatchBindFail': 0.000000,
            'sockstat.UDP4ConnFail': 0.000000,
            'sockstat.UDP6ConnFail': 0.000000,
            'sockstat.TCP4ConnFail': 0.000000,
            'sockstat.TCP6ConnFail': 0.000000,
            'sockstat.UnixConnFail': 0.000000,
            'sockstat.FDwatchConnFail': 0.000000,
            'sockstat.UDP4Conn': 0.000000,
            'sockstat.UDP6Conn': 0.000000,
            'sockstat.TCP4Conn': 0.000000,
            'sockstat.TCP6Conn': 0.000000,
            'sockstat.UnixConn': 0.000000,
            'sockstat.FDwatchConn': 0.000000,
            'sockstat.TCP4AcceptFail': 0.000000,
            'sockstat.TCP6AcceptFail': 0.000000,
            'sockstat.UnixAcceptFail': 0.000000,
            'sockstat.TCP4Accept': 0.000000,
            'sockstat.TCP6Accept': 0.000000,
            'sockstat.UnixAccept': 0.000000,
            'sockstat.UDP4SendErr': 0.000000,
            'sockstat.UDP6SendErr': 0.000000,
            'sockstat.TCP4SendErr': 0.000000,
            'sockstat.TCP6SendErr': 0.000000,
            'sockstat.UnixSendErr': 0.000000,
            'sockstat.FDwatchSendErr': 0.000000,
            'sockstat.UDP4RecvErr': 0.000000,
            'sockstat.UDP6RecvErr': 0.000000,
            'sockstat.TCP4RecvErr': 0.000000,
            'sockstat.TCP6RecvErr': 0.000000,
            'sockstat.UnixRecvErr': 0.000000,
            'sockstat.FDwatchRecvErr': 0.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)


################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = celerymon
# coding=utf-8

"""
Collects simple task stats out of a running celerymon process

#### Dependencies

 * celerymon connected to celery broker

Example config file CelerymonCollector.conf

```
enabled=True
host=celerymon.example.com
port=16379
```

"""

import diamond.collector
import urllib2
import time

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json


class CelerymonCollector(diamond.collector.Collector):

    LastCollectTime = None

    def get_default_config_help(self):
        config_help = super(CelerymonCollector, self).get_default_config_help()
        config_help.update({
            'path': 'celerymon',
            'host': 'A single hostname to get metrics from',
            'port': 'The celerymon port'

        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(CelerymonCollector, self).get_default_config()
        config.update({
            'host':     'localhost',
            'port':     '8989'
        })
        return config

    def collect(self):
        """
        Overrides the Collector.collect method
        """

        # Handle collection time intervals correctly
        CollectTime = int(time.time())
        time_delta = float(self.config['interval'])
        if not self.LastCollectTime:
            self.LastCollectTime = CollectTime - time_delta

        host = self.config['host']
        port = self.config['port']

        celerymon_url = "http://%s:%s/api/task/?since=%i" % (
            host, port, self.LastCollectTime)
        response = urllib2.urlopen(celerymon_url)
        body = response.read()
        celery_data = json.loads(body)

        results = dict()
        total_messages = 0
        for data in celery_data:
            name = str(data[1]['name'])
            if name not in results:
                results[name] = dict()
            state = str(data[1]['state'])
            if state not in results[name]:
                results[name][state] = 1
            else:
                results[name][state] += 1
            total_messages += 1

        # Publish Metric
        self.publish('total_messages', total_messages)
        for result in results:
            for state in results[result]:
                metric_value = results[result][state]
                metric_name = "%s.%s" % (result, state)
                self.publish(metric_name, metric_value)

        self.LastCollectTime = CollectTime

########NEW FILE########
__FILENAME__ = testcelerymon
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from celerymon import CelerymonCollector


###############################################################################

class TestCelerymonCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('CelerymonCollector', {
        })
        self.collector = CelerymonCollector(config, None)

    def test_import(self):
        self.assertTrue(CelerymonCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ceph
# coding=utf-8

"""
The CephCollector collects utilization info from the Ceph storage system.

Documentation for ceph perf counters:
http://ceph.com/docs/master/dev/perf_counters/

#### Dependencies

 * ceph [http://ceph.com/]

"""

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import glob
import os
import subprocess

import diamond.collector


def flatten_dictionary(input, sep='.', prefix=None):
    """Produces iterator of pairs where the first value is
    the joined key names and the second value is the value
    associated with the lowest level key. For example::

      {'a': {'b': 10},
       'c': 20,
       }

    produces::

      [('a.b', 10), ('c', 20)]
    """
    for name, value in sorted(input.items()):
        fullname = sep.join(filter(None, [prefix, name]))
        if isinstance(value, dict):
            for result in flatten_dictionary(value, sep, fullname):
                yield result
        else:
            yield (fullname, value)


class CephCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(CephCollector, self).get_default_config_help()
        config_help.update({
            'socket_path': 'The location of the ceph monitoring sockets.'
                           ' Defaults to "/var/run/ceph"',
            'socket_prefix': 'The first part of all socket names.'
                             ' Defaults to "ceph-"',
            'socket_ext': 'Extension for socket filenames.'
                          ' Defaults to "asok"',
            'ceph_binary': 'Path to "ceph" executable. '
                           'Defaults to /usr/bin/ceph.',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(CephCollector, self).get_default_config()
        config.update({
            'socket_path': '/var/run/ceph',
            'socket_prefix': 'ceph-',
            'socket_ext': 'asok',
            'ceph_binary': '/usr/bin/ceph',
        })
        return config

    def _get_socket_paths(self):
        """Return a sequence of paths to sockets for communicating
        with ceph daemons.
        """
        socket_pattern = os.path.join(self.config['socket_path'],
                                      (self.config['socket_prefix']
                                       + '*.' + self.config['socket_ext']))
        return glob.glob(socket_pattern)

    def _get_counter_prefix_from_socket_name(self, name):
        """Given the name of a UDS socket, return the prefix
        for counters coming from that source.
        """
        base = os.path.splitext(os.path.basename(name))[0]
        if base.startswith(self.config['socket_prefix']):
            base = base[len(self.config['socket_prefix']):]
        return 'ceph.' + base

    def _get_stats_from_socket(self, name):
        """Return the parsed JSON data returned when ceph is told to
        dump the stats from the named socket.

        In the event of an error error, the exception is logged, and
        an empty result set is returned.
        """
        try:
            json_blob = subprocess.check_output(
                [self.config['ceph_binary'],
                 '--admin-daemon',
                 name,
                 'perf',
                 'dump',
                 ])
        except subprocess.CalledProcessError, err:
            self.log.info('Could not get stats from %s: %s',
                          name, err)
            self.log.exception('Could not get stats from %s' % name)
            return {}

        try:
            json_data = json.loads(json_blob)
        except Exception, err:
            self.log.info('Could not parse stats from %s: %s',
                          name, err)
            self.log.exception('Could not parse stats from %s' % name)
            return {}

        return json_data

    def _publish_stats(self, counter_prefix, stats):
        """Given a stats dictionary from _get_stats_from_socket,
        publish the individual values.
        """
        for stat_name, stat_value in flatten_dictionary(
            stats,
            prefix=counter_prefix,
        ):
            self.publish_gauge(stat_name, stat_value)

    def collect(self):
        """
        Collect stats
        """
        for path in self._get_socket_paths():
            self.log.debug('checking %s', path)
            counter_prefix = self._get_counter_prefix_from_socket_name(path)
            stats = self._get_stats_from_socket(path)
            self._publish_stats(counter_prefix, stats)
        return

########NEW FILE########
__FILENAME__ = testceph
#!/usr/bin/python
# coding=utf-8

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import subprocess

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import patch, call

from diamond.collector import Collector
import ceph


def run_only_if_assertSequenceEqual_is_available(func):
    pred = lambda: 'assertSequenceEqual' in dir(unittest.TestCase)
    return run_only(func, pred)


def run_only_if_subprocess_check_output_is_available(func):
    pred = lambda: 'check_output' in dir(subprocess)
    return run_only(func, pred)


class TestCounterIterator(unittest.TestCase):

    @run_only_if_assertSequenceEqual_is_available
    def test_empty(self):
        data = {}
        expected = []
        actual = list(ceph.flatten_dictionary(data))
        self.assertSequenceEqual(actual, expected)

    @run_only_if_assertSequenceEqual_is_available
    def test_simple(self):
        data = {'a': 1, 'b': 2}
        expected = [('a', 1), ('b', 2)]
        actual = list(ceph.flatten_dictionary(data))
        self.assertSequenceEqual(actual, expected)

    @run_only_if_assertSequenceEqual_is_available
    def test_prefix(self):
        data = {'a': 1, 'b': 2}
        expected = [('Z.a', 1), ('Z.b', 2)]
        actual = list(ceph.flatten_dictionary(data, prefix='Z'))
        self.assertSequenceEqual(actual, expected)

    @run_only_if_assertSequenceEqual_is_available
    def test_sep(self):
        data = {'a': 1, 'b': 2}
        expected = [('Z:a', 1), ('Z:b', 2)]
        actual = list(ceph.flatten_dictionary(data, prefix='Z', sep=':'))
        self.assertSequenceEqual(actual, expected)

    @run_only_if_assertSequenceEqual_is_available
    def test_nested(self):
        data = {'a': 1, 'b': 2, 'c': {'d': 3}}
        expected = [('a', 1), ('b', 2), ('c.d', 3)]
        actual = list(ceph.flatten_dictionary(data))
        self.assertSequenceEqual(actual, expected)

    @run_only_if_assertSequenceEqual_is_available
    def test_doubly_nested(self):
        data = {'a': 1, 'b': 2, 'c': {'d': 3}, 'e': {'f': {'g': 1}}}
        expected = [('a', 1), ('b', 2), ('c.d', 3), ('e.f.g', 1)]
        actual = list(ceph.flatten_dictionary(data))
        self.assertSequenceEqual(actual, expected)

    @run_only_if_assertSequenceEqual_is_available
    def test_complex(self):
        data = {"val": 0,
                "max": 524288000,
                "get": 60910,
                "wait": {"avgcount": 0,
                         "sum": 0},
                }
        expected = [
            ('get', 60910),
            ('max', 524288000),
            ('val', 0),
            ('wait.avgcount', 0),
            ('wait.sum', 0),
        ]
        actual = list(ceph.flatten_dictionary(data))
        self.assertSequenceEqual(actual, expected)


class TestCephCollectorSocketNameHandling(CollectorTestCase):

    def setUp(self):
        config = get_collector_config('CephCollector', {
            'interval': 10,
        })
        self.collector = ceph.CephCollector(config, None)

    def test_counter_default_prefix(self):
        expected = 'ceph.osd.325'
        sock = '/var/run/ceph/ceph-osd.325.asok'
        actual = self.collector._get_counter_prefix_from_socket_name(sock)
        self.assertEquals(actual, expected)

    def test_counter_alternate_prefix(self):
        expected = 'ceph.keep-osd.325'
        sock = '/var/run/ceph/keep-osd.325.asok'
        actual = self.collector._get_counter_prefix_from_socket_name(sock)
        self.assertEquals(actual, expected)

    @patch('glob.glob')
    def test_get_socket_paths(self, glob_mock):
        config = get_collector_config('CephCollector', {
            'socket_path': '/path/',
            'socket_prefix': 'prefix-',
            'socket_ext': 'ext',
        })
        collector = ceph.CephCollector(config, None)

        collector._get_socket_paths()
        glob_mock.assert_called_with('/path/prefix-*.ext')


class TestCephCollectorGettingStats(CollectorTestCase):

    def setUp(self):
        config = get_collector_config('CephCollector', {
            'interval': 10,
        })
        self.collector = ceph.CephCollector(config, None)

    def test_import(self):
        self.assertTrue(ceph.CephCollector)

    @run_only_if_subprocess_check_output_is_available
    @patch('subprocess.check_output')
    def test_load_works(self, check_output):
        expected = {'a': 1,
                    'b': 2,
                    }
        check_output.return_value = json.dumps(expected)
        actual = self.collector._get_stats_from_socket('a_socket_name')
        check_output.assert_called_with(['/usr/bin/ceph',
                                         '--admin-daemon',
                                         'a_socket_name',
                                         'perf',
                                         'dump',
                                         ])
        self.assertEqual(actual, expected)

    @run_only_if_subprocess_check_output_is_available
    @patch('subprocess.check_output')
    def test_ceph_command_fails(self, check_output):
        check_output.side_effect = subprocess.CalledProcessError(
            255, ['/usr/bin/ceph'], 'error!',
        )
        actual = self.collector._get_stats_from_socket('a_socket_name')
        check_output.assert_called_with(['/usr/bin/ceph',
                                         '--admin-daemon',
                                         'a_socket_name',
                                         'perf',
                                         'dump',
                                         ])
        self.assertEqual(actual, {})

    @run_only_if_subprocess_check_output_is_available
    @patch('json.loads')
    @patch('subprocess.check_output')
    def test_json_decode_fails(self, check_output, loads):
        input = {'a': 1,
                 'b': 2,
                 }
        check_output.return_value = json.dumps(input)
        loads.side_effect = ValueError('bad data')
        actual = self.collector._get_stats_from_socket('a_socket_name')
        check_output.assert_called_with(['/usr/bin/ceph',
                                         '--admin-daemon',
                                         'a_socket_name',
                                         'perf',
                                         'dump',
                                         ])
        loads.assert_called_with(json.dumps(input))
        self.assertEqual(actual, {})


class TestCephCollectorPublish(CollectorTestCase):

    def setUp(self):
        config = get_collector_config('CephCollector', {
            'interval': 10,
        })
        self.collector = ceph.CephCollector(config, None)

    @patch.object(Collector, 'publish')
    def test_simple(self, publish_mock):
        self.collector._publish_stats('prefix', {'a': 1})
        publish_mock.assert_called_with('prefix.a', 1,
                                        metric_type='GAUGE', instance=None,
                                        precision=0)

    @patch.object(Collector, 'publish')
    def test_multiple(self, publish_mock):
        self.collector._publish_stats('prefix', {'a': 1, 'b': 2})
        publish_mock.assert_has_calls([call('prefix.a', 1,
                                            metric_type='GAUGE', instance=None,
                                            precision=0),
                                       call('prefix.b', 2,
                                            metric_type='GAUGE', instance=None,
                                            precision=0),
                                       ])

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = cephstats
# coding=utf-8

"""
Get ceph status from one node
"""

import subprocess
import re
import os
import sys

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)),
                                'ceph'))
from ceph import CephCollector


class CephStatsCollector(CephCollector):
    def _get_stats(self):
        """
        Get ceph stats
        """
        try:
            output = subprocess.check_output(['ceph', '-s'])
        except subprocess.CalledProcessError, err:
            self.log.info(
                'Could not get stats: %s' % err)
            self.log.exception('Could not get stats')
            return {}

        pattern = re.compile(r'\bclient io .*')
        ceph_stats = pattern.search(output).group()
        number = re.compile(r'\d+')
        rd = number.search(ceph_stats)
        wr = number.search(ceph_stats, rd.end())
        iops = number.search(ceph_stats, wr.end())

        return {'rd': rd.group(), 'wr': wr.group(), 'iops': iops.group()}

    def collect(self):
        """
        Collect ceph stats
        """
        stats = self._get_stats()
        self._publish_stats('cephstats', stats)

        return

########NEW FILE########
__FILENAME__ = test_ceph
#!/usr/bin/env python
import os
import sys

curdir = os.path.dirname(os.path.abspath(__file__))
os.chdir(curdir)
sys.path.insert(0, '../')

import unittest
import re


def get_ceph_info(info):
    # pattern for ceph information
    pattern = re.compile(r'\bclient io .*')
    ceph_info = pattern.search(info).group()

    # pattern to get number
    number = re.compile(r'\d+')

    read_per_second = number.search(ceph_info)
    write_per_second = number.search(
        ceph_info, read_per_second.end()
        )
    iops = number.search(ceph_info, write_per_second.end())

    return (
        read_per_second.group(),
        write_per_second.group(),
        iops.group()
        )


class TestCeph(unittest.TestCase):
    """
    Test collect ceph data
    """
    def test_sample_data(self):
        """
        Get ceph information from sample data
        """
        f = open('sample.txt')
        self.assertEqual(get_ceph_info(f.read()), ('8643', '4821', '481'))
        f.close()

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = conntrack
# coding=utf-8

"""
Shells out to get the value of sysctl net.netfilter.nf_conntrack_count and
net.netfilter.nf_conntrack_count_max

#### Dependencies

 * nf_conntrack module

"""

import diamond.collector


class ConnTrackCollector(diamond.collector.Collector):
    """
    Collector of number of conntrack connections
    """

    def get_default_config_help(self):
        """
        Return help text for collector configuration
        """
        config_help = super(ConnTrackCollector, self).get_default_config_help()
        config_help.update({
            "dir":         "Directory with files of interest",
            "files":       "List of files to collect statistics from",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ConnTrackCollector, self).get_default_config()
        config.update({
            "path":             "conntrack",
            "dir":              "/proc/sys/net/ipv4/netfilter",
            "files":            "ip_conntrack_count,ip_conntrack_max",
        })
        return config

    def collect(self):
        """
        Collect metrics
        """
        collected = {}
        for sfile in self.config["files"].split(","):
            fpath = "%s/%s" % (self.config["dir"], sfile)
            try:
                with open(fpath, "r") as fhandle:
                    collected[sfile] = float(fhandle.readline().rstrip("\n"))
            except Exception as exception:
                self.log.error("Failed to collect from '%s': %s",
                               fpath,
                               exception)

        for key in collected.keys():
            self.publish(key, collected[key])

########NEW FILE########
__FILENAME__ = testconntrack
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from conntrack import ConnTrackCollector

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

################################################################################


class TestConnTrackCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('ConnTrackCollector', {
            'interval': 10,
            'bin': 'true',
            'dir': self.getFixtureDirPath(),
        })

        self.collector = ConnTrackCollector(config, None)

    def test_import(self):
        self.assertTrue(ConnTrackCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        self.collector.collect()

        metrics = {
            'ip_conntrack_count': 33.0,
            'ip_conntrack_max': 36.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(
                return_value=(
                    'sysctl: cannot stat /proc/sys/net/net'
                    + 'filter/nf_conntrack_count: '
                    + 'No such file or directory', '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {})

    @patch('os.access', Mock(return_value=False))
    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully_2(self, publish_mock):
        self.collector.collect()
        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = cpu
# coding=utf-8

"""
The CPUCollector collects CPU utilization metric using /proc/stat.

#### Dependencies

 * /proc/stat

"""

import diamond.collector
import os
import time
from diamond.collector import str_to_bool

try:
    import psutil
    psutil  # workaround for pyflakes issue #13
except ImportError:
    psutil = None


class CPUCollector(diamond.collector.Collector):

    PROC = '/proc/stat'
    INTERVAL = 1

    MAX_VALUES = {
        'user': diamond.collector.MAX_COUNTER,
        'nice': diamond.collector.MAX_COUNTER,
        'system': diamond.collector.MAX_COUNTER,
        'idle': diamond.collector.MAX_COUNTER,
        'iowait': diamond.collector.MAX_COUNTER,
        'irq': diamond.collector.MAX_COUNTER,
        'softirq': diamond.collector.MAX_COUNTER,
        'steal': diamond.collector.MAX_COUNTER,
        'guest': diamond.collector.MAX_COUNTER,
        'guest_nice': diamond.collector.MAX_COUNTER,
    }

    def get_default_config_help(self):
        config_help = super(CPUCollector, self).get_default_config_help()
        config_help.update({
            'percore':  'Collect metrics per cpu core or just total',
            'simple':   'only return aggregate CPU% metric',
            'normalize': 'for cpu totals, divide by the number of CPUs',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(CPUCollector, self).get_default_config()
        config.update({
            'enabled':  'True',
            'path':     'cpu',
            'percore':  'True',
            'xenfix':   None,
            'simple':   'False',
            'normalize': 'False',
        })
        return config

    def collect(self):
        """
        Collector cpu stats
        """

        def cpu_time_list():
            """
            get cpu time list
            """

            statFile = open(self.PROC, "r")
            timeList = statFile.readline().split(" ")[2:6]
            for i in range(len(timeList)):
                timeList[i] = int(timeList[i])
            statFile.close()
            return timeList

        def cpu_delta_time(interval):
            """
            Get before and after cpu times for usage calc
            """
            pre_check = cpu_time_list()
            time.sleep(interval)
            post_check = cpu_time_list()
            for i in range(len(pre_check)):
                post_check[i] -= pre_check[i]
            return post_check

        if os.access(self.PROC, os.R_OK):

            #If simple only return aggregate CPU% metric
            if str_to_bool(self.config['simple']):
                dt = cpu_delta_time(self.INTERVAL)
                cpuPct = 100 - (dt[len(dt) - 1] * 100.00 / sum(dt))
                self.publish('percent', str('%.4f' % cpuPct))
                return True

            results = {}
            # Open file
            file = open(self.PROC)

            ncpus = -1  # dont want to count the 'cpu'(total) cpu.
            for line in file:
                if not line.startswith('cpu'):
                    continue

                ncpus += 1
                elements = line.split()

                cpu = elements[0]

                if cpu == 'cpu':
                    cpu = 'total'
                elif not str_to_bool(self.config['percore']):
                    continue

                results[cpu] = {}

                if len(elements) >= 2:
                    results[cpu]['user'] = elements[1]
                if len(elements) >= 3:
                    results[cpu]['nice'] = elements[2]
                if len(elements) >= 4:
                    results[cpu]['system'] = elements[3]
                if len(elements) >= 5:
                    results[cpu]['idle'] = elements[4]
                if len(elements) >= 6:
                    results[cpu]['iowait'] = elements[5]
                if len(elements) >= 7:
                    results[cpu]['irq'] = elements[6]
                if len(elements) >= 8:
                    results[cpu]['softirq'] = elements[7]
                if len(elements) >= 9:
                    results[cpu]['steal'] = elements[8]
                if len(elements) >= 10:
                    results[cpu]['guest'] = elements[9]
                if len(elements) >= 11:
                    results[cpu]['guest_nice'] = elements[10]

            # Close File
            file.close()

            metrics = {}

            for cpu in results.keys():
                stats = results[cpu]
                for s in stats.keys():
                    # Get Metric Name
                    metric_name = '.'.join([cpu, s])
                    # Get actual data
                    if (str_to_bool(self.config['normalize'])
                            and cpu == 'total' and ncpus > 0):
                        metrics[metric_name] = self.derivative(
                            metric_name,
                            long(stats[s]),
                            self.MAX_VALUES[s]) / ncpus
                    else:
                        metrics[metric_name] = self.derivative(
                            metric_name,
                            long(stats[s]),
                            self.MAX_VALUES[s])

            # Check for a bug in xen where the idle time is doubled for guest
            # See https://bugzilla.redhat.com/show_bug.cgi?id=624756
            if self.config['xenfix'] is None or self.config['xenfix'] is True:
                if os.path.isdir('/proc/xen'):
                    total = 0
                    for metric_name in metrics.keys():
                        if 'cpu0.' in metric_name:
                            total += int(metrics[metric_name])
                    if total > 110:
                        self.config['xenfix'] = True
                        for mname in metrics.keys():
                            if '.idle' in mname:
                                metrics[mname] = float(metrics[mname]) / 2
                    elif total > 0:
                        self.config['xenfix'] = False
                else:
                    self.config['xenfix'] = False

            # Publish Metric Derivative
            for metric_name in metrics.keys():
                self.publish(metric_name,
                             metrics[metric_name])
            return True

        else:
            if not psutil:
                self.log.error('Unable to import psutil')
                self.log.error('No cpu metrics retrieved')
                return None

            cpu_time = psutil.cpu_times(True)
            cpu_count = len(cpu_time)
            total_time = psutil.cpu_times()
            for i in range(0, len(cpu_time)):
                metric_name = 'cpu' + str(i)
                self.publish(metric_name + '.user',
                             self.derivative(metric_name + '.user',
                                             cpu_time[i].user,
                                             self.MAX_VALUES['user']))
                if hasattr(cpu_time[i], 'nice'):
                    self.publish(metric_name + '.nice',
                                 self.derivative(metric_name + '.nice',
                                                 cpu_time[i].nice,
                                                 self.MAX_VALUES['nice']))
                self.publish(metric_name + '.system',
                             self.derivative(metric_name + '.system',
                                             cpu_time[i].system,
                                             self.MAX_VALUES['system']))
                self.publish(metric_name + '.idle',
                             self.derivative(metric_name + '.idle',
                                             cpu_time[i].idle,
                                             self.MAX_VALUES['idle']))

            metric_name = 'total'
            self.publish(metric_name + '.user',
                         self.derivative(metric_name + '.user',
                                         total_time.user,
                                         self.MAX_VALUES['user'])
                         / cpu_count)
            if hasattr(total_time, 'nice'):
                self.publish(metric_name + '.nice',
                             self.derivative(metric_name + '.nice',
                                             total_time.nice,
                                             self.MAX_VALUES['nice'])
                             / cpu_count)
            self.publish(metric_name + '.system',
                         self.derivative(metric_name + '.system',
                                         total_time.system,
                                         self.MAX_VALUES['system'])
                         / cpu_count)
            self.publish(metric_name + '.idle',
                         self.derivative(metric_name + '.idle',
                                         total_time.idle,
                                         self.MAX_VALUES['idle'])
                         / cpu_count)

            return True

        return None

########NEW FILE########
__FILENAME__ = testcpu
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from cpu import CPUCollector

################################################################################


class TestCPUCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('CPUCollector', {
            'interval': 10,
            'normalize': False
        })

        self.collector = CPUCollector(config, None)

    def test_import(self):
        self.assertTrue(CPUCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_stat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/stat')

    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        patch_open = patch('__builtin__.open', Mock(return_value=StringIO(
            'cpu 100 200 300 400 500 0 0 0 0 0')))

        patch_open.start()
        self.collector.collect()
        patch_open.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch('__builtin__.open', Mock(return_value=StringIO(
            'cpu 110 220 330 440 550 0 0 0 0 0')))

        patch_open.start()
        self.collector.collect()
        patch_open.stop()

        self.assertPublishedMany(publish_mock, {
            'total.idle': 4.0,
            'total.iowait': 5.0,
            'total.nice': 2.0,
            'total.system': 3.0,
            'total.user': 1.0
        })

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        CPUCollector.PROC = self.getFixturePath('proc_stat_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        CPUCollector.PROC = self.getFixturePath('proc_stat_2')
        self.collector.collect()

        metrics = {
            'total.idle': 2440.8,
            'total.iowait': 0.2,
            'total.nice': 0.0,
            'total.system': 0.2,
            'total.user': 0.4
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_ec2_data(self, publish_mock):
        self.collector.config['interval'] = 30
        patch_open = patch('os.path.isdir', Mock(return_value=True))
        patch_open.start()

        CPUCollector.PROC = self.getFixturePath('ec2_stat_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        CPUCollector.PROC = self.getFixturePath('ec2_stat_2')
        self.collector.collect()

        patch_open.stop()

        metrics = {
            'total.idle': 68.4,
            'total.iowait': 0.6,
            'total.nice': 0.0,
            'total.system': 13.7,
            'total.user': 16.666666666666668
        }

        self.assertPublishedMany(publish_mock, metrics)


class TestCPUCollectorNormalize(CollectorTestCase):

    def setUp(self):
        config = get_collector_config('CPUCollector', {
            'interval': 1,
            'normalize': True,
        })

        self.collector = CPUCollector(config, None)

        self.num_cpu = 2

        # first measurement
        self.input_base = {
            'user': 100,
            'nice': 200,
            'system': 300,
            'idle': 400,
        }
        # second measurement
        self.input_next = {
            'user': 110,
            'nice': 220,
            'system': 330,
            'idle': 440,
        }
        # expected increment, divided by number of CPUs
        # for example, user should be 10/2 = 5
        self.expected = {
            'total.user': 5.0,
            'total.nice': 10.0,
            'total.system': 15.0,
            'total.idle': 20.0,
        }

    # convert an input dict with values to a string that might come from
    # /proc/stat
    def input_dict_to_proc_string(self, cpu_id, dict_):
        return ("cpu%s %i %i %i %i 0 0 0 0 0 0" %
                (cpu_id,
                 dict_['user'],
                 dict_['nice'],
                 dict_['system'],
                 dict_['idle'],
                 )
                )

    @patch.object(Collector, 'publish')
    def test_should_work_proc_stat(self, publish_mock):
        patch_open = patch('__builtin__.open', Mock(return_value=StringIO(
            "\n".join([self.input_dict_to_proc_string('', self.input_base),
                       self.input_dict_to_proc_string('0', self.input_base),
                       self.input_dict_to_proc_string('1', self.input_base),
                       ])
        )))

        patch_open.start()
        self.collector.collect()
        patch_open.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch('__builtin__.open', Mock(return_value=StringIO(
            "\n".join([self.input_dict_to_proc_string('', self.input_next),
                       self.input_dict_to_proc_string('0', self.input_next),
                       self.input_dict_to_proc_string('1', self.input_next),
                       ])
        )))

        patch_open.start()
        self.collector.collect()
        patch_open.stop()

        self.assertPublishedMany(publish_mock, self.expected)

    @patch.object(Collector, 'publish')
    @patch('cpu.os')
    @patch('cpu.psutil')
    def test_should_work_psutil(self, psutil_mock, os_mock, publish_mock):

        os_mock.access.return_value = False

        total = Mock(**self.input_base)
        cpu_time = [Mock(**self.input_base),
                    Mock(**self.input_base),
                    ]
        psutil_mock.cpu_times.side_effect = [cpu_time, total]

        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        total = Mock(**self.input_next)
        cpu_time = [Mock(**self.input_next),
                    Mock(**self.input_next),
                    ]
        psutil_mock.cpu_times.side_effect = [cpu_time, total]

        self.collector.collect()

        self.assertPublishedMany(publish_mock, self.expected)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = cpuacct_cgroup
# coding=utf-8

"""
The CpuAcctCGroupCollector collects CPU Acct metric for cgroups

#### Dependencies

A mounted cgroup fs. Defaults to /sys/fs/cgroup/cpuacct/

"""

import diamond.collector
import os


class CpuAcctCgroupCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(
            CpuAcctCgroupCollector, self).get_default_config_help()
        config_help.update({
            'path': """Directory path to where cpuacct is located,
defaults to /sys/fs/cgroup/cpuacct/. Redhat/CentOS/SL use /cgroup"""
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(CpuAcctCgroupCollector, self).get_default_config()
        config.update({
            'path':     '/sys/fs/cgroup/cpuacct/'
        })
        return config

    def collect(self):
        # find all cpuacct.stat files
        matches = []
        for root, dirnames, filenames in os.walk(self.config['path']):
            for filename in filenames:
                if filename == 'cpuacct.stat':
                    # matches will contain a tuple contain path to cpuacct.stat
                    # and the parent of the stat
                    parent = root.replace(self.config['path'],
                                          "").replace("/", ".")
                    if parent == '':
                        parent = 'system'
                    # If the parent starts with a dot, remove it
                    if parent[0] == '.':
                        parent = parent[1:]
                    matches.append((parent, os.path.join(root, filename)))

        # Read utime and stime from cpuacct files
        results = {}
        for match in matches:
            results[match[0]] = {}
            stat_file = open(match[1])
            elements = [line.split() for line in stat_file]
            for el in elements:
                results[match[0]][el[0]] = el[1]
                stat_file.close()

        # create metrics from collected utimes and stimes for cgroups
        for parent, cpuacct in results.iteritems():
            for key, value in cpuacct.iteritems():
                metric_name = '.'.join([parent, key])
                self.publish(metric_name, value, metric_type='GAUGE')
        return True

########NEW FILE########
__FILENAME__ = testcpuacct_cgroup
#!/usr/bin/python
# coding=utf-8
################################################################################
import os
from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from cpuacct_cgroup import CpuAcctCgroupCollector


class TestCpuAcctCgroupCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('CpuAcctCgroupCollector', {
            'interval': 10
        })

        self.collector = CpuAcctCgroupCollector(config, None)

    def test_import(self):
        self.assertTrue(CpuAcctCgroupCollector)

    @patch('__builtin__.open')
    @patch.object(Collector, 'publish')
    def test_should_open_all_cpuacct_stat(self, publish_mock, open_mock):
        return
        self.collector.config['path'] = self.getFixtureDirPath()
        open_mock.side_effect = lambda x: StringIO('')
        self.collector.collect()

        # All the fixtures we should be opening
        paths = [
            'lxc/testcontainer/cpuacct.stat',
            'lxc/cpuacct.stat',
            'cpuacct.stat',
        ]

        for path in paths:
            open_mock.assert_any_call(os.path.join(
                self.getFixtureDirPath(), path))

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        self.collector.config['path'] = self.getFixtureDirPath()
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {
            'lxc.testcontainer.user': 1318,
            'lxc.testcontainer.system': 332,
            'lxc.user': 36891,
            'lxc.system': 88927,
            'system.user': 3781253,
            'system.system': 4784004,
        })

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = diskspace
# coding=utf-8

"""
Uses /proc/mounts and os.statvfs() to get disk space usage

#### Dependencies

 * /proc/mounts

#### Examples

    # no exclude filters at all
    exclude_filters =,

    # exclude everything that begins /boot or /mnt
    exclude_filters = ^/boot, ^/mnt

    # exclude everything that includes the letter 'm'
    exclude_filters = m,

"""

import diamond.collector
import diamond.convertor
import os
import re

try:
    import psutil
    psutil  # workaround for pyflakes issue #13
except ImportError:
    psutil = None


class DiskSpaceCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(DiskSpaceCollector, self).get_default_config_help()
        config_help.update({
            'filesystems': "filesystems to examine",
            'exclude_filters': "A list of regex patterns. Any filesystem"
            + " matching any of these patterns will be excluded from disk"
            + " space metrics collection",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(DiskSpaceCollector, self).get_default_config()
        config.update({
            # Enabled by default
            'enabled': 'True',
            'path': 'diskspace',
            # filesystems to examine
            'filesystems': 'ext2, ext3, ext4, xfs, glusterfs, nfs, ntfs, hfs,'
            + ' fat32, fat16',

            # exclude_filters
            #   A list of regex patterns
            #   A filesystem matching any of these patterns will be excluded
            #   from disk space metrics collection.
            #
            # Examples:
            #       exclude_filters =,
            # no exclude filters at all
            #       exclude_filters = ^/boot, ^/mnt
            # exclude everything that begins /boot or /mnt
            #       exclude_filters = m,
            # exclude everything that includes the letter "m"
            'exclude_filters': ['^/export/home'],

            # We don't use any derivative data to calculate this value
            # Thus we can use a threaded model
            'method': 'Threaded',

            # Default numeric output
            'byte_unit': ['byte']
        })
        return config

    def __init__(self, config, handlers):
        super(DiskSpaceCollector, self).__init__(config, handlers)

        # Precompile things
        self.exclude_filters = self.config['exclude_filters']
        if isinstance(self.exclude_filters, basestring):
            self.exclude_filters = [self.exclude_filters]

        self.exclude_reg = re.compile('|'.join(self.exclude_filters))

        self.filesystems = []
        if isinstance(self.config['filesystems'], basestring):
            for filesystem in self.config['filesystems'].split(','):
                self.filesystems.append(filesystem.strip())
        elif isinstance(self.config['filesystems'], list):
            self.filesystems = self.config['filesystems']

    def get_disk_labels(self):
        """
        Creates a mapping of device nodes to filesystem labels
        """
        path = '/dev/disk/by-label/'
        labels = {}
        if not os.path.isdir(path):
            return labels

        for label in os.listdir(path):
            label = label.replace('\\x2f', '/')
            device = os.path.realpath(path + '/' + label)
            labels[device] = label

        return labels

    def get_file_systems(self):
        """
        Creates a map of mounted filesystems on the machine.

        iostat(1): Each sector has size of 512 bytes.

        Returns:
          (major, minor) -> FileSystem(device, mount_point)
        """
        result = {}
        if os.access('/proc/mounts', os.R_OK):
            file = open('/proc/mounts')
            for line in file:
                try:
                    mount = line.split()
                    device = mount[0]
                    mount_point = mount[1]
                    fs_type = mount[2]
                except (IndexError, ValueError):
                    continue

                # Skip the filesystem if it is not in the list of valid
                # filesystems
                if fs_type not in self.filesystems:
                    self.log.debug("Ignoring %s since it is of type %s which "
                                   + " is not in the list of filesystems.",
                                   mount_point, fs_type)
                    continue

                # Process the filters
                if self.exclude_reg.search(mount_point):
                    self.log.debug("Ignoring %s since it is in the "
                                   + "exclude_filter list.", mount_point)
                    continue

                if (mount_point.startswith('/dev')
                    or mount_point.startswith('/proc')
                        or mount_point.startswith('/sys')):
                    continue

                if '/' in device and mount_point.startswith('/'):
                    try:
                        stat = os.stat(mount_point)
                        major = os.major(stat.st_dev)
                        minor = os.minor(stat.st_dev)
                    except OSError:
                        self.log.debug("Path %s is not mounted - skipping.",
                                       mount_point)
                        continue

                    if (major, minor) in result:
                        continue

                    result[(major, minor)] = {
                        'device': device,
                        'mount_point': mount_point,
                        'fs_type': fs_type
                    }

            file.close()

        else:
            if not psutil:
                self.log.error('Unable to import psutil')
                return None

            partitions = psutil.disk_partitions(False)
            for partition in partitions:
                result[(0, len(result))] = {
                    'device': partition.device,
                    'mount_point': partition.mountpoint,
                    'fs_type': partition.fstype
                }
            pass

        return result

    def collect(self):
        labels = self.get_disk_labels()
        results = self.get_file_systems()
        if not results:
            self.log.error('No diskspace metrics retrieved')
            return None

        for key, info in results.iteritems():
            if info['device'] in labels:
                name = labels[info['device']]
            else:
                name = info['mount_point'].replace('/', '_')
                name = name.replace('.', '_').replace('\\', '')
                if name == '_':
                    name = 'root'

            if hasattr(os, 'statvfs'):  # POSIX
                try:
                    data = os.statvfs(info['mount_point'])
                except OSError, e:
                    self.log.exception(e)
                    continue

                block_size = data.f_bsize

                blocks_total = data.f_blocks
                blocks_free = data.f_bfree
                blocks_avail = data.f_bavail
                inodes_total = data.f_files
                inodes_free = data.f_ffree
                inodes_avail = data.f_favail

            elif os.name == 'nt':       # Windows
                # fixme: used still not exact compared to disk_usage.py
                # from psutil
                raw_data = psutil.disk_usage(info['mount_point'])

                block_size = 1  # fixme: ?

                blocks_total = raw_data.total
                blocks_free = raw_data.free

            else:
                raise NotImplementedError("platform not supported")

            for unit in self.config['byte_unit']:
                metric_name = '%s.%s_percentfree' % (name, unit)
                metric_value = float(blocks_free) / float(
                    blocks_free + (blocks_total - blocks_free)) * 100
                self.publish_gauge(metric_name, metric_value, 2)

                metric_name = '%s.%s_used' % (name, unit)
                metric_value = float(block_size) * float(
                    blocks_total - blocks_free)
                metric_value = diamond.convertor.binary.convert(
                    value=metric_value, oldUnit='byte', newUnit=unit)
                self.publish_gauge(metric_name, metric_value, 2)

                metric_name = '%s.%s_free' % (name, unit)
                metric_value = float(block_size) * float(blocks_free)
                metric_value = diamond.convertor.binary.convert(
                    value=metric_value, oldUnit='byte', newUnit=unit)
                self.publish_gauge(metric_name, metric_value, 2)

                if os.name != 'nt':
                    metric_name = '%s.%s_avail' % (name, unit)
                    metric_value = float(block_size) * float(blocks_avail)
                    metric_value = diamond.convertor.binary.convert(
                        value=metric_value, oldUnit='byte', newUnit=unit)
                    self.publish_gauge(metric_name, metric_value, 2)

            if os.name != 'nt':
                if float(inodes_total) > 0:
                    self.publish_gauge(
                        '%s.inodes_percentfree' % name,
                        float(inodes_free) / float(inodes_total) * 100)
                self.publish_gauge('%s.inodes_used' % name,
                                   inodes_total - inodes_free)
                self.publish_gauge('%s.inodes_free' % name, inodes_free)
                self.publish_gauge('%s.inodes_avail' % name, inodes_avail)

########NEW FILE########
__FILENAME__ = testdiskspace
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from diskspace import DiskSpaceCollector

################################################################################


def run_only_if_major_is_available(func):
    try:
        import os
        os.major
        major = True
    except AttributeError:
        major = None
    pred = lambda: major is not None
    return run_only(func, pred)


class TestDiskSpaceCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('DiskSpaceCollector', {
            'interval': 10,
            'byte_unit': ['gigabyte'],
            'exclude_filters': [
                '^/export/home',
                '^/tmpfs',
            ]
        })

        self.collector = DiskSpaceCollector(config, None)

    def test_import(self):
        self.assertTrue(DiskSpaceCollector)

    @run_only_if_major_is_available
    @patch('os.access', Mock(return_value=True))
    def test_get_file_systems(self):
        result = None

        os_stat_mock = patch('os.stat')
        os_major_mock = patch('os.major')
        os_minor_mock = patch('os.minor')
        open_mock = patch('__builtin__.open',
                          Mock(return_value=self.getFixture('proc_mounts')))

        stat_mock = os_stat_mock.start()
        stat_mock.return_value.st_dev = 42

        major_mock = os_major_mock.start()
        major_mock.return_value = 9

        minor_mock = os_minor_mock.start()
        minor_mock.return_value = 0

        omock = open_mock.start()

        result = self.collector.get_file_systems()
        os_stat_mock.stop()
        os_major_mock.stop()
        os_minor_mock.stop()
        open_mock.stop()

        stat_mock.assert_called_once_with('/')
        major_mock.assert_called_once_with(42)
        minor_mock.assert_called_once_with(42)

        self.assertEqual(result, {
            (9, 0): {
                'device':
                '/dev/disk/by-uuid/81969733-a724-4651-9cf5-64970f86daba',
                'fs_type': 'ext3',
                'mount_point': '/'}
        })

        omock.assert_called_once_with('/proc/mounts')
        return result

    @run_only_if_major_is_available
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        statvfs_mock = Mock()
        statvfs_mock.f_bsize = 4096
        statvfs_mock.f_frsize = 4096
        statvfs_mock.f_blocks = 360540255
        statvfs_mock.f_bfree = 285953527
        statvfs_mock.f_bavail = 267639130
        statvfs_mock.f_files = 91578368
        statvfs_mock.f_ffree = 91229495
        statvfs_mock.f_favail = 91229495
        statvfs_mock.f_flag = 4096
        statvfs_mock.f_namemax = 255

        os_stat_mock = patch('os.stat')
        os_major_mock = patch('os.major', Mock(return_value=9))
        os_minor_mock = patch('os.minor', Mock(return_value=0))
        os_path_isdir_mock = patch('os.path.isdir', Mock(return_value=False))
        open_mock = patch('__builtin__.open',
                          Mock(return_value=self.getFixture('proc_mounts')))
        os_statvfs_mock = patch('os.statvfs', Mock(return_value=statvfs_mock))

        os_stat_mock.start()
        os_major_mock.start()
        os_minor_mock.start()
        os_path_isdir_mock.start()
        open_mock.start()
        os_statvfs_mock.start()
        self.collector.collect()
        os_stat_mock.stop()
        os_major_mock.stop()
        os_minor_mock.stop()
        os_path_isdir_mock.stop()
        open_mock.stop()
        os_statvfs_mock.stop()

        metrics = {
            'root.gigabyte_used': (284.525, 2),
            'root.gigabyte_free': (1090.826, 2),
            'root.gigabyte_avail': (1020.962, 2),
            'root.inodes_used': 348873,
            'root.inodes_free': 91229495,
            'root.inodes_avail': 91229495
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = diskusage
# coding=utf-8

"""
Collect IO Stats

Note: You may need to artifically generate some IO load on a disk/partition
before graphite will generate the metrics.

 * http://www.kernel.org/doc/Documentation/iostats.txt

#### Dependencies

 * /proc/diskstats

"""

import diamond.collector
import diamond.convertor
import time
import os
import re

try:
    import psutil
    psutil  # workaround for pyflakes issue #13
except ImportError:
    psutil = None


class DiskUsageCollector(diamond.collector.Collector):

    MAX_VALUES = {
        'reads':                    4294967295,
        'reads_merged':             4294967295,
        'reads_milliseconds':       4294967295,
        'writes':                   4294967295,
        'writes_merged':            4294967295,
        'writes_milliseconds':      4294967295,
        'io_milliseconds':          4294967295,
        'io_milliseconds_weighted': 4294967295
    }

    LastCollectTime = None

    def get_default_config_help(self):
        config_help = super(DiskUsageCollector, self).get_default_config_help()
        config_help.update({
            'devices': "A regex of which devices to gather metrics for."
                       + " Defaults to md, sd, xvd, disk, and dm devices",
            'sector_size': 'The size to use to calculate sector usage',
            'send_zero': 'Send io data even when there is no io',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(DiskUsageCollector, self).get_default_config()
        config.update({
            'enabled':  'True',
            'path':     'iostat',
            'devices':  ('PhysicalDrive[0-9]+$'
                         + '|md[0-9]+$'
                         + '|sd[a-z]+[0-9]*$'
                         + '|x?vd[a-z]+[0-9]*$'
                         + '|disk[0-9]+$'
                         + '|dm\-[0-9]+$'),
            'sector_size': 512,
            'send_zero': False,
        })
        return config

    def get_disk_statistics(self):
        """
        Create a map of disks in the machine.

        http://www.kernel.org/doc/Documentation/iostats.txt

        Returns:
          (major, minor) -> DiskStatistics(device, ...)
        """
        result = {}

        if os.access('/proc/diskstats', os.R_OK):
            self.proc_diskstats = True
            fp = open('/proc/diskstats')

            try:
                for line in fp:
                    try:
                        columns = line.split()
                        # On early linux v2.6 versions, partitions have only 4
                        # output fields not 11. From linux 2.6.25 partitions
                        # have the full stats set.
                        if len(columns) < 14:
                            continue
                        major = int(columns[0])
                        minor = int(columns[1])
                        device = columns[2]

                        if (device.startswith('ram')
                                or device.startswith('loop')):
                            continue

                        result[(major, minor)] = {
                            'device': device,
                            'reads': float(columns[3]),
                            'reads_merged': float(columns[4]),
                            'reads_sectors': float(columns[5]),
                            'reads_milliseconds': float(columns[6]),
                            'writes': float(columns[7]),
                            'writes_merged': float(columns[8]),
                            'writes_sectors': float(columns[9]),
                            'writes_milliseconds': float(columns[10]),
                            'io_in_progress': float(columns[11]),
                            'io_milliseconds': float(columns[12]),
                            'io_milliseconds_weighted': float(columns[13])
                        }
                    except ValueError:
                        continue
            finally:
                fp.close()
        else:
            self.proc_diskstats = False
            if not psutil:
                self.log.error('Unable to import psutil')
                return None

            disks = psutil.disk_io_counters(True)
            for disk in disks:
                    result[(0, len(result))] = {
                        'device': disk,
                        'reads': disks[disk].read_count,
                        'reads_sectors': (disks[disk].read_bytes
                                          / int(self.config['sector_size'])),
                        'reads_milliseconds': disks[disk].read_time,
                        'writes': disks[disk].write_count,
                        'writes_sectors': (disks[disk].write_bytes
                                           / int(self.config['sector_size'])),
                        'writes_milliseconds': disks[disk].write_time,
                        'io_milliseconds':
                        disks[disk].read_time + disks[disk].write_time,
                        'io_milliseconds_weighted':
                        disks[disk].read_time + disks[disk].write_time
                    }

        return result

    def collect(self):

        # Handle collection time intervals correctly
        CollectTime = time.time()
        time_delta = float(self.config['interval'])
        if self.LastCollectTime:
            time_delta = CollectTime - self.LastCollectTime
        if not time_delta:
            time_delta = float(self.config['interval'])
        self.LastCollectTime = CollectTime

        exp = self.config['devices']
        reg = re.compile(exp)

        results = self.get_disk_statistics()
        if not results:
            self.log.error('No diskspace metrics retrieved')
            return None

        for key, info in results.iteritems():
            metrics = {}
            name = info['device']
            if not reg.match(name):
                continue

            for key, value in info.iteritems():
                if key == 'device':
                    continue
                oldkey = key

                for unit in self.config['byte_unit']:
                    key = oldkey

                    if key.endswith('sectors'):
                        key = key.replace('sectors', unit)
                        value /= (1024 / int(self.config['sector_size']))
                        value = diamond.convertor.binary.convert(value=value,
                                                                 oldUnit='kB',
                                                                 newUnit=unit)
                        self.MAX_VALUES[key] = diamond.convertor.binary.convert(
                            value=diamond.collector.MAX_COUNTER,
                            oldUnit='byte',
                            newUnit=unit)

                    metric_name = '.'.join([info['device'], key])
                    # io_in_progress is a point in time counter, !derivative
                    if key != 'io_in_progress':
                        metric_value = self.derivative(
                            metric_name,
                            value,
                            self.MAX_VALUES[key],
                            time_delta=False)
                    else:
                        metric_value = value

                    metrics[key] = metric_value

            if self.proc_diskstats:
                metrics['read_requests_merged_per_second'] = (
                    metrics['reads_merged'] / time_delta)
                metrics['write_requests_merged_per_second'] = (
                    metrics['writes_merged'] / time_delta)

            metrics['reads_per_second'] = metrics['reads'] / time_delta
            metrics['writes_per_second'] = metrics['writes'] / time_delta

            for unit in self.config['byte_unit']:
                metric_name = 'read_%s_per_second' % unit
                key = 'reads_%s' % unit
                metrics[metric_name] = metrics[key] / time_delta

                metric_name = 'write_%s_per_second' % unit
                key = 'writes_%s' % unit
                metrics[metric_name] = metrics[key] / time_delta

                # Set to zero so the nodes are valid even if we have 0 io for
                # the metric duration
                metric_name = 'average_request_size_%s' % unit
                metrics[metric_name] = 0

            metrics['io'] = metrics['reads'] + metrics['writes']

            metrics['average_queue_length'] = (
                metrics['io_milliseconds_weighted']
                / time_delta
                / 1000.0)

            metrics['util_percentage'] = (metrics['io_milliseconds']
                                          / time_delta
                                          / 10.0)

            if metrics['reads'] > 0:
                metrics['read_await'] = (
                    metrics['reads_milliseconds'] / metrics['reads'])
            else:
                metrics['read_await'] = 0

            if metrics['writes'] > 0:
                metrics['write_await'] = (
                    metrics['writes_milliseconds'] / metrics['writes'])
            else:
                metrics['write_await'] = 0

            for unit in self.config['byte_unit']:
                rkey = 'reads_%s' % unit
                wkey = 'writes_%s' % unit
                metric_name = 'average_request_size_%s' % unit
                if (metrics['io'] > 0):
                    metrics[metric_name] = (
                        metrics[rkey] + metrics[wkey]) / metrics['io']
                else:
                    metrics[metric_name] = 0

            metrics['iops'] = metrics['io'] / time_delta

            if (metrics['io'] > 0):
                metrics['service_time'] = (
                    metrics['io_milliseconds'] / metrics['io'])
                metrics['await'] = (
                    metrics['reads_milliseconds']
                    + metrics['writes_milliseconds']) / metrics['io']
            else:
                metrics['service_time'] = 0
                metrics['await'] = 0

            # http://www.scribd.com/doc/15013525
            # Page 28
            metrics['concurrent_io'] = (metrics['reads_per_second']
                                        + metrics['writes_per_second']
                                        ) * (metrics['service_time']
                                             / 1000.0)

            # Only publish when we have io figures
            if (metrics['io'] > 0 or self.config['send_zero']):
                for key in metrics:
                    metric_name = '.'.join([info['device'], key]).replace(
                        '/', '_')
                    self.publish(metric_name, metrics[key])

########NEW FILE########
__FILENAME__ = testdiskusage
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from diskusage import DiskUsageCollector

################################################################################


class TestDiskUsageCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('DiskUsageCollector', {
            'interval': 10,
            'sector_size': '512',
            'byte_unit': 'kilobyte'
        })

        self.collector = DiskUsageCollector(config, None)

    def test_config(self):
        self.assertFalse(self.collector.config['send_zero'])

    def test_import(self):
        self.assertTrue(DiskUsageCollector)

    @patch('os.access', Mock(return_value=True))
    def test_get_disk_statistics(self):

        patch_open = patch(
            '__builtin__.open',
            Mock(return_value=self.getFixture('diskstats')))

        open_mock = patch_open.start()
        result = self.collector.get_disk_statistics()
        patch_open.stop()

        open_mock.assert_called_once_with('/proc/diskstats')

        self.assertEqual(
            sorted(result.keys()),
            [(8,  0), (8,  1), (8, 16), (8, 17), (8, 32),
                (8, 33), (8, 48), (8, 49), (9,  0)])

        return result

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):

        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture('proc_diskstats_1')))
        patch_time = patch('time.time', Mock(return_value=10))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture('proc_diskstats_2')))
        patch_time = patch('time.time', Mock(return_value=20))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        metrics = self.getPickledResults('test_should_work_with_real_data.pkl')

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_verify_supporting_vda_and_xvdb(self, publish_mock):
        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_1_vda_xvdb')))
        patch_time = patch('time.time', Mock(return_value=10))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_2_vda_xvdb')))
        patch_time = patch('time.time', Mock(return_value=20))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        metrics = self.getPickledResults(
            'test_verify_supporting_vda_and_xvdb.pkl')

        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_verify_supporting_md_dm(self, publish_mock):
        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_1_md_dm')))
        patch_time = patch('time.time', Mock(return_value=10))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_2_md_dm')))
        patch_time = patch('time.time', Mock(return_value=20))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        metrics = self.getPickledResults('test_verify_supporting_md_dm.pkl')

        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_verify_supporting_disk(self, publish_mock):
        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_1_disk')))
        patch_time = patch('time.time', Mock(return_value=10))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_2_disk')))
        patch_time = patch('time.time', Mock(return_value=20))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        metrics = self.getPickledResults('test_verify_supporting_disk.pkl')
        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_service_Time(self, publish_mock):
        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_1_service_time')))
        patch_time = patch('time.time', Mock(return_value=10))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch(
            '__builtin__.open',
            Mock(
                return_value=self.getFixture(
                    'proc_diskstats_2_service_time')))
        patch_time = patch('time.time', Mock(return_value=70))

        patch_open.start()
        patch_time.start()
        self.collector.collect()
        patch_open.stop()
        patch_time.stop()

        metrics = self.getPickledResults('test_service_Time.pkl')

        self.assertPublishedMany(publish_mock, metrics)


################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = drbd
# coding=utf-8

"""
DRBD metric collector

  Read and publish metrics from all available resources in /proc/drbd
"""

import diamond.collector
import re


class DRBDCollector(diamond.collector.Collector):
    """
    DRBD Simple metric collector
    """
    def get_default_config_help(self):
        config_help = super(DRBDCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(DRBDCollector, self).get_default_config()
        config.update({
            'path': 'drbd'
        })
        return config

    def collect(self):
        """
        Overrides the Collector.collect method
        """
        performance_indicators = {
            'ns': 'network_send',
            'nr': 'network_receive',
            'dw': 'disk_write',
            'dr': 'disk_read',
            'al': 'activity_log',
            'bm': 'bit_map',
            'lo': 'local_count',
            'pe': 'pending',
            'ua': 'unacknowledged',
            'ap': 'application_pending',
            'ep': 'epochs',
            'wo': 'write_order',
            'oos': 'out_of_sync',
            'cs': 'connection_state',
            'ro': 'roles',
            'ds': 'disk_states'
            }

        results = dict()
        try:
            statusfile = open('/proc/drbd', 'r')
            current_resource = ''
            for line in statusfile:
                if re.search('version', line) is None:
                    if re.search(r' \d: cs', line):
                        matches = re.match(r' (\d): (cs:\w+) (ro:\w+/\w+) '
                                           '(ds:\w+/\w+) (\w{1}) .*', line)
                        current_resource = matches.group(1)
                        results[current_resource] = dict()
                    elif re.search(r'\sns:', line):
                        metrics = line.strip().split(" ")
                        for metric in metrics:
                            item, value = metric.split(":")
                            results[current_resource][
                                performance_indicators[item]] = value

                else:
                    continue
            statusfile.close()
        except IOError, errormsg:
            self.log.error("Can't read DRBD status file: {0}".format(errormsg))
            return

        for resource in results.keys():
            for metric_name, metric_value in results[resource].items():
                if metric_value.isdigit():
                    self.publish(resource + "." + metric_name, metric_value)
                else:
                    continue

########NEW FILE########
__FILENAME__ = testdrbd
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest

from drbd import DRBDCollector

################################################################################


class TestDRBDCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('DRBDCollector', {})

        self.collector = DRBDCollector(config, None)

    def test_import(self):
        self.assertTrue(DRBDCollector)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = dropwizard
# coding=utf-8

"""
Collect [dropwizard](http://dropwizard.codahale.com/) stats for the local node

#### Dependencies

 * urlib2

"""

import urllib2

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import diamond.collector


class DropwizardCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(DropwizardCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': "",
            'port': "",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(DropwizardCollector, self).get_default_config()
        config.update({
            'host':     '127.0.0.1',
            'port':     8081,
            'path':     'dropwizard',
        })
        return config

    def collect(self):
        if json is None:
            self.log.error('Unable to import json')
            return {}
        url = 'http://%s:%i/metrics' % (
            self.config['host'], int(self.config['port']))
        try:
            response = urllib2.urlopen(url)
        except urllib2.HTTPError, err:
            self.log.error("%s: %s", url, err)
            return

        try:
            result = json.load(response)
        except (TypeError, ValueError):
            self.log.error("Unable to parse response from elasticsearch as a"
                           + " json object")
            return

        metrics = {}

        memory = result['jvm']['memory']
        mempool = memory['memory_pool_usages']
        jvm = result['jvm']
        thread_st = jvm['thread-states']

        metrics['jvm.memory.totalInit'] = memory['totalInit']
        metrics['jvm.memory.totalUsed'] = memory['totalUsed']
        metrics['jvm.memory.totalMax'] = memory['totalMax']
        metrics['jvm.memory.totalCommitted'] = memory['totalCommitted']

        metrics['jvm.memory.heapInit'] = memory['heapInit']
        metrics['jvm.memory.heapUsed'] = memory['heapUsed']
        metrics['jvm.memory.heapMax'] = memory['heapMax']
        metrics['jvm.memory.heapCommitted'] = memory['heapCommitted']
        metrics['jvm.memory.heap_usage'] = memory['heap_usage']
        metrics['jvm.memory.non_heap_usage'] = memory['non_heap_usage']
        metrics['jvm.memory.code_cache'] = mempool['Code Cache']
        metrics['jvm.memory.eden_space'] = mempool['PS Eden Space']
        metrics['jvm.memory.old_gen'] = mempool['PS Old Gen']
        metrics['jvm.memory.perm_gen'] = mempool['PS Perm Gen']
        metrics['jvm.memory.survivor_space'] = mempool['PS Survivor Space']

        metrics['jvm.daemon_thread_count'] = jvm['daemon_thread_count']
        metrics['jvm.thread_count'] = jvm['thread_count']
        metrics['jvm.fd_usage'] = jvm['fd_usage']

        metrics['jvm.thread_states.timed_waiting'] = thread_st['timed_waiting']
        metrics['jvm.thread_states.runnable'] = thread_st['runnable']
        metrics['jvm.thread_states.blocked'] = thread_st['blocked']
        metrics['jvm.thread_states.waiting'] = thread_st['waiting']
        metrics['jvm.thread_states.new'] = thread_st['new']
        metrics['jvm.thread_states.terminated'] = thread_st['terminated']

        for key in metrics:
            self.publish(key, metrics[key])

########NEW FILE########
__FILENAME__ = testdropwizard
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from dropwizard import DropwizardCollector

################################################################################


class TestDropwizardCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('DropwizardCollector', {})

        self.collector = DropwizardCollector(config, None)

    def test_import(self):
        self.assertTrue(DropwizardCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen',
                              Mock(return_value=self.getFixture('stats')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'jvm.memory.totalInit': 9.142272E7,
            'jvm.memory.totalUsed': 1.29674584E8,
            'jvm.memory.totalMax': 1.13901568E9,
            'jvm.memory.totalCommitted': 1.62267136E8,
            'jvm.memory.heapInit': 6.7108864E7,
            'jvm.memory.heapUsed': 8.3715232E7,
            'jvm.memory.heapMax': 9.54466304E8,
            'jvm.memory.heapCommitted': 1.15539968E8,
            'jvm.memory.heap_usage': 0.08770894441130528,
            'jvm.memory.non_heap_usage': 0.24903553182428534,
            'jvm.memory.code_cache': 0.038289388020833336,
            'jvm.memory.eden_space': 0.1918924383560846,
            'jvm.memory.old_gen': 0.022127459689416828,
            'jvm.memory.perm_gen': 0.32806533575057983,
            'jvm.memory.survivor_space': 1.0,
            'jvm.daemon_thread_count': 10,
            'jvm.thread_count': 27,
            'jvm.fd_usage': 0.014892578125,
            'jvm.thread_states.timed_waiting': 0.5185185185185185,
            'jvm.thread_states.runnable': 0.0,
            'jvm.thread_states.blocked': 0.0,
            'jvm.thread_states.waiting': 0.2222222222222222,
            'jvm.thread_states.new': 0.0,
            'jvm.thread_states.terminated': 0.0
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch(
            'urllib2.urlopen',
            Mock(
                return_value=self.getFixture('stats_blank')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = elasticsearch
# coding=utf-8

"""
Collect the elasticsearch stats for the local node

#### Dependencies

 * urlib2

"""

import urllib2
import re
from diamond.collector import str_to_bool

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import diamond.collector

RE_LOGSTASH_INDEX = re.compile('^(.*)-\d\d\d\d\.\d\d\.\d\d$')


class ElasticSearchCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(ElasticSearchCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': "",
            'port': "",
            'stats': "Available stats: \n"
            + " - jvm (JVM information) \n"
            + " - thread_pool (Thread pool information) \n"
            + " - indices (Individual index stats)\n",
            'logstash_mode': "If 'indices' stats are gathered, remove "
            + "the YYYY.MM.DD suffix from the index name "
            + "(e.g. logstash-adm-syslog-2014.01.03) and use that "
            + "as a bucket for all 'day' index stats.",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ElasticSearchCollector, self).get_default_config()
        config.update({
            'host':     '127.0.0.1',
            'port':     9200,
            'path':     'elasticsearch',
            'stats':    ['jvm', 'thread_pool', 'indices'],
            'logstash_mode': False,
            'cluster':  False,
        })
        return config

    def _get(self, path):
        url = 'http://%s:%i/%s' % (
            self.config['host'], int(self.config['port']), path)
        try:
            response = urllib2.urlopen(url)
        except Exception, err:
            self.log.error("%s: %s", url, err)
            return False

        try:
            return json.load(response)
        except (TypeError, ValueError):
            self.log.error("Unable to parse response from elasticsearch as a"
                           + " json object")
            return False

    def _copy_one_level(self, metrics, prefix, data, filter=lambda key: True):
        for key, value in data.iteritems():
            if filter(key):
                metric_path = '%s.%s' % (prefix, key)
                self._set_or_sum_metric(metrics, metric_path, value)

    def _copy_two_level(self, metrics, prefix, data, filter=lambda key: True):
        for key1, d1 in data.iteritems():
            self._copy_one_level(metrics, '%s.%s' % (prefix, key1), d1, filter)

    def _index_metrics(self, metrics, prefix, index):
        if self.config['logstash_mode']:
            """Remove the YYYY.MM.DD bit from logstash indices.
            This way we keep using the same metric naming and not polute
            our metrics system (e.g. Graphite) with new metrics every day."""
            m = RE_LOGSTASH_INDEX.match(prefix)
            if m:
                prefix = m.group(1)

                # keep a telly of the number of indexes
                self._set_or_sum_metric(metrics,
                                        '%s.indexes_in_group' % prefix, 1)

        self._add_metric(metrics, '%s.docs.count' % prefix, index,
                         ['docs', 'count'])
        self._add_metric(metrics, '%s.docs.deleted' % prefix, index,
                         ['docs', 'deleted'])
        self._add_metric(metrics, '%s.datastore.size' % prefix, index,
                         ['store', 'size_in_bytes'])

        # publish all 'total' and 'time_in_millis' stats
        self._copy_two_level(
            metrics, prefix, index,
            lambda key: key.endswith('total') or key.endswith('time_in_millis'))

    def _add_metric(self, metrics, metric_path, data, data_path):
        """If the path specified by data_path (a list) exists in data,
        add to metrics.  Use when the data path may not be present"""
        current_item = data
        for path_element in data_path:
            current_item = current_item.get(path_element)
            if current_item is None:
                return

        self._set_or_sum_metric(metrics, metric_path, current_item)

    def _set_or_sum_metric(self, metrics, metric_path, value):
        """If we already have a datapoint for this metric, lets add
        the value. This is used when the logstash mode is enabled."""
        if metric_path in metrics:
            metrics[metric_path] += value
        else:
            metrics[metric_path] = value

    def collect(self):
        if json is None:
            self.log.error('Unable to import json')
            return {}

        result = self._get('_nodes/_local/stats?all=true')
        if not result:
            return

        metrics = {}
        node = result['nodes'].keys()[0]
        data = result['nodes'][node]

        #
        # http connections to ES
        metrics['http.current'] = data['http']['current_open']

        #
        # indices
        indices = data['indices']
        metrics['indices.docs.count'] = indices['docs']['count']
        metrics['indices.docs.deleted'] = indices['docs']['deleted']

        metrics['indices.datastore.size'] = indices['store']['size_in_bytes']

        transport = data['transport']
        metrics['transport.rx.count'] = transport['rx_count']
        metrics['transport.rx.size'] = transport['rx_size_in_bytes']
        metrics['transport.tx.count'] = transport['tx_count']
        metrics['transport.tx.size'] = transport['tx_size_in_bytes']

        # elasticsearch < 0.90RC2
        if 'cache' in indices:
            cache = indices['cache']

            self._add_metric(metrics, 'cache.bloom.size', cache,
                             ['bloom_size_in_bytes'])
            self._add_metric(metrics, 'cache.field.evictions', cache,
                             ['field_evictions'])
            self._add_metric(metrics, 'cache.field.size', cache,
                             ['field_size_in_bytes'])
            metrics['cache.filter.count'] = cache['filter_count']
            metrics['cache.filter.evictions'] = cache['filter_evictions']
            metrics['cache.filter.size'] = cache['filter_size_in_bytes']
            self._add_metric(metrics, 'cache.id.size', cache,
                             ['id_cache_size_in_bytes'])

        # elasticsearch >= 0.90RC2
        if 'filter_cache' in indices:
            cache = indices['filter_cache']

            metrics['cache.filter.evictions'] = cache['evictions']
            metrics['cache.filter.size'] = cache['memory_size_in_bytes']
            self._add_metric(metrics, 'cache.filter.count', cache, ['count'])

        # elasticsearch >= 0.90RC2
        if 'id_cache' in indices:
            cache = indices['id_cache']

            self._add_metric(metrics, 'cache.id.size', cache,
                             ['memory_size_in_bytes'])

        # elasticsearch >= 0.90
        if 'fielddata' in indices:
            fielddata = indices['fielddata']
            self._add_metric(metrics, 'fielddata.size', fielddata,
                             ['memory_size_in_bytes'])
            self._add_metric(metrics, 'fielddata.evictions', fielddata,
                             ['evictions'])

        #
        # process mem/cpu (may not be present, depending on access restrictions)
        self._add_metric(metrics, 'process.cpu.percent', data,
                         ['process', 'cpu', 'percent'])
        self._add_metric(metrics, 'process.mem.resident', data,
                         ['process', 'mem', 'resident_in_bytes'])
        self._add_metric(metrics, 'process.mem.share', data,
                         ['process', 'mem', 'share_in_bytes'])
        self._add_metric(metrics, 'process.mem.virtual', data,
                         ['process', 'mem', 'total_virtual_in_bytes'])

        #
        # filesystem (may not be present, depending on access restrictions)
        if 'fs' in data and 'data' in data['fs'] and data['fs']['data']:
            fs_data = data['fs']['data'][0]
            self._add_metric(metrics, 'disk.reads.count', fs_data,
                             ['disk_reads'])
            self._add_metric(metrics, 'disk.reads.size', fs_data,
                             ['disk_read_size_in_bytes'])
            self._add_metric(metrics, 'disk.writes.count', fs_data,
                             ['disk_writes'])
            self._add_metric(metrics, 'disk.writes.size', fs_data,
                             ['disk_write_size_in_bytes'])

        #
        # jvm
        if 'jvm' in self.config['stats']:
            jvm = data['jvm']
            mem = jvm['mem']
            for k in ('heap_used', 'heap_committed', 'non_heap_used',
                      'non_heap_committed'):
                metrics['jvm.mem.%s' % k] = mem['%s_in_bytes' % k]

            for pool, d in mem['pools'].iteritems():
                pool = pool.replace(' ', '_')
                metrics['jvm.mem.pools.%s.used' % pool] = d['used_in_bytes']
                metrics['jvm.mem.pools.%s.max' % pool] = d['max_in_bytes']

            metrics['jvm.threads.count'] = jvm['threads']['count']

            gc = jvm['gc']
            collection_count = 0
            collection_time_in_millis = 0
            for collector, d in gc['collectors'].iteritems():
                metrics['jvm.gc.collection.%s.count' % collector] = d[
                    'collection_count']
                collection_count += d['collection_count']
                metrics['jvm.gc.collection.%s.time' % collector] = d[
                    'collection_time_in_millis']
                collection_time_in_millis += d['collection_time_in_millis']
            # calculate the totals, as they're absent in elasticsearch > 0.90.10
            if 'collection_count' in gc:
                metrics['jvm.gc.collection.count'] = gc['collection_count']
            else:
                metrics['jvm.gc.collection.count'] = collection_count

            k = 'collection_time_in_millis'
            if k in gc:
                metrics['jvm.gc.collection.time'] = gc[k]
            else:
                metrics['jvm.gc.collection.time'] = collection_time_in_millis

        #
        # thread_pool
        if 'thread_pool' in self.config['stats']:
            self._copy_two_level(metrics, 'thread_pool', data['thread_pool'])

        #
        # network
        self._copy_two_level(metrics, 'network', data['network'])

         #cluster
        if str_to_bool(self.config['cluster']):
            result = self._get('_cluster/health')

            if not result:
                return

            self._add_metric(metrics, 'cluster_health.nodes.total',
                             result, ['number_of_nodes'])
            self._add_metric(metrics, 'cluster_health.nodes.data',
                             result, ['number_of_data_nodes'])
            self._add_metric(metrics, 'cluster_health.shards.active_primary',
                             result, ['active_primary_shards'])
            self._add_metric(metrics, 'cluster_health.shards.active',
                             result, ['active_shards'])
            self._add_metric(metrics, 'cluster_health.shards.relocating',
                             result, ['relocating_shards'])
            self._add_metric(metrics, 'cluster_health.shards.unassigned',
                             result, ['unassigned_shards'])
            self._add_metric(metrics, 'cluster_health.shards.initializing',
                             result, ['initializing_shards'])

        if 'indices' in self.config['stats']:
            #
            # individual index stats
            result = self._get('_stats?clear=true&docs=true&store=true&'
                               + 'indexing=true&get=true&search=true')
            if not result:
                return

            _all = result['_all']
            self._index_metrics(metrics, 'indices._all', _all['primaries'])

            if 'indices' in _all:
                indices = _all['indices']
            elif 'indices' in result:          # elasticsearch >= 0.90RC2
                indices = result['indices']
            else:
                return

            for name, index in indices.iteritems():
                self._index_metrics(metrics, 'indices.%s' % name,
                                    index['primaries'])

        for key in metrics:
            self.publish(key, metrics[key])

########NEW FILE########
__FILENAME__ = testelasticsearch
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from elasticsearch import ElasticSearchCollector

################################################################################


class TestElasticSearchCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('ElasticSearchCollector', {})

        self.collector = ElasticSearchCollector(config, None)

    def test_import(self):
        self.assertTrue(ElasticSearchCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        returns = [self.getFixture('stats'),
                   self.getFixture('indices_stats'),
                   self.getFixture('cluster_stats')]
        urlopen_mock = patch('urllib2.urlopen', Mock(
            side_effect=lambda *args: returns.pop(0)))

        urlopen_mock.start()
        self.collector.collect()
        urlopen_mock.stop()

        metrics = {
            'http.current': 1,

            'indices.docs.count': 11968062,
            'indices.docs.deleted': 2692068,
            'indices.datastore.size': 22724243633,

            'indices._all.docs.count': 4,
            'indices._all.docs.deleted': 0,
            'indices._all.datastore.size': 2674,

            'indices.test.docs.count': 4,
            'indices.test.docs.deleted': 0,
            'indices.test.datastore.size': 2674,

            'process.cpu.percent': 58,

            'process.mem.resident': 5192126464,
            'process.mem.share': 11075584,
            'process.mem.virtual': 7109668864,

            'disk.reads.count': 55996,
            'disk.reads.size': 1235387392,
            'disk.writes.count': 5808198,
            'disk.writes.size': 23287275520,

            'thread_pool.generic.threads': 1,

            'network.tcp.active_opens': 2299,

            'jvm.mem.pools.CMS_Old_Gen.used': 530915016,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_logstash_mode(self, publish_mock):
        returns = [
            self.getFixture('stats'),
            self.getFixture('logstash_indices_stats'),
            self.getFixture('cluster_stats'),
        ]
        urlopen_mock = patch('urllib2.urlopen', Mock(
            side_effect=lambda *args: returns.pop(0)))

        self.collector.config['logstash_mode'] = True

        urlopen_mock.start()
        self.collector.collect()
        urlopen_mock.stop()

        # Omit all non-indices metrics, since those were already
        # checked in previous test.
        metrics = {
            'indices.docs.count': 11968062,
            'indices.docs.deleted': 2692068,
            'indices.datastore.size': 22724243633,

            'indices._all.docs.count': 35856619,
            'indices._all.docs.deleted': 0,
            'indices._all.datastore.size': 21903813340,

            'indices._all.get.exists_time_in_millis': 0,
            'indices._all.get.exists_total': 0,
            'indices._all.get.missing_time_in_millis': 0,
            'indices._all.get.missing_total': 0,
            'indices._all.get.time_in_millis': 0,
            'indices._all.get.total': 0,
            'indices._all.indexing.delete_time_in_millis': 0,
            'indices._all.indexing.delete_total': 0,
            'indices._all.indexing.index_time_in_millis': 29251475,
            'indices._all.indexing.index_total': 35189321,
            'indices._all.search.fetch_time_in_millis': 6962,
            'indices._all.search.fetch_total': 4084,
            'indices._all.search.query_time_in_millis': 41211,
            'indices._all.search.query_total': 4266,
            'indices._all.store.throttle_time_in_millis': 0,

            'indices.logstash-adm-syslog.indexes_in_group': 3,

            'indices.logstash-adm-syslog.datastore.size': 21903813340,
            'indices.logstash-adm-syslog.docs.count': 35856619,
            'indices.logstash-adm-syslog.docs.deleted': 0,
            'indices.logstash-adm-syslog.get.exists_time_in_millis': 0,
            'indices.logstash-adm-syslog.get.exists_total': 0,
            'indices.logstash-adm-syslog.get.missing_time_in_millis': 0,
            'indices.logstash-adm-syslog.get.missing_total': 0,
            'indices.logstash-adm-syslog.get.time_in_millis': 0,
            'indices.logstash-adm-syslog.get.total': 0,
            'indices.logstash-adm-syslog.indexing.delete_time_in_millis': 0,
            'indices.logstash-adm-syslog.indexing.delete_total': 0,
            'indices.logstash-adm-syslog.indexing.index_time_in_millis': 29251475,  # NOQA
            'indices.logstash-adm-syslog.indexing.index_total': 35189321,
            'indices.logstash-adm-syslog.search.fetch_time_in_millis': 6962,
            'indices.logstash-adm-syslog.search.fetch_total': 4084,
            'indices.logstash-adm-syslog.search.query_time_in_millis': 41211,
            'indices.logstash-adm-syslog.search.query_total': 4266,
            'indices.logstash-adm-syslog.store.throttle_time_in_millis': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_0_90_data(self, publish_mock):
        returns = [
            self.getFixture('stats0.90'),
            self.getFixture('indices_stats'),
            self.getFixture('cluster_stats')]
        urlopen_mock = patch('urllib2.urlopen', Mock(
            side_effect=lambda *args: returns.pop(0)))

        urlopen_mock.start()
        self.collector.collect()
        urlopen_mock.stop()

        # test some 0.90 specific stats
        metrics = {
            'cache.filter.size': 1700,
            'cache.filter.evictions': 9,
            'cache.id.size': 98,
            'fielddata.size': 1448,
            'fielddata.evictions': 12,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        urlopen_mock = patch('urllib2.urlopen', Mock(
                             return_value=self.getFixture('stats_blank')))

        urlopen_mock.start()
        self.collector.collect()
        urlopen_mock.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = elb
# coding=utf-8

"""
The ELB collector collects metrics for one or more Amazon AWS ELBs

#### Configuration

Below is an example configuration for the ELBCollector.
You can specify an arbitrary amount of regions

```
    enabled = true
    interval = 60
    # max number of delayed metrics to tolerate
    max_delayed = 10

    # Optional
    access_key_id = ...
    secret_access_key = ...

    # Optional - Available keys: region, zone, elb_name, metric_name
    format = $elb_name.$zone.$metric_name

    [[regions]]

    [us-west-1]
    # Optional - queries all elbs if omitted
    elb_names = elb1, elb2, ...

    [us-west-2]
    ...

```

#### Dependencies

 * boto

"""
import calendar
#from datetime import datetime, timedelta
import datetime
import time
from string import Template

import diamond.collector
from diamond.metric import Metric

try:
    import boto.ec2.elb
    from boto.ec2 import cloudwatch
    from boto.exception import NoAuthHandlerFound
    cloudwatch  # Pyflakes
except ImportError:
    cloudwatch = False


def utc_to_local(utc_dt):
    """
    :param utc_dt: datetime in UTC
    :return: datetime in the local timezone
    """
    # get integer timestamp to avoid precision lost
    timestamp = calendar.timegm(utc_dt.timetuple())
    local_dt = datetime.datetime.fromtimestamp(timestamp)
    assert utc_dt.resolution >= datetime.timedelta(microseconds=1)
    return local_dt.replace(microsecond=utc_dt.microsecond)


class ElbCollector(diamond.collector.Collector):

    # (aws metric name, aws statistic type, diamond metric type, diamond
    # precision, emit zero when no value available)
    metrics = [
        ('HealthyHostCount', 'Average', 'GAUGE', 0, False),
        ('UnHealthyHostCount', 'Average', 'GAUGE', 0, False),
        ('RequestCount', 'Sum', 'COUNTER', 0, True),
        ('Latency', 'Average', 'GAUGE', 4, False),
        ('HTTPCode_ELB_4XX', 'Sum', 'COUNTER', 0, True),
        ('HTTPCode_ELB_5XX', 'Sum', 'COUNTER', 0, True),
        ('HTTPCode_Backend_2XX', 'Sum', 'COUNTER', 0, True),
        ('HTTPCode_Backend_3XX', 'Sum', 'COUNTER', 0, True),
        ('HTTPCode_Backend_4XX', 'Sum', 'COUNTER', 0, True),
        ('HTTPCode_Backend_5XX', 'Sum', 'COUNTER', 0, True),
        ('BackendConnectionErrors', 'Sum', 'COUNTER', 0, True),
        ('SurgeQueueLength', 'Maximum', 'GAUGE', 0, True),
        ('SpilloverCount', 'Sum', 'COUNTER', 0, True)
    ]

    def __init__(self, config, handlers):
        super(ElbCollector, self).__init__(config, handlers)

        def setup_creds():
            if ('access_key_id' in self.config
                    and 'secret_access_key' in self.config):
                self.auth_kwargs = {
                    'aws_access_key_id': self.config['access_key_id'],
                    'aws_secret_access_key': self.config['secret_access_key']
                }
            else:
                # If creds not present, assume we're using IAM roles with
                # instance profiles. Boto will automatically take care of using
                # the creds from the instance metatdata.
                self.auth_kwargs = {}

        if self.config['enabled']:
            self.interval = self.config.as_int('interval')
            if self.interval % 60 != 0:
                raise Exception('Interval must be a multiple of 60 seconds: %s'
                                % self.interval)
        setup_creds()
        self.max_delayed = self.config.as_int('max_delayed')
        self.history = dict()

    def check_boto(self):
        if not cloudwatch:
            self.log.error("boto module not found!")
            return False
        return True

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ElbCollector, self).get_default_config()
        config.update({
            'path': 'elb',
            'regions': ['us-west-1'],
            'interval': 60,
            'format': '$zone.$elb_name.$metric_name',
            'max_delayed': 10,
        })
        return config

    def get_elb_names(self, region, region_cfg):
        """
        :return: List of elb names to query in the given region
        """
        if 'elb_names' not in region_cfg:
            elb_conn = boto.ec2.elb.connect_to_region(region,
                                                      **self.auth_kwargs)
            elb_names = [elb.name for elb in elb_conn.get_all_load_balancers()]
        else:
            elb_names = region_cfg['elb_names']
        return elb_names

    def publish_delayed_metric(self, name, value, timestamp, raw_value=None,
                               precision=0, metric_type='GAUGE', instance=None):
        """
        Metrics may not be immediately available when querying cloudwatch.
        Hence, allow the ability to publish a metric from some the past given
        its timestamp.
        """
        # Get metric Path
        path = self.get_metric_path(name, instance)

        # Get metric TTL
        ttl = float(self.config['interval']) * float(
            self.config['ttl_multiplier'])

        # Create Metric
        metric = Metric(path, value, raw_value=raw_value, timestamp=timestamp,
                        precision=precision, host=self.get_hostname(),
                        metric_type=metric_type, ttl=ttl)

        # Publish Metric
        self.publish_metric(metric)

    def collect(self):
        if not self.check_boto():
            return

        def cache_zones():
            self.zones_by_region = {}
            for region in self.config['regions']:
                try:
                    ec2_conn = boto.ec2.connect_to_region(region,
                                                          **self.auth_kwargs)
                except NoAuthHandlerFound, e:
                    self.log.error(e)
                    continue

                self.zones_by_region[region] = [
                    zone.name for zone in ec2_conn.get_all_zones()]

        cache_zones()

        now = datetime.datetime.utcnow()
        end_time = now.replace(second=0, microsecond=0)
        start_time = end_time - datetime.timedelta(seconds=self.interval)

        for (region, region_cfg) in self.config['regions'].items():
            conn = cloudwatch.connect_to_region(region, **self.auth_kwargs)
            for elb_name in self.get_elb_names(region, region_cfg):
                for (metric_name, statistic,
                        metric_type, precision,
                        default_to_zero) in self.metrics:
                    for zone in self.zones_by_region[region]:

                        metric_key = (zone, elb_name, metric_name)
                        if metric_key not in self.history:
                            self.history[metric_key] = list()
                        current_history = self.history[metric_key]

                        tick = (start_time, end_time)
                        current_history.append(tick)

                        # only keep latest MAX_TICKS
                        if len(current_history) > self.max_delayed:
                            del current_history[0]

                        span_start, _ = current_history[0]
                        _, span_end = current_history[-1]

                        # get stats for the span of history for which we don't
                        # have values
                        stats = conn.get_metric_statistics(
                            self.config['interval'],
                            span_start,
                            span_end,
                            metric_name,
                            namespace='AWS/ELB',
                            statistics=[statistic],
                            dimensions={'LoadBalancerName': elb_name,
                                        'AvailabilityZone': zone})

                        #self.log.debug('history = %s' % current_history)
                        #self.log.debug('stats = %s' % stats)

                        # create a fake stat if the current metric
                        # should default to zero when a stat is
                        # not returned. Cloudwatch just skips the
                        # metric entirely instead of wasting space
                        # to store/emit a zero.
                        if len(stats) == 0 and default_to_zero:
                            stats.append({
                                u'Timestamp': span_start,
                                statistic: 0.0,
                                u'Unit': u'Count'
                            })

                        # match up each individual stat to what we have in
                        # history and publish it.
                        for stat in stats:
                            ts = stat['Timestamp']
                            # TODO: maybe use a dict for matching
                            for i, tick in enumerate(current_history):
                                tick_start, tick_end = tick
                                if ts == tick_start:
                                    #self.log.warn(tick)
                                    #self.log.warn(stat)
                                    del current_history[i]

                                    template_tokens = {
                                        'region': region,
                                        'zone': zone,
                                        'elb_name': elb_name,
                                        'metric_name': metric_name,
                                    }
                                    name_template = Template(
                                        self.config['format'])
                                    formatted_name = name_template.substitute(
                                        template_tokens)
                                    self.publish_delayed_metric(
                                        formatted_name,
                                        stat[statistic],
                                        metric_type=metric_type,
                                        precision=precision,
                                        timestamp=time.mktime(
                                            utc_to_local(tick_end).timetuple()))
                                    break

########NEW FILE########
__FILENAME__ = testelb
#!/usr/bin/python
# coding=utf-8

import datetime
import mock

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import patch
from test import run_only
from mock import Mock

from diamond.collector import Collector
from elb import ElbCollector, utc_to_local


def run_only_if_boto_is_available(func):
    try:
        import boto
        boto  # workaround for pyflakes issue #13
    except ImportError:
        boto = None
    pred = lambda: boto is not None
    return run_only(func, pred)


class TestElbCollector(CollectorTestCase):

    @run_only_if_boto_is_available
    def test_throws_exception_when_interval_not_multiple_of_60(self):
        config = get_collector_config(
            'ElbCollector',
            {'enabled': True,
             'interval': 10})
        assertRaisesAndContains(Exception, 'multiple of', ElbCollector,
                                *[config, None])

    @run_only_if_boto_is_available
    @patch('elb.cloudwatch')
    @patch('boto.ec2.connect_to_region')
    @patch.object(Collector, 'publish_metric')
    def test_collect(self, publish_metric, connect_to_region, cloudwatch):
        config = get_collector_config(
            'ElbCollector',
            {
                'enabled': True,
                'interval': 60,
                'regions': {
                    'us-west-1': {
                        'elb_names': ['elb1'],
                    }
                }
            })

        az = Mock()
        az.name = 'us-west-1a'

        ec2_conn = Mock()
        ec2_conn.get_all_zones = Mock()
        ec2_conn.get_all_zones.return_value = [az]
        connect_to_region.return_value = ec2_conn

        cw_conn = Mock()
        cw_conn.get_metric_statistics = Mock()
        ts = datetime.datetime.utcnow().replace(second=0, microsecond=0)

        cw_conn.get_metric_statistics.side_effect = [
            [{u'Timestamp': ts, u'Average': 1.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Average': 2.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 3.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Average': 4.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 6.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 7.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 8.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 9.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 10.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 11.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 12.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Maximum': 13.0, u'Unit': u'Count'}],
            [{u'Timestamp': ts, u'Sum': 14.0, u'Unit': u'Count'}],
        ]

        cloudwatch.connect_to_region = Mock()
        cloudwatch.connect_to_region.return_value = cw_conn

        collector = ElbCollector(config, handlers=[])

        target = ts + datetime.timedelta(minutes=1)
        with mock.patch.object(datetime, 'datetime',
                               mock.Mock(wraps=datetime.datetime)) as patched:
            patched.utcnow.return_value = target
            collector.collect()

        self.assertPublishedMetricMany(
            publish_metric,
            {
                'us-west-1a.elb1.HealthyHostCount': 1,
                'us-west-1a.elb1.UnHealthyHostCount': 2,
                'us-west-1a.elb1.RequestCount': 3,
                'us-west-1a.elb1.Latency': 4,
                'us-west-1a.elb1.HTTPCode_ELB_4XX': 6,
                'us-west-1a.elb1.HTTPCode_ELB_5XX': 7,
                'us-west-1a.elb1.HTTPCode_Backend_2XX': 8,
                'us-west-1a.elb1.HTTPCode_Backend_3XX': 9,
                'us-west-1a.elb1.HTTPCode_Backend_4XX': 10,
                'us-west-1a.elb1.HTTPCode_Backend_5XX': 11,
                'us-west-1a.elb1.BackendConnectionErrors': 12,
                'us-west-1a.elb1.SurgeQueueLength': 13,
                'us-west-1a.elb1.SpilloverCount': 14,
            })


def assertRaisesAndContains(excClass, contains_str, callableObj, *args,
                            **kwargs):
    try:
        callableObj(*args, **kwargs)
    except excClass, e:
        msg = str(e)
        if contains_str in msg:
            return
        else:
            raise AssertionError(
                "Exception message does not contain '%s': '%s'" % (
                    contains_str, msg))
    else:
        if hasattr(excClass, '__name__'):
            excName = excClass.__name__
        else:
            excName = str(excClass)
        raise AssertionError("%s not raised" % excName)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = entropy
# coding=utf-8

"""
Uses /proc to collect available entropty

#### Dependencies

 * /proc/sys/kernel/random/entropy_avail

"""

import diamond.collector
import os


class EntropyStatCollector(diamond.collector.Collector):

    PROC = '/proc/sys/kernel/random/entropy_avail'

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(EntropyStatCollector, self).get_default_config()
        config.update({
            'enabled':  'False',
            'path':     'entropy'
        })
        return config

    def collect(self):
        if not os.access(self.PROC, os.R_OK):
            return None

        # open file
        entropy_file = open(self.PROC)

        # read value
        entropy = entropy_file.read().strip()

        # Close file
        entropy_file.close()

        # Publish value
        self.publish_gauge("available", entropy)

########NEW FILE########
__FILENAME__ = testentropy
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest

from entropy import EntropyStatCollector

################################################################################


class TestEntropyStatCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('EntropyStatCollector', {
        })

        self.collector = EntropyStatCollector(config, None)

    def test_import(self):
        self.assertTrue(EntropyStatCollector)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = example
# coding=utf-8

"""
An example collector that verifies the answer to life, the universe, and
everything does not change.

#### Dependencies

 * A sane universe

#### Customizing a collector

Diamond collectors run within the diamond process and collect metrics that can
be published to a graphite server.

Collectors are subclasses of diamond.collector.Collector. In their simplest
form, they need to implement a single method called "collect".

    import diamond.collector

    class ExampleCollector(diamond.collector.Collector):

        def collect(self):
            # Set Metric Name
            metric_name = "my.example.metric"

            # Set Metric Value
            metric_value = 42

            # Publish Metric
            self.publish(metric_name, metric_value)

For testing collectors, create a directory (example below for /tmp/diamond)
containing your new collector(s), their .conf files, and a copy of diamond.conf
with the following options in diamond.conf:

    [server]

    user = ecuser
    group = ecuser

    handlers = diamond.handler.archive.ArchiveHandler
    handlers_config_path = /tmp/diamond/handlers/
    collectors_path = /tmp/diamond/collectors/
    collectors_config_path = /tmp/diamond/collectors/

    collectors_reload_interval = 3600

    [handlers]

    [[default]]

    [[ArchiveHandler]]
    log_file = /dev/stdout

    [collectors]
    [[default]]

and then run diamond in foreground mode:

    # diamond -f -l --skip-pidfile -c /tmp/diamond/diamond.conf

Diamond supports dynamic addition of collectors. Its configured to scan for new
collectors on a regular interval (configured in diamond.cfg).
If diamond detects a new collector, or that a collectors module has changed
(based on the file's mtime), it will be reloaded.

Diamond looks for collectors in /usr/lib/diamond/collectors/ (on Ubuntu). By
default diamond will invoke the *collect* method every 60 seconds.

Diamond collectors that require a separate configuration file should place a
.cfg file in /etc/diamond/collectors/.
The configuration file name should match the name of the diamond collector
class.  For example, a collector called
*examplecollector.ExampleCollector* could have its configuration file placed in
/etc/diamond/collectors/ExampleCollector.cfg.

"""

import diamond.collector


class ExampleCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(ExampleCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ExampleCollector, self).get_default_config()
        config.update({
            'path':     'example'
        })
        return config

    def collect(self):
        """
        Overrides the Collector.collect method
        """

        # Set Metric Name
        metric_name = "my.example.metric"
        # Set Metric Value
        metric_value = 42

        # Publish Metric
        self.publish(metric_name, metric_value)

########NEW FILE########
__FILENAME__ = testexample
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import patch

from diamond.collector import Collector
from example import ExampleCollector

################################################################################


class TestExampleCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('ExampleCollector', {
            'interval': 10
        })

        self.collector = ExampleCollector(config, None)

    def test_import(self):
        self.assertTrue(ExampleCollector)

    @patch.object(Collector, 'publish')
    def test(self, publish_mock):
        self.collector.collect()

        metrics = {
            'my.example.metric':  42
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = exim
# coding=utf-8

"""
Shells out to get the exim queue length

#### Dependencies

 * /usr/sbin/exim

"""

import diamond.collector
import subprocess
import os
from diamond.collector import str_to_bool


class EximCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(EximCollector, self).get_default_config_help()
        config_help.update({
            'bin':         'The path to the exim binary',
            'use_sudo':    'Use sudo?',
            'sudo_cmd':    'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(EximCollector, self).get_default_config()
        config.update({
            'path':            'exim',
            'method':          'threaded',
            'bin':              '/usr/sbin/exim',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
        })
        return config

    def collect(self):
        if not os.access(self.config['bin'], os.X_OK):
            return

        command = [self.config['bin'], '-bpc']

        if str_to_bool(self.config['use_sudo']):
            command.insert(0, self.config['sudo_cmd'])

        queuesize = subprocess.Popen(
            command, stdout=subprocess.PIPE).communicate()[0].split()

        if not len(queuesize):
            return
        queuesize = queuesize[-1]
        self.publish('queuesize', queuesize)

########NEW FILE########
__FILENAME__ = testexim
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from exim import EximCollector

################################################################################


class TestEximCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('EximCollector', {
            'interval': 10,
            'bin': 'true'
        })

        self.collector = EximCollector(config, None)

    def test_import(self):
        self.assertTrue(EximCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        patch_communicate = patch('subprocess.Popen.communicate',
                                  Mock(return_value=('33', '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        metrics = {
            'queuesize': 33.0
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_communicate = patch('subprocess.Popen.communicate',
                                  Mock(return_value=('', '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {})

    @patch('os.access', Mock(return_value=False))
    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully_2(self, publish_mock):
        self.collector.collect()
        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = files
# coding=utf-8

"""
This class collects data from plain text files

#### Dependencies

"""

import diamond.collector
import os
import re

_RE = re.compile(r'([a-z0-9._-]+)[\s=:]([0-9-]+)(\.?\d*)')


class FilesCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(FilesCollector, self).get_default_config_help()
        config_help.update({
            'path': 'Prefix added to all stats collected by this module, a '
                    'single dot means don''t add prefix',
            'dir': 'The directory that the performance files are in',
            'delete': 'Delete files after they are picked up',
        })
        return config_help

    def get_default_config(self):
        """
        Returns default collector settings.
        """
        config = super(FilesCollector, self).get_default_config()
        config.update({
            'path': '.',
            'dir': '/tmp/diamond',
            'delete': False,
        })
        return config

    def collect(self):
        if os.path.exists(self.config['dir']):
            for fn in os.listdir(self.config['dir']):
                if os.path.isfile(os.path.join(self.config['dir'], fn)):
                    try:
                        fh = open(os.path.join(self.config['dir'], fn))
                        found = False
                        for line in fh:
                            m = _RE.match(line)
                            if (m):
                                self.publish(
                                    m.groups()[0],
                                    m.groups()[1] + m.groups()[2],
                                    precision=max(0, len(m.groups()[2]) - 1))
                                found = True
                        fh.close()
                        if (found and self.config['delete']):
                            os.unlink(os.path.join(self.config['dir'], fn))
                    except:
                        pass

########NEW FILE########
__FILENAME__ = testfiles
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from files import FilesCollector


###############################################################################

class TestFilesCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('FilesCollector', {
        })
        self.collector = FilesCollector(config, None)

    def test_import(self):
        self.assertTrue(FilesCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = filestat
# coding=utf-8

"""
Uses lsof to collect data on number of open files per user per type

#### Config Options

 * user_include - This is list of users to collect data for. If this is left
    empty, its a wildcard to collector for all users (default = None)
 * user_exclude - This is a list of users to exclude from collecting data. If
    this is left empty, no specific users will be excluded (default = None)
 * group_include - This is a list of groups to include in data collection. If
    this DOES NOT override user_exclude. (default = None)
 * group_exclude - This is a list of groups to exclude from collecting data.
    It DOES NOT override user_include. (default = None)
 * uid_min - This creates a floor for the user's uid. This means that it WILL
    NOT collect data for any user with a uid LOWER than the specified minimum,
    unless the user is told to be included by user_include (default = None)
 * uid_max - This creates a ceiling for the user's uid. This means that it WILL
    NOT collect data for any user with a uid HIGHER than the specified maximum,
    unless the user is told to be included by user_include (default = None)

*** Priority Explaination ***
 This is an explainatino of the priority in which users, groups, and uid, are
    evaluated. EXLCUDE ALWAYS OVERRULES INCLUDE within the same level (ie within
    users or group)
  * user_include/exclude (top level/priority)
    * group_include/exclude (second level: if user not in user_include/exclude,
          groups takes affect)
      * uid_min/max (third level: if user not met above qualifications, uids
            take affect)

 * type_include - This is a list of file types to collect ('REG', 'DIR", 'FIFO'
    , etc). If left empty, will collect for all file types. (Note: it suggested
    to not leave type_include empty, as it would add significant load to your
    graphite box(es) (default = None)
 * type_exclude - This is a list of tile types to exlude from being collected
    for. If left empty, no file types will be excluded. (default = None)

 * collect_user_data - This enables or disables the collection of user specific
    file handles. (default = False)

#### Dependencies

 * /proc/sys/fs/file-nr
 * /usr/sbin/lsof

"""

import diamond.collector
import re
import os

_RE = re.compile(r'(\d+)\s+(\d+)\s+(\d+)')


class FilestatCollector(diamond.collector.Collector):

    PROC = '/proc/sys/fs/file-nr'

    def get_default_config_help(self):
        config_help = super(FilestatCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(FilestatCollector, self).get_default_config()
        config.update({
            'path':     'files',
            'method':   'Threaded',
            'user_include': None,
            'user_exclude': None,
            'group_include': None,
            'group_exclude': None,
            'uid_min': 0,
            'uid_max': 65536,
            'type_include': None,
            'type_exclude': None,
            'collect_user_data': False
        })
        return config

    def get_userlist(self):
        """
        This collects all the users with open files on the system, and filters
        based on the variables user_include and user_exclude
        """
    # convert user/group  lists to arrays if strings
        if isinstance(self.config['user_include'], basestring):
            self.config['user_include'] = self.config['user_include'].split()
        if isinstance(self.config['user_exclude'], basestring):
            self.config['user_exclude'] = self.config['user_exclude'].split()
        if isinstance(self.config['group_include'], basestring):
            self.config['group_include'] = self.config['group_include'].split()
        if isinstance(self.config['group_exclude'], basestring):
            self.config['group_exclude'] = self.config['group_exclude'].split()

        rawusers = os.popen("lsof | awk '{ print $3 }' | sort | uniq -d"
                            ).read().split()
        userlist = []

        # remove any not on the user include list
        if (self.config['user_include'] is None
                or len(self.config['user_include']) == 0):
            userlist = rawusers
        else:
            # only work with specified include list, which is added at the end
            userlist = []

        # add any user in the group include list
        addedByGroup = []
        if (self.config['group_include'] is not None
                and len(self.config['group_include']) > 0):
            for u in rawusers:
                self.log.info(u)
                # get list of groups of user
                user_groups = os.popen("id -Gn %s" % (u)).read().split()
                for gi in self.config['group_include']:
                    if gi in user_groups and u not in userlist:
                        userlist.append(u)
                        addedByGroup.append(u)
                        break

        # remove any user in the exclude group list
        if (self.config['group_exclude'] is not None
                and len(self.config['group_exclude']) > 0):
            # create tmp list to iterate over while editing userlist
            tmplist = userlist[:]
            for u in tmplist:
                # get list of groups of user
                groups = os.popen("id -Gn %s" % (u)).read().split()
                for gi in self.config['group_exclude']:
                    if gi in groups:
                        userlist.remove(u)
                        break

        # remove any that aren't within the uid limits
        # make sure uid_min/max are ints
        self.config['uid_min'] = int(self.config['uid_min'])
        self.config['uid_max'] = int(self.config['uid_max'])
        tmplist = userlist[:]
        for u in tmplist:
            if (self.config['user_include'] is None
                    or u not in self.config['user_include']):
                if u not in addedByGroup:
                    uid = int(os.popen("id -u %s" % (u)).read())
                    if (uid < self.config['uid_min']
                            and self.config['uid_min'] is not None
                            and u in userlist):
                        userlist.remove(u)
                    if (uid > self.config['uid_max']
                            and self.config['uid_max'] is not None
                            and u in userlist):
                        userlist.remove(u)

        # add users that are in the users include list
        if self.config['user_include'] is not None and len(
                self.config['user_include']) > 0:
            for u in self.config['user_include']:
                if u in rawusers and u not in userlist:
                    userlist.append(u)

        # remove any that is on the user exclude list
        if self.config['user_exclude'] is not None and len(
                self.config['user_exclude']) > 0:
            for u in self.config['user_exclude']:
                if u in userlist:
                    userlist.remove(u)

        return userlist

    def get_typelist(self):
        """
        This collects all avaliable types and applies include/exclude filters
        """
        typelist = []

        # convert type list into arrays if strings
        if isinstance(self.config['type_include'], basestring):
            self.config['type_include'] = self.config['type_include'].split()
        if isinstance(self.config['type_exclude'], basestring):
            self.config['type_exclude'] = self.config['type_exclude'].split()

        # remove any not in include list
        if self.config['type_include'] is None or len(
                self.config['type_include']) == 0:
            typelist = os.popen("lsof | awk '{ print $5 }' | sort | uniq -d"
                                ).read().split()
        else:
            typelist = self.config['type_include']

        # remove any in the exclude list
        if self.config['type_exclude'] is not None and len(
                self.config['type_include']) > 0:
            for t in self.config['type_exclude']:
                if t in typelist:
                    typelist.remove(t)

        return typelist

    def process_lsof(self, users, types):
        """
        Get the list of users and file types to collect for and collect the
        data from lsof
        """
        d = {}
        for u in users:
            d[u] = {}
            tmp = os.popen("lsof -bu %s | awk '{ print $5 }'" % (
                u)).read().split()
            for t in types:
                d[u][t] = tmp.count(t)
        return d

    def collect(self):
        if not os.access(self.PROC, os.R_OK):
            return None

        # collect total open files
        file = open(self.PROC)
        for line in file:
            match = _RE.match(line)
            if match:
                self.publish('assigned', int(match.group(1)))
                self.publish('unused',   int(match.group(2)))
                self.publish('max',      int(match.group(3)))
        file.close()

        # collect open files per user per type
        if self.config['collect_user_data']:
            data = self.process_lsof(self.get_userlist(), self.get_typelist())
            for ukey in data.iterkeys():
                for tkey in data[ukey].iterkeys():
                    self.log.info('files.user.%s.%s %s' % (
                        ukey, tkey, int(data[ukey][tkey])))
                    self.publish('user.%s.%s' % (ukey, tkey),
                                 int(data[ukey][tkey]))

########NEW FILE########
__FILENAME__ = testfilestat
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from filestat import FilestatCollector

################################################################################


class TestFilestatCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('FilestatCollector', {
            'interval': 10
        })

        self.collector = FilestatCollector(config, None)

    def test_import(self):
        self.assertTrue(FilestatCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_sys_fs_file_nr(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/sys/fs/file-nr')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        FilestatCollector.PROC = self.getFixturePath('proc_sys_fs_file-nr')
        self.collector.collect()

        metrics = {
            'assigned': 576,
            'unused': 0,
            'max': 4835852
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = flume
# coding=utf-8

"""
Collect statistics from Flume

#### Dependencies

 * urllib2
 * json or simplejson

"""

import urllib2
import diamond.collector

try:
    import simplejson as json
except ImportError:
    import json


class FlumeCollector(diamond.collector.Collector):

    # items to collect
    _metrics_collect = {
        'CHANNEL': [
            'ChannelFillPercentage',
            'EventPutAttemptCount',
            'EventPutSuccessCount',
            'EventTakeAttemptCount',
            'EventTakeSuccessCount'
        ],
        'SINK': [
            'BatchCompleteCount',
            'BatchEmptyCount',
            'BatchUnderflowCount',
            'ConnectionClosedCount',
            'ConnectionCreatedCount',
            'ConnectionFailedCount',
            'EventDrainAttemptCount',
            'EventDrainSuccessCount'
        ],
        'SOURCE': [
            'AppendAcceptedCount',
            'AppendBatchAcceptedCount',
            'AppendBatchReceivedCount',
            'AppendReceivedCount',
            'EventAcceptedCount',
            'EventReceivedCount',
            'OpenConnectionCount'
        ]
    }

    def get_default_config_help(self):
        config_help = super(FlumeCollector, self).get_default_config_help()
        config_help.update({
            'req_host': 'Hostname',
            'req_port': 'Port',
            'req_path': 'Path',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        default_config = super(FlumeCollector, self).get_default_config()
        default_config['path'] = 'flume'
        default_config['req_host'] = 'localhost'
        default_config['req_port'] = 41414
        default_config['req_path'] = '/metrics'
        return default_config

    def collect(self):
        url = 'http://{0}:{1}{2}'.format(
            self.config['req_host'],
            self.config['req_port'],
            self.config['req_path']
        )

        try:
            resp = urllib2.urlopen(url)
            try:
                j = json.loads(resp.read())
                resp.close()
            except Exception, e:
                resp.close()
                self.log.error('Cannot load json data: %s', e)
                return None
        except urllib2.URLError, e:
            self.log.error('Failed to open url: %s', e)
            return None
        except Exception, e:
            self.log.error('Unknown error opening url: %s', e)
            return None

        for comp in j.iteritems():
            comp_name = comp[0]
            comp_items = comp[1]
            comp_type = comp_items['Type']

            for item in self._metrics_collect[comp_type]:
                if item.endswith('Count'):
                    metric_name = '{0}.{1}'.format(comp_name, item[:-5])
                    metric_value = int(comp_items[item])
                    self.publish_counter(metric_name, metric_value)
                elif item.endswith('Percentage'):
                    metric_name = '{0}.{1}'.format(comp_name, item)
                    metric_value = float(comp_items[item])
                    self.publish_gauge(metric_name, metric_value)
                else:
                    metric_name = item
                    metric_value = int(comp_items[item])
                    self.publish_gauge(metric_name, metric_value)

########NEW FILE########
__FILENAME__ = testflume
#!/usr/bin/python
# coding=utf-8

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import patch
from mock import Mock

from diamond.collector import Collector
from flume import FlumeCollector


class TestFlumeCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('FlumeCollector', {
            'interval': 10
        })

        self.collector = FlumeCollector(config, None)

    def test_import(self):
        self.assertTrue(FlumeCollector)

    @patch.object(Collector, 'publish')
    @patch.object(Collector, 'publish_gauge')
    @patch.object(Collector, 'publish_counter')
    def test_collect_should_work(self,
                                 publish_mock,
                                 publish_gauge_mock,
                                 publish_counter_mock):
        patch_urlopen = patch('urllib2.urlopen',
                              Mock(return_value=self.getFixture('metrics')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'CHANNEL.channel1.ChannelFillPercentage': 0.0,
            'CHANNEL.channel1.EventPutAttempt': 50272828,
            'CHANNEL.channel1.EventPutSuccess': 50255318,
            'CHANNEL.channel1.EventTakeAttempt': 50409933,
            'CHANNEL.channel1.EventTakeSuccess': 50255318,
            'SINK.sink1.BatchComplete': 251705,
            'SINK.sink1.BatchEmpty': 76250,
            'SINK.sink1.BatchUnderflow': 379,
            'SINK.sink1.ConnectionClosed': 6,
            'SINK.sink1.ConnectionCreated': 7,
            'SINK.sink1.ConnectionFailed': 0,
            'SINK.sink1.EventDrainAttempt': 25190171,
            'SINK.sink1.EventDrainSuccess': 25189571,
            'SOURCE.source1.AppendAccepted': 0,
            'SOURCE.source1.AppendBatchAccepted': 56227,
            'SOURCE.source1.AppendBatchReceived': 56258,
            'SOURCE.source1.AppendReceived': 0,
            'SOURCE.source1.EventAccepted': 50282681,
            'SOURCE.source1.EventReceived': 50311681,
            'SOURCE.source1.OpenConnection': 0
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany([publish_mock,
                                  publish_gauge_mock,
                                  publish_counter_mock
                                  ], metrics)

    @patch.object(Collector, 'publish')
    def test_blank_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('metrics_blank')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

    @patch.object(Collector, 'publish')
    def test_invalid_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch(
            'urllib2.urlopen',
            Mock(return_value=self.getFixture('metrics_invalid')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = gridengine
# coding=utf-8

"""
The GridEngineCollector parses qstat statistics from Sun Grid Engine,
Univa Grid Engine and Open Grid Scheduler.

#### Dependencies

 * Grid Engine qstat

"""

import os
import re
import subprocess
import sys
import xml.dom.minidom

import diamond.collector


class GridEngineCollector(diamond.collector.Collector):
    """Diamond collector for Grid Engine performance data
    """

    class QueueStatsEntry:
        def __init__(self, name=None, load=None, used=None, resv=None,
                     available=None, total=None, temp_disabled=None,
                     manual_intervention=None):
            self.name = name
            self.load = load
            self.used = used
            self.resv = resv
            self.available = available
            self.total = total
            self.temp_disabled = temp_disabled
            self.manual_intervention = manual_intervention

    class StatsParser(object):
        def __init__(self, document):
            self.dom = xml.dom.minidom.parseString(document)

        def get_tag_text(self, node, tag_name):
            el = node.getElementsByTagName(tag_name)[0]
            return self.get_text(el)

        def get_text(self, node):
            rc = []
            for node in node.childNodes:
                if node.nodeType == node.TEXT_NODE:
                    rc.append(node.data)
            return ''.join(rc)

    class QueueStatsParser(StatsParser):
        def __init__(self, document):
            self.dom = xml.dom.minidom.parseString(document)

        def parse(self):
            cluster_queue_summaries = self.dom.getElementsByTagName(
                "cluster_queue_summary")
            return [
                self._parse_cluster_stats_entry(node)
                for node in cluster_queue_summaries]

        def _parse_cluster_stats_entry(self, node):
            name = self.get_tag_text(node, "name")
            load = float(self.get_tag_text(node, "load"))
            used = int(self.get_tag_text(node, "used"))
            resv = int(self.get_tag_text(node, "resv"))
            available = int(self.get_tag_text(node, "available"))
            total = int(self.get_tag_text(node, "total"))
            temp_disabled = int(self.get_tag_text(node, "temp_disabled"))
            manual_intervention = int(self.get_tag_text(
                node,
                "manual_intervention"))

            return GridEngineCollector.QueueStatsEntry(
                name=name,
                load=load,
                used=used,
                resv=resv,
                available=available,
                total=total,
                temp_disabled=temp_disabled,
                manual_intervention=manual_intervention)

    def __init__(self, config, handlers):
        super(GridEngineCollector, self).__init__(config, handlers)
        os.environ['SGE_ROOT'] = self.config['sge_root']

    def get_default_config_help(self):
        config_help = super(GridEngineCollector,
                            self).get_default_config_help()
        config_help.update({
            'bin_path': "The path to Grid Engine's qstat",
            'sge_root': "The SGE_ROOT value to provide to qstat"
        })
        return config_help

    def get_default_config(self):
        config = super(GridEngineCollector, self).get_default_config()
        config.update({
            'bin_path': '/opt/gridengine/bin/lx-amd64/qstat',
            'method': 'Threaded',
            'path': 'gridengine',
            'sge_root': self._sge_root(),
        })
        return config

    def collect(self):
        """Collect statistics from Grid Engine via qstat.
        """
        self._collect_queue_stats()

    def _capture_output(self, cmd):
        p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
        bytestr = p.communicate()[0]
        output = bytestr.decode(sys.getdefaultencoding())
        return output

    def _collect_queue_stats(self):
        output = self._queue_stats_xml()
        parser = self.QueueStatsParser(output)
        for cq in parser.parse():
            name = self._sanitize(cq.name)
            prefix = 'queues.%s' % (name)
            metrics = ['load', 'used', 'resv', 'available', 'total',
                       'temp_disabled', 'manual_intervention']
            for metric in metrics:
                path = '%s.%s' % (prefix, metric)
                value = getattr(cq, metric)
                self.publish(path, value)

    def _queue_stats_xml(self):
        bin_path = self.config['bin_path']
        return self._capture_output([bin_path, '-g', 'c', '-xml'])

    def _sanitize(self, s):
        """Sanitize the name of a metric to remove unwanted chars
        """
        return re.sub("[^\w-]", "_", s)

    def _sge_root(self):
        sge_root = os.environ.get('SGE_ROOT')
        if sge_root:
            return sge_root
        else:
            return '/opt/gridengine'

########NEW FILE########
__FILENAME__ = testgridengine
#!/usr/bin/python
# coding=utf-8

from test import CollectorTestCase
from test import get_collector_config
from mock import patch
import os

from diamond.collector import Collector
from gridengine import GridEngineCollector


class TestGridEngineCollector(CollectorTestCase):
    """Set up the fixtures for the test
    """
    def setUp(self):
        config = get_collector_config('GridEngineCollector', {})
        self.collector = GridEngineCollector(config, None)
        self.fixtures_dir = os.path.abspath(os.path.join(
            os.path.dirname(__file__), 'fixtures'))

    def test_import(self):
        """Test that import succeeds
        """
        self.assertTrue(GridEngineCollector)

    @patch.object(GridEngineCollector, '_queue_stats_xml')
    @patch.object(Collector, 'publish')
    def test_queue_stats_should_work_with_real_data(
            self, publish_mock, xml_mock):
        """Test that fixtures are parsed correctly
        """
        xml_mock.return_value = self.getFixture('queue_stats.xml').getvalue()
        self.collector._collect_queue_stats()

        published_metrics = {
            'queues.hadoop.load': 0.00532,
            'queues.hadoop.used': 0,
            'queues.hadoop.resv': 0,
            'queues.hadoop.available': 0,
            'queues.hadoop.total': 36,
            'queues.hadoop.temp_disabled': 0,
            'queues.hadoop.manual_intervention': 36,
            'queues.primary_q.load': 0.20509,
            'queues.primary_q.used': 1024,
            'queues.primary_q.resv': 0,
            'queues.primary_q.available': 1152,
            'queues.primary_q.total': 2176,
            'queues.primary_q.temp_disabled': 0,
            'queues.primary_q.manual_intervention': 0,
            'queues.secondary_q.load': 0.00460,
            'queues.secondary_q.used': 145,
            'queues.secondary_q.resv': 0,
            'queues.secondary_q.available': 1007,
            'queues.secondary_q.total': 1121,
            'queues.secondary_q.temp_disabled': 1,
            'queues.secondary_q.manual_intervention': 0
        }
        self.assertPublishedMany(publish_mock, published_metrics)

########NEW FILE########
__FILENAME__ = hadoop
# coding=utf-8

"""
Diamond collector for Hadoop metrics, see:

 * [http://www.cloudera.com/blog/2009/03/hadoop-metrics/](http://bit.ly/NKBcFm)

#### Dependencies

 * hadoop

"""

from diamond.metric import Metric
import diamond.collector
import glob
import re
import os


class HadoopCollector(diamond.collector.Collector):

    re_log = re.compile(r'^(?P<timestamp>\d+) (?P<name>\S+): (?P<metrics>.*)$')

    def get_default_config_help(self):
        config_help = super(HadoopCollector, self).get_default_config_help()
        config_help.update({
            'metrics': "List of paths to process metrics from",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(HadoopCollector, self).get_default_config()
        config.update({
            'path':     'hadoop',
            'method':   'Threaded',
            'metrics':  ['/var/log/hadoop/*-metrics.out'],
        })
        return config

    def collect(self):
        for pattern in self.config['metrics']:
            for filename in glob.glob(pattern):
                self.collect_from(filename)

    def collect_from(self, filename):
        if not os.access(filename, os.R_OK):
            self.log.error('HadoopCollector unable to read "%s"', filename)
            return False

        fd = open(filename, 'r')
        for line in fd:
            match = self.re_log.match(line)
            if not match:
                continue

            metrics = {}

            data = match.groupdict()
            for metric in data['metrics'].split(','):
                metric = metric.strip()
                if '=' in metric:
                    key, value = metric.split('=', 1)
                    metrics[key] = value

            for metric in metrics.keys():
                try:

                    if data['name'] == 'jvm.metrics':
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['processName'].replace(' ', '_'),
                            metric, ]))

                    elif data['name'] == 'mapred.job':
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['group'].replace(' ', '_'),
                            metrics['counter'].replace(' ', '_'),
                            metric, ]))

                    elif data['name'] == 'rpc.metrics':

                        if metric == 'port':
                            continue

                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['port'],
                            metric, ]))

                    else:
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metric, ]))

                    value = float(metrics[metric])

                    self.publish_metric(Metric(path,
                                        value,
                                        timestamp=int(data['timestamp'])))

                except ValueError:
                    pass
        fd.close()

########NEW FILE########
__FILENAME__ = testhadoop
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import patch

from diamond.collector import Collector
from hadoop import HadoopCollector

import os

################################################################################


class TestHadoopCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('HadoopCollector', {
            'metrics':  [os.path.dirname(__file__) + '/fixtures/*metrics.log'],
        })

        self.collector = HadoopCollector(config, {})

    def test_import(self):
        self.assertTrue(HadoopCollector)

    @patch.object(Collector, 'publish_metric')
    def test_should_work_with_real_data(self, publish_mock):
        self.collector.collect()

        metrics = self.getPickledResults('expected.pkl')

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMetricMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = haproxy
# coding=utf-8

"""
Collect HAProxy Stats

#### Dependencies

 * urlparse
 * urllib2

"""

import re
import urllib2
import base64
import csv
import diamond.collector


class HAProxyCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(HAProxyCollector, self).get_default_config_help()
        config_help.update({
            'url': "Url to stats in csv format",
            'user': "Username",
            'pass': "Password",
            'ignore_servers': "Ignore servers, just collect frontend and "
                              + "backend stats",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(HAProxyCollector, self).get_default_config()
        config.update({
            'path':             'haproxy',
            'url':              'http://localhost/haproxy?stats;csv',
            'user':             'admin',
            'pass':             'password',
            'ignore_servers':   False,
        })
        return config

    def _get_config_value(self, section, key):
        if section:
            if section not in self.config:
                self.log.error("Error: Config section '%s' not found", section)
                return None
            return self.config[section].get(key, self.config[key])
        else:
            return self.config[key]

    def get_csv_data(self, section=None):
        """
        Request stats from HAProxy Server
        """
        metrics = []
        req = urllib2.Request(self._get_config_value(section, 'url'))
        try:
            handle = urllib2.urlopen(req)
            return handle.readlines()
        except Exception, e:
            if not hasattr(e, 'code') or e.code != 401:
                self.log.error("Error retrieving HAProxy stats. %s", e)
                return metrics

        # get the www-authenticate line from the headers
        # which has the authentication scheme and realm in it
        authline = e.headers['www-authenticate']

        # this regular expression is used to extract scheme and realm
        authre = (r'''(?:\s*www-authenticate\s*:)?\s*'''
                  + '''(\w*)\s+realm=['"]([^'"]+)['"]''')
        authobj = re.compile(authre, re.IGNORECASE)
        matchobj = authobj.match(authline)
        if not matchobj:
            # if the authline isn't matched by the regular expression
            # then something is wrong
            self.log.error('The authentication header is malformed.')
            return metrics

        scheme = matchobj.group(1)
        # here we've extracted the scheme
        # and the realm from the header
        if scheme.lower() != 'basic':
            self.log.error('Invalid authentication scheme.')
            return metrics

        base64string = base64.encodestring(
            '%s:%s' % (self._get_config_value(section, 'user'),
                       self._get_config_value(section, 'pass')))[:-1]
        authheader = 'Basic %s' % base64string
        req.add_header("Authorization", authheader)
        try:
            handle = urllib2.urlopen(req)
            metrics = handle.readlines()
            return metrics
        except IOError, e:
            # here we shouldn't fail if the USER/PASS is right
            self.log.error("Error retrieving HAProxy stats. (Invalid username "
                           + "or password?) %s", e)
            return metrics

    def _generate_headings(self, row):
        headings = {}
        for index, heading in enumerate(row):
            headings[index] = self._sanitize(heading)
        return headings

    def _collect(self, section=None):
        """
        Collect HAProxy Stats
        """
        csv_data = self.get_csv_data(section)
        data = list(csv.reader(csv_data))
        headings = self._generate_headings(data[0])
        section_name = section and self._sanitize(section.lower()) + '.' or ''

        for row in data:
            if (self._get_config_value(section, 'ignore_servers')
                    and row[1].lower() not in ['frontend', 'backend']):
                continue

            part_one = self._sanitize(row[0].lower())
            part_two = self._sanitize(row[1].lower())
            metric_name = '%s%s.%s' % (section_name, part_one, part_two)

            for index, metric_string in enumerate(row):
                try:
                    metric_value = float(metric_string)
                except ValueError:
                    continue

                stat_name = '%s.%s' % (metric_name, headings[index])
                self.publish(stat_name, metric_value, metric_type='GAUGE')

    def collect(self):
        if 'servers' in self.config:
            if isinstance(self.config['servers'], list):
                for serv in self.config['servers']:
                    self._collect(serv)
            else:
                self._collect(self.config['servers'])
        else:
            self._collect()

    def _sanitize(self, s):
        """Sanitize the name of a metric to remove unwanted chars
        """
        return re.sub('[^\w-]', '_', s)

########NEW FILE########
__FILENAME__ = testhaproxy
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from haproxy import HAProxyCollector

################################################################################


class TestHAProxyCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('HAProxyCollector', {
            'interval': 10,
        })

        self.collector = HAProxyCollector(config, None)

    def test_import(self):
        self.assertTrue(HAProxyCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        self.collector.config['ignore_servers'] = False

        patch_urlopen = patch('urllib2.urlopen',
                              Mock(return_value=self.getFixture('stats.csv')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = self.getPickledResults('real_data.pkl')

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_and_ignore_servers(self, publish_mock):
        self.collector.config['ignore_servers'] = True

        patch_urlopen = patch('urllib2.urlopen',
                              Mock(return_value=self.getFixture('stats.csv')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = self.getPickledResults('real_data_ignore_servers.pkl')

        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = httpd
# coding=utf-8

"""
Collect stats from Apache HTTPD server using mod_status

#### Dependencies

 * mod_status
 * httplib
 * urlparse

"""

import re
import httplib
import urlparse
import diamond.collector


class HttpdCollector(diamond.collector.Collector):

    def __init__(self, *args, **kwargs):
        super(HttpdCollector, self).__init__(*args, **kwargs)
        if 'url' in self.config:
            self.config['urls'].append(self.config['url'])

        self.urls = {}
        if isinstance(self.config['urls'], basestring):
            self.config['urls'] = self.config['urls'].split(',')

        for url in self.config['urls']:
            # Handle the case where there is a trailing comman on the urls list
            if len(url) == 0:
                continue
            if ' ' in url:
                parts = url.split(' ')
                self.urls[parts[0]] = parts[1]
            else:
                self.urls[''] = url

    def get_default_config_help(self):
        config_help = super(HttpdCollector, self).get_default_config_help()
        config_help.update({
            'urls': "Urls to server-status in auto format, comma seperated,"
            + " Format 'nickname http://host:port/server-status?auto, "
            + ", nickname http://host:port/server-status?auto, etc'",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(HttpdCollector, self).get_default_config()
        config.update({
            'path':     'httpd',
            'urls':     ['localhost http://localhost:8080/server-status?auto']
        })
        return config

    def collect(self):
        for nickname in self.urls.keys():
            url = self.urls[nickname]

            try:
                while True:

                    # Parse Url
                    parts = urlparse.urlparse(url)

                    # Parse host and port
                    endpoint = parts[1].split(':')
                    if len(endpoint) > 1:
                        service_host = endpoint[0]
                        service_port = int(endpoint[1])
                    else:
                        service_host = endpoint[0]
                        service_port = 80

                    # Setup Connection
                    connection = httplib.HTTPConnection(service_host,
                                                        service_port)

                    url = "%s?%s" % (parts[2], parts[4])

                    connection.request("GET", url)
                    response = connection.getresponse()
                    data = response.read()
                    headers = dict(response.getheaders())
                    if ('location' not in headers
                            or headers['location'] == url):
                        connection.close()
                        break
                    url = headers['location']
                    connection.close()
            except Exception, e:
                self.log.error(
                    "Error retrieving HTTPD stats for host %s:%s, url '%s': %s",
                    service_host, str(service_port), url, e)
                continue

            exp = re.compile('^([A-Za-z ]+):\s+(.+)$')
            for line in data.split('\n'):
                if line:
                    m = exp.match(line)
                    if m:
                        k = m.group(1)
                        v = m.group(2)

                        # IdleWorkers gets determined from the scoreboard
                        if k == 'IdleWorkers':
                            continue

                        if k == 'Scoreboard':
                            for sb_kv in self._parseScoreboard(v):
                                self._publish(nickname, sb_kv[0], sb_kv[1])
                        else:
                            self._publish(nickname, k, v)

    def _publish(self, nickname, key, value):

        metrics = ['ReqPerSec', 'BytesPerSec', 'BytesPerReq', 'BusyWorkers',
                   'Total Accesses', 'IdleWorkers', 'StartingWorkers',
                   'ReadingWorkers', 'WritingWorkers', 'KeepaliveWorkers',
                   'DnsWorkers', 'ClosingWorkers', 'LoggingWorkers',
                   'FinishingWorkers', 'CleanupWorkers']

        if key in metrics:
            # Get Metric Name
            metric_name = "%s" % re.sub('\s+', '', key)

            # Prefix with the nickname?
            if len(nickname) > 0:
                metric_name = nickname + '.' + metric_name

            # Get Metric Value
            metric_value = "%d" % float(value)

            # Publish Metric
            self.publish(metric_name, metric_value)

    def _parseScoreboard(self, sb):

        ret = []

        ret.append(('IdleWorkers', sb.count('_')))
        ret.append(('ReadingWorkers', sb.count('R')))
        ret.append(('WritingWorkers', sb.count('W')))
        ret.append(('KeepaliveWorkers', sb.count('K')))
        ret.append(('DnsWorkers', sb.count('D')))
        ret.append(('ClosingWorkers', sb.count('C')))
        ret.append(('LoggingWorkers', sb.count('L')))
        ret.append(('FinishingWorkers', sb.count('G')))
        ret.append(('CleanupWorkers', sb.count('I')))

        return ret

########NEW FILE########
__FILENAME__ = testhttpd
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from httpd import HttpdCollector
import httplib

################################################################################


class TestHTTPResponse(httplib.HTTPResponse):
    def __init__(self):
        pass

    def read(self):
        pass


class TestHttpdCollector(CollectorTestCase):
    def setUp(self, config=None):
        if config is None:
            config = get_collector_config('HttpdCollector', {
                'interval': '10',
                'url':      'http://www.example.com:80/server-status?auto'
            })
        else:
            config = get_collector_config('HttpdCollector', config)

        self.collector = HttpdCollector(config, None)

        self.HTTPResponse = TestHTTPResponse()

        httplib.HTTPConnection.request = Mock(return_value=True)
        httplib.HTTPConnection.getresponse = Mock(
            return_value=self.HTTPResponse)

    def test_import(self):
        self.assertTrue(HttpdCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        self.setUp()

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-fake-1').getvalue()))

        patch_headers = patch.object(
            TestHTTPResponse,
            'getheaders',
            Mock(return_value={}))

        patch_headers.start()
        patch_read.start()
        self.collector.collect()
        patch_read.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-fake-2').getvalue()))

        patch_read.start()
        self.collector.collect()
        patch_read.stop()
        patch_headers.stop()

        self.assertPublishedMany(publish_mock, {
            'TotalAccesses': 100,
            'ReqPerSec': 10,
            'BytesPerSec': 20480,
            'BytesPerReq': 204,
            'BusyWorkers': 6,
            'IdleWorkers': 4,
            'WritingWorkers': 1,
            'KeepaliveWorkers': 2,
            'ReadingWorkers': 3,
            'DnsWorkers': 0,
            'ClosingWorkers': 0,
            'LoggingWorkers': 0,
            'FinishingWorkers': 0,
            'CleanupWorkers': 0,
        })

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        self.setUp()

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-live-1').getvalue()))

        patch_headers = patch.object(
            TestHTTPResponse,
            'getheaders',
            Mock(return_value={}))

        patch_headers.start()
        patch_read.start()
        self.collector.collect()
        patch_read.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-live-2').getvalue()))

        patch_read.start()
        self.collector.collect()
        patch_read.stop()
        patch_headers.stop()

        metrics = {
            'TotalAccesses': 8314,
            'ReqPerSec': 0,
            'BytesPerSec': 165,
            'BytesPerReq': 5418,
            'BusyWorkers': 9,
            'IdleWorkers': 0,
            'WritingWorkers': 1,
            'KeepaliveWorkers': 7,
            'ReadingWorkers': 1,
            'DnsWorkers': 0,
            'ClosingWorkers': 0,
            'LoggingWorkers': 0,
            'FinishingWorkers': 0,
            'CleanupWorkers': 0,
        }
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_multiple_servers(self, publish_mock):
        self.setUp(config={
            'urls': [
                'nickname1 http://localhost:8080/server-status?auto',
                'nickname2 http://localhost:8080/server-status?auto',
            ],
        })

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-live-1').getvalue()))

        patch_headers = patch.object(
            TestHTTPResponse,
            'getheaders',
            Mock(return_value={}))

        patch_headers.start()
        patch_read.start()
        self.collector.collect()
        patch_read.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-live-2').getvalue()))

        patch_read.start()
        self.collector.collect()
        patch_read.stop()
        patch_headers.stop()

        metrics = {
            'nickname1.TotalAccesses': 8314,
            'nickname1.ReqPerSec': 0,
            'nickname1.BytesPerSec': 165,
            'nickname1.BytesPerReq': 5418,
            'nickname1.BusyWorkers': 9,
            'nickname1.IdleWorkers': 0,
            'nickname1.WritingWorkers': 1,
            'nickname1.KeepaliveWorkers': 7,
            'nickname1.ReadingWorkers': 1,
            'nickname1.DnsWorkers': 0,
            'nickname1.ClosingWorkers': 0,
            'nickname1.LoggingWorkers': 0,
            'nickname1.FinishingWorkers': 0,
            'nickname1.CleanupWorkers': 0,

            'nickname2.TotalAccesses': 8314,
            'nickname2.ReqPerSec': 0,
            'nickname2.BytesPerSec': 165,
            'nickname2.BytesPerReq': 5418,
            'nickname2.BusyWorkers': 9,
            'nickname2.IdleWorkers': 0,
            'nickname2.WritingWorkers': 1,
            'nickname2.KeepaliveWorkers': 7,
            'nickname2.ReadingWorkers': 1,
            'nickname2.DnsWorkers': 0,
            'nickname2.ClosingWorkers': 0,
            'nickname2.LoggingWorkers': 0,
            'nickname2.FinishingWorkers': 0,
            'nickname2.CleanupWorkers': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_issue_456(self, publish_mock):
        self.setUp(config={
            'urls': 'vhost http://localhost/server-status?auto',
        })

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-live-3').getvalue()))

        patch_headers = patch.object(
            TestHTTPResponse,
            'getheaders',
            Mock(return_value={}))

        patch_headers.start()
        patch_read.start()
        self.collector.collect()
        patch_read.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_read = patch.object(
            TestHTTPResponse,
            'read',
            Mock(return_value=self.getFixture(
                'server-status-live-4').getvalue()))

        patch_read.start()
        self.collector.collect()
        patch_read.stop()
        patch_headers.stop()

        metrics = {
            'vhost.TotalAccesses': 329,
            'vhost.ReqPerSec': 0.156966,
            'vhost.BytesPerSec': 2417,
            'vhost.BytesPerReq': 15403,
            'vhost.BusyWorkers': 1,
            'vhost.IdleWorkers': 17,
            'vhost.WritingWorkers': 1,
            'vhost.KeepaliveWorkers': 0,
            'vhost.ReadingWorkers': 0,
            'vhost.DnsWorkers': 0,
            'vhost.ClosingWorkers': 0,
            'vhost.LoggingWorkers': 0,
            'vhost.FinishingWorkers': 0,
            'vhost.CleanupWorkers': 0,
        }
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_issue_533(self, publish_mock):
        self.setUp(config={
            'urls': 'localhost http://localhost:80/server-status?auto,',
        })

        expected_urls = {'localhost': 'http://localhost:80/server-status?auto'}

        self.assertEqual(self.collector.urls, expected_urls)

    @patch.object(Collector, 'publish')
    def test_url_with_port(self, publish_mock):
        self.setUp(config={
            'urls': 'localhost http://localhost:80/server-status?auto',
        })

        expected_urls = {'localhost': 'http://localhost:80/server-status?auto'}

        self.assertEqual(self.collector.urls, expected_urls)

    @patch.object(Collector, 'publish')
    def test_url_without_port(self, publish_mock):
        self.setUp(config={
            'urls': 'localhost http://localhost/server-status?auto',
        })

        expected_urls = {'localhost': 'http://localhost/server-status?auto'}

        self.assertEqual(self.collector.urls, expected_urls)

    @patch.object(Collector, 'publish')
    def test_url_without_nickname(self, publish_mock):
        self.setUp(config={
            'urls': 'http://localhost/server-status?auto',
        })

        expected_urls = {'': 'http://localhost/server-status?auto'}

        self.assertEqual(self.collector.urls, expected_urls)

    @patch.object(Collector, 'publish')
    def test_issue_538(self, publish_mock):
        self.setUp(config={
            'enabled': True,
            'path_suffix': "",
            'ttl_multiplier': 2,
            'measure_collector_time': False,
            'byte_unit': 'byte',
            'urls': 'localhost http://localhost:80/server-status?auto',
        })

        expected_urls = {'localhost': 'http://localhost:80/server-status?auto'}

        self.assertEqual(self.collector.urls, expected_urls)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = httpjson
# coding=utf-8

"""
Simple collector which get JSON and parse it into flat metrics

#### Dependencies

 * urllib2

"""

import urllib2
import json
import diamond.collector


class HTTPJSONCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(HTTPJSONCollector, self).get_default_config_help()
        config_help.update({
            'url': 'Full URL'
        })
        return config_help

    def get_default_config(self):
        default_config = super(HTTPJSONCollector, self).get_default_config()
        default_config.update({
            'path': 'httpjson',
            'url': 'http://localhost/stat'
        })
        return default_config

    def _json_to_flat_metrics(self, prefix, data):
        for key, value in data.items():
            if isinstance(value, dict):
                for k, v in self._json_to_flat_metrics(
                        "%s.%s" % (prefix, key), value):
                    yield k, v
            else:
                try:
                    int(value)
                except ValueError:
                    value = None
                finally:
                    yield ("%s.%s" % (prefix, key), value)

    def collect(self):
        url = self.config['url']

        req = urllib2.Request(url)
        req.add_header('Content-type', 'application/json')

        try:
            resp = urllib2.urlopen(req)
        except urllib2.URLError as e:
            self.log.error("Can't open url %s. %s", url, e)
        else:

            content = resp.read()

            try:
                data = json.loads(content)
            except ValueError as e:
                self.log.error("Can't parse JSON object from %s. %s", url, e)
            else:
                for metric_name, metric_value in self._json_to_flat_metrics(
                        "", data):
                    self.publish(metric_name, metric_value)

########NEW FILE########
__FILENAME__ = testhttpjson
#!/usr/bin/python
# coding=utf-8
################################################################################
from test import CollectorTestCase
from test import get_collector_config
from mock import Mock
from mock import patch
from diamond.collector import Collector
from httpjson import HTTPJSONCollector

################################################################################


class TestHTTPJSONCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('HTTPJSONCollector', {})
        self.collector = HTTPJSONCollector(config, None)

    def test_import(self):
        self.assertTrue(HTTPJSONCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        urlopen_mock = patch('urllib2.urlopen',
                             Mock(return_value=self.getFixture('stats.json')))

        urlopen_mock.start()
        self.collector.collect()
        urlopen_mock.stop()

        metrics = self.getPickledResults("real_stat.pkl")

        self.assertPublishedMany(publish_mock, metrics)

########NEW FILE########
__FILENAME__ = interrupt
# coding=utf-8

"""
The InterruptCollector class collects metrics on interrupts from
/proc/interrupts

#### Dependencies

 * /proc/interrupts

"""

import platform
import os
import diamond.collector

# Detect the architecture of the system
# and set the counters for MAX_VALUES
# appropriately. Otherwise, rolling over
# counters will cause incorrect or
# negative values.
if platform.architecture()[0] == '64bit':
    counter = (2 ** 64) - 1
else:
    counter = (2 ** 32) - 1


class InterruptCollector(diamond.collector.Collector):

    PROC = '/proc/interrupts'

    def get_default_config_help(self):
        config_help = super(InterruptCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(InterruptCollector, self).get_default_config()
        config.update({
            'path':     'interrupts'
        })
        return config

    def collect(self):
        """
        Collect interrupt data
        """
        if not os.access(self.PROC, os.R_OK):
            return False

        #Open PROC file
        file = open(self.PROC, 'r')
        #Get data
        cpuCount = None
        for line in file:
            if not cpuCount:
                cpuCount = len(line.split())
            else:
                data = line.strip().split(None, cpuCount + 2)
                data[0] = data[0].replace(':', '')

                if len(data) == 2:
                    metric_name = data[0]
                    metric_value = data[1]
                    self.publish(metric_name,
                                 self.derivative(metric_name,
                                                 long(metric_value),
                                                 counter))
                else:
                    if len(data[0]) == cpuCount + 1:
                        metric_name = data[0] + '.'
                    elif len(data[0]) == 3:
                        metric_name = (((data[-2] + ' '
                                         + data[-1]).replace(' ', '_')) + '.')
                    else:
                        metric_name = (((data[-2]).replace(' ', '_'))
                                       + '.'
                                       + ((data[-1]).replace(', ',
                                                             '-'
                                                             ).replace(' ',
                                                                       '_'))
                                       + '.' + data[0] + '.')
                    total = 0
                    for index, value in enumerate(data):
                        if index == 0 or index >= cpuCount + 1:
                            continue

                        metric_name_node = metric_name + 'CPU' + str(index - 1)
                        value = int(self.derivative(metric_name_node,
                                                    long(value), counter))
                        total += value
                        self.publish(metric_name_node, value)

                    # Roll up value
                    metric_name_node = metric_name + 'total'
                    self.publish(metric_name_node, total)

        #Close file
        file.close()

########NEW FILE########
__FILENAME__ = soft
# coding=utf-8

"""
The SoftInterruptCollector collects metrics on software interrupts from
/proc/stat

#### Dependencies

 * /proc/stat

"""

import platform
import os
import diamond.collector

# Detect the architecture of the system
# and set the counters for MAX_VALUES
# appropriately. Otherwise, rolling over
# counters will cause incorrect or
# negative values.
if platform.architecture()[0] == '64bit':
    counter = (2 ** 64) - 1
else:
    counter = (2 ** 32) - 1


class SoftInterruptCollector(diamond.collector.Collector):

    PROC = '/proc/stat'

    def get_default_config_help(self):
        config_help = super(SoftInterruptCollector,
                            self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(SoftInterruptCollector, self).get_default_config()
        config.update({
            'path':     'softirq'
        })
        return config

    def collect(self):
        """
        Collect interrupt data
        """
        if not os.access(self.PROC, os.R_OK):
            return False

        #Open PROC file
        file = open(self.PROC, 'r')

        #Get data
        for line in file:

            if not line.startswith('softirq'):
                continue

            data = line.split()

            metric_name = 'total'
            metric_value = int(data[1])
            metric_value = int(self.derivative(
                metric_name,
                long(metric_value), counter))
            self.publish(metric_name, metric_value)

            for i in range(2, len(data)):
                metric_name = str(i - 2)
                metric_value = int(data[i])
                metric_value = int(self.derivative(
                    metric_name,
                    long(metric_value), counter))
                self.publish(metric_name, metric_value)

        #Close file
        file.close()

########NEW FILE########
__FILENAME__ = testinterrupt
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from interrupt import InterruptCollector

################################################################################


class TestInterruptCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('InterruptCollector', {
            'interval': 1
        })

        self.collector = InterruptCollector(config, None)

    def test_import(self):
        self.assertTrue(InterruptCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_stat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/interrupts', 'r')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_24_core(self, publish_mock):
        InterruptCollector.PROC = self.getFixturePath('interrupts_24_core_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        InterruptCollector.PROC = self.getFixturePath('interrupts_24_core_2')
        self.collector.collect()

        metrics = {
            'IO-APIC-edge.timer.0.CPU0': 318660.000000,
            'IO-APIC-edge.timer.0.total': 318660.000000,
            'PCI-MSI-X.eth3-rx-1.51.CPU6': 293.000000,
            'PCI-MSI-X.eth3-rx-1.51.CPU7': 330.000000,
            'PCI-MSI-X.eth3-rx-1.51.CPU9': 286.000000,
            'PCI-MSI-X.eth3-rx-1.51.total': 909.000000,
            'PCI-MSI-X.eth3-rx-2.59.CPU21': 98790.000000,
            'PCI-MSI-X.eth3-rx-2.59.total': 98790.000000,
            'PCI-MSI-X.eth3-rx-3.67.CPU7': 743.000000,
            'PCI-MSI-X.eth3-rx-3.67.CPU9': 378.000000,
            'PCI-MSI-X.eth3-rx-3.67.total': 1121.000000,
            'PCI-MSI-X.eth3-tx-0.75.CPU23': 304345.000000,
            'PCI-MSI-X.eth3-tx-0.75.total': 304345.000000,
            'IO-APIC-level_3w-sas.CPU6': 301014.000000,
            'IO-APIC-level_3w-sas.total': 301014.000000,
            'PCI-MSI-X_eth2-rx-0.CPU20': 20570.000000,
            'PCI-MSI-X_eth2-rx-0.total': 20570.000000,
            'PCI-MSI-X_eth2-rx-1.CPU6': 94.000000,
            'PCI-MSI-X_eth2-rx-1.CPU7': 15.000000,
            'PCI-MSI-X_eth2-rx-1.CPU9': 50.000000,
            'PCI-MSI-X_eth2-rx-1.total': 159.000000,
            'PCI-MSI-X_eth2-rx-2.CPU17': 159.000000,
            'PCI-MSI-X_eth2-rx-2.total': 159.000000,
            'PCI-MSI-X_eth2-rx-3.CPU8': 159.000000,
            'PCI-MSI-X_eth2-rx-3.total': 159.000000,
            'PCI-MSI-X_eth2-tx-0.CPU16': 159.000000,
            'PCI-MSI-X_eth2-tx-0.total': 159.000000,
            'PCI-MSI_eth0.CPU18': 10397.000000,
            'PCI-MSI_eth0.total': 10397.000000,
            'PCI-MSI-X_eth3-rx-0.CPU22': 224074.000000,
            'PCI-MSI-X_eth3-rx-0.total': 224074.000000,
            'PCI-MSI_eth1.CPU19': 10386.000000,
            'PCI-MSI_eth1.total': 10386.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_kvm(self, publish_mock):
        InterruptCollector.PROC = self.getFixturePath('interrupts_kvm_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        InterruptCollector.PROC = self.getFixturePath('interrupts_kvm_2')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {
            'IO-APIC-edge.timer.0.CPU0': 279023.000000,
            'IO-APIC-edge.timer.0.total': 279023.000000,
            'IO-APIC-level.virtio0-virtio1.10.CPU0': 15068.000000,
            'IO-APIC-level.virtio0-virtio1.10.total': 15068.000000,
            'LOC.CPU0': 278993.000000,
            'LOC.CPU1': 279000.000000,
            'LOC.total': 557993.000000,
        })

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = testsoft
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from soft import SoftInterruptCollector

################################################################################


class TestSoftInterruptCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SoftInterruptCollector', {
            'interval': 1
        })

        self.collector = SoftInterruptCollector(config, None)

    def test_import(self):
        self.assertTrue(SoftInterruptCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_stat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/stat', 'r')

    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        patch_open = patch('__builtin__.open', Mock(return_value=StringIO(
            'softirq 0 0 0 0 0 0 0 0 0 0 0'
        )))

        patch_open.start()
        self.collector.collect()
        patch_open.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_open = patch('__builtin__.open', Mock(return_value=StringIO(
            'softirq 55 1 2 3 4 5 6 7 8 9 10'
        )))

        patch_open.start()
        self.collector.collect()
        patch_open.stop()

        self.assertPublishedMany(publish_mock, {
            'total': 55.0,
            '0': 1,
            '1': 2,
            '2': 3,
            '3': 4,
            '4': 5,
            '5': 6,
            '6': 7,
            '7': 8,
            '8': 9,
            '9': 10,
        })

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        SoftInterruptCollector.PROC = self.getFixturePath('proc_stat_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        SoftInterruptCollector.PROC = self.getFixturePath('proc_stat_2')
        self.collector.collect()

        metrics = {
            'total': 4971,
            '0': 0,
            '1': 1729,
            '2': 2,
            '3': 240,
            '4': 31,
            '5': 0,
            '6': 0,
            '7': 1480,
            '8': 0,
            '9': 1489,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = iodrivesnmp
# coding=utf-8

"""
SNMPCollector for Fusion IO DRives Metrics. ( Subclass of snmpCollector )
Based heavily on the NetscalerSNMPCollector.

This collecter currently assumes a single IODrive I or IODrive II and not the
DUO, Octals, or multiple IODrive I or IIs. It needs to be enhanced to account
for multiple fio devices. ( Donations being accepted )

The metric path is configured to be under servers.<host>.<device> where host
and device is defined in the IODriveSNMPCollector.conf.  So given the example
conf below the metricpath would be
"servers.my_host.iodrive.<metric> name.

# EXAMPLE CONF file

enabled = True
[devices]
[[iodrive]]
host = my_host
port = 161
community = mycommunitystring

"""

import sys
import os
import time
import struct

# Fix Path for locating the SNMPCollector
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
                                             '../',
                                             'snmp',
                                             )))

from diamond.metric import Metric
from snmp import SNMPCollector as parent_SNMPCollector


class IODriveSNMPCollector(parent_SNMPCollector):
    """
    SNMPCollector for a single Fusion IO Drive
    """

    IODRIVE_STATS = {

        "InternalTemp": "1.3.6.1.4.1.30018.1.2.1.1.1.24.5",

        "MilliVolts": "1.3.6.1.4.1.30018.1.2.1.1.1.32.5",
        "MilliWatts": "1.3.6.1.4.1.30018.1.2.1.1.1.35.5",
        "MilliAmps": "1.3.6.1.4.1.30018.1.2.1.1.1.37.5",
    }

    IODRIVE_BYTE_STATS = {

        "BytesReadU": "1.3.6.1.4.1.30018.1.2.2.1.1.12.5",
        "BytesReadL": "1.3.6.1.4.1.30018.1.2.2.1.1.13.5",

        "BytesWrittenU": "1.3.6.1.4.1.30018.1.2.2.1.1.14.5",
        "BytesWrittenL": "1.3.6.1.4.1.30018.1.2.2.1.1.15.5",

    }

    MAX_VALUE = 18446744073709551615

    def get_default_config_help(self):
        config_help = super(IODriveSNMPCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': 'Host address',
            'port': 'SNMP port to collect snmp data',
            'community': 'SNMP community',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(IODriveSNMPCollector, self).get_default_config()
        config.update({
            'path':     'iodrive',
            'timeout':  15,
        })
        return config

    def get_string_index_oid(self, s):
        """Turns a string into an oid format is length of name followed by
        name chars in ascii"""
        return (len(self.get_bytes(s)), ) + self.get_bytes(s)

    def get_bytes(self, s):
        """Turns a string into a list of byte values"""
        return struct.unpack('%sB' % len(s), s)

    def collect_snmp(self, device, host, port, community):
        """
        Collect Fusion IO Drive SNMP stats from device
        host and device are from the conf file. In the future device should be
        changed to be what IODRive device it being checked.
        i.e. fioa, fiob.
        """

        # Log
        #self.log.info("Collecting Fusion IO Drive statistics from: %s", device)

        # Set timestamp
        timestamp = time.time()

        for k, v in self.IODRIVE_STATS.items():
            # Get Metric Name and Value
            metricName = '.'.join([k])
            metricValue = int(self.get(v, host, port, community)[v])

            # Get Metric Path
            metricPath = '.'.join(['servers', host, device, metricName])

            # Create Metric
            metric = Metric(metricPath, metricValue, timestamp, 0)

            # Publish Metric
            self.publish_metric(metric)

        for k, v in self.IODRIVE_BYTE_STATS.items():
            # Get Metric Name and Value
            metricName = '.'.join([k])
            metricValue = int(self.get(v, host, port, community)[v])

            # Get Metric Path
            metricPath = '.'.join(['servers', host, device, metricName])

            # Create Metric
            metric = Metric(metricPath, metricValue, timestamp, 0)

            # Publish Metric
            self.publish_metric(metric)

########NEW FILE########
__FILENAME__ = testiodrivesnmp
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from iodrivesnmp import IODriveSNMPCollector


class TestIODriveSNMPCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('IODriveSNMPCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = IODriveSNMPCollector(config, None)

    def test_import(self):
        self.assertTrue(IODriveSNMPCollector)

########NEW FILE########
__FILENAME__ = ipmisensor
# coding=utf-8

"""
This collector uses the [ipmitool](http://openipmi.sourceforge.net/) to read
hardware sensors from servers
using the Intelligent Platform Management Interface (IPMI). IPMI is very common
with server hardware but usually not available in consumer hardware.

#### Dependencies

 * [ipmitool](http://openipmi.sourceforge.net/)

"""

import diamond.collector
import subprocess
import os
import re
import getpass


class IPMISensorCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(IPMISensorCollector, self).get_default_config_help()
        config_help.update({
            'bin': 'Path to the ipmitool binary',
            'use_sudo': 'Use sudo?',
            'sudo_cmd': 'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(IPMISensorCollector, self).get_default_config()
        config.update({
            'bin':              '/usr/bin/ipmitool',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
            'path':             'ipmi.sensors'
        })
        return config

    def collect(self):
        if (not os.access(self.config['bin'], os.X_OK)
            or (self.config['use_sudo']
                and not os.access(self.config['sudo_cmd'], os.X_OK))):
            return False

        command = [self.config['bin'], 'sensor']

        if self.config['use_sudo'] and getpass.getuser() != 'root':
            command.insert(0, self.config['sudo_cmd'])

        p = subprocess.Popen(command,
                             stdout=subprocess.PIPE).communicate()[0][:-1]

        for i, v in enumerate(p.split("\n")):
            data = v.split("|")
            try:
                # Complex keys are fun!
                metric_name = data[0].strip().replace(".",
                                                      "_").replace(" ", ".")
                value = data[1].strip()

                # Skip missing sensors
                if value == '0x0' or value == 'na':
                    continue

                # Extract out a float value
                vmatch = re.search("([0-9.]+)", value)
                if not vmatch:
                    continue
                metric_value = float(vmatch.group(1))

                # Publish
                self.publish(metric_name, metric_value)
            except ValueError:
                continue
            except IndexError:
                continue

        return True

########NEW FILE########
__FILENAME__ = testipmisensor
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from ipmisensor import IPMISensorCollector

################################################################################


class TestIPMISensorCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('IPMISensorCollector', {
            'interval': 10,
            'bin': 'true',
            'use_sudo': False
        })

        self.collector = IPMISensorCollector(config, None)

    def test_import(self):
        self.assertTrue(IPMISensorCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('ipmitool.out').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        metrics = {
            'System.Temp': 32.000000,
            'CPU1.Vcore': 1.080000,
            'CPU2.Vcore': 1.000000,
            'CPU1.VTT': 1.120000,
            'CPU2.VTT': 1.176000,
            'CPU1.DIMM': 1.512000,
            'CPU2.DIMM': 1.512000,
            '+1_5V': 1.512000,
            '+1_8V': 1.824000,
            '+5V': 4.992000,
            '+12V': 12.031000,
            '+1_1V': 1.112000,
            '+3_3V': 3.288000,
            '+3_3VSB': 3.240000,
            'VBAT': 3.240000,
            'Fan1': 4185.000000,
            'Fan2': 4185.000000,
            'Fan3': 4185.000000,
            'Fan7': 3915.000000,
            'Fan8': 3915.000000,
            'Intrusion': 0.000000,
            'PS.Status': 0.000000,
            'P1-DIMM1A.Temp': 41.000000,
            'P1-DIMM1B.Temp': 39.000000,
            'P1-DIMM2A.Temp': 38.000000,
            'P1-DIMM2B.Temp': 40.000000,
            'P1-DIMM3A.Temp': 37.000000,
            'P1-DIMM3B.Temp': 38.000000,
            'P2-DIMM1A.Temp': 39.000000,
            'P2-DIMM1B.Temp': 38.000000,
            'P2-DIMM2A.Temp': 39.000000,
            'P2-DIMM2B.Temp': 39.000000,
            'P2-DIMM3A.Temp': 39.000000,
            'P2-DIMM3B.Temp': 40.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ipvs
# coding=utf-8

"""
Shells out to get ipvs statistics, which may or may not require sudo access

#### Dependencies

 * /usr/sbin/ipvsadmin

"""

import diamond.collector
import subprocess
import os
import string
from diamond.collector import str_to_bool


class IPVSCollector(diamond.collector.Collector):

    def __init__(self, config, handlers):
        super(IPVSCollector, self).__init__(config, handlers)

        # Verify the --exact flag works
        self.statcommand = [self.config['bin'], '--list', '--stats',
                            '--numeric', '--exact']
        self.concommand = [self.config['bin'], '--list', '--numeric']

        if str_to_bool(self.config['use_sudo']):
            self.statcommand.insert(0, self.config['sudo_cmd'])
            self.concommand.insert(0, self.config['sudo_cmd'])
            # The -n (non-interactive) option prevents sudo from
            # prompting the user for a password.
            self.statcommand.insert(1, '-n')
            self.concommand.insert(1, '-n')

        p = subprocess.Popen(self.statcommand, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
        p.wait()

        if p.returncode == 255:
            self.statcommand = filter(
                lambda a: a != '--exact', self.statcommand)

    def get_default_config_help(self):
        config_help = super(IPVSCollector, self).get_default_config_help()
        config_help.update({
            'bin': 'Path to ipvsadm binary',
            'use_sudo': 'Use sudo?',
            'sudo_cmd': 'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(IPVSCollector, self).get_default_config()
        config.update({
            'bin':              '/usr/sbin/ipvsadm',
            'use_sudo':         True,
            'sudo_cmd':         '/usr/bin/sudo',
            'path':             'ipvs'
        })
        return config

    def collect(self):
        if not os.access(self.config['bin'], os.X_OK):
            self.log.error("%s is not executable", self.config['bin'])
            return False

        if (str_to_bool(self.config['use_sudo'])
                and not os.access(self.config['sudo_cmd'], os.X_OK)):
            self.log.error("%s is not executable", self.config['sudo_cmd'])
            return False

        p = subprocess.Popen(self.statcommand,
                             stdout=subprocess.PIPE).communicate()[0][:-1]

        columns = {
            'conns': 2,
            'inpkts': 3,
            'outpkts': 4,
            'inbytes': 5,
            'outbytes': 6,
        }

        external = ""
        backend = ""
        for i, line in enumerate(p.split("\n")):
            if i < 3:
                continue
            row = line.split()

            if row[0] == "TCP" or row[0] == "UDP":
                external = row[0] + "_" + string.replace(row[1], ".", "_")
                backend = "total"
            elif row[0] == "->":
                backend = string.replace(row[1], ".", "_")
            else:
                continue

            for metric, column in columns.iteritems():
                metric_name = ".".join([external, backend, metric])
                # metric_value = int(row[column])
                value = row[column]
                if (value.endswith('K')):
                        metric_value = int(value[0:len(value) - 1]) * 1024
                elif (value.endswith('M')):
                        metric_value = (int(value[0:len(value) - 1]) * 1024
                                        * 1024)
                elif (value.endswith('G')):
                        metric_value = (int(value[0:len(value) - 1]) * 1024.0
                                        * 1024.0 * 1024.0)
                else:
                        metric_value = float(value)

                self.publish(metric_name, metric_value)

        p = subprocess.Popen(self.concommand,
                             stdout=subprocess.PIPE).communicate()[0][:-1]

        columns = {
            'active': 4,
            'inactive': 5,
        }

        external = ""
        backend = ""
        total = {}
        for i, line in enumerate(p.split("\n")):
            if i < 3:
                continue
            row = line.split()

            if row[0] == "TCP" or row[0] == "UDP":
                if total:
                    for metric, value in total.iteritems():
                        self.publish(
                            ".".join([external, "total", metric]), value)

                for k in columns.keys():
                    total[k] = 0.0

                external = row[0] + "_" + string.replace(row[1], ".", "_")
                continue
            elif row[0] == "->":
                backend = string.replace(row[1], ".", "_")
            else:
                continue

            for metric, column in columns.iteritems():
                metric_name = ".".join([external, backend, metric])
                # metric_value = int(row[column])
                value = row[column]
                if (value.endswith('K')):
                        metric_value = int(value[0:len(value) - 1]) * 1024
                elif (value.endswith('M')):
                        metric_value = (int(value[0:len(value) - 1]) * 1024
                                        * 1024)
                elif (value.endswith('G')):
                        metric_value = (int(value[0:len(value) - 1]) * 1024.0
                                        * 1024.0 * 1024.0)
                else:
                        metric_value = float(value)

                total[metric] += metric_value
                self.publish(metric_name, metric_value)

        if total:
            for metric, value in total.iteritems():
                self.publish(".".join([external, "total", metric]), value)

########NEW FILE########
__FILENAME__ = testipvs
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from ipvs import IPVSCollector

################################################################################


class TestIPVSCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('IPVSCollector', {
            'interval': 10,
            'bin': 'true',
            'use_sudo': False
        })

        self.collector = IPVSCollector(config, None)

    def test_import(self):
        self.assertTrue(IPVSCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('ipvsadm').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        metrics = {
            "TCP_172_16_1_56:80.total.conns": 116,
            "TCP_172_16_1_56:443.total.conns": 59,
            "TCP_172_16_1_56:443.10_68_15_66:443.conns": 59,
            "TCP_172_16_1_56:443.10_68_15_66:443.outbytes": 216873,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = jbossapi
# coding=utf-8

"""
V. 1.0

JbossApiCollector is a collector that uses JBOSS 7 native API to collect data
Tested on jboss 7.X.X??

Much of the code was borrowed from:

http://bit.ly/XRrCWx
https://github.com/lukaf/munin-plugins/blob/master/jboss7_

References:
https://docs.jboss.org/author/display/AS7/Management+API+reference
http://middlewaremagic.com/jboss/?p=2476

TODO:
This code was made to work with the local system 'curl' command, due to
difficulties getting urllib2 or pycurl to work under the python 2.4 options
successfully doing SSL Digest Authentication.

Plan is to make this code work with newer versions of python and possibly
Requests. (http://docs.python-requests.org/en/latest/)

If possible, please make future updates backwards compatable to call the local
curl as an option.


#### Dependencies

 * java
 * jboss
 * curl
 * json

##### Configuration

# Uses local system curl until can be made to work with either urllib2, pycurl,
or requests (http://docs.python-requests.org/en/latest/)


enabled = True
path_suffix = ""
measure_collector_time = False
interface_regex = ^(.+?)\.
curl_bin = /usr/bin/curl
connect_timeout = 4
hosts = wasadmin:pass@host:9443:https, wasadmin:pass@host:9443:https
curl_options = "-s --digest -L "
ssl_options = "--sslv3 -k"
connector_stats = True | False
connector_options =  http, ajp
app_stats = True | False
jvm_memory_stats = True | False
jvm_buffer_pool_stats = True | False
jvm_memory_pool_stats = True | False
jvm_gc_stats = True | False
jvm_thread_stats = True | False


"""

import diamond.collector
import os
import re
import subprocess

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json


## Setup a set of VARs
# Set this for use in curl request
header = '"Content-Type: application/json"'

operational_type = [
    'app',
    'web',
    'jvm',
]


web_stats = [
    'errorCount',
    'requestCount',
    'bytesReceived',
    'bytesSent',
    'processingTime',
]

memory_types = [
    'init',
    'used',
    'committed',
    'max',
]

buffer_pool_types = [
    'count',
    'memory-used',
]

thread_types = [
    'thread-count',
    'daemon-thread-count'
]

memory_topics = [
    'heap-memory-usage',
    'non-heap-memory-usage',
]

gc_types = [
    'collection-count',
    'collection-time',
]


class JbossApiCollector(diamond.collector.Collector):

    def __init__(self, config, handlers):
        diamond.collector.Collector.__init__(self, config, handlers)

        if self.config['hosts'].__class__.__name__ != 'list':
            self.config['hosts'] = [self.config['hosts']]

        # get the params for each host
        if 'host' in self.config:
            hoststr = "%s:%s@%s:%s:%s" % (
                self.config['user'],
                self.config['password'],
                self.config['host'],
                self.config['port'],
                self.config['proto'],
            )
            self.config['hosts'].append(hoststr)

        self.db = None

        if type(self.config['connector_options']) is not list:
            self.config['connector_options'] = [
                self.config['connector_options']]

    def get_default_config_help(self):
        # Need to update this when done to help explain details when running
        # diamond-setup.
        config_help = super(JbossApiCollector, self).get_default_config_help()
        config_help.update({
            'curl_bin': 'Path to system curl executable',
            'hosts': 'List of hosts to collect from. Format is yourusername:yourpassword@host:port:proto',  # NOQA
            'app_stats': 'Collect application pool stats',
            'jvm_memory_pool_stats': 'Collect JVM memory-pool stats',
            'jvm_buffer_pool_stats': 'Collect JVM buffer-pool stats',
            'jvm_memory_stats': 'Collect JVM basic memory stats',
            'jvm_gc_stats': 'Collect JVM garbage-collector stats',
            'jvm_thread_stats': 'Collect JVM thread stas',
            'connector_stats': 'Collect HTTP and AJP Connector stats',
            'connector_options': 'Types of connectors to collect'
        })
        return config_help

    def get_default_config(self):

        # Initialize default config
        config = super(JbossApiCollector, self).get_default_config()
        config.update({
            'path': 'jboss',
            'method': 'Sequential',
            'curl_bin': '/usr/bin/curl',
            'connect_timeout': '4',
            'ssl_options': '--sslv3 -k',
            'curl_options': '-s --digest -L ',
            'interface_regex': '^(.+?)\.',  # matches up to first "."
            'hosts': [],
            'app_stats': 'True',
            'connector_options': ['http', 'ajp'],
            'jvm_memory_pool_stats': 'True',
            'jvm_buffer_pool_stats': 'True',
            'jvm_memory_stats': 'True',
            'jvm_gc_stats': 'True',
            'jvm_thread_stats': 'True',
            'connector_stats': 'True'
        })
        # Return default config
        return config

    def get_stats(self, current_host, current_port, current_proto, current_user,
                  current_pword):

        if not os.access(self.config['curl_bin'], os.X_OK):
            self.log.error("%s is not executable or does not exist.",
                           self.config['curl_bin'])

        # Check if there is a RegEx to perform on the interface names
        if self.config['interface_regex'] != '':
            interface = self.string_regex(self.config['interface_regex'],
                                          current_host)

        else:
            # Clean up any possible extra "."'s in the interface, keeps
            # graphite from creating directories
            interface = self.string_fix(current_host)

        for op_type in operational_type:
            output = self.get_data(op_type, current_host, current_port,
                                   current_proto, current_user, current_pword)
            if op_type == 'app' and self.config['app_stats'] == 'True':
                if output:
                # Grab the pool stats for each Instance
                    for instance in output['result']['data-source']:
                        datasource = output['result']['data-source'][instance]
                        for metric in datasource['statistics']['pool']:
                            metricName = '%s.%s.%s.statistics.pool.%s' % (
                                interface, op_type, instance, metric)
                            metricValue = datasource[
                                'statistics']['pool'][metric]
                            self.publish(metricName, float(metricValue))

            if op_type == 'web' and self.config['connector_stats'] == 'True':
                if output:
                    # Grab http and ajp info (make these options)
                    for c_type in self.config['connector_options']:
                    #for connector_type in self.config['connector_options']:
                        for metric in web_stats:
                            metricName = '%s.%s.connector.%s.%s' % (interface,
                                                                    op_type,
                                                                    c_type,
                                                                    metric)
                            connector = output['result']['connector']
                            metricValue = connector[c_type][metric]
                            self.publish(metricName, float(metricValue))

            if op_type == 'jvm':
                if output:
                    if self.config['jvm_memory_pool_stats'] == 'True':
                        # Grab JVM memory pool stats
                        mempool = output['result']['type']['memory-pool']
                        for pool_name in mempool['name']:
                            for metric in memory_types:
                                metricName = '%s.%s.%s.%s.%s.%s' % (interface,
                                                                    op_type,
                                                                    'memory-'
                                                                    + 'pool',
                                                                    pool_name,
                                                                    'usage',
                                                                    metric)
                                metricValue = mempool['name'][pool_name][
                                    'usage'][metric]
                                self.publish(metricName, float(metricValue))

                    # Grab JVM buffer-pool stats
                    if self.config['jvm_buffer_pool_stats'] == 'True':
                        bufferpool = output['result']['type']['buffer-pool']
                        for pool in bufferpool['name']:
                            for metric in buffer_pool_types:
                                metricName = '%s.%s.%s.%s.%s' % (interface,
                                                                 op_type,
                                                                 'buffer-pool',
                                                                 pool,
                                                                 metric)
                                metricValue = bufferpool['name'][pool][metric]
                                self.publish(metricName, float(metricValue))

                    # Grab basic memory stats
                    if self.config['jvm_memory_stats'] == 'True':
                        for mem_type in memory_topics:
                            for metric in memory_types:
                                metricName = '%s.%s.%s.%s.%s' % (interface,
                                                                 op_type,
                                                                 'memory',
                                                                 mem_type,
                                                                 metric)
                                memory = output['result']['type']['memory']
                                metricValue = memory[mem_type][metric]
                                self.publish(metricName, float(metricValue))

                    # Grab Garbage collection stats
                    if self.config['jvm_gc_stats'] == 'True':
                        garbage = output['result']['type']['garbage-collector']
                        for gc_name in garbage['name']:
                            for metric in gc_types:
                                metricName = '%s.%s.%s.%s.%s' % (interface,
                                                                 op_type,
                                                                 'garbage-'
                                                                 + 'collector',
                                                                 gc_name,
                                                                 metric)
                                metricValue = garbage['name'][gc_name][metric]
                                self.publish(metricName, float(metricValue))

                    # Grab threading stats
                    if self.config['jvm_thread_stats'] == 'True':
                        for metric in thread_types:
                            metricName = '%s.%s.%s.%s' % (interface, op_type,
                                                          'threading', metric)
                            threading = output['result']['type']['threading']
                            metricValue = threading[metric]
                            self.publish(metricName, float(metricValue))

        return True

    def get_data(self, op_type, current_host, current_port, current_proto,
                 current_user, current_pword):
        output = {}
        if op_type == 'app':
            data = ('{"operation":"read-resource", "include-runtime":"true", '
                    + '"recursive":"true" , "address":["subsystem",'
                    + '"datasources"]}')

        if op_type == 'web':
            data = ('{"operation":"read-resource", "include-runtime":"true", '
                    + '"recursive":"true" , "address":["subsystem","web"]}')

        if op_type == 'jvm':
            data = ('{"operation":"read-resource", "include-runtime":"true", '
                    + '"recursive":"true" , "address":["core-service",'
                    + '"platform-mbean"]}')

        the_cmd = (("%s --connect-timeout %s %s %s %s://%s:%s/management "
                   + "--header %s -d '%s' -u %s:%s") % (
            self.config['curl_bin'], self.config['connect_timeout'],
            self.config['ssl_options'], self.config['curl_options'],
            current_proto, current_host, current_port, header, data,
            current_user, current_pword))

        try:
            attributes = subprocess.Popen(the_cmd, shell=True,
                                          stdout=subprocess.PIPE
                                          ).communicate()[0]
            output = json.loads(attributes)
        except Exception, e:
            self.log.error("JbossApiCollector: There was an exception %s", e)
            output = ''
        return output

    def string_fix(self, s):
        return re.sub(r"[^a-zA-Z0-9_]", "_", s)

    def string_regex(self, pattern, s):
        tmp_result = re.match(pattern, s)
        return tmp_result.group(1)

    def collect(self):

        for host in self.config['hosts']:
            matches = re.search(
                '^([^:]*):([^@]*)@([^:]*):([^:]*):?(.*)', host)

            if not matches:
                continue

            current_host = matches.group(3)
            current_port = int(matches.group(4))
            current_proto = matches.group(5)
            current_user = matches.group(1)
            current_pword = matches.group(2)

            # Call get_stats for each instance of jboss
            self.get_stats(current_host, current_port, current_proto,
                           current_user, current_pword)

        return True

########NEW FILE########
__FILENAME__ = testjbossapi
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from jbossapi import JbossApiCollector


###############################################################################

class TestJbossApiCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('JbossApiCollector', {
        })
        self.collector = JbossApiCollector(config, None)

    def test_import(self):
        self.assertTrue(JbossApiCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = collectd_network
#! /usr/bin/env python
# -*- coding: utf-8 -*-
# vim: fileencoding=utf-8
#
# Copyright © 2009 Adrian Perez <aperez@igalia.com>
#
# Distributed under terms of the GPLv2 license or newer.
#
#
# Renzo Toma (rtoma@bol.com) 6 Aug 2013
# - added non-blocking I/O for receive
#
# Frank Marien (frank@apsu.be) 6 Sep 2012
# - quick fixes for 5.1 binary protocol
# - updated to python 3
# - fixed for larger packet sizes (possible on lo interface)
# - fixed comment typo (decode_network_string decodes a string)
#
# @see:
# https://raw.github.com/collectd/collectd/master/contrib/collectd_network.py

"""
Collectd network protocol implementation.
"""

import socket
import struct
import select
import platform
if platform.python_version() < '2.8.0':
    # Python 2.7 and below io.StringIO does not like unicode
    from StringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
else:
    try:
        from io import StringIO
        StringIO  # workaround for pyflakes issue #13
    except ImportError:
        from cStringIO import StringIO

from datetime import datetime
from copy import deepcopy


DEFAULT_PORT = 25826
"""Default port"""

DEFAULT_IPv4_GROUP = "239.192.74.66"
"""Default IPv4 multicast group"""

DEFAULT_IPv6_GROUP = "ff18::efc0:4a42"
"""Default IPv6 multicast group"""

HR_TIME_DIV = (2.0 ** 30)

# Message kinds
TYPE_HOST = 0x0000
TYPE_TIME = 0x0001
TYPE_TIME_HR = 0x0008
TYPE_PLUGIN = 0x0002
TYPE_PLUGIN_INSTANCE = 0x0003
TYPE_TYPE = 0x0004
TYPE_TYPE_INSTANCE = 0x0005
TYPE_VALUES = 0x0006
TYPE_INTERVAL = 0x0007
TYPE_INTERVAL_HR = 0x0009

# For notifications
TYPE_MESSAGE = 0x0100
TYPE_SEVERITY = 0x0101

# DS kinds
DS_TYPE_COUNTER = 0
DS_TYPE_GAUGE = 1
DS_TYPE_DERIVE = 2
DS_TYPE_ABSOLUTE = 3

if hasattr(struct, 'Struct'):
    header = struct.Struct("!2H")
    number = struct.Struct("!Q")
    short = struct.Struct("!H")
    double = struct.Struct("<d")


def decode_network_values(ptype, plen, buf):
    """Decodes a list of DS values in collectd network format
    """
    nvalues = short.unpack_from(buf, header.size)[0]
    off = header.size + short.size + nvalues
    valskip = double.size

    # Check whether our expected packet size is the reported one
    assert ((valskip + 1) * nvalues + short.size + header.size) == plen
    assert double.size == number.size

    result = []
    for dstype in [ord(x) for x in buf[header.size + short.size:off]]:
        if dstype == DS_TYPE_COUNTER:
            result.append((dstype, number.unpack_from(buf, off)[0]))
            off += valskip
        elif dstype == DS_TYPE_GAUGE:
            result.append((dstype, double.unpack_from(buf, off)[0]))
            off += valskip
        elif dstype == DS_TYPE_DERIVE:
            result.append((dstype, number.unpack_from(buf, off)[0]))
            off += valskip
        elif dstype == DS_TYPE_ABSOLUTE:
            result.append((dstype, number.unpack_from(buf, off)[0]))
            off += valskip
        else:
            raise ValueError("DS type %i unsupported" % dstype)

    return result


def decode_network_number(ptype, plen, buf):
    """Decodes a number (64-bit unsigned) from collectd network format.
    """
    return number.unpack_from(buf, header.size)[0]


def decode_network_string(msgtype, plen, buf):
    """Decodes a string from collectd network format.
    """
    return buf[header.size:plen - 1]


# Mapping of message types to decoding functions.
_decoders = {
    TYPE_VALUES:            decode_network_values,
    TYPE_TIME:              decode_network_number,
    TYPE_TIME_HR:           decode_network_number,
    TYPE_INTERVAL:          decode_network_number,
    TYPE_INTERVAL_HR:       decode_network_number,
    TYPE_HOST:              decode_network_string,
    TYPE_PLUGIN:            decode_network_string,
    TYPE_PLUGIN_INSTANCE:   decode_network_string,
    TYPE_TYPE:              decode_network_string,
    TYPE_TYPE_INSTANCE:     decode_network_string,
    TYPE_MESSAGE:           decode_network_string,
    TYPE_SEVERITY:          decode_network_number,
}


def decode_network_packet(buf):
    """Decodes a network packet in collectd format.
    """
    off = 0
    blen = len(buf)

    while off < blen:
        ptype, plen = header.unpack_from(buf, off)

        if plen > blen - off:
            raise ValueError("Packet longer than amount of data in buffer")

        if ptype not in _decoders:
            raise ValueError("Message type %i not recognized" % ptype)

        yield ptype, _decoders[ptype](ptype, plen, buf[off:])
        off += plen


class Data(object):
    time = 0
    host = None
    plugin = None
    plugininstance = None
    type = None
    typeinstance = None

    def __init__(self, **kw):
        [setattr(self, k, v) for k, v in kw.items()]

    @property
    def datetime(self):
        return datetime.fromtimestamp(self.time)

    @property
    def source(self):
        buf = StringIO()
        if self.host:
            buf.write(str(self.host))
        if self.plugin:
            buf.write("/")
            buf.write(str(self.plugin))
        if self.plugininstance:
            buf.write("/")
            buf.write(str(self.plugininstance))
        if self.type:
            buf.write("/")
            buf.write(str(self.type))
        if self.typeinstance:
            buf.write("/")
            buf.write(str(self.typeinstance))
        return buf.getvalue()

    def __str__(self):
        return "[%i] %s" % (self.time, self.source)


class Notification(Data):
    FAILURE = 1
    WARNING = 2
    OKAY = 4

    SEVERITY = {
        FAILURE: "FAILURE",
        WARNING: "WARNING",
        OKAY:    "OKAY",
    }

    __severity = 0
    message = ""

    def __set_severity(self, value):
        if value in (self.FAILURE, self.WARNING, self.OKAY):
            self.__severity = value

    severity = property(lambda self: self.__severity, __set_severity)

    @property
    def severitystring(self):
        return self.SEVERITY.get(self.severity, "UNKNOWN")

    def __str__(self):
        return "%s [%s] %s" % (
               super(Notification, self).__str__(),
               self.severitystring,
               self.message)


class Values(Data, list):
    def __str__(self):
        return "%s %s" % (Data.__str__(self), list.__str__(self))


def interpret_opcodes(iterable):
    vl = Values()
    nt = Notification()

    for kind, data in iterable:
        if kind == TYPE_TIME:
            vl.time = nt.time = data
        elif kind == TYPE_TIME_HR:
            vl.time = nt.time = data / HR_TIME_DIV
        elif kind == TYPE_INTERVAL:
            vl.interval = data
        elif kind == TYPE_INTERVAL_HR:
            vl.interval = data / HR_TIME_DIV
        elif kind == TYPE_HOST:
            vl.host = nt.host = data
        elif kind == TYPE_PLUGIN:
            vl.plugin = nt.plugin = data
        elif kind == TYPE_PLUGIN_INSTANCE:
            vl.plugininstance = nt.plugininstance = data
        elif kind == TYPE_TYPE:
            vl.type = nt.type = data
        elif kind == TYPE_TYPE_INSTANCE:
            vl.typeinstance = nt.typeinstance = data
        elif kind == TYPE_SEVERITY:
            nt.severity = data
        elif kind == TYPE_MESSAGE:
            nt.message = data
            yield deepcopy(nt)
        elif kind == TYPE_VALUES:
            vl[:] = data
            yield deepcopy(vl)


class Reader(object):
    """Network reader for collectd data.

    Listens on the network in a given address, which can be a multicast
    group address, and handles reading data when it arrives.
    """
    addr = None
    host = None
    port = DEFAULT_PORT

    BUFFER_SIZE = 16384

    def __init__(self, host=None, port=DEFAULT_PORT, multicast=False):
        if host is None:
            multicast = True
            host = DEFAULT_IPv4_GROUP

        self.host, self.port = host, port
        self.ipv6 = ":" in self.host

        if multicast:
            hostname = None
        else:
            hostname = self.host

        if self.ipv6:
            sock_type = socket.AF_INET6
        else:
            sock_type = socket.AF_UNSPEC

        family, socktype, proto, canonname, sockaddr = socket.getaddrinfo(
            hostname,
            self.port,
            sock_type,
            socket.SOCK_DGRAM, 0, socket.AI_PASSIVE)[0]

        self._sock = socket.socket(family, socktype, proto)
        self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self._sock.bind(sockaddr)

        if multicast:
            if hasattr(socket, "SO_REUSEPORT"):
                self._sock.setsockopt(
                    socket.SOL_SOCKET,
                    socket.SO_REUSEPORT, 1)

            val = None
            if family == socket.AF_INET:
                assert "." in self.host
                val = struct.pack("4sl",
                                  socket.inet_aton(self.host),
                                  socket.INADDR_ANY)
            elif family == socket.AF_INET6:
                raise NotImplementedError("IPv6 support not ready yet")
            else:
                raise ValueError("Unsupported network address family")

            if self.ipv6:
                sock_type = socket.IPPROTO_IPV6
            else:
                sock_type = socket.IPPROTO_IP

            self._sock.setsockopt(
                sock_type,
                socket.IP_ADD_MEMBERSHIP, val)
            self._sock.setsockopt(
                sock_type,
                socket.IP_MULTICAST_LOOP, 0)

        self._readlist = [self._sock]

    def receive(self, poll_interval):
        """Receives a single raw collect network packet.
        """
        readable, writeable, errored = select.select(self._readlist, [], [],
                                                     poll_interval)
        for s in readable:
            data, addr = s.recvfrom(self.BUFFER_SIZE)
            if data:
                return data

        return None

    def decode(self, poll_interval, buf=None):
        """Decodes a given buffer or the next received packet.
        """
        if buf is None:
            buf = self.receive(poll_interval)
        if buf is None:
            return None
        return decode_network_packet(buf)

    def interpret(self, iterable=None, poll_interval=0.2):
        """Interprets a sequence
        """
        if iterable is None:
            iterable = self.decode(poll_interval)
            if iterable is None:
                return None

        if isinstance(iterable, basestring):
            iterable = self.decode(poll_interval, iterable)

        return interpret_opcodes(iterable)

########NEW FILE########
__FILENAME__ = jcollectd
# coding=utf-8

"""
The JCollectdCollector is capable of receiving Collectd network traffic
as sent by the JCollectd jvm agent (and child Collectd processes).

Reason for developing this collector is allowing to use JCollectd, without
the need for Collectd.

A few notes:

This collector starts a UDP server to receive data. This server runs in
a separate thread and puts it on a queue, waiting for the collect() method
to pull. Because of this setup, the collector interval parameter is of
less importance. What matters is the 'sendinterval' JCollectd parameter.

See https://github.com/emicklei/jcollectd for an up-to-date jcollect fork.

#### Dependencies

 * jcollectd sending metrics

"""


import threading
import re
import Queue

import diamond.collector
import diamond.metric

import collectd_network


ALIVE = True


class JCollectdCollector(diamond.collector.Collector):

    def __init__(self, *args, **kwargs):
        super(JCollectdCollector, self).__init__(*args, **kwargs)
        self.listener_thread = None

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(JCollectdCollector, self).get_default_config()
        config.update({
            'path':     'jvm',
            'enabled':  'False',
            'method':   'Threaded',
            'listener_host': '127.0.0.1',
            'listener_port': 25826,
        })
        return config

    def collect(self):
        if not self.listener_thread:
            self.start_listener()

        q = self.listener_thread.queue
        while True:
            try:
                dp = q.get(False)
                metric = self.make_metric(dp)
            except Queue.Empty:
                break
            self.publish_metric(metric)

    def start_listener(self):
        self.listener_thread = ListenerThread(self.config['listener_host'],
                                              self.config['listener_port'],
                                              self.log)
        self.listener_thread.start()

    def stop_listener(self):
        global ALIVE
        ALIVE = False
        self.listener_thread.join()
        self.log.error('Listener thread is shut down.')

    def make_metric(self, dp):

        path = ".".join((dp.host, self.config['path'], dp.name))

        if 'path_prefix' in self.config:
            prefix = self.config['path_prefix']
            if prefix:
                path = ".".join((prefix, path))

        if 'path_suffix' in self.config:
            suffix = self.config['path_suffix']
            if suffix:
                path = ".".join((path, suffix))

        if dp.is_counter:
            metric_type = "COUNTER"
        else:
            metric_type = "GAUGE"
        metric = diamond.metric.Metric(path, dp.value, dp.time,
                                       metric_type=metric_type)

        return metric

    def __del__(self):
        if self.listener_thread:
            self.stop_listener()


class ListenerThread(threading.Thread):
    def __init__(self, host, port, log, poll_interval=0.4):
        super(ListenerThread, self).__init__()
        self.name = 'JCollectdListener'  # thread name

        self.host = host
        self.port = port
        self.log = log
        self.poll_interval = poll_interval

        self.queue = Queue.Queue()

    def run(self):
        self.log.info('ListenerThread started on {0}:{1}(udp)'.format(
            self.host, self.port))

        rdr = collectd_network.Reader(self.host, self.port)

        try:
            while ALIVE:
                items = rdr.interpret(poll_interval=self.poll_interval)
                self.send_to_collector(items)
        except Exception, e:
            self.log.error('caught exception: type={0}, exc={1}'.format(type(e),
                                                                        e))

        self.log.info('ListenerThread - stop')

    def send_to_collector(self, items):
        if items is None:
            return

        for item in items:
            try:
                metric = self.transform(item)
                self.queue.put(metric)
            except Queue.Full:
                self.log.error('Queue to collector is FULL')
            except Exception, e:
                self.log.error('B00M! type={0}, exception={1}'.format(type(e),
                                                                      e))

    def transform(self, item):

        parts = []

        path = item.plugininstance
        # extract jvm name from 'logstash-MemoryPool Eden Space'
        if '-' in path:
            (jvm, tail) = path.split('-', 1)
            path = tail
        else:
            jvm = 'unnamed'

        # add JVM name
        parts.append(jvm)

        # add mbean name (e.g. 'java_lang')
        parts.append(item.plugin)

        # get typed mbean: 'MemoryPool Eden Space'
        if ' ' in path:
            (mb_type, mb_name) = path.split(' ', 1)
            parts.append(mb_type)
            parts.append(mb_name)
        else:
            parts.append(path)

        # add property name
        parts.append(item.typeinstance)

        # construct full path, from safe parts
        name = '.'.join([re.sub('[\. ]', '_', part) for part in parts])

        if item[0][0] == 0:
            is_counter = True
        else:
            is_counter = False
        dp = Datapoint(item.host, item.time, name, item[0][1], is_counter)

        return dp


class Datapoint(object):
    def __init__(self, host, time, name, value, is_counter):
        self.host = host
        self.time = time
        self.name = name
        self.value = value
        self.is_counter = is_counter

########NEW FILE########
__FILENAME__ = testjcollectd
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from jcollectd import JCollectdCollector


###############################################################################

class TestJCollectdCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('JCollectdCollector', {
        })
        self.collector = JCollectdCollector(config, None)

    def test_import(self):
        self.assertTrue(JCollectdCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = kafka
# coding=utf-8

"""
Collect stats via MX4J from Kafka

#### Dependencies

 * urllib2
 * xml.etree
"""

import urllib2

from urllib import urlencode

try:
    from xml.etree import ElementTree
    ElementTree  # workaround for pyflakes issue #13
except ImportError:
    ElementTree = None

try:
    from ElementTree import ParseError as ETParseError
    ETParseError  # workaround for pyflakes issue #13
except ImportError:
    ETParseError = Exception

import diamond.collector


class KafkaCollector(diamond.collector.Collector):
    MBEAN_WHITELIST = frozenset([
        'kafka.log.LogStats',
        'kafka.network.SocketServerStats',
        'kafka.message.LogFlushStats',
        'kafka.server.BrokerTopicStat',
    ])

    JVM_MBEANS = {
        'java.lang:type=GarbageCollector,name=PS MarkSweep': 'jvm.gc.marksweep',
        'java.lang:type=GarbageCollector,name=PS Scavenge': 'jvm.gc.scavenge',
        'java.lang:type=Threading': 'jvm.threading',
    }

    ATTRIBUTE_TYPES = {
        'double': float,
        'float': float,
        'int': int,
        'long': long,
    }

    def get_default_config_help(self):
        config_help = super(KafkaCollector, self).get_default_config_help()
        config_help.update({
            'host': "",
            'port': "",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(KafkaCollector, self).get_default_config()
        config.update({
            'host': '127.0.0.1',
            'port': 7200,
            'path': 'kafka',
            'method': 'Threaded',
        })
        return config

    def _get(self, path, query_args=None):
        if not path.startswith('/'):
            path = '/' + path

        qargs = {'template': 'identity'}

        if query_args:
            qargs.update(query_args)

        url = 'http://%s:%i%s?%s' % (
            self.config['host'], int(self.config['port']),
            path, urlencode(qargs))

        try:
            response = urllib2.urlopen(url)
        except urllib2.URLError, err:
            self.log.error("%s: %s", url, err)
            return None

        try:
            return ElementTree.fromstring(response.read())
        except ETParseError:
            self.log.error("Unable to parse response from mx4j")
            return None

    def get_mbeans(self):
        query_args = {'querynames': 'kafka:*'}

        mbeans = self._get('/serverbydomain', query_args)
        if mbeans is None:
            return

        found_beans = set()

        for mbean in mbeans.getiterator(tag='MBean'):
            classname = mbean.get('classname')

            if classname not in self.MBEAN_WHITELIST:
                continue

            objectname = mbean.get('objectname')
            if objectname:
                found_beans.add(objectname)

        return found_beans

    def query_mbean(self, objectname, key_prefix=None):
        query_args = {
            'objectname': objectname,
            'operations': False,
            'constructors': False,
            'notifications': False,
        }

        attributes = self._get('/mbean', query_args)
        if attributes is None:
            return

        if key_prefix is None:
            key_prefix = objectname.split('=')[1]

        metrics = {}

        for attrib in attributes.getiterator(tag='Attribute'):
            atype = attrib.get('type')

            ptype = self.ATTRIBUTE_TYPES.get(atype)
            if not ptype:
                continue

            value = ptype(attrib.get('value'))

            name = '.'.join([key_prefix, attrib.get('name')])

            metrics[name] = value

        return metrics

    def collect(self):
        if ElementTree is None:
            self.log.error('Failed to import xml.etree.ElementTree')
            return

        # Get list of gatherable stats
        mbeans = self.get_mbeans()

        metrics = {}

        # Query each one for stats
        for mbean in mbeans:
            stats = self.query_mbean(mbean)
            metrics.update(stats)

        for mbean, key_prefix in self.JVM_MBEANS.iteritems():
            stats = self.query_mbean(mbean, key_prefix)
            metrics.update(stats)

        # Publish stats
        for metric, value in metrics.iteritems():
            self.publish(metric, value)

########NEW FILE########
__FILENAME__ = testkafka
#!/usr/bin/python
# coding=utf-8
###############################################################################
import urllib2

try:
    from xml.etree import ElementTree
    ElementTree  # workaround for pyflakes issue #13
except ImportError:
    ElementTree = None

from test import CollectorTestCase
from test import get_collector_config
from test import run_only
from test import unittest
from mock import patch

from diamond.collector import Collector
from kafka import KafkaCollector

##########


def run_only_if_ElementTree_is_available(func):
    try:
        from xml.etree import ElementTree
        ElementTree  # workaround for pyflakes issue #13
    except ImportError:
        ElementTree = None
    pred = lambda: ElementTree is not None
    return run_only(func, pred)


class TestKafkaCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('KafkaCollector', {
            'interval': 10
        })

        self.collector = KafkaCollector(config, None)

    def _get_xml_fixture(self, name):
        fixture = self.getFixture(name)

        return ElementTree.fromstring(fixture.getvalue())

    def test_import(self):
        self.assertTrue(KafkaCollector)

    @run_only_if_ElementTree_is_available
    @patch('urllib2.urlopen')
    def test_get(self, urlopen_mock):
        urlopen_mock.return_value = self.getFixture('empty.xml')

        result = self.collector._get('/path')
        result_string = ElementTree.tostring(result)

        self.assertEqual(result_string, '<Server />')

    @run_only_if_ElementTree_is_available
    @patch('urllib2.urlopen')
    def test_get_httperror(self, urlopen_mock):
        urlopen_mock.side_effect = urllib2.URLError('BOOM')

        result = self.collector._get('/path')

        self.assertFalse(result)

    @run_only_if_ElementTree_is_available
    @patch('urllib2.urlopen')
    def test_get_bad_xml(self, urlopen_mock):
        urlopen_mock.return_value = self.getFixture('bad.xml')

        result = self.collector._get('/path')

        self.assertFalse(result)

    @run_only_if_ElementTree_is_available
    @patch.object(KafkaCollector, '_get')
    def test_get_mbeans(self, get_mock):
        get_mock.return_value = self._get_xml_fixture('serverbydomain.xml')

        expected_names = set([
            'kafka:type=kafka.BrokerAllTopicStat',
            'kafka:type=kafka.BrokerTopicStat.mytopic',
            'kafka:type=kafka.LogFlushStats',
            'kafka:type=kafka.SocketServerStats',
            'kafka:type=kafka.logs.mytopic-0',
            'kafka:type=kafka.logs.mytopic-1',
        ])

        found_beans = self.collector.get_mbeans()

        self.assertEqual(found_beans, expected_names)

    @run_only_if_ElementTree_is_available
    @patch.object(KafkaCollector, '_get')
    def test_get_mbeans_get_fail(self, get_mock):
        get_mock.return_value = None

        found_beans = self.collector.get_mbeans()

        self.assertEqual(found_beans, None)

    @run_only_if_ElementTree_is_available
    @patch.object(KafkaCollector, '_get')
    def test_query_mbean(self, get_mock):
        get_mock.return_value = self._get_xml_fixture('mbean.xml')

        expected_metrics = {
            'kafka.logs.mytopic-1.CurrentOffset': long('213500615'),
            'kafka.logs.mytopic-1.NumAppendedMessages': long('224634137'),
            'kafka.logs.mytopic-1.NumberOfSegments': int('94'),
            'kafka.logs.mytopic-1.Size': long('50143615339'),
        }

        metrics = self.collector.query_mbean('kafka:type=kafka.logs.mytopic-1')

        self.assertEqual(metrics, expected_metrics)

    @run_only_if_ElementTree_is_available
    @patch.object(KafkaCollector, '_get')
    def test_query_mbean_with_prefix(self, get_mock):
        get_mock.return_value = self._get_xml_fixture('mbean.xml')

        expected_metrics = {
            'some.prefix.CurrentOffset': long('213500615'),
            'some.prefix.NumAppendedMessages': long('224634137'),
            'some.prefix.NumberOfSegments': int('94'),
            'some.prefix.Size': long('50143615339'),
        }

        metrics = self.collector.query_mbean('kafka:type=kafka.logs.mytopic-0',
                                             'some.prefix')

        self.assertEqual(metrics, expected_metrics)

    @run_only_if_ElementTree_is_available
    @patch.object(KafkaCollector, '_get')
    def test_query_mbean_fail(self, get_mock):
        get_mock.return_value = None

        metrics = self.collector.query_mbean('kafka:type=kafka.logs.mytopic-0')

        self.assertEqual(metrics, None)

    @run_only_if_ElementTree_is_available
    @patch('urllib2.urlopen')
    @patch.object(Collector, 'publish')
    def test(self, publish_mock, urlopen_mock):
        urlopen_mock.side_effect = [
            self.getFixture('serverbydomain_logs_only.xml'),
            self.getFixture('mbean.xml'),
            self.getFixture('gc_marksweep.xml'),
            self.getFixture('gc_scavenge.xml'),
            self.getFixture('threading.xml'),
        ]
        self.collector.collect()

        expected_metrics = {
            'kafka.logs.mytopic-1.CurrentOffset': 213500615,
            'kafka.logs.mytopic-1.NumAppendedMessages': 224634137,
            'kafka.logs.mytopic-1.NumberOfSegments': 94,
            'kafka.logs.mytopic-1.Size': 50143615339,
            'jvm.gc.marksweep.CurrentThreadCpuTime': 0,
            'jvm.gc.marksweep.CurrentThreadUserTime': 0,
            'jvm.gc.marksweep.DaemonThreadCount': 58,
            'jvm.gc.marksweep.PeakThreadCount': 90,
            'jvm.gc.marksweep.ThreadCount': 89,
            'jvm.gc.marksweep.TotalStartedThreadCount': 228,
            'jvm.gc.scavenge.CollectionCount': 2,
            'jvm.gc.scavenge.CollectionTime': 160,
            'jvm.threading.CollectionCount': 37577,
            'jvm.threading.CollectionTime': 112293,
        }

        self.assertPublishedMany(publish_mock, expected_metrics)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ksm
# coding=utf-8

"""
This class collects 'Kernel Samepage Merging' statistics.
KSM is a memory de-duplication feature of the Linux Kernel (2.6.32+).

It can be enabled, if compiled into your kernel, by echoing 1 to
/sys/kernel/mm/ksm/run. You can find more information about KSM at
[http://www.linux-kvm.org/page/KSM](http://www.linux-kvm.org/page/KSM).

#### Dependencies

 * KSM built into your kernel. It does not have to be enabled, but the stats
 will be less than useful if it isn't:-)

"""

import os
import glob
import diamond.collector


class KSMCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(KSMCollector, self).get_default_config_help()
        config_help.update({
            'ksm_path': "location where KSM kernel data can be found",
        })
        return config_help

    def get_default_config(self):
        """
        Return default config.

        path: Graphite path output
        ksm_path: location where KSM kernel data can be found
        """
        config = super(KSMCollector, self).get_default_config()
        config.update({
            'path': 'ksm',
            'ksm_path': '/sys/kernel/mm/ksm'})
        return config

    def collect(self):
        for item in glob.glob(os.path.join(self.config['ksm_path'], "*")):
            if os.access(item, os.R_OK):
                filehandle = open(item)
                try:
                    self.publish(os.path.basename(item),
                                 float(filehandle.readline().rstrip()))
                except ValueError:
                    pass
                filehandle.close()

########NEW FILE########
__FILENAME__ = testksm
#!/usr/bin/python
# coding=utf-8
################################################################################

import os

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from ksm import KSMCollector

################################################################################


class TestKSMCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('KSMCollector', {
            'interval': 10,
            'ksm_path': os.path.dirname(__file__) + '/fixtures/'
        })

        self.collector = KSMCollector(config, None)

    def test_import(self):
        self.assertTrue(KSMCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        self.collector.collect()

        metrics = {
            'full_scans': 123.0,
            'pages_shared': 124.0,
            'pages_sharing': 125.0,
            'pages_to_scan': 100.0,
            'pages_unshared': 126.0,
            'pages_volatile': 127.0,
            'run': 1.0,
            'sleep_millisecs': 20.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = kvm
# coding=utf-8

"""
Collects /sys/kernel/debug/kvm/*

#### Dependencies

 * /sys/kernel/debug/kvm

"""

import diamond.collector
import os


class KVMCollector(diamond.collector.Collector):

    PROC = '/sys/kernel/debug/kvm'

    def get_default_config_help(self):
        config_help = super(KVMCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(KVMCollector, self).get_default_config()
        config.update({
            'path': 'kvm',
        })
        return config

    def collect(self):
        if not os.path.isdir(self.PROC):
            self.log.error('/sys/kernel/debug/kvm is missing. Did you "mount -t'
                           + ' debugfs debugfs /sys/kernel/debug"?')
            return {}

        for filename in os.listdir(self.PROC):
            filepath = os.path.abspath(os.path.join(self.PROC, filename))
            fh = open(filepath, 'r')
            metric_value = self.derivative(filename,
                                           float(fh.readline()),
                                           4294967295)
            self.publish(filename, metric_value)

########NEW FILE########
__FILENAME__ = testkvm
#!/usr/bin/python
# coding=utf-8
################################################################################

import os

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from kvm import KVMCollector

################################################################################


class TestKVMCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('KVMCollector', {
            'interval': 10,
        })

        self.collector = KVMCollector(config, None)
        self.collector.PROC = os.path.dirname(__file__) + '/fixtures/'

    def test_import(self):
        self.assertTrue(KVMCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        patch_open = patch('__builtin__.open', Mock(return_value=StringIO(
            '0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0'
            + '\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n'
        )))

        patch_open.start()
        self.collector.collect()
        patch_open.stop()

        self.assertPublishedMany(publish_mock, {})

        self.collector.collect()

        metrics = {
            'efer_reload': 0.000000,
            'exits': 1436135848.000000,
            'fpu_reload': 121764903.500000,
            'halt_exits': 544586282.600000,
            'halt_wakeup': 235093451.400000,
            'host_state_reload': 801854250.600000,
            'hypercalls': 0.000000,
            'insn_emulation': 1314391264.700000,
            'insn_emulation_fail': 0.000000,
            'invlpg': 0.000000,
            'io_exits': 248822813.200000,
            'irq_exits': 701647108.400000,
            'irq_injections': 986654069.600000,
            'irq_window': 162240965.200000,
            'largepages': 351789.400000,
            'mmio_exits': 20169.400000,
            'mmu_cache_miss': 1643.300000,
            'mmu_flooded': 0.000000,
            'mmu_pde_zapped': 0.000000,
            'mmu_pte_updated': 0.000000,
            'mmu_pte_write': 11144.000000,
            'mmu_recycled': 0.000000,
            'mmu_shadow_zapped': 384.700000,
            'mmu_unsync': 0.000000,
            'nmi_injections': 0.000000,
            'nmi_window': 0.000000,
            'pf_fixed': 355636.100000,
            'pf_guest': 0.000000,
            'remote_tlb_flush': 111.200000,
            'request_irq': 0.000000,
            'signal_exits': 0.000000,
            'tlb_flush': 0.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = libvirtkvm
# coding=utf-8

"""
Uses libvirt to harvest per KVM instance stats

#### Dependencies

 * python-libvirt, xml

"""

import diamond.collector

try:
    from xml.etree import ElementTree
    ElementTree  # workaround for pyflakes issue #13
except ImportError:
    import cElementTree as ElementTree

try:
    import libvirt
    libvirt  # Pyflakes
except ImportError:
    libvirt = None


class LibvirtKVMCollector(diamond.collector.Collector):
    blockStats = {
        'read_reqs':   0,
        'read_bytes':  1,
        'write_reqs':  2,
        'write_bytes': 3
        }

    vifStats = {
        'rx_bytes':   0,
        'rx_packets': 1,
        'rx_errors':  2,
        'rx_drops':   3,
        'tx_bytes':   4,
        'tx_packets': 5,
        'tx_errors':  6,
        'tx_drops':   7
        }

    def get_default_config_help(self):
        config_help = super(LibvirtKVMCollector, self).get_default_config_help()
        config_help.update({
            'uri': """The libvirt connection URI. By default it's
'qemu:///system'. One decent option is
'qemu+unix:///system?socket=/var/run/libvirt/libvit-sock-ro'.""",
            'sort_by_uuid': """Use the <uuid> of the instance instead of the
 default <name>, useful in Openstack deploments where <name> is only
specific to the compute node""",
            'cpu_absolute': """CPU stats reported as percentage by default, or
as cummulative nanoseconds since VM creation if this is True."""
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(LibvirtKVMCollector, self).get_default_config()
        config.update({
            'path':     'libvirt-kvm',
            'sort_by_uuid': False,
            'uri':      'qemu:///system',
            'cpu_absolute': False
        })
        return config

    def get_devices(self, dom, type):
        devices = []

        # Create a XML tree from the domain XML description.
        tree = ElementTree.fromstring(dom.XMLDesc(0))

        for target in tree.findall("devices/%s/target" % type):
            dev = target.get("dev")
            if not dev in devices:
                devices.append(dev)

        return devices

    def get_disk_devices(self, dom):
        return self.get_devices(dom, 'disk')

    def get_network_devices(self, dom):
        return self.get_devices(dom, 'interface')

    def report_cpu_metric(self, statname, value, instance):
        # Value in cummulative nanoseconds
        if self.config['cpu_absolute'] is True:
            metric = value
        else:
            # Nanoseconds (10^9), however, we want to express in 100%
            metric = self.derivative(statname, float(value) / 10000000.0,
                                     max_value=diamond.collector.MAX_COUNTER,
                                     instance=instance)
        self.publish(statname, metric, instance=instance)

    def collect(self):
        if libvirt is None:
            self.log.error('Unable to import libvirt')
            return {}

        conn = libvirt.openReadOnly(self.config['uri'])
        for dom in [conn.lookupByID(n) for n in conn.listDomainsID()]:
            if self.config['sort_by_uuid'] is True:
                name = dom.UUIDString()
            else:
                name = dom.name()

            # CPU stats
            vcpus = dom.getCPUStats(True, 0)
            totalcpu = 0
            idx = 0
            for vcpu in vcpus:
                cputime = vcpu['cpu_time']
                self.report_cpu_metric('cpu.%s.time' % idx, cputime, name)
                idx += 1
                totalcpu += cputime
            self.report_cpu_metric('cpu.total.time', totalcpu, name)

            # Disk stats
            disks = self.get_disk_devices(dom)
            accum = {}
            for stat in self.blockStats.keys():
                accum[stat] = 0

            for disk in disks:
                stats = dom.blockStats(disk)
                for stat in self.blockStats.keys():
                    idx = self.blockStats[stat]
                    val = stats[idx]
                    accum[stat] += val
                    self.publish('block.%s.%s' % (disk, stat), val,
                                 instance=name)
            for stat in self.blockStats.keys():
                self.publish('block.total.%s' % stat, accum[stat],
                             instance=name)

            # Network stats
            vifs = self.get_network_devices(dom)
            accum = {}
            for stat in self.vifStats.keys():
                accum[stat] = 0

            for vif in vifs:
                stats = dom.interfaceStats(vif)
                for stat in self.vifStats.keys():
                    idx = self.vifStats[stat]
                    val = stats[idx]
                    accum[stat] += val
                    self.publish('net.%s.%s' % (vif, stat), val,
                                 instance=name)
            for stat in self.vifStats.keys():
                self.publish('net.total.%s' % stat, accum[stat],
                             instance=name)

            # Memory stats
            mem = dom.memoryStats()
            self.publish('memory.nominal', mem['actual'] * 1024,
                         instance=name)
            self.publish('memory.rss', mem['rss'] * 1024, instance=name)

########NEW FILE########
__FILENAME__ = testlibvirtkvm
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from libvirtkvm import LibvirtKVMCollector


###############################################################################

class TestLibvirtKVMCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('LibvirtKVMCollector', {
        })
        self.collector = LibvirtKVMCollector(config, None)

    def test_import(self):
        self.assertTrue(LibvirtKVMCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = lmsensors
# coding=utf-8

"""
This class collects data from libsensors. It should work against libsensors 2.x
and 3.x, pending support within the PySensors Ctypes binding:
[http://pypi.python.org/pypi/PySensors/](http://pypi.python.org/pypi/PySensors/)

Requires: 'sensors' to be installed, configured, and the relevant kernel modules
to be loaded. Requires: PySensors requires Python 2.6+

If you're having issues, check your version of 'sensors'. This collector written
against: sensors version 3.1.2 with libsensors version 3.1.2

#### Dependencies

 * [PySensors](http://pypi.python.org/pypi/PySensors/)

"""

import diamond.collector

try:
    import sensors
    sensors  # workaround for pyflakes issue #13
except ImportError:
    sensors = None


class LMSensorsCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(LMSensorsCollector, self).get_default_config_help()
        config_help.update({
            'fahrenheit': "True/False",
        })
        return config_help

    def get_default_config(self):
        """
        Returns default collector settings.
        """
        config = super(LMSensorsCollector, self).get_default_config()
        config.update({
            'path': 'sensors',
            'fahrenheit': 'True'
        })
        return config

    def collect(self):
        if sensors is None:
            self.log.error('Unable to import module sensors')
            return {}

        sensors.init()
        try:
            for chip in sensors.iter_detected_chips():
                for feature in chip:
                    self.publish(".".join([str(chip),
                                           feature.label.replace(' ', '-')]),
                                 feature.get_value())
        finally:
            sensors.cleanup()

########NEW FILE########
__FILENAME__ = testlmsensors
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from lmsensors import LMSensorsCollector


class TestLMSensorsCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('LMSensorsCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = LMSensorsCollector(config, None)

    def test_import(self):
        self.assertTrue(LMSensorsCollector)

########NEW FILE########
__FILENAME__ = loadavg
# coding=utf-8

"""
Uses /proc/loadavg to collect data on load average

#### Dependencies

 * /proc/loadavg

"""

import diamond.collector
import re
import os
from diamond.collector import str_to_bool


class LoadAverageCollector(diamond.collector.Collector):

    PROC_LOADAVG = '/proc/loadavg'
    PROC_LOADAVG_RE = re.compile(r'([\d.]+) ([\d.]+) ([\d.]+) (\d+)/(\d+)')

    def get_default_config_help(self):
        config_help = super(LoadAverageCollector,
                            self).get_default_config_help()
        config_help.update({
            'simple':   'Only collect the 1 minute load average'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(LoadAverageCollector, self).get_default_config()
        config.update({
            'enabled':  'True',
            'path':     'loadavg',
            'method':   'Threaded',
            'simple':   'False'
        })
        return config

    def collect(self):
        load01, load05, load15 = os.getloadavg()

        if not str_to_bool(self.config['simple']):
            self.publish_gauge('01', load01, 2)
            self.publish_gauge('05', load05, 2)
            self.publish_gauge('15', load15, 2)
        else:
            self.publish_gauge('load', load01, 2)

        # Legacy: add process/thread counters provided by
        # /proc/loadavg (if available).
        if os.access(self.PROC_LOADAVG, os.R_OK):
            file = open(self.PROC_LOADAVG)
            for line in file:
                match = self.PROC_LOADAVG_RE.match(line)
                if match:
                    self.publish_gauge('processes_running', int(match.group(4)))
                    self.publish_gauge('processes_total', int(match.group(5)))
            file.close()

########NEW FILE########
__FILENAME__ = testloadavg
#!/usr/bin/python
# coding=utf-8
################################################################################

import os
from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from loadavg import LoadAverageCollector

################################################################################


class TestLoadAverageCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('LoadAverageCollector', {
            'interval': 10
        })

        self.collector = LoadAverageCollector(config, None)

    def test_import(self):
        self.assertTrue(LoadAverageCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_loadavg(self, publish_mock, open_mock):
        if not os.path.exists('/proc/loadavg'):
            # on platforms that don't provide /proc/loadavg: don't bother
            # testing this.
            return
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/loadavg')

    @patch('os.getloadavg')
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock, getloadavg_mock):
        LoadAverageCollector.PROC_LOADAVG = self.getFixturePath('proc_loadavg')
        getloadavg_mock.return_value = (0.12, 0.23, 0.34)
        self.collector.collect()

        metrics = {
            '01': (0.12, 2),
            '05': (0.23, 2),
            '15': (0.34, 2),
            'processes_running': 1,
            'processes_total': 235
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = memcached
# coding=utf-8

"""
Collect memcached stats



#### Dependencies

 * subprocess

#### Example Configuration

MemcachedCollector.conf

```
    enabled = True
    hosts = localhost:11211, app-1@localhost:11212, app-2@localhost:11213, etc
```

TO use a unix socket, set a host string like this

```
    hosts = /path/to/blah.sock, app-1@/path/to/bleh.sock,
```
"""

import diamond.collector
import socket
import re


class MemcachedCollector(diamond.collector.Collector):
    GAUGES = [
        'bytes',
        'connection_structures',
        'curr_connections',
        'curr_items',
        'threads',
        'reserved_fds',
        'limit_maxbytes',
        'hash_power_level',
        'hash_bytes',
        'hash_is_expanding',
    ]

    def get_default_config_help(self):
        config_help = super(MemcachedCollector, self).get_default_config_help()
        config_help.update({
            'publish': "Which rows of 'status' you would like to publish."
            + " Telnet host port' and type stats and hit enter to see the list"
            + " of possibilities. Leave unset to publish all.",
            'hosts': "List of hosts, and ports to collect. Set an alias by "
            + " prefixing the host:port with alias@",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(MemcachedCollector, self).get_default_config()
        config.update({
            'path':     'memcached',

            # Which rows of 'status' you would like to publish.
            # 'telnet host port' and type stats and hit enter to see the list of
            # possibilities.
            # Leave unset to publish all
            #'publish': ''

            # Connection settings
            'hosts': ['localhost:11211']
        })
        return config

    def get_raw_stats(self, host, port):
        data = ''
        # connect
        try:
            if port is None:
                sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                sock.connect(host)
            else:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.connect((host, int(port)))
            # request stats
            sock.send('stats\n')
            # something big enough to get whatever is sent back
            data = sock.recv(4096)
        except socket.error:
            self.log.exception('Failed to get stats from %s:%s',
                               host, port)
        return data

    def get_stats(self, host, port):
        # stuff that's always ignored, aren't 'stats'
        ignored = ('libevent', 'pointer_size', 'time', 'version',
                   'repcached_version', 'replication', 'accepting_conns',
                   'pid')
        pid = None

        stats = {}
        data = self.get_raw_stats(host, port)

        # parse stats
        for line in data.splitlines():
            pieces = line.split(' ')
            if pieces[0] != 'STAT' or pieces[1] in ignored:
                continue
            elif pieces[1] == 'pid':
                pid = pieces[2]
                continue
            if '.' in pieces[2]:
                stats[pieces[1]] = float(pieces[2])
            else:
                stats[pieces[1]] = int(pieces[2])

        # get max connection limit
        self.log.debug('pid %s', pid)
        try:
            cmdline = "/proc/%s/cmdline" % pid
            f = open(cmdline, 'r')
            m = re.search("-c\x00(\d+)", f.readline())
            if m is not None:
                self.log.debug('limit connections %s', m.group(1))
                stats['limit_maxconn'] = m.group(1)
            f.close()
        except:
            self.log.debug("Cannot parse command line options for memcached")

        return stats

    def collect(self):
        hosts = self.config.get('hosts')

        # Convert a string config value to be an array
        if isinstance(hosts, basestring):
            hosts = [hosts]

        for host in hosts:
            matches = re.search('((.+)\@)?([^:]+)(:(\d+))?', host)
            alias = matches.group(2)
            hostname = matches.group(3)
            port = matches.group(5)

            if alias is None:
                alias = hostname

            stats = self.get_stats(hostname, port)

            # figure out what we're configured to get, defaulting to everything
            desired = self.config.get('publish', stats.keys())

            # for everything we want
            for stat in desired:
                if stat in stats:

                    # we have it
                    if stat in self.GAUGES:
                        self.publish_gauge(alias + "." + stat, stats[stat])
                    else:
                        self.publish_counter(alias + "." + stat, stats[stat])

                else:

                    # we don't, must be somehting configured in publish so we
                    # should log an error about it
                    self.log.error("No such key '%s' available, issue 'stats' "
                                   "for a full list", stat)

########NEW FILE########
__FILENAME__ = testmemcached
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from memcached import MemcachedCollector

################################################################################


class TestMemcachedCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MemcachedCollector', {
            'interval': 10,
            'hosts': ['localhost:11211'],
        })

        self.collector = MemcachedCollector(config, None)

    def test_import(self):
        self.assertTrue(MemcachedCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_raw_stats = patch.object(
            MemcachedCollector,
            'get_raw_stats',
            Mock(return_value=self.getFixture(
                'stats').getvalue()))

        patch_raw_stats.start()
        self.collector.collect()
        patch_raw_stats.stop()

        metrics = {
            'localhost.reclaimed': 0.000000,
            'localhost.expired_unfetched': 0.000000,
            'localhost.hash_is_expanding': 0.000000,
            'localhost.cas_hits': 0.000000,
            'localhost.uptime': 0,
            'localhost.touch_hits': 0.000000,
            'localhost.delete_misses': 0.000000,
            'localhost.listen_disabled_num': 0.000000,
            'localhost.cas_misses': 0.000000,
            'localhost.decr_hits': 0.000000,
            'localhost.cmd_touch': 0.000000,
            'localhost.incr_hits': 0.000000,
            'localhost.auth_cmds': 0.000000,
            'localhost.limit_maxbytes': 67108864.000000,
            'localhost.bytes_written': 0.000000,
            'localhost.incr_misses': 0.000000,
            'localhost.rusage_system': 0.195071,
            'localhost.total_items': 0.000000,
            'localhost.cmd_get': 0.000000,
            'localhost.curr_connections': 10.000000,
            'localhost.touch_misses': 0.000000,
            'localhost.threads': 4.000000,
            'localhost.total_connections': 0,
            'localhost.cmd_set': 0.000000,
            'localhost.curr_items': 0.000000,
            'localhost.conn_yields': 0.000000,
            'localhost.get_misses': 0.000000,
            'localhost.reserved_fds': 20.000000,
            'localhost.bytes_read': 0,
            'localhost.hash_bytes': 524288.000000,
            'localhost.evicted_unfetched': 0.000000,
            'localhost.cas_badval': 0.000000,
            'localhost.cmd_flush': 0.000000,
            'localhost.evictions': 0.000000,
            'localhost.bytes': 0.000000,
            'localhost.connection_structures': 11.000000,
            'localhost.hash_power_level': 16.000000,
            'localhost.auth_errors': 0.000000,
            'localhost.rusage_user': 0.231516,
            'localhost.delete_hits': 0.000000,
            'localhost.decr_misses': 0.000000,
            'localhost.get_hits': 0.000000,
            'localhost.repcached_qi_free': 0.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = memory
# coding=utf-8

"""
This class collects data on memory utilization

Note that MemFree may report no memory free. This may not actually be the case,
as memory is allocated to Buffers and Cache as well. See
[this link](http://www.linuxatemyram.com/) for more details.

#### Dependencies

* /proc/meminfo or psutil

"""

import diamond.collector
import diamond.convertor
import os

try:
    import psutil
    psutil  # workaround for pyflakes issue #13
except ImportError:
    psutil = None

_KEY_MAPPING = [
    'MemTotal',
    'MemFree',
    'Buffers',
    'Cached',
    'Active',
    'Dirty',
    'Inactive',
    'Shmem',
    'SwapTotal',
    'SwapFree',
    'SwapCached',
    'VmallocTotal',
    'VmallocUsed',
    'VmallocChunk'
]


class MemoryCollector(diamond.collector.Collector):

    PROC = '/proc/meminfo'

    def get_default_config_help(self):
        config_help = super(MemoryCollector, self).get_default_config_help()
        config_help.update({
            'detailed': 'Set to True to Collect all the nodes',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(MemoryCollector, self).get_default_config()
        config.update({
            'enabled':  'True',
            'path':     'memory',
            'method':   'Threaded',
            # Collect all the nodes or just a few standard ones?
            # Uncomment to enable
            #'detailed': 'True'
        })
        return config

    def collect(self):
        """
        Collect memory stats
        """
        if os.access(self.PROC, os.R_OK):
            file = open(self.PROC)
            data = file.read()
            file.close()

            for line in data.splitlines():
                try:
                    name, value, units = line.split()
                    name = name.rstrip(':')
                    value = int(value)

                    if (name not in _KEY_MAPPING
                            and 'detailed' not in self.config):
                        continue

                    for unit in self.config['byte_unit']:
                        value = diamond.convertor.binary.convert(value=value,
                                                                 oldUnit=units,
                                                                 newUnit=unit)
                        self.publish(name, value, metric_type='GAUGE')

                        # TODO: We only support one unit node here. Fix it!
                        break

                except ValueError:
                    continue
            return True
        else:
            if not psutil:
                self.log.error('Unable to import psutil')
                self.log.error('No memory metrics retrieved')
                return None

            phymem_usage = psutil.phymem_usage()
            virtmem_usage = psutil.virtmem_usage()
            units = 'B'

            for unit in self.config['byte_unit']:
                value = diamond.convertor.binary.convert(
                    value=phymem_usage.total, oldUnit=units, newUnit=unit)
                self.publish('MemTotal', value, metric_type='GAUGE')

                value = diamond.convertor.binary.convert(
                    value=phymem_usage.free, oldUnit=units, newUnit=unit)
                self.publish('MemFree', value, metric_type='GAUGE')

                value = diamond.convertor.binary.convert(
                    value=virtmem_usage.total, oldUnit=units, newUnit=unit)
                self.publish('SwapTotal', value, metric_type='GAUGE')

                value = diamond.convertor.binary.convert(
                    value=virtmem_usage.free, oldUnit=units, newUnit=unit)
                self.publish('SwapFree', value, metric_type='GAUGE')

                # TODO: We only support one unit node here. Fix it!
                break

            return True

        return None

########NEW FILE########
__FILENAME__ = testmemory
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from memory import MemoryCollector

################################################################################


class TestMemoryCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MemoryCollector', {
            'interval': 10,
            'byte_unit': 'kilobyte'
        })

        self.collector = MemoryCollector(config, None)

    def test_import(self):
        self.assertTrue(MemoryCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_meminfo(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/meminfo')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        MemoryCollector.PROC = self.getFixturePath('proc_meminfo')
        self.collector.collect()

        metrics = {
            'MemTotal': 49554212,
            'MemFree': 35194496,
            'Buffers': 1526304,
            'Cached': 10726736,
            'Active': 10022168,
            'Dirty': 24748,
            'Inactive': 2524928,
            'Shmem': 276,
            'SwapTotal': 262143996,
            'SwapFree': 262143996,
            'SwapCached': 0,
            'VmallocTotal': 34359738367,
            'VmallocUsed': 445452,
            'VmallocChunk': 34311049240
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = memory_cgroup
# coding=utf-8

"""
The MemoryCgroupCollector collects memory metric for cgroups

Stats that we are interested in tracking
cache - # of bytes of page cache memory.
rss   - # of bytes of anonymous and swap cache memory.
swap  - # of bytes of swap usage

#### Dependencies

/sys/fs/cgroup/memory/memory.stat
"""

import diamond.collector
import os

_KEY_MAPPING = [
    'cache',
    'rss',
    'swap'
]


class MemoryCgroupCollector(diamond.collector.Collector):
    MEMORY_PATH = '/sys/fs/cgroup/memory/'

    def get_default_config_help(self):
        config_help = super(
            MemoryCgroupCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(MemoryCgroupCollector, self).get_default_config()
        config.update({
            'path':     'memory_cgroup',
            'method':   'Threaded',
        })
        return config

    def collect(self):
        # find all memory.stat files
        matches = []
        for root, dirnames, filenames in os.walk(self.MEMORY_PATH):
            for filename in filenames:
                if filename == 'memory.stat':
                    # matches will contain a tuple contain path to cpuacct.stat
                    # and the parent of the stat
                    parent = root.replace(self.MEMORY_PATH,
                                          "").replace("/", ".")
                    if parent == '':
                        parent = 'system'
                    matches.append((parent, os.path.join(root, filename)))

        # Read metrics from cpuacct files
        results = {}
        for match in matches:
            results[match[0]] = {}
            stat_file = open(match[1])
            elements = [line.split() for line in stat_file]
            stat_file.close()

            for el in elements:
                name, value = el
                if name not in _KEY_MAPPING:
                    continue
                for unit in self.config['byte_unit']:
                    value = diamond.convertor.binary.convert(
                        value=value, oldUnit='B', newUnit=unit)
                    results[match[0]][name] = value
                    # TODO: We only support one unit node here. Fix it!
                    break

        # create metrics from collected utimes and stimes for cgroups
        for parent, cpuacct in results.iteritems():
            for key, value in cpuacct.iteritems():
                metric_name = '.'.join([parent, key])
                self.publish(metric_name, value, metric_type='GAUGE')
        return True

########NEW FILE########
__FILENAME__ = testmemory_cgroup
#!/usr/bin/python
# coding=utf-8
################################################################################
import os
from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from memory_cgroup import MemoryCgroupCollector

dirname = os.path.dirname(__file__)
fixtures_path = os.path.join(dirname, 'fixtures/')
fixtures = []
for root, dirnames, filenames in os.walk(fixtures_path):
    fixtures.append([root, dirnames, filenames])


class TestMemoryCgroupCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MemoryCgroupCollector', {
            'interval': 10,
            'byte_unit': 'megabyte'
        })

        self.collector = MemoryCgroupCollector(config, None)

    def test_import(self):
        self.assertTrue(MemoryCgroupCollector)

    @patch('__builtin__.open')
    @patch('os.walk', Mock(return_value=iter(fixtures)))
    @patch.object(Collector, 'publish')
    def test_should_open_all_cpuacct_stat(self, publish_mock, open_mock):
        open_mock.side_effect = lambda x: StringIO('')
        self.collector.collect()
        open_mock.assert_any_call(
            fixtures_path + 'lxc/testcontainer/memory.stat')
        open_mock.assert_any_call(fixtures_path + 'lxc/memory.stat')
        open_mock.assert_any_call(fixtures_path + 'memory.stat')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        MemoryCgroupCollector.MEMORY_PATH = fixtures_path
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {
            'lxc.testcontainer.cache': 1,
            'lxc.testcontainer.rss': 1,
            'lxc.testcontainer.swap': 1,
            'lxc.cache': 1,
            'lxc.rss': 1,
            'lxc.swap': 1,
            'system.cache': 1,
            'system.rss': 1,
            'system.swap': 1,
        })

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = memory_docker
# coding=utf-8

"""
The MemoryDockerCollector collects memory statistics from docker containers

#### Dependencies

 * docker

"""

try:
    import docker
except ImportError:
    docker = None

from memory_cgroup import MemoryCgroupCollector


class MemoryDockerCollector(MemoryCgroupCollector):
    def collect(self):
        if docker is None:
            self.log.error('Unable to import docker')
            return

        self.containers = dict(
            (c['Id'], c['Names'][0][1:])
            for c in docker.Client().containers(all=True)
            if c['Names'] is not None)
        return super(MemoryDockerCollector, self).collect()

    def publish(self, metric_name, value, metric_type):
        for container_id, container_name in self.containers.items():
            metric_name = metric_name.replace(
                'docker.'+container_id+'.', 'docker.'+container_name+'.')
        return super(MemoryDockerCollector, self).publish(
            metric_name, value, metric_type)

########NEW FILE########
__FILENAME__ = testmemory_docker
#!/usr/bin/python
# coding=utf-8
################################################################################
import os
from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

try:
    from docker import Client
    Client  # workaround for pyflakes issue #13
except ImportError:
    Client = None

from diamond.collector import Collector
from memory_docker import MemoryDockerCollector

dirname = os.path.dirname(__file__)
fixtures_path = os.path.join(dirname, 'fixtures/')
fixtures = []
for root, dirnames, filenames in os.walk(fixtures_path):
    fixtures.append([root, dirnames, filenames])

docker_fixture = [
    {u'Id': u'c3341726a9b4235a35b390c5f6f28e5a6869879a48da1d609db8f6bf4275bdc5',
     u'Names': [u'/testcontainer']},
    {u'Id': u'9c151939e20682b924d7299875e94a4aabbe946b30b407f89e276507432c625b',
     u'Names': None}]


def run_only_if_docker_client_is_available(func):
    try:
        from docker import Client
        Client  # workaround for pyflakes issue #13
    except ImportError:
        Client = None
    pred = lambda: Client is not None
    return run_only(func, pred)


class TestMemoryDockerCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MemoryDockerCollector', {
            'interval': 10,
            'byte_unit': 'megabyte'
        })

        self.collector = MemoryDockerCollector(config, None)

    def test_import(self):
        self.assertTrue(MemoryDockerCollector)

    @run_only_if_docker_client_is_available
    @patch('__builtin__.open')
    @patch('os.walk', Mock(return_value=iter(fixtures)))
    @patch.object(Client, 'containers', Mock(return_value=[]))
    @patch.object(Collector, 'publish')
    def test_should_open_all_cpuacct_stat(self, publish_mock, open_mock):
        open_mock.side_effect = lambda x: StringIO('')
        self.collector.collect()
        open_mock.assert_any_call(
            fixtures_path + 'lxc/testcontainer/memory.stat')
        open_mock.assert_any_call(fixtures_path + 'lxc/memory.stat')
        open_mock.assert_any_call(fixtures_path + 'memory.stat')

    @run_only_if_docker_client_is_available
    @patch('__builtin__.open')
    @patch('os.walk', Mock(return_value=iter(fixtures)))
    @patch.object(Client, 'containers')
    @patch.object(Collector, 'publish')
    def test_should_get_containers(self, publish_mock, containers_mock,
                                   open_mock):
        containers_mock.return_value = []
        open_mock.side_effect = lambda x: StringIO('')
        self.collector.collect()
        containers_mock.assert_any_call(all=True)

    @run_only_if_docker_client_is_available
    @patch.object(Collector, 'publish')
    @patch.object(Client, 'containers',
                  Mock(return_value=docker_fixture))
    def test_should_work_with_real_data(self, publish_mock):
        MemoryDockerCollector.MEMORY_PATH = fixtures_path
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {
            'lxc.testcontainer.cache': 1,
            'lxc.testcontainer.rss': 1,
            'lxc.testcontainer.swap': 1,
            'lxc.cache': 1,
            'lxc.rss': 1,
            'lxc.swap': 1,
            'system.cache': 1,
            'system.rss': 1,
            'system.swap': 1,
            'docker.testcontainer.cache': 1,
            'docker.testcontainer.rss': 1,
            'docker.testcontainer.swap': 1,
            'docker.cache': 1,
            'docker.rss': 1,
            'docker.swap': 1,
        })

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = memory_lxc
# coding=utf-8

"""
Collect Memory usage and limit of LXCs

#### Dependencies
 * cgroup, I guess

"""

from diamond.collector import Collector
import diamond.convertor
import os


class MemoryLxcCollector(Collector):

    def get_default_config_help(self):
        """
        Return help text for collector configuration.
        """
        config_help = super(MemoryLxcCollector, self).get_default_config_help()
        config_help.update({
            "sys_path": "Defaults to '/sys/fs/cgroup/lxc'",
            })
        return config_help

    def get_default_config(self):
        """
        Returns default settings for collector.
        """
        config = super(MemoryLxcCollector, self).get_default_config()
        config.update({
            "path":     "lxc",
            "sys_path": "/sys/fs/cgroup/lxc",
            })
        return config

    def collect(self):
        """
        Collect memory stats of LXCs.
        """
        lxc_metrics = ["memory.usage_in_bytes", "memory.limit_in_bytes"]
        if os.path.isdir(self.config["sys_path"]) is False:
            self.log.debug("sys_path '%s' isn't directory.",
                           self.config["sys_path"])
            return {}

        collected = {}
        for item in os.listdir(self.config["sys_path"]):
            fpath = "%s/%s" % (self.config["sys_path"], item)
            if os.path.isdir(fpath) is False:
                continue

            for lxc_metric in lxc_metrics:
                filename = "%s/%s" % (fpath, lxc_metric)
                metric_name = "%s.%s" % (
                    item.replace(".", "_"),
                    lxc_metric.replace("_in_bytes", ""))
                self.log.debug("Trying to collect from %s", filename)
                collected[metric_name] = self._read_file(filename)

        for key in collected.keys():
            if collected[key] is None:
                continue

            for unit in self.config["byte_unit"]:
                value = diamond.convertor.binary.convert(
                    collected[key],
                    oldUnit="B",
                    newUnit=unit)
                new_key = "%s_in_%ss" % (key, unit)
                self.log.debug("Publishing '%s %s'", new_key, value)
                self.publish(new_key, value, metric_type="GAUGE")

    def _read_file(self, filename):
        """
        Read contents of given file.
        """
        try:
            with open(filename, "r") as fhandle:
                stats = float(fhandle.readline().rstrip("\n"))
        except Exception as exception:
            stats = None

        return stats

########NEW FILE########
__FILENAME__ = mongodb
# coding=utf-8

"""
Collects all number values from the db.serverStatus() command, other
values are ignored.

#### Dependencies

 * pymongo

"""

import diamond.collector
from diamond.collector import str_to_bool
import re
import zlib

try:
    import pymongo
    pymongo  # workaround for pyflakes issue #13
except ImportError:
    pymongo = None

try:
    from pymongo import ReadPreference
    ReadPreference  # workaround for pyflakes issue #13
except ImportError:
    ReadPreference = None


class MongoDBCollector(diamond.collector.Collector):
    MAX_CRC32 = 4294967295

    def __init__(self, *args, **kwargs):
        self.__totals = {}
        super(MongoDBCollector, self).__init__(*args, **kwargs)

    def get_default_config_help(self):
        config_help = super(MongoDBCollector, self).get_default_config_help()
        config_help.update({
            'hosts': 'Array of hostname(:port) elements to get metrics from'
                     'Set an alias by prefixing host:port with alias@',
            'host': 'A single hostname(:port) to get metrics from'
                    ' (can be used instead of hosts and overrides it)',
            'user': 'Username for authenticated login (optional)',
            'passwd': 'Password for authenticated login (optional)',
            'databases': 'A regex of which databases to gather metrics for.'
                         ' Defaults to all databases.',
            'ignore_collections': 'A regex of which collections to ignore.'
                                  ' MapReduce temporary collections (tmp.mr.*)'
                                  ' are ignored by default.',
            'collection_sample_rate': 'Only send stats for a consistent subset '
            'of collections. This is applied after collections are ignored via'
            ' ignore_collections Sampling uses crc32 so it is consistent across'
            ' replicas. Value between 0 and 1. Default is 1',
            'network_timeout': 'Timeout for mongodb connection (in seconds).'
                               ' There is no timeout by default.',
            'simple': 'Only collect the same metrics as mongostat.',
            'translate_collections': 'Translate dot (.) to underscores (_)'
                                     ' in collection names.',
            'ssl': 'True to enable SSL connections to the MongoDB server.'
                    ' Default is False'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(MongoDBCollector, self).get_default_config()
        config.update({
            'path':      'mongo',
            'hosts':     ['localhost'],
            'user':      None,
            'passwd':      None,
            'databases': '.*',
            'ignore_collections': '^tmp\.mr\.',
            'network_timeout': None,
            'simple': 'False',
            'translate_collections': 'False',
            'collection_sample_rate': 1,
            'ssl': False
        })
        return config

    def collect(self):
        """Collect number values from db.serverStatus()"""

        if pymongo is None:
            self.log.error('Unable to import pymongo')
            return

        # we need this for backwards compatibility
        if 'host' in self.config:
            self.config['hosts'] = [self.config['host']]

        # convert network_timeout to integer
        if self.config['network_timeout']:
            self.config['network_timeout'] = int(
                self.config['network_timeout'])

        # convert collection_sample_rate to float
        if self.config['collection_sample_rate']:
            self.config['collection_sample_rate'] = float(
                self.config['collection_sample_rate'])

        # use auth if given
        if 'user' in self.config:
            user = self.config['user']
        else:
            user = None

        if 'passwd' in self.config:
            passwd = self.config['passwd']
        else:
            passwd = None

        for host in self.config['hosts']:
            if len(self.config['hosts']) == 1:
                # one host only, no need to have a prefix
                base_prefix = []
            else:
                matches = re.search('((.+)\@)?(.+)?', host)
                alias = matches.group(2)
                host = matches.group(3)

                if alias is None:
                    base_prefix = [re.sub('[:\.]', '_', host)]
                else:
                    base_prefix = [alias]

            try:
                # Ensure that the SSL option is a boolean.
                if type(self.config['ssl']) is str:
                    self.config['ssl'] = str_to_bool(self.config['ssl'])

                if ReadPreference is None:
                    conn = pymongo.Connection(
                        host,
                        network_timeout=self.config['network_timeout'],
                        ssl=self.config['ssl'],
                        slave_okay=True
                    )
                else:
                    conn = pymongo.Connection(
                        host,
                        network_timeout=self.config['network_timeout'],
                        ssl=self.config['ssl'],
                        read_preference=ReadPreference.SECONDARY,
                    )
            except Exception, e:
                self.log.error('Couldnt connect to mongodb: %s', e)
                continue

            # try auth
            if user:
                try:
                    conn.admin.authenticate(user, passwd)
                except Exception, e:
                    self.log.error('User auth given, but could not autheticate'
                                   + ' with host: %s, err: %s' % (host, e))
                    return{}

            data = conn.db.command('serverStatus')
            self._publish_transformed(data, base_prefix)
            if str_to_bool(self.config['simple']):
                data = self._extract_simple_data(data)

            self._publish_dict_with_prefix(data, base_prefix)
            db_name_filter = re.compile(self.config['databases'])
            ignored_collections = re.compile(self.config['ignore_collections'])
            sample_threshold = self.MAX_CRC32 * self.config[
                'collection_sample_rate']
            for db_name in conn.database_names():
                if not db_name_filter.search(db_name):
                    continue
                db_stats = conn[db_name].command('dbStats')
                db_prefix = base_prefix + ['databases', db_name]
                self._publish_dict_with_prefix(db_stats, db_prefix)
                for collection_name in conn[db_name].collection_names():
                    if ignored_collections.search(collection_name):
                        continue
                    if (self.config['collection_sample_rate'] < 1 and (
                            zlib.crc32(collection_name) & 0xffffffff
                            ) > sample_threshold):
                        continue

                    collection_stats = conn[db_name].command('collstats',
                                                             collection_name)
                    if str_to_bool(self.config['translate_collections']):
                        collection_name = collection_name.replace('.', '_')
                    collection_prefix = db_prefix + [collection_name]
                    self._publish_dict_with_prefix(collection_stats,
                                                   collection_prefix)

    def _publish_transformed(self, data, base_prefix):
        """ Publish values of type: counter or percent """
        self._publish_dict_with_prefix(data.get('opcounters', {}),
                                       base_prefix + ['opcounters_per_sec'],
                                       self.publish_counter)
        self._publish_dict_with_prefix(data.get('opcountersRepl', {}),
                                       base_prefix + ['opcountersRepl_per_sec'],
                                       self.publish_counter)
        self._publish_metrics(base_prefix + ['backgroundFlushing_per_sec'],
                              'flushes',
                              data.get('backgroundFlushing', {}),
                              self.publish_counter)
        self._publish_dict_with_prefix(data.get('network', {}),
                                       base_prefix + ['network_per_sec'],
                                       self.publish_counter)
        self._publish_metrics(base_prefix + ['extra_info_per_sec'],
                              'page_faults',
                              data.get('extra_info', {}),
                              self.publish_counter)

        def get_dotted_value(data, key_name):
            key_name = key_name.split('.')
            for i in key_name:
                data = data.get(i, {})
                if not data:
                    return 0
            return data

        def compute_interval(data, total_name):
            current_total = get_dotted_value(data, total_name)
            total_key = '.'.join(base_prefix + [total_name])
            last_total = self.__totals.get(total_key, current_total)
            interval = current_total - last_total
            self.__totals[total_key] = current_total
            return interval

        def publish_percent(value_name, total_name, data):
            value = float(get_dotted_value(data, value_name) * 100)
            interval = compute_interval(data, total_name)
            key = '.'.join(base_prefix + ['percent', value_name])
            self.publish_counter(key, value, time_delta=bool(interval),
                                 interval=interval)

        publish_percent('globalLock.lockTime', 'globalLock.totalTime', data)
        publish_percent('indexCounters.btree.misses',
                        'indexCounters.btree.accesses', data)

        locks = data.get('locks')
        if locks:
            if '.' in locks:
                locks['_global_'] = locks['.']
                del (locks['.'])
            key_prefix = '.'.join(base_prefix + ['percent'])
            db_name_filter = re.compile(self.config['databases'])
            interval = compute_interval(data, 'uptimeMillis')
            for db_name in locks:
                if not db_name_filter.search(db_name):
                    continue
                r = get_dotted_value(
                    locks,
                    '%s.timeLockedMicros.r' % db_name)
                R = get_dotted_value(
                    locks,
                    '.%s.timeLockedMicros.R' % db_name)
                value = float(r + R) / 10
                if value:
                    self.publish_counter(
                        key_prefix + '.locks.%s.read' % db_name,
                        value, time_delta=bool(interval),
                        interval=interval)
                w = get_dotted_value(
                    locks,
                    '%s.timeLockedMicros.w' % db_name)
                W = get_dotted_value(
                    locks,
                    '%s.timeLockedMicros.W' % db_name)
                value = float(w + W) / 10
                if value:
                    self.publish_counter(
                        key_prefix + '.locks.%s.write' % db_name,
                        value, time_delta=bool(interval), interval=interval)

    def _publish_dict_with_prefix(self, dict, prefix, publishfn=None):
        for key in dict:
            self._publish_metrics(prefix, key, dict, publishfn)

    def _publish_metrics(self, prev_keys, key, data, publishfn=None):
        """Recursively publish keys"""
        if not key in data:
            return
        value = data[key]
        keys = prev_keys + [key]
        if not publishfn:
            publishfn = self.publish
        if isinstance(value, dict):
            for new_key in value:
                self._publish_metrics(keys, new_key, value)
        elif isinstance(value, int) or isinstance(value, float):
            publishfn('.'.join(keys), value)
        elif isinstance(value, long):
            publishfn('.'.join(keys), float(value))

    def _extract_simple_data(self, data):
        return {
            'connections': data.get('connections'),
            'globalLock': data.get('globalLock'),
            'indexCounters': data.get('indexCounters')
        }

########NEW FILE########
__FILENAME__ = testmongodb
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import MagicMock
from mock import patch
from mock import call

from diamond.collector import Collector
from mongodb import MongoDBCollector

################################################################################


def run_only_if_pymongo_is_available(func):
    try:
        import pymongo
        pymongo  # workaround for pyflakes issue #13
    except ImportError:
        pymongo = None
    pred = lambda: pymongo is not None
    return run_only(func, pred)


class TestMongoDBCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MongoDBCollector', {
            'host': 'localhost:27017',
            'databases': '^db',
        })
        self.collector = MongoDBCollector(config, None)
        self.connection = MagicMock()

    def test_import(self):
        self.assertTrue(MongoDBCollector)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_server_stats(self,
                                                         publish_mock,
                                                         connector_mock):
        data = {'more_keys': {'nested_key': 1}, 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_once_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'more_keys.nested_key': 1,
            'key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_db_stats(self,
                                                     publish_mock,
                                                     connector_mock):
        data = {'db_keys': {'db_nested_key': 1}, 'dbkey': 2, 'dbstring': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection['db1'].command.assert_called_once_with('dbStats')
        metrics = {
            'db_keys.db_nested_key': 1,
            'dbkey': 2
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_stats_with_long_type(self,
                                                 publish_mock,
                                                 connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_once_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'more_keys': 1,
            'key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_databases(self,
                                              publish_mock,
                                              connector_mock):
        self._annotate_connection(connector_mock, {})

        self.collector.collect()

        assert call('baddb') not in self.connection.__getitem__.call_args_list

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_collections(self,
                                                publish_mock,
                                                connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.connection['db1'].collection_names.return_value = ['collection1',
                                                                'tmp.mr.tmp1']
        self.connection['db1'].command.return_value = {'key': 2,
                                                       'string': 'str'}

        self.collector.collect()

        self.connection.db.command.assert_called_once_with('serverStatus')
        self.connection['db1'].collection_names.assert_called_once_with()
        self.connection['db1'].command.assert_any_call('dbStats')
        self.connection['db1'].command.assert_any_call('collstats',
                                                       'collection1')
        assert call('collstats', 'tmp.mr.tmp1') not in self.connection['db1'].command.call_args_list  # NOQA
        metrics = {
            'databases.db1.collection1.key': 2,
        }

        self.assertPublishedMany(publish_mock, metrics)

    def _annotate_connection(self, connector_mock, data):
        connector_mock.return_value = self.connection
        self.connection.db.command.return_value = data
        self.connection.database_names.return_value = ['db1', 'baddb']


class TestMongoMultiHostDBCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MongoDBCollector', {
            'hosts': ['localhost:27017', 'localhost:27057'],
            'databases': '^db',
        })
        self.collector = MongoDBCollector(config, None)
        self.connection = MagicMock()

    def test_import(self):
        self.assertTrue(MongoDBCollector)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_server_stats(self,
                                                         publish_mock,
                                                         connector_mock):
        data = {'more_keys': {'nested_key': 1}, 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'localhost_27017.more_keys.nested_key': 1,
            'localhost_27057.more_keys.nested_key': 1,
            'localhost_27017.key': 2,
            'localhost_27057.key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_db_stats(self,
                                                     publish_mock,
                                                     connector_mock):
        data = {'db_keys': {'db_nested_key': 1}, 'dbkey': 2, 'dbstring': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection['db1'].command.assert_called_with('dbStats')
        metrics = {
            'localhost_27017.db_keys.db_nested_key': 1,
            'localhost_27057.db_keys.db_nested_key': 1,
            'localhost_27017.dbkey': 2,
            'localhost_27057.dbkey': 2
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_stats_with_long_type(self,
                                                 publish_mock,
                                                 connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'localhost_27017.more_keys': 1,
            'localhost_27057.more_keys': 1,
            'localhost_27017.key': 2,
            'localhost_27057.key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_databases(self,
                                              publish_mock,
                                              connector_mock):
        self._annotate_connection(connector_mock, {})

        self.collector.collect()

        assert call('baddb') not in self.connection.__getitem__.call_args_list

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_collections(self,
                                                publish_mock,
                                                connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.connection['db1'].collection_names.return_value = ['collection1',
                                                                'tmp.mr.tmp1']
        self.connection['db1'].command.return_value = {'key': 2,
                                                       'string': 'str'}

        self.collector.collect()

        self.connection.db.command.assert_called_with('serverStatus')
        self.connection['db1'].collection_names.assert_called_with()
        self.connection['db1'].command.assert_any_call('dbStats')
        self.connection['db1'].command.assert_any_call('collstats',
                                                       'collection1')
        assert call('collstats', 'tmp.mr.tmp1') not in self.connection['db1'].command.call_args_list  # NOQA
        metrics = {
            'localhost_27017.databases.db1.collection1.key': 2,
            'localhost_27057.databases.db1.collection1.key': 2,
        }

        self.assertPublishedMany(publish_mock, metrics)

    def _annotate_connection(self, connector_mock, data):
        connector_mock.return_value = self.connection
        self.connection.db.command.return_value = data
        self.connection.database_names.return_value = ['db1', 'baddb']


################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = monit
# coding=utf-8

"""
Collect the monit stats and report on cpu/memory for monitored processes

#### Dependencies

 * monit serving up /_status

"""

import urllib2
import base64

from xml.dom.minidom import parseString

import diamond.collector


class MonitCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(MonitCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(MonitCollector, self).get_default_config()
        config.update({
            'host':         '127.0.0.1',
            'port':         2812,
            'user':         'monit',
            'passwd':       'monit',
            'path':         'monit',
            'byte_unit':    ['byte'],
        })
        return config

    def collect(self):
        url = 'http://%s:%i/_status?format=xml' % (self.config['host'],
                                                   int(self.config['port']))
        try:
            request = urllib2.Request(url)

            #
            # shouldn't need to check this
            base64string = base64.encodestring('%s:%s' % (
                self.config['user'], self.config['passwd'])).replace('\n', '')
            request.add_header("Authorization", "Basic %s" % base64string)
            response = urllib2.urlopen(request)
        except urllib2.HTTPError, err:
            self.log.error("%s: %s", err, url)
            return

        metrics = {}

        try:
            dom = parseString("".join(response.readlines()))
        except:
            self.log.error("Got an empty response from the monit server")
            return

        for service in dom.getElementsByTagName('service'):
            if int(service.getAttribute('type')) == 3:
                name = service.getElementsByTagName('name')[0].firstChild.data
                if (service.getElementsByTagName(
                    'status')[0].firstChild.data == '0'
                    and service.getElementsByTagName(
                        'monitor')[0].firstChild.data == '1'):
                    try:
                        cpu = service.getElementsByTagName(
                            'cpu')[0].getElementsByTagName(
                            'percent')[0].firstChild.data
                        mem = int(service.getElementsByTagName(
                            'memory')[0].getElementsByTagName(
                            'kilobyte')[0].firstChild.data)
                        uptime = service.getElementsByTagName(
                            'uptime')[0].firstChild.data

                        metrics["%s.cpu.percent" % name] = cpu
                        for unit in self.config['byte_unit']:
                            metrics["%s.memory.%s_usage" % (name, unit)] = (
                                diamond.convertor.binary.convert(
                                    value=mem,
                                    oldUnit='kilobyte',
                                    newUnit=unit))
                        metrics["%s.uptime" % name] = uptime
                    except:
                        pass

        for key in metrics:
            self.publish(key, metrics[key])

########NEW FILE########
__FILENAME__ = testmonit
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from monit import MonitCollector

################################################################################


class TestMonitCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MonitCollector',
                                      {'byte_unit': 'kilobyte', })

        self.collector = MonitCollector(config, None)

    def test_import(self):
        self.assertTrue(MonitCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('status.xml')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'app_thin_8101.cpu.percent': 0.9,
            'app_thin_8101.memory.kilobyte_usage': 216104,
            'app_thin_8102.cpu.percent': 1.1,
            'app_thin_8102.memory.kilobyte_usage': 212736,
            'app_thin_8103.cpu.percent': 0.9,
            'app_thin_8103.memory.kilobyte_usage': 204948,
            'app_thin_8104.cpu.percent': 0.9,
            'app_thin_8104.memory.kilobyte_usage': 212464,
            'sshd.cpu.percent': 0.0,
            'sshd.memory.kilobyte_usage': 2588,
            'rsyslogd.cpu.percent': 0.0,
            'rsyslogd.memory.kilobyte_usage': 2664,
            'postfix.cpu.percent': 0.0,
            'postfix.memory.kilobyte_usage': 2304,
            'nginx.cpu.percent': 0.0,
            'nginx.memory.kilobyte_usage': 18684,
            'haproxy.cpu.percent': 0.0,
            'haproxy.memory.kilobyte_usage': 4040,
            'cron.cpu.percent': 0.0,
            'cron.memory.kilobyte_usage': 1036,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch(
            'urllib2.urlopen',
            Mock(
                return_value=self.getFixture(
                    'status_blank.xml')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = mountstats
# coding=utf-8
"""
The function of MountStatsCollector is to parse the detailed per-mount NFS
performance statistics provided by `/proc/self/mountstats` (reads, writes,
remote procedure call count/latency, etc.) and provide counters to Diamond.
Filesystems may be excluded using a regular expression filter, like the
existing disk check collectors.

#### Dependencies

 * /proc/self/mountstats

"""

import os
import re

import diamond.collector


class MountStatsCollector(diamond.collector.Collector):
    """Diamond collector for statistics from /proc/self/mountstats
    """

    BYTES_MAP = ['normalreadbytes', 'normalwritebytes', 'directreadbytes',
                 'directwritebytes', 'serverreadbytes', 'serverwritebytes']
    EVENTS_MAP = ['inoderevalidates', 'dentryrevalidates',
                  'datainvalidates', 'attrinvalidates', 'syncinodes',
                  'vfsopen', 'vfslookup', 'vfspermission', 'vfsreadpage',
                  'vfsreadpages', 'vfswritepage', 'vfswritepages',
                  'vfsreaddir', 'vfsflush', 'vfsfsync', 'vfsflock',
                  'vfsrelease', 'setattrtrunc', 'extendwrite',
                  'sillyrenames', 'shortreads', 'shortwrites', 'delay']
    XPRT_MAP = {'rdma': ['port', 'bind_count', 'connect_count',
                         'connect_time', 'idle_time', 'rpcsends',
                         'rpcreceives', 'badxids', 'backlogutil',
                         'read_chunks', 'write_chunks', 'reply_chunks',
                         'total_rdma_req', 'total_dma_rep', 'pullup',
                         'fixup', 'hardway', 'failed_marshal', 'bad_reply'],
                'tcp': ['port', 'bind_count', 'connect_count',
                        'connect_time', 'idle_time', 'rpcsends',
                        'rpcreceives', 'badxids', 'backlogutil'],
                'udp': ['port', 'bind_count', 'rpcsends', 'rpcreceives',
                        'badxids', 'backlogutil']}

    RPCS_MAP = ['ACCESS', 'CLOSE', 'COMMIT', 'CREATE', 'DELEGRETURN',
                'FSINFO', 'FSSTAT', 'FS_LOCATIONS', 'GETACL', 'GETATTR',
                'LINK', 'LOCK', 'LOCKT', 'LOCKU', 'LOOKUP', 'LOOKUP_ROOT',
                'MKDIR', 'MKNOD', 'NULL', 'OPEN', 'OPEN_CONFIRM',
                'OPEN_DOWNGRADE', 'OPEN_NOATTR', 'PATHCONF', 'READ',
                'READDIR', 'READDIRPLUS', 'READLINK', 'REMOVE', 'RENAME',
                'RENEW', 'RMDIR', 'SERVER_CAPS', 'SETACL', 'SETATTR',
                'SETCLIENTID', 'SETCLIENTID_CONFIRM', 'STATFS', 'SYMLINK',
                'WRITE']

    MOUNTSTATS = '/proc/self/mountstats'

    def __init__(self, config, handlers):
        super(MountStatsCollector, self).__init__(config, handlers)

        self.exclude_filters = self.config['exclude_filters']
        if isinstance(self.exclude_filters, basestring):
            self.exclude_filters = [self.exclude_filters]

        if len(self.exclude_filters) > 0:
            self.exclude_reg = re.compile('|'.join(self.exclude_filters))
        else:
            self.exclude_reg = None

    def get_default_config_help(self):
        config_help = super(MountStatsCollector,
                            self).get_default_config_help()
        config_help.update({
            'exclude_filters': "A list of regex patterns. Any filesystem"
            + " matching any of these patterns will be excluded from"
            + " mount stats metrics collection."
        })
        return config_help

    def get_default_config(self):
        config = super(MountStatsCollector, self).get_default_config()
        config.update({
            'enabled': 'False',
            'exclude_filters': [],
            'path': 'mountstats',
            'method': 'Threaded'
        })
        return config

    def collect(self):
        """Collect statistics from /proc/self/mountstats.

        Currently, we do fairly naive parsing and do not actually check
        the statvers value returned by mountstats.
        """

        if not os.access(self.MOUNTSTATS, os.R_OK):
            self.log.error("Cannot read path %s" % self.MOUNTSTATS)
            return None

        path = None
        f = open(self.MOUNTSTATS)
        for line in f:
            tokens = line.split()
            if len(tokens) == 0:
                continue

            if tokens[0] == 'device':
                path = tokens[4]

                if self.exclude_reg and self.exclude_reg.match(path):
                    self.log.debug("Ignoring %s since it is in the "
                                   + "exclude_filter list.", path)
                    skip = True
                else:
                    skip = False

                path = path.replace('.', '_')
                path = path.replace('/', '_')
            elif skip:
                # If we are in a skip state, don't pay any attention to
                # anything that isn't the next device line
                continue
            elif tokens[0] == 'events:':
                for i in range(0, len(self.EVENTS_MAP)):
                    metric_name = "%s.events.%s" % (path, self.EVENTS_MAP[i])
                    metric_value = long(tokens[i + 1])
                    self.publish_counter(metric_name, metric_value)
            elif tokens[0] == 'bytes:':
                for i in range(0, len(self.BYTES_MAP)):
                    metric_name = "%s.bytes.%s" % (path, self.BYTES_MAP[i])
                    metric_value = long(tokens[i + 1])
                    self.publish_counter(metric_name, metric_value)
            elif tokens[0] == 'xprt:':
                proto = tokens[1]
                if not self.XPRT_MAP[proto]:
                    self.log.error("Unknown protocol %s", proto)
                    continue

                for i in range(0, len(self.XPRT_MAP[proto])):
                    metric_name = "%s.xprt.%s.%s" % (path, proto,
                                                     self.XPRT_MAP[proto][i])
                    metric_value = long(tokens[i + 2])
                    self.publish_counter(metric_name, metric_value)
            elif tokens[0][:-1] in self.RPCS_MAP:
                rpc = tokens[0][:-1]
                ops = long(tokens[1])
                rtt = long(tokens[7])
                exe = long(tokens[8])

                metric_fmt = "%s.rpc.%s.%s"
                ops_name = metric_fmt % (path, rpc.lower(), 'ops')
                rtt_name = metric_fmt % (path, rpc.lower(), 'rtt')
                exe_name = metric_fmt % (path, rpc.lower(), 'exe')

                self.publish_counter(ops_name, ops)
                self.publish_counter(rtt_name, rtt)
                self.publish_counter(exe_name, exe)

        f.close()

########NEW FILE########
__FILENAME__ = testmountstats
#!/usr/bin/python
# coding=utf-8

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import MagicMock, Mock
from mock import patch

from diamond.collector import Collector
from mountstats import MountStatsCollector


class TestMountStatsCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MountStatsCollector', {
            'exclude_filters': ['^/mnt/path2'],
            'interval': 1
        })

        self.collector = MountStatsCollector(config, None)

    def test_import(self):
        self.assertTrue(MountStatsCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_mountstats(self, publish_mock, open_mock):
        open_mock.return_value = MagicMock()
        self.collector.collect()
        open_mock.assert_called_once_with(self.collector.MOUNTSTATS)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        # Test the first and last metric of each type
        published_metrics = {
            '_mnt_path1.events.inoderevalidates': 27110.0,
            '_mnt_path1.events.delay': 0.0,
            '_mnt_path1.bytes.normalreadbytes': 1424269.0,
            '_mnt_path1.bytes.serverwritebytes': 69460.0,
            '_mnt_path1.xprt.tcp.port': 0.0,
            '_mnt_path1.xprt.tcp.backlogutil': 11896527.0,
            '_mnt_path1.rpc.access.ops': 2988.0,
            '_mnt_path1.rpc.write.ops': 16.0
        }

        unpublished_metrics = {
            '_mnt_path2.events.delay': 0.0
        }

        self.collector.MOUNTSTATS = self.getFixturePath('mountstats_1')
        self.collector.collect()
        self.assertPublishedMany(publish_mock, {})

        self.collector.MOUNTSTATS = self.getFixturePath('mountstats_2')
        self.collector.collect()
        self.assertPublishedMany(publish_mock, published_metrics)
        self.assertUnpublishedMany(publish_mock, unpublished_metrics)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = mysql
# coding=utf-8

"""

#### Grants

 * Normal usage
```
GRANT REPLICATION CLIENT on *.* TO 'user'@'hostname' IDENTIFIED BY
'password';
```

 * For innodb engine status
```
GRANT SUPER ON *.* TO 'user'@'hostname' IDENTIFIED BY
'password';
```

 * For innodb engine status on MySQL versions 5.1.24+
```
GRANT PROCESS ON *.* TO 'user'@'hostname' IDENTIFIED BY
'password';
```

#### Dependencies

 * MySQLdb

"""

import diamond.collector
from diamond.collector import str_to_bool
import re
import time

try:
    import MySQLdb
    from MySQLdb import MySQLError
    MySQLdb  # workaround for pyflakes issue #13
    MySQLError  # workaround for pyflakes issue #13
except ImportError:
    MySQLdb = None
    MySQLError = ValueError


class MySQLCollector(diamond.collector.Collector):

    _GAUGE_KEYS = [
        'Innodb_buffer_pool_pages_data', 'Innodb_buffer_pool_pages_dirty',
        'Innodb_buffer_pool_pages_free',
        'Innodb_buffer_pool_pages_misc', 'Innodb_buffer_pool_pages_total',
        'Innodb_data_pending_fsyncs', 'Innodb_data_pending_reads',
        'Innodb_data_pending_writes',
        'Innodb_os_log_pending_fsyncs', 'Innodb_os_log_pending_writes',
        'Innodb_page_size',
        'Innodb_row_lock_current_waits', 'Innodb_row_lock_time',
        'Innodb_row_lock_time_avg',
        'Innodb_row_lock_time_max',
        'Key_blocks_unused', 'Last_query_cost', 'Max_used_connections',
        'Open_files', 'Open_streams', 'Open_table_definitions', 'Open_tables',
        'Qcache_free_blocks', 'Qcache_free_memory',
        'Qcache_queries_in_cache', 'Qcache_total_blocks',
        'Seconds_Behind_Master',
        'Threads_cached', 'Threads_connected', 'Threads_created',
        'Threads_running',
        # innodb status non counter keys
        'Innodb_bp_created_per_sec',
        'Innodb_bp_pages_evicted_no_access_per_sec',
        'Innodb_bp_pages_not_young_per_sec',
        'Innodb_bp_pages_read_ahead_per_sec', 'Innodb_bp_pages_young_per_sec',
        'Innodb_bp_reads_per_sec', 'Innodb_bp_written_per_sec',
        'Innodb_bp_add_alloc', 'Innodb_bp_db_pages',
        'Innodb_bp_dictionary_alloc', 'Innodb_bp_free_buffers',
        'Innodb_bp_hit_rate', 'Innodb_bp_io_cur_pages',
        'Innodb_bp_io_sum_pages', 'Innodb_bp_io_unzip_cur_pages',
        'Innodb_bp_io_unzip_sum_pages', 'Innodb_bp_lru_len',
        'Innodb_bp_modified_pages', 'Innodb_bp_not_young_hit_rate',
        'Innodb_bp_old_db_pages', 'Innodb_bp_pending_pages',
        'Innodb_bp_pending_writes_flush_list', 'Innodb_bp_pending_writes_lru',
        'Innodb_bp_pending_writes_single_page', 'Innodb_bp_size',
        'Innodb_bp_total_alloc', 'Innodb_bp_unzip_lru_len',
        'Innodb_bp_young_hit_rate',
        'Innodb_hash_searches_per_sec',
        'Innodb_io_syncs_per_sec',
        'Innodb_log_io_per_sec',
        'Innodb_non_hash_searches_per_sec',
        'Innodb_per_sec_avg',
        'Innodb_reads_per_sec',
        'Innodb_rows_deleted_per_sec', 'Innodb_rows_inserted_per_sec',
        'Innodb_rows_read_per_sec', 'Innodb_rows_updated_per_sec',
        'Innodb_sem_spins_per_wait_mutex', 'Innodb_sem_spins_per_wait_rw_excl',
        'Innodb_sem_spins_per_wait_rw_shared',
        'Innodb_writes_per_sec',
        'Innodb_bytes_per_read',
        'Innodb_hash_node_heap', 'Innodb_hash_table_size',
        'Innodb_hash_used_cells',
        'Innodb_ibuf_free_list_len', 'Innodb_ibuf_seg_size', 'Innodb_ibuf_size',
        'Innodb_io_ibuf_logs', 'Innodb_io_ibuf_reads', 'Innodb_io_ibuf_syncs',
        'Innodb_io_pending_flush_bp', 'Innodb_io_pending_flush_log',
        'Innodb_io_pending_reads', 'Innodb_io_pending_writes', '',
        'Innodb_log_pending_checkpoint_writes', 'Innodb_log_pending_log_writes',
        'Innodb_row_queries_inside', 'Innodb_row_queries_queue',
        'Innodb_trx_history_list_length', 'Innodb_trx_total_lock_structs',
        'Innodb_status_process_time', ]
    _IGNORE_KEYS = [
        'Master_Port', 'Master_Server_Id',
        'Last_Errno', 'Last_IO_Errno', 'Last_SQL_Errno', ]

    innodb_status_keys = {
        'Innodb_bp_total_alloc,Innodb_bp_add_alloc':
        'Total memory allocated (\d+)\; in additional pool allocated (\d+)',
        'Innodb_bp_reads_per_sec,Innodb_bp_created_per_sec,'
        + 'Innodb_bp_written_per_sec':
        '(^\d+.\d+) reads/s, (\d+.\d+) creates/s, (\d+.\d+) writes/s',
        'Innodb_io_ibuf_reads,Innodb_io_ibuf_logs,Innodb_io_ibuf_syncs':
        ' ibuf aio reads: (\d+), log i/o\'s: (\d+), sync i/o\'s: (\d+)',
        'Innodb_log_pending_log_writes,Innodb_log_pending_checkpoint_writes':
        '(\d+) pending log writes, (\d+) pending chkp writes',
        'Innodb_hash_searches_per_sec,Innodb_non_hash_searches_per_sec':
        '(\d+.\d+) hash searches/s, (\d+.\d+) non-hash searches/s',
        'Innodb_row_queries_inside,Innodb_row_queries_queue':
        '(\d+) queries inside InnoDB, (\d+) queries in queue',
        'Innodb_trx_total_lock_structs':
        '(\d+) lock struct\(s\), heap size (\d+), (\d+) row lock\(s\), undo'
        + ' log entries (\d+)',
        'Innodb_log_io_total,Innodb_log_io_per_sec':
        '(\d+) log i\/o\'s done, (\d+.\d+) log i\/o\'s\/second',
        'Innodb_io_os_file_reads,Innodb_io_os_file_writes,'
        + 'Innodb_io_os_file_fsyncs':
        '(\d+) OS file reads, (\d+) OS file writes, (\d+) OS fsyncs',
        'Innodb_rows_inserted_per_sec,Innodb_rows_updated_per_sec,'
        + 'Innodb_rows_deleted_per_sec,Innodb_rows_read_per_sec':
        '(\d+.\d+) inserts\/s, (\d+.\d+) updates\/s, (\d+.\d+) deletes\/s, '
        + '(\d+.\d+) reads\/s',
        'Innodb_reads_per_sec,Innodb_bytes_per_read,Innodb_io_syncs_per_sec,'
        + 'Innodb_writes_per_sec':
        '(\d+.\d+) reads\/s, (\d+) avg bytes\/read, (\d+.\d+) writes\/s, '
        + '(\d+.\d+) fsyncs\/s',
        'Innodb_bp_pages_young_per_sec,Innodb_bp_pages_not_young_per_sec':
        '(\d+.\d+) youngs\/s, (\d+.\d+) non-youngs\/s',
        'Innodb_bp_hit_rate,Innodb_bp_young_hit_rate,'
        + 'Innodb_bp_not_young_hit_rate':
        'Buffer pool hit rate (\d+) \/ \d+, young-making rate (\d+) \/ \d+ '
        + 'not (\d+) \/ \d+',
        'Innodb_bp_size':
        'Buffer pool size   (\d+)',
        'Innodb_bp_db_pages':
        'Database pages     (\d+)',
        'Innodb_bp_dictionary_alloc':
        'Dictionary memory allocated (\d+)',
        'Innodb_bp_free_buffers':
        'Free buffers       (\d+)',
        'Innodb_hash_table_size,Innodb_hash_node_heap':
        'Hash table size (\d+), node heap has (\d+) buffer\(s\)',
        'Innodb_trx_history_list_length':
        'History list length (\d+)',
        'Innodb_bp_io_sum_pages,Innodb_bp_io_cur_pages,'
        + 'Innodb_bp_io_unzip_sum_pages,Innodb_bp_io_unzip_cur_pages':
        'I\/O sum\[(\d+)\]:cur\[(\d+)\], unzip sum\[(\d+)\]:cur\[(\d+)\]',
        'Innodb_ibuf_size,Innodb_ibuf_free_list_len,Innodb_ibuf_seg_size,'
        + 'Innodb_ibuf_merges':
        'Ibuf: size (\d+), free list len (\d+), seg size (\d+), (\d+) '
        + 'merges',
        'Innodb_bp_lru_len,Innodb_bp_unzip_lru_len':
        'LRU len: (\d+), unzip_LRU len: (\d+)',
        'Innodb_bp_modified_pages':
        'Modified db pages  (\d+)',
        'Innodb_sem_mutex_spin_waits,Innodb_sem_mutex_rounds,'
        + 'Innodb_sem_mutex_os_waits':
        'Mutex spin waits (\d+), rounds (\d+), OS waits (\d+)',
        'Innodb_rows_inserted,Innodb_rows_updated,Innodb_rows_deleted,'
        + 'Innodb_rows_read':
        'Number of rows inserted (\d+), updated (\d+), deleted (\d+), '
        + 'read (\d+)',
        'Innodb_bp_old_db_pages':
        'Old database pages (\d+)',
        'Innodb_sem_os_reservation_count,Innodb_sem_os_signal_count':
        'OS WAIT ARRAY INFO: reservation count (\d+), signal count (\d+)',
        'Innodb_bp_pages_young,Innodb_bp_pages_not_young':
        'Pages made young (\d+), not young (\d+)',
        'Innodb_bp_pages_read,Innodb_bp_pages_created,Innodb_bp_pages_written':
        'Pages read (\d+), created (\d+), written (\d+)',
        'Innodb_bp_pages_read_ahead_per_sec,'
        + 'Innodb_bp_pages_evicted_no_access_per_sec,'
        + 'Innodb_status_bp_pages_random_read_ahead':
        'Pages read ahead (\d+.\d+)/s, evicted without access (\d+.\d+)\/s,'
        + ' Random read ahead (\d+.\d+)/s',
        'Innodb_io_pending_flush_log,Innodb_io_pending_flush_bp':
        'Pending flushes \(fsync\) log: (\d+); buffer pool: (\d+)',
        'Innodb_io_pending_reads,Innodb_io_pending_writes':
        'Pending normal aio reads: (\d+) \[\d+, \d+, \d+, \d+\], aio '
        + 'writes: (\d+) \[\d+, \d+, \d+, \d+\]',
        'Innodb_bp_pending_writes_lru,Innodb_bp_pending_writes_flush_list,'
        + 'Innodb_bp_pending_writes_single_page':
        'Pending writes: LRU (\d+), flush list (\d+), single page (\d+)',
        'Innodb_per_sec_avg':
        'Per second averages calculated from the last (\d+) seconds',
        'Innodb_sem_rw_excl_spins,Innodb_sem_rw_excl_rounds,'
        + 'Innodb_sem_rw_excl_os_waits':
        'RW-excl spins (\d+), rounds (\d+), OS waits (\d+)',
        'Innodb_sem_shared_spins,Innodb_sem_shared_rounds,'
        + 'Innodb_sem_shared_os_waits':
        'RW-shared spins (\d+), rounds (\d+), OS waits (\d+)',
        'Innodb_sem_spins_per_wait_mutex,Innodb_sem_spins_per_wait_rw_shared,'
        + 'Innodb_sem_spins_per_wait_rw_excl':
        'Spin rounds per wait: (\d+.\d+) mutex, (\d+.\d+) RW-shared, '
        + '(\d+.\d+) RW-excl',
        'Innodb_main_thd_log_flush_writes':
        'srv_master_thread log flush and writes: (\d+)',
        'Innodb_main_thd_loops_one_sec,Innodb_main_thd_loops_sleeps,'
        + 'Innodb_main_thd_loops_ten_sec,Innodb_main_thd_loops_background,'
        + 'Innodb_main_thd_loops_flush':
        'srv_master_thread loops: (\d+) 1_second, (\d+) sleeps, (\d+) '
        + '10_second, (\d+) background, (\d+) flush',
        'Innodb_ibuf_inserts,Innodb_ibuf_merged_recs,Innodb_ibuf_merges':
        '(\d+) inserts, (\d+) merged recs, (\d+) merges',
    }
    innodb_status_match = {}

    def __init__(self, *args, **kwargs):
        super(MySQLCollector, self).__init__(*args, **kwargs)
        for key in self.innodb_status_keys:
            self.innodb_status_keys[key] = re.compile(
                self.innodb_status_keys[key])

        if self.config['hosts'].__class__.__name__ != 'list':
            self.config['hosts'] = [self.config['hosts']]

        # Move legacy config format to new format
        if 'host' in self.config:
            hoststr = "%s:%s@%s:%s/%s" % (
                self.config['user'],
                self.config['passwd'],
                self.config['host'],
                self.config['port'],
                self.config['db'],
            )
            self.config['hosts'].append(hoststr)

        # Normalize some config vars
        self.config['master'] = str_to_bool(self.config['master'])
        self.config['slave'] = str_to_bool(self.config['slave'])
        self.config['innodb'] = str_to_bool(self.config['innodb'])

        self.db = None

    def get_default_config_help(self):
        config_help = super(MySQLCollector, self).get_default_config_help()
        config_help.update({
            'publish': "Which rows of '[SHOW GLOBAL STATUS](http://dev.mysql."
                       + "com/doc/refman/5.1/en/show-status.html)' you would "
                       + "like to publish. Leave unset to publish all",
            'slave': 'Collect SHOW SLAVE STATUS',
            'master': 'Collect SHOW MASTER STATUS',
            'innodb': 'Collect SHOW ENGINE INNODB STATUS',
            'hosts': 'List of hosts to collect from. Format is '
            + 'yourusername:yourpassword@host:port/db[/nickname]'
            + 'use db "None" to avoid connecting to a particular db'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(MySQLCollector, self).get_default_config()
        config.update({
            'path':     'mysql',
            # Connection settings
            'hosts':    [],

            # Which rows of 'SHOW GLOBAL STATUS' you would like to publish.
            # http://dev.mysql.com/doc/refman/5.1/en/show-status.html
            # Leave unset to publish all
            #'publish': '',

            'slave':    False,
            'master':   False,
            'innodb':   False,
        })
        return config

    def get_db_stats(self, query):
        cursor = self.db.cursor(cursorclass=MySQLdb.cursors.DictCursor)

        try:
            cursor.execute(query)
            return cursor.fetchall()
        except MySQLError, e:
            self.log.error('MySQLCollector could not get db stats', e)
            return ()

    def connect(self, params):
        try:
            self.db = MySQLdb.connect(**params)
            self.log.debug('MySQLCollector: Connected to database.')
        except MySQLError, e:
            self.log.error('MySQLCollector couldnt connect to database %s', e)
            return False
        return True

    def disconnect(self):
        self.db.close()

    def get_db_global_status(self):
        return self.get_db_stats('SHOW GLOBAL STATUS')

    def get_db_master_status(self):
        return self.get_db_stats('SHOW MASTER STATUS')

    def get_db_slave_status(self):
        return self.get_db_stats('SHOW SLAVE STATUS')

    def get_db_innodb_status(self):
        return self.get_db_stats('SHOW ENGINE INNODB STATUS')

    def get_stats(self, params):
        metrics = {'status': {}}

        if not self.connect(params):
            return metrics

        rows = self.get_db_global_status()
        for row in rows:
            try:
                metrics['status'][row['Variable_name']] = float(row['Value'])
            except:
                pass

        if self.config['master']:
            metrics['master'] = {}
            try:
                rows = self.get_db_master_status()
                for row_master in rows:
                    for key, value in row_master.items():
                        if key in self._IGNORE_KEYS:
                            continue
                        try:
                            metrics['master'][key] = float(row_master[key])
                        except:
                            pass
            except:
                self.log.error('MySQLCollector: Couldnt get master status')
                pass

        if self.config['slave']:
            metrics['slave'] = {}
            try:
                rows = self.get_db_slave_status()
                for row_slave in rows:
                    for key, value in row_slave.items():
                        if key in self._IGNORE_KEYS:
                            continue
                        try:
                            metrics['slave'][key] = float(row_slave[key])
                        except:
                            pass
            except:
                self.log.error('MySQLCollector: Couldnt get slave status')
                pass

        if self.config['innodb']:
            metrics['innodb'] = {}
            innodb_status_timer = time.time()
            try:
                rows = self.get_db_innodb_status()

                innodb_status_output = rows[0]

                todo = self.innodb_status_keys.keys()
                for line in innodb_status_output['Status'].split('\n'):
                    for key in todo:
                        match = self.innodb_status_keys[key].match(line)
                        if match is not None:
                            todo.remove(key)
                            match_index = 1
                            for key_index in key.split(','):
                                try:
                                    value = float(match.group(match_index))
                                    # store value
                                    if key_index in metrics:
                                        self.log.debug("MySQLCollector: %s"
                                                       + " already defined, "
                                                       + " ignoring new value",
                                                       key_index)
                                    else:
                                        metrics['innodb'][key_index] = value
                                    match_index += 1
                                except IndexError:
                                    self.log.debug("MySQLCollector: Cannot find"
                                                   + " value in innodb status "
                                                   + "for %s", key_index)
                for key in todo:
                    self.log.debug("MySQLCollector: %s regexp not matched in"
                                   + " innodb status", key)
            except Exception, innodb_status_error:
                self.log.error('MySQLCollector: Couldnt get engine innodb'
                               + ' status, check user permissions: %s',
                               innodb_status_error)
            Innodb_status_process_time = time.time() - innodb_status_timer
            self.log.debug("MySQLCollector: innodb status process time: %f",
                           Innodb_status_process_time)
            subkey = "Innodb_status_process_time"
            metrics['innodb'][subkey] = Innodb_status_process_time

        self.disconnect()

        return metrics

    def _publish_stats(self, nickname, metrics):

        for key in metrics:
            for metric_name in metrics[key]:
                metric_value = metrics[key][metric_name]

                if type(metric_value) is not float:
                    continue

                if metric_name not in self._GAUGE_KEYS:
                    metric_value = self.derivative(nickname + metric_name,
                                                   metric_value)
                if key == 'status':
                    if ('publish' not in self.config
                            or metric_name in self.config['publish']):
                        self.publish(nickname + metric_name, metric_value)
                else:
                    self.publish(nickname + metric_name, metric_value)

    def collect(self):

        if MySQLdb is None:
            self.log.error('Unable to import MySQLdb')
            return False

        for host in self.config['hosts']:
            matches = re.search(
                '^([^:]*):([^@]*)@([^:]*):?([^/]*)/([^/]*)/?(.*)', host)

            if not matches:
                self.log.error(
                    'Connection string not in required format, skipping: %s',
                    host)
                continue

            params = {}

            params['host'] = matches.group(3)
            try:
                params['port'] = int(matches.group(4))
            except ValueError:
                params['port'] = 3306
            params['db'] = matches.group(5)
            params['user'] = matches.group(1)
            params['passwd'] = matches.group(2)

            nickname = matches.group(6)
            if len(nickname):
                nickname += '.'

            if params['db'] == 'None':
                del params['db']

            try:
                metrics = self.get_stats(params=params)
            except Exception, e:
                try:
                    self.disconnect()
                except MySQLdb.ProgrammingError:
                    pass
                self.log.error('Collection failed for %s %s', nickname, e)
                continue

            # Warn if publish contains an unknown variable
            if 'publish' in self.config and metrics['status']:
                    for k in self.config['publish'].split():
                        if k not in metrics['status']:
                            self.log.error("No such key '%s' available, issue"
                                           + " 'show global status' for a full"
                                           + " list", k)
            self._publish_stats(nickname, metrics)

########NEW FILE########
__FILENAME__ = mysql55
# coding=utf-8

"""

Diamond collector that monitors relevant MySQL performance_schema values
For now only monitors replication load

[Blog](http://bit.ly/PbSkbN) announcement.

[Sniplet](http://bit.ly/SHwYhT) to build example graph.

#### Dependencies

 * MySQLdb
 * MySQL 5.5.3+

"""

from __future__ import division

try:
    import MySQLdb
    from MySQLdb import MySQLError
    MySQLdb  # workaround for pyflakes issue #13
except ImportError:
    MySQLdb = None
import diamond
import time
import re


class MySQLPerfCollector(diamond.collector.Collector):

    def __init__(self, *args, **kwargs):
        super(MySQLPerfCollector, self).__init__(*args, **kwargs)
        self.db = None
        self.last_wait_count = {}
        self.last_wait_sum = {}
        self.last_timestamp = {}
        self.last_data = {}
        self.monitors = {
            'slave_sql': {
                'wait/synch/cond/sql/MYSQL_RELAY_LOG::update_cond':
                'wait_for_update',
                'wait/io/file/innodb/innodb_data_file':
                'innodb_data_file',
                'wait/io/file/innodb/innodb_log_file':
                'innodb_log_file',
                'wait/io/file/myisam/dfile':
                'myisam_dfile',
                'wait/io/file/myisam/kfile':
                'myisam_kfile',
                'wait/io/file/sql/binlog':
                'binlog',
                'wait/io/file/sql/relay_log_info':
                'relaylog_info',
                'wait/io/file/sql/relaylog':
                'relaylog',
                'wait/synch/mutex/innodb':
                'innodb_mutex',
                'wait/synch/mutex':
                'other_mutex',
                'wait/synch/rwlock':
                'rwlocks',
                'wait/io':
                'other_io',
            },
            'slave_io': {
                'wait/io/file/sql/relaylog_index':
                'relaylog_index',
                'wait/synch/mutex/sql/MYSQL_RELAY_LOG::LOCK_index':
                'relaylog_index_lock',
                'wait/synch/mutex/sql/Master_info::data_lock':
                'master_info_lock',
                'wait/synch/mutex/mysys/IO_CACHE::append_buffer_lock':
                'append_buffer_lock',
                'wait/synch/mutex/sql/LOG::LOCK_log':
                'log_lock',
                'wait/io/file/sql/master_info':
                'master_info',
                'wait/io/file/sql/relaylog':
                'relaylog',
                'wait/synch/mutex':
                'other_mutex',
                'wait/synch/rwlock':
                'rwlocks',
                'wait/io':
                'other_io',
            }
        }

        if self.config['hosts'].__class__.__name__ != 'list':
            self.config['hosts'] = [self.config['hosts']]

            # Move legacy config format to new format
        if 'host' in self.config:
            hoststr = "%s:%s@%s:%s/%s" % (
                self.config['user'],
                self.config['passwd'],
                self.config['host'],
                self.config['port'],
                self.config['db'],
            )
            self.config['hosts'].append(hoststr)

    def get_default_config_help(self):
        config_help = super(MySQLPerfCollector, self).get_default_config_help()
        config_help.update({
            'hosts': 'List of hosts to collect from. Format is '
            + 'yourusername:yourpassword@host:'
            + 'port/performance_schema[/nickname]',
            'slave': 'Collect Slave Replication Metrics',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(MySQLPerfCollector, self).get_default_config()
        config.update({
            'path':     'mysql',
            # Connection settings
            'hosts':    [],

            'slave':    'False',
        })
        return config

    def connect(self, params):
        if MySQLdb is None:
            self.log.error('Unable to import MySQLdb')
            return

        try:
            self.db = MySQLdb.connect(**params)
        except MySQLError, e:
            self.log.error('MySQLPerfCollector couldnt connect to database %s',
                           e)
            return {}
        self.log.debug('MySQLPerfCollector: Connected to database.')

    def query_list(self, query, params):
        cursor = self.db.cursor()
        cursor.execute(query, params)
        return list(cursor.fetchall())

    def slave_load(self, nickname, thread):
        data = self.query_list("""
            SELECT
                his.event_name,
                his.sum_timer_wait,
                his.count_star,
                cur.event_name,
                UNIX_TIMESTAMP(SYSDATE())
            FROM
                events_waits_summary_by_thread_by_event_name his
                JOIN threads thr USING (thread_id)
                JOIN events_waits_current cur USING (thread_id)
            WHERE
                name = %s
            ORDER BY
                his.event_name
            """, (thread,))

        wait_sum = sum([x[1] for x in data])
        wait_count = sum([x[2] for x in data])
        timestamp = int(time.time())

        if 0 in data and len(data[0]) > 5:
            cur_event_name, timestamp = data[0][3:]

        if thread not in self.last_wait_sum:
            # Avoid bogus data
            self.last_wait_sum[thread] = wait_sum
            self.last_wait_count[thread] = wait_count
            self.last_timestamp[thread] = timestamp
            self.last_data[thread] = data
            return

        wait_delta = wait_sum - self.last_wait_sum[thread]
        time_delta = (timestamp - self.last_timestamp[thread]) * 1000000000000

        if time_delta == 0:
            return

        # Summarize a few things
        thread_name = thread[thread.rfind('/') + 1:]
        data.append(['wait/synch/mutex/innodb',
                     sum([x[1] for x in data if x[0].startswith(
                         'wait/synch/mutex/innodb')])])
        data.append(['wait/synch/mutex',
                     sum([x[1] for x in data if x[0].startswith(
                         'wait/synch/mutex')
                         and x[0] not in self.monitors[thread_name]])
                     - data[-1][1]])
        data.append(['wait/synch/rwlock',
                     sum([x[1] for x in data if x[0].startswith(
                         'wait/synch/rwlock')])])
        data.append(['wait/io',
                     sum([x[1] for x in data if x[0].startswith(
                         'wait/io')
                         and x[0] not in self.monitors[thread_name]])])

        for d in zip(self.last_data[thread], data):
            if d[0][0] in self.monitors[thread_name]:
                self.publish(nickname + thread_name + '.'
                             + self.monitors[thread_name][d[0][0]],
                             (d[1][1] - d[0][1]) / time_delta * 100)

        # Also log what's unaccounted for. This is where Actual Work gets done
        self.publish(nickname + thread_name + '.other_work',
                     float(time_delta - wait_delta) / time_delta * 100)

        self.last_wait_sum[thread] = wait_sum
        self.last_wait_count[thread] = wait_count
        self.last_timestamp[thread] = timestamp
        self.last_data[thread] = data

    def collect(self):
        for host in self.config['hosts']:
            matches = re.search(
                '^([^:]*):([^@]*)@([^:]*):?([^/]*)/([^/]*)/?(.*)$', host)

            if not matches:
                continue

            params = {}

            params['host'] = matches.group(3)
            try:
                params['port'] = int(matches.group(4))
            except ValueError:
                params['port'] = 3306
            params['db'] = matches.group(5)
            params['user'] = matches.group(1)
            params['passwd'] = matches.group(2)

            nickname = matches.group(6)
            if len(nickname):
                nickname += '.'

            self.connect(params=params)
            if self.config['slave']:
                self.slave_load(nickname, 'thread/sql/slave_io')
                self.slave_load(nickname, 'thread/sql/slave_sql')
            self.db.close()

########NEW FILE########
__FILENAME__ = testmysql
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from mysql import MySQLCollector

################################################################################


def run_only_if_MySQLdb_is_available(func):
    try:
        import MySQLdb
        MySQLdb  # workaround for pyflakes issue #13
    except ImportError:
        MySQLdb = None
    pred = lambda: MySQLdb is not None
    return run_only(func, pred)


class TestMySQLCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MySQLCollector', {
            'slave':    'True',
            'master':   'True',
            'innodb':   'True',
            'hosts': ['root:@localhost:3306/mysql'],
            'interval': '1',
        })

        self.collector = MySQLCollector(config, None)

    def test_import(self):
        self.assertTrue(MySQLCollector)

    @run_only_if_MySQLdb_is_available
    @patch.object(MySQLCollector, 'connect', Mock(return_value=True))
    @patch.object(MySQLCollector, 'disconnect', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_real_data(self, publish_mock):

        p_global_status = patch.object(
            MySQLCollector,
            'get_db_global_status',
            Mock(return_value=self.getPickledResults(
                'mysql_get_db_global_status_1.pkl')))
        p_master_status = patch.object(
            MySQLCollector,
            'get_db_master_status',
            Mock(return_value=self.getPickledResults(
                'get_db_master_status_1.pkl')))
        p_slave_status = patch.object(
            MySQLCollector,
            'get_db_slave_status',
            Mock(return_value=self.getPickledResults(
                'get_db_slave_status_1.pkl')))
        p_innodb_status = patch.object(
            MySQLCollector,
            'get_db_innodb_status',
            Mock(return_value=[{}]))

        p_global_status.start()
        p_master_status.start()
        p_slave_status.start()
        p_innodb_status.start()
        self.collector.collect()
        p_global_status.stop()
        p_master_status.stop()
        p_slave_status.stop()
        p_innodb_status.stop()

        self.assertPublishedMany(publish_mock, {})

        p_global_status = patch.object(
            MySQLCollector,
            'get_db_global_status',
            Mock(return_value=self.getPickledResults(
                'mysql_get_db_global_status_2.pkl')))
        p_master_status = patch.object(
            MySQLCollector,
            'get_db_master_status',
            Mock(return_value=self.getPickledResults(
                'get_db_master_status_2.pkl')))
        p_slave_status = patch.object(
            MySQLCollector,
            'get_db_slave_status',
            Mock(return_value=self.getPickledResults(
                'get_db_slave_status_2.pkl')))
        p_innodb_status = patch.object(
            MySQLCollector,
            'get_db_innodb_status',
            Mock(return_value=[{}]))

        p_global_status.start()
        p_master_status.start()
        p_slave_status.start()
        p_innodb_status.start()
        self.collector.collect()
        p_global_status.stop()
        p_master_status.stop()
        p_slave_status.stop()
        p_innodb_status.stop()

        metrics = {}
        metrics.update(self.getPickledResults(
            'mysql_get_db_global_status_expected.pkl'))
        metrics.update(self.getPickledResults(
            'get_db_master_status_expected.pkl'))
        metrics.update(self.getPickledResults(
            'get_db_slave_status_expected.pkl'))

        self.assertPublishedMany(publish_mock, metrics)

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = testmysql55
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from mysql55 import MySQLPerfCollector


class TestMySQLPerfCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('MySQLPerfCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = MySQLPerfCollector(config, None)

    def test_import(self):
        self.assertTrue(MySQLPerfCollector)

########NEW FILE########
__FILENAME__ = nagios
# coding=utf-8

"""
Shells out to get nagios statistics, which may or may not require sudo access

#### Dependencies

 * /usr/sbin/nagios3stats

"""

import diamond.collector
import subprocess
import os
from diamond.collector import str_to_bool


class NagiosStatsCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(NagiosStatsCollector,
                            self).get_default_config_help()
        config_help.update({
            'bin': 'Path to nagios3stats binary',
            'vars': 'What vars to collect',
            'use_sudo': 'Use sudo?',
            'sudo_cmd': 'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(NagiosStatsCollector, self).get_default_config()
        config.update({
            'bin':              '/usr/sbin/nagios3stats',
            'vars':            ['AVGACTHSTLAT',
                                'AVGACTSVCLAT',
                                'AVGACTHSTEXT',
                                'AVGACTSVCEXT',
                                'NUMHSTUP',
                                'NUMHSTDOWN',
                                'NUMHSTUNR',
                                'NUMSVCOK',
                                'NUMSVCWARN',
                                'NUMSVCUNKN',
                                'NUMSVCCRIT',
                                'NUMHSTACTCHK5M',
                                'NUMHSTPSVCHK5M',
                                'NUMSVCACTCHK5M',
                                'NUMSVCPSVCHK5M',
                                'NUMACTHSTCHECKS5M',
                                'NUMOACTHSTCHECKS5M',
                                'NUMCACHEDHSTCHECKS5M',
                                'NUMSACTHSTCHECKS5M',
                                'NUMPARHSTCHECKS5M',
                                'NUMSERHSTCHECKS5M',
                                'NUMPSVHSTCHECKS5M',
                                'NUMACTSVCCHECKS5M',
                                'NUMOACTSVCCHECKS5M',
                                'NUMCACHEDSVCCHECKS5M',
                                'NUMSACTSVCCHECKS5M',
                                'NUMPSVSVCCHECKS5M'],
            'use_sudo':         True,
            'sudo_cmd':         '/usr/bin/sudo',
            'path':             'nagiosstats'
        })
        return config

    def collect(self):
        if (not os.access(self.config['bin'], os.X_OK)
            or (str_to_bool(self.config['use_sudo'])
                and not os.access(self.config['sudo_cmd'], os.X_OK))):
            return

        command = [self.config['bin'],
                   '--data', ",".join(self.config['vars']),
                   '--mrtg']

        if str_to_bool(self.config['use_sudo']):
            command.insert(0, self.config['sudo_cmd'])

        p = subprocess.Popen(command,
                             stdout=subprocess.PIPE).communicate()[0][:-1]

        for i, v in enumerate(p.split("\n")):
            metric_name = self.config['vars'][i]
            metric_value = int(v)
            self.publish(metric_name, metric_value)

########NEW FILE########
__FILENAME__ = testnagios
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from nagios import NagiosStatsCollector

################################################################################


class TestNagiosStatsCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('NagiosStatsCollector', {
            'interval': 10,
            'bin': 'true',
            'use_sudo': False
        })

        self.collector = NagiosStatsCollector(config, None)

    def test_import(self):
        self.assertTrue(NagiosStatsCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('nagiostat').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        metrics = {
            'AVGACTHSTLAT': 196,
            'AVGACTSVCLAT': 242,
            'AVGACTHSTEXT': 4037,
            'AVGACTSVCEXT': 340,
            'NUMHSTUP': 63,
            'NUMHSTDOWN': 0,
            'NUMHSTUNR': 0,
            'NUMSVCOK': 1409,
            'NUMSVCWARN': 3,
            'NUMSVCUNKN': 0,
            'NUMSVCCRIT': 7,
            'NUMHSTACTCHK5M': 56,
            'NUMHSTPSVCHK5M': 0,
            'NUMSVCACTCHK5M': 541,
            'NUMSVCPSVCHK5M': 0,
            'NUMACTHSTCHECKS5M': 56,
            'NUMOACTHSTCHECKS5M': 1,
            'NUMCACHEDHSTCHECKS5M': 1,
            'NUMSACTHSTCHECKS5M': 55,
            'NUMPARHSTCHECKS5M': 55,
            'NUMSERHSTCHECKS5M': 0,
            'NUMPSVHSTCHECKS5M': 0,
            'NUMACTSVCCHECKS5M': 1101,
            'NUMOACTSVCCHECKS5M': 0,
            'NUMCACHEDSVCCHECKS5M': 0,
            'NUMSACTSVCCHECKS5M': 1101,
            'NUMPSVSVCCHECKS5M': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = nagiosperfdata
# coding=utf-8

"""
The NagiosPerfdataCollector parses Nagios performance data in the
PNP4Nagios/Graphios/Metricinga key-value format.

#### Dependencies

 * Nagios configured to periodically dump performance data files in
   PNP4Nagios format

Configuring Nagios/Icinga
-------------------------
If you're already using Graphios, you're already set up to send metrics through
Metricinga, and you can skip to the next section! If not, read on.

### Modifying the daemon configuration

The default performance data output format used by Nagios and Icinga can't be
easily extended to contain new attributes, so we're going to replace it with
one that prints key-value pairs instead. This will allow us to add in whatever
kind of bookkeeping attributes we want! We need these to do things like override
the display name of a service with a metric name more meaningful to Graphite.

We'll need to edit one of the following files:

* **For Nagios:** /etc/nagios/nagios.cfg
* **For Icinga:** /etc/icinga/icinga.cfg

Make sure that the following configuration keys are set to something like the
values below:

    process_performance_data=1
    host_perfdata_file=/var/spool/nagios/host-perfdata
    host_perfdata_file_mode=a
    host_perfdata_file_processing_command=process-host-perfdata-file
    host_perfdata_file_processing_interval=60
    host_perfdata_file_template=DATATYPE::HOSTPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tHOSTPERFDATA::$HOSTPERFDATA$\tHOSTCHECKCOMMAND::$HOSTCHECKCOMMAND$\tHOSTSTATE::$HOSTSTATE$\tHOSTSTATETYPE::$HOSTSTATETYPE$\tGRAPHITEPREFIX::$_HOSTGRAPHITEPREFIX$\tGRAPHITEPOSTFIX::$_HOSTGRAPHITEPOSTFIX$  # NOQA
    service_perfdata_file=/var/spool/nagios/service-perfdata
    service_perfdata_file_mode=a
    service_perfdata_file_processing_command=process-service-perfdata-file
    service_perfdata_file_processing_interval=60
    service_perfdata_file_template=DATATYPE::SERVICEPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tSERVICEDESC::$SERVICEDESC$\tSERVICEPERFDATA::$SERVICEPERFDATA$\tSERVICECHECKCOMMAND::$SERVICECHECKCOMMAND$\tHOSTSTATE::$HOSTSTATE$\tHOSTSTATETYPE::$HOSTSTATETYPE$\tSERVICESTATE::$SERVICESTATE$\tSERVICESTATETYPE::$SERVICESTATETYPE$\tGRAPHITEPREFIX::$_SERVICEGRAPHITEPREFIX$\tGRAPHITEPOSTFIX::$_SERVICEGRAPHITEPOSTFIX$  # NOQA

Note that you most likely will wish to change $_SERVICEGRAPHITEPREFIX$,
$_HOSTGRAPHITEPREFIX$, $_SERVICEGRAPHITEPOSTFIX$, and $_HOSTGRAPHITEPOSTFIX$

### Configuring file rotation

Next, the rotation commands need to be configured so the performance data files
are periodically moved into the Metrnagios spool directory. Depending on your
system configuration, these commands may be located in
`/etc/nagios/objects/commands.d`:

    define command {
        command_name    process-host-perfdata-file
        command_line    /bin/mv /var/spool/nagios/host-perfdata /var/spool/diamond/host-perfdata.$TIMET$  # NOQA
    }

    define command {
        command_name    process-service-perfdata-file
        command_line    /bin/mv /var/spool/nagios/service-perfdata /var/spool/diamond/service-perfdata.$TIMET$  # NOQA
    }
"""

import os
import re

import diamond.collector


class NagiosPerfdataCollector(diamond.collector.Collector):
    """Diamond collector for Nagios performance data
    """

    GENERIC_FIELDS = ['DATATYPE', 'HOSTNAME', 'TIMET']
    HOST_FIELDS = ['HOSTPERFDATA']
    SERVICE_FIELDS = ['SERVICEDESC', 'SERVICEPERFDATA']
    TOKENIZER_RE = (
        r"([^\s]+|'[^']+')=([-.\d]+)(c|s|ms|us|B|KB|MB|GB|TB|%)?"
        + r"(?:;([-.\d]+))?(?:;([-.\d]+))?(?:;([-.\d]+))?(?:;([-.\d]+))?")

    def __init__(self, config, handlers):
        super(NagiosPerfdataCollector, self).__init__(config, handlers)

    def get_default_config_help(self):
        config_help = super(NagiosPerfdataCollector,
                            self).get_default_config_help()
        config_help.update({
            'perfdata_dir': 'The directory containing Nagios perfdata files'
        })
        return config_help

    def get_default_config(self):
        config = super(NagiosPerfdataCollector, self).get_default_config()
        config.update({
            'enabled': 'False',
            'path': 'nagiosperfdata',
            'perfdata_dir': '/var/spool/diamond/nagiosperfdata',
            'method': 'Threaded'
        })
        return config

    def collect(self):
        """Collect statistics from a Nagios perfdata directory.
        """
        perfdata_dir = self.config['perfdata_dir']

        try:
            filenames = os.listdir(perfdata_dir)
        except OSError:
            self.log.error("Cannot read directory `{dir}'".format(
                dir=perfdata_dir))
            return

        for filename in filenames:
            self._process_file(os.path.join(perfdata_dir, filename))

    def _extract_fields(self, line):
        """Extract the key/value fields from a line of performance data
        """
        acc = {}
        field_tokens = line.split("\t")
        for field_token in field_tokens:
            kv_tokens = field_token.split('::')
            if len(kv_tokens) == 2:
                (key, value) = kv_tokens
                acc[key] = value

        return acc

    def _fields_valid(self, d):
        """Verify that all necessary fields are present

        Determine whether the fields parsed represent a host or
        service perfdata. If the perfdata is unknown, return False.
        If the perfdata does not contain all fields required for that
        type, return False. Otherwise, return True.
        """
        if 'DATATYPE' not in d:
            return False

        datatype = d['DATATYPE']
        if datatype == 'HOSTPERFDATA':
            fields = self.GENERIC_FIELDS + self.HOST_FIELDS
        elif datatype == 'SERVICEPERFDATA':
            fields = self.GENERIC_FIELDS + self.SERVICE_FIELDS
        else:
            return False

        for field in fields:
            if field not in d:
                return False

        return True

    def _normalize_to_unit(self, value, unit):
        """Normalize the value to the unit returned.

        We use base-1000 for second-based units, and base-1024 for
        byte-based units. Sadly, the Nagios-Plugins specification doesn't
        disambiguate base-1000 (KB) and base-1024 (KiB).
        """
        if unit == 'ms':
            return value / 1000.0
        if unit == 'us':
            return value / 1000000.0
        if unit == 'KB':
            return value * 1024.0
        if unit == 'MB':
            return value * 1024768.0
        if unit == 'GB':
            return value * 1073741824.0
        if unit == 'TB':
            return value * 1099511627776.0

        return value

    def _parse_perfdata(self, s):
        """Parse performance data from a perfdata string
        """
        metrics = []
        counters = re.findall(self.TOKENIZER_RE, s)
        if counters is None:
            self.log.warning("Failed to parse performance data: {s}".format(
                s=s))
            return metrics

        for (key, value, uom, warn, crit, min, max) in counters:
            try:
                norm_value = self._normalize_to_unit(float(value), uom)
                metrics.append((key, norm_value))
            except ValueError:
                self.log.warning(
                    "Couldn't convert value '{value}' to float".format(
                        value=value))

        return metrics

    def _process_file(self, path):
        """Parse and submit the metrics from a file
        """
        try:
            f = open(path)
            for line in f:
                self._process_line(line)

            os.remove(path)
        except IOError, ex:
            self.log.error("Could not open file `{path}': {error}".format(
                path=path, error=ex.strerror))

    def _process_line(self, line):
        """Parse and submit the metrics from a line of perfdata output
        """
        fields = self._extract_fields(line)
        if not self._fields_valid(fields):
            self.log.warning("Missing required fields for line: {line}".format(
                line=line))

        metric_path_base = []
        graphite_prefix = fields.get('GRAPHITEPREFIX')
        graphite_postfix = fields.get('GRAPHITEPOSTFIX')

        if graphite_prefix:
            metric_path_base.append(graphite_prefix)

        hostname = fields['HOSTNAME'].lower()
        metric_path_base.append(hostname)

        datatype = fields['DATATYPE']
        if datatype == 'HOSTPERFDATA':
            metric_path_base.append('host')
        elif datatype == 'SERVICEPERFDATA':
            service_desc = fields.get('SERVICEDESC')
            graphite_postfix = fields.get('GRAPHITEPOSTFIX')
            if graphite_postfix:
                metric_path_base.append(graphite_postfix)
            else:
                metric_path_base.append(service_desc)

        perfdata = fields[datatype]
        counters = self._parse_perfdata(perfdata)

        for (counter, value) in counters:
            metric_path = metric_path_base + [counter]
            metric_path = [self._sanitize(x) for x in metric_path]
            metric_name = '.'.join(metric_path)
            self.publish(metric_name, value)

    def _sanitize(self, s):
        """Sanitize the name of a metric to remove unwanted chars
        """
        return re.sub("[^\w-]", "_", s)

########NEW FILE########
__FILENAME__ = testnagiosperfdata
#!/usr/bin/python
# coding=utf-8

from test import CollectorTestCase
from test import get_collector_config
from mock import patch
import os

from diamond.collector import Collector
from nagiosperfdata import NagiosPerfdataCollector


class TestNagiosPerfdataCollector(CollectorTestCase):
    def setUp(self):
        """Set up the fixtures for the test
        """
        fixtures_dir = os.path.abspath(os.path.join(
            os.path.dirname(__file__), 'fixtures'))

        config = get_collector_config('NagiosPerfdataCollector', {
            'perfdata_dir': fixtures_dir
        })

        self.collector = NagiosPerfdataCollector(config, None)
        self.fixtures = os.listdir(fixtures_dir)

    def test_import(self):
        """Test that import works correctly
        """
        self.assertTrue(NagiosPerfdataCollector)

    @patch.object(NagiosPerfdataCollector, '_process_file')
    def test_collect_should_list_fixtures(self, process_mock):
        """Test that collect() finds our test fixtures
        """
        self.collector.collect()
        self.assertTrue(process_mock.called)

    def test_extract_fields_should_extract_fields(self):
        """Test that extract_fields() actually extracts fields
        """
        s = "KEY1::VALUE1\tKEY2::VALUE2 KEY3::VALUE3"
        fields = self.collector._extract_fields(s)
        self.assertEqual(fields.get('KEY1'), 'VALUE1')
        self.assertFalse('KEY2' in fields)

    def test_fields_valid_should_not_validate_invalid_datatype(self):
        fields = {'DATATYPE': 'BAD HOSTPERFDATA',
                  'HOSTNAME': 'testhost',
                  'HOSTPERFDATA': '',
                  'TIMET': 5304577351}
        self.assertFalse(self.collector._fields_valid(fields))

    def test_fields_valid_should_validate_complete_host_fields(self):
        fields = {'DATATYPE': 'HOSTPERFDATA',
                  'HOSTNAME': 'testhost',
                  'HOSTPERFDATA': '',
                  'TIMET': 5304577351}
        self.assertTrue(self.collector._fields_valid(fields))

    def test_fields_valid_should_not_validate_incomplete_host_fields(self):
        fields = {'DATATYPE': 'HOSTPERFDATA',
                  'HOSTNAME': 'testhost',
                  'TIMET': 5304577351}
        self.assertFalse(self.collector._fields_valid(fields))

    def test_fields_valid_should_validate_complete_service_fields(self):
        fields = {'DATATYPE': 'SERVICEPERFDATA',
                  'HOSTNAME': 'testhost',
                  'TIMET': 5304577351,
                  'SERVICEDESC': 'Ping',
                  'SERVICEPERFDATA': ''}
        self.assertTrue(self.collector._fields_valid(fields))

    def test_fields_valid_should_not_validate_incomplete_service_fields(self):
        fields = {'DATATYPE': 'SERVICEPERFDATA',
                  'HOSTNAME': 'testhost',
                  'TIMET': 5304577351,
                  'SERVICEDESC': 'Ping'}
        self.assertFalse(self.collector._fields_valid(fields))

    def test_normalize_to_unit_should_normalize(self):
        self.assertEqual(self.collector._normalize_to_unit(1, None), 1.0)
        self.assertEqual(self.collector._normalize_to_unit(1, 'KB'), 1024.0)

    def test_parse_perfdata_should_parse_valid_perfdata(self):
        perf = self.collector._parse_perfdata(
            'rta=0.325ms;300.000;500.000;0; pl=0%;20;60;;')
        expected_result = [('rta', 0.000325), ('pl', 0.0)]
        self.assertEqual(perf, expected_result)

    def test_parse_perfdata_should_not_parse_invalid_perfdata(self):
        perf = self.collector._parse_perfdata(
            'something with spaces=0.325ms;300.000;500.000;0; pl=0%;20;60;;')
        unexpected_result = [('something with spaces', 0.000325), ('pl', 0.0)]
        self.assertNotEqual(perf, unexpected_result)

    @patch('os.remove')
    @patch.object(Collector, 'publish')
    def test_process_file_should_work_with_real_host_perfdata(
            self, publish_mock, remove_mock):
        path = self.getFixturePath('host-perfdata.0')
        self.collector._process_file(path)
        expected = {
            'nagios.testhost.host.pl': 0,
            'nagios.testhost.host.rta': 0.000325
        }
        self.assertPublishedMany(publish_mock, expected)

    @patch('os.remove')
    @patch.object(Collector, 'publish')
    def test_process_file_should_work_with_real_service_perfdata(
            self, publish_mock, remove_mock):
        path = self.getFixturePath('service-perfdata.0')
        self.collector._process_file(path)
        publish_mock.assert_called_once()

    def test_sanitize_should_sanitize(self):
        orig1 = 'myhost.mydomain'
        sani1 = self.collector._sanitize(orig1)
        self.assertEqual(sani1, 'myhost_mydomain')

        orig2 = '/test/path'
        sani2 = self.collector._sanitize(orig2)
        self.assertEqual(sani2, '_test_path')

########NEW FILE########
__FILENAME__ = netapp
# coding=utf-8

"""
The NetAppCollector collects metric from a NetApp installation using the
NetApp Manageability SDK. This allows access to many metrics not available
via SNMP.

For this to work you'll the SDK available on the system.
This module has been developed using v5.0 of the SDK.
As of writing the SDK can be found at
https://communities.netapp.com/docs/DOC-1152

You'll also need to specify which NetApp instances the collecter should
get data from.

Example NetAppCollector.conf:
```
    enabled = True
    path_prefix = netapp

    [devices]

    [[na_filer]]
    ip = 123.123.123.123
    user = root
    password = strongpassword

````

The primary source for documentation about the API has been
"NetApp unified storage performance management using open interfaces"
https://communities.netapp.com/docs/DOC-1044

"""

import sys
import time
import re
import unicodedata

from diamond.metric import Metric
import diamond.convertor


class NetAppCollector(diamond.collector.Collector):

    # This is the list of metrics to collect.
    # This is a dict of lists with tuples, which is parsed as such:
    # The dict name is the object name in the NetApp API.
    # For each object we have a list of metrics to retrieve.
    # Each tuple is built like this;
    # ("metric name in netapp api", "output name of metric", multiplier)
    # The purpose of the output name is to enable replacement of reported
    # metric names, since some the names in the API can be confusing.
    # The purpose of the multiplier is to scale all metrics to a common
    # scale, which is latencies in milliseconds, and data in bytes per sec.
    # This is needed since the API will return a mixture of percentages,
    # nanoseconds, milliseconds, bytes and kilobytes.
    METRICS = {
        'aggregate': [
            ("user_reads", "user_read_iops", 1),
            ("user_writes", "user_write_iops", 1)
            ],
        'disk': [
            ("disk_busy", "disk_busy_pct", 100),
            ("base_for_disk_busy", "base_for_disk_busy", 1),
            ("user_read_blocks", "user_read_blocks_per_sec", 1),
            ("user_write_blocks", "user_write_blocks_per_sec", 1),
            ("user_read_latency", "user_read_latency", 0.001),
            ("user_write_latency", "user_write_latency", 0.001)
            ],
        'ifnet': [
            ("send_data", "tx_bytes_per_sec", 1),
            ("recv_data", "rx_bytes_per_sec", 1)
            ],
        'lun': [
            ("total_ops", "total_iops", 1),
            ("read_ops", "read_iops", 1),
            ("write_ops", "write_iops", 1),
            ("avg_latency", "avg_latency", 1)
            ],
        'processor': [
            ("processor_busy", "processor_busy_pct", 100),
            ("processor_elapsed_time", "processor_elapsed_time", 1)
            ],
        'system': [
            ("nfs_ops", "nfs_iops", 1),
            ("cifs_ops", "cifs_iops", 1),
            ("http_ops", "http_iops", 1),
            ("fcp_ops", "fcp_iops", 1),
            ("http_ops", "http_iops", 1),
            ("iscsi_ops", "iscsi_iops", 1),
            ("read_ops", "read_iops", 1),
            ("write_ops", "write_iops", 1),
            ("total_ops", "total_iops", 1),
            ("cpu_elapsed_time", "cpu_elapsed_time", 1),
            ("total_processor_busy", "total_processor_busy_pct", 100),
            ("avg_processor_busy", "avg_processor_busy_pct", 100),
            ("net_data_recv", "total_rx_bytes_per_sec", 1000),
            ("net_data_sent", "total_tx_bytes_per_sec", 1000),
            ("disk_data_read", "total_read_bytes_per_sec", 1000),
            ("disk_data_written", "total_write_bytes_per_sec", 1000),
            ("sys_read_latency", "sys_read_latency", 1),
            ("sys_write_latency", "sys_write_latency", 1),
            ("sys_avg_latency", "sys_avg_latency", 1)
            ],
        'vfiler': [
            ("vfiler_cpu_busy", "cpu_busy_pct", 100),
            ("vfiler_cpu_busy_base", "cpu_busy_base", 1),
            ("vfiler_net_data_recv", "rx_bytes_per_sec", 1000),
            ("vfiler_net_data_sent", "tx_bytes_per_sec", 1000),
            ("vfiler_read_ops", "read_iops", 1),
            ("vfiler_write_ops", "write_iops", 1),
            ("vfiler_read_bytes", "read_bytes_per_sec", 1000),
            ("vfiler_write_bytes", "write_bytes_per_sec", 1000),
            ],
        'volume': [
            ("total_ops", "total_iops", 1),
            ("avg_latency", "avg_latency", 0.001),
            ("read_ops", "read_iops", 1),
            ("write_ops", "write_iops", 1),
            ("read_latency", "read_latency", 0.001),
            ("write_latency", "write_latency", 0.001),
            ("read_data", "read_bytes_per_sec", 1),
            ("write_data", "write_bytes_per_sec", 1),
            ("cifs_read_data", "cifs_read_bytes_per_sec", 1),
            ("cifs_write_data", "cifs_write_bytes_per_sec", 1),
            ("cifs_read_latency", "cifs_read_latency", 0.001),
            ("cifs_write_latency", "cifs_write_latency", 0.001),
            ("cifs_read_ops", "cifs_read_iops", 1),
            ("cifs_write_ops", "cifs_write_iops", 1),
            ("fcp_read_data", "fcp_read_bytes_per_sec", 1),
            ("fcp_write_data", "fcp_write_bytes_per_sec", 1),
            ("fcp_read_latency", "fcp_read_latency", 0.001),
            ("fcp_write_latency", "fcp_write_latency", 0.001),
            ("fcp_read_ops", "fcp_read_iops", 1),
            ("fcp_write_ops", "fcp_write_iops", 1),
            ("iscsi_read_data", "iscsi_read_bytes_per_sec", 1),
            ("iscsi_write_data", "iscsi_write_bytes_per_sec", 1),
            ("iscsi_read_latency", "iscsi_read_latency", 0.001),
            ("iscsi_write_latency", "iscsi_write_latency", 0.001),
            ("iscsi_read_ops", "iscsi_read_iops", 1),
            ("iscsi_write_ops", "iscsi_write_iops", 1),
            ("nfs_read_data", "nfs_read_bytes_per_sec", 1),
            ("nfs_write_data", "nfs_write_bytes_per_sec", 1),
            ("nfs_read_latency", "nfs_read_latency", 0.001),
            ("nfs_write_latency", "nfs_write_latency", 0.001),
            ("nfs_read_ops", "nfs_read_iops", 1),
            ("nfs_write_ops", "nfs_write_iops", 1)
            ],
    }

    # For some metrics we need to divide one value from the API with another.
    # This is a key-value list of the connected values.
    DIVIDERS = {
        "avg_latency": "total_ops",
        "read_latency": "read_ops",
        "write_latency": "write_ops",
        "sys_avg_latency": "total_ops",
        "sys_read_latency": "read_ops",
        "sys_write_latency": "write_ops",
        "cifs_read_latency": "cifs_read_ops",
        "cifs_write_latency": "cifs_write_ops",
        "fcp_read_latency": "fcp_read_ops",
        "fcp_write_latency": "fcp_write_ops",
        "iscsi_read_latency": "iscsi_read_ops",
        "iscsi_write_latency": "iscsi_write_ops",
        "nfs_read_latency": "nfs_read_ops",
        "nfs_write_latency": "nfs_write_ops",
        "user_read_latency": "user_read_blocks",
        "user_write_latency": "user_write_blocks",
        "total_processor_busy": "cpu_elapsed_time",
        "avg_processor_busy": "cpu_elapsed_time",
        "processor_busy": "processor_elapsed_time",
        "disk_busy": "base_for_disk_busy",
        "vfiler_cpu_busy": "vfiler_cpu_busy_base",
    }

    # Some metrics are collected simply to calculate other metrics.
    # These should not be reported.
    DROPMETRICS = [
        "cpu_elapsed_time",
        "processor_elapsed_time",
        "base_for_disk_busy",
        "vfiler_cpu_busy_base",
    ]

    # Since we might have large collections collected often,
    # we need a pretty good time_delta.
    # We'll use a dict for this, keeping time_delta for each object.
    LastCollectTime = {}

    def get_default_config_help(self):
        config_help = super(NetAppCollector, self).get_default_config_help()
        return config_help

    def get_default_config(self):
        default_config = super(NetAppCollector, self).get_default_config()
        default_config['enabled'] = "false"
        default_config['path_prefix'] = "netapp"
        default_config['netappsdkpath'] = "/opt/netapp/lib/python/NetApp"
        return default_config

    def get_schedule(self):
        """
        Override Collector.get_schedule, and create one per filer.
        """
        schedule = {}
        if 'devices' in self.config:
            for device in self.config['devices']:
                # Get Device Config
                c = self.config['devices'][device]
                # Get Task Name
                task = "_".join([self.__class__.__name__, device])
                # Check if task is already in schedule
                if task in schedule:
                    raise KeyError("Duplicate device scheduled")
                schedule[task] = (self.collect,
                                  (device, c['ip'], c['user'], c['password']),
                                  int(self.config['splay']),
                                  int(self.config['interval']))
        self.log.info("Set up scheduler for %s" % device)
        return schedule

    def _replace_and_publish(self, path, prettyname, value, device):
        """
        Inputs a complete path for a metric and a value.
        Replace the metric name and publish.
        """
        if value is None:
            return
        newpath = path
        # Change metric name before publish if needed.
        newpath = ".".join([".".join(path.split(".")[:-1]), prettyname])
        metric = Metric(newpath, value, precision=4, host=device)
        self.publish_metric(metric)

    def _gen_delta_depend(self, path, derivative, multiplier, prettyname,
                          device):
        """
        For some metrics we need to divide the delta for one metric
        with the delta of another.
        Publishes a metric if the convertion goes well.
        """
        primary_delta = derivative[path]
        shortpath = ".".join(path.split(".")[:-1])
        basename = path.split(".")[-1]
        secondary_delta = None
        if basename in self.DIVIDERS.keys():
            mateKey = ".".join([shortpath, self.DIVIDERS[basename]])
        else:
            return
        if mateKey in derivative.keys():
            secondary_delta = derivative[mateKey]
        else:
            return

        # If we find a corresponding secondary_delta, publish a metric
        if primary_delta > 0 and secondary_delta > 0:
            value = (float(primary_delta) / secondary_delta) * multiplier
            self._replace_and_publish(path, prettyname, value, device)

    def _gen_delta_per_sec(self, path, value_delta, time_delta, multiplier,
                           prettyname, device):
        """
        Calulates the difference between to point, and scales is to per second.
        """
        if time_delta < 0:
            return
        value = (value_delta / time_delta) * multiplier
        # Only publish if there is any data.
        # This helps keep unused metrics out of Graphite
        if value > 0.0:
            self._replace_and_publish(path, prettyname, value, device)

    def collect(self, device, ip, user, password):
        """
        This function collects the metrics for one filer.
        """
        sys.path.append(self.config['netappsdkpath'])
        try:
            import NaServer
        except ImportError:
            self.log.error("Unable to load NetApp SDK from %s" % (
                self.config['netappsdkpath']))
            return

        # Set up the parameters
        server = NaServer.NaServer(ip, 1, 3)
        server.set_transport_type('HTTPS')
        server.set_style('LOGIN')
        server.set_admin_user(user, password)

        # We're only able to query a single object at a time,
        # so we'll loop over the objects.
        for na_object in self.METRICS.keys():

            # For easy reference later, generate a new dict for this object
            LOCALMETRICS = {}
            for metric in self.METRICS[na_object]:
                metricname, prettyname, multiplier = metric
                LOCALMETRICS[metricname] = {}
                LOCALMETRICS[metricname]["prettyname"] = prettyname
                LOCALMETRICS[metricname]["multiplier"] = multiplier

            # Keep track of how long has passed since we checked last
            CollectTime = time.time()
            time_delta = None
            if na_object in self.LastCollectTime.keys():
                time_delta = CollectTime - self.LastCollectTime[na_object]
            self.LastCollectTime[na_object] = CollectTime

            self.log.debug("Collecting metric of object %s" % na_object)
            query = NaServer.NaElement("perf-object-get-instances-iter-start")
            query.child_add_string("objectname", na_object)
            counters = NaServer.NaElement("counters")
            for metric in LOCALMETRICS.keys():
                counters.child_add_string("counter", metric)
            query.child_add(counters)

            res = server.invoke_elem(query)
            if(res.results_status() == "failed"):
                self.log.error("Connection to filer %s failed; %s" % (
                    device, res.results_reason()))
                return

            iter_tag = res.child_get_string("tag")
            num_records = 1
            max_records = 100

            # For some metrics there are dependencies between metrics for
            # a single object, so we'll need to collect all, so we can do
            # calculations later.
            raw = {}

            while(num_records != 0):
                query = NaServer.NaElement(
                    "perf-object-get-instances-iter-next")
                query.child_add_string("tag", iter_tag)
                query.child_add_string("maximum", max_records)
                res = server.invoke_elem(query)

                if(res.results_status() == "failed"):
                    print "Connection to filer %s failed; %s" % (
                        device, res.results_reason())
                    return

                num_records = res.child_get_int("records")

                if(num_records > 0):
                    instances_list = res.child_get("instances")
                    instances = instances_list.children_get()

                    for instance in instances:
                        raw_name = unicodedata.normalize(
                            'NFKD',
                            instance.child_get_string("name")).encode(
                            'ascii', 'ignore')
                        # Shorten the name for disks as they are very long and
                        # padded with zeroes, eg:
                        # 5000C500:3A236B0B:00000000:00000000:00000000:...
                        if na_object is "disk":
                            non_zero_blocks = [
                                block for block in raw_name.split(":")
                                if block != "00000000"
                                ]
                            raw_name = "".join(non_zero_blocks)
                        instance_name = re.sub(r'\W', '_', raw_name)
                        counters_list = instance.child_get("counters")
                        counters = counters_list.children_get()

                        for counter in counters:
                            metricname = unicodedata.normalize(
                                'NFKD',
                                counter.child_get_string("name")).encode(
                                'ascii', 'ignore')
                            metricvalue = counter.child_get_string("value")
                            # We'll need a long complete pathname to not
                            # confuse self.derivative
                            pathname = ".".join([self.config["path_prefix"],
                                                 device, na_object,
                                                 instance_name, metricname])
                            raw[pathname] = int(metricvalue)

            # Do the math
            self.log.debug("Processing %i metrics for object %s" % (len(raw),
                                                                    na_object))

            # Since the derivative function both returns the derivative
            # and saves a new point, we'll need to store all derivatives
            # for local reference.
            derivative = {}
            for key in raw.keys():
                derivative[key] = self.derivative(key, raw[key])

            for key in raw.keys():
                metricname = key.split(".")[-1]
                prettyname = LOCALMETRICS[metricname]["prettyname"]
                multiplier = LOCALMETRICS[metricname]["multiplier"]

                if metricname in self.DROPMETRICS:
                    continue
                elif metricname in self.DIVIDERS.keys():
                    self._gen_delta_depend(key, derivative, multiplier,
                                           prettyname, device)
                else:
                    self._gen_delta_per_sec(key, derivative[key], time_delta,
                                            multiplier, prettyname, device)

########NEW FILE########
__FILENAME__ = netappDisk
""" This collector retrieves disk information from netapp filers

    !Thread-safe!

    The following metrics are measured in this diamond collector:
        * Number of disk in spare pool and not zeroed.
        * Number of spare disks per disk type, ie SAS/SATA
        * Number of disk in 'maintenance center'
        * monitor CP
        * Monitor average disk busyness per aggregate.

        Config File:
            [devices]
            [[filer-corp-201]] <--- You can have as many filers as you want!
            ip = 192.168.1.45
            user = netapp_monitor
            password = b0bl0rd!

"""

import diamond.collector
import time
from diamond.metric import Metric

try:
    import xml.etree.ElementTree as ET
    ET  # workaround for pyflakes issue #13
except ImportError:
    import cElementTree as ET

try:
    from netappsdk.NaServer import *
    from netappsdk.NaElement import *
    netappsdk = 0
except ImportError:
    netappsdk = 1

__author__ = 'peter@phyn3t.com'


class netappDiskCol(object):
    """ Our netappDisk Collector
    """

    def __init__(self, device, ip, user, password, parent):
        """ Collectors our metrics for our netapp filer
        """

        self.device = device
        self.ip = ip
        self.netapp_user = user
        self.netapp_password = password
        self.path_prefix = parent[0]
        self.publish_metric = parent[1]
        self.log = parent[2]
        self._netapp_login()

        # Grab our netapp XML
        disk_xml = self.get_netapp_elem(
            NaElement('disk-list-info'), 'disk-details')
        storage_disk_xml = self.get_netapp_elem(
            NaElement('storage-disk-get-iter'), 'attributes-list')

        # Our metric collection and publishing goes here
        self.zero_disk(disk_xml)
        self.spare_disk(disk_xml)
        self.maintenance_center(storage_disk_xml)
        self.consistency_point()
        self.agr_busy()

    def agr_busy(self):
        """ Collector for average disk busyness per aggregate

            As of Nov 22nd 2013 there is no API call for agr busyness.
            You have to collect all disk busyness and then compute agr
            busyness. #fml

        """

        c1 = {}  # Counters from time a
        c2 = {}  # Counters from time b
        disk_results = {}  # Disk busyness results %
        agr_results = {}  # Aggregate busyness results $
        names = ['disk_busy', 'base_for_disk_busy', 'raid_name',
                 'base_for_disk_busy', 'instance_uuid']
        netapp_api = NaElement('perf-object-get-instances')
        netapp_api.child_add_string('objectname', 'disk')
        disk_1 = self.get_netapp_elem(netapp_api, 'instances')
        time.sleep(1)
        disk_2 = self.get_netapp_elem(netapp_api, 'instances')

        for instance_data in disk_1:
            temp = {}
            for element in instance_data.findall(".//counters/counter-data"):
                if element.find('name').text in names:
                    temp[element.find('name').text] = element.find('value').text

            agr_name = temp['raid_name']
            agr_name = agr_name[agr_name.find('/', 0):agr_name.find('/', 1)]
            temp['raid_name'] = agr_name.lstrip('/')
            c1[temp.pop('instance_uuid')] = temp

        for instance_data in disk_2:
            temp = {}
            for element in instance_data.findall(".//counters/counter-data"):
                if element.find('name').text in names:
                    temp[element.find('name').text] = element.find('value').text

            agr_name = temp['raid_name']
            agr_name = agr_name[agr_name.find('/', 0):agr_name.find('/', 1)]
            temp['raid_name'] = agr_name.lstrip('/')
            c2[temp.pop('instance_uuid')] = temp

        for item in c1:
            t_c1 = int(c1[item]['disk_busy'])  # time_counter_1
            t_b1 = int(c1[item]['base_for_disk_busy'])  # time_base_1
            t_c2 = int(c2[item]['disk_busy'])
            t_b2 = int(c2[item]['base_for_disk_busy'])

            disk_busy = 100 * (t_c2 - t_c1) / (t_b2 - t_b1)

            if c1[item]['raid_name'] in disk_results:
                disk_results[c1[item]['raid_name']].append(disk_busy)
            else:
                disk_results[c1[item]['raid_name']] = [disk_busy]

        for aggregate in disk_results:
            agr_results[aggregate] = \
                sum(disk_results[aggregate]) / len(disk_results[aggregate])

        for aggregate in agr_results:
            self.push('avg_busy', 'aggregate.' + aggregate,
                      agr_results[aggregate])

    def consistency_point(self):
        """ Collector for getting count of consistancy points
        """

        cp_delta = {}
        xml_path = 'instances/instance-data/counters'
        netapp_api = NaElement('perf-object-get-instances')
        netapp_api.child_add_string('objectname', 'wafl')
        instance = NaElement('instances')
        instance.child_add_string('instance', 'wafl')
        counter = NaElement('counters')
        counter.child_add_string('counter', 'cp_count')
        netapp_api.child_add(counter)
        netapp_api.child_add(instance)

        cp_1 = self.get_netapp_elem(netapp_api, xml_path)
        time.sleep(3)
        cp_2 = self.get_netapp_elem(netapp_api, xml_path)

        for element in cp_1:
            if element.find('name').text == 'cp_count':
                cp_1 = element.find('value').text.rsplit(',')
                break
        for element in cp_2:
            if element.find('name').text == 'cp_count':
                cp_2 = element.find('value').text.rsplit(',')
                break

        if not type(cp_2) is list or not type(cp_1) is list:
            log.error("consistency point data not available for filer: %s"
                      % self.device)
            return

        cp_1 = {
            'wafl_timer': cp_1[0],
            'snapshot': cp_1[1],
            'wafl_avail_bufs': cp_1[2],
            'dirty_blk_cnt': cp_1[3],
            'full_nv_log': cp_1[4],
            'b2b': cp_1[5],
            'flush_gen': cp_1[6],
            'sync_gen': cp_1[7],
            'def_b2b': cp_1[8],
            'con_ind_pin': cp_1[9],
            'low_mbuf_gen': cp_1[10],
            'low_datavec_gen': cp_1[11]
        }

        cp_2 = {
            'wafl_timer': cp_2[0],
            'snapshot': cp_2[1],
            'wafl_avail_bufs': cp_2[2],
            'dirty_blk_cnt': cp_2[3],
            'full_nv_log': cp_2[4],
            'b2b': cp_2[5],
            'flush_gen': cp_2[6],
            'sync_gen': cp_2[7],
            'def_b2b': cp_2[8],
            'con_ind_pin': cp_2[9],
            'low_mbuf_gen': cp_2[10],
            'low_datavec_gen': cp_2[11]
        }

        for item in cp_1:
            c1 = int(cp_1[item])
            c2 = int(cp_2[item])
            cp_delta[item] = c2 - c1

        for item in cp_delta:
            self.push(item + '_CP', 'system.system', cp_delta[item])

    def maintenance_center(self, storage_disk_xml=None):
        """ Collector for how many disk(s) are in NetApp maintenance center

            For more information on maintenance center please see:
              bit.ly/19G4ptr

        """

        disk_in_maintenance = 0

        for filer_disk in storage_disk_xml:
            disk_status = filer_disk.find('disk-raid-info/container-type')
            if disk_status.text == 'maintenance':
                disk_in_maintenance += 1

        self.push('maintenance_disk', 'disk', disk_in_maintenance)

    def zero_disk(self, disk_xml=None):
        """ Collector and publish not zeroed disk metrics
        """

        troubled_disks = 0
        for filer_disk in disk_xml:
            raid_state = filer_disk.find('raid-state').text
            if not raid_state == 'spare':
                continue
            is_zeroed = filer_disk.find('is-zeroed').text

            if is_zeroed == 'false':
                troubled_disks += 1
        self.push('not_zeroed', 'disk', troubled_disks)

    def spare_disk(self, disk_xml=None):
        """ Number of spare disk per type.

            For example: storage.ontap.filer201.disk.SATA

        """

        spare_disk = {}
        disk_types = set()

        for filer_disk in disk_xml:
            disk_types.add(filer_disk.find('effective-disk-type').text)
            if not filer_disk.find('raid-state').text == 'spare':
                continue

            disk_type = filer_disk.find('effective-disk-type').text
            if disk_type in spare_disk:
                spare_disk[disk_type] += 1
            else:
                spare_disk[disk_type] = 1

        for disk_type in disk_types:
            if disk_type in spare_disk:
                self.push('spare_' + disk_type, 'disk', spare_disk[disk_type])
            else:
                self.push('spare_' + disk_type, 'disk', 0)

    def get_netapp_elem(self, netapp_api=None, sub_element=None):
        """ Retrieve netapp elem
        """

        netapp_data = self.server.invoke_elem(netapp_api)

        if netapp_data.results_status() == 'failed':
            self.log.error(
                'While using netapp API failed to retrieve '
                'disk-list-info for netapp filer %s' % self.device)
            print netapp_data.sprintf()
            return
        netapp_xml = \
            ET.fromstring(netapp_data.sprintf()).find(sub_element)

        return netapp_xml

    def _netapp_login(self):
        """ Login to our netapp filer
        """

        self.server = NaServer(self.ip, 1, 3)
        self.server.set_transport_type('HTTPS')
        self.server.set_style('LOGIN')
        self.server.set_admin_user(self.netapp_user, self.netapp_password)

    def push(self, metric_name=None, type=None, metric_value=None):
        """ Ship that shit off to graphite broski
        """

        graphite_path = self.path_prefix
        graphite_path += '.' + self.device + '.' + type
        graphite_path += '.' + metric_name

        metric = Metric(
            graphite_path,
            metric_value,
            precision=4,
            host=self.device)

        self.publish_metric(metric)


class netappDisk(diamond.collector.Collector):
    """ Netapp disk diamond scheduler
    """

    running = set()

    def collect(self, device, ip, user, password):
        """ Collectors our metrics for our netapp filer --START HERE--
        """

        if netappsdk:
            self.log.error(
                'Failed to import netappsdk.NaServer or netappsdk.NaElement')
            return

        if device in self.running:
            return

        self.running.add(device)
        parent = (self.config['path_prefix'], self.publish_metric, self.log)

        netappDiskCol(device, ip, user, password, parent)
        self.running.remove(device)

    def get_schedule(self):
        """ Override Collector.get_schedule

            We override collector.get_schedule so we can increase speed
            by having a task per netapp filer

        """

        schedule = {}

        if 'devices' in self.config:

            for device in self.config['devices']:
                filer_config = self.config['devices'][device]
                task_name = '_'.join([self.__class__.__name__, device])
                if task_name in schedule:
                    raise KeyError('Duplicate netapp filer scheduled')

                schedule[task_name] = (
                    self.collect,
                    (device,
                     filer_config['ip'],
                     filer_config['user'],
                     filer_config['password']),
                    int(self.config['splay']),
                    int(self.config['interval']))
                self.log.info("Set up scheduler for %s" % device)

        return schedule

########NEW FILE########
__FILENAME__ = netapp_inode
""" This collector retrieves inode data from netapp filers

    !Thread-safe!

    The following metrics are measured in this diamond collector:
        * number of inodes per volume ()
        * current inode max (One inode per block; 4kB blocks)

        Config File:
            [devices]
            [[filer-corp-201]] <--- You can have as many filers as you want!
            ip = 192.168.1.45
            user = netapp_monitor
            password = b0bl0rd!

"""

import diamond.collector
from diamond.metric import Metric

try:
    import xml.etree.ElementTree as ET
    ET  # workaround for pyflakes issue #13
except ImportError:
    import cElementTree as ET

try:
    from netappsdk.NaServer import *
    from netappsdk.NaElement import *
except ImportError:
    netappsdk = None


__author__ = 'peter@phyn3t.com'


class netapp_inodeCol(object):
    """ Our netapp_inode Collector
    """

    def __init__(self, device, ip, user, password, prefix, pm):
        """Instantiate _our_ stuff
        """

        self.device = device
        self.ip = ip
        self.netapp_user = user
        self.netapp_password = password
        self.path_prefix = prefix
        self.publish_metric = pm
        self._netapp_login()

        filers_xml = self.get_netapp_data()

        for volume in filers_xml:
            max_inodes = volume.find('files-total').text
            used_inodes = volume.find('files-used').text
            volume = volume.find('name').text
            self.push('max_inodes', max_inodes, volume)
            self.push('used_inodes', used_inodes, volume)

    def push(self, metric_name=None, metric_value=None, volume=None):
        """ Ship that shit off to graphite broski
        """

        graphite_path = self.path_prefix
        graphite_path += '.' + self.device + '.' + 'volume'
        graphite_path += '.' + volume + '.' + metric_name

        metric = Metric(graphite_path, metric_value, precision=4,
                        host=self.device)

        self.publish_metric(metric)

    def get_netapp_data(self):
        """ Retrieve netapp volume information

            returns ElementTree of netapp volume information

        """

        netapp_data = self.server.invoke('volume-list-info')

        if netapp_data.results_status() == 'failed':
            self.log.error(
                'While using netapp API failed to retrieve '
                'volume-list-info for netapp filer %s' % self.device)
            return

        netapp_xml = ET.fromstring(netapp_data.sprintf()).find('volumes')

        return netapp_xml

    def _netapp_login(self):
        """ Login to our netapp filer
        """

        self.server = NaServer(self.ip, 1, 3)
        self.server.set_transport_type('HTTPS')
        self.server.set_style('LOGIN')
        self.server.set_admin_user(self.netapp_user, self.netapp_password)


class netapp_inode(diamond.collector.Collector):
    """ Netapp inode diamond collector
    """

    running = set()

    def collect(self, device, ip, user, password):
        """ Collects metrics for our netapp filer --START HERE--

        """

        if netappsdk is None:
            self.log.error(
                'Failed to import netappsdk.NaServer or netappsdk.NaElement')
            return

        if device in self.running:
            return

        self.running.add(device)
        prefix = self.config['path_prefix']
        pm = self.publish_metric

        netapp_inodeCol(device, ip, user, password, prefix, pm)
        self.running.remove(device)

    def get_schedule(self):
        """ Override Collector.get_schedule

           We override collector.get_schedule so we can increase speed
           by having a task per netapp filer

            return schedule - dictionary with key being className_device
              value is tuple containing:
                0(tuple) - object collector method
                1(tuple) - device, ip, user, password
                2(int)   - splay, set in diamond.conf
                3(int)   - interval, set in diamond.conf

        """

        schedule = {}

        if 'devices' in self.config:

            for device in self.config['devices']:
                filer_config = self.config['devices'][device]
                task_name = '_'.join([self.__class__.__name__, device])

                if task_name in schedule:
                    raise KeyError('Duplicate netapp filer scheduled')

                schedule[task_name] = (
                    self.collect,
                    (device,
                     filer_config['ip'],
                     filer_config['user'],
                     filer_config['password']),
                    int(self.config['splay']),
                    int(self.config['interval']))

                self.log.info("Set up scheduler for %s" % device)

        return schedule

########NEW FILE########
__FILENAME__ = testnetapp
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from netapp import NetAppCollector


###############################################################################

class TestNetAppCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('NetAppCollector', {
        })
        self.collector = NetAppCollector(config, None)

    def test_import(self):
        self.assertTrue(NetAppCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = testnetappDisk
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from netappDisk import netappDisk


###############################################################################

class TestnetappDisk(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('netappDisk', {
        })
        self.collector = netappDisk(config, None)

    def test_import(self):
        self.assertTrue(netappDisk)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = testnetapp_inode
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from netapp_inode import netapp_inode


###############################################################################

class Testnetapp_inode(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('netapp_inode', {
        })
        self.collector = netapp_inode(config, None)

    def test_import(self):
        self.assertTrue(netapp_inode)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = netscalersnmp
# coding=utf-8

"""
SNMPCollector for Netscaler Metrics

NetScaler is a network appliance manufactured by Citrix providing level 4 load
balancing, firewall, proxy and VPN functions.

"""

import sys
import os
import time
import struct
import re

# Fix Path for locating the SNMPCollector
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
                                             '../',
                                             'snmp',
                                             )))

from diamond.metric import Metric
from snmp import SNMPCollector as parent_SNMPCollector


class NetscalerSNMPCollector(parent_SNMPCollector):
    """
    SNMPCollector for Netscaler Metrics
    """

    """
    EntityProtocolType ::=
    INTEGER{    http(0),
                ftp(1),
                tcp(2),
                udp(3),
                sslBridge(4),
                monitor(5),
                monitorUdp(6),
                nntp(7),
                httpserver(8),
                httpclient(9),
                rpcserver(10),
                rpcclient(11),
                nat(12),
                any(13),
                ssl(14),
                dns(15),
                adns(16),
                snmp(17),
                ha(18),
                monitorPing(19),
                sslOtherTcp(20),
                aaa(21),
                secureMonitor(23),
                sslvpnUdp(24),
                rip(25),
                dnsClient(26),
                rpcServer(27),
                rpcClient(28),
                dhcrpa(36),
                sipudp(39),
                dnstcp(44),
                adnstcp(45),
                rtsp(46),
                push(48),
                sslPush(49),
                dhcpClient(50),
                radius(51),
                serviceUnknown(62) }

    EntityState ::=
    INTEGER{    down(1),
                unknown(2),
                busy(3),
                outOfService(4),
                transitionToOutOfService(5),
                up(7),
                transitionToOutOfServiceDown(8) }
    """

    NETSCALER_SYSTEM_GUAGES = {
        "cpuUsage": "1.3.6.1.4.1.5951.4.1.1.41.1.0",
        "memUsage": "1.3.6.1.4.1.5951.4.1.1.41.2.0",
        "surgeQueue": "1.3.6.1.4.1.5951.4.1.1.46.15.0",
        "establishedServerConnections": "1.3.6.1.4.1.5951.4.1.1.46.10.0",
        "establishedClientConnections": "1.3.6.1.4.1.5951.4.1.1.46.12.0"
    }

    NETSCALER_SYSTEM_COUNTERS = {
        "httpTotRequests": "1.3.6.1.4.1.5951.4.1.1.48.67.0"
    }

    NETSCALER_VSERVER_NAMES = "1.3.6.1.4.1.5951.4.1.3.1.1.1"

    NETSCALER_VSERVER_TYPE = "1.3.6.1.4.1.5951.4.1.3.1.1.4"

    NETSCALER_VSERVER_STATE = "1.3.6.1.4.1.5951.4.1.3.1.1.5"

    NETSCALER_VSERVER_GUAGES = {
        "vsvrRequestRate": "1.3.6.1.4.1.5951.4.1.3.1.1.43",
        "vsvrRxBytesRate": "1.3.6.1.4.1.5951.4.1.3.1.1.44",
        "vsvrTxBytesRate": "1.3.6.1.4.1.5951.4.1.3.1.1.45",
        "vsvrCurServicesUp": "1.3.6.1.4.1.5951.4.1.3.1.1.41",
        "vsvrCurServicesDown": "1.3.6.1.4.1.5951.4.1.3.1.1.37",
        "vsvrCurServicesUnknown": "1.3.6.1.4.1.5951.4.1.3.1.1.38",
        "vsvrCurServicesTransToOutOfSvc": "1.3.6.1.4.1.5951.4.1.3.1.1.40"
    }

    NETSCALER_SERVICE_NAMES = "1.3.6.1.4.1.5951.4.1.2.1.1.1"

    NETSCALER_SERVICE_TYPE = "1.3.6.1.4.1.5951.4.1.2.1.1.4"

    NETSCALER_SERVICE_STATE = "1.3.6.1.4.1.5951.4.1.2.1.1.5"

    NETSCALER_SERVICE_GUAGES = {
        "svcRequestRate": "1.3.6.1.4.1.5951.4.1.2.1.1.42",
        "svcSurgeCount": "1.3.6.1.4.1.5951.4.1.2.1.1.10",
        "svcEstablishedConn": "1.3.6.1.4.1.5951.4.1.2.1.1.8",
        "svcActiveConn": "1.3.6.1.4.1.5951.4.1.2.1.1.9",
        "svcCurClntConnections": "1.3.6.1.4.1.5951.4.1.2.1.1.41"
    }

    MAX_VALUE = 18446744073709551615

    def get_default_config_help(self):
        config_help = super(NetscalerSNMPCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': 'netscaler dns address',
            'port': 'Netscaler port to collect snmp data',
            'community': 'SNMP community',
            'exclude_service_type': "list of service types to exclude"
            + " (see MIB EntityProtocolType)",
            'exclude_vserver_type': "list of vserver types to exclude"
            + " (see MIB EntityProtocolType)"
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(NetscalerSNMPCollector, self).get_default_config()
        config.update({
            'path':     'netscaler',
            'timeout':  15,
            'exclude_service_type': [],
            'exclude_vserver_type': [],
            'exclude_service_state': [],
            'exclude_vserver_state': []
        })
        return config

    def get_string_index_oid(self, s):
        """Turns a string into an oid format is length of name followed by
        name chars in ascii"""
        return (len(self.get_bytes(s)), ) + self.get_bytes(s)

    def get_bytes(self, s):
        """Turns a string into a list of byte values"""
        return struct.unpack('%sB' % len(s), s)

    def collect_snmp(self, device, host, port, community):
        """
        Collect Netscaler SNMP stats from device
        """
        # Log
        self.log.info("Collecting Netscaler statistics from: %s", device)

        # Set timestamp
        timestamp = time.time()

        # Collect Netscaler System OIDs
        for k, v in self.NETSCALER_SYSTEM_GUAGES.items():
            # Get Metric Name and Value
            metricName = '.'.join([k])
            metricValue = int(self.get(v, host, port, community)[v])
            # Get Metric Path
            metricPath = '.'.join(['devices', device, 'system', metricName])
            # Create Metric
            metric = Metric(metricPath, metricValue, timestamp, 0)
            # Publish Metric
            self.publish_metric(metric)

        # Collect Netscaler System Counter OIDs
        for k, v in self.NETSCALER_SYSTEM_COUNTERS.items():
            # Get Metric Name and Value
            metricName = '.'.join([k])
            # Get Metric Path
            metricPath = '.'.join(['devices', device, 'system', metricName])
            # Get Metric Value
            metricValue = self.derivative(metricPath, long(
                self.get(v, host, port, community)[v]), self.MAX_VALUE)
            # Create Metric
            metric = Metric(metricPath, metricValue, timestamp, 0)
            # Publish Metric
            self.publish_metric(metric)

        # Collect Netscaler Services
        serviceNames = [v.strip("\'") for v in self.walk(
            self.NETSCALER_SERVICE_NAMES, host, port, community).values()]

        for serviceName in serviceNames:
            # Get Service Name in OID form
            serviceNameOid = self.get_string_index_oid(serviceName)

            # Get Service Type
            serviceTypeOid = ".".join([self.NETSCALER_SERVICE_TYPE,
                                       self._convert_from_oid(serviceNameOid)])
            serviceType = int(self.get(serviceTypeOid,
                                       host,
                                       port,
                                       community)[serviceTypeOid].strip("\'"))

            # Filter excluded service types
            if serviceType in map(lambda v: int(v),
                                  self.config.get('exclude_service_type')):
                continue

            # Get Service State
            serviceStateOid = ".".join([self.NETSCALER_SERVICE_STATE,
                                        self._convert_from_oid(serviceNameOid)])
            serviceState = int(self.get(serviceStateOid,
                                        host,
                                        port,
                                        community)[serviceStateOid].strip("\'"))

            # Filter excluded service states
            if serviceState in map(lambda v: int(v),
                                   self.config.get('exclude_service_state')):
                continue

            for k, v in self.NETSCALER_SERVICE_GUAGES.items():
                serviceGuageOid = ".".join(
                    [v, self._convert_from_oid(serviceNameOid)])
                # Get Metric Name
                metricName = '.'.join([re.sub(r'\.|\\', '_', serviceName), k])
                # Get Metric Value
                metricValue = int(self.get(serviceGuageOid,
                                           host,
                                           port,
                                           community
                                           )[serviceGuageOid].strip("\'"))
                # Get Metric Path
                metricPath = '.'.join(['devices',
                                       device,
                                       'service',
                                       metricName])
                # Create Metric
                metric = Metric(metricPath, metricValue, timestamp, 0)
                # Publish Metric
                self.publish_metric(metric)

        # Collect Netscaler Vservers
        vserverNames = [v.strip("\'") for v in self.walk(
            self.NETSCALER_VSERVER_NAMES, host, port, community).values()]

        for vserverName in vserverNames:
            # Get Vserver Name in OID form
            vserverNameOid = self.get_string_index_oid(vserverName)

            # Get Vserver Type
            vserverTypeOid = ".".join([self.NETSCALER_VSERVER_TYPE,
                                       self._convert_from_oid(vserverNameOid)])
            vserverType = int(self.get(vserverTypeOid,
                                       host,
                                       port,
                                       community)[vserverTypeOid].strip("\'"))

            # filter excluded vserver types
            if vserverType in map(lambda v: int(v),
                                  self.config.get('exclude_vserver_type')):
                continue

            # Get Service State
            vserverStateOid = ".".join([self.NETSCALER_VSERVER_STATE,
                                        self._convert_from_oid(vserverNameOid)])
            vserverState = int(self.get(vserverStateOid,
                                        host,
                                        port,
                                        community)[vserverStateOid].strip("\'"))

            # Filter excluded vserver state
            if vserverState in map(lambda v: int(v),
                                   self.config.get('exclude_vserver_state')):
                continue

            for k, v in self.NETSCALER_VSERVER_GUAGES.items():
                vserverGuageOid = ".".join(
                    [v, self._convert_from_oid(vserverNameOid)])
                # Get Metric Name
                metricName = '.'.join([re.sub(r'\.|\\', '_', vserverName), k])
                # Get Metric Value
                metricValue = int(self.get(vserverGuageOid,
                                           host,
                                           port,
                                           community
                                           )[vserverGuageOid].strip("\'"))
                # Get Metric Path
                metricPath = '.'.join(['devices',
                                       device,
                                       'vserver',
                                       metricName])
                # Create Metric
                metric = Metric(metricPath, metricValue, timestamp, 0)
                # Publish Metric
                self.publish_metric(metric)

########NEW FILE########
__FILENAME__ = testnetscalersnmp
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from netscalersnmp import NetscalerSNMPCollector


class TestNetscalerSNMPCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('NetscalerSNMPCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = NetscalerSNMPCollector(config, None)

    def test_import(self):
        self.assertTrue(NetscalerSNMPCollector)

########NEW FILE########
__FILENAME__ = network
# coding=utf-8

"""
The NetworkCollector class collects metrics on network interface usage
using /proc/net/dev.

#### Dependencies

 * /proc/net/dev

"""

import diamond.collector
import diamond.convertor
import os
import re

try:
    import psutil
    psutil  # workaround for pyflakes issue #13
except ImportError:
    psutil = None


class NetworkCollector(diamond.collector.Collector):

    PROC = '/proc/net/dev'

    def get_default_config_help(self):
        config_help = super(NetworkCollector, self).get_default_config_help()
        config_help.update({
            'interfaces': 'List of interface types to collect',
            'greedy': 'Greedy match interfaces',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(NetworkCollector, self).get_default_config()
        config.update({
            'path':         'network',
            'interfaces':   ['eth', 'bond', 'em', 'p1p'],
            'byte_unit':    ['bit', 'byte'],
            'greedy':       'true',
        })
        return config

    def collect(self):
        """
        Collect network interface stats.
        """

        # Initialize results
        results = {}

        if os.access(self.PROC, os.R_OK):

            # Open File
            file = open(self.PROC)
            # Build Regular Expression
            greed = ''
            if self.config['greedy'].lower() == 'true':
                greed = '\S+'

            exp = ('^(?:\s*)((?:%s)%s):(?:\s*)'
                   + '(?P<rx_bytes>\d+)(?:\s*)'
                   + '(?P<rx_packets>\w+)(?:\s*)'
                   + '(?P<rx_errors>\d+)(?:\s*)'
                   + '(?P<rx_drop>\d+)(?:\s*)'
                   + '(?P<rx_fifo>\d+)(?:\s*)'
                   + '(?P<rx_frame>\d+)(?:\s*)'
                   + '(?P<rx_compressed>\d+)(?:\s*)'
                   + '(?P<rx_multicast>\d+)(?:\s*)'
                   + '(?P<tx_bytes>\d+)(?:\s*)'
                   + '(?P<tx_packets>\w+)(?:\s*)'
                   + '(?P<tx_errors>\d+)(?:\s*)'
                   + '(?P<tx_drop>\d+)(?:\s*)'
                   + '(?P<tx_fifo>\d+)(?:\s*)'
                   + '(?P<tx_frame>\d+)(?:\s*)'
                   + '(?P<tx_compressed>\d+)(?:\s*)'
                   + '(?P<tx_multicast>\d+)(?:.*)$') % (
                ('|'.join(self.config['interfaces'])), greed)
            reg = re.compile(exp)
            # Match Interfaces
            for line in file:
                match = reg.match(line)
                if match:
                    device = match.group(1)
                    results[device] = match.groupdict()
            # Close File
            file.close()
        else:
            if not psutil:
                self.log.error('Unable to import psutil')
                self.log.error('No network metrics retrieved')
                return None

            network_stats = psutil.network_io_counters(True)
            for device in network_stats.keys():
                network_stat = network_stats[device]
                results[device] = {}
                results[device]['rx_bytes'] = network_stat.bytes_recv
                results[device]['tx_bytes'] = network_stat.bytes_sent
                results[device]['rx_packets'] = network_stat.packets_recv
                results[device]['tx_packets'] = network_stat.packets_sent

        for device in results:
            stats = results[device]
            for s, v in stats.items():
                # Get Metric Name
                metric_name = '.'.join([device, s])
                # Get Metric Value
                metric_value = self.derivative(metric_name,
                                               long(v),
                                               diamond.collector.MAX_COUNTER)

                # Convert rx_bytes and tx_bytes
                if s == 'rx_bytes' or s == 'tx_bytes':
                    convertor = diamond.convertor.binary(value=metric_value,
                                                         unit='byte')

                    for u in self.config['byte_unit']:
                        # Public Converted Metric
                        self.publish(metric_name.replace('bytes', u),
                                     convertor.get(unit=u), 2)
                else:
                    # Publish Metric Derivative
                    self.publish(metric_name, metric_value)

        return None

########NEW FILE########
__FILENAME__ = testnetwork
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from network import NetworkCollector

################################################################################


class TestNetworkCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('NetworkCollector', {
            'interfaces': ['eth', 'em', 'bond', 'veth', 'br-lxc'],
            'interval':  10,
            'byte_unit': ['bit', 'megabit', 'megabyte'],
        })

        self.collector = NetworkCollector(config, None)

    def test_import(self):
        self.assertTrue(NetworkCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_net_dev(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/net/dev')

    @patch.object(Collector, 'publish')
    def test_should_work_with_virtual_interfaces_and_bridges(self,
                                                             publish_mock):
        NetworkCollector.PROC = self.getFixturePath('proc_net_dev_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        NetworkCollector.PROC = self.getFixturePath('proc_net_dev_2')
        self.collector.collect()

        metrics = {
            'eth0.rx_megabyte': (2.504, 2),
            'eth0.tx_megabyte': (4.707, 2),
            'eth1.rx_megabyte': (0.0, 2),
            'eth1.tx_megabyte': (0.0, 2),
            'em2.rx_megabyte': (2.504, 2),
            'em2.tx_megabyte': (4.707, 2),
            'bond3.rx_megabyte': (2.504, 2),
            'bond3.tx_megabyte': (4.707, 2),
            'vethmR3i5e.tx_megabyte': (0.223, 2),
            'vethmR3i5e.rx_megabyte': (0.033, 2),
            'br-lxc-247.tx_megabyte': (0.307, 2),
            'br-lxc-247.rx_megabyte': (0.032, 2)
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        NetworkCollector.PROC = self.getFixturePath('proc_net_dev_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        NetworkCollector.PROC = self.getFixturePath('proc_net_dev_2')
        self.collector.collect()

        metrics = {
            'eth0.rx_megabyte': (2.504, 2),
            'eth0.tx_megabyte': (4.707, 2),
            'eth1.rx_megabyte': (0.0, 2),
            'eth1.tx_megabyte': (0.0, 2),
            'em2.rx_megabyte': (2.504, 2),
            'em2.tx_megabyte': (4.707, 2),
            'bond3.rx_megabyte': (2.504, 2),
            'bond3.tx_megabyte': (4.707, 2)
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    # Named test_z_* to run after test_should_open_proc_net_dev
    @patch.object(Collector, 'publish')
    def test_z_issue_208_a(self, publish_mock):
        NetworkCollector.PROC = self.getFixturePath('208-a_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        NetworkCollector.PROC = self.getFixturePath('208-a_2')
        self.collector.collect()

        metrics = {
            'bond0.rx_bit': 2687979419428.0,
            'bond0.rx_compressed': 0.0,
            'bond0.rx_drop': 0.0,
            'bond0.rx_errors': 0.0,
            'bond0.rx_fifo': 0.0,
            'bond0.rx_frame': 0.0,
            'bond0.rx_multicast': 8481087.9,
            'bond0.rx_packets': 264585067.9,
            'bond0.tx_bit': 1569889402921.6,
            'bond0.tx_compressed': 0.0,
            'bond0.tx_drop': 0.0,
            'bond0.tx_errors': 0.0,
            'bond0.tx_fifo': 0.0,
            'bond0.tx_frame': 0.0,
            'bond0.tx_multicast': 0.0,
            'bond0.tx_packets': 200109891.6,
            'bond1.rx_bit': 16933606875970.4,
            'bond1.rx_compressed': 0.0,
            'bond1.rx_drop': 0.0,
            'bond1.rx_errors': 0.0,
            'bond1.rx_fifo': 0.0,
            'bond1.rx_frame': 0.0,
            'bond1.rx_multicast': 7.8,
            'bond1.rx_packets': 2419703159.9,
            'bond1.tx_bit': 17842573410005.6,
            'bond1.tx_compressed': 0.0,
            'bond1.tx_drop': 0.0,
            'bond1.tx_errors': 0.0,
            'bond1.tx_fifo': 0.0,
            'bond1.tx_frame': 0.0,
            'bond1.tx_multicast': 0.0,
            'bond1.tx_packets': 2654259261.0,
            'em1.rx_bit': 2687881969344.8,
            'em1.rx_compressed': 0.0,
            'em1.rx_drop': 0.0,
            'em1.rx_errors': 0.0,
            'em1.rx_fifo': 0.0,
            'em1.rx_frame': 0.0,
            'em1.rx_multicast': 8471878.8,
            'em1.rx_packets': 264382058.1,
            'em1.tx_bit': 1569889402921.6,
            'em1.tx_compressed': 0.0,
            'em1.tx_drop': 0.0,
            'em1.tx_errors': 0.0,
            'em1.tx_fifo': 0.0,
            'em1.tx_frame': 0.0,
            'em1.tx_multicast': 0.0,
            'em1.tx_packets': 200109891.6,
            'em2.rx_bit': 97450083.2,
            'em2.rx_compressed': 0.0,
            'em2.rx_drop': 0.0,
            'em2.rx_errors': 0.0,
            'em2.rx_fifo': 0.0,
            'em2.rx_frame': 0.0,
            'em2.rx_multicast': 9209.1,
            'em2.rx_packets': 203009.8,
            'em2.tx_bit': 0,
            'em2.tx_compressed': 0.0,
            'em2.tx_drop': 0.0,
            'em2.tx_errors': 0.0,
            'em2.tx_fifo': 0.0,
            'em2.tx_frame': 0.0,
            'em2.tx_multicast': 0.0,
            'em2.tx_packets': 0.0,
            'em3.rx_bit': 514398.4,
            'em3.rx_compressed': 0.0,
            'em3.rx_drop': 0.0,
            'em3.rx_errors': 0.0,
            'em3.rx_fifo': 0.0,
            'em3.rx_frame': 0.0,
            'em3.rx_multicast': 0.0,
            'em3.rx_packets': 1071.6,
            'em3.tx_bit': 0.0,
            'em3.tx_compressed': 0.0,
            'em3.tx_drop': 0.0,
            'em3.tx_errors': 0.0,
            'em3.tx_fifo': 0.0,
            'em3.tx_frame': 0.0,
            'em3.tx_multicast': 0.0,
            'em3.tx_packets': 0.0,
            'em4.rx_bit': 16933606361572.0,
            'em4.rx_compressed': 0.0,
            'em4.rx_drop': 0.0,
            'em4.rx_errors': 0.0,
            'em4.rx_fifo': 0.0,
            'em4.rx_frame': 0.0,
            'em4.rx_multicast': 7.8,
            'em4.rx_packets': 2419702088.3,
            'em4.tx_bit': 17842573410005.6,
            'em4.tx_compressed': 0.0,
            'em4.tx_drop': 0.0,
            'em4.tx_errors': 0.0,
            'em4.tx_fifo': 0.0,
            'em4.tx_frame': 0.0,
            'em4.tx_multicast': 0.0,
            'em4.tx_packets': 2654259261.0,
        }

        self.assertPublishedMany(publish_mock, metrics)

    # Named test_z_* to run after test_should_open_proc_net_dev
    @patch.object(Collector, 'publish')
    def test_z_issue_208_b(self, publish_mock):
        NetworkCollector.PROC = self.getFixturePath('208-b_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        NetworkCollector.PROC = self.getFixturePath('208-b_2')
        self.collector.collect()

        metrics = {
            'bond0.rx_bit': 12754357408.8,
            'bond0.rx_compressed': 0.0,
            'bond0.rx_drop': 0.0,
            'bond0.rx_errors': 0.0,
            'bond0.rx_fifo': 0.0,
            'bond0.rx_frame': 0.0,
            'bond0.rx_multicast': 8483853.6,
            'bond0.rx_packets': 13753449.5,
            'bond0.tx_bit': 51593345279.2,
            'bond0.tx_compressed': 0.0,
            'bond0.tx_drop': 0.0,
            'bond0.tx_errors': 0.0,
            'bond0.tx_fifo': 0.0,
            'bond0.tx_frame': 0.0,
            'bond0.tx_multicast': 0.0,
            'bond0.tx_packets': 58635426.6,
            'bond1.rx_bit': 48298217736175.2,
            'bond1.rx_compressed': 0.0,
            'bond1.rx_drop': 0.0,
            'bond1.rx_errors': 0.0,
            'bond1.rx_fifo': 473.8,
            'bond1.rx_frame': 0.0,
            'bond1.rx_multicast': 2.9,
            'bond1.rx_packets': 4869871086.2,
            'bond1.tx_bit': 23149038213964.0,
            'bond1.tx_compressed': 0.0,
            'bond1.tx_drop': 0.0,
            'bond1.tx_errors': 0.0,
            'bond1.tx_fifo': 0.0,
            'bond1.tx_frame': 0.0,
            'bond1.tx_multicast': 0.0,
            'bond1.tx_packets': 2971941537.3,
            'em1.rx_bit': 12657057999.2,
            'em1.rx_compressed': 0.0,
            'em1.rx_drop': 0.0,
            'em1.rx_errors': 0.0,
            'em1.rx_fifo': 0.0,
            'em1.rx_frame': 0.0,
            'em1.rx_multicast': 8474644.4,
            'em1.rx_packets': 13550781.5,
            'em1.tx_bit': 51593345279.2,
            'em1.tx_compressed': 0.0,
            'em1.tx_drop': 0.0,
            'em1.tx_errors': 0.0,
            'em1.tx_fifo': 0.0,
            'em1.tx_frame': 0.0,
            'em1.tx_multicast': 0.0,
            'em1.tx_packets': 58635426.6,
            'em2.rx_bit': 97299409.6,
            'em2.rx_compressed': 0.0,
            'em2.rx_drop': 0.0,
            'em2.rx_errors': 0.0,
            'em2.rx_fifo': 0.0,
            'em2.rx_frame': 0.0,
            'em2.rx_multicast': 9209.2,
            'em2.rx_packets': 202668.0,
            'em2.tx_bit': 0,
            'em2.tx_compressed': 0.0,
            'em2.tx_drop': 0.0,
            'em2.tx_errors': 0.0,
            'em2.tx_fifo': 0.0,
            'em2.tx_frame': 0.0,
            'em2.tx_multicast': 0.0,
            'em2.tx_packets': 0.0,
            'em3.rx_bit': 48298184648012.0,
            'em3.rx_compressed': 0.0,
            'em3.rx_drop': 0.0,
            'em3.rx_errors': 0.0,
            'em3.rx_fifo': 473.8,
            'em3.rx_frame': 0.0,
            'em3.rx_multicast': 2.9,
            'em3.rx_packets': 4869866440.5,
            'em3.tx_bit': 23149038213964.0,
            'em3.tx_compressed': 0.0,
            'em3.tx_drop': 0.0,
            'em3.tx_errors': 0.0,
            'em3.tx_fifo': 0.0,
            'em3.tx_frame': 0.0,
            'em3.tx_multicast': 0.0,
            'em3.tx_packets': 2971941537.3,
            'em4.rx_bit': 33088163.2,
            'em4.rx_compressed': 0.0,
            'em4.rx_drop': 0.0,
            'em4.rx_errors': 0.0,
            'em4.rx_fifo': 0.0,
            'em4.rx_frame': 0.0,
            'em4.rx_multicast': 0.0,
            'em4.rx_packets': 4645.7,
            'em4.tx_bit': 0,
            'em4.tx_compressed': 0.0,
            'em4.tx_drop': 0.0,
            'em4.tx_errors': 0.0,
            'em4.tx_fifo': 0.0,
            'em4.tx_frame': 0.0,
            'em4.tx_multicast': 0.0,
            'em4.tx_packets': 0.0,
        }

        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = nfs
# coding=utf-8

"""
The NfsCollector collects nfs utilization metrics using /proc/net/rpc/nfs.

#### Dependencies

 * /proc/net/rpc/nfs

"""

import diamond.collector
import os


class NfsCollector(diamond.collector.Collector):

    PROC = '/proc/net/rpc/nfs'

    def get_default_config_help(self):
        config_help = super(NfsCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(NfsCollector, self).get_default_config()
        config.update({
            'enabled':  False,
            'path':     'nfs'
        })
        return config

    def collect(self):
        """
        Collect stats
        """
        if os.access(self.PROC, os.R_OK):

            results = {}
            # Open file
            file = open(self.PROC)

            for line in file:
                line = line.split()

                if line[0] == 'net':
                    results['net.packets'] = line[1]
                    results['net.udpcnt'] = line[2]
                    results['net.tcpcnt'] = line[3]
                    results['net.tcpconn'] = line[4]
                elif line[0] == 'rpc':
                    results['rpc.calls'] = line[1]
                    results['rpc.retrans'] = line[2]
                    results['rpc.authrefrsh'] = line[3]
                elif line[0] == 'proc2':
                    results['v2.null'] = line[1]
                    results['v2.getattr'] = line[2]
                    results['v2.setattr'] = line[3]
                    results['v2.root'] = line[4]
                    results['v2.lookup'] = line[5]
                    results['v2.readlink'] = line[6]
                    results['v2.read'] = line[7]
                    results['v2.wrcache'] = line[8]
                    results['v2.write'] = line[9]
                    results['v2.create'] = line[10]
                    results['v2.remove'] = line[11]
                    results['v2.rename'] = line[12]
                    results['v2.link'] = line[13]
                    results['v2.symlink'] = line[14]
                    results['v2.mkdir'] = line[15]
                    results['v2.rmdir'] = line[16]
                    results['v2.readdir'] = line[17]
                    results['v2.fsstat'] = line[18]
                elif line[0] == 'proc3':
                    results['v3.null'] = line[1]
                    results['v3.getattr'] = line[2]
                    results['v3.setattr'] = line[3]
                    results['v3.lookup'] = line[4]
                    results['v3.access'] = line[5]
                    results['v3.readlink'] = line[6]
                    results['v3.read'] = line[7]
                    results['v3.write'] = line[8]
                    results['v3.create'] = line[9]
                    results['v3.mkdir'] = line[10]
                    results['v3.symlink'] = line[11]
                    results['v3.mknod'] = line[12]
                    results['v3.remove'] = line[13]
                    results['v3.rmdir'] = line[14]
                    results['v3.rename'] = line[15]
                    results['v3.link'] = line[16]
                    results['v3.readdir'] = line[17]
                    results['v3.readdirplus'] = line[18]
                    results['v3.fsstat'] = line[19]
                    results['v3.fsinfo'] = line[20]
                    results['v3.pathconf'] = line[21]
                    results['v3.commit'] = line[22]
                elif line[0] == 'proc4':
                    results['v4.null'] = line[1]
                    results['v4.read'] = line[2]
                    results['v4.write'] = line[3]
                    results['v4.commit'] = line[4]
                    results['v4.open'] = line[5]
                    results['v4.open_conf'] = line[6]
                    results['v4.open_noat'] = line[7]
                    results['v4.open_dgrd'] = line[8]
                    results['v4.close'] = line[9]
                    results['v4.setattr'] = line[10]
                    results['v4.fsinfo'] = line[11]
                    results['v4.renew'] = line[12]
                    results['v4.setclntid'] = line[13]
                    results['v4.confirm'] = line[14]
                    results['v4.lock'] = line[15]
                    results['v4.lockt'] = line[16]
                    results['v4.locku'] = line[17]
                    results['v4.access'] = line[18]
                    results['v4.getattr'] = line[19]
                    results['v4.lookup'] = line[20]
                    results['v4.lookup_root'] = line[21]
                    results['v4.remove'] = line[22]
                    results['v4.rename'] = line[23]
                    results['v4.link'] = line[24]
                    results['v4.symlink'] = line[25]
                    results['v4.create'] = line[26]
                    results['v4.pathconf'] = line[27]
                    results['v4.statfs'] = line[28]
                    results['v4.readlink'] = line[29]
                    results['v4.readdir'] = line[30]
                    try:
                        results['v4.server_caps'] = line[31]
                    except IndexError:
                        pass
                    try:
                        results['v4.delegreturn'] = line[32]
                    except IndexError:
                        pass
                    try:
                        results['v4.getacl'] = line[33]
                    except IndexError:
                        pass
                    try:
                        results['v4.setacl'] = line[34]
                    except IndexError:
                        pass
                    try:
                        results['v4.fs_locations'] = line[35]
                    except IndexError:
                        pass
                    try:
                        results['v4.rel_lkowner'] = line[36]
                    except IndexError:
                        pass
                    try:
                        results['v4.exchange_id'] = line[37]
                    except IndexError:
                        pass
                    try:
                        results['v4.create_ses'] = line[38]
                    except IndexError:
                        pass
                    try:
                        results['v4.destroy_ses'] = line[39]
                    except IndexError:
                        pass
                    try:
                        results['v4.sequence'] = line[40]
                    except IndexError:
                        pass
                    try:
                        results['v4.get_lease_t'] = line[41]
                    except IndexError:
                        pass
                    try:
                        results['v4.reclaim_comp'] = line[42]
                    except IndexError:
                        pass
                    try:
                        results['v4.layoutget'] = line[43]
                    except IndexError:
                        pass
                    try:
                        results['v4.layoutcommit'] = line[44]
                    except IndexError:
                        pass
                    try:
                        results['v4.layoutreturn'] = line[45]
                    except IndexError:
                        pass
                    try:
                        results['v4.getdevlist'] = line[46]
                    except IndexError:
                        pass
                    try:
                        results['v4.getdevinfo'] = line[47]
                    except IndexError:
                        pass
                    try:
                        results['v4.ds_write'] = line[48]
                    except IndexError:
                        pass
                    try:
                        results['v4.ds_commit'] = line[49]
                    except IndexError:
                        pass
                    try:
                        results['v4.getdevlist'] = line[50]
                    except IndexError:
                        pass

            # Close File
            file.close()

            for stat in results.keys():
                metric_name = '.' + stat
                metric_value = long(float(results[stat]))
                metric_value = self.derivative(metric_name, metric_value)
                self.publish(metric_name, metric_value)
            return True

        return False

########NEW FILE########
__FILENAME__ = testnfs
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from nfs import NfsCollector

################################################################################


class TestNfsCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('NfsCollector', {
            'interval': 1
        })

        self.collector = NfsCollector(config, None)

    def test_import(self):
        self.assertTrue(NfsCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_stat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/net/rpc/nfs')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        #NfsCollector.PROC = self.getFixturePath('proc_nfs_1')
        #self.collector.collect()

        #self.assertPublishedMany(publish_mock, {})

        #NfsCollector.PROC = self.getFixturePath('proc_nfs_2')
        #self.collector.collect()

        metrics = {
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        #self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = nfsd
# coding=utf-8

"""
The NfsdCollector collects nfsd utilization metrics using /proc/net/rpc/nfsd.

#### Dependencies

 * /proc/net/rpc/nfsd

"""

import diamond.collector
import os


class NfsdCollector(diamond.collector.Collector):

    PROC = '/proc/net/rpc/nfsd'

    def get_default_config_help(self):
        config_help = super(NfsdCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(NfsdCollector, self).get_default_config()
        config.update({
            'enabled':  False,
            'path':     'nfsd'
        })
        return config

    def collect(self):
        """
        Collect stats
        """
        if os.access(self.PROC, os.R_OK):

            results = {}
            # Open file
            file = open(self.PROC)

            for line in file:
                line = line.split()

                if line[0] == 'rc':
                    results['reply_cache.hits'] = line[1]
                    results['reply_cache.misses'] = line[2]
                    results['reply_cache.nocache'] = line[3]
                elif line[0] == 'fh':
                    results['filehandle.stale'] = line[1]
                    results['filehandle.total-lookups'] = line[2]
                    results['filehandle.anonlookups'] = line[3]
                    results['filehandle.dir-not-in-cache'] = line[4]
                    results['filehandle.nodir-not-in-cache'] = line[5]
                elif line[0] == 'io':
                    results['input_output.bytes-read'] = line[1]
                    results['input_output.bytes-written'] = line[2]
                elif line[0] == 'th':
                    results['threads.threads'] = line[1]
                    results['threads.fullcnt'] = line[2]
                    results['threads.10-20-pct'] = line[3]
                    results['threads.20-30-pct'] = line[4]
                    results['threads.30-40-pct'] = line[5]
                    results['threads.40-50-pct'] = line[6]
                    results['threads.50-60-pct'] = line[7]
                    results['threads.60-70-pct'] = line[8]
                    results['threads.70-80-pct'] = line[9]
                    results['threads.80-90-pct'] = line[10]
                    results['threads.90-100-pct'] = line[11]
                    results['threads.100-pct'] = line[12]
                elif line[0] == 'ra':
                    results['read-ahead.cache-size'] = line[1]
                    results['read-ahead.10-pct'] = line[2]
                    results['read-ahead.20-pct'] = line[3]
                    results['read-ahead.30-pct'] = line[4]
                    results['read-ahead.40-pct'] = line[5]
                    results['read-ahead.50-pct'] = line[6]
                    results['read-ahead.60-pct'] = line[7]
                    results['read-ahead.70-pct'] = line[8]
                    results['read-ahead.80-pct'] = line[9]
                    results['read-ahead.90-pct'] = line[10]
                    results['read-ahead.100-pct'] = line[11]
                    results['read-ahead.not-found'] = line[12]
                elif line[0] == 'net':
                    results['net.cnt'] = line[1]
                    results['net.udpcnt'] = line[2]
                    results['net.tcpcnt'] = line[3]
                    results['net.tcpconn'] = line[4]
                elif line[0] == 'rpc':
                    results['rpc.cnt'] = line[1]
                    results['rpc.badfmt'] = line[2]
                    results['rpc.badauth'] = line[3]
                    results['rpc.badclnt'] = line[4]
                elif line[0] == 'proc2':
                    results['v2.unknown'] = line[1]
                    results['v2.null'] = line[2]
                    results['v2.getattr'] = line[3]
                    results['v2.setattr'] = line[4]
                    results['v2.root'] = line[5]
                    results['v2.lookup'] = line[6]
                    results['v2.readlink'] = line[7]
                    results['v2.read'] = line[8]
                    results['v2.wrcache'] = line[9]
                    results['v2.write'] = line[10]
                    results['v2.create'] = line[11]
                    results['v2.remove'] = line[12]
                    results['v2.rename'] = line[13]
                    results['v2.link'] = line[14]
                    results['v2.symlink'] = line[15]
                    results['v2.mkdir'] = line[16]
                    results['v2.rmdir'] = line[17]
                    results['v2.readdir'] = line[18]
                    results['v2.fsstat'] = line[19]
                elif line[0] == 'proc3':
                    results['v3.unknown'] = line[1]
                    results['v3.null'] = line[2]
                    results['v3.getattr'] = line[3]
                    results['v3.setattr'] = line[4]
                    results['v3.lookup'] = line[5]
                    results['v3.access'] = line[6]
                    results['v3.readlink'] = line[7]
                    results['v3.read'] = line[8]
                    results['v3.write'] = line[9]
                    results['v3.create'] = line[10]
                    results['v3.mkdir'] = line[11]
                    results['v3.symlink'] = line[12]
                    results['v3.mknod'] = line[13]
                    results['v3.remove'] = line[14]
                    results['v3.rmdir'] = line[15]
                    results['v3.rename'] = line[16]
                    results['v3.link'] = line[17]
                    results['v3.readdir'] = line[18]
                    results['v3.readdirplus'] = line[19]
                    results['v3.fsstat'] = line[20]
                    results['v3.fsinfo'] = line[21]
                    results['v3.pathconf'] = line[22]
                    results['v3.commit'] = line[23]
                elif line[0] == 'proc4':
                    results['v4.unknown'] = line[1]
                    results['v4.null'] = line[2]
                    results['v4.compound'] = line[3]
                elif line[0] == 'proc4ops':
                    results['v4.ops.unknown'] = line[1]
                    results['v4.ops.op0-unused'] = line[2]
                    results['v4.ops.op1-unused'] = line[3]
                    results['v4.ops.op2-future'] = line[4]
                    results['v4.ops.access'] = line[5]
                    results['v4.ops.close'] = line[6]
                    results['v4.ops.commit'] = line[7]
                    results['v4.ops.create'] = line[8]
                    results['v4.ops.delegpurge'] = line[9]
                    results['v4.ops.delegreturn'] = line[10]
                    results['v4.ops.getattr'] = line[11]
                    results['v4.ops.getfh'] = line[12]
                    results['v4.ops.link'] = line[13]
                    results['v4.ops.lock'] = line[14]
                    results['v4.ops.lockt'] = line[15]
                    results['v4.ops.locku'] = line[16]
                    results['v4.ops.lookup'] = line[17]
                    results['v4.ops.lookup_root'] = line[18]
                    results['v4.ops.nverify'] = line[19]
                    results['v4.ops.open'] = line[20]
                    results['v4.ops.openattr'] = line[21]
                    results['v4.ops.open_conf'] = line[22]
                    results['v4.ops.open_dgrd'] = line[23]
                    results['v4.ops.putfh'] = line[24]
                    results['v4.ops.putpubfh'] = line[25]
                    results['v4.ops.putrootfh'] = line[26]
                    results['v4.ops.read'] = line[27]
                    results['v4.ops.readdir'] = line[28]
                    results['v4.ops.readlink'] = line[29]
                    results['v4.ops.remove'] = line[30]
                    results['v4.ops.rename'] = line[31]
                    results['v4.ops.renew'] = line[32]
                    results['v4.ops.restorefh'] = line[33]
                    results['v4.ops.savefh'] = line[34]
                    results['v4.ops.secinfo'] = line[35]
                    results['v4.ops.setattr'] = line[36]
                    results['v4.ops.setcltid'] = line[37]
                    results['v4.ops.setcltidconf'] = line[38]
                    results['v4.ops.verify'] = line[39]
                    results['v4.ops.write'] = line[40]
                    results['v4.ops.rellockowner'] = line[41]

            # Close File
            file.close()

            for stat in results.keys():
                metric_name = '.' + stat
                metric_value = long(float(results[stat]))
                metric_value = self.derivative(metric_name, metric_value)
                self.publish(metric_name, metric_value)
            return True

        return False

########NEW FILE########
__FILENAME__ = testnfsd
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from nfsd import NfsdCollector

################################################################################


class TestNfsdCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('NfsdCollector', {
            'interval': 1
        })

        self.collector = NfsdCollector(config, None)

    def test_import(self):
        self.assertTrue(NfsdCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_stat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/net/rpc/nfsd')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        NfsdCollector.PROC = self.getFixturePath('proc_nfsd_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        NfsdCollector.PROC = self.getFixturePath('proc_nfsd_2')
        self.collector.collect()

        metrics = {
            '.input_output.bytes-read': 3139369493.0,
            '.input_output.bytes-written': 15691669.0,
            '.net.cnt': 14564086.0,
            '.net.tcpcnt': 14562696.0,
            '.net.tcpconn': 30773.0,
            '.read-ahead.10-pct': 8751152.0,
            '.read-ahead.cache-size': 32.0,
            '.read-ahead.not-found': 18612.0,
            '.reply_cache.misses': 71080.0,
            '.reply_cache.nocache': 14491982.0,
            '.rpc.cnt': 14563007.0,
            '.threads.10-20-pct': 22163.0,
            '.threads.100-pct': 22111.0,
            '.threads.20-30-pct': 8448.0,
            '.threads.30-40-pct': 1642.0,
            '.threads.50-60-pct': 5072.0,
            '.threads.60-70-pct': 1210.0,
            '.threads.70-80-pct': 3889.0,
            '.threads.80-90-pct': 2654.0,
            '.threads.fullcnt': 1324492.0,
            '.threads.threads': 8.0,
            '.v2.unknown': 18.0,
            '.v3.access': 136921.0,
            '.v3.commit': 635.0,
            '.v3.create': 1655.0,
            '.v3.fsinfo': 11.0,
            '.v3.fsstat': 34450.0,
            '.v3.getattr': 724974.0,
            '.v3.lookup': 213165.0,
            '.v3.null': 8.0,
            '.v3.read': 8761683.0,
            '.v3.readdir': 11295.0,
            '.v3.readdirplus': 132298.0,
            '.v3.remove': 1488.0,
            '.v3.unknown': 22.0,
            '.v3.write': 67937.0,
            '.v4.compound': 4476320.0,
            '.v4.null': 18.0,
            '.v4.ops.access': 2083822.0,
            '.v4.ops.close': 34801.0,
            '.v4.ops.commit': 3955.0,
            '.v4.ops.getattr': 2302848.0,
            '.v4.ops.getfh': 51791.0,
            '.v4.ops.lookup': 68501.0,
            '.v4.ops.open': 34847.0,
            '.v4.ops.open_conf': 29002.0,
            '.v4.ops.putfh': 4435270.0,
            '.v4.ops.putrootfh': 6237.0,
            '.v4.ops.read': 8030.0,
            '.v4.ops.readdir': 272.0,
            '.v4.ops.remove': 7802.0,
            '.v4.ops.renew': 28594.0,
            '.v4.ops.restorefh': 34839.0,
            '.v4.ops.savefh': 34847.0,
            '.v4.ops.setattr': 7870.0,
            '.v4.ops.setcltid': 6226.0,
            '.v4.ops.setcltidconf': 6227.0,
            '.v4.ops.unknown': 40.0,
            '.v4.ops.write': 76562.0,
            '.v4.unknown': 2.0
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = nginx
# coding=utf-8

"""
Collect statistics from Nginx

#### Dependencies

 * urllib2

#### Usage

To enable the nginx status page to work with defaults,
add a file to /etc/nginx/sites-enabled/ (on Ubuntu) with the
following content:
<pre>
  server {
      listen 127.0.0.1:8080;
      server_name localhost;
      location /nginx_status {
          stub_status on;
          access_log /data/server/shared/log/access.log;
          allow 127.0.0.1;
          deny all;
      }
  }
</pre>

"""

import urllib2
import re
import diamond.collector


class NginxCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(NginxCollector, self).get_default_config_help()
        config_help.update({
            'req_host': 'Hostname',
            'req_port': 'Port',
            'req_path': 'Path',
        })
        return config_help

    def get_default_config(self):
        default_config = super(NginxCollector, self).get_default_config()
        default_config['req_host'] = 'localhost'
        default_config['req_port'] = 8080
        default_config['req_path'] = '/nginx_status'
        default_config['path'] = 'nginx'
        return default_config

    def collect(self):
        activeConnectionsRE = re.compile(r'Active connections: (?P<conn>\d+)')
        totalConnectionsRE = re.compile('^\s+(?P<conn>\d+)\s+'
                                        + '(?P<acc>\d+)\s+(?P<req>\d+)')
        connectionStatusRE = re.compile('Reading: (?P<reading>\d+) '
                                        + 'Writing: (?P<writing>\d+) '
                                        + 'Waiting: (?P<waiting>\d+)')
        req = urllib2.Request('http://%s:%i%s' % (self.config['req_host'],
                                                  int(self.config['req_port']),
                                                  self.config['req_path']))
        try:
            handle = urllib2.urlopen(req)
            for l in handle.readlines():
                l = l.rstrip('\r\n')
                if activeConnectionsRE.match(l):
                    self.publish_gauge(
                        'active_connections',
                        int(activeConnectionsRE.match(l).group('conn')))
                elif totalConnectionsRE.match(l):
                    m = totalConnectionsRE.match(l)
                    req_per_conn = float(m.group('req')) / float(m.group('acc'))
                    self.publish_counter('conn_accepted', int(m.group('conn')))
                    self.publish_counter('conn_handled', int(m.group('acc')))
                    self.publish_counter('req_handled', int(m.group('req')))
                    self.publish_gauge('req_per_conn', float(req_per_conn))
                elif connectionStatusRE.match(l):
                    m = connectionStatusRE.match(l)
                    self.publish_gauge('act_reads', int(m.group('reading')))
                    self.publish_gauge('act_writes', int(m.group('writing')))
                    self.publish_gauge('act_waits', int(m.group('waiting')))
        except IOError, e:
            self.log.error("Unable to open http://%s:%i%s",
                           self.config['req_host'],
                           int(self.config['req_port']),
                           self.config['req_path'])
        except Exception, e:
            self.log.error("Unknown error opening url: %s", e)

########NEW FILE########
__FILENAME__ = testnginx
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from nginx import NginxCollector

################################################################################


class TestNginxCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('NginxCollector', {})

        self.collector = NginxCollector(config, None)

    def test_import(self):
        self.assertTrue(NginxCollector)

    @patch.object(Collector, 'publish')
    @patch.object(Collector, 'publish_gauge')
    @patch.object(Collector, 'publish_counter')
    def test_should_work_with_real_data(self, publish_counter_mock,
                                        publish_gauge_mock, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('status')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'active_connections': 3,
            'conn_accepted': 396396,
            'conn_handled': 396396,
            'req_handled': 396396,
            'act_reads': 2,
            'act_writes': 1,
            'act_waits': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany([publish_mock,
                                  publish_gauge_mock,
                                  publish_counter_mock
                                  ], metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('status_blank')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ntpd
# coding=utf-8

"""
Collect stats from ntpd

#### Dependencies

    * subprocess

"""

import subprocess

import diamond.collector
from diamond.collector import str_to_bool


class NtpdCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(NtpdCollector, self).get_default_config_help()
        config_help.update({
            'ntpq_bin':     'Path to ntpq binary',
            'ntpdc_bin':    'Path to ntpdc binary',
            'use_sudo':     'Use sudo?',
            'sudo_cmd':     'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(NtpdCollector, self).get_default_config()
        config.update({
            'path':         'ntpd',
            'ntpq_bin':     self.find_binary('/usr/bin/ntpq'),
            'ntpdc_bin':    self.find_binary('/usr/bin/ntpdc'),
            'use_sudo':     False,
            'sudo_cmd':     self.find_binary('/usr/bin/sudo'),
        })
        return config

    def run_command(self, command):
        try:
            if str_to_bool(self.config['use_sudo']):
                command.insert(0, self.config['sudo_cmd'])

            return subprocess.Popen(command,
                                    stdout=subprocess.PIPE).communicate()[0]
        except OSError:
            self.log.exception("Unable to run %s", command)
            return ""

    def get_ntpq_output(self):
        return self.run_command([self.config['ntpq_bin'], '-np'])

    def get_ntpq_stats(self):
        output = self.get_ntpq_output()

        data = {}

        for line in output.splitlines():
            # Only care about system peer
            if not line.startswith('*'):
                continue

            parts = line[1:].split()

            data['stratum'] = parts[2]
            data['when'] = parts[4]
            if data['when'] == '-':
                # sometimes, ntpq returns value '-' for 'when', continuos
                # and try other system peer
                continue
            data['poll'] = parts[5]
            data['reach'] = parts[6]
            data['delay'] = parts[7]
            data['jitter'] = parts[9]

        def convert_to_second(when_ntpd_ouput):
            value = float(when_ntpd_ouput[:-1])
            if when_ntpd_ouput.endswith('m'):
                return value * 60
            elif when_ntpd_ouput.endswith('h'):
                return value * 3600
            elif when_ntpd_ouput.endswith('d'):
                return value * 86400

        if 'when' in data:
            if data['when'] == '-':
                self.log.warning('ntpq returned bad value for "when"')
                return []

            if data['when'].endswith(('m', 'h', 'd')):
                data['when'] = convert_to_second(data['when'])

        return data.items()

    def get_ntpdc_output(self):
        return self.run_command([self.config['ntpdc_bin'], '-c', 'kerninfo'])

    def get_ntpdc_stats(self):
        output = self.get_ntpdc_output()

        data = {}

        for line in output.splitlines():
            key, val = line.split(':')
            val = float(val.split()[0])

            if key == 'pll offset':
                data['offset'] = val
            elif key == 'pll frequency':
                data['frequency'] = val
            elif key == 'maximum error':
                data['max_error'] = val
            elif key == 'estimated error':
                data['est_error'] = val

        return data.items()

    def collect(self):
        for stat, val in self.get_ntpq_stats():
            self.publish(stat, val)

        for stat, val in self.get_ntpdc_stats():
            self.publish(stat, val)

########NEW FILE########
__FILENAME__ = testntpd
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from ntpd import NtpdCollector

################################################################################


class TestNtpdCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('NtpdCollector', {})

        self.collector = NtpdCollector(config, None)

    def test_import(self):
        self.assertTrue(NtpdCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_wtih_real_data(self, publish_mock):
        ntpq_data = Mock(return_value=self.getFixture('ntpq').getvalue())
        ntpdc_data = Mock(return_value=self.getFixture('ntpdc').getvalue())
        collector_mock = patch.multiple(NtpdCollector,
                                        get_ntpq_output=ntpq_data,
                                        get_ntpdc_output=ntpdc_data)

        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        metrics = {
            'jitter': 0.026,
            'when': 39,
            'stratum': 2,
            'reach': 377,
            'delay': 0.127,
            'offset': -0.005,
            'poll': 1024,
            'max_error': 0.039793,
            'est_error': 5.1e-05,
            'frequency': -14.24,
            'offset': -5.427e-06,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        ntpq_data = Mock(return_value='')
        ntpdc_data = Mock(return_value='')
        collector_mock = patch.multiple(NtpdCollector,
                                        get_ntpq_output=ntpq_data,
                                        get_ntpdc_output=ntpdc_data)

        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = onewire
# coding=utf-8

"""
The OneWireCollector collects data from 1-Wire Filesystem

You can configure which sensors are read in two way:

-  add section [scan] with attributes and aliases,
   (collector will scan owfs to find attributes)

or

- add sections with format id:$SENSOR_ID

See also: http://owfs.org/
Author: Tomasz Prus

#### Dependencies

 * owfs

"""

import os
import diamond.collector


class OneWireCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(OneWireCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(OneWireCollector, self).get_default_config()
        config.update({
            'path': 'owfs',
            'owfs': '/mnt/1wire',
            #'scan': {'temperature': 't'},
            #'id:24.BB000000': {'file_with_value': 'alias'},
        })
        return config

    def collect(self):
        """
        Overrides the Collector.collect method
        """

        metrics = {}

        if 'scan' in self.config:
            for ld in os.listdir(self.config['owfs']):
                if '.' in ld:
                    self.read_values(ld, self.config['scan'], metrics)

        for oid, files in self.config.iteritems():
            if oid[:3] == 'id:':
                self.read_values(oid[3:], files, metrics)

        for fn, fv in metrics.iteritems():
            self.publish(fn, fv, 2)

    def read_values(self, oid, files, metrics):
        """
        Reads values from owfs/oid/{files} and update
        metrics with format [oid.alias] = value
        """

        oid_path = os.path.join(self.config['owfs'], oid)
        oid = oid.replace('.', '_')

        for fn, alias in files.iteritems():
            fv = os.path.join(oid_path, fn)
            if os.path.isfile(fv):
                try:
                    f = open(fv)
                    v = f.read()
                    f.close()
                except:
                    self.log.error("Unable to read %s", fv)
                    raise

                try:
                    v = float(v)
                except:
                    self.log.error("Unexpected value %s in %s", v, fv)
                    raise

                metrics["%s.%s" % (oid, alias)] = v

########NEW FILE########
__FILENAME__ = testonewire
#!/usr/bin/python
# coding=utf-8

###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import patch

from diamond.collector import Collector
from onewire import OneWireCollector

###############################################################################


class TestOneWireCollector(CollectorTestCase):

    def setUp(self):
        config = get_collector_config('OneWireCollector', {
            'owfs': self.getFixturePath('.'),
            'scan': {'temperature': 't'},
            'id:28.2F702A010000': {'presure': 'p11'}})
        self.collector = OneWireCollector(config, None)

    def test_import(self):
        self.assertTrue(OneWireCollector)

    @patch.object(Collector, 'publish')
    def test(self, publish_mock):

        self.collector.collect()

        metrics = {
            '28_A76569020000.t': 22.4375,
            '28_2F702A010000.p11': 999
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)


###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = openstackswift
# coding=utf-8

"""
Openstack swift collector.

#### Dependencies

 * swift-dispersion-report commandline tool (for dispersion report)
   if using this, make sure swift.conf and dispersion.conf are reable by diamond
   also get an idea of the runtime of a swift-dispersion-report call and make
   sure the collect interval is high enough to avoid contention.
 * swift commandline tool (for container_metrics)

both of these should come installed with swift
"""

import diamond.collector
from subprocess import Popen, PIPE

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json


class OpenstackSwiftCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(OpenstackSwiftCollector,
                            self).get_default_config_help()
        config_help.update({
            'enable_dispersion_report': 'gather swift-dispersion-report metrics'
            + ' (default False)',
            'enable_container_metrics': 'gather containers metrics'
            + '(# objects, bytes used, x_timestamp.  default True)',
            'auth_url': 'authentication url (for enable_container_metrics)',
            'account': 'swift auth account (for enable_container_metrics)',
            'user': 'swift auth user (for enable_container_metrics)',
            'password': 'swift auth password (for enable_container_metrics)',
            'containers': 'containers on which to count number of objects, '
            + 'space separated list (for enable_container_metrics)'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(OpenstackSwiftCollector, self).get_default_config()
        config.update({
            'path': 'openstackswift',
            'enable_dispersion_report': False,
            'enable_container_metrics': True,
            # don't use the threaded model with this one.
            # for some reason it crashes.
            'interval': 1200,  # by default, every 20 minutes
        })
        return config

    def collect(self):
        # dispersion report.  this can take easily >60s. beware!
        if (self.config['enable_dispersion_report']):
            p = Popen(
                ['swift-dispersion-report', '-j'],
                stdout=PIPE,
                stderr=PIPE)
            stdout, stderr = p.communicate()
            self.publish('dispersion.errors', len(stderr.split('\n')) - 1)
            data = json.loads(stdout)
            for t in ('object', 'container'):
                for (k, v) in data[t].items():
                    self.publish('dispersion.%s.%s' % (t, k), v)

        # container metrics returned by stat <container>
        if(self.config['enable_container_metrics']):
            account = '%s:%s' % (self.config['account'], self.config['user'])
            for container in self.config['containers'].split(','):
                cmd = ['swift', '-A', self.config['auth_url'],
                       '-U', account,
                       '-K', self.config['password'],
                       'stat', container]
                p = Popen(cmd, stdout=PIPE, stderr=PIPE)
                stdout, stderr = p.communicate()
                stats = {}
                # stdout is some lines in 'key   : val' format
                for line in stdout.split('\n'):
                    if line:
                        line = line.split(':', 2)
                        stats[line[0].strip()] = line[1].strip()
                key = 'container_metrics.%s.%s' % (self.config['account'],
                                                   container)
                self.publish('%s.objects' % key, stats['Objects'])
                self.publish('%s.bytes' % key, stats['Bytes'])
                self.publish('%s.x_timestamp' % key, stats['X-Timestamp'])

########NEW FILE########
__FILENAME__ = testopenstackswift
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from openstackswift import OpenstackSwiftCollector


class TestOpenstackSwiftCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('OpenstackSwiftCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = OpenstackSwiftCollector(config, None)

    def test_import(self):
        self.assertTrue(OpenstackSwiftCollector)

########NEW FILE########
__FILENAME__ = openstackswiftrecon
# coding=utf-8

"""
Openstack Swift Recon collector. Reads any present recon cache files and
reports their current metrics.

#### Dependencies

 * Running Swift services must have a recon enabled

"""

import os
try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import diamond.collector


class OpenstackSwiftReconCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(OpenstackSwiftReconCollector,
                            self).get_default_config_help()
        config_help.update({
            'recon_account_cache': 'path to swift recon account cache '
            '(default /var/cache/swift/account.recon)',
            'recon_container_cache': 'path to swift recon container cache '
            '(default /var/cache/swift/container.recon)',
            'recon_object_cache': 'path to swift recon object cache '
            '(default /var/cache/swift/object.recon)'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(OpenstackSwiftReconCollector, self).get_default_config()
        config.update({
            'path': 'swiftrecon',
            'recon_account_cache': '/var/cache/swift/account.recon',
            'recon_container_cache': '/var/cache/swift/container.recon',
            'recon_object_cache': '/var/cache/swift/object.recon',
            'method': 'Threaded',
            'interval': 300,
        })
        return config

    def _process_cache(self, d, path=()):
        """Recusively walk a nested recon cache dict to obtain path/values"""
        for k, v in d.iteritems():
            if not isinstance(v, dict):
                self.metrics.append((path + (k,), v))
            else:
                self._process_cache(v, path + (k,))

    def collect(self):
        self.metrics = []
        recon_cache = {'account': self.config['recon_account_cache'],
                       'container': self.config['recon_container_cache'],
                       'object': self.config['recon_object_cache']}
        for recon_type in recon_cache:
            if not os.access(recon_cache[recon_type], os.R_OK):
                continue
            try:
                f = open(recon_cache[recon_type])
                try:
                    rmetrics = json.loads(f.readlines()[0].strip())
                    self.metrics = []
                    self._process_cache(rmetrics)
                    for k, v in self.metrics:
                        metric_name = '%s.%s' % (recon_type, ".".join(k))
                        if isinstance(v, (int, float)):
                            self.publish(metric_name, v)
                except (ValueError, IndexError):
                    continue
            finally:
                f.close()

########NEW FILE########
__FILENAME__ = testopenstackswiftrecon
#!/usr/bin/python
# coding=utf-8
##########################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest

from mock import patch, Mock

from diamond.collector import Collector
from openstackswiftrecon import OpenstackSwiftReconCollector


class TestOpenstackSwiftReconCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('OpenstackSwiftReconCollector', {
            'allowed_names': allowed_names,
            'interval': 1,
            'recon_object_cache': self.getFixturePath('object.recon'),
            'recon_container_cache': self.getFixturePath('container.recon'),
            'recon_account_cache': self.getFixturePath('account.recon')
        })
        self.collector = OpenstackSwiftReconCollector(config, None)

    def test_import(self):
        self.assertTrue(OpenstackSwiftReconCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=False))
    @patch.object(Collector, 'publish')
    def test_recon_no_access(self, publish_mock, open_mock):
        self.assertFalse(open_mock.called)
        self.assertFalse(publish_mock.called)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_recon_publish(self, publish_mock):
        self.collector.collect()
        metrics = {'object.object_replication_time': 2409.806068432331,
                   'object.object_auditor_stats_ALL.passes': 43887,
                   'object.object_auditor_stats_ALL.errors': 0,
                   'object.object_auditor_stats_ALL.audit_time':
                   301695.1047577858,
                   'object.object_auditor_stats_ALL.start_time':
                   1357979417.104742,
                   'object.object_auditor_stats_ALL.quarantined': 0,
                   'object.object_auditor_stats_ALL.bytes_processed':
                   24799969235,
                   'object.async_pending': 0,
                   'object.object_updater_sweep': 0.767723798751831,
                   'object.object_auditor_stats_ZBF.passes': 99350,
                   'object.object_auditor_stats_ZBF.errors': 0,
                   'object.object_auditor_stats_ZBF.audit_time':
                   152991.46442770958,
                   'object.object_auditor_stats_ZBF.start_time':
                   1357979462.621007,
                   'object.object_auditor_stats_ZBF.quarantined': 0,
                   'object.object_auditor_stats_ZBF.bytes_processed': 0}
        self.assertPublishedMany(publish_mock, metrics)

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

##########################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = openvpn
# coding=utf-8

"""
Processes OpenVPN metrics. This collector can process multiple OpenVPN
instances (even from a server box). In addition to the path, you may
also specify a name of the instance.

You can use both the status file or the tcp management connection to
retrieve the metrics.

To parse the status file::

    instances = file:///var/log/openvpn/status.log

Or, to override the name (now "status"):

    instances = file:///var/log/openvpn/status.log?developers

To use the management connection::

    instances = tcp://127.0.0.1:1195

Or, to override the name (now "127_0_0_1"):

    instances = tcp://127.0.0.1:1195?developers

You can also specify multiple and mixed instances::

    instances = file:///var/log/openvpn/openvpn.log, tcp://10.0.0.1:1195?admins

#### Dependencies

 * urlparse

"""

import socket
import diamond.collector
import os.path
import urlparse
import time


class OpenVPNCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(OpenVPNCollector, self).get_default_config_help()
        config_help.update({
            'instances': 'List of instances to collect stats from',
            'timeout': 'network timeout'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(OpenVPNCollector, self).get_default_config()
        config.update({
            'path':      'openvpn',
            'method':    'Threaded',
            'instances': 'file:///var/log/openvpn/status.log',
            'timeout':   '10',
        })
        return config

    def parse_url(self, uri):
        """
        Convert urlparse from a python 2.4 layout to a python 2.7 layout
        """
        parsed = urlparse.urlparse(uri)
        if 'scheme' not in parsed:
            class Object(object):
                pass
            newparsed = Object()
            newparsed.scheme = parsed[0]
            newparsed.netloc = parsed[1]
            newparsed.path = parsed[2]
            newparsed.params = parsed[3]
            newparsed.query = parsed[4]
            newparsed.fragment = parsed[5]
            newparsed.username = ''
            newparsed.password = ''
            newparsed.hostname = ''
            newparsed.port = ''
            parsed = newparsed
        return parsed

    def collect(self):
        if isinstance(self.config['instances'], basestring):
            instances = [self.config['instances']]
        else:
            instances = self.config['instances']

        for uri in instances:
            parsed = self.parse_url(uri)
            collect = getattr(self, 'collect_%s' % (parsed.scheme,), None)
            if collect:
                collect(uri)
            else:
                self.log.error('OpenVPN no handler for %s', uri)

    def collect_file(self, uri):
        parsed = self.parse_url(uri)
        filename = parsed.path
        if '?' in filename:
            filename, name = filename.split('?')
        else:
            name = os.path.splitext(os.path.basename(filename))[0]

        if not os.access(filename, os.R_OK):
            self.log.error('OpenVPN collect failed: unable to read "%s"',
                           filename)
            return
        else:
            self.log.info('OpenVPN parsing "%s" file: %s', name, filename)

        fd = open(filename, 'r')
        lines = fd.readlines()
        fd.close()

        self.parse(name, lines)

    def collect_tcp(self, uri):
        parsed = self.parse_url(uri)
        try:
            host, port = parsed.netloc.split(':')
            port = int(port)
        except ValueError:
            self.log.error('OpenVPN expected host:port in URI, got "%s"',
                           parsed.netloc)
            return

        if '?' in parsed.path:
            name = parsed.path[1:]
        else:
            name = host.replace('.', '_')

        self.log.info('OpenVPN parsing "%s" tcp: %s:%d', name, host, port)

        try:
            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            server.settimeout(int(self.config['timeout']))
            server.connect((host, port))

            fd = server.makefile('rb')
            line = fd.readline()
            if not line.startswith('>INFO:OpenVPN'):
                self.log.debug('OpenVPN received: %s', line.rstrip())
                self.log.error('OpenVPN protocol error')
                server.close()
                return

            server.send('status\r\n')

            lines = []
            while True:
                line = fd.readline()
                lines.append(line)
                if line.strip() == 'END':
                    break

            # Hand over data to the parser
            self.parse(name, lines)

            # Bye
            server.close()

        except socket.error, e:
            self.log.error('OpenVPN management connection error: %s', e)
            return

    def parse(self, name, lines):
        for line in lines:
            self.log.debug('OpenVPN: %s', line.rstrip())

        time.sleep(0.5)

        number_connected_clients = 0
        section = ''
        heading = []
        for line in lines:
            if line.strip() == 'END':
                break
            elif line.lower().startswith('openvpn statistics'):
                section = 'statistics'
            elif line.lower().startswith('openvpn client list'):
                section = 'clients'
            elif line.lower().startswith('routing table'):
                # ignored
                section = ''
            elif line.lower().startswith('global stats'):
                section = 'global'
            elif ',' in line:
                key, value = line.split(',', 1)
                if key.lower() == 'updated':
                    continue

                if section == 'statistics':
                    # All values here are numeric
                    self.publish_number('.'.join([
                        name,
                        'global',
                        key, ]), value)

                elif section == 'clients':
                    # Clients come with a heading
                    if not heading:
                        heading = line.strip().split(',')
                    else:
                        info = {}
                        number_connected_clients += 1
                        for k, v in zip(heading, line.strip().split(',')):
                            info[k.lower()] = v

                        self.publish_number('.'.join([
                            name,
                            section,
                            info['common name'].replace('.', '_'),
                            'bytes_rx']), info['bytes received'])
                        self.publish_number('.'.join([
                            name,
                            section,
                            info['common name'].replace('.', '_'),
                            'bytes_tx']), info['bytes sent'])

                elif section == 'global':
                    # All values here are numeric
                    self.publish_number('.'.join([
                        name,
                        section,
                        key, ]), value)

            elif line.startswith('END'):
                break
        self.publish('%s.clients.connected' % name, number_connected_clients)

    def publish_number(self, key, value):
        key = key.replace('/', '-').replace(' ', '_').lower()
        try:
            value = long(value)
        except ValueError:
            self.log.error('OpenVPN expected a number for "%s", got "%s"',
                           key, value)
            return
        else:
            self.publish(key, value)

########NEW FILE########
__FILENAME__ = testopenvpn
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import patch

from diamond.collector import Collector
from openvpn import OpenVPNCollector

################################################################################


class TestOpenVPNCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('OpenVPNCollector', {
            'interval': 10,
            'method':   None,
            'instances': 'file://' + self.getFixturePath('status.log'),
        })

        self.collector = OpenVPNCollector(config, None)

    def test_import(self):
        self.assertTrue(OpenVPNCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        self.collector.collect()

        metrics = {
            'status.clients.a_example_org.bytes_rx': 109619579.000000,
            'status.clients.a_example_org.bytes_tx': 935436488.000000,
            'status.clients.b_example_org.bytes_rx': 25067295.000000,
            'status.clients.b_example_org.bytes_tx': 10497532.000000,
            'status.clients.c_example_org.bytes_rx': 21842093.000000,
            'status.clients.c_example_org.bytes_tx': 20185134.000000,
            'status.clients.d_example_org.bytes_rx': 4559242.000000,
            'status.clients.d_example_org.bytes_tx': 11133831.000000,
            'status.clients.e_example_org.bytes_rx': 13090090.000000,
            'status.clients.e_example_org.bytes_tx': 13401853.000000,
            'status.clients.connected': 5,
            'status.global.max_bcast-mcast_queue_length': 14.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = passenger_stats
# coding=utf-8

"""
The PasengerCollector collects CPU and memory utilization of apache, nginx
and passenger processes.

Four key attributes to be published:

 * phusion_passenger_cpu
 * total_apache_memory
 * total_passenger_memory
 * total_nginx_memory

#### Dependencies

 * passenger-memory-stats

"""
import diamond.collector
import os
import re
import subprocess
from diamond.collector import str_to_bool


class PassengerCollector(diamond.collector.Collector):
    """
    Collect Memory and CPU Utilization for Passenger
    """

    def get_default_config_help(self):
        """
        Return help text
        """
        config_help = super(PassengerCollector, self).get_default_config_help()
        config_help.update({
            "bin":         "The path to the binary",
            "use_sudo":    "Use sudo?",
            "sudo_cmd":    "Path to sudo",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PassengerCollector, self).get_default_config()
        config.update({
            "path":         "passenger_stats",
            "bin":          "/usr/lib/ruby-flo/bin/passenger-memory-stats",
            "use_sudo":     False,
            "sudo_cmd":     "/usr/bin/sudo",
            })
        return config

    def get_passenger_memory_stats(self):
        """
        Execute passenger-memory-stats, parse its output, return dictionary with
        stats.
        """
        command = [self.config["bin"]]
        if str_to_bool(self.config["use_sudo"]):
            command.insert(0, self.config["sudo_cmd"])

        try:
            proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)
            (std_out, std_err) = proc1.communicate()
        except OSError as exception:
            return {}

        if std_out is None:
            return {}

        dict_stats = {
            "apache_procs": [],
            "nginx_procs": [],
            "passenger_procs": [],
            "apache_mem_total": 0.0,
            "nginx_mem_total": 0.0,
            "passenger_mem_total": 0.0,
        }
        #
        re_colour = re.compile("\x1B\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]")
        re_digit = re.compile("^\d")
        #
        apache_flag = 0
        nginx_flag = 0
        passenger_flag = 0
        for raw_line in std_out.splitlines():
            line = re_colour.sub("", raw_line)
            if "Apache processes" in line:
                apache_flag = 1
            elif "Nginx processes" in line:
                nginx_flag = 1
            elif "Passenger processes" in line:
                passenger_flag = 1
            elif re_digit.match(line):
                # If line starts with digit, then store PID and memory consumed
                line_splitted = line.split()
                if apache_flag == 1:
                    dict_stats["apache_procs"].append(line_splitted[0])
                    dict_stats["apache_mem_total"] += float(line_splitted[4])
                elif nginx_flag == 1:
                    dict_stats["nginx_procs"].append(line_splitted[0])
                    dict_stats["nginx_mem_total"] += float(line_splitted[4])
                elif passenger_flag == 1:
                    dict_stats["passenger_procs"].append(line_splitted[0])
                    dict_stats["passenger_mem_total"] += float(line_splitted[3])

            elif "Processes:" in line:
                passenger_flag = 0
                apache_flag = 0
                nginx_flag = 0

        return dict_stats

    def get_passenger_cpu_usage(self, dict_stats):
        """
        Execute % top; and return STDOUT.
        """
        try:
            proc1 = subprocess.Popen(
                ["top", "-b", "-n", "2"],
                stdout=subprocess.PIPE)
            (std_out, std_err) = proc1.communicate()
        except OSError as exception:
            return (-1)

        re_lspaces = re.compile("^\s*")
        re_digit = re.compile("^\d")
        overall_cpu = 0
        for raw_line in std_out.splitlines():
            line = re_lspaces.sub("", raw_line)
            if not re_digit.match(line):
                continue

            line_splitted = line.split()
            if line_splitted[0] in dict_stats["apache_procs"]:
                overall_cpu += float(line_splitted[8])
            elif line_splitted[0] in dict_stats["nginx_procs"]:
                overall_cpu += float(line_splitted[8])
            elif line_splitted[0] in dict_stats["passenger_procs"]:
                overall_cpu += float(line_splitted[8])

        return overall_cpu

    def collect(self):
        """
        Collector Passenger stats
        """
        if not os.access(self.config["bin"], os.X_OK):
            self.log.error("Path %s does not exist or is not executable",
                           self.config["bin"])
            return {}

        dict_stats = self.get_passenger_memory_stats()
        if len(dict_stats.keys()) == 0:
            return {}

        overall_cpu = self.get_passenger_cpu_usage(dict_stats)
        if overall_cpu >= 0:
            self.publish("phusion_passenger_cpu", overall_cpu)

        self.publish("total_apache_memory", dict_stats["apache_mem_total"])
        self.publish("total_nginx_memory", dict_stats["nginx_mem_total"])
        self.publish("total_passenger_memory",
                     dict_stats["passenger_mem_total"])

########NEW FILE########
__FILENAME__ = testpassenger_stats
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest

from passenger_stats import PassengerCollector

################################################################################


class TestPassengerCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PassengerCollector', {})
        self.collector = PassengerCollector(config, None)

    def test_import(self):
        self.assertTrue(PassengerCollector)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ping
# coding=utf-8

"""
Collect icmp round trip times
Only valid for ipv4 hosts currently

#### Dependencies

 * ping

#### Configuration

Configuration is done by:

Create a file named: PingCollector.conf in the collectors_config_path

 * enabled = true
 * interval = 60
 * target_1 = example.org
 * target_fw = 192.168.0.1
 * target_localhost = localhost

Test your configuration using the following command:

diamond-setup --print -C PingCollector

You should get a reponse back that indicates 'enabled': True and see entries
for your targets in pairs like:

'target_1': 'example.org'

We extract out the key after target_ and use it in the graphite node we push.

"""

import subprocess
import diamond.collector
import os
from diamond.collector import str_to_bool


class PingCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(PingCollector, self).get_default_config_help()
        config_help.update({
            'bin':         'The path to the ping binary',
            'use_sudo':    'Use sudo?',
            'sudo_cmd':    'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PingCollector, self).get_default_config()
        config.update({
            'path':             'ping',
            'bin':              '/bin/ping',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
        })
        return config

    def collect(self):
        for key in self.config.keys():
            if key[:7] == "target_":
                host = self.config[key]
                metric_name = host.replace('.', '_')

                if not os.access(self.config['bin'], os.X_OK):
                    self.log.error("Path %s does not exist or is not executable"
                                   % self.config['bin'])
                    return

                command = [self.config['bin'], '-nq', '-c 1', host]

                if str_to_bool(self.config['use_sudo']):
                    command.insert(0, self.config['sudo_cmd'])

                ping = subprocess.Popen(
                    command, stdout=subprocess.PIPE).communicate()[0].strip(
                    ).split("\n")[-1]

                # Linux
                if ping.startswith('rtt'):
                    ping = ping.split()[3].split('/')[0]
                    metric_value = float(ping)
                # OS X
                elif ping.startswith('round-trip '):
                    ping = ping.split()[3].split('/')[0]
                    metric_value = float(ping)
                # Unknown
                else:
                    metric_value = 10000

                self.publish(metric_name, metric_value)

########NEW FILE########
__FILENAME__ = testping
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from ping import PingCollector

################################################################################


class TestPingCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PingCollector', {
            'interval': 10,
            'target_a': 'localhost',
            'bin': 'true'
        })

        self.collector = PingCollector(config, None)

    def test_import(self):
        self.assertTrue(PingCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_bad_gentoo(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('bad_gentoo').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 10000
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_host_gentoo(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('host_gentoo').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        metrics = {
            'localhost': 11
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_ip_gentoo(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('ip_gentoo').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 0
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_longhost_gentoo(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture(
                    'longhost_gentoo').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 10
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_timeout_gentoo(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture(
                    'timeout_gentoo').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 10000
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_host_osx(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('host_osx').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 38
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_ip_osx(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('ip_osx').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 0
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_longhost_osx(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('longhost_osx').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 42
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_timeout_osx(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('timeout_osx').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'localhost': 10000
        })

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = postfix
# coding=utf-8

"""
Collect stats from postfix-stats. postfix-stats is a simple threaded stats
aggregator for Postfix. When running as a syslog destination, it can be used to
get realtime cumulative stats.

#### Dependencies

 * socket
 * json (or simeplejson)
 * [postfix-stats](https://github.com/disqus/postfix-stats)

"""

import socket
import sys

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import diamond.collector

from diamond.collector import str_to_bool

if sys.version_info < (2, 6):
    from string import maketrans
    DOTS_TO_UNDERS = maketrans('.', '_')
else:
    DOTS_TO_UNDERS = {ord(u'.'): u'_'}


class PostfixCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(PostfixCollector,
                            self).get_default_config_help()
        config_help.update({
            'host':             'Hostname to coonect to',
            'port':             'Port to connect to',
            'include_clients':  'Include client connection stats',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PostfixCollector, self).get_default_config()
        config.update({
            'path':             'postfix',
            'host':             'localhost',
            'port':             7777,
            'include_clients':  True,
            'method':           'Threaded',
        })
        return config

    def get_json(self):
        json_string = ''

        address = (self.config['host'], int(self.config['port']))

        s = None
        try:
            try:
                s = socket.create_connection(address, timeout=1)

                s.sendall('stats\n')

                while 1:
                    data = s.recv(4096)
                    if not data:
                        break
                    json_string += data
            except socket.error:
                self.log.exception("Error talking to postfix-stats")
                return ''
        finally:
            if s:
                s.close()

        return json_string

    def get_data(self):
        json_string = self.get_json()

        try:
            data = json.loads(json_string)
        except (ValueError, TypeError):
            self.log.exception("Error parsing json from postfix-stats")
            return None

        return data

    def collect(self):
        data = self.get_data()

        if not data:
            return

        if str_to_bool(self.config['include_clients']) and u'clients' in data:
            for client, value in data['clients'].iteritems():
                # translate dots to underscores in client names
                metric = u'.'.join(['clients',
                                    client.translate(DOTS_TO_UNDERS)])

                dvalue = self.derivative(metric, value)

                self.publish(metric, dvalue)

        for action in (u'in', u'recv', u'send'):
            if action not in data:
                continue

            for sect, stats in data[action].iteritems():
                for status, value in stats.iteritems():
                    metric = '.'.join([action,
                                       sect,
                                       status.translate(DOTS_TO_UNDERS)])

                    dvalue = self.derivative(metric, value)

                    self.publish(metric, dvalue)

        if u'local' in data:
            for key, value in data[u'local'].iteritems():
                metric = '.'.join(['local', key])

                dvalue = self.derivative(metric, value)

                self.publish(metric, dvalue)

########NEW FILE########
__FILENAME__ = testpostfix
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from postfix import PostfixCollector

################################################################################


class TestPostfixCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PostfixCollector', {
            'host':     'localhost',
            'port':     7777,
            'interval': '1',
        })

        self.collector = PostfixCollector(config, None)

    def test_import(self):
        self.assertTrue(PostfixCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_synthetic_data(self, publish_mock):
        first_resp = self.getFixture('postfix-stats.1.json').getvalue()
        patch_collector = patch.object(
            PostfixCollector,
            'get_json',
            Mock(return_value=first_resp))

        patch_collector.start()
        self.collector.collect()
        patch_collector.stop()

        self.assertPublishedMany(publish_mock, {})

        second_resp = self.getFixture('postfix-stats.2.json').getvalue()
        patch_collector = patch.object(PostfixCollector,
                                       'get_json',
                                       Mock(return_value=second_resp))

        patch_collector.start()
        self.collector.collect()
        patch_collector.stop()

        metrics = {
            'send.status.sent': 4,
            'send.resp_codes.2_0_0': 5,
            'clients.127_0_0_1': 1,
        }

        self.assertPublishedMany(publish_mock, metrics)

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = postgres
# coding=utf-8

"""
Collect metrics from postgresql

#### Dependencies

 * psycopg2

"""

import diamond.collector
from diamond.collector import str_to_bool

try:
    import psycopg2
    import psycopg2.extras
    psycopg2  # workaround for pyflakes issue #13
except ImportError:
    psycopg2 = None


class PostgresqlCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(PostgresqlCollector, self).get_default_config_help()
        config_help.update({
            'host': 'Hostname',
            'user': 'Username',
            'password': 'Password',
            'port': 'Port number',
            'underscore': 'Convert _ to .',
            'extended': 'Enable collection of extended database stats.',
            'metrics': 'List of enabled metrics to collect',
            'pg_version':
            "The version of postgres that you'll be monitoring eg in format 9.2"
        })
        return config_help

    def get_default_config(self):
        """
        Return default config.
        """
        config = super(PostgresqlCollector, self).get_default_config()
        config.update({
            'path': 'postgres',
            'host': 'localhost',
            'user': 'postgres',
            'password': 'postgres',
            'port': 5432,
            'underscore': False,
            'extended': False,
            'method': 'Threaded',
            'metrics': [],
            'pg_version': 9.2
        })
        return config

    def collect(self):
        if psycopg2 is None:
            self.log.error('Unable to import module psycopg2')
            return {}

        # Create database-specific connections
        self.connections = {}
        for db in self._get_db_names():
            self.connections[db] = self._connect(database=db)

        if self.config['metrics']:
            metrics = self.config['metrics']
        elif str_to_bool(self.config['extended']):
            metrics = registry['extended']
        else:
            metrics = registry['basic']

        # Iterate every QueryStats class
        for metric_name in set(metrics):
            if metric_name not in metrics_registry:
                continue
            klass = metrics_registry[metric_name]
            stat = klass(self.connections, underscore=self.config['underscore'])
            stat.fetch(self.config['pg_version'])
            for metric, value in stat:
                if value is not None:
                    self.publish(metric, value)

        # Cleanup
        [conn.close() for conn in self.connections.itervalues()]

    def _get_db_names(self):
        query = """
            SELECT datname FROM pg_database
            WHERE datallowconn AND NOT datistemplate
            AND NOT datname='postgres' ORDER BY 1
        """
        conn = self._connect()
        cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
        cursor.execute(query)
        datnames = [d['datname'] for d in cursor.fetchall()]
        conn.close()

        # Exclude `postgres` database list, unless it is the
        # only database available (required for querying pg_stat_database)
        if not datnames:
            datnames = ['postgres']
        return datnames

    def _connect(self, database=None):
        conn_args = {
            'host': self.config['host'],
            'user': self.config['user'],
            'password': self.config['password'],
            'port': self.config['port']
        }

        if database:
            conn_args['database'] = database
        else:
            conn_args['database'] = 'postgres'

        conn = psycopg2.connect(**conn_args)

        # Avoid using transactions, set isolation level to autocommit
        conn.set_isolation_level(0)
        return conn


class QueryStats(object):
    def __init__(self, conns, parameters=None, underscore=False):
        self.connections = conns
        self.underscore = underscore
        self.parameters = parameters

    def _translate_datname(self, db):
        if self.underscore:
            db = db.replace("_", ".")
        return db

    def fetch(self, pg_version):
        self.data = list()

        for db, conn in self.connections.iteritems():
            cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            if float(pg_version) >= 9.2:
                pid = 'pid'
                query = 'query'
            else:
                pid = 'procpid'
                query = 'current_query'
            q = self.query.format(pid=pid, query=query)
            cursor.execute(q, self.parameters)

            for row in cursor.fetchall():
                # If row is length 2, assume col1, col2 forms key: value
                if len(row) == 2:
                    self.data.append({
                        'datname': self._translate_datname(db),
                        'metric': row[0],
                        'value': row[1],
                    })

                # If row > length 2, assume each column name maps to
                # key => value
                else:
                    for key, value in row.iteritems():
                        if key in ('datname', 'schemaname', 'relname',
                                   'indexrelname',):
                            continue

                        self.data.append({
                            'datname': self._translate_datname(row.get(
                                'datname', db)),
                            'schemaname': row.get('schemaname', None),
                            'relname': row.get('relname', None),
                            'indexrelname': row.get('indexrelname', None),
                            'metric': key,
                            'value': value,
                        })

            # Setting multi_db to True will run this query on all known
            # databases. This is bad for queries that hit views like
            # pg_database, which are shared across databases.
            #
            # If multi_db is False, bail early after the first query
            # iteration. Otherwise, continue to remaining databases.
            if not self.multi_db:
                break

    def __iter__(self):
        for data_point in self.data:
            yield (self.path % data_point, data_point['value'])


class DatabaseStats(QueryStats):
    """
    Database-level summary stats
    """
    path = "database.%(datname)s.%(metric)s"
    multi_db = False
    query = """
        SELECT pg_stat_database.datname as datname,
               pg_stat_database.numbackends as numbackends,
               pg_stat_database.xact_commit as xact_commit,
               pg_stat_database.xact_rollback as xact_rollback,
               pg_stat_database.blks_read as blks_read,
               pg_stat_database.blks_hit as blks_hit,
               pg_stat_database.tup_returned as tup_returned,
               pg_stat_database.tup_fetched as tup_fetched,
               pg_stat_database.tup_inserted as tup_inserted,
               pg_stat_database.tup_updated as tup_updated,
               pg_stat_database.tup_deleted as tup_deleted,
               pg_database_size(pg_database.datname) AS size
        FROM pg_database
        JOIN pg_stat_database
        ON pg_database.datname = pg_stat_database.datname
        WHERE pg_stat_database.datname
        NOT IN ('template0','template1','postgres')
    """


class UserTableStats(QueryStats):
    path = "%(datname)s.tables.%(schemaname)s.%(relname)s.%(metric)s"
    multi_db = True
    query = """
        SELECT relname,
               schemaname,
               seq_scan,
               seq_tup_read,
               idx_scan,
               idx_tup_fetch,
               n_tup_ins,
               n_tup_upd,
               n_tup_del,
               n_tup_hot_upd,
               n_live_tup,
               n_dead_tup
        FROM pg_stat_user_tables
    """


class UserIndexStats(QueryStats):
    path = "%(datname)s.indexes.%(schemaname)s.%(relname)s." \
           "%(indexrelname)s.%(metric)s"
    multi_db = True
    query = """
        SELECT relname,
               schemaname,
               indexrelname,
               idx_scan,
               idx_tup_read,
               idx_tup_fetch
        FROM pg_stat_user_indexes
    """


class UserTableIOStats(QueryStats):
    path = "%(datname)s.tables.%(schemaname)s.%(relname)s.%(metric)s"
    multi_db = True
    query = """
        SELECT relname,
               schemaname,
               heap_blks_read,
               heap_blks_hit,
               idx_blks_read,
               idx_blks_hit
        FROM pg_statio_user_tables
    """


class UserIndexIOStats(QueryStats):
    path = "%(datname)s.indexes.%(schemaname)s.%(relname)s." \
           "%(indexrelname)s.%(metric)s"
    multi_db = True
    query = """
        SELECT relname,
               schemaname,
               indexrelname,
               idx_blks_read,
               idx_blks_hit
        FROM pg_statio_user_indexes
    """


class ConnectionStateStats(QueryStats):
    path = "%(datname)s.connections.%(metric)s"
    multi_db = True
    query = """
        SELECT tmp.state AS key,COALESCE(count,0) FROM
               (VALUES ('active'),
                       ('waiting'),
                       ('idle'),
                       ('idletransaction'),
                       ('unknown')
                ) AS tmp(state)
        LEFT JOIN
             (SELECT CASE WHEN waiting THEN 'waiting'
                          WHEN {query} = '<IDLE>' THEN 'idle'
                          WHEN {query} = '<IDLE> in transaction'
                              THEN 'idletransaction'
                          WHEN {query} = '<insufficient privilege>'
                              THEN 'unknown'
                          ELSE 'active' END AS state,
                     count(*) AS count
               FROM pg_stat_activity
               WHERE {pid} != pg_backend_pid()
               GROUP BY CASE WHEN waiting THEN 'waiting'
                             WHEN {query} = '<IDLE>' THEN 'idle'
                             WHEN {query} = '<IDLE> in transaction'
                                 THEN 'idletransaction'
                             WHEN {query} = '<insufficient privilege>'
                                 THEN 'unknown' ELSE 'active' END
             ) AS tmp2
        ON tmp.state=tmp2.state ORDER BY 1
    """


class LockStats(QueryStats):
    path = "%(datname)s.locks.%(metric)s"
    multi_db = False
    query = """
        SELECT lower(mode) AS key,
               count(*) AS value
        FROM pg_locks
        WHERE database IS NOT NULL
        GROUP BY mode ORDER BY 1
    """


class RelationSizeStats(QueryStats):
    path = "%(datname)s.sizes.%(schemaname)s.%(relname)s.%(metric)s"
    multi_db = True
    query = """
        SELECT pg_class.relname,
               pg_namespace.nspname as schemaname,
               pg_relation_size(pg_class.oid) as relsize
        FROM pg_class
        INNER JOIN
          pg_namespace
        ON pg_namespace.oid = pg_class.relnamespace
        WHERE reltype != 0
        AND relkind != 'S'
        AND nspname NOT IN ('pg_catalog', 'information_schema')
    """


class BackgroundWriterStats(QueryStats):
    path = "bgwriter.%(metric)s"
    multi_db = False
    query = """
        SELECT checkpoints_timed,
               checkpoints_req,
               buffers_checkpoint,
               buffers_clean,
               maxwritten_clean,
               buffers_backend,
               buffers_alloc
        FROM pg_stat_bgwriter
    """


class WalSegmentStats(QueryStats):
    path = "wals.%(metric)s"
    multi_db = False
    query = """
        SELECT count(*) AS segments
        FROM pg_ls_dir('pg_xlog') t(fn)
        WHERE fn ~ '^[0-9A-Z]{{24}}\$'
    """


class TransactionCount(QueryStats):
    path = "transactions.%(metric)s"
    multi_db = False
    query = """
        SELECT 'commit' AS type,
               sum(pg_stat_get_db_xact_commit(oid))
        FROM pg_database
        UNION ALL
        SELECT 'rollback',
               sum(pg_stat_get_db_xact_rollback(oid))
        FROM pg_database
    """


class IdleInTransactions(QueryStats):
    path = "%(datname)s.idle_in_tranactions.%(metric)s"
    multi_db = True
    query = """
        SELECT 'idle_in_transactions',
               max(COALESCE(ROUND(EXTRACT(epoch FROM now()-query_start)),0))
                   AS idle_in_transaction
        FROM pg_stat_activity
        WHERE {query} = '<IDLE> in transaction'
        GROUP BY 1
    """


class LongestRunningQueries(QueryStats):
    path = "%(datname)s.longest_running.%(metric)s"
    multi_db = True
    query = """
        SELECT 'query',
            COALESCE(max(extract(epoch FROM CURRENT_TIMESTAMP-query_start)),0)
        FROM pg_stat_activity
        WHERE {query} NOT LIKE '<IDLE%'
        UNION ALL
        SELECT 'transaction',
            COALESCE(max(extract(epoch FROM CURRENT_TIMESTAMP-xact_start)),0)
        FROM pg_stat_activity
        WHERE 1=1
    """


class UserConnectionCount(QueryStats):
    path = "%(datname)s.user_connections.%(metric)s"
    multi_db = True
    query = """
        SELECT usename,
               count(*) as count
        FROM pg_stat_activity
        WHERE {pid} != pg_backend_pid()
        GROUP BY usename
        ORDER BY 1
    """


class DatabaseConnectionCount(QueryStats):
    path = "database.%(metric)s.connections"
    multi_db = False
    query = """
        SELECT datname,
               count(datname) as connections
        FROM pg_stat_activity
        GROUP BY pg_stat_activity.datname
    """


class TableScanStats(QueryStats):
    path = "%(datname)s.scans.%(metric)s"
    multi_db = True
    query = """
        SELECT 'relname' AS relname,
               COALESCE(sum(seq_scan),0) AS sequential,
               COALESCE(sum(idx_scan),0) AS index
        FROM pg_stat_user_tables
    """


class TupleAccessStats(QueryStats):
    path = "%(datname)s.tuples.%(metric)s"
    multi_db = True
    query = """
        SELECT COALESCE(sum(seq_tup_read),0) AS seqread,
               COALESCE(sum(idx_tup_fetch),0) AS idxfetch,
               COALESCE(sum(n_tup_ins),0) AS inserted,
               COALESCE(sum(n_tup_upd),0) AS updated,
               COALESCE(sum(n_tup_del),0) AS deleted,
               COALESCE(sum(n_tup_hot_upd),0) AS hotupdated
        FROM pg_stat_user_tables
    """


metrics_registry = {
    'DatabaseStats': DatabaseStats,
    'DatabaseConnectionCount': DatabaseConnectionCount,
    'UserTableStats': UserTableStats,
    'UserIndexStats': UserIndexStats,
    'UserTableIOStats': UserTableIOStats,
    'UserIndexIOStats': UserIndexIOStats,
    'ConnectionStateStats': ConnectionStateStats,
    'LockStats': LockStats,
    'RelationSizeStats': RelationSizeStats,
    'BackgroundWriterStats': BackgroundWriterStats,
    'WalSegmentStats': WalSegmentStats,
    'TransactionCount': TransactionCount,
    'IdleInTransactions': IdleInTransactions,
    'LongestRunningQueries': LongestRunningQueries,
    'UserConnectionCount': UserConnectionCount,
    'TableScanStats': TableScanStats,
    'TupleAccessStats': TupleAccessStats,
}

registry = {
    'basic': (
        'DatabaseStats',
        'DatabaseConnectionCount',
    ),
    'extended': (
        'DatabaseStats',
        'DatabaseConnectionCount',
        'UserTableStats',
        'UserIndexStats',
        'UserTableIOStats',
        'UserIndexIOStats',
        'ConnectionStateStats',
        'LockStats',
        'RelationSizeStats',
        'BackgroundWriterStats',
        'WalSegmentStats',
        'TransactionCount',
        'IdleInTransactions',
        'LongestRunningQueries',
        'UserConnectionCount',
        'TableScanStats',
        'TupleAccessStats',
    ),
}

########NEW FILE########
__FILENAME__ = testpostgres
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from postgres import PostgresqlCollector


class TestPostgresqlCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('PostgresqlCollector', {
        })
        self.collector = PostgresqlCollector(config, None)

    def test_import(self):
        self.assertTrue(PostgresqlCollector)

########NEW FILE########
__FILENAME__ = postqueue
# coding=utf-8

"""
Collect the emails in the postfix queue

#### Dependencies

 * subprocess

"""

import subprocess
import diamond.collector
from diamond.collector import str_to_bool


class PostqueueCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(PostqueueCollector, self).get_default_config_help()
        config_help.update({
            'bin':         'The path to the postqueue binary',
            'use_sudo':    'Use sudo?',
            'sudo_cmd':    'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PostqueueCollector, self).get_default_config()
        config.update({
            'path':             'postqueue',
            'bin':              '/usr/bin/postqueue',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
        })
        return config

    def get_postqueue_output(self):
        try:
            command = [self.config['bin'], '-p']

            if str_to_bool(self.config['use_sudo']):
                command.insert(0, self.config['sudo_cmd'])

            return subprocess.Popen(command,
                                    stdout=subprocess.PIPE).communicate()[0]
        except OSError:
            return ""

    def collect(self):
        output = self.get_postqueue_output()

        try:
            postqueue_count = int(output.strip().split("\n")[-1].split()[-2])
        except:
            postqueue_count = 0

        self.publish('count', postqueue_count)

########NEW FILE########
__FILENAME__ = testpostqueue
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from postqueue import PostqueueCollector

################################################################################


class TestPostqueueCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PostqueueCollector', {
        })

        self.collector = PostqueueCollector(config, {})

    def test_import(self):
        self.assertTrue(PostqueueCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_emails_in_queue(self, publish_mock):
        patch_collector = patch.object(
            PostqueueCollector,
            'get_postqueue_output',
            Mock(return_value=self.getFixture(
                'postqueue_emails').getvalue()))
        patch_collector.start()
        self.collector.collect()
        patch_collector.stop()

        metrics = {
            'count': 3
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_empty_queue(self, publish_mock):
        patch_collector = patch.object(
            PostqueueCollector,
            'get_postqueue_output',
            Mock(return_value=self.getFixture(
                'postqueue_empty').getvalue()))
        patch_collector.start()
        self.collector.collect()
        patch_collector.stop()

        metrics = {
            'count': 0
        }

        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = powerdns
# coding=utf-8

"""
Collects all metrics exported by the powerdns nameserver using the
pdns_control binary.

#### Dependencies

 * pdns_control

"""

import diamond.collector
import subprocess
import os
from diamond.collector import str_to_bool


class PowerDNSCollector(diamond.collector.Collector):

    _GAUGE_KEYS = [
        'cache-bytes', 'cache-entries', 'chain-resends',
        'concurrent-queries', 'dlg-only-drops', 'dont-outqueries',
        'ipv6-outqueries', 'latency', 'max-mthread-stack', 'negcache-entries',
        'nsspeeds-entries',
        'packetcache-bytes', 'packetcache-entries', 'packetcache-size',
        'qa-latency', 'throttle-entries']

    def get_default_config_help(self):
        config_help = super(PowerDNSCollector, self).get_default_config_help()
        config_help.update({
            'bin':         'The path to the pdns_control binary',
            'use_sudo':    'Use sudo?',
            'sudo_cmd':    'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PowerDNSCollector, self).get_default_config()
        config.update({
            'bin': '/usr/bin/pdns_control',
            'path': 'powerdns',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
        })
        return config

    def collect(self):
        if not os.access(self.config['bin'], os.X_OK):
            self.log.error("%s is not executable", self.config['bin'])
            return False

        command = [self.config['bin'], 'list']

        if str_to_bool(self.config['use_sudo']):
            command.insert(0, self.config['sudo_cmd'])

        data = subprocess.Popen(command,
                                stdout=subprocess.PIPE).communicate()[0]

        for metric in data.split(','):
            if not metric.strip():
                continue
            metric, value = metric.split('=')
            try:
                value = float(value)
            except:
                pass
            if metric not in self._GAUGE_KEYS:
                value = self.derivative(metric, value)
                if value < 0:
                    continue
            self.publish(metric, value)

########NEW FILE########
__FILENAME__ = testpowerdns
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from powerdns import PowerDNSCollector

################################################################################


class TestPowerDNSCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PowerDNSCollector', {
            'interval': 1,
            'bin': 'true',
            'use_sudo': False,
        })

        self.collector = PowerDNSCollector(config, None)

    def test_import(self):
        self.assertTrue(PowerDNSCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_fake_data(self, publish_mock):
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture(
                    'pdns_control-2.9.22.6-1.el6-A'
                    ).getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('pdns_control-2.9.22.6-1.el6-B').getvalue(),
                '')))

        patch_communicate.start()
        self.collector.collect()
        patch_communicate.stop()

        metrics = {
            'corrupt-packets': 1.0,
            'deferred-cache-inserts': 2.0,
            'deferred-cache-lookup': 3.0,
            'latency': 4.0,
            'packetcache-hit': 5.0,
            'packetcache-miss': 6.0,
            'packetcache-size': 7.0,
            'qsize-q': 8.0,
            'query-cache-hit': 9.0,
            'query-cache-miss': 10.0,
            'recursing-answers': 11.0,
            'recursing-questions': 12.0,
            'servfail-packets': 13.0,
            'tcp-answers': 14.0,
            'tcp-queries': 15.0,
            'timedout-packets': 16.0,
            'udp-answers': 17.0,
            'udp-queries': 18.0,
            'udp4-answers': 19.0,
            'udp4-queries': 20.0,
            'udp6-answers': 21.0,
            'udp6-queries': 22.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = proc
# coding=utf-8

"""
The ProcessStatCollector collects metrics on process stats from
/proc/stat

#### Dependencies

 * /proc/stat

"""

import platform
import os
import diamond.collector

# Detect the architecture of the system
# and set the counters for MAX_VALUES
# appropriately. Otherwise, rolling over
# counters will cause incorrect or
# negative values.
if platform.architecture()[0] == '64bit':
    counter = (2 ** 64) - 1
else:
    counter = (2 ** 32) - 1


class ProcessStatCollector(diamond.collector.Collector):

    PROC = '/proc/stat'

    def get_default_config_help(self):
        config_help = super(ProcessStatCollector,
                            self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ProcessStatCollector, self).get_default_config()
        config.update({
            'path':     'proc'
        })
        return config

    def collect(self):
        """
        Collect process stat data
        """
        if not os.access(self.PROC, os.R_OK):
            return False

        #Open PROC file
        file = open(self.PROC, 'r')

        #Get data
        for line in file:

            if line.startswith('ctxt') or line.startswith('processes'):
                data = line.split()
                metric_name = data[0]
                metric_value = int(data[1])
                metric_value = int(self.derivative(metric_name,
                                                   long(metric_value),
                                                   counter))
                self.publish(metric_name, metric_value)

            if line.startswith('procs_') or line.startswith('btime'):
                data = line.split()
                metric_name = data[0]
                metric_value = int(data[1])
                self.publish(metric_name, metric_value)

        #Close file
        file.close()

########NEW FILE########
__FILENAME__ = testproc
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from proc import ProcessStatCollector

################################################################################


class TestProcessStatCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('ProcessStatCollector', {
            'interval': 1
        })

        self.collector = ProcessStatCollector(config, None)

    def test_import(self):
        self.assertTrue(ProcessStatCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_stat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/stat', 'r')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        ProcessStatCollector.PROC = self.getFixturePath('proc_stat_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        ProcessStatCollector.PROC = self.getFixturePath('proc_stat_2')
        self.collector.collect()

        metrics = {
            'ctxt': 0,
            'btime': 1319181102,
            'processes': 0,
            'procs_running': 1,
            'procs_blocked': 0,
            'ctxt': 1791,
            'btime': 1319181102,
            'processes': 2,
            'procs_running': 1,
            'procs_blocked': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = processresources
# coding=utf-8

"""
A Diamond collector that collects memory usage of each process defined in it's
config file by matching them with their executable filepath or the process name.
This collector can also be used to collect memory usage for the Diamond process.

Example config file ProcessResourcesCollector.conf

```
enabled=True
unit=B
cpu_interval=0.1
[process]
[[postgres]]
exe=^\/usr\/lib\/postgresql\/+d.+d\/bin\/postgres$
name=^postgres,^pg

[[diamond]]
selfmon=True
```

exe and name are both lists of comma-separated regexps.

count_workers defined under [process] will determine whether to count how many
workers are there of processes which match this [process],
for example: cgi workers.

"""

import os
import re

import diamond.collector
import diamond.convertor

try:
    import psutil
    psutil
except ImportError:
    psutil = None


def match_process(pid, name, cmdline, exe, cfg):
    """
    Decides whether a process matches with a given process descriptor

    :param pid: process pid
    :param exe: process executable
    :param name: process name
    :param cmdline: process cmdline
    :param cfg: the dictionary from processes that describes with the
        process group we're testing for
    :return: True if it matches
    :rtype: bool
    """
    if cfg['selfmon'] and pid == os.getpid():
        return True
    for exe_re in cfg['exe']:
        if exe_re.search(exe):
            return True
    for name_re in cfg['name']:
        if name_re.search(name):
            return True
    for cmdline_re in cfg['cmdline']:
        if cmdline_re.search(' '.join(cmdline)):
            return True
    return False


def process_info(process, info_keys):
    results = {}
    process_info = process.as_dict()
    metrics = ((key, process_info.get(key, None)) for key in info_keys)
    for key, value in metrics:
        if type(value) in [float, int]:
            results.update({key: value})
        elif hasattr(value, '_asdict'):
            for subkey, subvalue in value._asdict().iteritems():
                results.update({"%s.%s" % (key, subkey): subvalue})
    return results


def get_value(process, name):
    result = getattr(process, name)
    try:
        return result()
    except TypeError:
        return result


class ProcessResourcesCollector(diamond.collector.Collector):
    def __init__(self, *args, **kwargs):
        """
        prepare self.processes, which is a descriptor dictionary in
        pg_name: {
            exe: [regex],
            name: [regex],
            cmdline: [regex],
            selfmon: [boolean],
            procs: [psutil.Process],
            count_workers: [boolean]
        }
        """
        super(ProcessResourcesCollector, self).__init__(*args, **kwargs)
        self.processes = {}
        self.processes_info = {}
        for pg_name, cfg in self.config['process'].items():
            pg_cfg = {}
            for key in ('exe', 'name', 'cmdline'):
                pg_cfg[key] = cfg.get(key, [])
                if not isinstance(pg_cfg[key], list):
                    pg_cfg[key] = [pg_cfg[key]]
                pg_cfg[key] = [re.compile(e) for e in pg_cfg[key]]
            pg_cfg['selfmon'] = cfg.get('selfmon', '').lower() == 'true'
            pg_cfg['count_workers'] = cfg.get(
                'count_workers', '').lower() == 'true'
            self.processes[pg_name] = pg_cfg
            self.processes_info[pg_name] = {}

    def get_default_config_help(self):
        config_help = super(ProcessResourcesCollector,
                            self).get_default_config_help()
        config_help.update({
            'unit': 'The unit in which memory data is collected.',
            'process': ("A subcategory of settings inside of which each "
                        "collected process has it's configuration"),
        })
        return config_help

    def get_default_config(self):
        """
        Default settings are:
            path: 'process'
            unit: 'B'
        """
        config = super(ProcessResourcesCollector, self).get_default_config()
        config.update({
            'path': 'process',
            'unit': 'B',
            'process': {},
        })
        return config

    default_info_keys = [
        'num_ctx_switches',
        'cpu_percent',
        'cpu_times',
        'io_counters',
        'num_threads',
        'memory_percent',
        'ext_memory_info',
    ]

    def save_process_info(self, pg_name, process_info):
        for key, value in process_info.iteritems():
            if key in self.processes_info[pg_name]:
                self.processes_info[pg_name][key] += value
            else:
                self.processes_info[pg_name][key] = value

    def collect_process_info(self, process):
        try:
            pid = get_value(process, 'pid')
            name = get_value(process, 'name')
            cmdline = get_value(process, 'cmdline')
            try:
                exe = get_value(process, 'exe')
            except psutil.AccessDenied:
                exe = ""
            for pg_name, cfg in self.processes.items():
                if match_process(pid, name, cmdline, exe, cfg):
                    pi = process_info(process, self.default_info_keys)
                    if cfg['count_workers']:
                        pi.update({'workers_count': 1})
                    self.save_process_info(pg_name, pi)
        except psutil.NoSuchProcess, e:
            self.log.info("Process exited while trying to get info: %s", e)

    def collect(self):
        """
        Collects resources usage of each process defined under the
        `process` subsection of the config file
        """
        if not psutil:
            self.log.error('Unable to import psutil')
            self.log.error('No process resource metrics retrieved')
            return None

        for process in psutil.process_iter():
            self.collect_process_info(process)

        # publish results
        for pg_name, counters in self.processes_info.iteritems():
            metrics = (
                ("%s.%s" % (pg_name, key), value)
                for key, value in counters.iteritems())
            [self.publish(*metric) for metric in metrics]
            # reinitialize process info
            self.processes_info[pg_name] = {}

########NEW FILE########
__FILENAME__ = testprocessresources
#!/usr/bin/python
# coding=utf-8
################################################################################

import os
from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import patch

from diamond.collector import Collector
from processresources import ProcessResourcesCollector

################################################################################


def run_only_if_psutil_is_available(func):
    try:
        import psutil
        psutil  # workaround for pyflakes issue #13
    except ImportError:
        psutil = None
    pred = lambda: psutil is not None
    return run_only(func, pred)


class TestProcessResourcesCollector(CollectorTestCase):
    TEST_CONFIG = {
        'interval': 10,
        'process': {
            'postgres': {
                'exe': '^\/usr\/lib\/postgresql\/+d.+d\/bin\/postgres',
                'name': ['postgres', 'pg'],
            },
            'foo': {
                'exe': '^\/usr\/bin\/foo',
            },
            'bar': {
                'name': '^bar',
            },
            'barexe': {
                'exe': 'bar$'
            },
            'diamond-selfmon': {
                'selfmon': 'true',
            }
        }
    }
    SELFMON_PID = 10001  # used for selfmonitoring

    def setUp(self):
        config = get_collector_config('ProcessResourcesCollector',
                                      self.TEST_CONFIG)

        self.collector = ProcessResourcesCollector(config, None)

    def test_import(self):
        self.assertTrue(ProcessResourcesCollector)

    @run_only_if_psutil_is_available
    @patch.object(os, 'getpid')
    @patch.object(Collector, 'publish')
    def test(self, publish_mock, getpid_mock):
        process_info_list = [
            # postgres processes
            {
                'exe': '/usr/lib/postgresql/9.1/bin/postgres',
                'name': 'postgres',
                'pid': 1427,
                'rss': 1000000,
                'vms': 1000000
            },
            {
                'exe': '',
                'name': 'postgres: writer process   ',
                'pid': 1445,
                'rss': 100000,
                'vms': 200000
            },
            {
                'exe': '',
                'name': 'postgres: wal writer process   ',
                'pid': 1446,
                'rss': 10000,
                'vms': 20000
            },
            {
                'exe': '',
                'name': 'postgres: autovacuum launcher process   ',
                'pid': 1447,
                'rss': 1000,
                'vms': 2000
            },
            {
                'exe': '',
                'name': 'postgres: stats collector process   ',
                'pid': 1448,
                'rss': 100,
                'vms': 200},
            # postgres-y process
            {
                'exe': '',
                'name': 'posgre: not really',
                'pid': 9999,
                'rss': 10,
                'vms': 20,
            },
            {
                'exe': '/usr/bin/foo',
                'name': 'bar',
                'pid': 9998,
                'rss': 1,
                'vms': 1
            },
            {
                'exe': '',
                'name': 'barein',
                'pid': 9997,
                'rss': 2,
                'vms': 2
            },
            {
                'exe': '/usr/bin/bar',
                'name': '',
                'pid': 9996,
                'rss': 3,
                'vms': 3,
            },
            # diamond self mon process
            {
                'exe': 'DUMMY',
                'name': 'DUMMY',
                'pid': self.SELFMON_PID,
                'rss': 1234,
                'vms': 4,
            },
        ]

        class ProcessMock:
            def __init__(self, pid, name, rss, vms, exe=None):
                self.pid = pid
                self.name = name
                self.rss = rss
                self.vms = vms
                if exe is not None:
                    self.exe = exe

                self.cmdline = [self.exe]

            def as_dict(self):
                from collections import namedtuple
                meminfo = namedtuple('meminfo', 'rss vms')
                ext_meminfo = namedtuple('meminfo',
                                         'rss vms shared text lib data dirty')
                cputimes = namedtuple('cputimes', 'user system')
                openfile = namedtuple('openfile', 'path fd')
                thread = namedtuple('thread', 'id user_time system_time')
                user = namedtuple('user', 'real effective saved')
                group = namedtuple('group', 'real effective saved')
                io = namedtuple('io',
                                'read_count write_count read_bytes write_bytes')
                ionice = namedtuple('ionice', 'ioclass value')
                amount = namedtuple('amount', 'voluntary involuntary')
                return {
                    'status': 'sleeping',
                    'num_ctx_switches': amount(voluntary=2243, involuntary=221),
                    'pid': self.pid,
                    'connections': None,
                    'cmdline': [self.exe],
                    'create_time': 1389294592.02,
                    'ionice': ionice(ioclass=0, value=0),
                    'num_fds': None,
                    'memory_maps': None,
                    'cpu_percent': 0.0,
                    'terminal': None,
                    'ppid': 0,
                    'cwd': None,
                    'nice': 0,
                    'username': 'root',
                    'cpu_times': cputimes(user=0.27, system=1.05),
                    'io_counters': None,
                    'ext_memory_info': ext_meminfo(rss=self.rss,
                                                   vms=self.vms,
                                                   shared=1310720,
                                                   text=188416,
                                                   lib=0,
                                                   data=868352,
                                                   dirty=0),
                    'threads': [thread(id=1, user_time=0.27, system_time=1.04)],
                    'open_files': None,
                    'name': self.name,
                    'num_threads': 1,
                    'exe': self.exe,
                    'uids': user(real=0, effective=0, saved=0),
                    'gids': group(real=0, effective=0, saved=0),
                    'cpu_affinity': [0, 1, 2, 3],
                    'memory_percent': 0.03254831000922748,
                    'memory_info': meminfo(rss=self.rss, vms=self.vms)}

        process_iter_mock = (ProcessMock(
            pid=x['pid'],
            name=x['name'],
            rss=x['rss'],
            vms=x['vms'],
            exe=x['exe'])
            for x in process_info_list)

        getpid_mock.return_value = self.SELFMON_PID

        patch_psutil_process_iter = patch('psutil.process_iter',
                                          return_value=process_iter_mock)
        patch_psutil_process_iter.start()
        self.collector.collect()
        patch_psutil_process_iter.stop()
        self.assertPublished(publish_mock, 'postgres.ext_memory_info.rss',
                             1000000 + 100000 + 10000 + 1000 + 100)
        self.assertPublished(publish_mock, 'foo.ext_memory_info.rss', 1)
        self.assertPublished(publish_mock, 'bar.ext_memory_info.rss', 3)
        self.assertPublished(publish_mock, 'barexe.ext_memory_info.rss', 3)
        self.assertPublished(publish_mock,
                             'diamond-selfmon.ext_memory_info.rss', 1234)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = puppetagent
# coding=utf-8

"""
Collect stats from puppet agent's last_run_summary.yaml

#### Dependencies

 * yaml

"""

try:
    import yaml
    yaml  # workaround for pyflakes issue #13
except ImportError:
    yaml = None

import diamond.collector


class PuppetAgentCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(PuppetAgentCollector,
                            self).get_default_config_help()
        config_help.update({
            'yaml_path': "Path to last_run_summary.yaml",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PuppetAgentCollector, self).get_default_config()
        config.update({
            'yaml_path': '/var/lib/puppet/state/last_run_summary.yaml',
            'path':     'puppetagent',
            'method':   'Threaded',
        })
        return config

    def _get_summary(self):
        summary_fp = open(self.config['yaml_path'], 'r')

        try:
            summary = yaml.load(summary_fp)
        finally:
            summary_fp.close()

        return summary

    def collect(self):
        if yaml is None:
            self.log.error('Unable to import yaml')
            return

        summary = self._get_summary()

        for sect, data in summary.iteritems():
            for stat, value in data.iteritems():
                if value is None or isinstance(value, basestring):
                    continue

                metric = '.'.join([sect, stat])
                self.publish(metric, value)

########NEW FILE########
__FILENAME__ = testpuppetagent
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import patch

from diamond.collector import Collector
from puppetagent import PuppetAgentCollector

################################################################################


def run_only_if_yaml_is_available(func):
    try:
        import yaml
        yaml  # workaround for pyflakes issue #13
    except ImportError:
        yaml = None
    pred = lambda: yaml is not None
    return run_only(func, pred)


class TestPuppetAgentCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PuppetAgentCollector', {
            'interval': 10,
            'yaml_path': self.getFixturePath('last_run_summary.yaml')
        })

        self.collector = PuppetAgentCollector(config, None)

    def test_import(self):
        self.assertTrue(PuppetAgentCollector)

    @run_only_if_yaml_is_available
    @patch.object(Collector, 'publish')
    def test(self, publish_mock):

        self.collector.collect()

        metrics = {
            'changes.total': 1,
            'events.failure': 0,
            'events.success': 1,
            'events.total': 1,
            'resources.changed': 1,
            'resources.failed': 0,
            'resources.failed_to_restart': 0,
            'resources.out_of_sync': 1,
            'resources.restarted': 0,
            'resources.scheduled': 0,
            'resources.skipped': 6,
            'resources.total': 439,
            'time.anchor': 0.009641,
            'time.augeas': 1.286514,
            'time.config_retrieval': 8.06442093849182,
            'time.cron': 0.00089,
            'time.exec': 9.780635,
            'time.file': 1.729348,
            'time.filebucket': 0.000633,
            'time.firewall': 0.007807,
            'time.group': 0.013421,
            'time.last_run': 1377125556,
            'time.mailalias': 0.000335,
            'time.mount': 0.002749,
            'time.package': 1.831337,
            'time.resources': 0.000371,
            'time.service': 0.734021,
            'time.ssh_authorized_key': 0.017625,
            'time.total': 23.5117989384918,
            'time.user': 0.02927,
            'version.config': 1377123965,
        }

        unpublished_metrics = {
            'version.puppet': '2.7.14',
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

        self.assertPublishedMany(publish_mock, metrics)
        self.assertUnpublishedMany(publish_mock, unpublished_metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = puppetdashboard
# coding=utf-8

"""
Collect metrics from Puppet Dashboard

#### Dependencies

 * urllib2

"""

import urllib2
import re
import diamond.collector


class PuppetDashboardCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(PuppetDashboardCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': 'Hostname to collect from',
            'port': 'Port number to collect from',
            'path': 'Path to the dashboard',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PuppetDashboardCollector, self).get_default_config()
        config.update({
            'host': 'localhost',
            'port': 5678,
            'path': 'puppetdashboard',
        })
        return config

    def collect(self):
        try:
            response = urllib2.urlopen("http://%s:%s/" % (
                self.config['host'], int(self.config['port'])))
        except Exception, e:
            self.log.error('Couldnt connect to puppet-dashboard: %s', e)
            return {}

        for line in response.readlines():
            line = line.strip()

            if line == "":
                continue

            try:
                regex = re.compile(
                    "<a href=\"/nodes/(?P<key>[\w.]+)\">(?P<count>[\d.]+)</a>")
                r = regex.search(line)
                results = r.groupdict()

                self.publish(results['key'], results['count'])
            except Exception, e:
                self.log.error('Couldnt parse the output: %s', e)

########NEW FILE########
__FILENAME__ = testpuppetdashboard
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from puppetdashboard import PuppetDashboardCollector

################################################################################


class TestPuppetDashboardCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PuppetDashboardCollector', {
            'interval': 10
        })

        self.collector = PuppetDashboardCollector(config, None)

    def test_import(self):
        self.assertTrue(PuppetDashboardCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('index.html')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'unresponsive': 3,
            'pending': 0,
            'changed': 10,
            'unchanged': 4,
            'unreported': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('index.blank')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = puppetdb
# coding=utf-8

"""
Collect metrics from Puppet DB Dashboard

#### Dependencies

 * urllib2
 * json

"""

import urllib2
import diamond.collector
from diamond.convertor import time as time_convertor

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json


class PuppetDBCollector(diamond.collector.Collector):

    PATHS = {
        'memory': "v2/metrics/mbean/java.lang:type=Memory",
        'queue': "v2/metrics/mbean/org.apache.activemq:BrokerName=localhost"
        + ",Type=Queue,Destination=com.puppetlabs.puppetdb.commands",
        'processing-time': "v2/metrics/mbean/com.puppetlabs.puppetdb.command:"
        + "type=global,name=processing-time",
        'processed': "v2/metrics/mbean/com.puppetlabs.puppetdb.command:"
        + "type=global,name=processed",
        'retried': "v2/metrics/mbean/com.puppetlabs.puppetdb.command:"
        + "type=global,name=retried",
        'discarded': "v2/metrics/mbean/com.puppetlabs.puppetdb.command:"
        + "type=global,name=discarded",
        'fatal': "v2/metrics/mbean/com.puppetlabs.puppetdb.command:"
        + "type=global,name=fatal",
        'commands.service-time': "v2/metrics/mbean/com.puppetlabs.puppetdb."
        + "http.server:type=/v3/commands,name=service-time",
        'resources.service-time': "v2/metrics/mbean/com.puppetlabs.puppetdb."
        + "http.server:type=/v3/resources,name=service-time",
        'gc-time': "v2/metrics/mbean/com.puppetlabs.puppetdb.scf.storage:"
        + "type=default,name=gc-time",
        'duplicate-pct': "v2/metrics/mbean/com.puppetlabs.puppetdb.scf.storage:"
        + "type=default,name=duplicate-pct",
        'pct-resource-dupes': "v2/metrics/mbean/com.puppetlabs.puppetdb.query."
        + "population:type=default,name=pct-resource-dupes",
        'num-nodes': "v2/metrics/mbean/com.puppetlabs.puppetdb.query."
        + "population:type=default,name=num-nodes",
        'num-resources': "v2/metrics/mbean/com.puppetlabs.puppetdb.query."
        + "population:type=default,name=num-resources",
    }

    def get_default_config_help(self):
        config_help = super(PuppetDBCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': 'Hostname to collect from',
            'port': 'Port number to collect from',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PuppetDBCollector, self).get_default_config()
        config.update({
            'host': 'localhost',
            'port': 8080,
            'path': 'PuppetDB',
        })
        return config

    def fetch_metrics(self, url):
        try:
            url = "http://%s:%s/%s" % (
                self.config['host'], int(self.config['port']), url)
            response = urllib2.urlopen(url)
        except Exception, e:
            self.log.error('Couldnt connect to puppetdb: %s -> %s', url, e)
            return {}
        return json.load(response)

    def collect(self):
        rawmetrics = {}
        for subnode in self.PATHS:
            path = self.PATHS[subnode]
            rawmetrics[subnode] = self.fetch_metrics(path)

        self.publish_gauge('num_resources',
                           rawmetrics['num-resources']['Value'])
        self.publish_gauge('catalog_duplicate_pct',
                           rawmetrics['duplicate-pct']['Value'])
        self.publish_gauge(
            'sec_command',
            time_convertor.convert(
                rawmetrics['processing-time']['50thPercentile'],
                rawmetrics['processing-time']['LatencyUnit'],
                'seconds'))
        self.publish_gauge(
            'resources_service_time',
            time_convertor.convert(
                rawmetrics['resources.service-time']['50thPercentile'],
                rawmetrics['resources.service-time']['LatencyUnit'],
                'seconds'))
        self.publish_gauge(
            'enqueueing_service_time',
            time_convertor.convert(
                rawmetrics['commands.service-time']['50thPercentile'],
                rawmetrics['commands.service-time']['LatencyUnit'],
                'seconds'))

        self.publish_gauge('discarded', rawmetrics['discarded']['Count'])
        self.publish_gauge('processed', rawmetrics['processed']['Count'])
        self.publish_gauge('rejected', rawmetrics['fatal']['Count'])
        self.publish_gauge(
            'DB_Compaction',
            time_convertor.convert(
                rawmetrics['gc-time']['50thPercentile'],
                rawmetrics['gc-time']['LatencyUnit'],
                'seconds'))
        self.publish_gauge('resource_duplicate_pct',
                           rawmetrics['pct-resource-dupes']['Value'])
        self.publish_gauge('num_nodes',
                           rawmetrics['num-nodes']['Value'])

        self.publish_counter('queue.ProducerCount',
                             rawmetrics['queue']['ProducerCount'])
        self.publish_counter('queue.DequeueCount',
                             rawmetrics['queue']['DequeueCount'])
        self.publish_counter('queue.ConsumerCount',
                             rawmetrics['queue']['ConsumerCount'])
        self.publish_gauge('queue.QueueSize',
                           rawmetrics['queue']['QueueSize'])
        self.publish_counter('queue.ExpiredCount',
                             rawmetrics['queue']['ExpiredCount'])
        self.publish_counter('queue.EnqueueCount',
                             rawmetrics['queue']['EnqueueCount'])
        self.publish_counter('queue.InFlightCount',
                             rawmetrics['queue']['InFlightCount'])
        self.publish_gauge('queue.CursorPercentUsage',
                           rawmetrics['queue']['CursorPercentUsage'])
        self.publish_gauge('queue.MemoryUsagePortion',
                           rawmetrics['queue']['MemoryUsagePortion'])

        self.publish_gauge('memory.NonHeapMemoryUsage.used',
                           rawmetrics['memory']['NonHeapMemoryUsage']['used'])
        self.publish_gauge(
            'memory.NonHeapMemoryUsage.committed',
            rawmetrics['memory']['NonHeapMemoryUsage']['committed'])
        self.publish_gauge('memory.HeapMemoryUsage.used',
                           rawmetrics['memory']['HeapMemoryUsage']['used'])
        self.publish_gauge('memory.HeapMemoryUsage.committed',
                           rawmetrics['memory']['HeapMemoryUsage']['committed'])

########NEW FILE########
__FILENAME__ = testpuppetdb
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest

from puppetdb import PuppetDBCollector

################################################################################


class TestPuppetDBCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('PuppetDBCollector', {
            'interval': 10
        })

        self.collector = PuppetDBCollector(config, None)

    def test_import(self):
        self.assertTrue(PuppetDBCollector)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = rabbitmq
# coding=utf-8

"""
Collects data from RabbitMQ through the admin interface

#### Notes
  ** With added support for breaking down queue metrics by vhost, we have
     attempted to keep results generated by existing configurations from
     changing. This means that the old behaviour of clobbering queue metrics
     when a single queue name exists in multiple vhosts still exists if the
     configuration is not updated. If no vhosts block is defined it will also
     keep the metric path as it was historically with no vhost name in it.

        old path => systems.myServer.rabbitmq.queues.myQueue.*
        new path => systems.myServer.rabbitmq.myVhost.queues.myQueue.*

  ** If a [vhosts] section exists but is empty, then no queues will be polled.
  ** To poll all vhosts and all queues, add the following.
  **   [vhosts]
  **   * = *
  **

#### Dependencies

 * pyrabbit

"""

import diamond.collector

try:
    from numbers import Number
    Number  # workaround for pyflakes issue #13
    import pyrabbit.api
except ImportError:
    Number = None


class RabbitMQCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(RabbitMQCollector, self).get_default_config_help()
        config_help.update({
            'host': 'Hostname and port to collect from',
            'user': 'Username',
            'password': 'Password',
            'queues': 'Queues to publish. Leave empty to publish all.',
            'vhosts': 'A list of vhosts and queues for which we want to collect'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(RabbitMQCollector, self).get_default_config()
        config.update({
            'path': 'rabbitmq',
            'host': 'localhost:55672',
            'user': 'guest',
            'password': 'guest'
        })
        return config

    def collect(self):
        if Number is None:
            self.log.error('Unable to import either Number or pyrabbit.api')
            return {}

        try:
            client = pyrabbit.api.Client(self.config['host'],
                                         self.config['user'],
                                         self.config['password'])

            legacy = False

            if 'vhosts' not in self.config:
                legacy = True

                if 'queues' in self.config:
                    self.config['vhosts'] = {"*": self.config['queues']}
                else:
                    self.config['vhosts'] = {"*": ""}

            # Legacy configurations, those that don't include the [vhosts]
            # section require special care so that we do not break metric
            # gathering for people that were using this collector before the
            # update to support vhosts.

            if not legacy:
                vhost_names = client.get_vhost_names()
                if "*" in self.config['vhosts']:
                    for vhost in vhost_names:
                        # Copy the glob queue list to each vhost not
                        # specifically defined in the configuration.
                        if vhost not in self.config['vhosts']:
                            self.config['vhosts'][vhost] = self.config[
                                'vhosts']['*']

                    del self.config['vhosts']["*"]

            # Iterate all vhosts in our vhosts configuration.  For legacy this
            # is "*" to force a single run.
            for vhost in self.config['vhosts']:
                queues = self.config['vhosts'][vhost]

                # Allow the use of a asterix to glob the queues, but replace
                # with a empty string to match how legacy config was.
                if queues == "*":
                    queues = ""
                allowed_queues = queues.split()

                # When we fetch queues, we do not want to define a vhost if
                # legacy.
                if legacy:
                    vhost = None

                for queue in client.get_queues(vhost):
                    # If queues are defined and it doesn't match, then skip.
                    if (queue['name'] not in allowed_queues
                            and len(allowed_queues) > 0):
                        continue

                    for key in queue:
                        prefix = "queues"
                        if not legacy:
                            prefix = "vhosts.%s.%s" % (vhost, "queues")

                        name = '{0}.{1}'.format(prefix, queue['name'])
                        self._publish_metrics(name, [], key, queue)

            overview = client.get_overview()
            for key in overview:
                self._publish_metrics('', [], key, overview)
        except Exception, e:
            self.log.error('An error occurred collecting from RabbitMQ, %s', e)
            return {}

    def _publish_metrics(self, name, prev_keys, key, data):
        """Recursively publish keys"""
        value = data[key]
        keys = prev_keys + [key]
        if isinstance(value, dict):
            for new_key in value:
                self._publish_metrics(name, keys, new_key, value)
        elif isinstance(value, Number):
            joined_keys = '.'.join(keys)
            if name:
                publish_key = '{0}.{1}'.format(name, joined_keys)
            else:
                publish_key = joined_keys

            self.publish(publish_key, value)

########NEW FILE########
__FILENAME__ = testrabbitmq
#!/usr/bin/python
# coding=utf-8
################################################################################

import sys
from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from rabbitmq import RabbitMQCollector

################################################################################


def run_only_if_pyrabbit_is_available(func):
    pyrabbit = None
    if sys.version_info > (2, 5):
        try:
            import pyrabbit
            pyrabbit  # workaround for pyflakes issue #13
        except ImportError:
            pyrabbit = None
    pred = lambda: pyrabbit is not None
    return run_only(func, pred)


class TestRabbitMQCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('RabbitMQCollector', {
            'host': 'localhost:55672',
            'user': 'guest',
            'password': 'password'
        })
        self.collector = RabbitMQCollector(config, None)

    def test_import(self):
        self.assertTrue(RabbitMQCollector)

    @run_only_if_pyrabbit_is_available
    @patch('pyrabbit.api.Client')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys(self, publish_mock, client_mock):
        client = Mock()
        queue_data = {
            'more_keys': {'nested_key': 1},
            'key': 2,
            'string': 'str',
            'name': 'test_queue'
        }
        overview_data = {
            'more_keys': {'nested_key': 3},
            'key': 4,
            'string': 'string',
        }
        client_mock.return_value = client
        client.get_queues.return_value = [queue_data]
        client.get_overview.return_value = overview_data

        self.collector.collect()

        client.get_queues.assert_called_once_with(None)
        client.get_overview.assert_called_once_with()
        metrics = {
            'queues.test_queue.more_keys.nested_key': 1,
            'queues.test_queue.key': 2,
            'more_keys.nested_key': 3,
            'key': 4
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = redisstat
# coding=utf-8

"""
Collects data from one or more Redis Servers

#### Dependencies

 * redis

#### Notes

The collector is named an odd redisstat because of an import issue with
having the python library called redis and this collector's module being called
redis, so we use an odd name for this collector. This doesn't affect the usage
of this collector.

Example config file RedisCollector.conf

```
enabled=True
host=redis.example.com
port=16379
auth=PASSWORD
```

or for multi-instance mode:

```
enabled=True
instances = nick1@host1:port1, nick2@host2:port2/PASSWORD, ...
```

Note: when using the host/port config mode, the port number is used in
the metric key. When using the multi-instance mode, the nick will be used.
If not specified the port will be used.


"""

import diamond.collector
import time

try:
    import redis
    redis  # workaround for pyflakes issue #13
except ImportError:
    redis = None


class RedisCollector(diamond.collector.Collector):

    _DATABASE_COUNT = 16
    _DEFAULT_DB = 0
    _DEFAULT_HOST = 'localhost'
    _DEFAULT_PORT = 6379
    _DEFAULT_SOCK_TIMEOUT = 5
    _KEYS = {'clients.blocked': 'blocked_clients',
             'clients.connected': 'connected_clients',
             'clients.longest_output_list': 'client_longest_output_list',
             'cpu.parent.sys': 'used_cpu_sys',
             'cpu.children.sys': 'used_cpu_sys_children',
             'cpu.parent.user': 'used_cpu_user',
             'cpu.children.user': 'used_cpu_user_children',
             'hash_max_zipmap.entries': 'hash_max_zipmap_entries',
             'hash_max_zipmap.value': 'hash_max_zipmap_value',
             'keys.evicted': 'evicted_keys',
             'keys.expired': 'expired_keys',
             'keyspace.hits': 'keyspace_hits',
             'keyspace.misses': 'keyspace_misses',
             'last_save.changes_since': 'changes_since_last_save',
             'last_save.time': 'last_save_time',
             'memory.internal_view': 'used_memory',
             'memory.external_view': 'used_memory_rss',
             'memory.fragmentation_ratio': 'mem_fragmentation_ratio',
             'process.commands_processed': 'total_commands_processed',
             'process.connections_received': 'total_connections_received',
             'process.uptime': 'uptime_in_seconds',
             'pubsub.channels': 'pubsub_channels',
             'pubsub.patterns': 'pubsub_patterns',
             'slaves.connected': 'connected_slaves'}
    _RENAMED_KEYS = {'last_save.changes_since': 'rdb_changes_since_last_save',
                     'last_save.time': 'rdb_last_save_time'}

    def __init__(self, *args, **kwargs):
        super(RedisCollector, self).__init__(*args, **kwargs)

        instance_list = self.config['instances']
        # configobj make str of single-element list, let's convert
        if isinstance(instance_list, basestring):
            instance_list = [instance_list]

        # process original single redis instance
        if len(instance_list) == 0:
            host = self.config['host']
            port = int(self.config['port'])
            auth = self.config['auth']
            if auth is not None:
                instance_list.append('%s:%d/%s' % (host, port, auth))
            else:
                instance_list.append('%s:%d' % (host, port))

        self.instances = {}
        for instance in instance_list:

            if '@' in instance:
                (nickname, hostport) = instance.split('@', 2)
            else:
                nickname = None
                hostport = instance

            if '/' in hostport:
                parts = hostport.split('/')
                hostport = parts[0]
                auth = parts[1]
            else:
                auth = None

            if ':' in hostport:
                if hostport[0] == ':':
                    host = self._DEFAULT_HOST
                    port = int(hostport[1:])
                else:
                    parts = hostport.split(':')
                    host = parts[0]
                    port = int(parts[1])
            else:
                host = hostport
                port = self._DEFAULT_PORT

            if nickname is None:
                nickname = str(port)

            self.instances[nickname] = (host, port, auth)

    def get_default_config_help(self):
        config_help = super(RedisCollector, self).get_default_config_help()
        config_help.update({
            'host': 'Hostname to collect from',
            'port': 'Port number to collect from',
            'timeout': 'Socket timeout',
            'db': '',
            'auth': 'Password?',
            'databases': 'how many database instances to collect',
            'instances': "Redis addresses, comma separated, syntax:"
            + " nick1@host:port, nick2@:port or nick3@host"
        })
        return config_help

    def get_default_config(self):
        """
        Return default config

:rtype: dict

        """
        config = super(RedisCollector, self).get_default_config()
        config.update({
            'host': self._DEFAULT_HOST,
            'port': self._DEFAULT_PORT,
            'timeout': self._DEFAULT_SOCK_TIMEOUT,
            'db': self._DEFAULT_DB,
            'auth': None,
            'databases': self._DATABASE_COUNT,
            'path': 'redis',
            'instances': [],
        })
        return config

    def _client(self, host, port, auth):
        """Return a redis client for the configuration.

:param str host: redis host
:param int port: redis port
:rtype: redis.Redis

        """
        db = int(self.config['db'])
        timeout = int(self.config['timeout'])
        try:
            cli = redis.Redis(host=host, port=port,
                              db=db, socket_timeout=timeout, password=auth)
            cli.ping()
            return cli
        except Exception, ex:
            self.log.error("RedisCollector: failed to connect to %s:%i. %s.",
                           host, port, ex)

    def _precision(self, value):
        """Return the precision of the number

:param str value: The value to find the precision of
:rtype: int

        """
        value = str(value)
        decimal = value.rfind('.')
        if decimal == -1:
            return 0
        return len(value) - decimal - 1

    def _publish_key(self, nick, key):
        """Return the full key for the partial key.

:param str nick: Nickname for Redis instance
:param str key: The key name
:rtype: str

        """
        return '%s.%s' % (nick, key)

    def _get_info(self, host, port, auth):
        """Return info dict from specified Redis instance

:param str host: redis host
:param int port: redis port
:rtype: dict

        """

        client = self._client(host, port, auth)
        if client is None:
            return None

        info = client.info()
        del client
        return info

    def collect_instance(self, nick, host, port, auth):
        """Collect metrics from a single Redis instance

:param str nick: nickname of redis instance
:param str host: redis host
:param int port: redis port

        """

        # Connect to redis and get the info
        info = self._get_info(host, port, auth)
        if info is None:
            return

        # The structure should include the port for multiple instances per
        # server
        data = dict()

        # Iterate over the top level keys
        for key in self._KEYS:
            if self._KEYS[key] in info:
                data[key] = info[self._KEYS[key]]

        # Iterate over renamed keys for 2.6 support
        for key in self._RENAMED_KEYS:
            if self._RENAMED_KEYS[key] in info:
                data[key] = info[self._RENAMED_KEYS[key]]

        # Look for databaase speific stats
        for dbnum in range(0, int(self.config.get('databases',
                                  self._DATABASE_COUNT))):
            db = 'db%i' % dbnum
            if db in info:
                for key in info[db]:
                    data['%s.%s' % (db, key)] = info[db][key]

        # Time since last save
        for key in ['last_save_time', 'rdb_last_save_time']:
            if key in info:
                data['last_save.time_since'] = int(time.time()) - info[key]

        # Publish the data to graphite
        for key in data:
            self.publish(self._publish_key(nick, key),
                         data[key],
                         precision=self._precision(data[key]),
                         metric_type='GAUGE')

    def collect(self):
        """Collect the stats from the redis instance and publish them.

        """
        if redis is None:
            self.log.error('Unable to import module redis')
            return {}

        for nick in self.instances.keys():
            (host, port, auth) = self.instances[nick]
            self.collect_instance(nick, host, int(port), auth)

########NEW FILE########
__FILENAME__ = testredisstat
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch, call

from diamond.collector import Collector
from redisstat import RedisCollector

################################################################################


def run_only_if_redis_is_available(func):
    """Decorator for checking if python-redis is available.
    Note: this test will be silently skipped if python-redis is missing.
    """
    try:
        import redis
        redis  # workaround for pyflakes issue #13
    except ImportError:
        redis = None
    pred = lambda: redis is not None
    return run_only(func, pred)


class TestRedisCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('RedisCollector', {
            'interval': '1',
            'databases': 1,
        })

        self.collector = RedisCollector(config, None)

    def test_import(self):
        self.assertTrue(RedisCollector)

    @run_only_if_redis_is_available
    @patch.object(Collector, 'publish')
    def test_real_data(self, publish_mock):

        data_1 = {'pubsub_channels': 0,
                  'used_memory_peak_human': '700.71K',
                  'bgrewriteaof_in_progress': 0,
                  'connected_slaves': 0,
                  'uptime_in_days': 0,
                  'multiplexing_api': 'epoll',
                  'lru_clock': 954113,
                  'last_save_time': 1351718385,
                  'redis_version': '2.4.10',
                  'redis_git_sha1': 0,
                  'gcc_version': '4.4.6',
                  'connected_clients': 1,
                  'keyspace_misses': 0,
                  'used_memory': 726144,
                  'vm_enabled': 0,
                  'used_cpu_user_children': '0.00',
                  'used_memory_peak': 717528,
                  'role': 'master',
                  'total_commands_processed': 1,
                  'latest_fork_usec': 0,
                  'loading': 0,
                  'used_memory_rss': 7254016,
                  'total_connections_received': 1,
                  'pubsub_patterns': 0,
                  'aof_enabled': 0,
                  'used_cpu_sys': '0.02',
                  'used_memory_human': '709.12K',
                  'used_cpu_sys_children': '0.00',
                  'blocked_clients': 0,
                  'used_cpu_user': '0.00',
                  'client_biggest_input_buf': 0,
                  'arch_bits': 64,
                  'mem_fragmentation_ratio': '9.99',
                  'expired_keys': 0,
                  'evicted_keys': 0,
                  'bgsave_in_progress': 0,
                  'client_longest_output_list': 0,
                  'mem_allocator': 'jemalloc-2.2.5',
                  'process_id': 3020,
                  'uptime_in_seconds': 32,
                  'changes_since_last_save': 0,
                  'redis_git_dirty': 0,
                  'keyspace_hits': 0
                  }
        data_2 = {'pubsub_channels': 1,
                  'used_memory_peak_human': '1700.71K',
                  'bgrewriteaof_in_progress': 4,
                  'connected_slaves': 2,
                  'uptime_in_days': 1,
                  'multiplexing_api': 'epoll',
                  'lru_clock': 5954113,
                  'last_save_time': 51351718385,
                  'redis_version': '2.4.10',
                  'redis_git_sha1': 0,
                  'gcc_version': '4.4.6',
                  'connected_clients': 100,
                  'keyspace_misses': 670,
                  'used_memory': 1726144,
                  'vm_enabled': 0,
                  'used_cpu_user_children': '2.00',
                  'used_memory_peak': 1717528,
                  'role': 'master',
                  'total_commands_processed': 19764,
                  'latest_fork_usec': 8,
                  'loading': 0,
                  'used_memory_rss': 17254016,
                  'total_connections_received': 18764,
                  'pubsub_patterns': 0,
                  'aof_enabled': 0,
                  'used_cpu_sys': '0.05',
                  'used_memory_human': '1709.12K',
                  'used_cpu_sys_children': '0.09',
                  'blocked_clients': 8,
                  'used_cpu_user': '0.09',
                  'client_biggest_input_buf': 40,
                  'arch_bits': 64,
                  'mem_fragmentation_ratio': '0.99',
                  'expired_keys': 0,
                  'evicted_keys': 0,
                  'bgsave_in_progress': 0,
                  'client_longest_output_list': 0,
                  'mem_allocator': 'jemalloc-2.2.5',
                  'process_id': 3020,
                  'uptime_in_seconds': 95732,
                  'changes_since_last_save': 759,
                  'redis_git_dirty': 0,
                  'keyspace_hits': 5700
                  }

        patch_collector = patch.object(RedisCollector, '_get_info',
                                       Mock(return_value=data_1))
        patch_time = patch('time.time', Mock(return_value=10))

        patch_collector.start()
        patch_time.start()
        self.collector.collect()
        patch_collector.stop()
        patch_time.stop()

        self.assertPublishedMany(publish_mock, {})

        patch_collector = patch.object(RedisCollector, '_get_info',
                                       Mock(return_value=data_2))
        patch_time = patch('time.time', Mock(return_value=20))

        patch_collector.start()
        patch_time.start()
        self.collector.collect()
        patch_collector.stop()
        patch_time.stop()

        metrics = {'6379.process.uptime': 95732,
                   '6379.pubsub.channels': 1,
                   '6379.slaves.connected': 2,
                   '6379.process.connections_received': 18764,
                   '6379.clients.longest_output_list': 0,
                   '6379.process.commands_processed': 19764,
                   '6379.last_save.changes_since': 759,
                   '6379.memory.external_view': 17254016,
                   '6379.memory.fragmentation_ratio': 0.99,
                   '6379.last_save.time': 51351718385,
                   '6379.clients.connected': 100,
                   '6379.clients.blocked': 8,
                   '6379.pubsub.patterns': 0,
                   '6379.cpu.parent.user': 0.09,
                   '6379.last_save.time_since': -51351718365,
                   '6379.memory.internal_view': 1726144,
                   '6379.cpu.parent.sys': 0.05,
                   '6379.keyspace.misses': 670,
                   '6379.keys.expired': 0,
                   '6379.keys.evicted': 0,
                   '6379.keyspace.hits': 5700,
                   }

        self.assertPublishedMany(publish_mock, metrics)

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

    @run_only_if_redis_is_available
    @patch.object(Collector, 'publish')
    def test_hostport_or_instance_config(self, publish_mock):

        testcases = {
            'default': {
                'config': {},  # test default settings
                'calls': [call('6379', 'localhost', 6379, None)],
            },
            'host_set': {
                'config': {'host': 'myhost'},
                'calls': [call('6379', 'myhost', 6379, None)],
            },
            'port_set': {
                'config': {'port': 5005},
                'calls': [call('5005', 'localhost', 5005, None)],
            },
            'hostport_set': {
                'config': {'host': 'megahost', 'port': 5005},
                'calls': [call('5005', 'megahost', 5005, None)],
            },
            'instance_1_host': {
                'config': {'instances': ['nick@myhost']},
                'calls': [call('nick', 'myhost', 6379, None)],
            },
            'instance_1_port': {
                'config': {'instances': ['nick@:9191']},
                'calls': [call('nick', 'localhost', 9191, None)],
            },
            'instance_1_hostport': {
                'config': {'instances': ['nick@host1:8765']},
                'calls': [call('nick', 'host1', 8765, None)],
            },
            'instance_2': {
                'config': {'instances': ['foo@hostX', 'bar@:1000']},
                'calls': [
                    call('foo', 'hostX', 6379, None),
                    call('bar', 'localhost', 1000, None)
                ],
            },
            'old_and_new': {
                'config': {
                    'host': 'myhost',
                    'port': 1234,
                    'instances': [
                        'foo@hostX',
                        'bar@:1000',
                        'hostonly',
                        ':1234'
                    ]
                },
                'calls': [
                    call('foo', 'hostX', 6379, None),
                    call('bar', 'localhost', 1000, None),
                    call('6379', 'hostonly', 6379, None),
                    call('1234', 'localhost', 1234, None),
                ],
            },
        }

        for testname, data in testcases.items():
            config = get_collector_config('RedisCollector', data['config'])

            collector = RedisCollector(config, None)

            mock = Mock(return_value={}, name=testname)
            patch_c = patch.object(RedisCollector, 'collect_instance', mock)

            patch_c.start()
            collector.collect()
            patch_c.stop()

            expected_call_count = len(data['calls'])
            self.assertEqual(mock.call_count, expected_call_count,
                             msg='[%s] mock.calls=%d != expected_calls=%d' %
                             (testname, mock.call_count, expected_call_count))
            for exp_call in data['calls']:
                # Test expected calls 1 by 1,
                # because self.instances is a dict (=random order)
                mock.assert_has_calls(exp_call)

    @run_only_if_redis_is_available
    @patch.object(Collector, 'publish')
    def test_key_naming_when_using_instances(self, publish_mock):

        config_data = {
            'instances': [
                'nick1@host1:1111',
                'nick2@:2222',
                'nick3@host3',
                'bla'
            ]
        }
        get_info_data = {
            'total_connections_received': 200,
            'total_commands_processed': 100,
        }
        expected_calls = [
            call('nick1.process.connections_received', 200, precision=0,
                 metric_type='GAUGE'),
            call('nick1.process.commands_processed', 100, precision=0,
                 metric_type='GAUGE'),
            call('nick2.process.connections_received', 200, precision=0,
                 metric_type='GAUGE'),
            call('nick2.process.commands_processed', 100, precision=0,
                 metric_type='GAUGE'),
            call('nick3.process.connections_received', 200, precision=0,
                 metric_type='GAUGE'),
            call('nick3.process.commands_processed', 100, precision=0,
                 metric_type='GAUGE'),
            call('6379.process.connections_received', 200, precision=0,
                 metric_type='GAUGE'),
            call('6379.process.commands_processed', 100, precision=0,
                 metric_type='GAUGE'),
        ]

        config = get_collector_config('RedisCollector', config_data)
        collector = RedisCollector(config, None)

        patch_c = patch.object(RedisCollector, '_get_info',
                               Mock(return_value=get_info_data))

        patch_c.start()
        collector.collect()
        patch_c.stop()

        self.assertEqual(publish_mock.call_count, len(expected_calls))
        for exp_call in expected_calls:
            # Test expected calls 1 by 1,
            # because self.instances is a dict (=random order)
            publish_mock.assert_has_calls(exp_call)


################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = resqueweb
# coding=utf-8

"""
Collects data for Resque Web

#### Dependencies

 * urllib2

"""

import urllib2
import diamond.collector


class ResqueWebCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(ResqueWebCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ResqueWebCollector, self).get_default_config()
        config.update({
            'host': 'localhost',
            'port': 5678,
            'path': 'resqueweb',
        })
        return config

    def collect(self):
        try:
            response = urllib2.urlopen("http://%s:%s/stats.txt" % (
                self.config['host'], int(self.config['port'])))
        except Exception, e:
            self.log.error('Couldnt connect to resque-web: %s', e)
            return {}

        for data in response.read().split("\n"):
            if data == "":
                continue

            item, count = data.strip().split("=")
            try:
                count = int(count)
                (item, queue) = item.split(".")

                if item == "resque":
                    if queue[-1] == "+":
                        self.publish("%s.total" % queue.replace("+", ""), count)
                    else:
                        self.publish("%s.current" % queue, count)
                else:
                    self.publish("queue.%s.current" % queue, count)

            except Exception, e:
                self.log.error('Couldnt parse the queue: %s', e)

########NEW FILE########
__FILENAME__ = testresqueweb
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from resqueweb import ResqueWebCollector

################################################################################


class TestResqueWebCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('ResqueWebCollector', {
            'interval': 10
        })

        self.collector = ResqueWebCollector(config, None)

    def test_import(self):
        self.assertTrue(ResqueWebCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('stats.txt')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'pending.current': 2,
            'processed.total': 11686516,
            'failed.total': 38667,
            'workers.current': 9,
            'working.current': 2,
            'queue.low.current': 4,
            'queue.mail.current': 3,
            'queue.realtime.current': 9,
            'queue.normal.current': 1,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('stats_blank.txt')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = s3
# coding=utf-8

"""
The S3BucketCollector collects bucket size using boto

#### Dependencies

  * boto (https://github.com/boto/boto)
"""

import diamond.collector
try:
    import boto
    boto
    from boto.s3.connection import S3Connection
except ImportError:
    boto = None


class S3BucketCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(S3BucketCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(S3BucketCollector, self).get_default_config()
        config.update({
            'enabled':   'False',
            'path':      'aws.s3',
            'byte_unit': 'byte'
        })
        return config

    def getBucket(self, aws_access, aws_secret, bucket_name):
        self.log.info("S3: Open Bucket, %s, %s, %s" % (bucket_name, aws_access,
                                                       aws_secret))
        s3 = S3Connection(aws_access, aws_secret)
        return s3.lookup(bucket_name)

    def getBucketSize(self, bucket):
        total_bytes = 0
        for key in bucket:
            total_bytes += key.size
        return total_bytes

    def collect(self):
        """
        Collect s3 bucket stats
        """
        if boto is None:
            self.log.error("Unable to import boto python module")
            return {}
        for s3instance in self.config['s3']:
            self.log.info("S3: byte_unit: %s" % self.config['byte_unit'])
            aws_access = self.config['s3'][s3instance]['aws_access_key']
            aws_secret = self.config['s3'][s3instance]['aws_secret_key']
            for bucket_name in self.config['s3'][s3instance]['buckets']:
                bucket = self.getBucket(aws_access, aws_secret, bucket_name)

                # collect bucket size
                total_size = self.getBucketSize(bucket)
                for byte_unit in self.config['byte_unit']:
                    new_size = diamond.convertor.binary.convert(
                        value=total_size,
                        oldUnit='byte',
                        newUnit=byte_unit
                    )
                    self.publish("%s.size.%s" % (bucket_name, byte_unit),
                                 new_size)

########NEW FILE########
__FILENAME__ = tests3
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest

from s3 import S3BucketCollector

################################################################################


class TestS3BucketCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('S3BucketCollector', {
            'interval': 10
        })

        self.collector = S3BucketCollector(config, None)

    def test_import(self):
        self.assertTrue(S3BucketCollector)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = servertechpdu
# coding=utf-8

"""
SNMPCollector for Server Tech PDUs

Server Tech is a manufacturer of PDUs
http://www.servertech.com/

"""

import time
import re
import os
import sys

# Fix Path for locating the SNMPCollector
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
                                             '../',
                                             'snmp',
                                             )))

from diamond.metric import Metric
from snmp import SNMPCollector as parent_SNMPCollector


class ServerTechPDUCollector(parent_SNMPCollector):
    """
    SNMPCollector for ServerTech PDUs
    """

    PDU_SYSTEM_GAUGES = {
        "systemTotalWatts": "1.3.6.1.4.1.1718.3.1.6"
    }

    PDU_INFEED_NAMES = "1.3.6.1.4.1.1718.3.2.2.1.3"

    PDU_INFEED_GAUGES = {
        "infeedCapacityAmps": "1.3.6.1.4.1.1718.3.2.2.1.10",
        "infeedVolts": "1.3.6.1.4.1.1718.3.2.2.1.11",
        "infeedAmps": "1.3.6.1.4.1.1718.3.2.2.1.7",
        "infeedWatts": "1.3.6.1.4.1.1718.3.2.2.1.12"
    }

    def get_default_config_help(self):
        config_help = super(ServerTechPDUCollector,
                            self).get_default_config_help()
        config_help.update({
            'host': 'PDU dns address',
            'port': 'PDU port to collect snmp data',
            'community': 'SNMP community'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ServerTechPDUCollector, self).get_default_config()
        config.update({
            'path':     'pdu',
            'timeout': 15,
            'retries': 3,
        })
        return config

    def collect_snmp(self, device, host, port, community):
        """
        Collect stats from device
        """
        # Log
        self.log.info("Collecting ServerTech PDU statistics from: %s" % device)

        # Set timestamp
        timestamp = time.time()

        inputFeeds = {}

        # Collect PDU input gauge values
        for gaugeName, gaugeOid in self.PDU_SYSTEM_GAUGES.items():
            systemGauges = self.walk(gaugeOid, host, port, community)
            for o, gaugeValue in systemGauges.items():
                # Get Metric Name
                metricName = gaugeName
                # Get Metric Value
                metricValue = float(gaugeValue)
                # Get Metric Path
                metricPath = '.'.join(['devices', device, 'system', metricName])
                # Create Metric
                metric = Metric(metricPath, metricValue, timestamp, 2)
                # Publish Metric
                self.publish_metric(metric)

        # Collect PDU input feed names
        inputFeedNames = self.walk(self.PDU_INFEED_NAMES, host, port, community)
        for o, inputFeedName in inputFeedNames.items():
            # Extract input feed name
            inputFeed = ".".join(o.split(".")[-2:])
            inputFeeds[inputFeed] = inputFeedName

        # Collect PDU input gauge values
        for gaugeName, gaugeOid in self.PDU_INFEED_GAUGES.items():
            inputFeedGauges = self.walk(gaugeOid, host, port, community)
            for o, gaugeValue in inputFeedGauges.items():
                # Extract input feed name
                inputFeed = ".".join(o.split(".")[-2:])

                # Get Metric Name
                metricName = '.'.join([re.sub(r'\.|\\', '_',
                                              inputFeeds[inputFeed]),
                                       gaugeName])

                # Get Metric Value
                if gaugeName == "infeedVolts":
                    # Note: Voltage is in "tenth volts", so divide by 10
                    metricValue = float(gaugeValue) / 10.0
                elif gaugeName == "infeedAmps":
                    # Note: Amps is in "hundredth amps", so divide by 100
                    metricValue = float(gaugeValue) / 100.0
                else:
                    metricValue = float(gaugeValue)

                # Get Metric Path
                metricPath = '.'.join(['devices', device, 'input', metricName])
                # Create Metric
                metric = Metric(metricPath, metricValue, timestamp, 2)
                # Publish Metric
                self.publish_metric(metric)

########NEW FILE########
__FILENAME__ = testservertechpdu
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from servertechpdu import ServerTechPDUCollector


class TestServerTechPDUCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('ServerTechPDUCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = ServerTechPDUCollector(config, None)

    def test_import(self):
        self.assertTrue(ServerTechPDUCollector)

########NEW FILE########
__FILENAME__ = sidekiqweb
# coding=utf-8

"""
Collects data form sidekiq web

#### Dependencies

 * urllib2
 * json (or simeplejson)

"""

try:
    import json

    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import urllib2
import diamond.collector


class SidekiqWebCollector(diamond.collector.Collector):
    def get_default_config_help(self):
        config_help = super(SidekiqWebCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(SidekiqWebCollector, self).get_default_config()
        config.update({
            'host': 'localhost',
            'port': 9999,
            'byte_unit': ['byte'],
        })
        return config

    def collect(self):
        try:
            response = urllib2.urlopen("http://%s:%s/dashboard/stats" % (
                self.config['host'], int(self.config['port'])))
        except Exception, e:
            self.log.error('Couldnt connect to sidekiq-web: %s', e)
            return {}

        try:
            j = json.loads(response.read())
        except Exception, e:
            self.log.error('Couldnt parse json: %s', e)
            return {}

        for k in j:
            for item, value in j[k].items():

                if isinstance(value, (str, unicode)) and 'M' in value:
                    value = float(value.replace('M', ''))
                    for unit in self.config['byte_unit']:
                        unit_value = diamond.convertor.binary.convert(
                            value=value,
                            oldUnit='megabyte',
                            newUnit=unit)

                        self.publish("%s.%s_%s" % (k, item, unit), unit_value)
                else:
                    self.publish("%s.%s" % (k, item), value)

########NEW FILE########
__FILENAME__ = testsidekiqweb
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from sidekiqweb import SidekiqWebCollector

################################################################################


class TestSidekiqWebCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SidekiqWebCollector', {
            'interval': 10
        })

        self.collector = SidekiqWebCollector(config, None)

    def test_import(self):
        self.assertTrue(SidekiqWebCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('stats')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        metrics = {
            'redis.connected_clients': 22,
            'redis.uptime_in_days': 62,
            'redis.used_memory_human_byte': 1426063.36,
            'redis.used_memory_peak_human_byte': 8598323.2,
            'sidekiq.busy': 0,
            'sidekiq.default_latency': 0,
            'sidekiq.enqueued': 0,
            'sidekiq.failed': 22,
            'sidekiq.processed': 4622701,
            'sidekiq.retries': 0,
            'sidekiq.scheduled': 30,
        }

        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        patch_urlopen = patch('urllib2.urlopen', Mock(
            return_value=self.getFixture('stats_blank')))

        patch_urlopen.start()
        self.collector.collect()
        patch_urlopen.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = slabinfo
# coding=utf-8

"""
The SlabInfoCollector collects metrics on process stats from
/proc/slabinfo

#### Dependencies

 * /proc/slabinfo

"""

import platform
import os
import diamond.collector

# Detect the architecture of the system
# and set the counters for MAX_VALUES
# appropriately. Otherwise, rolling over
# counters will cause incorrect or
# negative values.
if platform.architecture()[0] == '64bit':
    counter = (2 ** 64) - 1
else:
    counter = (2 ** 32) - 1


class SlabInfoCollector(diamond.collector.Collector):

    PROC = '/proc/slabinfo'

    def get_default_config_help(self):
        config_help = super(SlabInfoCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(SlabInfoCollector, self).get_default_config()
        config.update({
            'path':     'slabinfo'
        })
        return config

    def collect(self):
        """
        Collect process stat data
        """
        if not os.access(self.PROC, os.R_OK):
            return False

        #Open PROC file
        file = open(self.PROC, 'r')

        #Get data
        for line in file:
            if line.startswith('slabinfo'):
                continue

            if line.startswith('#'):
                keys = line.split()[1:]
                continue

            data = line.split()

            for key in ['<active_objs>', '<num_objs>', '<objsize>',
                        '<objperslab>', '<pagesperslab>']:
                i = keys.index(key)
                metric_name = data[0] + '.' + key.replace(
                    '<', '').replace('>', '')
                metric_value = int(data[i])
                self.publish(metric_name, metric_value)

            for key in ['<limit>', '<batchcount>', '<sharedfactor>']:
                i = keys.index(key)
                metric_name = data[0] + '.tunables.' + key.replace(
                    '<', '').replace('>', '')
                metric_value = int(data[i])
                self.publish(metric_name, metric_value)

            for key in ['<active_slabs>', '<num_slabs>', '<sharedavail>']:
                i = keys.index(key)
                metric_name = data[0] + '.slabdata.' + key.replace(
                    '<', '').replace('>', '')
                metric_value = int(data[i])
                self.publish(metric_name, metric_value)

        #Close file
        file.close()

########NEW FILE########
__FILENAME__ = testslabinfo
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from slabinfo import SlabInfoCollector

################################################################################


class TestSlabInfoCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SlabInfoCollector', {
            'interval': 1
        })

        self.collector = SlabInfoCollector(config, None)

    def test_import(self):
        self.assertTrue(SlabInfoCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_stat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/slabinfo', 'r')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        SlabInfoCollector.PROC = self.getFixturePath('slabinfo')
        self.collector.collect()

        metrics = self.getPickledResults('expected.pkl')

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = smart
# coding=utf-8

"""
Collect data from S.M.A.R.T.'s attribute reporting.

#### Dependencies

 * [smartmontools](http://sourceforge.net/apps/trac/smartmontools/wiki)

"""

import diamond.collector
import subprocess
import re
import os
from diamond.collector import str_to_bool


class SmartCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(SmartCollector, self).get_default_config_help()
        config_help.update({
            'devices': "device regex to collect stats on",
            'bin':         'The path to the smartctl binary',
            'use_sudo':    'Use sudo?',
            'sudo_cmd':    'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns default configuration options.
        """
        config = super(SmartCollector, self).get_default_config()
        config.update({
            'path': 'smart',
            'bin': 'smartctl',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
            'devices': '^disk[0-9]$|^sd[a-z]$|^hd[a-z]$',
            'method': 'Threaded'
        })
        return config

    def collect(self):
        """
        Collect and publish S.M.A.R.T. attributes
        """
        devices = re.compile(self.config['devices'])

        for device in os.listdir('/dev'):
            if devices.match(device):

                command = [self.config['bin'], "-A", os.path.join('/dev',
                                                                  device)]

                if str_to_bool(self.config['use_sudo']):
                    command.insert(0, self.config['sudo_cmd'])

                attributes = subprocess.Popen(
                    command,
                    stdout=subprocess.PIPE
                ).communicate()[0].strip().splitlines()

                metrics = {}

                start_line = self.find_attr_start_line(attributes)
                for attr in attributes[start_line:]:
                    attribute = attr.split()
                    if attribute[1] != "Unknown_Attribute":
                        metric = "%s.%s" % (device, attribute[1])
                    else:
                        metric = "%s.%s" % (device, attribute[0])

                    # New metric? Store it
                    if metric not in metrics:
                        metrics[metric] = attribute[9]
                    # Duplicate metric? Only store if it has a larger value
                    # This happens semi-often with the Temperature_Celsius
                    # attribute You will have a PASS/FAIL after the real temp,
                    # so only overwrite if The earlier one was a
                    # PASS/FAIL (0/1)
                    elif metrics[metric] == 0 and attribute[9] > 0:
                        metrics[metric] = attribute[9]
                    else:
                        continue

                for metric in metrics.keys():
                    self.publish(metric, metrics[metric])

    def find_attr_start_line(self, lines, min_line=4, max_line=9):
        """
        Return line number of the first real attribute and value.
        The first line is 0.  If the 'ATTRIBUTE_NAME' header is not
        found, return the index after max_line.
        """
        for idx, line in enumerate(lines[min_line:max_line]):
            col = line.split()
            if len(col) > 1 and col[1] == 'ATTRIBUTE_NAME':
                return idx + min_line + 1

        self.log.warn('ATTRIBUTE_NAME not found in second column of'
                      ' smartctl output between lines %d and %d.'
                      % (min_line, max_line))

        return max_line + 1

########NEW FILE########
__FILENAME__ = testsmart
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import call
from mock import patch

from diamond.collector import Collector
from smart import SmartCollector

################################################################################


class TestSmartCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SmartCollector', {
            'interval': 10,
            'bin': 'true',
        })

        self.collector = SmartCollector(config, None)

    def test_import(self):
        self.assertTrue(SmartCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_osx_missing(self, publish_mock):
        patch_listdir = patch('os.listdir', Mock(return_value=['disk0']))
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('osx_missing').getvalue(),
                '')))
        patch_listdir.start()
        patch_communicate.start()
        self.collector.collect()
        patch_listdir.stop()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {})

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_osx_ssd(self, publish_mock):
        patch_listdir = patch('os.listdir', Mock(return_value=['disk0']))
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('osx_ssd').getvalue(),
                '')))
        patch_listdir.start()
        patch_communicate.start()
        self.collector.collect()
        patch_listdir.stop()
        patch_communicate.stop()

        self.assertPublishedMany(publish_mock, {
            'disk0.172': 0,
            'disk0.Head_Amplitude': 100,
            'disk0.Reallocated_Sector_Ct': 0,
            'disk0.Temperature_Celsius': 128,
            'disk0.174': 3,
            'disk0.Reported_Uncorrect': 0,
            'disk0.Raw_Read_Error_Rate': 5849487,
            'disk0.Power_On_Hours': 199389561752279,
            'disk0.Total_LBAs_Read': 17985,
            'disk0.Power_Cycle_Count': 381,
            'disk0.Hardware_ECC_Recovered': 5849487,
            'disk0.171': 0,
            'disk0.Soft_Read_Error_Rate': 5849487,
            'disk0.234': 2447,
            'disk0.Program_Fail_Cnt_Total': 0,
            'disk0.Media_Wearout_Indicator': 4881,
            'disk0.Erase_Fail_Count_Total': 0,
            'disk0.Wear_Leveling_Count': 2,
            'disk0.Reallocated_Event_Count': 0,
            'disk0.Total_LBAs_Written': 2447,
            'disk0.Soft_ECC_Correction': 5849487,
        })

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_centos55_hdd(self, publish_mock):
        patch_listdir = patch('os.listdir', Mock(return_value=['sda']))
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('centos5.5_hdd').getvalue(),
                '')))

        patch_listdir.start()
        patch_communicate.start()
        self.collector.collect()
        patch_listdir.stop()
        patch_communicate.stop()

        metrics = {
            'sda.Temperature_Celsius': 28,
            'sda.Power_On_Hours': 6827,
            'sda.Power_Cycle_Count': 7,
            'sda.Power-Off_Retract_Count': 5,
            'sda.UDMA_CRC_Error_Count': 0,
            'sda.Load_Cycle_Count': 2,
            'sda.Calibration_Retry_Count': 0,
            'sda.Spin_Up_Time': 3991,
            'sda.Spin_Retry_Count': 0,
            'sda.Multi_Zone_Error_Rate': 0,
            'sda.Raw_Read_Error_Rate': 0,
            'sda.Reallocated_Event_Count': 0,
            'sda.Start_Stop_Count': 8,
            'sda.Offline_Uncorrectable': 0,
            'sda.Current_Pending_Sector': 0,
            'sda.Reallocated_Sector_Ct': 0,
            'sda.Seek_Error_Rate': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_debian_invalid_checksum_warning(
            self, publish_mock):
        fixture_data = self.getFixture(
            'debian_invalid_checksum_warning').getvalue()
        patch_listdir = patch('os.listdir', Mock(return_value=['sda']))
        patch_communicate = patch('subprocess.Popen.communicate',
                                  Mock(return_value=(fixture_data, '')))

        patch_listdir.start()
        patch_communicate.start()
        self.collector.collect()
        patch_listdir.stop()
        patch_communicate.stop()

        metrics = {
            'sda.Raw_Read_Error_Rate': 0,
            'sda.Spin_Up_Time': 4225,
            'sda.Start_Stop_Count': 13,
            'sda.Reallocated_Sector_Ct': 0,
            'sda.Seek_Error_Rate': 0,
            'sda.Power_On_Hours': 88,
            'sda.Spin_Retry_Count': 0,
            'sda.Calibration_Retry_Count': 0,
            'sda.Power_Cycle_Count': 13,
            'sda.Power-Off_Retract_Count': 7,
            'sda.Load_Cycle_Count': 5,
            'sda.Temperature_Celsius': 35,
            'sda.Reallocated_Event_Count': 0,
            'sda.Current_Pending_Sector': 0,
            'sda.Offline_Uncorrectable': 0,
            'sda.UDMA_CRC_Error_Count': 0,
            'sda.Multi_Zone_Error_Rate': 0,
        }

        header_call = call('sda.ATTRIBUTE_NAME', 'RAW_VALUE')
        published_metric_header = header_call in publish_mock.mock_calls
        assert not published_metric_header, "published metric for header row"

        self.assertPublishedMany(publish_mock, metrics)

    def test_find_attr_start_line(self):
        def get_fixture_lines(fixture):
            return self.getFixture(fixture).getvalue().strip().splitlines()

        def assert_attrs_start_at(expected, fixture):
            lines = get_fixture_lines(fixture)
            self.assertEqual(expected,
                             self.collector.find_attr_start_line(lines))

        lines = get_fixture_lines('osx_missing')
        self.assertEqual(5, self.collector.find_attr_start_line(lines, 2, 4))

        assert_attrs_start_at(7, 'osx_ssd')
        assert_attrs_start_at(7, 'centos5.5_hdd')
        assert_attrs_start_at(8, 'debian_invalid_checksum_warning')

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = snmp
# coding=utf-8

"""
SNMPCollector is a special collector for collecting data from using SNMP

#### Dependencies

 * pysnmp

"""

import socket

import warnings

# pysnmp packages on debian 6.0 use sha and md5 which are deprecated
# packages. there is nothing to be done about it until pysnmp
# updates to use new hashlib module -- ignoring warning for now
old_showwarning = warnings.showwarning
warnings.filterwarnings("ignore", category=DeprecationWarning)

cmdgen = None

try:
    import pysnmp.entity.rfc3413.oneliner.cmdgen as cmdgen
    import pysnmp.debug
    cmdgen  # workaround for pyflakes issue #13
    pysnmp  # workaround for pyflakes issue #13
except ImportError:
    pysnmp = None
    cmdgen = None

warnings.showwarning = old_showwarning

import diamond.collector


class SNMPCollector(diamond.collector.Collector):

    def __init__(self, config, handlers):
        """
        Create a new instance of the SNMPCollector class
        """
        # Initialize base Class
        diamond.collector.Collector.__init__(self, config, handlers)

    def get_default_config_help(self):
        config_help = super(SNMPCollector, self).get_default_config_help()
        config_help.update({
            'timeout': 'Seconds before timing out the snmp connection',
            'retries': 'Number of times to retry before bailing',
        })
        return config_help

    def get_default_config(self):
        # Initialize default config
        default_config = super(SNMPCollector, self).get_default_config()
        default_config['path_suffix'] = ''
        default_config['path_prefix'] = 'systems'
        default_config['timeout'] = 5
        default_config['retries'] = 3
        # Return default config
        return default_config

    def get_schedule(self):
        """
        Override SNMPCollector.get_schedule
        """
        schedule = {}

        if not cmdgen:
            self.log.error(
                'pysnmp.entity.rfc3413.oneliner.cmdgen failed to load')
            return

        # Initialize SNMP Command Generator
        self.snmpCmdGen = cmdgen.CommandGenerator()

        if 'devices' in self.config:
            for device in self.config['devices']:
                # Get Device Config
                c = self.config['devices'][device]
                # Get Task Name
                task = "_".join([self.__class__.__name__, device])
                # Check if task is already in schedule
                if task in schedule:
                    raise KeyError("Duplicate device scheduled")
                schedule[task] = (self.collect_snmp, (device,
                                                      c['host'],
                                                      int(c['port']),
                                                      c['community']),
                                  int(self.config['splay']),
                                  int(self.config['interval']))
        return schedule

    def _convert_to_oid(self, s):
        d = s.split(".")
        return tuple([int(x) for x in d])

    def _convert_from_oid(self, oid):
        return ".".join([str(x) for x in oid])

    def get(self, oid, host, port, community):
        """
        Perform SNMP get for a given OID
        """
        # Initialize return value
        ret = {}

        # Convert OID to tuple if necessary
        if not isinstance(oid, tuple):
            oid = self._convert_to_oid(oid)

        # Convert Host to IP if necessary
        host = socket.gethostbyname(host)

        # Assemble SNMP Auth Data
        snmpAuthData = cmdgen.CommunityData('agent', community)

        # Assemble SNMP Transport Data
        snmpTransportData = cmdgen.UdpTransportTarget(
            (host, port),
            int(self.config['timeout']),
            int(self.config['retries']))

        # Assemble SNMP Next Command
        result = self.snmpCmdGen.getCmd(snmpAuthData, snmpTransportData, oid)
        varBind = result[3]

        # TODO: Error check

        for o, v in varBind:
            ret[o.prettyPrint()] = v.prettyPrint()

        return ret

    def walk(self, oid, host, port, community):
        """
        Perform an SNMP walk on a given OID
        """
        # Initialize return value
        ret = {}

        # Convert OID to tuple if necessary
        if not isinstance(oid, tuple):
            oid = self._convert_to_oid(oid)

        # Convert Host to IP if necessary
        host = socket.gethostbyname(host)

        # Assemble SNMP Auth Data
        snmpAuthData = cmdgen.CommunityData('agent', community)

        # Assemble SNMP Transport Data
        snmpTransportData = cmdgen.UdpTransportTarget(
            (host, port),
            int(self.config['timeout']),
            int(self.config['retries']))

        # Assemble SNMP Next Command
        resultTable = self.snmpCmdGen.nextCmd(snmpAuthData,
                                              snmpTransportData,
                                              oid)
        varBindTable = resultTable[3]

        # TODO: Error Check

        for varBindTableRow in varBindTable:
            for o, v in varBindTableRow:
                ret[o.prettyPrint()] = v.prettyPrint()

        return ret

########NEW FILE########
__FILENAME__ = testsnmp
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from snmp import SNMPCollector


class TestSNMPCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('SNMPCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = SNMPCollector(config, None)

    def test_import(self):
        self.assertTrue(SNMPCollector)

########NEW FILE########
__FILENAME__ = snmpinterface
# coding=utf-8

"""
The SNMPInterfaceCollector is designed for collecting interface data from
remote SNMP-enabled devices such as routers and switches using SNMP IF_MIB

#### Installation

The snmpinterfacecollector.py module should be installed into your Diamond
installation collectors directory. This directory is defined
in diamond.cfg under the *collectors_path* directive. This defaults to
*/usr/lib/diamond/collectors/* on Ubuntu.

The SNMPInterfaceCollector.cfg file should be installed into your diamond
installation config directory. This directory is defined
in diamond.cfg under the *collectors_config_path* directive. This defaults to
*/etc/diamond/* on Ubuntu.

Once the collector is installed and configured, you can wait for diamond to
pick up the new collector automatically, or simply restart diamond.

#### Configuration

Below is an example configuration for the SNMPInterfaceCollector. The collector
can collect data any number of devices by adding configuration sections
under the *devices* header. By default the collector will collect every 60
seconds. This might be a bit excessive and put unnecessary load on the
devices being polled. You may wish to change this to every 300 seconds. However
you need modify your graphite data retentions to handle this properly.

```
    # Options for SNMPInterfaceCollector
    path = interface
    interval = 60

    [devices]

    [[router1]]
    host = router1.example.com
    port = 161
    community = public

    [[router2]]
    host = router1.example.com
    port = 161
    community = public
```

Note: If you modify the SNMPInterfaceCollector configuration, you will need to
restart diamond.

#### Dependencies

 * pysmnp

"""

import os
import sys
import re

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)),
                                'snmp'))
from snmp import SNMPCollector as parent_SNMPCollector
import diamond.convertor


class SNMPInterfaceCollector(parent_SNMPCollector):

    # IF-MIB OID
    IF_MIB_INDEX_OID = "1.3.6.1.2.1.2.2.1.1"
    IF_MIB_NAME_OID = "1.3.6.1.2.1.31.1.1.1.1"
    IF_MIB_TYPE_OID = "1.3.6.1.2.1.2.2.1.3"

    # A list of IF-MIB 32bit counters to walk
    IF_MIB_GAUGE_OID_TABLE = {'ifInDiscards': "1.3.6.1.2.1.2.2.1.13",
                              'ifInErrors': "1.3.6.1.2.1.2.2.1.14",
                              'ifOutDiscards': "1.3.6.1.2.1.2.2.1.19",
                              'ifOutErrors': "1.3.6.1.2.1.2.2.1.20"}

    # A list of IF-MIB 64bit counters to talk
    IF_MIB_COUNTER_OID_TABLE = {'ifHCInOctets': "1.3.6.1.2.1.31.1.1.1.6",
                                'ifInUcastPkts': "1.3.6.1.2.1.31.1.1.1.7",
                                'ifInMulticastPkts': "1.3.6.1.2.1.31.1.1.1.8",
                                'ifInBroadcastPkts': "1.3.6.1.2.1.31.1.1.1.9",
                                'ifHCOutOctets': "1.3.6.1.2.1.31.1.1.1.10",
                                'ifOutUcastPkts': "1.3.6.1.2.1.31.1.1.1.11",
                                'ifOutMulticastPkts': "1.3.6.1.2.1.31.1.1.1.12",
                                'ifOutBroadcastPkts': "1.3.6.1.2.1.31.1.1.1.13"}

    # A list of interface types we care about
    IF_TYPES = ["6"]

    def get_default_config_help(self):
        config_help = super(SNMPInterfaceCollector,
                            self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Override SNMPCollector.get_default_config method to provide
        default_config for the SNMPInterfaceCollector
        """
        default_config = super(SNMPInterfaceCollector,
                               self).get_default_config()
        default_config['path'] = 'interface'
        default_config['byte_unit'] = ['bit', 'byte']
        return default_config

    def collect_snmp(self, device, host, port, community):
        """
        Collect SNMP interface data from device
        """
        # Log
        self.log.info("Collecting SNMP interface statistics from: %s", device)

        # Define a list of interface indexes
        ifIndexes = []

        # Get Interface Indexes
        ifIndexOid = '.'.join([self.IF_MIB_INDEX_OID])
        ifIndexData = self.walk(ifIndexOid, host, port, community)
        ifIndexes = [v for v in ifIndexData.values()]

        for ifIndex in ifIndexes:
            # Get Interface Type
            ifTypeOid = '.'.join([self.IF_MIB_TYPE_OID, ifIndex])
            ifTypeData = self.get(ifTypeOid, host, port, community)
            if ifTypeData[ifTypeOid] not in self.IF_TYPES:
                # Skip Interface
                continue
            # Get Interface Name
            ifNameOid = '.'.join([self.IF_MIB_NAME_OID, ifIndex])
            ifNameData = self.get(ifNameOid, host, port, community)
            ifName = ifNameData[ifNameOid]
            # Remove quotes from string
            ifName = re.sub(r'(\"|\')', '', ifName)

            # Get Gauges
            for gaugeName, gaugeOid in self.IF_MIB_GAUGE_OID_TABLE.items():
                ifGaugeOid = '.'.join([self.IF_MIB_GAUGE_OID_TABLE[gaugeName],
                                       ifIndex])
                ifGaugeData = self.get(ifGaugeOid, host, port, community)
                ifGaugeValue = ifGaugeData[ifGaugeOid]
                if not ifGaugeValue:
                    continue

                # Get Metric Name and Value
                metricIfDescr = re.sub(r'\W', '_', ifName)
                metricName = '.'.join([metricIfDescr, gaugeName])
                metricValue = int(ifGaugeValue)
                # Get Metric Path
                metricPath = '.'.join(['devices',
                                       device,
                                       self.config['path'],
                                       metricName])
                # Publish Metric
                self.publish_gauge(metricPath, metricValue)

            # Get counters (64bit)
            counterItems = self.IF_MIB_COUNTER_OID_TABLE.items()
            for counterName, counterOid in counterItems:
                ifCounterOid = '.'.join(
                    [self.IF_MIB_COUNTER_OID_TABLE[counterName], ifIndex])
                ifCounterData = self.get(ifCounterOid, host, port, community)
                ifCounterValue = ifCounterData[ifCounterOid]
                if not ifCounterValue:
                    continue

                # Get Metric Name and Value
                metricIfDescr = re.sub(r'\W', '_', ifName)

                if counterName in ['ifHCInOctets', 'ifHCOutOctets']:
                    for unit in self.config['byte_unit']:
                        # Convert Metric
                        metricName = '.'.join([metricIfDescr,
                                               counterName.replace('Octets',
                                                                   unit)])
                        metricValue = diamond.convertor.binary.convert(
                            value=ifCounterValue,
                            oldUnit='byte',
                            newUnit=unit)

                        # Get Metric Path
                        metricPath = '.'.join(['devices',
                                               device,
                                               self.config['path'],
                                               metricName])
                        # Publish Metric
                        self.publish_counter(metricPath,
                                             metricValue,
                                             max_value=18446744073709600000,
                                             )
                else:
                    metricName = '.'.join([metricIfDescr, counterName])
                    metricValue = int(ifCounterValue)

                    # Get Metric Path
                    metricPath = '.'.join(['devices',
                                           device,
                                           self.config['path'],
                                           metricName])
                    # Publish Metric
                    self.publish_counter(metricPath,
                                         metricValue,
                                         max_value=18446744073709600000,
                                         )

########NEW FILE########
__FILENAME__ = testsnmpinterface
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from snmpinterface import SNMPInterfaceCollector


class TestSNMPInterfaceCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('SNMPInterfaceCollector', {
        })
        self.collector = SNMPInterfaceCollector(config, None)

    def test_import(self):
        self.assertTrue(SNMPInterfaceCollector)

########NEW FILE########
__FILENAME__ = snmpraw
# coding=utf-8

"""
The SNMPRawCollector is designed for collecting data from SNMP-enables devices,
using a set of specified OIDs

#### Configuration

Below is an example configuration for the SNMPRawCollector. The collector
can collect data any number of devices by adding configuration sections
under the *devices* header. By default the collector will collect every 60
seconds. This might be a bit excessive and put unnecessary load on the
devices being polled. You may wish to change this to every 300 seconds. However
you need modify your graphite data retentions to handle this properly.

```
    # Options for SNMPRawCollector
    enabled = True
    interval = 60

    [devices]

    # Start the device configuration
    # Note: this name will be used in the metric path.
    [[my-identification-for-this-host]]
    host = localhost
    port = 161
    community = public

    # Start the OID list for this device
    # Note: the value part will be used in the metric path.
    [[[oids]]]
    1.3.6.1.4.1.2021.10.1.3.1 = cpu.load.1min
    1.3.6.1.4.1.2021.10.1.3.2 = cpu.load.5min
    1.3.6.1.4.1.2021.10.1.3.3 = cpu.load.15min

    # If you want another host, you can. But you probably won't need it.
    [[another-identification]]
    host = router1.example.com
    port = 161
    community = public
    [[[oids]]]
    oid = metric.path
    oid = metric.path
```

Note: If you modify the SNMPRawCollector configuration, you will need to
restart diamond.

#### Dependencies

 * pysmnp (which depends on pyasn1 0.1.7 and pycrypto)

"""

import os
import sys
import time

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)),
                                'snmp'))
from snmp import SNMPCollector as parent_SNMPCollector
from diamond.metric import Metric


class SNMPRawCollector(parent_SNMPCollector):

    def __init__(self, *args, **kwargs):
        super(SNMPRawCollector, self).__init__(*args, **kwargs)

        # list to save non-existing oid's per device, to avoid repetition of
        # errors in logging. restart diamond/collector to flush this
        self.skip_list = []

    def get_default_config(self):
        """
        Override SNMPCollector.get_default_config method to provide
        default_config for the SNMPInterfaceCollector
        """
        default_config = super(SNMPRawCollector,
                               self).get_default_config()
        default_config.update({
            'oids': {},
            'path_prefix': 'servers',
            'path_suffix': 'snmp',
        })
        return default_config

    def _precision(self, value):
        """
        Return the precision of the number
        """
        value = str(value)
        decimal = value.rfind('.')
        if decimal == -1:
            return 0
        return len(value) - decimal - 1

    def _skip(self, device, oid, reason=None):
        self.skip_list.append((device, oid))
        if reason is not None:
            self.log.warn('Muted \'{0}\' on \'{1}\', because: {2}'.format(
                oid, device, reason))

    def _get_value_walk(self, device, oid, host, port, community):
        data = self.walk(oid, host, port, community)

        if data is None:
            self._skip(device, oid, 'device down (#2)')
            return

        self.log.debug('Data received from WALK \'{0}\': [{1}]'.format(
            device, data))

        if len(data) != 1:
            self._skip(
                device,
                oid,
                'unexpected response, data has {0} entries'.format(
                    len(data)))
            return

        # because we only allow 1-key dicts, we can pick with absolute index
        value = data.items()[0][1]
        return value

    def _get_value(self, device, oid, host, port, community):
        data = self.get(oid, host, port, community)

        if data is None:
            self._skip(device, oid, 'device down (#1)')
            return

        self.log.debug('Data received from GET \'{0}\': [{1}]'.format(
            device, data))

        if len(data) == 0:
            self._skip(device, oid, 'empty response, device down?')
            return

        if oid not in data:
            # oid is not even in hierarchy, happens when using 9.9.9.9
            # but not when using 1.9.9.9
            self._skip(device, oid, 'no object at OID (#1)')
            return

        value = data[oid]
        if value == 'No Such Object currently exists at this OID':
            self._skip(device, oid, 'no object at OID (#2)')
            return

        if value == 'No Such Instance currently exists at this OID':
            return self._get_value_walk(device, oid, host, port, community)

        return value

    def collect_snmp(self, device, host, port, community):
        """
        Collect SNMP interface data from device
        """
        self.log.debug(
            'Collecting raw SNMP statistics from device \'{0}\''.format(device))

        for device in self.config['devices']:
            dev_config = self.config['devices'][device]
            if not 'oids' in dev_config:
                continue

            for oid, metricName in dev_config['oids'].items():

                if (device, oid) in self.skip_list:
                    self.log.debug(
                        'Skipping OID \'{0}\' ({1}) on device \'{2}\''.format(
                            oid, metricName, device))
                    continue

                timestamp = time.time()
                value = self._get_value(device, oid, host, port, community)
                if value is None:
                    continue

                self.log.debug(
                    '\'{0}\' ({1}) on device \'{2}\' - value=[{3}]'.format(
                        oid, metricName, device, value))

                path = '.'.join([self.config['path_prefix'], device,
                                 self.config['path_suffix'], metricName])
                metric = Metric(path, value, timestamp, self._precision(value),
                                None, 'GAUGE')
                self.publish_metric(metric)

########NEW FILE########
__FILENAME__ = testsnmpraw
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from snmpraw import SNMPRawCollector


###############################################################################

class TestSNMPRawCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SNMPRawCollector', {
        })
        self.collector = SNMPRawCollector(config, None)

    def test_import(self):
        self.assertTrue(SNMPRawCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = sockstat
# coding=utf-8

"""
Uses /proc/net/sockstat to collect data on number of open sockets

#### Dependencies

 * /proc/net/sockstat

"""

import diamond.collector
import re
import os

_RE = re.compile('|'.join([
    r'sockets: used (?P<used>\d+)',
    r'TCP: inuse (?P<tcp_inuse>\d+) '
    + 'orphan (?P<tcp_orphan>\d+) '
    + 'tw (?P<tcp_tw>\d+) '
    + 'alloc (?P<tcp_alloc>\d+) '
    + 'mem (?P<tcp_mem>\d+)',
    r'UDP: inuse (?P<udp_inuse>\d+) mem (?P<udp_mem>\d+)'
]))


class SockstatCollector(diamond.collector.Collector):

    PROC = '/proc/net/sockstat'

    def get_default_config_help(self):
        config_help = super(SockstatCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(SockstatCollector, self).get_default_config()
        config.update({
            'path':     'sockets',
            'method':   'Threaded',
        })
        return config

    def collect(self):
        if not os.access(self.PROC, os.R_OK):
            return None

        result = {}

        file = open(self.PROC)
        for line in file:
            match = _RE.match(line)
            if match:
                for key, value in match.groupdict().items():
                    if value:
                        result[key] = int(value)
        file.close()

        for key, value in result.items():
            self.publish(key, value, metric_type='GAUGE')

########NEW FILE########
__FILENAME__ = testsockstat
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from sockstat import SockstatCollector

################################################################################


class TestSockstatCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SockstatCollector', {
            'interval': 10
        })

        self.collector = SockstatCollector(config, None)

    def test_import(self):
        self.assertTrue(SockstatCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_net_sockstat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/net/sockstat')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        SockstatCollector.PROC = self.getFixturePath('proc_net_sockstat')
        self.collector.collect()

        metrics = {
            'used': 118,
            'tcp_inuse': 10,
            'tcp_orphan': 0,
            'tcp_tw': 1,
            'tcp_alloc': 13,
            'tcp_mem': 1,
            'udp_inuse': 0,
            'udp_mem': 0
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = solr
# coding=utf-8

"""
Collect the solr stats for the local node

#### Dependencies

 * posixpath
 * urllib2
 * json

"""

import posixpath
import urllib2
import re

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json

import diamond.collector


class SolrCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(SolrCollector, self).get_default_config_help()
        config_help.update({
            'host': "",
            'port': "",
            'core': "Which core info should collect (default: all cores)",
            'stats': "Available stats: \n"
            " - core (Core stats)\n"
            " - response (Ping response stats)\n"
            " - query (Query Handler stats)\n"
            " - update (Update Handler stats)\n"
            " - cache (fieldValue, filter,"
            " document & queryResult cache stats)\n"
            " - jvm (JVM information) \n"
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(SolrCollector, self).get_default_config()
        config.update({
            'host':     'localhost',
            'port':     8983,
            'path':     'solr',
            'core':     None,
            'stats':    ['jvm', 'core', 'response',
                         'query', 'update', 'cache'],
        })
        return config

    def _try_convert(self, value):
        if isinstance(value, (int, float)):
            return value
        try:
            if '.' in value:
                return float(value)
            return int(value)
        except ValueError:
            return value

    def _get(self, path):
        url = 'http://%s:%i/%s' % (
            self.config['host'], int(self.config['port']), path)
        try:
            response = urllib2.urlopen(url)
        except Exception, err:
            self.log.error("%s: %s", url, err)
            return False

        try:
            return json.load(response)
        except (TypeError, ValueError):
            self.log.error("Unable to parse response from solr as a"
                           " json object")
            return False

    def collect(self):
        if json is None:
            self.log.error('Unable to import json')
            return {}

        cores = []
        if self.config['core']:
            cores = [self.config['core']]
        else:
            # If no core is specified, provide statistics for all cores
            result = self._get('/solr/admin/cores?action=STATUS&wt=json')
            if result:
                cores = result['status'].keys()

        metrics = {}
        for core in cores:
            if core:
                path = "{0}.".format(core)
            else:
                path = ""

            ping_url = posixpath.normpath(
                "/solr/{0}/admin/ping?wt=json".format(core))

            if 'response' in self.config['stats']:
                result = self._get(ping_url)
                if not result:
                    continue

                metrics.update({
                    "{0}response.QueryTime".format(path):
                    result["responseHeader"]["QTime"],
                    "{0}response.Status".format(path):
                    result["responseHeader"]["status"],
                })

            stats_url = posixpath.normpath(
                "/solr/{0}/admin/mbeans?stats=true&wt=json".format(core))

            result = self._get(stats_url)
            if not result:
                continue

            s = result['solr-mbeans']
            stats = dict((s[i], s[i+1]) for i in xrange(0, len(s), 2))

            if 'core' in self.config['stats']:
                core_searcher = stats["CORE"]["searcher"]["stats"]

                metrics.update([
                    ("{0}core.{1}".format(path, key),
                     core_searcher[key])
                    for key in ("maxDoc", "numDocs", "warmupTime")
                ])

            if 'query' in self.config['stats']:
                standard = stats["QUERYHANDLER"]["standard"]["stats"]
                update = stats["QUERYHANDLER"]["/update"]["stats"]

                metrics.update([
                    ("{0}queryhandler.standard.{1}".format(path, key),
                     standard[key])
                    for key in ("requests", "errors", "timeouts", "totalTime",
                                "avgTimePerRequest", "avgRequestsPerSecond")
                ])

                metrics.update([
                    ("{0}queryhandler.update.{1}".format(path, key),
                     update[key])
                    for key in ("requests", "errors", "timeouts", "totalTime",
                                "avgTimePerRequest", "avgRequestsPerSecond")
                    if update[key] != 'NaN'
                ])

            if 'update' in self.config['stats']:
                updatehandler = \
                    stats["UPDATEHANDLER"]["updateHandler"]["stats"]

                metrics.update([
                    ("{0}updatehandler.{1}".format(path, key),
                     updatehandler[key])
                    for key in (
                        "commits", "autocommits", "optimizes",
                        "rollbacks", "docsPending", "adds", "errors",
                        "cumulative_adds", "cumulative_errors")
                ])

            if 'cache' in self.config['stats']:
                cache = stats["CACHE"]

                metrics.update([
                    ("{0}cache.{1}.{2}".format(path, cache_type, key),
                     self._try_convert(cache[cache_type]['stats'][key]))
                    for cache_type in (
                        'fieldValueCache', 'filterCache',
                        'documentCache', 'queryResultCache')
                    for key in (
                        'lookups', 'hits', 'hitratio', 'inserts',
                        'evictions', 'size', 'warmupTime',
                        'cumulative_lookups', 'cumulative_hits',
                        'cumulative_hitratio', 'cumulative_inserts',
                        'cumulative_evictions')
                ])

            if 'jvm' in self.config['stats']:
                system_url = posixpath.normpath(
                    "/solr/{0}/admin/system?stats=true&wt=json".format(core))

                result = self._get(system_url)
                if not result:
                    continue

                mem = result['jvm']['memory']
                metrics.update([
                    ('{0}jvm.mem.{1}'.format(path, key),
                     self._try_convert(mem[key].split()[0]))
                    for key in ('free', 'total', 'max', 'used')
                ])

        for key in metrics:
            self.publish(key, metrics[key])

########NEW FILE########
__FILENAME__ = testsolr
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from solr import SolrCollector

################################################################################


class TestSolrCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SolrCollector', {})
        self.collector = SolrCollector(config, None)

    def test_import(self):
        self.assertTrue(SolrCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        returns = [self.getFixture('cores'),
                   self.getFixture('ping'),
                   self.getFixture('stats'),
                   self.getFixture('system')]
        urlopen_mock = patch('urllib2.urlopen', Mock(
            side_effect=lambda *args: returns.pop(0)))

        urlopen_mock.start()
        self.collector.collect()
        urlopen_mock.stop()

        metrics = {
            'response.QueryTime': 5,
            'response.Status': 0,

            "core.maxDoc": 321,
            "core.numDocs": 184,
            "core.warmupTime": 0,

            "queryhandler.standard.requests": 3,
            "queryhandler.standard.errors": 0,
            "queryhandler.standard.timeouts": 0,
            "queryhandler.standard.totalTime": 270,
            "queryhandler.standard.avgTimePerRequest": 90,
            "queryhandler.standard.avgRequestsPerSecond": 0.00016776958,

            "queryhandler.update.requests": 0,
            "queryhandler.update.errors": 0,
            "queryhandler.update.timeouts": 0,
            "queryhandler.update.totalTime": 0,
            "queryhandler.update.avgRequestsPerSecond": 0,

            "updatehandler.commits": 0,
            "updatehandler.autocommits": 0,
            "updatehandler.optimizes": 0,
            "updatehandler.rollbacks": 0,
            "updatehandler.docsPending": 0,
            "updatehandler.adds": 0,
            "updatehandler.errors": 0,
            "updatehandler.cumulative_adds": 0,
            "updatehandler.cumulative_errors": 0,

            'cache.fieldValueCache.lookups': 0,
            'cache.fieldValueCache.hits': 0,
            'cache.fieldValueCache.hitratio': 0.0,
            'cache.fieldValueCache.inserts': 0,
            'cache.fieldValueCache.evictions': 0,
            'cache.fieldValueCache.size': 0,
            'cache.fieldValueCache.warmupTime': 0,
            'cache.fieldValueCache.cumulative_lookups': 0,
            'cache.fieldValueCache.cumulative_hits': 0,
            'cache.fieldValueCache.cumulative_hitratio': 0.0,
            'cache.fieldValueCache.cumulative_inserts': 0,
            'cache.fieldValueCache.cumulative_evictions': 0,

            'cache.filterCache.lookups': 0,
            'cache.filterCache.hits': 0,
            'cache.filterCache.hitratio': 0.0,
            'cache.filterCache.inserts': 0,
            'cache.filterCache.evictions': 0,
            'cache.filterCache.size': 0,
            'cache.filterCache.warmupTime': 0,
            'cache.filterCache.cumulative_lookups': 0,
            'cache.filterCache.cumulative_hits': 0,
            'cache.filterCache.cumulative_hitratio': 0.0,
            'cache.filterCache.cumulative_inserts': 0,
            'cache.filterCache.cumulative_evictions': 0,

            'cache.documentCache.lookups': 0,
            'cache.documentCache.hits': 0,
            'cache.documentCache.hitratio': 0.0,
            'cache.documentCache.inserts': 0,
            'cache.documentCache.evictions': 0,
            'cache.documentCache.size': 0,
            'cache.documentCache.warmupTime': 0,
            'cache.documentCache.cumulative_lookups': 0,
            'cache.documentCache.cumulative_hits': 0,
            'cache.documentCache.cumulative_hitratio': 0.0,
            'cache.documentCache.cumulative_inserts': 0,
            'cache.documentCache.cumulative_evictions': 0,

            'cache.queryResultCache.lookups': 3,
            'cache.queryResultCache.hits': 2,
            'cache.queryResultCache.hitratio': 0.66,
            'cache.queryResultCache.inserts': 1,
            'cache.queryResultCache.evictions': 0,
            'cache.queryResultCache.size': 1,
            'cache.queryResultCache.warmupTime': 0,
            'cache.queryResultCache.cumulative_lookups': 3,
            'cache.queryResultCache.cumulative_hits': 2,
            'cache.queryResultCache.cumulative_hitratio': 0.66,
            'cache.queryResultCache.cumulative_inserts': 1,
            'cache.queryResultCache.cumulative_evictions': 0,

            'jvm.mem.free': 42.7,
            'jvm.mem.total': 61.9,
            'jvm.mem.max': 185.6,
            'jvm.mem.used': 19.2,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics)
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        urlopen_mock = patch('urllib2.urlopen', Mock(
                             return_value=self.getFixture('stats_blank')))

        urlopen_mock.start()
        self.collector.collect()
        urlopen_mock.stop()

        self.assertPublishedMany(publish_mock, {})

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = sqs
# coding=utf-8

"""
The SQS collector collects metrics for one or more Amazon AWS SQS queues

#### Configuration

Below is an example configuration for the SQSCollector.
You can specify an arbitrary amount of regions

```
    enabled = True
    interval = 60

    [regions]
    [[region-code]]
    access_key_id = '...'
    secret_access_key = '''
    queues = queue_name[,queue_name2[,..]]

```

Note: If you modify the SQSCollector configuration, you will need to
restart diamond.

#### Dependencies

 * boto

"""

import diamond.collector
try:
    from boto import sqs
    sqs  # Pyflakes
except ImportError:
    sqs = False


class SqsCollector(diamond.collector.Collector):

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(SqsCollector, self).get_default_config()
        config.update({
            'path': 'sqs',
        })
        return config

    def collect(self):
        attribs = ['ApproximateNumberOfMessages',
                   'ApproximateNumberOfMessagesNotVisible',
                   'ApproximateNumberOfMessagesDelayed',
                   'CreatedTimestamp',
                   'DelaySeconds',
                   'LastModifiedTimestamp',
                   'MaximumMessageSize',
                   'MessageRetentionPeriod',
                   'ReceiveMessageWaitTimeSeconds',
                   'VisibilityTimeout']
        if not sqs:
            self.log.error("boto module not found!")
            return
        for (region, region_cfg) in self.config['regions'].items():
            assert 'access_key_id' in region_cfg
            assert 'secret_access_key' in region_cfg
            assert 'queues' in region_cfg
            queues = region_cfg['queues'].split(',')
            for queue_name in queues:
                conn = sqs.connect_to_region(
                    region,
                    aws_access_key_id=region_cfg['access_key_id'],
                    aws_secret_access_key=region_cfg['secret_access_key'],
                )
                queue = conn.get_queue(queue_name)

                for attrib in attribs:
                    d = queue.get_attributes(attrib)
                    self.publish(
                        '%s.%s.%s' % (region, queue_name, attrib),
                        d[attrib]
                    )

########NEW FILE########
__FILENAME__ = testsqs
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from sqs import SqsCollector


###############################################################################

class TestSqsCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SqsCollector', {
        })
        self.collector = SqsCollector(config, None)

    def test_import(self):
        self.assertTrue(SqsCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = squid
# coding=utf-8

"""
Collects data from squid servers

#### Dependencies

"""

import re
import socket
import diamond.collector


class SquidCollector(diamond.collector.Collector):

    def __init__(self, *args, **kwargs):
        super(SquidCollector, self).__init__(*args, **kwargs)
        self.squid_hosts = {}

        host_pattern = re.compile("(([^@]+)@)?([^:]+)(:([0-9]+))?")
        self.stat_pattern = re.compile("^([^ ]+) = ([0-9\.]+)$")

        for host in self.config['hosts']:
            matches = host_pattern.match(host)

            if matches.group(5):
                port = matches.group(5)
            else:
                port = 3128

            if matches.group(2):
                nick = matches.group(2)
            else:
                nick = port

            self.squid_hosts[nick] = {
                'host': matches.group(3),
                'port': int(port)
            }

    def get_default_config_help(self):
        config_help = super(SquidCollector, self).get_default_config_help()
        config_help.update({
            'hosts': 'List of hosts to collect from. Format is '
            + '[nickname@]host[:port], [nickname@]host[:port], etc',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(SquidCollector, self).get_default_config()
        config.update({
            'hosts': ['localhost:3128'],
            'path': 'squid',
        })
        return config

    def _getData(self, host, port):
        try:
            squid_sock = socket.socket()
            squid_sock.connect((host, int(port)))
            squid_sock.settimeout(0.25)
            squid_sock.sendall("GET cache_object://localhost/counters HTTP/1.0"
                               + "\r\nHost: localhost\r\nAccept: */*\r\nConnec"
                               + "tion: close\r\n\r\n")

            fulldata = ''

            while True:
                data = squid_sock.recv(1024)
                if not data:
                    break
                fulldata = fulldata + data
        except Exception, e:
            self.log.error('Couldnt connect to squid: %s', e)
            return None
        squid_sock.close()

        return fulldata

    def collect(self):
        for nickname in self.squid_hosts.keys():
            squid_host = self.squid_hosts[nickname]

            fulldata = self._getData(squid_host['host'],
                                     squid_host['port'])

            if fulldata is not None:
                fulldata = fulldata.splitlines()

                for data in fulldata:
                    matches = self.stat_pattern.match(data)
                    if matches:
                        self.publish_counter("%s.%s" % (nickname,
                                                        matches.group(1)),
                                             float(matches.group(2)))

########NEW FILE########
__FILENAME__ = testsquid
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from squid import SquidCollector

################################################################################


class TestSquidCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SquidCollector', {
            'interval': 1,
        })

        self.collector = SquidCollector(config, None)

    def test_import(self):
        self.assertTrue(SquidCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_fake_data(self, publish_mock):
        _getData_mock = patch.object(
            SquidCollector,
            '_getData',
            Mock(
                return_value=self.getFixture('fake_counters_1').getvalue()))
        _getData_mock.start()
        self.collector.collect()
        _getData_mock.stop()

        self.assertPublishedMany(publish_mock, {})

        _getData_mock = patch.object(
            SquidCollector,
            '_getData',
            Mock(
                return_value=self.getFixture('fake_counters_2').getvalue()))
        _getData_mock.start()
        self.collector.collect()
        _getData_mock.stop()

        metrics = {
            '3128.client_http.requests': 1,
            '3128.client_http.hits': 2,
            '3128.client_http.errors': 3,
            '3128.client_http.kbytes_in': 4,
            '3128.client_http.kbytes_out': 5,
            '3128.client_http.hit_kbytes_out': 6,
            '3128.server.all.requests': 7,
            '3128.server.all.errors': 8,
            '3128.server.all.kbytes_in': 9,
            '3128.server.all.kbytes_out': 10,
            '3128.server.http.requests': 1,
            '3128.server.http.errors': 12,
            '3128.server.http.kbytes_in': 13,
            '3128.server.http.kbytes_out': 14,
            '3128.server.ftp.requests': 15,
            '3128.server.ftp.errors': 16,
            '3128.server.ftp.kbytes_in': 17,
            '3128.server.ftp.kbytes_out': 18,
            '3128.server.other.requests': 19,
            '3128.server.other.errors': 20,
            '3128.server.other.kbytes_in': 21,
            '3128.server.other.kbytes_out': 22,
            '3128.icp.pkts_sent': 23,
            '3128.icp.pkts_recv': 24,
            '3128.icp.queries_sent': 25,
            '3128.icp.replies_sent': 26,
            '3128.icp.queries_recv': 27,
            '3128.icp.replies_recv': 28,
            '3128.icp.query_timeouts': 29,
            '3128.icp.replies_queued': 30,
            '3128.icp.kbytes_sent': 31,
            '3128.icp.kbytes_recv': 32,
            '3128.icp.q_kbytes_sent': 33,
            '3128.icp.r_kbytes_sent': 34,
            '3128.icp.q_kbytes_recv': 35,
            '3128.icp.r_kbytes_recv': 36,
            '3128.icp.times_used': 37,
            '3128.cd.times_used': 38,
            '3128.cd.msgs_sent': 39,
            '3128.cd.msgs_recv': 40,
            '3128.cd.memory': 41,
            '3128.cd.local_memory': 42,
            '3128.cd.kbytes_sent': 43,
            '3128.cd.kbytes_recv': 44,
            '3128.unlink.requests': 45,
            '3128.page_faults': 46,
            '3128.select_loops': 47,
            '3128.cpu_time': 48.1234567890,
            '3128.wall_time': 49.1234567890,
            '3128.swap.outs': 50,
            '3128.swap.ins': 51,
            '3128.swap.files_cleaned': 52,
            '3128.aborted_requests': 53
        }

        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        _getData_mock = patch.object(
            SquidCollector,
            '_getData',
            Mock(
                return_value=self.getFixture('counters_1').getvalue()))
        _getData_mock.start()
        self.collector.collect()
        _getData_mock.stop()

        self.assertPublishedMany(publish_mock, {})

        _getData_mock = patch.object(
            SquidCollector,
            '_getData',
            Mock(
                return_value=self.getFixture('counters_2').getvalue()))
        _getData_mock.start()
        self.collector.collect()
        _getData_mock.stop()

        metrics = {
            '3128.client_http.requests': 2,
            '3128.client_http.hits': 1,
            '3128.client_http.errors': 0,
            '3128.client_http.kbytes_in': 1,
            '3128.client_http.kbytes_out': 12.0,
            '3128.client_http.hit_kbytes_out': 10,
            '3128.server.all.requests': 0,
            '3128.server.all.errors': 0,
            '3128.server.all.kbytes_in': 0,
            '3128.server.all.kbytes_out': 0,
            '3128.server.http.requests': 0,
            '3128.server.http.errors': 0,
            '3128.server.http.kbytes_in': 0,
            '3128.server.http.kbytes_out': 0,
            '3128.server.ftp.requests': 0,
            '3128.server.ftp.errors': 0,
            '3128.server.ftp.kbytes_in': 0,
            '3128.server.ftp.kbytes_out': 0,
            '3128.server.other.requests': 0,
            '3128.server.other.errors': 0,
            '3128.server.other.kbytes_in': 0,
            '3128.server.other.kbytes_out': 0,
            '3128.icp.pkts_sent': 0,
            '3128.icp.pkts_recv': 0,
            '3128.icp.queries_sent': 0,
            '3128.icp.replies_sent': 0,
            '3128.icp.queries_recv': 0,
            '3128.icp.replies_recv': 0,
            '3128.icp.query_timeouts': 0,
            '3128.icp.replies_queued': 0,
            '3128.icp.kbytes_sent': 0,
            '3128.icp.kbytes_recv': 0,
            '3128.icp.q_kbytes_sent': 0,
            '3128.icp.r_kbytes_sent': 0,
            '3128.icp.q_kbytes_recv': 0,
            '3128.icp.r_kbytes_recv': 0,
            '3128.icp.times_used': 0,
            '3128.cd.times_used': 0,
            '3128.cd.msgs_sent': 0,
            '3128.cd.msgs_recv': 0,
            '3128.cd.memory': 0,
            '3128.cd.local_memory': 0,
            '3128.cd.kbytes_sent': 0,
            '3128.cd.kbytes_recv': 0,
            '3128.unlink.requests': 0,
            '3128.page_faults': 0,
            '3128.select_loops': 10827.0,
            '3128.cpu_time': 0,
            '3128.wall_time': 10,
            '3128.swap.outs': 0,
            '3128.swap.ins': 2,
            '3128.swap.files_cleaned': 0,
            '3128.aborted_requests': 0
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = stats
# coding=utf-8

"""

This is a slightly unusual 'collector'. It collects and uploads stats to a
app engine instance run by Rob Smith. The stats collected are:

 * Per collector enabled or not
 * Per collector interval time
 * Global collector reload interval
 * Global handlers
 * Any custom collector set stats

These stats are stored anonymously (other then UUID), processed and the results
are at [http://diamond-stats.appspot.com/](http://diamond-stats.appspot.com/).

These values can help us know more about how Diamond is being used and can help
us target development efforts in the future.

#### Requirements

You can install Smolt and run it once or run the following from a terminal

Linux:

 * mkdir /etc/smolt
 * cd /etc/smolt
 * cat /proc/sys/kernel/random/uuid > hw-uuid

Others:

 * mkdir /etc/smolt
 * cd /etc/smolt
 * curl -Lo- http://utils.kormoc.com/uuid/get.php > hw-uuid

#### Dependencies

 * /etc/smolt/hw-uuid
 * urllib
 * json/simplejson

"""

from diamond.collector import Collector
from diamond.util import get_diamond_version
import diamond

import urllib
import os
import sys
import platform

try:
    import json
    json  # workaround for pyflakes issue #13
except ImportError:
    import simplejson as json


def getIncludePaths(path):
    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))

        if os.path.isfile(cPath) and len(f) > 3 and f[-3:] == '.py':
            sys.path.append(os.path.dirname(cPath))

    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))
        if os.path.isdir(cPath):
            getIncludePaths(cPath)

collectors = {}


def getCollectors(path):
    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))

        if (os.path.isfile(cPath)
                and len(f) > 3
                and f[-3:] == '.py'
                and f[0:4] != 'test'):
            modname = f[:-3]
            try:
                # Import the module
                module = __import__(modname, globals(), locals(), ['*'])

                # Find the name
                for attr in dir(module):
                    cls = getattr(module, attr)
                    try:
                        if (issubclass(cls, Collector)
                                and cls.__name__ not in collectors):
                            collectors[cls.__name__] = module
                            break
                    except TypeError:
                        continue
            except Exception:
                collectors[modname] = False
                continue

    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))
        if os.path.isdir(cPath):
            getCollectors(cPath)


class StatsCollector(diamond.collector.Collector):
    full_config = None

    def __init__(self, config, handlers):
        self.full_config = config
        super(StatsCollector, self).__init__(config, handlers)

    def get_default_config_help(self):
        config_help = super(StatsCollector, self).get_default_config_help()
        config_help.update({
            'url': 'The url to post stats to.',
            'uuidfile': 'The path to the uuid file'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(StatsCollector, self).get_default_config()
        config.update({
            'url': 'http://diamond-stats.appspot.com/submitstats',
            'uuidfile': '/etc/smolt/hw-uuid',
            'interval': 604800,
        })
        return config

    def collect(self):
        getIncludePaths(self.full_config['server']['collectors_path'])
        getCollectors(self.full_config['server']['collectors_path'])

        stats = {}

        stats['version'] = get_diamond_version()
        stats['python_version'] = platform.python_version()

        if platform.system() == 'Darwin':
            ver = platform.mac_ver()
            os_version = ('Darwin', ver[0], '')

        elif platform.system() == 'Linux':
            os_version = platform.linux_distribution()

        elif platform.system() == 'Windows':
            ver = platform.win32_ver()
            os_version = ('Windows', ver[0], ver[2])

        stats['os'] = "%s %s" % (os_version[0], os_version[1])
        stats['os_distname'] = os_version[0]
        stats['os_version'] = os_version[1]
        stats['os_id'] = os_version[2]

        uuid_file = open(self.config['uuidfile'])
        stats['uuid'] = uuid_file.read().strip()
        uuid_file.close()

        stats['collectors'] = {}

        stats['collectors']['Default'] = self.get_stats_for_upload(
            config=self.full_config['collectors']['default'])

        for collector in collectors:
            if not hasattr(collectors[collector], collector):
                continue

            cls = getattr(collectors[collector], collector)
            obj = cls(config=self.full_config, handlers={})
            stats['collectors'][collector] = obj.get_stats_for_upload()

        stats['collectors']['StatsCollector'] = self.get_stats_for_upload()

        stats['server'] = {}

        if type(self.full_config['server']['handlers']) is list:
            handlers = self.full_config['server']['handlers']
        else:
            handlers = self.full_config['server']['handlers'].split(',')
        stats['server']['handlers'] = handlers

        reload_i = self.full_config['server']['collectors_reload_interval']
        stats['server']['collectors_reload_interval'] = reload_i

        hmeth = 'Default'
        if 'hostname_method' in self.full_config['collectors']['default']:
            hmeth = self.full_config['collectors']['default']['hostname_method']
        stats['server']['hostname_method'] = hmeth

        data = urllib.urlencode({'stats': json.dumps(stats)})
        f = urllib.urlopen(self.config['url'], data)
        f.read()
        f.close()

        return True

########NEW FILE########
__FILENAME__ = teststats
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config

from stats import StatsCollector


class TestStatsCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('StatsCollector', {
        })
        self.collector = StatsCollector(config, None)

    def test_import(self):
        self.assertTrue(StatsCollector)

########NEW FILE########
__FILENAME__ = supervisord
# coding=utf-8

"""
Custom collector for supervisord process control system
(github.com/Supervisor/supervisor)

Supervisor runs an XML-RPC server, which this collector uses to gather a few
basic stats on each registered process.

#### Dependencies

 * xmlrpclib
 * supervisor
 * diamond

#### Usage

Configure supervisor's XML-RPC server (either over HTTP or Unix socket). See
supervisord.org/configuration.html for details. In the collector configuration
file, you may specify the protocol and path configuration; below are the
defaults.

<pre>
xmlrpc_server_protocol = unix
xmlrpc_server_path = /var/run/supervisor.sock
</pre>

"""

import xmlrpclib

try:
    import supervisor.xmlrpc
    supervisor  # workaround for pyflakes issue #13
except ImportError:
    supervisor = None

import diamond.collector


class SupervisordCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(SupervisordCollector,
                            self).get_default_config_help()
        config_help.update({
            'xmlrpc_server_protocol': 'XML-RPC server protocol. Options: unix, http',  # NOQA
            'xmlrpc_server_path': 'XML-RPC server path.'
        })
        return config_help

    def get_default_config(self):
        default_config = super(SupervisordCollector, self).get_default_config()
        default_config['path'] = 'supervisor'
        default_config['xmlrpc_server_protocol'] = 'unix'
        default_config['xmlrpc_server_path'] = '/var/run/supervisor.sock'
        return default_config

    def getAllProcessInfo(self):

        server = None
        protocol = self.config['xmlrpc_server_protocol']
        path = self.config['xmlrpc_server_path']
        uri = '{0}://{1}'.format(protocol, path)

        self.log.debug(
            'Attempting to connect to XML-RPC server "%s"', uri)

        if protocol == 'unix':
            server = xmlrpclib.ServerProxy(
                'http://127.0.0.1',
                supervisor.xmlrpc.SupervisorTransport(None, None, uri)
                ).supervisor

        elif protocol == 'http':
            server = xmlrpclib.Server(uri).supervisor

        else:
            self.log.debug(
                'Invalid xmlrpc_server_protocol config setting "%s"',
                protocol)
            return None

        return server.getAllProcessInfo()

    def collect(self):

        processes = self.getAllProcessInfo()

        self.log.debug('Found %s supervisord processes', len(processes))

        for process in processes:
            statPrefix = "%s.%s" % (process["group"], process["name"])

            # state

            self.publish(statPrefix + ".state", process["state"])

            # uptime

            uptime = 0

            if process["statename"] == "RUNNING":
                uptime = process["now"] - process["start"]

            self.publish(statPrefix + ".uptime", uptime)

########NEW FILE########
__FILENAME__ = testsupervisord
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from supervisord import SupervisordCollector

################################################################################


class TestSupervisordCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('SupervisordCollector', {})
        self.collector = SupervisordCollector(config, None)
        self.assertTrue(self.collector)

    def test_import(self):
        self.assertTrue(SupervisordCollector)

    @patch.object(Collector, 'publish')
    def test_success(self, publish_mock):

        self.collector.getAllProcessInfo = Mock(
            return_value=eval(self.getFixture('valid_fixture').getvalue()))

        self.collector.collect()

        metrics = {
            'test_group.test_name_1.state': 20,
            'test_group.test_name_1.uptime':  5,
            'test_group.test_name_2.state': 200,
            'test_group.test_name_2.uptime': 500
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = tcp
# coding=utf-8

"""
The TCPCollector class collects metrics on TCP stats

#### Dependencies

 * /proc/net/netstat
 * /proc/net/snmp

#### Allowed Metric Names
<table>
<tr><th>Name</th><th>Description</th></tr>
<tr><td>SyncookiesSent</td><td>An application wasn't able to accept a
connection fast enough, so the kernel couldn't store an entry in the queue for
this connection. Instead of dropping it, it sent a cookie to the client.
</td></tr>
<tr><td>SyncookiesRecv</td><td>After sending a cookie, it came back to us
and passed the check.</td></tr>
<tr><td>SyncookiesFailed</td><td>After sending a cookie, it came back to us
but looked invalid.</td></tr>
<tr><td>EmbryonicRsts</td><td></td></tr>
<tr><td>PruneCalled</td><td></td></tr>
<tr><td>RcvPruned</td><td>If the kernel is really really desperate and cannot
give more memory to this socket even after dropping the ofo queue, it will
simply discard the packet it received.  This is Really Bad.</td></tr>
<tr><td>OfoPruned</td><td>When a socket is using too much memory (rmem), the
kernel will first discard any out-of-order packet that has been queued (with
SACK).</td></tr>
<tr><td>OutOfWindowIcmps</td><td></td></tr>
<tr><td>LockDroppedIcmps</td><td></td></tr>
<tr><td>ArpFilter</td><td></td></tr>
<tr><td>TW</td><td></td></tr>
<tr><td>TWRecycled</td><td></td></tr>
<tr><td>TWKilled</td><td></td></tr>
<tr><td>PAWSPassive</td><td></td></tr>
<tr><td>PAWSActive</td><td></td></tr>
<tr><td>PAWSEstab</td><td></td></tr>
<tr><td>DelayedACKs</td><td>We waited for another packet to send an ACK, but
didn't see any, so a timer ended up sending a delayed ACK.</td></tr>
<tr><td>DelayedACKLocked</td><td>We wanted to send a delayed ACK but failed
because the socket was locked.  So the timer was reset.</td></tr>
<tr><td>DelayedACKLost</td><td>We sent a delayed and duplicated ACK because the
remote peer retransmitted a packet, thinking that it didn't get to us.</td></tr>
<tr><td>ListenOverflows</td><td>We completed a 3WHS but couldn't put the socket
on the accept queue, so we had to discard the connection.</td></tr>
<tr><td>ListenDrops</td><td>We couldn't accept a connection because one of: we
had no route to the destination, we failed to allocate a socket, we failed to
allocate a new local port bind bucket.  Note: this counter also include all the
increments made to ListenOverflows</td></tr>
<tr><td>TCPPrequeued</td><td></td></tr>
<tr><td>TCPDirectCopyFromBacklog</td><td></td></tr>
<tr><td>TCPDirectCopyFromPrequeue</td><td></td></tr>
<tr><td>TCPPrequeueDropped</td><td></td></tr>
<tr><td>TCPHPHits</td><td></td></tr>
<tr><td>TCPHPHitsToUser</td><td></td></tr>
<tr><td>TCPPureAcks</td><td></td></tr>
<tr><td>TCPHPAcks</td><td></td></tr>
<tr><td>TCPRenoRecovery</td><td>A packet was lost and we recovered after a
fast retransmit.</td></tr>
<tr><td>TCPSackRecovery</td><td>A packet was lost and we recovered by using
selective acknowledgements.</td></tr>
<tr><td>TCPSACKReneging</td><td></td></tr>
<tr><td>TCPFACKReorder</td><td>We detected re-ordering using FACK (Forward ACK
-- the highest sequence number known to have been received by the peer when
using SACK -- FACK is used during congestion control).</td></tr>
<tr><td>TCPSACKReorder</td><td>We detected re-ordering using SACK.</td></tr>
<tr><td>TCPRenoReorder</td><td>We detected re-ordering using fast retransmit.
</td></tr>
<tr><td>TCPTSReorder</td><td>We detected re-ordering using the timestamp option.
</td></tr>
<tr><td>TCPFullUndo</td><td>We detected some erroneous retransmits and undid our
CWND reduction.</td></tr>
<tr><td>TCPPartialUndo</td><td>We detected some erroneous retransmits, a partial
ACK arrived while we were fast retransmitting, so we were able to partially undo
some of our CWND reduction.</td></tr>
<tr><td>TCPDSACKUndo</td><td>We detected some erroneous retransmits, a D-SACK
arrived and ACK'ed all the retransmitted data, so we undid our CWND reduction.
</td></tr>
<tr><td>TCPLossUndo</td><td>We detected some erroneous retransmits, a partial
ACK arrived, so we undid our CWND reduction.</td></tr>
<tr><td>TCPLoss</td><td></td></tr>
<tr><td>TCPLostRetransmit</td><td></td></tr>
<tr><td>TCPRenoFailures</td><td></td></tr>
<tr><td>TCPSackFailures</td><td></td></tr>
<tr><td>TCPLossFailures</td><td></td></tr>
<tr><td>TCPFastRetrans</td><td></td></tr>
<tr><td>TCPForwardRetrans</td><td></td></tr>
<tr><td>TCPSlowStartRetrans</td><td></td></tr>
<tr><td>TCPTimeouts</td><td></td></tr>
<tr><td>TCPRenoRecoveryFail</td><td></td></tr>
<tr><td>TCPSackRecoveryFail</td><td></td></tr>
<tr><td>TCPSchedulerFailed</td><td></td></tr>
<tr><td>TCPRcvCollapsed</td><td></td></tr>
<tr><td>TCPDSACKOldSent</td><td></td></tr>
<tr><td>TCPDSACKOfoSent</td><td></td></tr>
<tr><td>TCPDSACKRecv</td><td></td></tr>
<tr><td>TCPDSACKOfoRecv</td><td></td></tr>

<tr><td>TCPSACKDiscard</td><td>We got a completely invalid SACK block and
discarded it.</td></tr>
<tr><td>TCPDSACKIgnoredOld</td><td>We got a duplicate SACK while retransmitting
so we discarded it.</td></tr>
<tr><td>TCPDSACKIgnoredNoUndo</td><td>We got a duplicate SACK and discarded it.
</td></tr>

<tr><td>TCPAbortOnSyn</td><td>We received an unexpected SYN so we sent a RST to
the peer.</td></tr>
<tr><td>TCPAbortOnData</td><td>We were in FIN_WAIT1 yet we received a data
packet with a sequence number that's beyond the last one for this connection,
so we RST'ed.</td></tr>
<tr><td>TCPAbortOnClose</td><td>We received data but the user has closed the
socket, so we have no wait of handing it to them, so we RST'ed.</td></tr>
<tr><td>TCPAbortOnMemory</td><td>This is Really Bad.  It happens when there are
too many orphaned sockets (not attached a FD) and the kernel has to drop a
connection. Sometimes it will send a reset to the peer, sometimes it wont.
</td></tr>
<tr><td>TCPAbortOnTimeout</td><td>The connection timed out really hard.
</td></tr>
<tr><td>TCPAbortOnLinger</td><td>We killed a socket that was closed by the
application and lingered around for long enough.</td></tr>
<tr><td>TCPAbortFailed</td><td>We tried to send a reset, probably during one of
teh TCPABort* situations above, but we failed e.g. because we couldn't allocate
enough memory (very bad).</td></tr>
<tr><td>TCPMemoryPressures</td><td>Number of times a socket was put in "memory
pressure" due to a non fatal memory allocation failure (reduces the send buffer
size etc).</td></tr>

<tr><td>TCPBacklogDrop</td><td>We received something but had to drop it because
the socket's receive queue was full.</td></tr>

<tr><td>RtoAlgorithm</td><td>The algorithm used to determine the timeout value
used for retransmitting unacknowledged octets.</td></tr>
<tr><td>RtoMin</td><td>The minimum value permitted by a TCP implementation
for the retransmission timeout, measured in milliseconds.
More refined semantics for objects of this type depend upon the algorithm used
to determine the retransmission timeout. In particular,
when the timeout algorithm is ``rsre '' (3), an object of this type has the
semantics of the LBOUND quantity described in RFC 793.</td></tr>
<tr><td>RtoMax</td><td>The maximum value permitted by a TCP implementation for
the retransmission timeout, measured in milliseconds. More refined semantics
for objects of this type depend upon the algorithm used to determine the
retransmission timeout. In particular, when the timeout algorithm is ``rsre''
(3), an object of this type has the semantics of the UBOUND quantity described
in RFC 793.</td></tr>
<tr><td>MaxConn</td><td>The limit on the total number of TCP connections the
entity can support. In entities where the maximum number of connections is
dynamic, this object should contain the value -1.</td></tr>
<tr><td>ActiveOpens</td><td>The number of times TCP connections have made a
direct transition to the SYN-SENT state from the CLOSED state.</td></tr>
<tr><td>PassiveOpens</td><td>The number of times TCP connections have made a
direct transition to the SYN-RCVD state from the LISTEN state.</td></tr>
<tr><td>AttemptFails</td><td>The number of times TCP connections have made a
direct transition to the CLOSED state from either the SYN-SENT state or the
SYN-RCVD state, plus the number of times TCP connections have made a direct
transition to the LISTEN state from the SYN-RCVD state.</td></tr>
<tr><td>EstabResets</td><td>The number of times TCP connections have made a
direct transition to the CLOSED state from either the ESTABLISHED state or the
CLOSE-WAIT state.</td></tr>
<tr><td>CurrEstab</td><td>The number of TCP connections for which the current
state is either ESTABLISHED or CLOSE- WAIT.</td></tr>
<tr><td>InSegs</td><td>The total number of segments received, including those
received in error. This count includes segments received on currently
established connections.</td></tr>
<tr><td>OutSegs</td><td>The total number of segments sent, including those on
current connections but excluding those containing only retransmitted octets.
</td></tr>
<tr><td>RetransSegs</td><td>The total number of segments retransmitted - that
is, the number of TCP segments transmitted containing one or more previously
transmitted octets.</td></tr>
<tr><td>InErrs</td><td>The total number of segments received in error (for
example, bad TCP checksums).</td></tr>
<tr><td>OutRsts</td><td>The number of TCP segments sent containing the RST
flag.</td></tr>
</table>

"""

import diamond.collector
import os


class TCPCollector(diamond.collector.Collector):

    PROC = [
        '/proc/net/netstat',
        '/proc/net/snmp'
    ]

    GAUGES = [
        'CurrEstab',
        'MaxConn',
    ]

    def __init__(self, config, handlers):
        super(TCPCollector, self).__init__(config, handlers)
        if self.config['allowed_names'] is None:
            self.config['allowed_names'] = []

    def get_default_config_help(self):
        config_help = super(TCPCollector, self).get_default_config_help()
        config_help.update({
            'allowed_names': 'list of entries to collect, empty to collect all',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(TCPCollector, self).get_default_config()
        config.update({
            'path':             'tcp',
            'allowed_names':    'ListenOverflows, ListenDrops, TCPLoss, '
            + 'TCPTimeouts, TCPFastRetrans, TCPLostRetransmit, '
            + 'TCPForwardRetrans, TCPSlowStartRetrans, CurrEstab, '
            + 'TCPAbortOnMemory, TCPBacklogDrop, AttemptFails, '
            + 'EstabResets, InErrs, ActiveOpens, PassiveOpens',
        })
        return config

    def collect(self):
        metrics = {}

        for filepath in self.PROC:
            if not os.access(filepath, os.R_OK):
                self.log.error('Permission to access %s denied', filepath)
                continue

            header = ''
            data = ''

            # Seek the file for the lines that start with Tcp
            file = open(filepath)

            if not file:
                self.log.error('Failed to open %s', filepath)
                continue

            while True:
                line = file.readline()

                # Reached EOF?
                if len(line) == 0:
                    break

                # Line has metrics?
                if line.startswith("Tcp"):
                    header = line
                    data = file.readline()
                    break
            file.close()

            # No data from the file?
            if header == '' or data == '':
                self.log.error('%s has no lines with Tcp', filepath)
                continue

            header = header.split()
            data = data.split()

            for i in xrange(1, len(header)):
                metrics[header[i]] = data[i]

        for metric_name in metrics.keys():
            if (len(self.config['allowed_names']) > 0
                    and metric_name not in self.config['allowed_names']):
                continue

            value = long(metrics[metric_name])

            # Publish the metric
            if metric_name in self.GAUGES:
                self.publish_gauge(metric_name, value, 0)
            else:
                self.publish_counter(metric_name, value, 0)

########NEW FILE########
__FILENAME__ = testtcp
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from tcp import TCPCollector

################################################################################


class TestTCPCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('TCPCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = TCPCollector(config, None)

    def test_import(self):
        self.assertTrue(TCPCollector)

    @patch('os.access', Mock(return_value=True))
    @patch('__builtin__.open')
    @patch('diamond.collector.Collector.publish')
    def test_should_open_proc_net_netstat(self, publish_mock, open_mock):
        TCPCollector.PROC = ['/proc/net/netstat']
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/net/netstat')

    @patch('os.access', Mock(return_value=True))
    @patch('__builtin__.open')
    @patch('diamond.collector.Collector.publish')
    def test_should_work_with_synthetic_data(self, publish_mock, open_mock):
        TCPCollector.PROC = ['/proc/net/netstat']
        self.setUp(['A', 'C'])
        open_mock.return_value = StringIO('''
TcpExt: A B C
TcpExt: 0 0 0
'''.strip())

        self.collector.collect()
        self.assertPublishedMany(publish_mock, {})

        open_mock.return_value = StringIO('''
TcpExt: A B C
TcpExt: 0 1 2
'''.strip())

        self.collector.collect()

        self.assertEqual(len(publish_mock.call_args_list), 2)

        metrics = {
            'A': 0,
            'C': 2,
        }

        self.assertPublishedMany(publish_mock, metrics)

    @patch('diamond.collector.Collector.publish')
    def test_should_work_with_real_data(self, publish_mock):
        self.setUp(['ListenOverflows', 'ListenDrops', 'TCPLoss', 'TCPTimeouts'])

        TCPCollector.PROC = [self.getFixturePath('proc_net_netstat_1')]
        self.collector.collect()
        self.assertPublishedMany(publish_mock, {})

        TCPCollector.PROC = [self.getFixturePath('proc_net_netstat_2')]
        self.collector.collect()

        metrics = {
            'ListenOverflows': 0,
            'ListenDrops': 0,
            'TCPLoss': 188,
            'TCPTimeouts': 15265
        }

        self.assertPublishedMany(publish_mock, metrics)

    @patch('diamond.collector.Collector.publish')
    def test_should_work_with_all_data(self, publish_mock):
        self.setUp([])

        TCPCollector.PROC = [
            self.getFixturePath('proc_net_netstat_1'),
            self.getFixturePath('proc_net_snmp_1'),
            ]
        self.collector.collect()
        self.assertPublishedMany(publish_mock, {})

        TCPCollector.PROC = [
            self.getFixturePath('proc_net_netstat_2'),
            self.getFixturePath('proc_net_snmp_2'),
            ]
        self.collector.collect()

        metrics = {
            'TCPMD5Unexpected':             0.0,
            'ArpFilter':                    0.0,
            'TCPBacklogDrop':               0.0,
            'TCPDSACKRecv':                 1580.0,
            'TCPDSACKIgnoredOld':           292.0,
            'MaxConn':                      (-1.0),
            'RcvPruned':                    0.0,
            'TCPSackMerged':                1121.0,
            'OutOfWindowIcmps':             10.0,
            'TCPDeferAcceptDrop':           0.0,
            'TCPLossUndo':                  6538.0,
            'TCPHPHitsToUser':              5667.0,
            'TCPTimeouts':                  15265.0,
            'TCPForwardRetrans':            41.0,
            'TCPTSReorder':                 0.0,
            'RtoMin':                       0.0,
            'TCPAbortOnData':               143.0,
            'TCPFullUndo':                  0.0,
            'TCPSackRecoveryFail':          13.0,
            'InErrs':                       0.0,
            'TCPAbortOnClose':              38916.0,
            'TCPAbortOnTimeout':            68.0,
            'TCPFACKReorder':               0.0,
            'LockDroppedIcmps':             4.0,
            'RtoMax':                       0.0,
            'TCPSchedulerFailed':           0.0,
            'EstabResets':                  0.0,
            'DelayedACKs':                  125491.0,
            'TCPSACKReneging':              0.0,
            'PruneCalled':                  0.0,
            'OutRsts':                      0.0,
            'TCPRenoRecoveryFail':          0.0,
            'TCPSackShifted':               2356.0,
            'DelayedACKLocked':             144.0,
            'TCPHPHits':                    10361792.0,
            'EmbryonicRsts':                0.0,
            'TCPLossFailures':              7.0,
            'TWKilled':                     0.0,
            'TCPSACKDiscard':               0.0,
            'TCPAbortFailed':               0.0,
            'TCPSackRecovery':              364.0,
            'TCPDirectCopyFromBacklog':     35660.0,
            'TCPFastRetrans':               1184.0,
            'TCPPartialUndo':               0.0,
            'TCPMinTTLDrop':                0.0,
            'SyncookiesSent':               0.0,
            'OutSegs':                      0.0,
            'TCPSackShiftFallback':         3091.0,
            'RetransSegs':                  0.0,
            'IPReversePathFilter':          0.0,
            'TCPRcvCollapsed':              0.0,
            'TCPDSACKUndo':                 2448.0,
            'SyncookiesFailed':             9.0,
            'TCPSACKReorder':               0.0,
            'TCPDSACKOldSent':              10175.0,
            'TCPAbortOnLinger':             0.0,
            'TCPSpuriousRTOs':              9.0,
            'TCPRenoRecovery':              0.0,
            'TCPPrequeued':                 114232.0,
            'TCPLostRetransmit':            7.0,
            'TCPLoss':                      188.0,
            'TCPHPAcks':                    12673896.0,
            'TCPDSACKOfoRecv':              0.0,
            'TWRecycled':                   0.0,
            'TCPRenoFailures':              0.0,
            'OfoPruned':                    0.0,
            'TCPMD5NotFound':               0.0,
            'ActiveOpens':                  0.0,
            'TCPDSACKIgnoredNoUndo':        1025.0,
            'TCPPrequeueDropped':           0.0,
            'RtoAlgorithm':                 0.0,
            'TCPAbortOnMemory':             0.0,
            'TCPTimeWaitOverflow':          0.0,
            'TCPAbortOnSyn':                0.0,
            'TCPDirectCopyFromPrequeue':    19340531.0,
            'DelayedACKLost':               10118.0,
            'PassiveOpens':                 0.0,
            'InSegs':                       1.0,
            'PAWSPassive':                  0.0,
            'TCPRenoReorder':               0.0,
            'CurrEstab':                    3.0,
            'TW':                           89479.0,
            'AttemptFails':                 0.0,
            'PAWSActive':                   0.0,
            'ListenDrops':                  0.0,
            'SyncookiesRecv':               0.0,
            'TCPDSACKOfoSent':              0.0,
            'TCPSlowStartRetrans':          2540.0,
            'TCPMemoryPressures':           0.0,
            'PAWSEstab':                    0.0,
            'TCPSackFailures':              502.0,
            'ListenOverflows':              0.0,
            'TCPPureAcks':                  1003528.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = testtokumx
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import MagicMock
from mock import patch
from mock import call

from diamond.collector import Collector
from tokumx import TokuMXCollector

################################################################################


def run_only_if_pymongo_is_available(func):
    try:
        import pymongo
        pymongo  # workaround for pyflakes issue #13
    except ImportError:
        pymongo = None
    pred = lambda: pymongo is not None
    return run_only(func, pred)


class TestTokuMXCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('TokuMXCollector', {
            'host': 'localhost:27017',
            'databases': '^db',
        })
        self.collector = TokuMXCollector(config, None)
        self.connection = MagicMock()

    def test_import(self):
        self.assertTrue(TokuMXCollector)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_server_stats(self,
                                                         publish_mock,
                                                         connector_mock):
        data = {'more_keys': {'nested_key': 1}, 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_has_calls(
            [call('serverStatus'), call('engineStatus')], any_order=False)
        self.assertPublishedMany(publish_mock, {
            'more_keys.nested_key': 1,
            'key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_db_stats(self,
                                                     publish_mock,
                                                     connector_mock):
        data = {'db_keys': {'db_nested_key': 1}, 'dbkey': 2, 'dbstring': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection['db1'].command.assert_called_once_with('dbStats')
        metrics = {
            'db_keys.db_nested_key': 1,
            'dbkey': 2
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_stats_with_long_type(self,
                                                 publish_mock,
                                                 connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_has_calls(
            [call('serverStatus'), call('engineStatus')], any_order=False)
        self.assertPublishedMany(publish_mock, {
            'more_keys': 1,
            'key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_databases(self,
                                              publish_mock,
                                              connector_mock):
        self._annotate_connection(connector_mock, {})

        self.collector.collect()

        assert call('baddb') not in self.connection.__getitem__.call_args_list

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_collections(self,
                                                publish_mock,
                                                connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.connection['db1'].collection_names.return_value = ['collection1',
                                                                'tmp.mr.tmp1']
        self.connection['db1'].command.return_value = {'key': 2,
                                                       'string': 'str'}

        self.collector.collect()

        self.connection.db.command.assert_has_calls(
            [call('serverStatus'), call('engineStatus')], any_order=False)
        self.connection['db1'].collection_names.assert_called_once_with()
        self.connection['db1'].command.assert_any_call('dbStats')
        self.connection['db1'].command.assert_any_call('collstats',
                                                       'collection1')
        assert call('collstats', 'tmp.mr.tmp1') not in self.connection['db1'].command.call_args_list  # NOQA
        metrics = {
            'databases.db1.collection1.key': 2,
        }

        self.assertPublishedMany(publish_mock, metrics)

    def _annotate_connection(self, connector_mock, data):
        connector_mock.return_value = self.connection
        self.connection.db.command.return_value = data
        self.connection.database_names.return_value = ['db1', 'baddb']


class TestMongoMultiHostDBCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('TokuMXCollector', {
            'hosts': ['localhost:27017', 'localhost:27057'],
            'databases': '^db',
        })
        self.collector = TokuMXCollector(config, None)
        self.connection = MagicMock()

    def test_import(self):
        self.assertTrue(TokuMXCollector)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_server_stats(self,
                                                         publish_mock,
                                                         connector_mock):
        data = {'more_keys': {'nested_key': 1}, 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_with('engineStatus')
        self.assertPublishedMany(publish_mock, {
            'localhost_27017.more_keys.nested_key': 1,
            'localhost_27057.more_keys.nested_key': 1,
            'localhost_27017.key': 2,
            'localhost_27057.key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_db_stats(self,
                                                     publish_mock,
                                                     connector_mock):
        data = {'db_keys': {'db_nested_key': 1}, 'dbkey': 2, 'dbstring': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection['db1'].command.assert_called_with('dbStats')
        metrics = {
            'localhost_27017.db_keys.db_nested_key': 1,
            'localhost_27057.db_keys.db_nested_key': 1,
            'localhost_27017.dbkey': 2,
            'localhost_27057.dbkey': 2
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_stats_with_long_type(self,
                                                 publish_mock,
                                                 connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_with('engineStatus')
        self.assertPublishedMany(publish_mock, {
            'localhost_27017.more_keys': 1,
            'localhost_27057.more_keys': 1,
            'localhost_27017.key': 2,
            'localhost_27057.key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_databases(self,
                                              publish_mock,
                                              connector_mock):
        self._annotate_connection(connector_mock, {})

        self.collector.collect()

        assert call('baddb') not in self.connection.__getitem__.call_args_list

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_collections(self,
                                                publish_mock,
                                                connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.connection['db1'].collection_names.return_value = ['collection1',
                                                                'tmp.mr.tmp1']
        self.connection['db1'].command.return_value = {'key': 2,
                                                       'string': 'str'}

        self.collector.collect()

        self.connection.db.command.assert_has_calls(
            [call('serverStatus'), call('engineStatus')], any_order=False)
        self.connection['db1'].collection_names.assert_called_with()
        self.connection['db1'].command.assert_any_call('dbStats')
        self.connection['db1'].command.assert_any_call('collstats',
                                                       'collection1')
        assert call('collstats', 'tmp.mr.tmp1') not in self.connection['db1'].command.call_args_list  # NOQA
        metrics = {
            'localhost_27017.databases.db1.collection1.key': 2,
            'localhost_27057.databases.db1.collection1.key': 2,
        }

        self.assertPublishedMany(publish_mock, metrics)

    def _annotate_connection(self, connector_mock, data):
        connector_mock.return_value = self.connection
        self.connection.db.command.return_value = data
        self.connection.database_names.return_value = ['db1', 'baddb']


################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = tokumx
# coding=utf-8

"""
Collects all number values from the db.serverStatus() and db.engineStatus()
command, other values are ignored.

#### Dependencies

 * pymongo

"""

import diamond.collector
from diamond.collector import str_to_bool
import re

try:
    import pymongo
    pymongo  # workaround for pyflakes issue #13
except ImportError:
    pymongo = None

try:
    from pymongo import ReadPreference
    ReadPreference  # workaround for pyflakes issue #13
except ImportError:
    ReadPreference = None


class TokuMXCollector(diamond.collector.Collector):

    def __init__(self, *args, **kwargs):
        self.__totals = {}
        super(TokuMXCollector, self).__init__(*args, **kwargs)

    def get_default_config_help(self):
        config_help = super(TokuMXCollector, self).get_default_config_help()
        config_help.update({
            'hosts': 'Array of hostname(:port) elements to get metrics from'
                     'Set an alias by prefixing host:port with alias@',
            'host': 'A single hostname(:port) to get metrics from'
                    ' (can be used instead of hosts and overrides it)',
            'user': 'Username for authenticated login (optional)',
            'passwd': 'Password for authenticated login (optional)',
            'databases': 'A regex of which databases to gather metrics for.'
                         ' Defaults to all databases.',
            'ignore_collections': 'A regex of which collections to ignore.'
                                  ' MapReduce temporary collections (tmp.mr.*)'
                                  ' are ignored by default.',
            'network_timeout': 'Timeout for mongodb connection (in seconds).'
                               ' There is no timeout by default.',
            'simple': 'Only collect the same metrics as mongostat.',
            'translate_collections': 'Translate dot (.) to underscores (_)'
                                     ' in collection names.'
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(TokuMXCollector, self).get_default_config()
        config.update({
            'path':      'mongo',
            'hosts':     ['localhost'],
            'user':      None,
            'passwd':      None,
            'databases': '.*',
            'ignore_collections': '^tmp\.mr\.',
            'network_timeout': None,
            'simple': 'False',
            'translate_collections': 'False'
        })
        return config

    def collect(self):
        """Collect number values from db.serverStatus() and db.engineStatus()"""

        if pymongo is None:
            self.log.error('Unable to import pymongo')
            return

        # we need this for backwards compatibility
        if 'host' in self.config:
            self.config['hosts'] = [self.config['host']]

        # convert network_timeout to integer
        if self.config['network_timeout']:
            self.config['network_timeout'] = int(
                self.config['network_timeout'])

        # use auth if given
        if 'user' in self.config:
            user = self.config['user']
        else:
            user = None

        if 'passwd' in self.config:
            passwd = self.config['passwd']
        else:
            passwd = None

        for host in self.config['hosts']:
            if len(self.config['hosts']) == 1:
                # one host only, no need to have a prefix
                base_prefix = []
            else:
                matches = re.search('((.+)\@)?(.+)?', host)
                alias = matches.group(2)
                host = matches.group(3)

                if alias is None:
                    base_prefix = [re.sub('[:\.]', '_', host)]
                else:
                    base_prefix = [alias]

            try:
                if ReadPreference is None:
                    conn = pymongo.Connection(
                        host,
                        network_timeout=self.config['network_timeout'],
                        slave_okay=True
                    )
                else:
                    conn = pymongo.Connection(
                        host,
                        network_timeout=self.config['network_timeout'],
                        read_preference=ReadPreference.SECONDARY,
                    )
            except Exception, e:
                self.log.error('Couldnt connect to mongodb: %s', e)
                continue

            # try auth
            if user:
                try:
                    conn.admin.authenticate(user, passwd)
                except Exception, e:
                    self.log.error('User auth given, but could not autheticate'
                                   + ' with host: %s, err: %s' % (host, e))
                    return{}

            serverStatus = conn.db.command('serverStatus')
            engineStatus = conn.db.command('engineStatus')
            data = dict(serverStatus.items() + engineStatus.items())

            self._publish_transformed(data, base_prefix)
            if str_to_bool(self.config['simple']):
                data = self._extract_simple_data(data)

            self._publish_dict_with_prefix(data, base_prefix)
            db_name_filter = re.compile(self.config['databases'])
            ignored_collections = re.compile(self.config['ignore_collections'])
            for db_name in conn.database_names():
                if not db_name_filter.search(db_name):
                    continue
                db_stats = conn[db_name].command('dbStats')
                db_prefix = base_prefix + ['databases', db_name]
                self._publish_dict_with_prefix(db_stats, db_prefix)
                for collection_name in conn[db_name].collection_names():
                    if ignored_collections.search(collection_name):
                        continue
                    collection_stats = conn[db_name].command('collstats',
                                                             collection_name)
                    if str_to_bool(self.config['translate_collections']):
                        collection_name = collection_name.replace('.', '_')
                    collection_prefix = db_prefix + [collection_name]
                    self._publish_dict_with_prefix(collection_stats,
                                                   collection_prefix)

    def _publish_transformed(self, data, base_prefix):
        """ Publish values of type: counter or percent """
        self._publish_dict_with_prefix(data.get('opcounters', {}),
                                       base_prefix + ['opcounters_per_sec'],
                                       self.publish_counter)
        self._publish_dict_with_prefix(data.get('opcountersRepl', {}),
                                       base_prefix + ['opcountersRepl_per_sec'],
                                       self.publish_counter)
        self._publish_dict_with_prefix(data.get('network', {}),
                                       base_prefix + ['network_per_sec'],
                                       self.publish_counter)
        self._publish_metrics(base_prefix + ['extra_info_per_sec'],
                              'page_faults',
                              data.get('extra_info', {}),
                              self.publish_counter)

        def get_dotted_value(data, key_name):
            key_name = key_name.split('.')
            for i in key_name:
                data = data.get(i, {})
                if not data:
                    return 0
            return data

        def compute_interval(data, total_name):
            current_total = get_dotted_value(data, total_name)
            total_key = '.'.join(base_prefix + [total_name])
            last_total = self.__totals.get(total_key, current_total)
            interval = current_total - last_total
            self.__totals[total_key] = current_total
            return interval

        def publish_percent(value_name, total_name, data):
            value = float(get_dotted_value(data, value_name) * 100)
            interval = compute_interval(data, total_name)
            key = '.'.join(base_prefix + ['percent', value_name])
            self.publish_counter(key, value, time_delta=bool(interval),
                                 interval=interval)

        publish_percent('globalLock.lockTime', 'globalLock.totalTime', data)

        locks = data.get('locks')
        if locks:
            if '.' in locks:
                locks['_global_'] = locks['.']
                del (locks['.'])
            key_prefix = '.'.join(base_prefix + ['percent'])
            db_name_filter = re.compile(self.config['databases'])
            interval = compute_interval(data, 'uptimeMillis')
            for db_name in locks:
                if not db_name_filter.search(db_name):
                    continue
                r = get_dotted_value(
                    locks,
                    '%s.timeLockedMicros.r' % db_name)
                R = get_dotted_value(
                    locks,
                    '.%s.timeLockedMicros.R' % db_name)
                value = float(r + R) / 10
                if value:
                    self.publish_counter(
                        key_prefix + '.locks.%s.read' % db_name,
                        value, time_delta=bool(interval),
                        interval=interval)
                w = get_dotted_value(
                    locks,
                    '%s.timeLockedMicros.w' % db_name)
                W = get_dotted_value(
                    locks,
                    '%s.timeLockedMicros.W' % db_name)
                value = float(w + W) / 10
                if value:
                    self.publish_counter(
                        key_prefix + '.locks.%s.write' % db_name,
                        value, time_delta=bool(interval), interval=interval)

    def _publish_dict_with_prefix(self, dict, prefix, publishfn=None):
        for key in dict:
            self._publish_metrics(prefix, key, dict, publishfn)

    def _publish_metrics(self, prev_keys, key, data, publishfn=None):
        """Recursively publish keys"""
        if not key in data:
            return
        value = data[key]
        keys = prev_keys + [key]
        if not publishfn:
            publishfn = self.publish
        if isinstance(value, dict):
            for new_key in value:
                self._publish_metrics(keys, new_key, value)
        elif isinstance(value, int) or isinstance(value, float):
            publishfn('.'.join(keys), value)
        elif isinstance(value, long):
            publishfn('.'.join(keys), float(value))

    def _extract_simple_data(self, data):
        return {
            'connections': data.get('connections'),
            'globalLock': data.get('globalLock'),
            'indexCounters': data.get('indexCounters')
        }

########NEW FILE########
__FILENAME__ = testudp
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from udp import UDPCollector

################################################################################


class TestUDPCollector(CollectorTestCase):
    def setUp(self, allowed_names=None):
        if not allowed_names:
            allowed_names = []
        config = get_collector_config('UDPCollector', {
            'allowed_names': allowed_names,
            'interval': 1
        })
        self.collector = UDPCollector(config, None)

    def test_import(self):
        self.assertTrue(UDPCollector)

    @patch('os.access', Mock(return_value=True))
    @patch('__builtin__.open')
    @patch.object(Collector, 'publish')
    def test_should_open_proc_net_snmp(self, publish_mock, open_mock):
        UDPCollector.PROC = ['/proc/net/snmp']
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/net/snmp')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        self.setUp([])

        UDPCollector.PROC = [
            self.getFixturePath('proc_net_snmp_1'),
            ]
        self.collector.collect()
        self.assertPublishedMany(publish_mock, {})

        UDPCollector.PROC = [
            self.getFixturePath('proc_net_snmp_2'),
            ]
        self.collector.collect()

        metrics = {
            'InDatagrams': 352320636.0,
            'InErrors': 5.0,
            'NoPorts': 449.0,
            'OutDatagrams': 352353358.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = udp
# coding=utf-8

"""
The UDPCollector class collects metrics on UDP stats (surprise!)

#### Dependencies

 * /proc/net/snmp

"""

import diamond.collector
import os


class UDPCollector(diamond.collector.Collector):

    PROC = [
        '/proc/net/snmp'
    ]

    def __init__(self, config, handlers):
        super(UDPCollector, self).__init__(config, handlers)
        if self.config['allowed_names'] is None:
            self.config['allowed_names'] = []

    def get_default_config_help(self):
        config_help = super(UDPCollector, self).get_default_config_help()
        config_help.update({
            'allowed_names': 'list of entries to collect, empty to collect all',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(UDPCollector, self).get_default_config()
        config.update({
            'path':             'udp',
            'allowed_names':    'InDatagrams, NoPorts, '
            + 'InErrors, OutDatagrams, RcvbufErrors, SndbufErrors'
        })
        return config

    def collect(self):
        metrics = {}

        for filepath in self.PROC:
            if not os.access(filepath, os.R_OK):
                self.log.error('Permission to access %s denied', filepath)
                continue

            header = ''
            data = ''

            # Seek the file for the lines that start with Tcp
            file = open(filepath)

            if not file:
                self.log.error('Failed to open %s', filepath)
                continue

            while True:
                line = file.readline()

                # Reached EOF?
                if len(line) == 0:
                    break

                # Line has metrics?
                if line.startswith("Udp"):
                    header = line
                    data = file.readline()
                    break
            file.close()

            # No data from the file?
            if header == '' or data == '':
                self.log.error('%s has no lines with Udp', filepath)
                continue

            header = header.split()
            data = data.split()

            for i in xrange(1, len(header)):
                metrics[header[i]] = data[i]

        for metric_name in metrics.keys():
            if (len(self.config['allowed_names']) > 0
                    and metric_name not in self.config['allowed_names']):
                continue

            value = metrics[metric_name]
            value = self.derivative(metric_name, long(value))

            # Publish the metric
            self.publish(metric_name, value, 0)

########NEW FILE########
__FILENAME__ = testunbound
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from unbound import UnboundCollector

################################################################################


class TestUnboundCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('UnboundCollector', {})

        self.collector = UnboundCollector(config, None)

    def test_import(self):
        self.assertTrue(UnboundCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_wtih_real_data(self, publish_mock):
        fixture_data = self.getFixture('unbound_stats').getvalue()
        collector_mock = patch.object(UnboundCollector,
                                      'get_unbound_control_output',
                                      Mock(return_value=fixture_data))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        metrics = {
            'thread0.num.queries': 10028,
            'thread0.num.cachehits': 10021,
            'thread0.num.cachemiss': 7,
            'thread0.num.prefetch': 1,
            'thread0.num.recursivereplies': 9,
            'thread0.requestlist.avg': 1.25,
            'thread0.requestlist.max': 2,
            'thread0.requestlist.overwritten': 0,
            'thread0.requestlist.exceeded': 0,
            'thread0.requestlist.current.all': 1,
            'thread0.requestlist.current.user': 1,
            'thread0.recursion.time.avg': 9.914812,
            'thread0.recursion.time.median': 0.08192,
            'total.num.queries': 125609,
            'total.num.cachehits': 125483,
            'total.num.cachemiss': 126,
            'total.num.prefetch': 16,
            'total.num.recursivereplies': 136,
            'total.requestlist.avg': 5.07746,
            'total.requestlist.max': 10,
            'total.requestlist.overwritten': 0,
            'total.requestlist.exceeded': 0,
            'total.requestlist.current.all': 23,
            'total.requestlist.current.user': 23,
            'total.recursion.time.avg': 13.045485,
            'total.recursion.time.median': 0.06016,
            'time.now': 1361926066.384873,
            'time.up': 3006293.632453,
            'time.elapsed': 9.981882,
            'mem.total.sbrk': 26767360,
            'mem.cache.rrset': 142606276,
            'mem.cache.message': 71303005,
            'mem.mod.iterator': 16532,
            'mem.mod.validator': 1114579,
            'num.query.type.A': 25596,
            'num.query.type.PTR': 39,
            'num.query.type.MX': 91,
            'num.query.type.AAAA': 99883,
            'num.query.class.IN': 125609,
            'num.query.opcode.QUERY': 125609,
            'num.query.tcp': 0,
            'num.query.ipv6': 0,
            'num.query.flags.QR': 0,
            'num.query.flags.AA': 0,
            'num.query.flags.TC': 0,
            'num.query.flags.RD': 125609,
            'num.query.flags.RA': 0,
            'num.query.flags.Z': 0,
            'num.query.flags.AD': 0,
            'num.query.flags.CD': 62,
            'num.query.edns.present': 62,
            'num.query.edns.DO': 62,
            'num.answer.rcode.NOERROR': 46989,
            'num.answer.rcode.SERVFAIL': 55,
            'num.answer.rcode.NXDOMAIN': 78575,
            'num.answer.rcode.nodata': 20566,
            'num.answer.secure': 0,
            'num.answer.bogus': 0,
            'num.rrset.bogus': 0,
            'unwanted.queries': 0,
            'unwanted.replies': 0,
            'histogram.16s+': 0.0,
            'histogram.256ms+': 3.0,
            'histogram.4s+': 1.0,
            'histogram.2s+': 0.0,
            'histogram.1s+': 0.0,
            'histogram.2ms+': 0.0,
            'histogram.1ms': 39.0,
            'histogram.32ms+': 18.0,
            'histogram.4ms+': 0.0,
            'histogram.16ms+': 10.0,
            'histogram.1ms+': 5.0,
            'histogram.32s+': 3.0,
            'histogram.512ms+': 6.0,
            'histogram.128ms+': 19.0,
            'histogram.64ms+': 20.0,
            'histogram.8ms+': 3.0,
            'histogram.64s+': 9.0,
            'histogram.8s+': 0.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        collector_mock = patch.object(UnboundCollector,
                                      'get_unbound_control_output',
                                      Mock(return_value=''))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        self.assertPublishedMany(publish_mock, {})

    @patch.object(Collector, 'publish')
    def test_exclude_histogram(self, publish_mock):
        self.collector.config['histogram'] = False

        fixture_data = self.getFixture('unbound_stats').getvalue()
        collector_mock = patch.object(UnboundCollector,
                                      'get_unbound_control_output',
                                      Mock(return_value=fixture_data))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        metrics = {
            'thread0.num.queries': 10028,
            'thread0.num.cachehits': 10021,
            'thread0.num.cachemiss': 7,
            'thread0.num.prefetch': 1,
            'thread0.num.recursivereplies': 9,
            'thread0.requestlist.avg': 1.25,
            'thread0.requestlist.max': 2,
            'thread0.requestlist.overwritten': 0,
            'thread0.requestlist.exceeded': 0,
            'thread0.requestlist.current.all': 1,
            'thread0.requestlist.current.user': 1,
            'thread0.recursion.time.avg': 9.914812,
            'thread0.recursion.time.median': 0.08192,
            'total.num.queries': 125609,
            'total.num.cachehits': 125483,
            'total.num.cachemiss': 126,
            'total.num.prefetch': 16,
            'total.num.recursivereplies': 136,
            'total.requestlist.avg': 5.07746,
            'total.requestlist.max': 10,
            'total.requestlist.overwritten': 0,
            'total.requestlist.exceeded': 0,
            'total.requestlist.current.all': 23,
            'total.requestlist.current.user': 23,
            'total.recursion.time.avg': 13.045485,
            'total.recursion.time.median': 0.06016,
            'time.now': 1361926066.384873,
            'time.up': 3006293.632453,
            'time.elapsed': 9.981882,
            'mem.total.sbrk': 26767360,
            'mem.cache.rrset': 142606276,
            'mem.cache.message': 71303005,
            'mem.mod.iterator': 16532,
            'mem.mod.validator': 1114579,
            'num.query.type.A': 25596,
            'num.query.type.PTR': 39,
            'num.query.type.MX': 91,
            'num.query.type.AAAA': 99883,
            'num.query.class.IN': 125609,
            'num.query.opcode.QUERY': 125609,
            'num.query.tcp': 0,
            'num.query.ipv6': 0,
            'num.query.flags.QR': 0,
            'num.query.flags.AA': 0,
            'num.query.flags.TC': 0,
            'num.query.flags.RD': 125609,
            'num.query.flags.RA': 0,
            'num.query.flags.Z': 0,
            'num.query.flags.AD': 0,
            'num.query.flags.CD': 62,
            'num.query.edns.present': 62,
            'num.query.edns.DO': 62,
            'num.answer.rcode.NOERROR': 46989,
            'num.answer.rcode.SERVFAIL': 55,
            'num.answer.rcode.NXDOMAIN': 78575,
            'num.answer.rcode.nodata': 20566,
            'num.answer.secure': 0,
            'num.answer.bogus': 0,
            'num.rrset.bogus': 0,
            'unwanted.queries': 0,
            'unwanted.replies': 0,
        }

        histogram = {
            'histogram.16s+': 0.0,
            'histogram.256ms+': 3.0,
            'histogram.4s+': 1.0,
            'histogram.2s+': 0.0,
            'histogram.1s+': 0.0,
            'histogram.2ms+': 0.0,
            'histogram.1ms': 39.0,
            'histogram.32ms+': 18.0,
            'histogram.4ms+': 0.0,
            'histogram.16ms+': 10.0,
            'histogram.1ms+': 5.0,
            'histogram.32s+': 3.0,
            'histogram.512ms+': 6.0,
            'histogram.128ms+': 19.0,
            'histogram.64ms+': 20.0,
            'histogram.8ms+': 3.0,
            'histogram.64s+': 9.0,
            'histogram.8s+': 0.0,
        }

        self.assertPublishedMany(publish_mock, metrics)
        self.assertUnpublishedMany(publish_mock, histogram)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = unbound
# coding=utf-8

"""
Collect stats from the unbound resolver

#### Dependencies

    * subprocess
    * collections.defaultdict or kitchen

"""

import subprocess

try:
    from collections import defaultdict
    defaultdict  # Pyflakes
except ImportError:
    from kitchen.pycompat25.collections import defaultdict

import diamond.collector
from diamond.collector import str_to_bool


class UnboundCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(UnboundCollector, self).get_default_config_help()
        config_help.update({
            'bin':          'Path to unbound-control binary',
            'use_sudo':     'Use sudo?',
            'sudo_cmd':     'Path to sudo',
            'histogram':    'Include histogram in collection',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(UnboundCollector, self).get_default_config()
        config.update({
            'path':         'unbound',
            'bin':          self.find_binary('/usr/sbin/unbound-control'),
            'use_sudo':     False,
            'sudo_cmd':     self.find_binary('/usr/bin/sudo'),
            'histogram':    True,
        })
        return config

    def get_unbound_control_output(self):
        try:
            command = [self.config['bin'], ' stats']

            if str_to_bool(self.config['use_sudo']):
                command.insert(0, self.config['sudo_cmd'])

            return subprocess.Popen(command,
                                    stdout=subprocess.PIPE).communicate()[0]
        except OSError:
            self.log.exception("Unable to run %s", command)
            return ""

    def get_massaged_histogram(self, raw_histogram):
        histogram = defaultdict(int)

        for intv in sorted(raw_histogram.keys()):
            if intv <= 0.001024:
                # Let's compress <1ms into 1 data point
                histogram['1ms'] += raw_histogram[intv]
            elif intv < 1.0:
                # Convert to ms and since we're using the upper limit
                # divide by 2 for lower limit
                intv_name = ''.join([str(int(intv / 0.001024 / 2)), 'ms+'])
                histogram[intv_name] = raw_histogram[intv]
            elif intv == 1.0:
                histogram['512ms+'] = raw_histogram[intv]
            elif intv > 1.0 and intv <= 64.0:
                # Convert upper limit into lower limit seconds
                intv_name = ''.join([str(int(intv / 2)), 's+'])
                histogram[intv_name] = raw_histogram[intv]
            else:
                # Compress everything >64s into 1 data point
                histogram['64s+'] += raw_histogram[intv]

        return histogram

    def collect(self):
        stats_output = self.get_unbound_control_output()

        raw_histogram = {}

        for line in stats_output.splitlines():
            stat_name, stat_value = line.split('=')

            if not stat_name.startswith('histogram'):
                self.publish(stat_name, stat_value)
            elif self.config['histogram']:
                hist_intv = float(stat_name.split('.', 4)[4])
                raw_histogram[hist_intv] = float(stat_value)

        if self.config['histogram']:
            histogram = self.get_massaged_histogram(raw_histogram)

            for intv, value in histogram.iteritems():
                self.publish('histogram.' + intv, value)

########NEW FILE########
__FILENAME__ = testups
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector
from ups import UPSCollector

################################################################################


class TestUPSCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('UPSCollector', {
            'interval': 10,
            'bin': 'true',
            'use_sudo': False,
        })

        self.collector = UPSCollector(config, None)

    def test_import(self):
        self.assertTrue(UPSCollector)

    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data_cp550slg(self, publish_mock):
        patch_listdir = patch('os.listdir', Mock(return_value=['sda']))
        patch_communicate = patch(
            'subprocess.Popen.communicate',
            Mock(return_value=(
                self.getFixture('cp550slg').getvalue(),
                '')))
        patch_listdir.start()
        patch_communicate.start()
        self.collector.collect()
        patch_listdir.stop()
        patch_communicate.stop()

        metrics = {
            'battery.charge.charge': 100.0,
            'battery.charge.low': 10.0,
            'battery.charge.warning': 20.0,
            'battery.runtime.runtime': 960.0,
            'battery.runtime.low': 300.0,
            'battery.voltage.voltage': 4.9,
            'battery.voltage.nominal': 12.0,
            'driver.parameter.pollfreq': 30.0,
            'driver.parameter.pollinterval': 2.0,
            'driver.version.internal': 0.34,
            'input.transfer.high': 0.0,
            'input.transfer.low': 0.0,
            'input.voltage.voltage': 121.0,
            'input.voltage.nominal': 120.0,
            'output.voltage.voltage': 120.0,
            'ups.delay.shutdown': 20.0,
            'ups.delay.start': 30.0,
            'ups.load.load': 46.0,
            'ups.productid.productid': 501.0,
            'ups.realpower.nominal': 330.0,
            'ups.timer.shutdown': -60.0,
            'ups.timer.start': 0.0,
            'ups.vendorid.vendorid': 764.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ups
# coding=utf-8

"""
This class collects data from NUT, a UPS interface for linux.

#### Dependencies

 * nut/upsc to be installed, configured and running.

"""

import diamond.collector
import os
import subprocess
from diamond.collector import str_to_bool


class UPSCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(UPSCollector, self).get_default_config_help()
        config_help.update({
            'ups_name':    'The name of the ups to collect data for',
            'bin':         'The path to the upsc binary',
            'use_sudo':    'Use sudo?',
            'sudo_cmd':    'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns default collector settings.
        """

        config = super(UPSCollector, self).get_default_config()
        config.update({
            'path':             'ups',
            'ups_name':         'cyberpower',
            'bin':              '/bin/upsc',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
        })
        return config

    def collect(self):
        if not os.access(self.config['bin'], os.X_OK):
            self.log.error("%s is not executable", self.config['bin'])
            return False

        command = [self.config['bin'], self.config['ups_name']]

        if str_to_bool(self.config['use_sudo']):
            command.insert(0, self.config['sudo_cmd'])

        p = subprocess.Popen(command,
                             stdout=subprocess.PIPE).communicate()[0]

        for ln in p.strip().splitlines():
            datapoint = ln.split(": ")

            try:
                val = float(datapoint[1])
            except:
                continue

            if len(datapoint[0].split(".")) == 2:
                # If the metric name is the same as the subfolder
                # double it so it's visible.
                name = ".".join([datapoint[0], datapoint[0].split(".")[1]])
            else:
                name = datapoint[0]

            self.publish(name, val)

########NEW FILE########
__FILENAME__ = testusers
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import patch

from diamond.collector import Collector
from users import UsersCollector

import sys


################################################################################


def run_only_if_pyutmp_is_available(func):
    try:
        import pyutmp
        pyutmp  # workaround for pyflakes issue #13
    except ImportError:
        pyutmp = None
    try:
        import utmp
        utmp  # workaround for pyflakes issue #13
    except ImportError:
        utmp = None
    pred = lambda: pyutmp is not None or utmp is not None
    return run_only(func, pred)


class TestUsersCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('UsersCollector', {
            'utmp': self.getFixturePath('utmp.centos6'),
        })

        self.collector = UsersCollector(config, None)

    def test_import(self):
        self.assertTrue(UsersCollector)

    @run_only_if_pyutmp_is_available
    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):

        metrics = {
            'kormoc':   2,
            'root':     3,
            'total':    5,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

        # Because of the compiled nature of pyutmp, we can't actually test
        # different operating system versions then the currently running
        # one
        if sys.platform.startswith('linux'):
            self.collector.collect()

            self.assertPublishedMany(publish_mock, metrics)

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = users
# coding=utf-8

"""
Collects the number of users logged in and shells per user

#### Dependencies

 * [pyutmp](http://software.clapper.org/pyutmp/)
or
 * [utmp] (python-utmp on Debian and derivatives)

"""

import diamond.collector

try:
    from pyutmp import UtmpFile
    UtmpFile  # workaround for pyflakes issue #13
except ImportError:
    UtmpFile = None
try:
    from utmp import UtmpRecord
    import UTMPCONST
    UtmpRecord  # workaround for pyflakes issue #13
except ImportError:
    UtmpRecord = None


class UsersCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        """
        Returns the default collector help text
        """
        config_help = super(UsersCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(UsersCollector, self).get_default_config()
        config.update({
            'path':     'users',
            'method':   'Threaded',
            'utmp':     None,
        })
        return config

    def collect(self):
        if UtmpFile is None and UtmpRecord is None:
            self.log.error('Unable to import either pyutmp or python-utmp')
            return False

        metrics = {}
        metrics['total'] = 0

        if UtmpFile:
            for utmp in UtmpFile(path=self.config['utmp']):
                if utmp.ut_user_process:
                    metrics[utmp.ut_user] = metrics.get(utmp.ut_user, 0) + 1
                    metrics['total'] = metrics['total'] + 1

        if UtmpRecord:
            for utmp in UtmpRecord(fname=self.config['utmp']):
                if utmp.ut_type == UTMPCONST.USER_PROCESS:
                    metrics[utmp.ut_user] = metrics.get(utmp.ut_user, 0) + 1
                    metrics['total'] = metrics['total'] + 1

        for metric_name in metrics.keys():
            self.publish(metric_name, metrics[metric_name])

        return True

########NEW FILE########
__FILENAME__ = testuserscripts
#!/usr/bin/python
# coding=utf-8
################################################################################

import os
import sys

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import patch

from diamond.collector import Collector
from userscripts import UserScriptsCollector

################################################################################


def run_only_if_kitchen_is_available(func):
    if sys.version_info < (2, 7):
        try:
            from kitchen.pycompat27 import subprocess
            subprocess  # workaround for pyflakes issue #13
        except ImportError:
            subprocess = None
    else:
        import subprocess
    pred = lambda: subprocess is not None
    return run_only(func, pred)


class TestUserScriptsCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('UserScriptsCollector', {
            'interval': 10,
            'scripts_path': os.path.dirname(__file__) + '/fixtures/',
        })

        self.collector = UserScriptsCollector(config, None)

    def test_import(self):
        self.assertTrue(UserScriptsCollector)

    @run_only_if_kitchen_is_available
    @patch.object(Collector, 'publish')
    def test_should_work_with_example(self, publish_mock):
        self.collector.collect()

        metrics = {
            'example.1': 42,
            'example.2': 24,
            'example.3': 12.1212,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics)
        self.assertPublishedMany(publish_mock, metrics)

    @run_only_if_kitchen_is_available
    @patch.object(Collector, 'publish')
    def test_should_skip_over_unrunnable_files(self, publish_mock):
        self.collector.collect()
        # Just make sure publish got called >0 times, if this test fails it'll
        # be due to raising an exception. Meh.
        assert publish_mock.call_args_list

################################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = userscripts
# coding=utf-8

"""
Runs third party scripts and collects their output.

Scripts need to be +x and should output metrics in the form of

```
metric.path.a 1
metric.path.b 2
metric.path.c 3
```

They are not passed any arguments and if they return an error code,
no metrics are collected.

#### Dependencies

 * [subprocess](http://docs.python.org/library/subprocess.html)

"""

import diamond.collector
import diamond.convertor
import os
import subprocess


class UserScriptsCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(UserScriptsCollector,
                            self).get_default_config_help()
        config_help.update({
            'scripts_path': "Path to find the scripts to run",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(UserScriptsCollector, self).get_default_config()
        config.update({
            'path':         '.',
            'scripts_path': '/etc/diamond/user_scripts/',
            'method':       'Threaded',
            'floatprecision': 4,
        })
        return config

    def collect(self):
        scripts_path = self.config['scripts_path']
        if not os.access(scripts_path, os.R_OK):
            return None
        for script in os.listdir(scripts_path):
            absolutescriptpath = os.path.join(scripts_path, script)
            executable = os.access(absolutescriptpath, os.X_OK)
            is_file = os.path.isfile(absolutescriptpath)
            if is_file:
                if not executable:
                    self.log.info("%s is not executable" % absolutescriptpath)
                    continue
            else:
                # Don't bother logging skipped non-file files (typically
                # directories)
                continue
            out = None
            self.log.debug("Executing %s" % absolutescriptpath)
            try:
                proc = subprocess.Popen([absolutescriptpath],
                                        stdout=subprocess.PIPE,
                                        stderr=subprocess.PIPE)
                (out, err) = proc.communicate()
            except subprocess.CalledProcessError, e:
                self.log.error("%s error launching: %s; skipping" %
                               (absolutescriptpath, e))
                continue
            if proc.returncode:
                self.log.error("%s return exit value %s; skipping" %
                               (absolutescriptpath, proc.returncode))
            if not out:
                self.log.info("%s return no output" % absolutescriptpath)
                continue
            if err:
                self.log.error("%s return error output: %s" %
                               (absolutescriptpath, err))
            # Use filter to remove empty lines of output
            for line in filter(None, out.split('\n')):
                name, value = line.split()
                floatprecision = 0
                if "." in value:
                    floatprecision = self.config['floatprecision']
                self.publish(name, value, floatprecision)

########NEW FILE########
__FILENAME__ = testvarnish
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from varnish import VarnishCollector

###############################################################################


class TestVarnishCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('VarnishCollector', {})

        self.collector = VarnishCollector(config, None)

    def test_import(self):
        self.assertTrue(VarnishCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        collector_mock = patch.object(VarnishCollector, 'poll', Mock(
            return_value=self.getFixture('varnish_stats').getvalue()))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        metrics = {
            'client_conn': 10799,
            'client_drop': 0,
            'client_req': 10796,
            'cache_hit': 6580,
            'cache_hitpass': 0,
            'cache_miss': 2566,
            'backend_conn': 13363,
            'backend_unhealthy': 0,
            'backend_busy': 0,
            'backend_fail': 0,
            'backend_reuse': 0,
            'backend_toolate': 0,
            'backend_recycle': 0,
            'backend_retry': 0,
            'fetch_head': 0,
            'fetch_length': 12986,
            'fetch_chunked': 0,
            'fetch_eof': 0,
            'fetch_bad': 0,
            'fetch_close': 331,
            'fetch_oldhttp': 0,
            'fetch_zero': 0,
            'fetch_failed': 0,
            'n_sess_mem': 19,
            'n_sess': 1,
            'n_object': 9,
            'n_vampireobject': 0,
            'n_objectcore': 17,
            'n_objecthead': 27,
            'n_waitinglist': 10,
            'n_vbc': 1,
            'n_wrk': 10,
            'n_wrk_create': 10,
            'n_wrk_failed': 0,
            'n_wrk_max': 11451,
            'n_wrk_lqueue': 0,
            'n_wrk_queued': 0,
            'n_wrk_drop': 0,
            'n_backend': 4,
            'n_expired': 2557,
            'n_lru_nuked': 0,
            'n_lru_moved': 5588,
            'losthdr': 0,
            'n_objsendfile': 0,
            'n_objwrite': 2546,
            'n_objoverflow': 0,
            's_sess': 10798,
            's_req': 10796,
            's_pipe': 0,
            's_pass': 10796,
            's_fetch': 13362,
            's_hdrbytes': 4764593,
            's_bodybytes': 23756354,
            'sess_closed': 10798,
            'sess_pipeline': 0,
            'sess_readahead': 0,
            'sess_linger': 0,
            'sess_herd': 0,
            'shm_records': 1286246,
            'shm_writes': 102894,
            'shm_flushes': 0,
            'shm_cont': 0,
            'shm_cycles': 0,
            'sms_nreq': 0,
            'sms_nobj': 0,
            'sms_nbytes': 0,
            'sms_balloc': 0,
            'sms_bfree': 0,
            'backend_req': 13363,
            'n_vcl': 1,
            'n_vcl_avail': 1,
            'n_vcl_discard': 0,
            'n_ban': 1,
            'n_ban_add': 1,
            'n_ban_retire': 0,
            'n_ban_obj_test': 0,
            'n_ban_re_test': 0,
            'n_ban_dups': 0,
            'hcb_nolock': 9146,
            'hcb_lock': 2379,
            'hcb_insert': 2379,
            'accept_fail': 0,
            'client_drop_late': 0,
            'uptime': 35440,
            'dir_dns_lookups': 0,
            'dir_dns_failed': 0,
            'dir_dns_hit': 0,
            'dir_dns_cache_full': 0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        collector_mock = patch.object(VarnishCollector, 'poll', Mock(
            return_value=self.getFixture(
                'varnish_stats_blank').getvalue()))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        self.assertPublishedMany(publish_mock, {})

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = varnish
# coding=utf-8

"""
VarnishCollector grabs stats from Varnish and submits them the Graphite

#### Dependencies

 * /usr/bin/varnishstat

"""

import diamond.collector
import re
import subprocess
from diamond.collector import str_to_bool


class VarnishCollector(diamond.collector.Collector):

    _RE = re.compile("^(?P<stat>[\w_\(\)\.,]*)\s+(?P<psa>\d*)\s+"
                     "(?P<psan>[\d.]*)\s(?P<desc>[\w\., /]*)$", re.M)
    _KEYS_v3 = frozenset([
        'client_conn', 'client_drop', 'client_req', 'cache_hit',
        'cache_hitpass', 'cache_miss', 'backend_conn', 'backend_unhealthy',
        'backend_busy', 'backend_fail', 'backend_reuse', 'backend_toolate',
        'backend_recycle', 'backend_retry', 'fetch_head', 'fetch_length',
        'fetch_chunked', 'fetch_eof', 'fetch_bad', 'fetch_close',
        'fetch_oldhttp', 'fetch_zero', 'fetch_failed', 'n_sess_mem',
        'n_sess', 'n_object', 'n_vampireobject', 'n_objectcore',
        'n_objecthead', 'n_waitinglist', 'n_vbc', 'n_wrk', 'n_wrk_create',
        'n_wrk_failed', 'n_wrk_max', 'n_wrk_lqueue', 'n_wrk_queued',
        'n_wrk_drop', 'n_backend', 'n_expired', 'n_lru_nuked',
        'n_lru_moved', 'losthdr', 'n_objsendfile', 'n_objwrite',
        'n_objoverflow', 's_sess', 's_req', 's_pipe', 's_pass', 's_fetch',
        's_hdrbytes', 's_bodybytes', 'sess_closed', 'sess_pipeline',
        'sess_readahead', 'sess_linger', 'sess_herd', 'shm_records',
        'shm_writes', 'shm_flushes', 'shm_cont', 'shm_cycles', 'sms_nreq',
        'sms_nobj', 'sms_nbytes', 'sms_balloc', 'sms_bfree', 'backend_req',
        'n_vcl', 'n_vcl_avail', 'n_vcl_discard', 'n_ban', 'n_ban_add',
        'n_ban_retire', 'n_ban_obj_test', 'n_ban_re_test', 'n_ban_dups',
        'hcb_nolock', 'hcb_lock', 'hcb_insert', 'accept_fail',
        'client_drop_late', 'uptime', 'dir_dns_lookups', 'dir_dns_failed',
        'dir_dns_hit', 'dir_dns_cache_full'
    ])

    _KEYS_v4 = frozenset([
        'MAIN.uptime', 'MAIN.sess_conn', 'MAIN.sess_drop', 'MAIN.sess_fail',
        'MAIN.sess_pipe_overflow', 'MAIN.client_req_400', 'MAIN.client_req_411',
        'MAIN.client_req_413', 'MAIN.client_req_417', 'MAIN.client_req',
        'MAIN.cache_hit', 'MAIN.cache_hitpass', 'MAIN.cache_miss',
        'MAIN.backend_conn', 'MAIN.backend_unhealthy', 'MAIN.backend_busy',
        'MAIN.backend_fail', 'MAIN.backend_reuse', 'MAIN.backend_toolate',
        'MAIN.backend_recycle', 'MAIN.backend_retry', 'MAIN.fetch_head',
        'MAIN.fetch_length', 'MAIN.fetch_chunked', 'MAIN.fetch_eof',
        'MAIN.fetch_bad', 'MAIN.fetch_close', 'MAIN.fetch_oldhttp',
        'MAIN.fetch_zero', 'MAIN.fetch_1xx', 'MAIN.fetch_204', 'MAIN.fetch_304',
        'MAIN.fetch_failed', 'MAIN.pools', 'MAIN.threads',
        'MAIN.threads_limited',
        'MAIN.threads_created', 'MAIN.threads_destroyed', 'MAIN.threads_failed',
        'MAIN.thread_queue_len', 'MAIN.busy_sleep', 'MAIN.busy_wakeup',
        'MAIN.sess_queued', 'MAIN.sess_dropped', 'MAIN.n_object',
        'MAIN.n_vampireobject', 'MAIN.n_objectcore', 'MAIN.n_objecthead',
        'MAIN.n_waitinglist', 'MAIN.n_backend', 'MAIN.n_expired',
        'MAIN.n_lru_nuked', 'MAIN.n_lru_moved', 'MAIN.losthdr', 'MAIN.s_sess',
        'MAIN.s_req', 'MAIN.s_pipe', 'MAIN.s_pass', 'MAIN.s_fetch',
        'MAIN.s_synth',
        'MAIN.s_req_hdrbytes', 'MAIN.s_req_bodybytes', 'MAIN.s_resp_hdrbytes',
        'MAIN.s_resp_bodybytes', 'MAIN.s_pipe_hdrbytes', 'MAIN.s_pipe_in',
        'MAIN.s_pipe_out', 'MAIN.sess_closed', 'MAIN.sess_pipeline',
        'MAIN.sess_readahead', 'MAIN.sess_herd', 'MAIN.shm_records',
        'MAIN.shm_writes', 'MAIN.shm_flushes', 'MAIN.shm_cont',
        'MAIN.shm_cycles',
        'MAIN.sms_nreq', 'MAIN.sms_nobj', 'MAIN.sms_nbytes', 'MAIN.sms_balloc',
        'MAIN.sms_bfree', 'MAIN.backend_req', 'MAIN.n_vcl', 'MAIN.n_vcl_avail',
        'MAIN.n_vcl_discard', 'MAIN.bans', 'MAIN.bans_completed',
        'MAIN.bans_obj',
        'MAIN.bans_req', 'MAIN.bans_added', 'MAIN.bans_deleted',
        'MAIN.bans_tested',
        'MAIN.bans_obj_killed', 'MAIN.bans_lurker_tested',
        'MAIN.bans_tests_tested',
        'MAIN.bans_lurker_tests_tested', 'MAIN.bans_lurker_obj_killed',
        'MAIN.bans_dups', 'MAIN.bans_lurker_contention',
        'MAIN.bans_persisted_bytes',
        'MAIN.bans_persisted_fragmentation', 'MAIN.exp_mailed',
        'MAIN.exp_received',
        'MAIN.hcb_nolock', 'MAIN.hcb_lock', 'MAIN.hcb_insert',
        'MAIN.esi_errors',
        'MAIN.esi_warnings', 'MAIN.vmods', 'MAIN.n_gzip', 'MAIN.n_gunzip',
        'MAIN.vsm_free', 'MAIN.vsm_used', 'MAIN.vsm_cooling',
        'MAIN.vsm_overflow',
        'MAIN.vsm_overflowed', 'MGT.uptime', 'MGT.child_start',
        'MGT.child_exit',
        'MGT.child_stop', 'MGT.child_died', 'MGT.child_dump', 'MGT.child_panic',
        'LCK.sms.creat', 'LCK.sms.destroy', 'LCK.sms.locks', 'LCK.smp.creat',
        'LCK.smp.destroy', 'LCK.smp.locks', 'LCK.sma.creat', 'LCK.sma.destroy',
        'LCK.sma.locks', 'LCK.smf.creat', 'LCK.smf.destroy', 'LCK.smf.locks',
        'LCK.hsl.creat', 'LCK.hsl.destroy', 'LCK.hsl.locks', 'LCK.hcb.creat',
        'LCK.hcb.destroy', 'LCK.hcb.locks', 'LCK.hcl.creat', 'LCK.hcl.destroy',
        'LCK.hcl.locks', 'LCK.vcl.creat', 'LCK.vcl.destroy', 'LCK.vcl.locks',
        'LCK.sessmem.creat', 'LCK.sessmem.destroy', 'LCK.sessmem.locks',
        'LCK.sess.creat', 'LCK.sess.destroy', 'LCK.sess.locks',
        'LCK.wstat.creat',
        'LCK.wstat.destroy', 'LCK.wstat.locks', 'LCK.herder.creat',
        'LCK.herder.destroy', 'LCK.herder.locks', 'LCK.wq.creat',
        'LCK.wq.destroy',
        'LCK.wq.locks', 'LCK.objhdr.creat', 'LCK.objhdr.destroy',
        'LCK.objhdr.locks',
        'LCK.exp.creat', 'LCK.exp.destroy', 'LCK.exp.locks', 'LCK.lru.creat',
        'LCK.lru.destroy', 'LCK.lru.locks', 'LCK.cli.creat', 'LCK.cli.destroy',
        'LCK.cli.locks', 'LCK.ban.creat', 'LCK.ban.destroy', 'LCK.ban.locks',
        'LCK.vbp.creat', 'LCK.vbp.destroy', 'LCK.vbp.locks',
        'LCK.backend.creat',
        'LCK.backend.destroy', 'LCK.backend.locks', 'LCK.vcapace.creat',
        'LCK.vcapace.destroy', 'LCK.vcapace.locks', 'LCK.nbusyobj.creat',
        'LCK.nbusyobj.destroy', 'LCK.nbusyobj.locks', 'LCK.busyobj.creat',
        'LCK.busyobj.destroy', 'LCK.busyobj.locks', 'LCK.mempool.creat',
        'LCK.mempool.destroy', 'LCK.mempool.locks', 'LCK.vxid.creat',
        'LCK.vxid.destroy', 'LCK.vxid.locks', 'LCK.pipestat.creat',
        'LCK.pipestat.destroy', 'LCK.pipestat.locks'
    ])

    def get_default_config_help(self):
        config_help = super(VarnishCollector, self).get_default_config_help()
        config_help.update({
            'bin':         'The path to the smartctl binary',
            'use_sudo':    'Use sudo?',
            'sudo_cmd':    'Path to sudo',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(VarnishCollector, self).get_default_config()
        config.update({
            'path':             'varnish',
            'bin':             '/usr/bin/varnishstat',
            'use_sudo':         False,
            'sudo_cmd':         '/usr/bin/sudo',
        })
        return config

    def collect(self):
        data = {}
        output = self.poll()

        matches = self._RE.findall(output)
        # No matches at all, bail out
        if not matches:
            return

        # Check first line to see if it begins with MAIN.,
        # If so, this is varnish 4.0 stats
        if matches[0][0].startswith('MAIN.'):
            keys = self._KEYS_v4
        else:
            keys = self._KEYS_v3

        for line in matches:
            if line[0] in keys:
                data[line[0]] = line[1]

        for key in data:
            self.publish(key, int(data[key]))

    def poll(self):
        try:
            command = [self.config['bin'], '-1']

            if str_to_bool(self.config['use_sudo']):
                command.insert(0, self.config['sudo_cmd'])

            output = subprocess.Popen(command,
                                      stdout=subprocess.PIPE).communicate()[0]
        except OSError:
            output = ""

        return output

########NEW FILE########
__FILENAME__ = testvmsdoms
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from vmsdoms import VMSDomsCollector


###############################################################################

class TestVMSDomsCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('VMSDomsCollector', {
        })
        self.collector = VMSDomsCollector(config, None)

    def test_import(self):
        self.assertTrue(VMSDomsCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = vmsdoms
# coding=utf-8

"""
Uses the vms suite to query per instance memory metrics, for VMS-enabled
instances

#### Dependencies

 * vms

"""

# Note, the DIAMOUND_USER has to be added to the VMS_GROUP specified
# in /etc/sysconfig/vms. VMS_GROUP may not be defined in which case it
# defaults to the login group of VMS_USER. If you are running diamond
# as root, no worries. In most other cases:
#     usermod -G kvm diamond
# should suffice

import diamond.collector
try:
    import vms
    vms  # Pyflakes
except ImportError:
    vms = None


class VMSDomsCollector(diamond.collector.Collector):
    PLUGIN_STATS = {
        'nominal': ('pages', 4096),
        'current': ('memory.current', 4096),
        'clean': ('memory.clean', 4096),
        'dirty': ('memory.dirty', 4096),
        'limit': ('memory.limit', 4096),
        'target': ('memory.target', 4096),
        'evicted': ('eviction.dropped', 4096),
        'pagedout': ('eviction.pagedout', 4096),
        'pagedin': ('eviction.pagedin', 4096),
    }

    def get_default_config_help(self):
        config_help = super(VMSDomsCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(VMSDomsCollector, self).get_default_config()
        config.update({
            'path':     'vms'
        })
        return config

    def collect(self):
        if vms is None:
            self.log.error('Unable to import vms')
            return {}

        vms.virt.init()
        hypervisor = vms.virt.AUTO.Hypervisor()

        # Get list of domains and iterate.
        domains = hypervisor.domain_list()
        vms_domains = []
        count = 0

        # Pre filter VMS domains.
        for d in domains:
            # Skip non-VMS domains.
            if not vms.control.exists(d):
                continue

            # Grab a control connection.
            dom = hypervisor.domain_lookup(d)
            if dom is None:
                continue
            ctrl = dom._wait_for_control(wait=False)
            if ctrl is None:
                continue

            try:
                # Skip ghost domains.
                if ctrl.get('gd.isghost') == '1':
                    continue
            except vms.control.ControlException:
                continue

            vms_domains.append((dom, ctrl))
            count += 1

        # Add the number of domains.
        self.publish('domains', count)

        # For each stat,
        for stat in self.PLUGIN_STATS:
            key = self.PLUGIN_STATS[stat][0]
            scale = self.PLUGIN_STATS[stat][1]
            total = 0

            # For each domain,
            for dom, ctrl in vms_domains:
                try:
                    # Get value and scale.
                    value = long(ctrl.get(key)) * scale
                except vms.control.ControlException:
                    continue

                # Dispatch.
                self.publish(stat, value, instance=dom.name())

                # Add to total.
                total = total + value

            # Dispatch total value.
            self.publish(stat, total)

########NEW FILE########
__FILENAME__ = testvmsfs
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from vmsfs import VMSFSCollector


###############################################################################

class TestVMSFSCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('VMSFSCollector', {
        })
        self.collector = VMSFSCollector(config, None)

    def test_import(self):
        self.assertTrue(VMSFSCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = vmsfs
# coding=utf-8

"""
Uses /sys/fs/vmsfs to collect host-global data on VMS memory usage

#### Dependencies

 * /sys/fs/vmsfs <- vmsfs package, vmsfs mounted

"""

import diamond.collector
import os


class VMSFSCollector(diamond.collector.Collector):

    SYSFS = '/sys/fs/vmsfs'

    VMSFS_STATS = {
        'resident': ('cur_resident', 4096),
        'allocated': ('cur_allocated', 4096)
    }

    def vmsfs_stats_read(self, filename):
        stats = {}

        # Open vmsfs sys info.
        stats_fd = None
        try:
            stats_fd = open(filename)

            for line in stats_fd:
                tokens = line.split()
                stats[tokens[0][0:-1]] = long(tokens[1])
        except:
            if stats_fd:
                stats_fd.close()

        return stats

    def vmsfs_stats_dispatch(self, filename, prefix=''):
        stats = self.vmsfs_stats_read(filename)
        for stat in self.VMSFS_STATS:
            name = self.VMSFS_STATS[stat][0]
            scale = self.VMSFS_STATS[stat][1]
            if name in stats:
                self.publish(prefix + name, stats[name] * scale)

    def get_default_config_help(self):
        config_help = super(VMSFSCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(VMSFSCollector, self).get_default_config()
        config.update({
            'path':     'vmsfs'
        })
        return config

    def collect(self):
        if not os.access(self.SYSFS, os.R_OK | os.X_OK):
            return None

        # Dispatch total stats.
        self.vmsfs_stats_dispatch(os.path.join(self.SYSFS, 'stats'))

        # Dispatch per-generation stats.
        # NOTE: We do not currently report the per-generation statistics to
        # diamond. This is because we do not have a good strategy for
        # aggregating generation data and exposing it in a sensible way. There
        # are three strategies:
        #  1) Collect everything at the host level.
        #     The problem here is that the number of metrics will explode for
        #     that individual host (and keep growing).
        #  2) Collect at the top-level (one virtual host per generation).
        #     Then the problem is finding the generation through UI tools, etc.
        #  3) Figure out some way to put the stats in each instance associated
        #     with that generation.
        # We favor (2) currently, but there's not much value in implementing it
        # until it can be exposed to the user.
        if False:
            TO_IGNORE = ('stats', 'version',
                         '00000000-0000-0000-0000-000000000000')
            files = os.listdir(self.SYSFS)
            for f in files:
                if f not in TO_IGNORE:
                    self.vmsfs_stats_dispatch('/sys/fs/vmsfs/' + f,
                                              prefix=('%s.' % f))

########NEW FILE########
__FILENAME__ = testvmstat
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

from diamond.collector import Collector
from vmstat import VMStatCollector

###############################################################################


class TestVMStatCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('VMStatCollector', {
            'interval': 10
        })

        self.collector = VMStatCollector(config, None)

    def test_import(self):
        self.assertTrue(VMStatCollector)

    @patch('__builtin__.open')
    @patch('os.access', Mock(return_value=True))
    @patch.object(Collector, 'publish')
    def test_should_open_proc_vmstat(self, publish_mock, open_mock):
        open_mock.return_value = StringIO('')
        self.collector.collect()
        open_mock.assert_called_once_with('/proc/vmstat')

    @patch.object(Collector, 'publish')
    def test_should_work_with_real_data(self, publish_mock):
        VMStatCollector.PROC = self.getFixturePath('proc_vmstat_1')
        self.collector.collect()

        self.assertPublishedMany(publish_mock, {})

        VMStatCollector.PROC = self.getFixturePath('proc_vmstat_2')
        self.collector.collect()

        metrics = {
            'pgpgin': 0.0,
            'pgpgout': 9.2,
            'pswpin': 0.0,
            'pswpout': 0.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)


###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = vmstat
# coding=utf-8

"""
Uses /proc/vmstat to collect data on virtual memory manager

#### Dependencies

 * /proc/vmstat

"""

import diamond.collector
import os
import re


class VMStatCollector(diamond.collector.Collector):

    PROC = '/proc/vmstat'
    MAX_VALUES = {
        'pgpgin': diamond.collector.MAX_COUNTER,
        'pgpgout': diamond.collector.MAX_COUNTER,
        'pswpin': diamond.collector.MAX_COUNTER,
        'pswpout': diamond.collector.MAX_COUNTER,
    }

    def get_default_config_help(self):
        config_help = super(VMStatCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(VMStatCollector, self).get_default_config()
        config.update({
            'enabled':  'True',
            'path':     'vmstat'
        })
        return config

    def collect(self):
        if not os.access(self.PROC, os.R_OK):
            return None

        results = {}
        # open file
        file = open(self.PROC)
        exp = '^(pgpgin|pgpgout|pswpin|pswpout)\s(\d+)'
        reg = re.compile(exp)
        # Build regex
        for line in file:
            match = reg.match(line)
            if match:
                name = match.group(1)
                value = match.group(2)
                results[name] = self.derivative(name,
                                                int(value),
                                                self.MAX_VALUES[name])

        # Close file
        file.close()

        for key, value in results.items():
            self.publish(key, value, 2)

########NEW FILE########
__FILENAME__ = testwebsitemonitor
#!/usr/bin/python
# coding=utf-8
########################################################################

from test import CollectorTestCase
from test import get_collector_config
from mock import patch

from diamond.collector import Collector

from websitemonitor import WebsiteMonitorCollector

########################################################################


class MockResponse(object):

    def __init__(self, resp_data, code=200):
        self.resp_data = resp_data
        self.code = code

    def read(self):
        return self.resp_data

    def getcode(self):
        return self.code


class TestWebsiteCollector(CollectorTestCase):
    def setUp(self, config=None):
        if config is None:
            config = get_collector_config('WebsiteCollector', {
                'url': ''
                })
        else:
            config = get_collector_config('WebsiteCollector', config)

        self.collector = WebsiteMonitorCollector(config, None)

        self.patcher = patch('urllib2.urlopen')
        self.urlopen_mock = self.patcher.start()

    def test_import(self):
        self.assertTrue(WebsiteMonitorCollector)

    @patch.object(Collector, 'publish')
    def test_websitemonitorcollector_with_data(self, publish_mock):

        self.collector.collect()

        self.urlopen_mock.return_value = MockResponse(200)

        metrics = {}

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])

        self.assertPublishedMany([publish_mock], metrics)

    @patch.object(Collector, 'publish')
    def test_websitemonitorcollector(self, publish_mock):
        self.setUp()

        self.collector.collect()

        self.assertPublishedMany(publish_mock, {
        })

    def tearDown(self):
        self.patcher.stop()

########NEW FILE########
__FILENAME__ = websitemonitor
# coding=utf-8
"""
Gather HTTP Response code and Duration of HTTP request

#### Dependencies
  * urllib2

"""

import urllib2
import time
from datetime import datetime
import diamond.collector


class WebsiteMonitorCollector(diamond.collector.Collector):
    """
    Gather HTTP response code and Duration of HTTP request
    """
    def get_default_config_help(self):
        config_help = super(WebsiteMonitorCollector,
                            self).get_default_config_help()
        config_help.update({
            'URL': "FQDN of HTTP endpoint to test",

        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        default_config = super(WebsiteMonitorCollector,
                               self).get_default_config()
        default_config['URL'] = ''
        default_config['path'] = 'websitemonitor'
        return default_config

    def collect(self):
        req = urllib2.Request('%s' % (self.config['URL']))

        try:
            # time in seconds since epoch as a floating number
            start_time = time.time()
            # human-readable time e.g November 25, 2013 18:15:56
            st = datetime.fromtimestamp(start_time
                                        ).strftime('%B %d, %Y %H:%M:%S')
            self.log.debug('Start time: %s' % (st))

            resp = urllib2.urlopen(req)
            # time in seconds since epoch as a floating number
            end_time = time.time()
            # human-readable end time e.eg. November 25, 2013 18:15:56
            et = datetime.fromtimestamp(end_time).strftime('%B %d, %Y %H:%M%S')
            self.log.debug('End time: %s' % (et))
            # Response time in milliseconds
            rt = int(format((end_time - start_time) * 1000, '.0f'))
            # Publish metrics
            self.publish('response_time.%s' % (resp.code), rt,
                         metric_type='COUNTER')
        # urllib2 will puke on non HTTP 200/OK URLs
        except urllib2.URLError, e:
            if e.code != 200:
                # time in seconds since epoch as a floating number
                end_time = time.time()
                # Response time in milliseconds
                rt = int(format((end_time - start_time) * 1000, '.0f'))
                # Publish metrics -- this is recording a failure, rt will
                # likely be 0 but does capture HTTP Status Code
                self.publish('response_time.%s' % (e.code), rt,
                             metric_type='COUNTER')

        except IOError, e:
            self.log.error('Unable to open %s' % (self.config['URL']))

        except Exception, e:
            self.log.error("Unknown error opening url: %s", e)

########NEW FILE########
__FILENAME__ = testxen
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from xen_collector import XENCollector


###############################################################################


def run_only_if_libvirt_is_available(func):
    try:
        import libvirt
        libvirt  # workaround for pyflakes issue #13
    except ImportError:
        libvirt = None
    pred = lambda: libvirt is not None
    return run_only(func, pred)


class TestXENCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('XENCollector', {
        })
        self.collector = XENCollector(config, None)

    def test_import(self):
        self.assertTrue(XENCollector)

    @run_only_if_libvirt_is_available
    @patch('os.statvfs')
    @patch('libvirt.openReadOnly')
    @patch.object(Collector, 'publish')
    def test_centos6(self, publish_mock, libvirt_mock, os_mock):

        class info:
            def __init__(self, id):
                self.id = id

            def info(self):
                if self.id == 0:
                    return [1, 49420888L, 49420888L, 8, 911232000000000L]
                if self.id == 1:
                    return [1, 2097152L,  2097152L,  2, 310676150000000L]
                if self.id == 2:
                    return [1, 2097152L,  2097152L,  2, 100375300000000L]
                if self.id == 3:
                    return [1, 10485760L, 10485760L, 2, 335312040000000L]
                if self.id == 4:
                    return [1, 10485760L, 10485760L, 2, 351313480000000L]

        libvirt_m = Mock()
        libvirt_m.getInfo.return_value = ['x86_64', 48262, 8, 1200, 2, 1, 4, 1]
        libvirt_m.listDomainsID.return_value = [0, 2, 1, 4, 3]

        def lookupByIdMock(id):
            lookup = info(id)
            return lookup

        libvirt_m.lookupByID = lookupByIdMock

        libvirt_mock.return_value = libvirt_m

        statsvfs_mock = Mock()
        statsvfs_mock.f_bavail = 74492145
        statsvfs_mock.f_frsize = 4096

        os_mock.return_value = statsvfs_mock

        self.collector.collect()

        metrics = {
            'TotalCores': 8.000000,
            'InstalledMem': 48262.000000,
            'MemAllocated': 24576.000000,
            'MemFree': 23686.000000,
            'DiskFree': 297968580.000000,
            'FreeCores': 0.000000,
            'AllocatedCores': 8.000000,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)


###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = xen_collector
# coding=utf-8

"""
The XENCollector grabs usage/allocation metrics using libvirt

#### Dependencies
 * python-libvirt

"""

from diamond.collector import Collector

import os
try:
    import libvirt
    libvirt  # workaround for pyflakes issue #13
except ImportError:
    libvirt = None


class XENCollector(Collector):

    def get_default_config_help(self):
        config_help = super(XENCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(XENCollector, self).get_default_config()
        config.update({
            'path':     'xen'
        })
        return config

    def collect(self):
        """
        Collect libvirt data
        """
        if libvirt is None:
            self.log.error('Unable to import either libvirt')
            return {}
        #Open a restricted (non-root) connection to the hypervisor
        conn = libvirt.openReadOnly(None)
        #Get hardware info
        conninfo = conn.getInfo()
        #Initialize variables
        memallocated = 0
        coresallocated = 0
        totalcores = 0
        results = {}
        domIds = conn.listDomainsID()
        if 0 in domIds:
        #Total cores
            domU = conn.lookupByID(0)
            totalcores = domU.info()[3]
        #Free Space
        s = os.statvfs('/')
        freeSpace = (s.f_bavail * s.f_frsize) / 1024
        #Calculate allocated memory and cores
        for i in domIds:
            # Ignore 0
            if i == 0:
                continue
            domU = conn.lookupByID(i)
            dominfo = domU.info()
            memallocated += dominfo[2]
            if i > 0:
                coresallocated += dominfo[3]
        results = {
            'InstalledMem': conninfo[1],
            'MemAllocated': memallocated / 1024,
            'MemFree': conninfo[1] - (memallocated / 1024),
            'AllocatedCores': coresallocated,
            'DiskFree': freeSpace,
            'TotalCores': totalcores,
            'FreeCores': (totalcores - coresallocated)
        }
        for k in results.keys():
            self.publish(k, results[k], 0)

########NEW FILE########
__FILENAME__ = testzookeeper
#!/usr/bin/python
# coding=utf-8
###############################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import Mock
from mock import patch

from diamond.collector import Collector
from zookeeper import ZookeeperCollector


###############################################################################

class TestZookeeperCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('ZookeeperCollector', {
        })
        self.collector = ZookeeperCollector(config, None)

    def test_import(self):
        self.assertTrue(ZookeeperCollector)

###############################################################################
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = zookeeper
# coding=utf-8

"""
Collect zookeeper stats. ( Modified from memcached collector )

#### Dependencies

 * subprocess
 * Zookeeper 'mntr' command (zookeeper version => 3.4.0)

#### Example Configuration

ZookeeperCollector.conf

```
    enabled = True
    hosts = localhost:2181, app-1@localhost:2181, app-2@localhost:2181, etc
```

TO use a unix socket, set a host string like this

```
    hosts = /path/to/blah.sock, app-1@/path/to/bleh.sock,
```
"""

import diamond.collector
import socket
import re


class ZookeeperCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(ZookeeperCollector, self).get_default_config_help()
        config_help.update({
            'publish': "Which rows of 'status' you would like to publish."
            + " Telnet host port' and type stats and hit enter to see the list"
            + " of possibilities. Leave unset to publish all.",
            'hosts': "List of hosts, and ports to collect. Set an alias by "
            + " prefixing the host:port with alias@",
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(ZookeeperCollector, self).get_default_config()
        config.update({
            'path':     'zookeeper',

            # Which rows of 'status' you would like to publish.
            # 'telnet host port' and type mntr and hit enter to see the list of
            # possibilities.
            # Leave unset to publish all
            #'publish': ''

            # Connection settings
            'hosts': ['localhost:2181']
        })
        return config

    def get_raw_stats(self, host, port):
        data = ''
        # connect
        try:
            if port is None:
                sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                sock.connect(host)
            else:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.connect((host, int(port)))
            # request stats
            sock.send('mntr\n')
            # something big enough to get whatever is sent back
            data = sock.recv(4096)
        except socket.error:
            self.log.exception('Failed to get stats from %s:%s',
                               host, port)
        return data

    def get_stats(self, host, port):
        # stuff that's always ignored, aren't 'stats'
        ignored = ('zk_version', 'zk_server_state')
        pid = None

        stats = {}
        data = self.get_raw_stats(host, port)

        # parse stats
        for line in data.splitlines():

            pieces = line.split()

            if pieces[0] in ignored:
                continue
            stats[pieces[0]] = pieces[1]

        # get max connection limit
        self.log.debug('pid %s', pid)
        try:
            cmdline = "/proc/%s/cmdline" % pid
            f = open(cmdline, 'r')
            m = re.search("-c\x00(\d+)", f.readline())
            if m is not None:
                self.log.debug('limit connections %s', m.group(1))
                stats['limit_maxconn'] = m.group(1)
            f.close()
        except:
            self.log.debug("Cannot parse command line options for zookeeper")

        return stats

    def collect(self):
        hosts = self.config.get('hosts')

        # Convert a string config value to be an array
        if isinstance(hosts, basestring):
            hosts = [hosts]

        for host in hosts:
            matches = re.search('((.+)\@)?([^:]+)(:(\d+))?', host)
            alias = matches.group(2)
            hostname = matches.group(3)
            port = matches.group(5)

            stats = self.get_stats(hostname, port)

            # figure out what we're configured to get, defaulting to everything
            desired = self.config.get('publish', stats.keys())

            # for everything we want
            for stat in desired:
                if stat in stats:

                    # we have it
                    if alias is not None:
                        self.publish(alias + "." + stat, stats[stat])
                    else:
                        self.publish(stat, stats[stat])
                else:

                    # we don't, must be somehting configured in publish so we
                    # should log an error about it
                    self.log.error("No such key '%s' available, issue 'stats' "
                                   "for a full list", stat)

########NEW FILE########
__FILENAME__ = collector
# coding=utf-8

"""
The Collector class is a base class for all metric collectors.
"""

import os
import socket
import platform
import logging
import configobj
import traceback
import time

from diamond.metric import Metric
from error import DiamondException

# Detect the architecture of the system and set the counters for MAX_VALUES
# appropriately. Otherwise, rolling over counters will cause incorrect or
# negative values.

if platform.architecture()[0] == '64bit':
    MAX_COUNTER = (2 ** 64) - 1
else:
    MAX_COUNTER = (2 ** 32) - 1


def get_hostname(config, method=None):
    """
    Returns a hostname as configured by the user
    """
    if 'hostname' in config:
        return config['hostname']

    if method is None:
        if 'hostname_method' in config:
            method = config['hostname_method']
        else:
            method = 'smart'

    # case insensitive method
    method = method.lower()

    if method in get_hostname.cached_results:
        return get_hostname.cached_results[method]

    if method == 'smart':
        hostname = get_hostname(config, 'fqdn_short')
        if hostname != 'localhost':
            get_hostname.cached_results[method] = hostname
            return hostname
        hostname = get_hostname(config, 'hostname_short')
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'fqdn_short':
        hostname = socket.getfqdn().split('.')[0]
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'fqdn':
        hostname = socket.getfqdn().replace('.', '_')
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'fqdn_rev':
        hostname = socket.getfqdn().split('.')
        hostname.reverse()
        hostname = '.'.join(hostname)
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'uname_short':
        hostname = os.uname()[1].split('.')[0]
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'uname_rev':
        hostname = os.uname()[1].split('.')
        hostname.reverse()
        hostname = '.'.join(hostname)
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'hostname':
        hostname = socket.gethostname()
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'hostname_short':
        hostname = socket.gethostname().split('.')[0]
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'hostname_rev':
        hostname = socket.gethostname().split('.')
        hostname.reverse()
        hostname = '.'.join(hostname)
        get_hostname.cached_results[method] = hostname
        return hostname

    if method == 'none':
        get_hostname.cached_results[method] = None
        return None

    raise NotImplementedError(config['hostname_method'])

get_hostname.cached_results = {}


def str_to_bool(value):
    """
    Converts string truthy/falsey strings to a bool
    Empty strings are false
    """
    if isinstance(value, basestring):
        value = value.strip().lower()
        if value in ['true', 't', 'yes', 'y']:
            return True
        elif value in ['false', 'f', 'no', 'n', '']:
            return False
        else:
            raise NotImplementedError("Unknown bool %s" % value)

    return value


class Collector(object):
    """
    The Collector class is a base class for all metric collectors.
    """

    def __init__(self, config, handlers):
        """
        Create a new instance of the Collector class
        """
        # Initialize Logger
        self.log = logging.getLogger('diamond')
        # Initialize Members
        self.name = self.__class__.__name__
        self.handlers = handlers
        self.last_values = {}

        # Get Collector class
        cls = self.__class__

        # Initialize config
        self.config = configobj.ConfigObj()

        # Check if default config is defined
        if self.get_default_config() is not None:
            # Merge default config
            self.config.merge(self.get_default_config())

        # Merge default Collector config
        self.config.merge(config['collectors']['default'])

        # Check if Collector config section exists
        if cls.__name__ in config['collectors']:
            # Merge Collector config section
            self.config.merge(config['collectors'][cls.__name__])

        # Check for config file in config directory
        configfile = os.path.join(config['server']['collectors_config_path'],
                                  cls.__name__) + '.conf'
        if os.path.exists(configfile):
            # Merge Collector config file
            self.config.merge(configobj.ConfigObj(configfile))

        # Handle some config file changes transparently
        if isinstance(self.config['byte_unit'], basestring):
            self.config['byte_unit'] = self.config['byte_unit'].split()

        self.config['enabled'] = str_to_bool(self.config['enabled'])

        self.config['measure_collector_time'] = str_to_bool(
            self.config['measure_collector_time'])

        self.collect_running = False

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this collector
        """
        return {
            'enabled': 'Enable collecting these metrics',
            'byte_unit': 'Default numeric output(s)',
            'measure_collector_time': 'Collect the collector run time in ms',
        }

    def get_default_config(self):
        """
        Return the default config for the collector
        """
        return {
            ### Defaults options for all Collectors

            # Uncomment and set to hardcode a hostname for the collector path
            # Keep in mind, periods are seperators in graphite
            # 'hostname': 'my_custom_hostname',

            # If you perfer to just use a different way of calculating the
            # hostname
            # Uncomment and set this to one of these values:
            # fqdn_short  = Default. Similar to hostname -s
            # fqdn        = hostname output
            # fqdn_rev    = hostname in reverse (com.example.www)
            # uname_short = Similar to uname -n, but only the first part
            # uname_rev   = uname -r in reverse (com.example.www)
            # 'hostname_method': 'fqdn_short',

            # All collectors are disabled by default
            'enabled': False,

            # Path Prefix
            'path_prefix': 'servers',

            # Path Prefix for Virtual Machine metrics
            'instance_prefix': 'instances',

            # Path Suffix
            'path_suffix': '',

            # Default splay time (seconds)
            'splay': 1,

            # Default Poll Interval (seconds)
            'interval': 300,

            # Default Event TTL (interval multiplier)
            'ttl_multiplier': 2,

            # Default collector threading model
            'method': 'Sequential',

            # Default numeric output
            'byte_unit': 'byte',

            # Collect the collector run time in ms
            'measure_collector_time': False,
        }

    def get_stats_for_upload(self, config=None):
        if config is None:
            config = self.config

        stats = {}

        if 'enabled' in config:
            stats['enabled'] = config['enabled']
        else:
            stats['enabled'] = False

        if 'interval' in config:
            stats['interval'] = config['interval']

        return stats

    def get_schedule(self):
        """
        Return schedule for the collector
        """
        # Return a dict of tuples containing (collector function,
        # collector function args, splay, interval)
        return {self.__class__.__name__: (self._run,
                                          None,
                                          int(self.config['splay']),
                                          int(self.config['interval']))}

    def get_metric_path(self, name, instance=None):
        """
        Get metric path.
        Instance indicates that this is a metric for a
            virtual machine and should have a different
            root prefix.
        """
        if 'path' in self.config:
            path = self.config['path']
        else:
            path = self.__class__.__name__

        if instance is not None:
            if 'instance_prefix' in self.config:
                prefix = self.config['instance_prefix']
            else:
                prefix = 'instances'
            if path == '.':
                return '.'.join([prefix, instance, name])
            else:
                return '.'.join([prefix, instance, path, name])

        if 'path_prefix' in self.config:
            prefix = self.config['path_prefix']
        else:
            prefix = 'systems'

        if 'path_suffix' in self.config:
            suffix = self.config['path_suffix']
        else:
            suffix = None

        hostname = get_hostname(self.config)
        if hostname is not None:
            if prefix:
                prefix = ".".join((prefix, hostname))
            else:
                prefix = hostname

        # if there is a suffix, add after the hostname
        if suffix:
            prefix = '.'.join((prefix, suffix))

        if path == '.':
            return '.'.join([prefix, name])
        else:
            return '.'.join([prefix, path, name])

    def get_hostname(self):
        return get_hostname(self.config)

    def collect(self):
        """
        Default collector method
        """
        raise NotImplementedError()

    def publish(self, name, value, raw_value=None, precision=0,
                metric_type='GAUGE', instance=None):
        """
        Publish a metric with the given name
        """
        # Get metric Path
        path = self.get_metric_path(name, instance=instance)

        # Get metric TTL
        ttl = float(self.config['interval']) * float(
            self.config['ttl_multiplier'])

        # Create Metric
        try:
            metric = Metric(path, value, raw_value=raw_value, timestamp=None,
                            precision=precision, host=self.get_hostname(),
                            metric_type=metric_type, ttl=ttl)
        except DiamondException:
            self.log.error(('Error when creating new Metric: path=%r, '
                            'value=%r'), path, value)
            raise

        # Publish Metric
        self.publish_metric(metric)

    def publish_metric(self, metric):
        """
        Publish a Metric object
        """
        # Process Metric
        for handler in self.handlers:
            handler._process(metric)

    def publish_gauge(self, name, value, precision=0, instance=None):
        return self.publish(name, value, precision=precision,
                            metric_type='GAUGE', instance=instance)

    def publish_counter(self, name, value, precision=0, max_value=0,
                        time_delta=True, interval=None, allow_negative=False,
                        instance=None):
        raw_value = value
        value = self.derivative(name, value, max_value=max_value,
                                time_delta=time_delta, interval=interval,
                                allow_negative=allow_negative,
                                instance=instance)
        return self.publish(name, value, raw_value=raw_value,
                            precision=precision, metric_type='COUNTER',
                            instance=instance)

    def derivative(self, name, new, max_value=0,
                   time_delta=True, interval=None,
                   allow_negative=False, instance=None):
        """
        Calculate the derivative of the metric.
        """
        # Format Metric Path
        path = self.get_metric_path(name, instance=instance)

        if path in self.last_values:
            old = self.last_values[path]
            # Check for rollover
            if new < old:
                old = old - max_value
            # Get Change in X (value)
            derivative_x = new - old

            # If we pass in a interval, use it rather then the configured one
            if interval is None:
                interval = int(self.config['interval'])

            # Get Change in Y (time)
            if time_delta:
                derivative_y = interval
            else:
                derivative_y = 1

            result = float(derivative_x) / float(derivative_y)
            if result < 0 and not allow_negative:
                result = 0
        else:
            result = 0

        # Store Old Value
        self.last_values[path] = new

        # Return result
        return result

    def _run(self):
        """
        Run the collector unless it's already running
        """
        if self.collect_running:
            return
        # Log
        self.log.debug("Collecting data from: %s" % self.__class__.__name__)
        try:
            try:
                start_time = time.time()
                self.collect_running = True

                # Collect Data
                self.collect()

                end_time = time.time()

                if 'measure_collector_time' in self.config:
                    if self.config['measure_collector_time']:
                        metric_name = 'collector_time_ms'
                        metric_value = int((end_time - start_time) * 1000)
                        self.publish(metric_name, metric_value)

            except Exception:
                # Log Error
                self.log.error(traceback.format_exc())
        finally:
            self.collect_running = False
            # After collector run, invoke a flush
            # method on each handler.
            for handler in self.handlers:
                handler._flush()

    def find_binary(self, binary):
        """
        Scan and return the first path to a binary that we can find
        """
        if os.path.exists(binary):
            return binary

        # Extract out the filename if we were given a full path
        binary_name = os.path.basename(binary)

        # Gather $PATH
        search_paths = os.environ['PATH'].split(':')

        # Extra paths to scan...
        default_paths = [
            '/usr/bin',
            '/bin'
            '/usr/local/bin',
            '/usr/sbin',
            '/sbin'
            '/usr/local/sbin',
        ]

        for path in default_paths:
            if path not in search_paths:
                search_paths.append(path)

        for path in search_paths:
            if os.path.isdir(path):
                filename = os.path.join(path, binary_name)
                if os.path.exists(filename):
                    return filename

        return binary

########NEW FILE########
__FILENAME__ = convertor
# coding=utf-8

import re

_RE_FIND_FIRST_CAP = re.compile('(.)([A-Z][a-z]+)')
_RE_SPAN_OF_CAPS = re.compile('([a-z0-9])([A-Z])')


def camelcase_to_underscore(name):
    return _RE_SPAN_OF_CAPS.sub(r'\1_\2',
                                _RE_FIND_FIRST_CAP.sub(r'\1_\2', name)
                                ).lower()


class binary:
    """
    Store the value in bits so we can convert between things easily
    """
    value = None

    def __init__(self, value=None, unit=None):
        self.do(value=value, unit=unit)

    @staticmethod
    def convert(value=None, oldUnit=None, newUnit=None):
        convertor = binary(value=value, unit=oldUnit)
        return convertor.get(unit=newUnit)

    def set(self, value, unit=None):
        return self.do(value=value, unit=unit)

    def get(self, unit=None):
        return self.do(unit=unit)

    def do(self, value=None, unit=None):
        if not unit:
            return self.bit(value=value)

        if unit in ['bit', 'b']:
            return self.bit(value=value)
        if unit in ['kilobit', 'kbit', 'Kibit']:
            return self.kilobit(value=value)
        if unit in ['megabit', 'Mbit', 'Mibit', 'Mbit']:
            return self.megabit(value=value)
        if unit in ['gigabit', 'Gbit', 'Gibit']:
            return self.gigabit(value=value)
        if unit in ['terabit', 'Tbit', 'Tibit']:
            return self.terabit(value=value)
        if unit in ['petabit', 'Pbit', 'Pibit']:
            return self.petabit(value=value)
        if unit in ['exabit', 'Ebit', 'Eibit']:
            return self.exabit(value=value)
        if unit in ['zettabit', 'Zbit', 'Zibit']:
            return self.zettabit(value=value)
        if unit in ['yottabit', 'Ybit', 'Yibit']:
            return self.yottabit(value=value)

        if unit in ['byte', 'B']:
            return self.byte(value=value)
        if unit in ['kilobyte', 'kB', 'KiB']:
            return self.kilobyte(value=value)
        if unit in ['megabyte', 'MB', 'MiB', 'Mbyte']:
            return self.megabyte(value=value)
        if unit in ['gigabyte', 'GB', 'GiB']:
            return self.gigabyte(value=value)
        if unit in ['terabyte', 'TB', 'TiB']:
            return self.terabyte(value=value)
        if unit in ['petabyte', 'PB', 'PiB']:
            return self.petabyte(value=value)
        if unit in ['exabyte', 'EB', 'EiB']:
            return self.exabyte(value=value)
        if unit in ['zettabyte', 'ZB', 'ZiB']:
            return self.zettabyte(value=value)
        if unit in ['yottabyte', 'YB', 'YiB']:
            return self.yottabyte(value=value)

        raise NotImplementedError("unit %s" % unit)

    def bit(self, value=None):
        if value is None:
            return self.value
        else:
            self.value = float(value)

    def convertb(self, value, source, offset=1):
        if value is None:
            return source() / pow(1024, offset)
        else:
            source(value * pow(1024, offset))

    def kilobit(self, value=None):
        return self.convertb(value, self.bit)

    def megabit(self, value=None):
        return self.convertb(value, self.bit, 2)

    def gigabit(self, value=None):
        return self.convertb(value, self.bit, 3)

    def terabit(self, value=None):
        return self.convertb(value, self.bit, 4)

    def petabit(self, value=None):
        return self.convertb(value, self.bit, 5)

    def exabit(self, value=None):
        return self.convertb(value, self.bit, 6)

    def zettabit(self, value=None):
        return self.convertb(value, self.bit, 7)

    def yottabit(self, value=None):
        return self.convertb(value, self.bit, 8)

    def byte(self, value=None):
        if value is None:
            return self.value / 8
        else:
            self.value = float(value) * 8

    def kilobyte(self, value=None):
        return self.convertb(value, self.byte)

    def megabyte(self, value=None):
        return self.convertb(value, self.byte, 2)

    def gigabyte(self, value=None):
        return self.convertb(value, self.byte, 3)

    def terabyte(self, value=None):
        return self.convertb(value, self.byte, 4)

    def petabyte(self, value=None):
        return self.convertb(value, self.byte, 5)

    def exabyte(self, value=None):
        return self.convertb(value, self.byte, 6)

    def zettabyte(self, value=None):
        return self.convertb(value, self.byte, 7)

    def yottabyte(self, value=None):
        return self.convertb(value, self.byte, 8)


class time:
    """
    Store the value in miliseconds so we can convert between things easily
    """
    value = None

    def __init__(self, value=None, unit=None):
        self.do(value=value, unit=unit)

    @staticmethod
    def convert(value=None, oldUnit=None, newUnit=None):
        convertor = time(value=value, unit=oldUnit)
        return convertor.get(unit=newUnit)

    def set(self, value, unit=None):
        return self.do(value=value, unit=unit)

    def get(self, unit=None):
        return self.do(unit=unit)

    def do(self, value=None, unit=None):
        if not unit:
            return self.millisecond(value=value)
        else:
            unit = unit.lower()

        if unit in ['millisecond', 'milliseconds', 'ms']:
            return self.millisecond(value=value)
        if unit in ['second', 'seconds', 's']:
            return self.second(value=value)

        raise NotImplementedError("unit %s" % unit)

    def millisecond(self, value=None):
        if value is None:
            return self.value
        else:
            self.value = float(value)

    def second(self, value=None):
        if value is None:
            return self.millisecond() / 1000
        else:
            self.millisecond(value * 1000)

########NEW FILE########
__FILENAME__ = error
# coding=utf-8


class DiamondException(Exception):
    def __init__(self, message):
        self.message = message

    def __str__(self):
        return self.message

    def __repr__(self):
        return self.message

########NEW FILE########
__FILENAME__ = gmetric
#!/usr/bin/env python
# coding=utf-8

# This is the MIT License
# http://www.opensource.org/licenses/mit-license.php
#
# Copyright (c) 2007,2008 Nick Galbreath
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#

#
# Version 1.0 - 21-Apr-2007
#   initial
# Version 2.0 - 16-Nov-2008
#   made class Gmetric thread safe
#   made gmetrix xdr writers _and readers_
#   Now this only works for gmond 2.X packets, not tested with 3.X
#
# Version 3.0 - 09-Jan-2011 Author: Vladimir Vuksan
#   Made it work with the Ganglia 3.1 data format


from xdrlib import Packer, Unpacker
import socket

slope_str2int = {'zero': 0,
                 'positive': 1,
                 'negative': 2,
                 'both': 3,
                 'unspecified': 4}

# could be autogenerated from previous but whatever
slope_int2str = {0: 'zero',
                 1: 'positive',
                 2: 'negative',
                 3: 'both',
                 4: 'unspecified'}


class Gmetric:
    """
    Class to send gmetric/gmond 2.X packets

    Thread safe
    """

    type = ('', 'string', 'uint16', 'int16', 'uint32', 'int32', 'float',
            'double', 'timestamp')
    protocol = ('udp', 'multicast')

    def __init__(self, host, port, protocol):
        if protocol not in self.protocol:
            raise ValueError("Protocol must be one of: " + str(self.protocol))

        self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        if protocol == 'multicast':
            self.socket.setsockopt(socket.IPPROTO_IP,
                                   socket.IP_MULTICAST_TTL, 20)
        self.hostport = (host, int(port))
        #self.socket.connect(self.hostport)

    def send(self, NAME, VAL, TYPE='', UNITS='', SLOPE='both',
             TMAX=60, DMAX=0, GROUP=""):
        if SLOPE not in slope_str2int:
            raise ValueError("Slope must be one of: " + str(self.slope.keys()))
        if TYPE not in self.type:
            raise ValueError("Type must be one of: " + str(self.type))
        if len(NAME) == 0:
            raise ValueError("Name must be non-empty")

        (meta_msg, data_msg) = gmetric_write(NAME,
                                             VAL,
                                             TYPE,
                                             UNITS,
                                             SLOPE,
                                             TMAX,
                                             DMAX,
                                             GROUP)
        # print msg

        self.socket.sendto(meta_msg, self.hostport)
        self.socket.sendto(data_msg, self.hostport)


def gmetric_write(NAME, VAL, TYPE, UNITS, SLOPE, TMAX, DMAX, GROUP):
    """
    Arguments are in all upper-case to match XML
    """
    packer = Packer()
    HOSTNAME = "test"
    SPOOF = 0
    # Meta data about a metric
    packer.pack_int(128)
    packer.pack_string(HOSTNAME)
    packer.pack_string(NAME)
    packer.pack_int(SPOOF)
    packer.pack_string(TYPE)
    packer.pack_string(NAME)
    packer.pack_string(UNITS)
    packer.pack_int(slope_str2int[SLOPE])  # map slope string to int
    packer.pack_uint(int(TMAX))
    packer.pack_uint(int(DMAX))
    # Magic number. Indicates number of entries to follow. Put in 1 for GROUP
    if GROUP == "":
        packer.pack_int(0)
    else:
        packer.pack_int(1)
        packer.pack_string("GROUP")
        packer.pack_string(GROUP)

    # Actual data sent in a separate packet
    data = Packer()
    data.pack_int(128 + 5)
    data.pack_string(HOSTNAME)
    data.pack_string(NAME)
    data.pack_int(SPOOF)
    data.pack_string("%s")
    data.pack_string(str(VAL))

    return packer.get_buffer(),  data.get_buffer()


def gmetric_read(msg):
    unpacker = Unpacker(msg)
    values = dict()
    unpacker.unpack_int()
    values['TYPE'] = unpacker.unpack_string()
    values['NAME'] = unpacker.unpack_string()
    values['VAL'] = unpacker.unpack_string()
    values['UNITS'] = unpacker.unpack_string()
    values['SLOPE'] = slope_int2str[unpacker.unpack_int()]
    values['TMAX'] = unpacker.unpack_uint()
    values['DMAX'] = unpacker.unpack_uint()
    unpacker.done()
    return values


if __name__ == '__main__':
    import optparse
    parser = optparse.OptionParser()
    parser.add_option("", "--protocol", dest="protocol", default="udp",
                      help="The gmetric internet protocol, either udp or"
                          + "multicast, default udp")
    parser.add_option("", "--host",  dest="host",  default="127.0.0.1",
                      help="GMond aggregator hostname to send data to")
    parser.add_option("", "--port",  dest="port",  default="8649",
                      help="GMond aggregator port to send data to")
    parser.add_option("", "--name",  dest="name",  default="",
                      help="The name of the metric")
    parser.add_option("", "--value", dest="value", default="",
                      help="The value of the metric")
    parser.add_option("", "--units", dest="units", default="",
                      help="The units for the value, e.g. 'kb/sec'")
    parser.add_option("", "--slope", dest="slope", default="both",
                      help="The sign of the derivative of the value over time,"
                          + "one of zero, positive, negative, both (default)")
    parser.add_option("", "--type",  dest="type",  default="",
                      help="The value data type, one of string, int8, uint8,"
                          + "int16, uint16, int32, uint32, float, double")
    parser.add_option("", "--tmax",  dest="tmax",  default="60",
                      help="The maximum time in seconds between gmetric calls,"
                          + "default 60")
    parser.add_option("", "--dmax",  dest="dmax",  default="0",
                      help="The lifetime in seconds of this metric, default=0,"
                          + "meaning unlimited")
    parser.add_option("", "--group",  dest="group",  default="",
                      help="Group metric belongs to. If not specified Ganglia"
                          + "will show it as no_group")
    (options, args) = parser.parse_args()

    g = Gmetric(options.host, options.port, options.protocol)
    g.send(options.name, options.value, options.type, options.units,
           options.slope, options.tmax, options.dmax, options.group)

########NEW FILE########
__FILENAME__ = archive
# coding=utf-8

"""
Write the collected stats to a locally stored log file. Rotate the log file
every night and remove after 7 days.
"""

from Handler import Handler
import logging
import logging.handlers


class ArchiveHandler(Handler):
    """
    Implements the Handler abstract class, archiving data to a log file
    """
    def __init__(self, config):
        """
        Create a new instance of the ArchiveHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        # Create Archive Logger
        self.archive = logging.getLogger('archive')
        self.archive.setLevel(logging.DEBUG)
        # Create Archive Log Formatter
        formatter = logging.Formatter('%(message)s')
        # Create Archive Log Handler
        handler = logging.handlers.TimedRotatingFileHandler(
            filename=self.config['log_file'],
            when='midnight',
            interval=1,
            backupCount=int(self.config['days']),
            encoding=self.config['encoding']
            )
        handler.setFormatter(formatter)
        handler.setLevel(logging.DEBUG)
        self.archive.addHandler(handler)

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(ArchiveHandler, self).get_default_config_help()

        config.update({
            'log_file': 'Path to the logfile',
            'days': 'How many days to store',
            'encoding': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(ArchiveHandler, self).get_default_config()

        config.update({
            'log_file': '',
            'days': 7,
            'encoding': None,
        })

        return config

    def process(self, metric):
        """
        Send a Metric to the Archive.
        """
        # Archive Metric
        self.archive.info(str(metric).strip())

########NEW FILE########
__FILENAME__ = cloudwatch
# coding=utf-8
"""
Output the collected values to AWS CloudWatch

Automatically adds the InstanceID Dimension

#### Dependencies

 * [boto](http://boto.readthedocs.org/en/latest/index.html)

#### Configuration

Enable this handler

 * handers = diamond.handler.cloudwatch.cloudwatchHandler

Example Config:

[[cloudwatchHandler]]
region = us-east-1

[[[LoadAvg01]]]
collector = loadavg
metric = 01
namespace = MachineLoad
name = Avg01
unit = None

[[[LoadAvg05]]]
collector = loadavg
metric = 05
namespace = MachineLoad
name = Avg05
unit = None
"""

import sys
import datetime

from Handler import Handler
from configobj import Section

try:
    import boto
    import boto.ec2.cloudwatch
    import boto.utils
except ImportError:
    boto = None


class cloudwatchHandler(Handler):
    """
      Implements the abstract Handler class
      Sending data to a AWS CloudWatch
    """

    def __init__(self, config=None):
        """
          Create a new instance of cloudwatchHandler class
        """

        # Initialize Handler
        Handler.__init__(self, config)

        if not boto:
            self.log.error(
                "CloudWatch: Boto is not installed, please install boto.")
            return

        # Initialize Data
        self.connection = None

        # Initialize Options
        self.region = self.config['region']
        instances = boto.utils.get_instance_metadata()
        if 'instance-id' not in instances:
            self.log.error('CloudWatch: Failed to load instance metadata')
            return
        self.instance_id = instances['instance-id']
        self.log.debug("Setting InstanceID: " + self.instance_id)

        self.valid_config = ('region', 'collector', 'metric', 'namespace',
                             'name', 'unit')

        self.rules = []
        for key_name, section in self.config.items():
            if section.__class__ is Section:
                keys = section.keys()
                rules = {}
                for key in keys:
                    if key not in self.valid_config:
                        self.log.warning("invalid key %s in section %s",
                                         key, section.name)
                    else:
                        rules[key] = section[key]

                self.rules.append(rules)

        # Create CloudWatch Connection
        self._bind()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(cloudwatchHandler, self).get_default_config_help()

        config.update({
            'region': '',
            'metric': '',
            'namespace': '',
            'name': '',
            'unit': '',
            'collector': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(cloudwatchHandler, self).get_default_config()

        config.update({
            'region': 'us-east-1',
            'collector': 'loadavg',
            'metric': '01',
            'namespace': 'MachineLoad',
            'name': 'Avg01',
            'unit': 'None',
        })

        return config

    def _bind(self):
        """
           Create CloudWatch Connection
        """

        self.log.debug(
            "CloudWatch: Attempting to connect to CloudWatch at Region: %s",
            self.region)
        try:
            self.connection = boto.ec2.cloudwatch.connect_to_region(self.region)
            self.log.debug(
                "CloudWatch: Succesfully Connected to CloudWatch at Region: %s",
                self.region)
        except boto.exception.EC2ResponseError:
            self.log.error('CloudWatch: CloudWatch Exception Handler: ')

    def __del__(self):
        """
          Destroy instance of the cloudWatchHandler class
        """
        try:
            self.connection = None
        except AttributeError:
            pass

    def process(self, metric):
        """
          Process a metric and send it to CloudWatch
        """
        if not boto:
            return

        collector = str(metric.getCollectorPath())
        metricname = str(metric.getMetricPath())
        timestamp = datetime.datetime.fromtimestamp(metric.timestamp)

        # Send the data as ......

        for rule in self.rules:
            self.log.debug(
                "Comparing Collector: [%s] with (%s) "
                "and Metric: [%s] with (%s)",
                str(rule['collector']),
                collector,
                str(rule['metric']),
                metricname
                )

            if (str(rule['collector']) == collector
                    and str(rule['metric']) == metricname):
                self.log.debug(
                    "CloudWatch: Attempting to publish metric: %s to %s "
                    "with value (%s) @%s",
                    rule['name'],
                    rule['namespace'],
                    str(metric.value),
                    str(metric.timestamp)
                    )
                try:
                    self.connection.put_metric_data(
                        str(rule['namespace']),
                        str(rule['name']),
                        str(metric.value),
                        timestamp, str(rule['unit']),
                        {'InstanceID': self.instance_id})
                    self.log.debug(
                        "CloudWatch: Successfully published metric: %s to"
                        " %s with value (%s)",
                        rule['name'],
                        rule['namespace'],
                        str(metric.value)
                        )
                except AttributeError, e:
                    self.log.error(
                        "CloudWatch: Failed publishing - %s ", str(e))
                except Exception:  # Rough connection re-try logic.
                    self.log.error(
                        "CloudWatch: Failed publishing - %s ",
                        str(sys.exc_info()[0]))
                    self._bind()

########NEW FILE########
__FILENAME__ = datadog
"""
[Datadog](http://www.datadoghq.com/) is a monitoring service for IT,
Operations and Development teams who write and run applications
at scale, and want to turn the massive amounts of data produced
by their apps, tools and services into actionable insight.

#### Dependencies

  * [dogapi]

#### Configuration

Enable handler

  * handlers = diamond.handler.datadog.DatadogHandler,

  * apikey = DATADOG_API_KEY

  * queue_size = [optional | 1]

"""

from Handler import Handler
import logging
import time
import re
from collections import deque

try:
    import dogapi
except ImportError:
    dogapi = None


class DatadogHandler(Handler):

    def __init__(self, config=None):
        """
        New instance of DatadogHandler class
        """

        Handler.__init__(self, config)
        logging.debug("Initialized Datadog handler.")

        if dogapi is None:
            logging.error("Failed to load dogapi module.")
            return

        self.api = dogapi.dog_http_api
        self.api.api_key = self.config.get('api_key', '')
        self.queue_size = self.config.get('queue_size', 1)
        self.queue = deque([])

    def get_default_config_help(self):
        """
        Help text
        """
        config = super(DatadogHandler, self).get_default_config_help()

        config.update({
            'api_key': '',
            'queue_size': '',
        })

        return config

    def get_default_config(self):
        """
        Return default config for the handler
        """
        config = super(DatadogHandler, self).get_default_config()

        config.update({
            'api_key': '',
            'queue_size': '',
        })

        return config

    def process(self, metric):
        """
        Process metric by sending it to datadog api
        """

        self.queue.append(metric)
        if len(self.queue) >= self.queue_size:
            self._send()

    def flush(self):
        """
        Flush metrics
        """

        self._send()

    def _send(self):
        """
        Take metrics from queue and send it to Datadog API
        """

        while len(self.queue) > 0:
            metric = self.queue.popleft()

            path = '%s.%s.%s' % (
                metric.getPathPrefix(),
                metric.getCollectorPath(),
                metric.getMetricPath()
            )

            topic, value, timestamp = str(metric).split()
            logging.debug(
                "Sending.. topic[%s], value[%s], timestamp[%s]",
                path,
                value,
                timestamp
            )

            self.api.metric(path, (timestamp, value), host=metric.host)

########NEW FILE########
__FILENAME__ = graphite
# coding=utf-8

"""
Send metrics to a [graphite](http://graphite.wikidot.com/) using the default
interface.

Graphite is an enterprise-scale monitoring tool that runs well on cheap
hardware. It was originally designed and written by Chris Davis at Orbitz in
2006 as side project that ultimately grew to be a foundational monitoring tool.
In 2008, Orbitz allowed Graphite to be released under the open source Apache
2.0 license. Since then Chris has continued to work on Graphite and has
deployed it at other companies including Sears, where it serves as a pillar of
the e-commerce monitoring system. Today many
[large companies](http://graphite.readthedocs.org/en/latest/who-is-using.html)
use it.

"""

from Handler import Handler
import socket


class GraphiteHandler(Handler):
    """
    Implements the abstract Handler class, sending data to graphite
    """

    def __init__(self, config=None):
        """
        Create a new instance of the GraphiteHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        # Initialize Data
        self.socket = None

        # Initialize Options
        self.proto = self.config['proto'].lower().strip()
        self.host = self.config['host']
        self.port = int(self.config['port'])
        self.timeout = int(self.config['timeout'])
        self.keepalive = bool(self.config['keepalive'])
        self.keepaliveinterval = int(self.config['keepaliveinterval'])
        self.batch_size = int(self.config['batch'])
        self.max_backlog_multiplier = int(
            self.config['max_backlog_multiplier'])
        self.trim_backlog_multiplier = int(
            self.config['trim_backlog_multiplier'])
        self.metrics = []

        # Connect
        self._connect()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(GraphiteHandler, self).get_default_config_help()

        config.update({
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
            'trim_backlog_multiplier': 'Trim down how many batches',
            'keepalive': 'Enable keepalives for tcp streams',
            'keepaliveinterval': 'How frequently to send keepalives',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(GraphiteHandler, self).get_default_config()

        config.update({
            'host': 'localhost',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
            'keepalive': 0,
            'keepaliveinterval': 10,
        })

        return config

    def __del__(self):
        """
        Destroy instance of the GraphiteHandler class
        """
        self._close()

    def process(self, metric):
        """
        Process a metric by sending it to graphite
        """
        # Append the data to the array as a string
        self.metrics.append(str(metric))
        if len(self.metrics) >= self.batch_size:
            self._send()

    def flush(self):
        """Flush metrics in queue"""
        self._send()

    def _send_data(self, data):
        """
        Try to send all data in buffer.
        """
        try:
            self.socket.sendall(data)
            self._reset_errors()
        except:
            self._close()
            self._throttle_error("GraphiteHandler: Socket error, "
                                 "trying reconnect.")
            self._connect()
            self.socket.sendall(data)
            self._reset_errors()

    def _send(self):
        """
        Send data to graphite. Data that can not be sent will be queued.
        """
        # Check to see if we have a valid socket. If not, try to connect.
        try:
            try:
                if self.socket is None:
                    self.log.debug("GraphiteHandler: Socket is not connected. "
                                   "Reconnecting.")
                    self._connect()
                if self.socket is None:
                    self.log.debug("GraphiteHandler: Reconnect failed.")
                else:
                    # Send data to socket
                    self._send_data(''.join(self.metrics))
                    self.metrics = []
            except Exception:
                self._close()
                self._throttle_error("GraphiteHandler: Error sending metrics.")
                raise
        finally:
            if len(self.metrics) >= (
                    self.batch_size * self.max_backlog_multiplier):
                trim_offset = (self.batch_size
                               * self.trim_backlog_multiplier * -1)
                self.log.warn('GraphiteHandler: Trimming backlog. Removing'
                              + ' oldest %d and keeping newest %d metrics',
                              len(self.metrics) - abs(trim_offset),
                              abs(trim_offset))
                self.metrics = self.metrics[trim_offset:]

    def _connect(self):
        """
        Connect to the graphite server
        """
        if (self.proto == 'udp'):
            stream = socket.SOCK_DGRAM
        else:
            stream = socket.SOCK_STREAM

        # Create socket
        self.socket = socket.socket(socket.AF_INET, stream)
        if self.socket is None:
            # Log Error
            self.log.error("GraphiteHandler: Unable to create socket.")
            # Close Socket
            self._close()
            return
        # Enable keepalives?
        if self.proto != 'udp' and self.keepalive:
            self.log.error("GraphiteHandler: Setting socket keepalives...")
            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE,
                                   self.keepaliveinterval)
            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL,
                                   self.keepaliveinterval)
            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 3)
        # Set socket timeout
        self.socket.settimeout(self.timeout)
        # Connect to graphite server
        try:
            self.socket.connect((self.host, self.port))
            # Log
            self.log.debug("GraphiteHandler: Established connection to "
                           "graphite server %s:%d.",
                           self.host, self.port)
        except Exception, ex:
            # Log Error
            self._throttle_error("GraphiteHandler: Failed to connect to "
                                 "%s:%i. %s.", self.host, self.port, ex)
            # Close Socket
            self._close()
            return

    def _close(self):
        """
        Close the socket
        """
        if self.socket is not None:
            self.socket.close()
        self.socket = None

########NEW FILE########
__FILENAME__ = graphitepickle
# coding=utf-8

"""
Send metrics to a [graphite](http://graphite.wikidot.com/) using the high
performace pickle interface.

Graphite is an enterprise-scale monitoring tool that runs well on cheap
hardware. It was originally designed and written by Chris Davis at Orbitz in
2006 as side project that ultimately grew to be a foundational monitoring tool.
In 2008, Orbitz allowed Graphite to be released under the open source Apache
2.0 license. Since then Chris has continued to work on Graphite and has
deployed it at other companies including Sears, where it serves as a pillar of
the e-commerce monitoring system. Today many
[large companies](http://graphite.readthedocs.org/en/latest/who-is-using.html)
use it.

- enable it in `diamond.conf` :

`    handlers = diamond.handler.graphitepickle.GraphitePickleHandler
`

"""

import struct

from graphite import GraphiteHandler

try:
    import cPickle as pickle
    pickle  # workaround for pyflakes issue #13
except ImportError:
    import pickle as pickle


class GraphitePickleHandler(GraphiteHandler):
    """
    Overrides the GraphiteHandler class
    Sending data to graphite using batched pickle format
    """
    def __init__(self, config=None):
        """
        Create a new instance of the GraphitePickleHandler
        """
        # Initialize GraphiteHandler
        GraphiteHandler.__init__(self, config)
        # Initialize Data
        self.batch = []
        # Initialize Options
        self.batch_size = int(self.config['batch'])

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(GraphitePickleHandler, self).get_default_config_help()

        config.update({
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(GraphitePickleHandler, self).get_default_config()

        config.update({
        })

        return config

    def process(self, metric):
        # Convert metric to pickle format
        m = (metric.path, (metric.timestamp, metric.value))
        # Add the metric to the match
        self.batch.append(m)
        # If there are sufficient metrics, then pickle and send
        if len(self.batch) >= self.batch_size:
            # Log
            self.log.debug("GraphitePickleHandler: Sending batch size: %d",
                           self.batch_size)
            # Pickle the batch of metrics
            self.metrics = [self._pickle_batch()]
            # Send pickled batch
            self._send()
            # Flush the metric pack down the wire
            self.flush()
            # Clear Batch
            self.batch = []

    def _pickle_batch(self):
        """
        Pickle the metrics into a form that can be understood
        by the graphite pickle connector.
        """
        # Pickle
        payload = pickle.dumps(self.batch)

        # Pack Message
        header = struct.pack("!L", len(payload))
        message = header + payload

        # Return Message
        return message

########NEW FILE########
__FILENAME__ = g_metric
# coding=utf-8

"""
Emulate a gmetric client for usage with
[Ganglia Monitoring System](http://ganglia.sourceforge.net/)
"""

from Handler import Handler
import logging
try:
    import gmetric
    gmetric  # Pyflakes
except ImportError:
    gmetric = None


class GmetricHandler(Handler):
    """
    Implements the abstract Handler class, sending data the same way that
    gmetric does.
    """

    def __init__(self, config=None):
        """
        Create a new instance of the GmetricHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        if gmetric is None:
            logging.error("Failed to load gmetric module")
            return

        # Initialize Data
        self.socket = None

        # Initialize Options
        self.host = self.config['host']
        self.port = int(self.config['port'])
        self.protocol = self.config['protocol']
        if not self.protocol:
            self.protocol = 'udp'

        # Initialize
        self.gmetric = gmetric.Gmetric(self.host, self.port, self.protocol)

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(GmetricHandler, self).get_default_config_help()

        config.update({
            'host': 'Hostname',
            'port': 'Port',
            'protocol': 'udp or tcp',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(GmetricHandler, self).get_default_config()

        config.update({
            'host': 'localhost',
            'port': 8651,
            'protocol': 'udp',
        })

        return config

    def __del__(self):
        """
        Destroy instance of the GmetricHandler class
        """
        self._close()

    def process(self, metric):
        """
        Process a metric by sending it to a gmond instance
        """
        # Just send the data as a string
        self._send(metric)

    def _send(self, metric):
        """
        Send data to gmond.
        """
        metric_name = self.get_name_from_path(metric.path)
        tmax = "60"
        dmax = "0"
        slope = "both"
        # FIXME: Badness, shouldn't *assume* double type
        metric_type = "double"
        units = ""
        group = ""
        self.gmetric.send(metric_name,
                          metric.value,
                          metric_type,
                          units,
                          slope,
                          tmax,
                          dmax,
                          group)

    def _close(self):
        """
        Close the connection
        """
        self.gmetric = None

########NEW FILE########
__FILENAME__ = Handler
# coding=utf-8

import logging
import threading
import traceback
from configobj import ConfigObj
import time


class Handler(object):
    """
    Handlers process metrics that are collected by Collectors.
    """
    def __init__(self, config=None):
        """
        Create a new instance of the Handler class
        """

        # Initialize Log
        self.log = logging.getLogger('diamond')

        # Initialize Blank Configs
        self.config = ConfigObj()

        # Load default
        self.config.merge(self.get_default_config())

        # Load in user
        self.config.merge(config)

        # error logging throttling
        self.server_error_interval = float(self.config['server_error_interval'])
        self._errors = {}

        # Initialize Lock
        self.lock = threading.Lock()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        return {
            'get_default_config_help': 'get_default_config_help',
            'server_error_interval': ('How frequently to send repeated server '
                                      'errors'),
        }

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        return {
            'get_default_config': 'get_default_config',
            'server_error_interval': 120,
        }

    def _process(self, metric):
        """
        Decorator for processing handlers with a lock, catching exceptions
        """
        try:
            try:
                self.lock.acquire()
                self.process(metric)
            except Exception:
                self.log.error(traceback.format_exc())
        finally:
            if self.lock.locked():
                self.lock.release()

    def process(self, metric):
        """
        Process a metric

        Should be overridden in subclasses
        """
        raise NotImplementedError

    def _flush(self):
        """
        Decorator for flushing handlers with an lock, catching exceptions
        """
        try:
            try:
                self.lock.acquire()
                self.flush()
            except Exception:
                self.log.error(traceback.format_exc())
        finally:
            if self.lock.locked():
                self.lock.release()

    def flush(self):
        """
        Flush metrics

        Optional: Should be overridden in subclasses
        """
        pass

    def _throttle_error(self, msg, *args, **kwargs):
        """
        Avoids sending errors repeatedly. Waits at least
        `self.server_error_interval` seconds before sending the same error
        string to the error logging facility. If not enough time has passed,
        it calls `log.debug` instead

        Receives the same parameters as `Logger.error` an passes them on to the
        selected logging function, but ignores all parameters but the main
        message string when checking the last emission time.

        :returns: the return value of `Logger.debug` or `Logger.error`
        """
        now = time.time()
        if msg in self._errors:
            if ((now - self._errors[msg]) >=
                    self.server_error_interval):
                fn = self.log.error
                self._errors[msg] = now
            else:
                fn = self.log.debug
        else:
            self._errors[msg] = now
            fn = self.log.error

        return fn(msg, *args, **kwargs)

    def _reset_errors(self, msg=None):
        """
        Resets the logging throttle cache, so the next error is emitted
        regardless of the value in `self.server_error_interval`

        :param msg: if present, only this key is reset. Otherwise, the whole
            cache is cleaned.
        """
        if msg is not None and msg in self._errors:
            del self._errors[msg]
        else:
            self._errors = {}

########NEW FILE########
__FILENAME__ = hostedgraphite
# coding=utf-8

"""
[Hosted Graphite](https://www.hostedgraphite.com/) is the powerful open-source
application metrics system used by hundreds of companies. We take away the
headaches of scaling, maintenance, and upgrades and let you do what you do
best - write great software.

#### Configuration

Enable this handler

 * handlers = diamond.handler.hostedgraphite.HostedGraphiteHandler,

 * apikey = API_KEY

"""

from Handler import Handler
from graphite import GraphiteHandler


class HostedGraphiteHandler(Handler):

    def __init__(self, config=None):
        """
        Create a new instance of the HostedGraphiteHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        self.key = self.config['apikey'].lower().strip()

        self.graphite = GraphiteHandler(self.config)

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(HostedGraphiteHandler, self).get_default_config_help()

        config.update({
            'apikey': 'Api key to use',
            'host': 'Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(HostedGraphiteHandler, self).get_default_config()

        config.update({
            'apikey': '',
            'host': 'carbon.hostedgraphite.com',
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        """
        Process a metric by sending it to graphite
        """
        metric = self.key + '.' + str(metric)
        self.graphite.process(metric)

    def _process(self, metric):
        """
        Process a metric by sending it to graphite
        """
        metric = self.key + '.' + str(metric)
        self.graphite._process(metric)

    def _flush(self):
        self.graphite._flush()

    def flush(self):
        self.graphite.flush()

########NEW FILE########
__FILENAME__ = httpHandler
#!/usr/bin/env python
# coding=utf-8

"""
Send metrics to a http endpoint via POST

#### Dependencies

 * urllib2


#### Configuration
Enable this handler

 * handers = diamond.handler.httpHandler.HttpPostHandler

 * url = http://www.example.com/endpoint

"""

from Handler import Handler
import urllib2


class HttpPostHandler(Handler):

    # Inititalize Handler with url and batch size
    def __init__(self, config=None):
        Handler.__init__(self, config)
        self.metrics = []
        self.batch_size = int(self.config['batch'])
        self.url = self.config.get('url')

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(HttpPostHandler, self).get_default_config_help()

        config.update({
            'url': 'Fully qualified url to send metrics to',
            'batch': 'How many to store before sending to the graphite server',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(HttpPostHandler, self).get_default_config()

        config.update({
            'url': 'http://localhost/blah/blah/blah',
            'batch': 100,
        })

        return config

    # Join batched metrics and push to url mentioned in config
    def process(self, metric):
        self.metrics.append(str(metric))
        if len(self.metrics) >= self.batch_size:
            self.post()

    #Overriding flush to post metrics for every collector.
    def flush(self):
        """Flush metrics in queue"""
        self.post()

    def post(self):
        req = urllib2.Request(self.url, "\n".join(self.metrics))
        urllib2.urlopen(req)
        self.metrics = []

########NEW FILE########
__FILENAME__ = influxdbHandler
# coding=utf-8

"""
Send metrics to a [influxdb](https://github.com/influxdb/influxdb/) using the
http interface.

v1.0 : creation
v1.1 : force influxdb driver with SSL
       Sebastien Prune THOMAS - prune@lecentre.net

- Dependency:
    - influxdb client (pip install influxdb)
      you need version > 0.1.6 for HTTPS (not yet released)

- enable it in `diamond.conf` :

handlers = diamond.handler.influxdbHandler.InfluxdbHandler

- add config to `diamond.conf` :

[[InfluxdbHandler]]
hostname = localhost
port = 8086 #8084 for HTTPS
batch_size = 100 # default to 1
username = root
password = root
database = graphite
time_precision = s
"""

import struct
from Handler import Handler

try:
    import influxdb
    from influxdb.client import InfluxDBClient
    InfluxDBClient
except ImportError:
    InfluxDBClient = None


class InfluxdbHandler(Handler):
    """
    Sending data to Influxdb using batched format
    """
    def __init__(self, config=None):
        """
        Create a new instance of the InfluxdbeHandler
        """
        # Initialize Handler
        Handler.__init__(self, config)

        if not InfluxDBClient:
            self.log.error('influxdb.client.InfluxDBClient import failed. '
                           'Handler disabled')

        # Initialize Options
        if self.config['ssl'] == "True":
              self.ssl = True
        else:
              self.ssl = False
        self.hostname = self.config['hostname']
        self.port = int(self.config['port'])
        self.username = self.config['username']
        self.password = self.config['password']
        self.database = self.config['database']
        self.batch_size = int(self.config['batch_size'])
        self.batch_count = 0
        self.time_precision = self.config['time_precision']

        # Initialize Data
        self.batch = {}
        self.influx = None

        # Connect
        self._connect()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(InfluxdbHandler, self).get_default_config_help()

        config.update({
            'hostname': 'Hostname',
            'port': 'Port',
            'ssl': 'set to True to use HTTPS instead of http',
            'batch_size': 'How many to store before sending to the influxdb '
            'server',
            'username': 'Username for connection',
            'password': 'Password for connection',
            'database': 'Database name',
            'time_precision': 'time precision in second(s), milisecond(m) or '
            'microsecond (u)',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(InfluxdbHandler, self).get_default_config()

        config.update({
            'hostname': 'localhost',
            'port': 8086,
            'ssl': False,
            'username': 'root',
            'password': 'root',
            'database': 'graphite',
            'batch_size': 1,
            'time_precision': 's',
        })

        return config

    def __del__(self):
        """
        Destroy instance of the InfluxdbHandler class
        """
        self._close()

    def process(self, metric):
        # Add the data to the batch
        self.batch.setdefault(metric.path, []).append([metric.timestamp,
                                                       metric.value])
        self.batch_count += 1
        # If there are sufficient metrics, then pickle and send
        if self.batch_count >= self.batch_size:
            # Log
            self.log.debug("InfluxdbHandler: Sending batch size: %d",
                           self.batch_size)
            # Send pickled batch
            self._send()

    def _send(self):
        """
        Send data to Influxdb. Data that can not be sent will be kept in queued.
        """
        # Check to see if we have a valid socket. If not, try to connect.
        try:
                if self.influx is None:
                    self.log.debug("InfluxdbHandler: Socket is not connected. "
                                   "Reconnecting.")
                    self._connect()
                if self.influx is None:
                    self.log.debug("InfluxdbHandler: Reconnect failed.")
                else:
                    # build metrics data
                    metrics = []
                    for path in self.batch:
                        metrics.append({
                            "points": self.batch[path],
                            "name": path,
                            "columns": ["time", "value"]})
                    # Send data to socket
                    self.influx.write_points(metrics,
                                             time_precision=self.time_precision)

                    # empty batch buffer
                    self.batch = {}
                    self.batch_count = 0

        except Exception:
                self._close()
                self._throttle_error("InfluxdbHandler: Error sending metrics.")
                raise

    def _connect(self):
        """
        Connect to the influxdb server
        """

        try:
            # Open Connection
            self.influx = InfluxDBClient(self.hostname, self.port, 
                                         self.username, self.password, 
                                         self.database, self.ssl)
            # Log
            self.log.debug("InfluxdbHandler: Established connection to "
                           "%s:%d/%s.",
                           self.hostname, self.port, self.database)
        except Exception, ex:
            # Log Error
            self._throttle_error("InfluxdbHandler: Failed to connect to "
                                 "%s:%d/%s. %s",
                                 self.hostname, self.port, self.database, ex)
            # Close Socket
            self._close()
            return

    def _close(self):
        """
        Close the socket = do nothing for influx which is http stateless
        """
        self.influx = None

########NEW FILE########
__FILENAME__ = libratohandler
# coding=utf-8

"""
[Librato](http://librato.com) is an infrastructure software as a service company
dedicated to delivering beautiful, easy to use tools that make managing your
operations more fun and efficient.

#### Dependencies

 * [librato-metrics](https://github.com/librato/python-librato)

#### Configuration

Enable this handler

 * handlers = diamond.handler.libratohandler.LibratoHandler,

 * user = LIBRATO_USERNAME
 * apikey = LIBRATO_API_KEY

 * queue_max_size = [optional | 300] max measurements to queue before submitting
 * queue_max_interval [optional | 60] @max seconds to wait before submitting
     For best behavior, be sure your highest collector poll interval is lower
     than or equal to the queue_max_interval setting.

 * include_filters = [optional | '^.*'] A list of regex patterns.
     Only measurements whose path matches a filter will be submitted.
     Useful for limiting usage to *only* desired measurements, e.g.
       include_filters = "^diskspace\..*\.byte_avail$", "^loadavg\.01"
       include_filters = "^sockets\.",
                                     ^ note trailing comma to indicate a list

"""

from Handler import Handler
import logging
import time
import re

try:
    import librato
    librato  # workaround for pyflakes issue #13
except ImportError:
    librato = None


class LibratoHandler(Handler):

    def __init__(self, config=None):
        """
        Create a new instance of the LibratoHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)
        logging.debug("Initialized Librato handler.")

        if librato is None:
            logging.error("Failed to load librato module")
            return

        # Initialize Options
        api = librato.connect(self.config['user'],
                              self.config['apikey'])
        self.queue = api.new_queue()
        self.queue_max_size = int(self.config['queue_max_size'])
        self.queue_max_interval = int(self.config['queue_max_interval'])
        self.queue_max_timestamp = int(time.time() + self.queue_max_interval)
        self.current_n_measurements = 0

        # If a user leaves off the ending comma, cast to a array for them
        include_filters = self.config['include_filters']
        if isinstance(include_filters, basestring):
            include_filters = [include_filters]

        self.include_reg = re.compile(r'(?:%s)' % '|'.join(include_filters))

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(LibratoHandler, self).get_default_config_help()

        config.update({
            'user': '',
            'apikey': '',
            'queue_max_size': '',
            'queue_max_interval': '',
            'include_filters': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(LibratoHandler, self).get_default_config()

        config.update({
            'user': '',
            'apikey': '',
            'queue_max_size': 300,
            'queue_max_interval': 60,
            'include_filters': ['^.*'],
        })

        return config

    def process(self, metric):
        """
        Process a metric by sending it to Librato
        """
        path = metric.getCollectorPath()
        path += '.'
        path += metric.getMetricPath()

        if self.include_reg.match(path):
            if metric.metric_type == 'GAUGE':
                m_type = 'gauge'
            else:
                m_type = 'counter'
            self.queue.add(path,                # name
                           float(metric.value),  # value
                           type=m_type,
                           source=metric.host,
                           measure_time=metric.timestamp)
            self.current_n_measurements += 1
        else:
            self.log.debug("LibratoHandler: Skip %s, no include_filters match",
                           path)

        if (self.current_n_measurements >= self.queue_max_size or
                time.time() >= self.queue_max_timestamp):
            self.log.debug("LibratoHandler: Sending batch size: %d",
                           self.current_n_measurements)
            self._send()

    def _send(self):
        """
        Send data to Librato.
        """
        self.queue.submit()
        self.queue_max_timestamp = int(time.time() + self.queue_max_interval)
        self.current_n_measurements = 0

########NEW FILE########
__FILENAME__ = mqtt
# coding=utf-8

"""
Send metrics to an MQTT broker.

### Dependencies

* [mosquitto](http://mosquitto.org/documentation/python/)
* Python `ssl` module (and Python >= 2.7)

In order for this to do something useful, you'll need an
MQTT broker (e.g. [mosquitto](http://mosquitto.org) and
a `diamond.conf` containing something along these lines:

        [server]
        handlers = diamond.handler.mqtt.MQTTHandler
        ...
        [handlers]

        [[MQTTHandler]]
        host = address-of-mqtt-broker  (default: localhost)
        port = 1883         (default: 1883; with tls, default: 8883)
        qos = 0             (default: 0)

        # If False, do not include timestamp in the MQTT payload
        # i.e. just the metric number
        timestamp = True

        # Optional topic-prefix to prepend to metrics en-route to
        # MQTT broker
        prefix = some/pre/fix       (default: "")

        # If you want to connect to your MQTT broker with TLS, you'll have
        # to set the following four parameters
        tls = True          (default: False)
        cafile =        /path/to/ca/cert.pem
        certfile =      /path/to/certificate.pem
        keyfile =       /path/to/key.pem

Test by launching an MQTT subscribe, e.g.:

        mosquitto_sub  -v -t 'servers/#'

or
        mosquitto_sub -v -t 'some/pre/fix/#'

### To Graphite

You may be interested in
[mqtt2graphite](https://github.com/jpmens/mqtt2graphite)
which subscribes to an MQTT broker and sends metrics off to Graphite.

### Notes

* This handler sets a last will and testament, so that the broker
  publishes its death at a topic called clients/diamond/<hostname>
* Support for reconnecting to a broker is implemented and ought to
  work.

"""

__author__ = 'Jan-Piet Mens'
__email__ = 'jpmens@gmail.com'

from Handler import Handler
from diamond.collector import get_hostname
import os
HAVE_SSL = True
try:
    import ssl
except ImportError:
    HAVE_SSL = False

try:
    import mosquitto
    mosquitto  # Pyflakes
except ImportError:
    mosquitto = None


class MQTTHandler(Handler):
    """
    """

    def __init__(self, config=None):
        """
        Create a new instance of the MQTTHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        # Initialize Data
        self.mqttc = None
        self.hostname = get_hostname(self.config)
        self.client_id = "%s_%s" % (self.hostname, os.getpid())

        # Initialize Options
        self.host = self.config.get('host', 'localhost')
        self.port = 0
        self.qos = int(self.config.get('qos', 0))
        self.prefix = self.config.get('prefix', "")
        self.tls = self.config.get('tls', False)
        self.timestamp = 0
        try:
            self.timestamp = self.config['timestamp']
            if not self.timestamp:
                self.timestamp = 1
            else:
                self.timestamp = 0
        except:
            self.timestamp = 1

        if not mosquitto:
            self.log.error('mosquitto import failed. Handler disabled')
            return

        # Initialize
        self.mqttc = mosquitto.Mosquitto(self.client_id, clean_session=True)

        if not self.tls:
            self.port = int(self.config.get('port', 1883))
        else:
            # Set up TLS if requested

            self.port = int(self.config.get('port', 8883))

            self.cafile = self.config.get('cafile', None)
            self.certfile = self.config.get('certfile', None)
            self.keyfile = self.config.get('keyfile', None)

            if None in [self.cafile, self.certfile, self.keyfile]:
                self.log.error("MQTTHandler: TLS configuration missing.")
                return

            try:
                self.mqttc.tls_set(
                    self.cafile,
                    certfile=self.certfile,
                    keyfile=self.keyfile,
                    cert_reqs=ssl.CERT_REQUIRED,
                    tls_version=3,
                    ciphers=None)
            except:
                self.log.error("MQTTHandler: Cannot set up TLS "
                               + "configuration. Files missing?")

        self.mqttc.will_set("clients/diamond/%s" % (self.hostname),
                            payload="Adios!", qos=0, retain=False)
        self.mqttc.connect(self.host, self.port, 60)

        self.mqttc.on_disconnect = self._disconnect

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(MQTTHandler, self).get_default_config_help()

        config.update({
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(MQTTHandler, self).get_default_config()

        config.update({
        })

        return config

    def process(self, metric):
        """
        Process a metric by converting metric name to MQTT topic name;
        the payload is metric and timestamp.
        """

        if not mosquitto:
            return

        line = str(metric)
        topic, value, timestamp = line.split()
        if len(self.prefix):
            topic = "%s/%s" % (self.prefix, topic)
        topic = topic.replace('.', '/')
        topic = topic.replace('#', '&')     # Topic must not contain wildcards

        if self.timestamp == 0:
            self.mqttc.publish(topic, "%s" % (value), self.qos)
        else:
            self.mqttc.publish(topic, "%s %s" % (value, timestamp), self.qos)

    def _disconnect(self, mosq, obj, rc):

        self.log.debug("MQTTHandler: reconnecting to broker...")
        mosq.reconnect()

########NEW FILE########
__FILENAME__ = multigraphite
# coding=utf-8

"""
Send metrics to a [graphite](http://graphite.wikidot.com/) using the default
interface. Unlike GraphiteHandler, this one supports multiple graphite servers.
Specify them as a list of hosts divided by comma.
"""

from Handler import Handler
from graphite import GraphiteHandler
from copy import deepcopy


class MultiGraphiteHandler(Handler):
    """
    Implements the abstract Handler class, sending data to multiple
    graphite servers by using two instances of GraphiteHandler
    """

    def __init__(self, config=None):
        """
        Create a new instance of the MultiGraphiteHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        self.handlers = []

        # Initialize Options
        hosts = self.config['host']
        for host in hosts:
            config = deepcopy(self.config)
            config['host'] = host
            self.handlers.append(GraphiteHandler(config))

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(MultiGraphiteHandler, self).get_default_config_help()

        config.update({
            'host': 'Hostname, Hostname, Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(MultiGraphiteHandler, self).get_default_config()

        config.update({
            'host': ['localhost'],
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        """
        Process a metric by passing it to GraphiteHandler
        instances
        """
        for handler in self.handlers:
            handler.process(metric)

    def flush(self):
        """Flush metrics in queue"""
        for handler in self.handlers:
            handler.flush()

########NEW FILE########
__FILENAME__ = multigraphitepickle
# coding=utf-8

"""
Send metrics to a [graphite](http://graphite.wikidot.com/) using the pickle
interface. Unlike GraphitePickleHandler, this one supports multiple graphite
servers. Specify them as a list of hosts divided by comma.
"""

from Handler import Handler
from graphitepickle import GraphitePickleHandler
from copy import deepcopy


class MultiGraphitePickleHandler(Handler):
    """
    Implements the abstract Handler class, sending data to multiple
    graphite servers by using two instances of GraphitePickleHandler
    """

    def __init__(self, config=None):
        """
        Create a new instance of the MultiGraphitePickleHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        self.handlers = []

        # Initialize Options
        hosts = self.config['host']
        for host in hosts:
            config = deepcopy(self.config)
            config['host'] = host
            self.handlers.append(GraphitePickleHandler(config))

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(MultiGraphitePickleHandler,
                       self).get_default_config_help()

        config.update({
            'host': 'Hostname, Hostname, Hostname',
            'port': 'Port',
            'proto': 'udp or tcp',
            'timeout': '',
            'batch': 'How many to store before sending to the graphite server',
            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
            'trim_backlog_multiplier': 'Trim down how many batches',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(MultiGraphitePickleHandler, self).get_default_config()

        config.update({
            'host': ['localhost'],
            'port': 2003,
            'proto': 'tcp',
            'timeout': 15,
            'batch': 1,
            'max_backlog_multiplier': 5,
            'trim_backlog_multiplier': 4,
        })

        return config

    def process(self, metric):
        """
        Process a metric by passing it to GraphitePickleHandler
        instances
        """
        for handler in self.handlers:
            handler.process(metric)

    def flush(self):
        """Flush metrics in queue"""
        for handler in self.handlers:
            handler.flush()

########NEW FILE########
__FILENAME__ = mysql
# coding=utf-8

"""
Insert the collected values into a mysql table
"""

from handler import Handler
import MySQLdb


class MySQLHandler(Handler):
    """
    Implements the abstract Handler class, sending data to a mysql table
    """
    conn = None

    def __init__(self, config=None):
        """
        Create a new instance of the MySQLHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        # Initialize Options
        self.hostname = self.config['hostname']
        self.port = int(self.config['port'])
        self.username = self.config['username']
        self.password = self.config['password']
        self.database = self.config['database']
        self.table = self.config['table']
        self.col_time = self.config['col_time']
        self.col_metric = self.config['col_metric']
        self.col_value = self.config['col_value']

        # Connect
        self._connect()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(MySQLHandler, self).get_default_config_help()

        config.update({
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(MySQLHandler, self).get_default_config()

        config.update({
        })

        return config

    def __del__(self):
        """
        Destroy instance of the MySQLHandler class
        """
        self._close()

    def process(self, metric):
        """
        Process a metric
        """
        # Just send the data
        self._send(str(metric))

    def _send(self, data):
        """
        Insert the data
        """
        data = data.strip().split(' ')
        try:
            cursor = self.conn.cursor()
            cursor.execute("INSERT INTO %s (%s, %s, %s) VALUES(%%s, %%s, %%s)"
                           % (self.table, self.col_metric,
                              self.col_time, self.col_value),
                           (data[0], data[2], data[1]))
            cursor.close()
            self.conn.commit()
            # Insert data
            pass
            #self.socket.sendall(data)
            # Done
        except BaseException, e:
            # Log Error
            self.log.error("MySQLHandler: Failed sending data. %s.", e)
            # Attempt to restablish connection
            self._connect()

    def _connect(self):
        """
        Connect to the MySQL server
        """
        self._close()
        self.conn = MySQLdb.Connect(host=self.hostname,
                                    port=self.port,
                                    user=self.username,
                                    passwd=self.password,
                                    db=self.database)

    def _close(self):
        """
        Close the connection
        """
        if self.conn:
            self.conn.commit()
            self.conn.close()

########NEW FILE########
__FILENAME__ = null
# coding=utf-8

"""
Output the collected values to the debug log channel.
"""

from Handler import Handler


class NullHandler(Handler):
    """
    Implements the abstract Handler class, doing nothing except log
    """
    def process(self, metric):
        """
        Process a metric by doing nothing
        """
        self.log.debug("Process: %s", str(metric).rstrip())

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(NullHandler, self).get_default_config_help()

        config.update({
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(NullHandler, self).get_default_config()

        config.update({
        })

        return config

########NEW FILE########
__FILENAME__ = rabbitmq_pubsub
# coding=utf-8

"""
Output the collected values to RabitMQ pub/sub channel
"""

from Handler import Handler
import time

try:
    import pika
    pika  # Pyflakes
except ImportError:
    pika = None


class rmqHandler (Handler):
    """
      Implements the abstract Handler class
      Sending data to a RabbitMQ pub/sub channel
    """

    def __init__(self, config=None):
        """
          Create a new instance of rmqHandler class
        """

        # Initialize Handler
        Handler.__init__(self, config)

        if pika is None:
            self.log.error('pika import failed. Handler disabled')
            return

        # Initialize Data
        self.connections = {}
        self.channels = {}
        self.reconnect_interval = 1

        # Initialize Options
        tmp_rmq_server = self.config['rmq_server']
        if type(tmp_rmq_server) is list:
            self.rmq_server = tmp_rmq_server
        else:
            self.rmq_server = [tmp_rmq_server]

        self.rmq_port = 5672
        self.rmq_exchange = self.config['rmq_exchange']
        self.rmq_user = None
        self.rmq_password = None
        self.rmq_vhost = '/'
        self.rmq_exchange_type = 'fanout'
        self.rmq_durable = True
        self.rmq_heartbeat_interval = 300

        self.get_config()
        # Create rabbitMQ pub socket and bind
        try:
            self._bind_all()
        except pika.exceptions.AMQPConnectionError:
            self.log.error('Failed to bind to rabbitMQ pub socket')

    def get_config(self):
        """ Get and set config options from config file """
        if 'rmq_port' in self.config:
            self.rmq_port = int(self.config['rmq_port'])

        if 'rmq_user' in self.config:
            self.rmq_user = self.config['rmq_user']

        if 'rmq_password' in self.config:
            self.rmq_password = self.config['rmq_password']

        if 'rmq_vhost' in self.config:
            self.rmq_vhost = self.config['rmq_vhost']

        if 'rmq_exchange_type' in self.config:
            self.rmq_exchange_type = self.config['rmq_exchange_type']

        if 'rmq_durable' in self.config:
            self.rmq_durable = bool(self.config['rmq_durable'])

        if 'rmq_heartbeat_interval' in self.config:
            self.rmq_heartbeat_interval = int(
                self.config['rmq_heartbeat_interval'])

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(rmqHandler, self).get_default_config_help()

        config.update({
            'server': '',
            'rmq_exchange': '',
        })
        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(rmqHandler, self).get_default_config()

        config.update({
            'server': '127.0.0.1',
            'rmq_exchange': 'diamond',
        })

        return config

    def _bind_all(self):
        """
        Bind all RMQ servers defined in config
        """
        for rmq_server in self.rmq_server:
            self._bind(rmq_server)

    def _bind(self, rmq_server):
        """
           Create PUB socket and bind
        """
        if (rmq_server in self.connections.keys()
                and self.connections[rmq_server] is not None
                and self.connections[rmq_server].is_open):
            # It seems we already have this server, so let's try _unbind just
            # to be safe.
            self._unbind(rmq_server)

        credentials = None
        if self.rmq_user and self.rmq_password:
            credentials = pika.PlainCredentials(
                self.rmq_user,
                self.rmq_password)

        parameters = pika.ConnectionParameters(
            host=rmq_server,
            port=self.rmq_port,
            virtual_host=self.rmq_vhost,
            credentials=credentials,
            heartbeat_interval=self.rmq_heartbeat_interval,
            retry_delay=5,
            connection_attempts=3)

        self.connections[rmq_server] = None
        while (self.connections[rmq_server] is None
                or self.connections[rmq_server].is_open is False):
            try:
                self.connections[rmq_server] = pika.BlockingConnection(
                    parameters)
                self.channels[rmq_server] = self.connections[
                    rmq_server].channel()
                self.channels[rmq_server].exchange_declare(
                    exchange=self.rmq_exchange,
                    type=self.rmq_exchange_type,
                    durable=self.rmq_durable)
                # Reset reconnect_interval after a successful connection
                self.reconnect_interval = 1
            except Exception as exception:
                self.log.debug("Caught exception in _bind: %s", exception)
                if rmq_server in self.connections.keys():
                    self._unbind(rmq_server)

                if self.reconnect_interval >= 16:
                    break

                if self.reconnect_interval < 16:
                    self.reconnect_interval = self.reconnect_interval * 2

                time.sleep(self.reconnect_interval)

    def _unbind(self, rmq_server=None):
        """ Close AMQP connection and unset channel """
        try:
            self.connections[rmq_server].close()
        except AttributeError:
            pass

        self.connections[rmq_server] = None
        self.channels[rmq_server] = None

    def __del__(self):
        """
          Destroy instance of the rmqHandler class
        """
        for rmq_server in self.connections.keys():
            self._unbind(rmq_server)

    def process(self, metric):
        """
          Process a metric and send it to RMQ pub socket
        """
        for rmq_server in self.connections.keys():
            try:
                if (self.connections[rmq_server] is None
                        or self.connections[rmq_server].is_open is False):
                    self._bind(rmq_server)

                channel = self.channels[rmq_server]
                channel.basic_publish(exchange=self.rmq_exchange,
                                      routing_key='', body="%s" % metric)
            except Exception as exception:
                self.log.error(
                    "Failed publishing to %s, attempting reconnect",
                    rmq_server)
                self.log.debug("Caught exception: %s", exception)
                self._unbind(rmq_server)
                self._bind(rmq_server)

########NEW FILE########
__FILENAME__ = rabbitmq_topic
# coding=utf-8

"""
Output the collected values to RabitMQ Topic Exchange
This allows for 'subscribing' to messages based on the routing key, which is
the metric path
"""

from Handler import Handler

try:
    import pika
    pika  # Pyflakes
except ImportError:
    pika = None


class rmqHandler (Handler):
    """
      Implements the abstract Handler class
      Sending data to a RabbitMQ topic exchange.
      The routing key will be the full name of the metric being sent.

      Based on the rmqHandler and zmpqHandler code.
    """

    def __init__(self, config=None):
        """
          Create a new instance of rmqHandler class
        """

        # Initialize Handler
        Handler.__init__(self, config)

        # Initialize Data
        self.connection = None
        self.channel = None

        # Initialize Options
        self.server = self.config.get('server', '127.0.0.1')
        self.port = int(self.config.get('port', 5672))
        self.topic_exchange = self.config.get('topic_exchange', 'diamond')
        self.vhost = self.config.get('vhost', '')
        self.user = self.config.get('user', 'guest')
        self.password = self.config.get('password', 'guest')
        self.routing_key = self.config.get('routing_key', 'metric')
        self.custom_routing_key = self.config.get(
            'custom_routing_key', 'diamond')

        if not pika:
            self.log.error('pika import failed. Handler disabled')
            return

        # Create rabbitMQ topic exchange and bind
        try:
            self._bind()
        except pika.exceptions.AMQPConnectionError:
            self.log.error('Failed to bind to rabbitMQ topic exchange')

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(rmqHandler, self).get_default_config_help()

        config.update({
            'server': '',
            'topic_exchange': '',
            'vhost': '',
            'user': '',
            'password': '',
            'routing_key': '',
            'custom_routing_key': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(rmqHandler, self).get_default_config()

        config.update({
            'server': '127.0.0.1',
            'topic_exchange': 'diamond',
            'vhost': '/',
            'user': 'guest',
            'password': 'guest',
            'port': '5672',
        })

        return config

    def _bind(self):
        """
           Create  socket and bind
        """

        credentials = pika.PlainCredentials(self.user, self.password)
        params = pika.ConnectionParameters(credentials=credentials,
                                           host=self.server,
                                           virtual_host=self.vhost,
                                           port=self.port)

        self.connection = pika.BlockingConnection(params)
        self.channel = self.connection.channel()

        # NOTE : PIKA version uses 'exchange_type' instead of 'type'

        self.channel.exchange_declare(exchange=self.topic_exchange,
                                      exchange_type="topic")

    def __del__(self):
        """
          Destroy instance of the rmqHandler class
        """
        try:
            self.connection.close()
        except AttributeError:
            pass

    def process(self, metric):
        """
          Process a metric and send it to RabbitMQ topic exchange
        """
        # Send the data as ......
        if not pika:
            return

        routingKeyDic = {
            'metric': lambda: metric.path,
            'custom': lambda: self.custom_routing_key,

            # These option and the below are really not needed because
            #  with Rabbitmq you can use regular expressions to indicate
            #  what routing_keys to subscribe to. But I figure this is
            #  a good example of how to allow more routing keys

            'host': lambda: metric.host,
            'metric.path': metric.getMetricPath,
            'path.prefix': metric.getPathPrefix,
            'collector.path': metric.getCollectorPath,
        }

        try:
            self.channel.basic_publish(
                exchange=self.topic_exchange,
                routing_key=routingKeyDic[self.routing_key](),
                body="%s" % metric)

        except Exception:  # Rough connection re-try logic.
            self.log.info("Failed publishing to rabbitMQ. Attempting reconnect")
            self._bind()

########NEW FILE########
__FILENAME__ = riemann
# coding=utf-8

"""
Send metrics to [Riemann](http://aphyr.github.com/riemann/).

#### Dependencies

 * [Bernhard](https://github.com/banjiewen/bernhard).

#### Configuration

Add `diamond.handler.riemann.RiemannHandler` to your handlers.
It has these options:

 * `host` - The Riemann host to connect to.
 * `port` - The port it's on.
 * `transport` - Either `tcp` or `udp`. (default: `tcp`)

"""

from Handler import Handler
import logging
try:
    import bernhard
    bernhard  # Pyflakes
except ImportError:
    bernhard = None


class RiemannHandler(Handler):
    def __init__(self, config=None):
        # Initialize Handler
        Handler.__init__(self, config)

        if bernhard is None:
            logging.error("Failed to load bernhard module")
            return

        # Initialize options
        self.host = self.config['host']
        self.port = int(self.config['port'])
        self.transport = self.config['transport']

        # Initialize client
        if self.transport == 'tcp':
            transportCls = bernhard.TCPTransport
        else:
            transportCls = bernhard.UDPTransport
        self.client = bernhard.Client(self.host, self.port, transportCls)

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(RiemannHandler, self).get_default_config_help()

        config.update({
            'host': '',
            'port': '',
            'transport': 'tcp or udp',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(RiemannHandler, self).get_default_config()

        config.update({
            'host': '',
            'port': 123,
            'transport': 'tcp',
        })

        return config

    def process(self, metric):
        """
        Send a metric to Riemann.
        """
        event = self._metric_to_riemann_event(metric)
        try:
            self.client.send(event)
        except Exception, e:
            self.log.error("RiemannHandler: Error sending event to Riemann: %s",
                           e)

    def _metric_to_riemann_event(self, metric):
        """
        Convert a metric to a dictionary representing a Riemann event.
        """
        # Riemann has a separate "host" field, so remove from the path.
        path = '%s.%s.%s' % (
            metric.getPathPrefix(),
            metric.getCollectorPath(),
            metric.getMetricPath()
        )

        return {
            'host': metric.host,
            'service': path,
            'time': metric.timestamp,
            'metric': float(metric.value),
            'ttl': metric.ttl,
        }

    def _close(self):
        """
        Disconnect from Riemann.
        """
        try:
            self.client.disconnect()
        except AttributeError:
            pass

    def __del__(self):
        self._close()

########NEW FILE########
__FILENAME__ = rrdtool
# coding=utf-8

"""
Save stats in RRD files using rrdtool.
"""

import os
import re
import subprocess
import Queue

from Handler import Handler

#
# Constants for RRD file creation.
#

# NOTE: We default to the collectd RRD directory
# simply as a compatibility tool. Users that have
# tools that look in that location and would like
# to switch to Diamond need to make zero changes.

BASEDIR = '/var/lib/collectd/rrd'

METRIC_STEP = 10

BATCH_SIZE = 1

# NOTE: We don't really have a rigorous defition
# for metrics, particularly how often they will be
# reported, etc. Because of this, we have to guess
# at the steps and RRAs used for creation of the
# RRD files. These are a fairly sensible default,
# and basically allow for aggregated up from a single
# datapoint (because the XFF is 0.1, and each step
# aggregates not more than 10 of the previous step).
#
# Given a METRIC_STEP of 10 seconds, then these will
# represent data for up to the last full year.

RRA_SPECS = [
    "RRA:AVERAGE:0.1:1:1200",
    "RRA:MIN:0.1:1:1200",
    "RRA:MAX:0.1:1:1200",
    "RRA:AVERAGE:0.1:7:1200",
    "RRA:MIN:0.1:7:1200",
    "RRA:MAX:0.1:7:1200",
    "RRA:AVERAGE:0.1:50:1200",
    "RRA:MIN:0.1:50:1200",
    "RRA:MAX:0.1:50:1200",
    "RRA:AVERAGE:0.1:223:1200",
    "RRA:MIN:0.1:223:1200",
    "RRA:MAX:0.1:223:1200",
    "RRA:AVERAGE:0.1:2635:1200",
    "RRA:MIN:0.1:2635:1200",
    "RRA:MAX:0.1:2635:1200",
]


class RRDHandler(Handler):

    # NOTE: This handler is fairly loose about locking (none),
    # and the reason is because the calls are always protected
    # by locking done in the _process and _flush routines.
    # If this were to change at some point, we would definitely
    # want to be a bit more sensible about how we lock.
    #
    # We would probably also want to restructure this as a
    # consumer and producer so that one thread can continually
    # write out data, but that really depends on the design
    # at the top level.

    def __init__(self, *args, **kwargs):
        super(RRDHandler, self).__init__(*args, **kwargs)
        self._exists_cache = dict()
        self._basedir = self.config['basedir']
        self._batch = self.config['batch']
        self._step = self.config['step']
        self._queues = {}
        self._last_update = {}

    def get_default_config_help(self):
        config = super(RRDHandler, self).get_default_config_help()
        config.update({
            'basedir': 'The base directory for all RRD files.',
            'batch': 'Wait for this many updates before saving to the RRD file',
            'step': 'The minimum interval represented in generated RRD files.',
        })
        return config

    def get_default_config(self):
        config = super(RRDHandler, self).get_default_config()
        config.update({
            'basedir': BASEDIR,
            'batch': BATCH_SIZE,
            'step': METRIC_STEP,
        })
        return config

    def _ensure_exists(self, filename, metric_name, metric_type):
        # We're good to go!
        if filename in self._exists_cache:
            return True

        # Does the file already exist?
        if os.path.exists(filename):
            self._exists_cache[filename] = True
            return True

        # Attempt the creation.
        self._create(filename, metric_name, metric_type)
        self._exists_cache[filename] = True
        return True

    def _create(self, filename, metric_name, metric_type):
        # Sanity check the metric name.
        if not re.match("^[a-zA-Z0-9_]+$", metric_name):
            raise Exception("Invalid metric name: %s" % metric_name)

        # Sanity check the metric type.
        if metric_type not in ("GAUGE", "COUNTER"):
            raise Exception("Unknown metric type: %s" % metric_type)

        # Try to create the directory.
        # NOTE: If we aren't successful, the check_call()
        # will fail anyways so we can do this optimistically.
        try:
            os.makedirs(os.path.dirname(filename))
        except OSError:
            pass

        ds_spec = "DS:%s:%s:%d:U:U" % (
            metric_name, metric_type, self._step * 2)
        rrd_create_cmd = [
            "rrdtool", "create", filename,
            "--no-overwrite",
            "--step", str(self._step),
            ds_spec
        ]
        rrd_create_cmd.extend(RRA_SPECS)
        subprocess.check_call(rrd_create_cmd, close_fds=True)

    def process(self, metric):
        # Extract the filename given the metric.
        # NOTE: We have to tweak the metric name and limit
        # the length to 19 characters for the RRD file format.
        collector = metric.getCollectorPath()
        metric_name = metric.getMetricPath().replace(".", "_")[:19]

        dirname = os.path.join(self._basedir, metric.host, collector)
        filename = os.path.join(dirname, metric_name + ".rrd")

        # Ensure that there is an RRD file for this metric.
        # This is done inline because it's quickly cached and
        # we would like to have exceptions related to creating
        # the RRD file raised in the main thread.
        self._ensure_exists(filename, metric_name, metric.metric_type)
        if self._queue(filename, metric.timestamp, metric.value) >= self._batch:
            self._flush_queue(filename)

    def _queue(self, filename, timestamp, value):
        if not filename in self._queues:
            queue = Queue.Queue()
            self._queues[filename] = queue
        else:
            queue = self._queues[filename]
        queue.put((timestamp, value))
        return queue.qsize()

    def flush(self):
        # Grab all current queues.
        for filename in self._queues.keys():
            self._flush_queue(filename)

    def _flush_queue(self, filename):
        queue = self._queues[filename]

        # Collect all pending updates.
        updates = {}
        max_timestamp = 0
        while True:
            try:
                (timestamp, value) = queue.get(block=False)
                # RRD only supports granularity at a
                # per-second level (not milliseconds, etc.).
                timestamp = int(timestamp)

                # Remember the latest update done.
                last_update = self._last_update.get(filename, 0)
                if last_update >= timestamp:
                    # Yikes. RRDtool won't let us do this.
                    # We need to drop this update and log a warning.
                    self.log.warning(
                        "Dropping update to %s. Too frequent!" % filename)
                    continue
                max_timestamp = max(timestamp, max_timestamp)

                # Add this update.
                if not timestamp in updates:
                    updates[timestamp] = []
                updates[timestamp].append(value)
            except Queue.Empty:
                break

        # Save the last update time.
        self._last_update[filename] = max_timestamp

        if len(updates) > 0:
            # Construct our command line.
            # This will look like <time>:<value1>[:<value2>...]
            # The timestamps must be sorted, and we each of the
            # <time> values must be unique (like a snowflake).
            data_points = map(
                lambda (timestamp, values): "%d:%s" %
                (timestamp, ":".join(map(str, values))),
                sorted(updates.items()))

            # Optimisticly update.
            # Nothing can really be done if we fail.
            rrd_update_cmd = ["rrdupdate", filename, "--"]
            rrd_update_cmd.extend(data_points)
            self.log.info("update: %s" % str(rrd_update_cmd))
            subprocess.call(rrd_update_cmd)

########NEW FILE########
__FILENAME__ = sentry
# coding=utf-8

"""
Diamond handler that check if values are too high or too low, if so send an
alert to a Sentry server

This handler requires the Python module Raven:
http://raven.readthedocs.org/en/latest/index.html

To work this handler need a similar configuration:

[[SentryHandler]]

# Create a new project in Sentry and copy the DSN here:
dsn = http://user:pass@hostname/id

[[[load]]]

name = Load Average
# check for load average of the last 15 minutes
path = loadavg.15
max = 8.5

[[[free_memory]]]

name = Free Memory
path = memory.MemFree
min = 66020000
"""

__author__ = 'Bruno Clermont'
__email__ = 'bruno.clermont@gmail.com'

import logging
import re

from Handler import Handler
from diamond.collector import get_hostname
from configobj import Section

try:
    import raven.handlers.logging
except ImportError:
    raven = None


class InvalidRule(ValueError):
    """
    invalid rule
    """
    pass


class BaseResult(object):
    """
    Base class for a Rule minimum/maximum check result
    """
    adjective = None

    def __init__(self, value, threshold):
        """
        @type value: float
        @param value: metric value
        @type threshold: float
        @param threshold: value that trigger a warning
        """
        self.value = value
        self.threshold = threshold

        if not raven:
            self.log.error('raven.handlers.logging import failed. '
                           'Handler disabled')

    @property
    def verbose_message(self):
        """return more complete message"""
        if self.threshold is None:
            return 'No threshold'
        return '%.1f is %s than %.1f' % (self.value,
                                         self.adjective,
                                         self.threshold)

    @property
    def _is_error(self):
        raise NotImplementedError('_is_error')

    @property
    def is_error(self):
        """
        for some reason python do this:
        >>> 1.0 > None
        True
        >>> 1.0 < None
        False
        so we just check if min/max is not None before return _is_error
        """
        if self.threshold is None:
            return False
        return self._is_error

    def __str__(self):
        name = self.__class__.__name__.lower()
        if self.threshold is None:
            return '%s: %.1f no threshold' % (name, self.value)
        return '%.1f (%s: %.1f)' % (self.value, name, self.threshold)


class Minimum(BaseResult):
    """
    Minimum result
    """
    adjective = 'lower'

    @property
    def _is_error(self):
        """if it's too low"""
        return self.value < self.threshold


class Maximum(BaseResult):
    """
    Maximum result
    """
    adjective = 'higher'

    @property
    def _is_error(self):
        """if it's too high"""
        return self.value > self.threshold


class Rule(object):
    """
    Alert rule
    """

    def __init__(self, name, path, min=None, max=None):
        """
        @type name: string
        @param name: rule name, used to identify this rule in Sentry
        @type path: string
        @param path: un-compiled regular expression of the path of the rule
        @type min: string of float/int, int or float. will be convert to float
        @param min: optional minimal value that if value goes below it send
            an alert to Sentry
        @type max: string of float/int, int or float. will be convert to float
        @param max: optional maximal value that if value goes over it send
            an alert to Sentry
        """
        self.name = name
        # counters that can be used to debug rule
        self.counter_errors = 0
        self.counter_pass = 0

        # force min and max to be float
        try:
            self.min = float(min)
        except TypeError:
            self.min = None
        try:
            self.max = float(max)
        except TypeError:
            self.max = None

        if self.min is None and self.max is None:
            raise InvalidRule("%s: %s: both min and max are unset or invalid"
                              % (name, path))

        if self.min is not None and self.max is not None:
            if self.min > self.max:
                raise InvalidRule("min %.1f is larger than max %.1f" % (
                    self.min, self.max))

        # compile path regular expression
        self.regexp = re.compile(r'(?P<prefix>.*)\.(?P<path>%s)$' % path)

    def process(self, metric, handler):
        """
        process a single diamond metric
        @type metric: diamond.metric.Metric
        @param metric: metric to process
        @type handler: diamond.handler.sentry.SentryHandler
        @param handler: configured Sentry graphite handler
        @rtype None
        """
        match = self.regexp.match(metric.path)
        if match:
            minimum = Minimum(metric.value, self.min)
            maximum = Maximum(metric.value, self.max)

            if minimum.is_error or maximum.is_error:
                self.counter_errors += 1
                message = "%s Warning on %s: %.1f" % (self.name,
                                                      handler.hostname,
                                                      metric.value)
                culprit = "%s %s" % (handler.hostname, match.group('path'))
                handler.raven_logger.error(message, extra={
                    'culprit': culprit,
                    'data': {
                        'metric prefix': match.group('prefix'),
                        'metric path': match.group('path'),
                        'minimum check': minimum.verbose_message,
                        'maximum check': maximum.verbose_message,
                        'metric original path': metric.path,
                        'metric value': metric.value,
                        'metric precision': metric.precision,
                        'metric timestamp': metric.timestamp,
                        'minimum threshold': self.min,
                        'maximum threshold': self.max,
                        'path regular expression': self.regexp.pattern,
                        'total errors': self.counter_errors,
                        'total pass': self.counter_pass,
                        'hostname': handler.hostname
                    }
                }
                )
            else:
                self.counter_pass += 1

    def __repr__(self):
        return '%s: min:%s max:%s %s' % (self.name, self.min, self.max,
                                         self.regexp.pattern)


class SentryHandler(Handler):
    """
    Diamond handler that check if a metric goes too low or too high
    """

    # valid key name in rules sub-section
    VALID_RULES_KEYS = ('name', 'path', 'min', 'max')

    def __init__(self, config=None):
        """
        @type config: configobj.ConfigObj
        """
        Handler.__init__(self, config)
        if not raven:
            return
        # init sentry/raven
        self.sentry_log_handler = raven.handlers.logging.SentryHandler(
            self.config['dsn'])
        self.raven_logger = logging.getLogger(self.__class__.__name__)
        self.raven_logger.addHandler(self.sentry_log_handler)
        self.configure_sentry_errors()
        self.rules = self.compile_rules()
        self.hostname = get_hostname(self.config)
        if not len(self.rules):
            self.log.warning("No rules, this graphite handler is unused")

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(SentryHandler, self).get_default_config_help()

        config.update({
            'dsn': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(SentryHandler, self).get_default_config()

        config.update({
            'dsn': '',
        })

        return config

    def compile_rules(self):
        """
        Compile alert rules
        @rtype list of Rules
        """
        output = []
        # validate configuration, skip invalid section
        for key_name, section in self.config.items():
            rule = self.compile_section(section)
            if rule is not None:
                output.append(rule)
        return output

    def compile_section(self, section):
        """
        Validate if a section is a valid rule
        @type section: configobj.Section
        @param section: section to validate
        @rtype Rule or None
        @return None if invalid
        """
        if section.__class__ != Section:
            # not a section, just skip
            return

        # name and path are mandatory
        keys = section.keys()
        for key in ('name', 'path'):
            if key not in keys:
                self.log.warning("section %s miss key '%s' ignore", key,
                                 section.name)
                return

        # just warn if invalid key in section
        for key in keys:
            if key not in self.VALID_RULES_KEYS:
                self.log.warning("invalid key %s in section %s",
                                 key, section.name)

        # need at least a min or a max
        if 'min' not in keys and 'max' not in keys:
            self.log.warning("either 'min' or 'max' is defined in %s",
                             section.name)
            return

        # add rule to the list
        kwargs = {
            'name': section['name'],
            'path': section['path']
        }
        for argument in ('min', 'max'):
            try:
                kwargs[argument] = section[argument]
            except KeyError:
                pass

        # init rule
        try:
            return Rule(**kwargs)
        except InvalidRule, err:
            self.log.error(str(err))

    def configure_sentry_errors(self):
        """
        Configure sentry.errors to use the same loggers as the root handler
        @rtype: None
        """
        sentry_errors_logger = logging.getLogger('sentry.errors')
        root_logger = logging.getLogger()
        for handler in root_logger.handlers:
            sentry_errors_logger.addHandler(handler)

    def process(self, metric):
        """
        process a single metric
        @type metric: diamond.metric.Metric
        @param metric: metric to process
        @rtype None
        """
        for rule in self.rules:
            rule.process(metric, self)

    def __repr__(self):
        return "SentryHandler '%s' %d rules" % (
            self.sentry_log_handler.client.servers, len(self.rules))

########NEW FILE########
__FILENAME__ = statsite
# coding=utf-8

"""
Send metrics to a [Statsite](https://github.com/armon/statsite/)
using the default interface.

Statsite
========

This is a stats aggregation server. Statsite is based heavily
on Etsy's [StatsD](https://github.com/etsy/statsd). This is
a re-implementation of the Python version of
[statsite](https://github.com/kiip/statsite).

Features
--------

* Basic key/value metrics
* Send timer data, statsite will calculate:
  - Mean
  - Min/Max
  - Standard deviation
  - Median, Percentile 95, Percentile 99
* Send counters that statsite will aggregate


Architecture
-------------

Statsite is designed to be both highly performant,
and very flexible. To achieve this, it implements the stats
collection and aggregation in pure C, using libev to be
extremely fast. This allows it to handle hundreds of connections,
and millions of metrics. After each flush interval expires,
statsite performs a fork/exec to start a new stream handler
invoking a specified application. Statsite then streams the
aggregated metrics over stdin to the application, which is
free to handle the metrics as it sees fit.

This allows statsite to aggregate metrics and then ship metrics
to any number of sinks (Graphite, SQL databases, etc). There
is an included Python script that ships metrics to graphite.

Additionally, statsite tries to minimize memory usage by not
storing all the metrics that are received. Counter values are
aggregated as they are received, and timer values are stored
and aggregated using the Cormode-Muthurkrishnan algorithm from
"Effective Computation of Biased Quantiles over Data Streams".
This means that the percentile values are not perfectly accurate,
and are subject to a specifiable error epsilon. This allows us to
store only a fraction of the samples.

"""

from Handler import Handler
import socket


class StatsiteHandler(Handler):
    """
    Implements the abstract Handler class, sending data to statsite
    """
    RETRY = 3

    def __init__(self, config=None):
        """
        Create a new instance of the StatsiteHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        # Initialize Data
        self.socket = None

        # Initialize Options
        self.host = self.config['host']
        self.tcpport = int(self.config['tcpport'])
        self.udpport = int(self.config['udpport'])
        self.timeout = int(self.config['timeout'])

        # Connect
        self._connect()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(StatsiteHandler, self).get_default_config_help()

        config.update({
            'host': '',
            'tcpport': '',
            'udpport': '',
            'timeout': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(StatsiteHandler, self).get_default_config()

        config.update({
            'host': '',
            'tcpport': 1234,
            'udpport': 1234,
            'timeout': 5,
        })

        return config

    def __del__(self):
        """
        Destroy instance of the StatsiteHandler class
        """
        self._close()

    def process(self, metric):
        """
        Process a metric by sending it to statsite
        """
        # Just send the data as a string
        self._send(str(metric))

    def _send(self, data):
        """
        Send data to statsite. Data that can not be sent will be queued.
        """
        retry = self.RETRY
        # Attempt to send any data in the queue
        while retry > 0:
            # Check socket
            if not self.socket:
                # Log Error
                self.log.error("StatsiteHandler: Socket unavailable.")
                # Attempt to restablish connection
                self._connect()
                # Decrement retry
                retry -= 1
                # Try again
                continue
            try:
                # Send data to socket
                data = data.split()
                data = data[0] + ":" + data[1] + "|kv\n"
                self.socket.sendall(data)
                # Done
                break
            except socket.error, e:
                # Log Error
                self.log.error("StatsiteHandler: Failed sending data. %s.", e)
                # Attempt to restablish connection
                self._close()
                # Decrement retry
                retry -= 1
                # try again
                continue

    def _connect(self):
        """
        Connect to the statsite server
        """
        # Create socket
        if self.udpport > 0:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            self.port = self.udpport
        elif self.tcpport > 0:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.port = self.tcpport
        if socket is None:
            # Log Error
            self.log.error("StatsiteHandler: Unable to create socket.")
            # Close Socket
            self._close()
            return
        # Set socket timeout
        self.socket.settimeout(self.timeout)
        # Connect to statsite server
        try:
            self.socket.connect((self.host, self.port))
            # Log
            self.log.debug("Established connection to statsite server %s:%d",
                           self.host, self.port)
        except Exception, ex:
            # Log Error
            self.log.error("StatsiteHandler: Failed to connect to %s:%i. %s",
                           self.host, self.port, ex)
            # Close Socket
            self._close()
            return

    def _close(self):
        """
        Close the socket
        """
        if self.socket is not None:
            self.socket.close()
        self.socket = None

########NEW FILE########
__FILENAME__ = stats_d
# coding=utf-8

"""
Implements the abstract Handler class, sending data to statsd.
This is a UDP service, sending datagrams.  They may be lost.
It's OK.

#### Dependencies

 * [python-statsd](http://pypi.python.org/pypi/python-statsd/)
 * [statsd](https://github.com/etsy/statsd) v0.1.1 or newer.

#### Configuration

Enable this handler

 * handlers = diamond.handler.stats_d.StatsdHandler


#### Notes


The handler file is named an odd stats_d.py because of an import issue with
having the python library called statsd and this handler's module being called
statsd, so we use an odd name for this handler. This doesn't affect the usage
of this handler.

"""

from Handler import Handler
import logging
try:
    import statsd
    statsd  # Pyflakes
except ImportError:
    statsd = None


class StatsdHandler(Handler):

    def __init__(self, config=None):
        """
        Create a new instance of the StatsdHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)
        logging.debug("Initialized statsd handler.")

        if not statsd:
            self.log.error('statsd import failed. Handler disabled')

        # Initialize Options
        self.host = self.config['host']
        self.port = int(self.config['port'])
        self.batch_size = int(self.config['batch'])
        self.metrics = []
        self.old_values = {}

        # Connect
        self._connect()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(StatsdHandler, self).get_default_config_help()

        config.update({
            'host': '',
            'port': '',
            'batch': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(StatsdHandler, self).get_default_config()

        config.update({
            'host': '',
            'port': 1234,
            'batch': 1,
        })

        return config

    def process(self, metric):
        """
        Process a metric by sending it to statsd
        """

        self.metrics.append(metric)

        if len(self.metrics) >= self.batch_size:
            self._send()

    def _send(self):
        """
        Send data to statsd. Fire and forget.  Cross fingers and it'll arrive.
        """
        if not statsd:
            return
        for metric in self.metrics:

            # Split the path into a prefix and a name
            # to work with the statsd module's view of the world.
            # It will get re-joined by the python-statsd module.
            #
            # For the statsd module, you specify prefix in the constructor
            # so we just use the full metric path.
            (prefix, name) = metric.path.rsplit(".", 1)
            logging.debug("Sending %s %s|g", name, metric.value)

            if metric.metric_type == 'GAUGE':
                if hasattr(statsd, 'StatsClient'):
                    self.connection.gauge(metric.path, metric.value)
                else:
                    statsd.Gauge(prefix, self.connection).send(
                        name, metric.value)
            else:
                # To send a counter, we need to just send the delta
                # but without any time delta changes
                value = metric.raw_value
                if metric.path in self.old_values:
                    value = value - self.old_values[metric.path]
                self.old_values[metric.path] = metric.raw_value

                if hasattr(statsd, 'StatsClient'):
                    self.connection.incr(metric.path, value)
                else:
                    statsd.Counter(prefix, self.connection).increment(
                        name, value)

        self.metrics = []

    def flush(self):
        """Flush metrics in queue"""
        self._send()

    def _connect(self):
        """
        Connect to the statsd server
        """
        if not statsd:
            return

        if hasattr(statsd, 'StatsClient'):
            self.connection = statsd.StatsClient(
                host=self.host,
                port=self.port
            )
        else:
            # Create socket
            self.connection = statsd.Connection(
                host=self.host,
                port=self.port,
                sample_rate=1.0
            )

########NEW FILE########
__FILENAME__ = testgraphitehandler
#!/usr/bin/python
# coding=utf-8
################################################################################

import time

from test import unittest
from mock import Mock
from mock import patch
from mock import call

import configobj

from diamond.handler.graphite import GraphiteHandler
from diamond.metric import Metric


class TestGraphiteHandler(unittest.TestCase):

    def test_single_metric(self):
        config = configobj.ConfigObj()
        config['host'] = 'graphite.example.com'
        config['batch'] = 1

        metric = Metric('servers.com.example.www.cpu.total.idle',
                        0, timestamp=1234567, host='will-be-ignored')

        expected_data = [
            call("servers.com.example.www.cpu.total.idle 0 1234567\n"),
        ]

        handler = GraphiteHandler(config)

        patch_sock = patch.object(handler, 'socket', True)
        sendmock = Mock()
        patch_send = patch.object(GraphiteHandler, '_send_data', sendmock)

        patch_sock.start()
        patch_send.start()
        handler.process(metric)
        patch_send.stop()
        patch_sock.stop()

        self.assertEqual(sendmock.call_count, len(expected_data))
        self.assertEqual(sendmock.call_args_list, expected_data)

    def test_multi_no_batching(self):
        config = configobj.ConfigObj()
        config['host'] = 'graphite.example.com'
        config['batch'] = 1

        metrics = [
            Metric('metricname1', 0, timestamp=123),
            Metric('metricname2', 0, timestamp=123),
            Metric('metricname3', 0, timestamp=123),
            Metric('metricname4', 0, timestamp=123),
        ]

        expected_data = [
            call("metricname1 0 123\n"),
            call("metricname2 0 123\n"),
            call("metricname3 0 123\n"),
            call("metricname4 0 123\n"),
        ]

        handler = GraphiteHandler(config)

        patch_sock = patch.object(handler, 'socket', True)
        sendmock = Mock()
        patch_send = patch.object(GraphiteHandler, '_send_data', sendmock)

        patch_sock.start()
        patch_send.start()
        for m in metrics:
            handler.process(m)
        patch_send.stop()
        patch_sock.stop()

        self.assertEqual(sendmock.call_count, len(expected_data))
        self.assertEqual(sendmock.call_args_list, expected_data)

    def test_multi_with_batching(self):
        config = configobj.ConfigObj()
        config['host'] = 'graphite.example.com'
        config['batch'] = 2

        metrics = [
            Metric('metricname1', 0, timestamp=123),
            Metric('metricname2', 0, timestamp=123),
            Metric('metricname3', 0, timestamp=123),
            Metric('metricname4', 0, timestamp=123),
        ]

        expected_data = [
            call("metricname1 0 123\nmetricname2 0 123\n"),
            call("metricname3 0 123\nmetricname4 0 123\n"),
        ]

        handler = GraphiteHandler(config)

        patch_sock = patch.object(handler, 'socket', True)
        sendmock = Mock()
        patch_send = patch.object(GraphiteHandler, '_send_data', sendmock)

        patch_sock.start()
        patch_send.start()
        for m in metrics:
            handler.process(m)
        patch_send.stop()
        patch_sock.stop()

        self.assertEqual(sendmock.call_count, len(expected_data))
        self.assertEqual(sendmock.call_args_list, expected_data)

    def test_backlog(self):
        config = configobj.ConfigObj()
        config['host'] = 'graphite.example.com'
        config['batch'] = 1

        # start trimming after X batchsizes in buffer
        config['max_backlog_multiplier'] = 4
        # when trimming: keep last X batchsizes
        config['trim_backlog_multiplier'] = 3

        metrics = [
            Metric('metricname1', 0, timestamp=123),
            Metric('metricname2', 0, timestamp=123),
            Metric('metricname3', 0, timestamp=123),
            Metric('metricname4', 0, timestamp=123),
            Metric('metricname5', 0, timestamp=123),
            Metric('metricname6', 0, timestamp=123),
            Metric('metricname7', 0, timestamp=123),
            Metric('metricname8', 0, timestamp=123),
        ]

        expected_data = [
            "metricname6 0 123\n",
            "metricname7 0 123\n",
            "metricname8 0 123\n",
        ]

        handler = GraphiteHandler(config)

        # simulate an unreachable graphite host
        # thus force backlog functionality
        connect_mock = Mock()
        patch_connect = patch.object(GraphiteHandler, '_connect', connect_mock)
        send_mock = Mock()
        patch_send = patch.object(GraphiteHandler, '_send_data', send_mock)

        patch_connect.start()
        patch_send.start()
        for m in metrics:
            handler.process(m)
        patch_send.stop()
        patch_connect.stop()

        self.assertEqual(connect_mock.call_count, len(metrics))
        self.assertEqual(send_mock.call_count, 0)
        self.assertEqual(handler.metrics, expected_data)

    def test_error_throttling(self):
        """
        This is more of a generic test checking that the _throttle_error method
        works as expected

        TODO: test that the graphite handler calls _throttle_error in the right
        circumstances.
        """
        config = configobj.ConfigObj()
        config['server_error_interval'] = '0.1'

        handler = GraphiteHandler(config)

        debug_mock = Mock()
        patch_debug = patch.object(handler.log, 'debug', debug_mock)
        error_mock = Mock()
        patch_error = patch.object(handler.log, 'error', error_mock)

        patch_debug.start()
        patch_error.start()

        calls = 5
        for _ in range(calls):
            handler._throttle_error('Error Message')

        # .error should have been called only once
        self.assertEqual(error_mock.call_count, 1)
        self.assertEqual(debug_mock.call_count, calls - 1)

        handler._reset_errors()

        debug_mock.reset_mock()
        error_mock.reset_mock()

        for _ in range(calls):
            handler._throttle_error('Error Message')
            time.sleep(0.065)

        # error should have been called 0.065 * 5 / 0.1 = 3 times
        self.assertEqual(error_mock.call_count, 3)
        self.assertEqual(debug_mock.call_count, 2)
        patch_debug.stop()
        patch_error.stop()

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = testriemann
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import unittest
from test import run_only
import configobj

from diamond.handler.riemann import RiemannHandler
from diamond.metric import Metric


def run_only_if_bernhard_is_available(func):
    try:
        import bernhard
        bernhard  # workaround for pyflakes issue #13
    except ImportError:
        bernhard = None
    pred = lambda: bernhard is not None
    return run_only(func, pred)


class TestRiemannHandler(unittest.TestCase):

    @run_only_if_bernhard_is_available
    def test_metric_to_riemann_event(self):
        config = configobj.ConfigObj()
        config['host'] = 'localhost'
        config['port'] = 5555

        handler = RiemannHandler(config)
        metric = Metric('servers.com.example.www.cpu.total.idle',
                        0,
                        timestamp=1234567,
                        host='com.example.www')

        event = handler._metric_to_riemann_event(metric)

        self.assertEqual(event, {
            'host': 'com.example.www',
            'service': 'servers.cpu.total.idle',
            'time': 1234567,
            'metric': 0.0,
            'ttl': None
        })

########NEW FILE########
__FILENAME__ = teststatsdhandler
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import unittest
from test import run_only
from mock import patch
from mock import ANY

import configobj

from diamond.handler.stats_d import StatsdHandler
from diamond.metric import Metric
try:
    import statsd
    statsd  # Pyflakes
except ImportError:
    pass


def run_only_if_statsd_is_available(func):
    try:
        import statsd
        statsd  # workaround for pyflakes issue #13
    except ImportError:
        statsd = None
    pred = lambda: statsd is not None
    return run_only(func, pred)


class TestStatsdHandler(unittest.TestCase):

    @run_only_if_statsd_is_available
    @patch('statsd.Client')
    def test_single_gauge(self, mock_client):
        instance = mock_client.return_value
        instance._send.return_value = 1

        config = configobj.ConfigObj()
        config['host'] = 'localhost'
        config['port'] = '9999'
        config['batch'] = 1

        metric = Metric('servers.com.example.www.cpu.total.idle',
                        123, raw_value=123, timestamp=1234567,
                        host='will-be-ignored', metric_type='GAUGE')

        expected_data = {
            'servers.com.example.www.cpu.total.idle': '123|g'
        }

        handler = StatsdHandler(config)
        handler.process(metric)
        mock_client._send.assert_called_with(ANY, expected_data)

    @run_only_if_statsd_is_available
    @patch('statsd.Client')
    def test_single_counter(self, mock_client):
        instance = mock_client.return_value
        instance._send.return_value = 1

        config = configobj.ConfigObj()
        config['host'] = 'localhost'
        config['port'] = '9999'
        config['batch'] = 1

        metric = Metric('servers.com.example.www.cpu.total.idle',
                        5, raw_value=123, timestamp=1234567,
                        host='will-be-ignored', metric_type='COUNTER')

        expected_data = {
            'servers.com.example.www.cpu.total.idle': '123|c'
        }

        handler = StatsdHandler(config)
        handler.process(metric)
        mock_client._send.assert_called_with(ANY, expected_data)

    @run_only_if_statsd_is_available
    @patch('statsd.Client')
    def test_multiple_counter(self, mock_client):
        instance = mock_client.return_value
        instance._send.return_value = 1

        config = configobj.ConfigObj()
        config['host'] = 'localhost'
        config['port'] = '9999'
        config['batch'] = 1

        metric1 = Metric('servers.com.example.www.cpu.total.idle',
                         5, raw_value=123, timestamp=1234567,
                         host='will-be-ignored', metric_type='COUNTER')

        metric2 = Metric('servers.com.example.www.cpu.total.idle',
                         7, raw_value=128, timestamp=1234567,
                         host='will-be-ignored', metric_type='COUNTER')

        expected_data1 = {
            'servers.com.example.www.cpu.total.idle': '123|c'
        }

        expected_data2 = {
            'servers.com.example.www.cpu.total.idle': '5|c'
        }

        handler = StatsdHandler(config)
        handler.process(metric1)
        mock_client._send.assert_called_with(ANY, expected_data1)

        handler.process(metric2)
        mock_client._send.assert_called_with(ANY, expected_data2)

########NEW FILE########
__FILENAME__ = tsdb
# coding=utf-8

"""
Send metrics to a [OpenTSDB](http://opentsdb.net/) server.

[OpenTSDB](http://opentsdb.net/) is a distributed, scalable Time Series
Database (TSDB) written on top of [HBase](http://hbase.org/). OpenTSDB was
written to address a common need: store, index and serve metrics collected from
computer systems (network gear, operating systems, applications) at a large
scale, and make this data easily accessible and graphable.

Thanks to HBase's scalability, OpenTSDB allows you to collect many thousands of
metrics from thousands of hosts and applications, at a high rate (every few
seconds). OpenTSDB will never delete or downsample data and can easily store
billions of data points. As a matter of fact, StumbleUpon uses it to keep track
of hundred of thousands of time series and collects over 1 billion data points
per day in their main production datacenter.

Imagine having the ability to quickly plot a graph showing the number of DELETE
statements going to your MySQL database along with the number of slow queries
and temporary files created, and correlate this with the 99th percentile of
your service's latency. OpenTSDB makes generating such graphs on the fly a
trivial operation, while manipulating millions of data point for very fine
grained, real-time monitoring.

==== Notes

We don't automatically make the metrics via mkmetric, so we recommand you run
with the null handler and log the output and extract the key values to mkmetric
yourself.

- enable it in `diamond.conf` :

`    handlers = diamond.handler.tsdb.TSDBHandler
`

"""

from Handler import Handler
import socket


class TSDBHandler(Handler):
    """
    Implements the abstract Handler class, sending data to graphite
    """
    RETRY = 3

    def __init__(self, config=None):
        """
        Create a new instance of the TSDBHandler class
        """
        # Initialize Handler
        Handler.__init__(self, config)

        # Initialize Data
        self.socket = None

        # Initialize Options
        self.host = self.config['host']
        self.port = int(self.config['port'])
        self.timeout = int(self.config['timeout'])
        self.metric_format = str(self.config['format'])
        self.tags = str(self.config['tags'])

        # Connect
        self._connect()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(TSDBHandler, self).get_default_config_help()

        config.update({
            'host': '',
            'port': '',
            'timeout': '',
            'format': '',
            'tags': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(TSDBHandler, self).get_default_config()

        config.update({
            'host': '',
            'port': 1234,
            'timeout': 5,
            'format': '{Collector}.{Metric} {timestamp} {value} hostname={host}'
                      '{tags}',
            'tags': '',
        })

        return config

    def __del__(self):
        """
        Destroy instance of the TSDBHandler class
        """
        self._close()

    def process(self, metric):
        """
        Process a metric by sending it to TSDB
        """

        metric_str = self.metric_format.format(
            Collector=metric.getCollectorPath(),
            Path=metric.path,
            Metric=metric.getMetricPath(),
            host=metric.host,
            timestamp=metric.timestamp,
            value=metric.value,
            tags=self.tags
        )
        # Just send the data as a string
        self._send("put " + str(metric_str) + "\n")

    def _send(self, data):
        """
        Send data to TSDB. Data that can not be sent will be queued.
        """
        retry = self.RETRY
        # Attempt to send any data in the queue
        while retry > 0:
            # Check socket
            if not self.socket:
                # Log Error
                self.log.error("TSDBHandler: Socket unavailable.")
                # Attempt to restablish connection
                self._connect()
                # Decrement retry
                retry -= 1
                # Try again
                continue
            try:
                # Send data to socket
                self.socket.sendall(data)
                # Done
                break
            except socket.error, e:
                # Log Error
                self.log.error("TSDBHandler: Failed sending data. %s.", e)
                # Attempt to restablish connection
                self._close()
                # Decrement retry
                retry -= 1
                # try again
                continue

    def _connect(self):
        """
        Connect to the TSDB server
        """
        # Create socket
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        if socket is None:
            # Log Error
            self.log.error("TSDBHandler: Unable to create socket.")
            # Close Socket
            self._close()
            return
        # Set socket timeout
        self.socket.settimeout(self.timeout)
        # Connect to graphite server
        try:
            self.socket.connect((self.host, self.port))
            # Log
            self.log.debug("Established connection to TSDB server %s:%d",
                           self.host, self.port)
        except Exception, ex:
            # Log Error
            self.log.error("TSDBHandler: Failed to connect to %s:%i. %s",
                           self.host, self.port, ex)
            # Close Socket
            self._close()
            return

    def _close(self):
        """
        Close the socket
        """
        if self.socket is not None:
            self.socket.close()
        self.socket = None

########NEW FILE########
__FILENAME__ = zmq_pubsub
# coding=utf-8

"""
Output the collected values to a Zer0MQ pub/sub channel
"""

from Handler import Handler

try:
    import zmq
    zmq  # Pyflakes
except ImportError:
    zmq = None


class zmqHandler (Handler):
    """
      Implements the abstract Handler class
      Sending data to a Zer0MQ pub channel
    """

    def __init__(self, config=None):

        """
          Create a new instance of zmqHandler class
        """

        # Initialize Handler
        Handler.__init__(self, config)

        if not zmq:
            self.log.error('zmq import failed. Handler disabled')

        # Initialize Data
        self.context = None

        self.socket = None

        # Initialize Options
        self.port = int(self.config['port'])

        # Create ZMQ pub socket and bind
        self._bind()

    def get_default_config_help(self):
        """
        Returns the help text for the configuration options for this handler
        """
        config = super(zmqHandler, self).get_default_config_help()

        config.update({
            'port': '',
        })

        return config

    def get_default_config(self):
        """
        Return the default config for the handler
        """
        config = super(zmqHandler, self).get_default_config()

        config.update({
            'port': 1234,
        })

        return config

    def _bind(self):
        """
           Create PUB socket and bind
        """
        if not zmq:
            return
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.PUB)
        self.socket.bind("tcp://*:%i" % self.port)

    def __del__(self):
        """
          Destroy instance of the zmqHandler class
        """
        pass

    def process(self, metric):
        """
          Process a metric and send it to zmq pub socket
        """
        if not zmq:
            return
        # Send the data as ......
        self.socket.send("%s" % str(metric))

########NEW FILE########
__FILENAME__ = timedrotating
# coding=utf-8

from logging.handlers import TimedRotatingFileHandler as TRFH
import sys


class TimedRotatingFileHandler(TRFH):

    def flush(self):
        try:
            super(TimedRotatingFileHandler, self).flush()
        except IOError:
            sys.stderr.write('TimedRotatingFileHandler received a IOError!')
            sys.stderr.flush()

########NEW FILE########
__FILENAME__ = metric
# coding=utf-8

import time
import re
import logging
from error import DiamondException


class Metric(object):

    _METRIC_TYPES = ['COUNTER', 'GAUGE']

    def __init__(self, path, value, raw_value=None, timestamp=None, precision=0,
                 host=None, metric_type='COUNTER', ttl=None):
        """
        Create new instance of the Metric class

        Takes:
            path=string: string the specifies the path of the metric
            value=[float|int]: the value to be submitted
            timestamp=[float|int]: the timestamp, in seconds since the epoch
            (as from time.time()) precision=int: the precision to apply.
            Generally the default (2) should work fine.
        """

        # Validate the path, value and metric_type submitted
        if (None in [path, value] or metric_type not in self._METRIC_TYPES):
            raise DiamondException(("Invalid parameter when creating new "
                                    "Metric with path: %r value: %r "
                                    "metric_type: %r")
                                   % (path, value, metric_type))

        # If no timestamp was passed in, set it to the current time
        if timestamp is None:
            timestamp = int(time.time())
        else:
            # If the timestamp isn't an int, then make it one
            if not isinstance(timestamp, int):
                try:
                    timestamp = int(timestamp)
                except ValueError, e:
                    raise DiamondException(("Invalid timestamp when "
                                            "creating new Metric %r: %s")
                                           % (path, e))

        # The value needs to be a float or an int.  If it is, great.  If not,
        # try to cast it to one of those.
        if not isinstance(value, (int, float)):
            try:
                if precision == 0:
                    value = round(float(value))
                else:
                    value = float(value)
            except ValueError, e:
                raise DiamondException(("Invalid value when creating new "
                                        "Metric %r: %s") % (path, e))

        self.path = path
        self.value = value
        self.raw_value = raw_value
        self.timestamp = timestamp
        self.precision = precision
        self.host = host
        self.metric_type = metric_type
        self.ttl = ttl

    def __repr__(self):
        """
        Return the Metric as a string
        """
        if not isinstance(self.precision, (int, long)):
            log = logging.getLogger('diamond')
            log.warn('Metric %s does not have a valid precision', self.path)
            self.precision = 0

        # Set the format string
        fstring = "%%s %%0.%if %%i\n" % self.precision

        # Return formated string
        return fstring % (self.path, self.value, self.timestamp)

    @classmethod
    def parse(cls, string):
        """
        Parse a string and create a metric
        """
        match = re.match(r'^(?P<name>[A-Za-z0-9\.\-_]+)\s+'
                         + '\(?P<value>[0-9\.]+)\s+'
                         + '(?P<timestamp>[0-9\.]+)(\n?)$',
                         string)
        try:
            groups = match.groupdict()
            # TODO: get precision from value string
            return Metric(groups['name'],
                          groups['value'],
                          float(groups['timestamp']))
        except:
            raise DiamondException(
                "Metric could not be parsed from string: %s." % string)

    def getPathPrefix(self):
        """
            Returns the path prefix path
            servers.host.cpu.total.idle
            return "servers"
        """
        # If we don't have a host name, assume it's just the first part of the
        # metric path
        if self.host is None:
            return self.path.split('.')[0]

        offset = self.path.index(self.host) - 1
        return self.path[0:offset]

    def getCollectorPath(self):
        """
            Returns collector path
            servers.host.cpu.total.idle
            return "cpu"
        """
        # If we don't have a host name, assume it's just the third part of the
        # metric path
        if self.host is None:
            return self.path.split('.')[2]

        offset = self.path.index(self.host)
        offset += len(self.host) + 1
        endoffset = self.path.index('.', offset)
        return self.path[offset:endoffset]

    def getMetricPath(self):
        """
            Returns the metric path after the collector name
            servers.host.cpu.total.idle
            return "total.idle"
        """
        # If we don't have a host name, assume it's just the fourth+ part of the
        # metric path
        if self.host is None:
            path = self.path.split('.')[3:]
            return '.'.join(path)

        prefix = '.'.join([self.getPathPrefix(), self.host,
                           self.getCollectorPath()])

        offset = len(prefix) + 1
        return self.path[offset:]

########NEW FILE########
__FILENAME__ = scheduler
# coding=utf-8

"""Module that provides a cron-like task scheduler.

This task scheduler is designed to be used from inside your own program.
You can schedule Python functions to be called at specific intervals or
days. It uses the standard 'sched' module for the actual task scheduling,
but provides much more:

* repeated tasks (at intervals, or on specific days)
* error handling (exceptions in tasks don't kill the scheduler)
* optional to run scheduler in its own thread or separate process
* optional to run a task in its own thread or separate process

If the threading module is available, you can use the various Threaded
variants of the scheduler and associated tasks. If threading is not
available, you could still use the forked variants. If fork is also
not available, all processing is done in a single process, sequentially.

There are three Scheduler classes:

    Scheduler    ThreadedScheduler    ForkedScheduler

You usually add new tasks to a scheduler using the add_interval_task or
add_daytime_task methods, with the appropriate processmethod argument
to select sequential, threaded or forked processing. NOTE: it is impossible
to add new tasks to a ForkedScheduler, after the scheduler has been started!
For more control you can use one of the following Task classes
and use schedule_task or schedule_task_abs:

    IntervalTask    ThreadedIntervalTask    ForkedIntervalTask
    SingleTask      ThreadedSingleTask      ForkedSingleTask
    WeekdayTask     ThreadedWeekdayTask     ForkedWeekdayTask
    MonthdayTask    ThreadedMonthdayTask    ForkedMonthdayTask

Kronos is the Greek God of Time.

Kronos scheduler (c) Irmen de Jong.
This version has been extracted from the Turbogears source repository
and slightly changed to be completely stand-alone again. Also some fixes
have been made to make it work on Python 2.6 (sched module changes).
The version in Turbogears is based on the original stand-alone Kronos.
This is open-source software, released under the MIT Software License:
http://www.opensource.org/licenses/mit-license.php

"""

__version__ = "2.0"

__all__ = [
    "DayTaskRescheduler",
    "ForkedIntervalTask",
    "ForkedMonthdayTask",
    "ForkedScheduler",
    "ForkedSingleTask",
    "ForkedTaskMixin",
    "ForkedWeekdayTask",
    "IntervalTask",
    "MonthdayTask",
    "Scheduler",
    "SingleTask",
    "Task",
    "ThreadedIntervalTask",
    "ThreadedMonthdayTask",
    "ThreadedScheduler",
    "ThreadedSingleTask",
    "ThreadedTaskMixin",
    "ThreadedWeekdayTask",
    "WeekdayTask",
]

import os
import sys
import sched
import time
import logging
import traceback
import weakref


class method:
    sequential = "sequential"
    forked = "forked"
    threaded = "threaded"


class SchedulerNotRunning(Exception):
    """Interrupt a running scheduler.

    This exception is used to break out of sched.sched.run when we
    are not running.

    """
    pass


class Scheduler:
    """The Scheduler itself."""

    def __init__(self):
        self.running = True
        self.log = logging.getLogger('diamond')
        self.sched = sched.scheduler(time.time, self.__delayfunc)

    def __delayfunc(self, delay):
        # This delay function is basically a time.sleep() that is
        # divided up, so that we can check the self.running flag while
        # delaying. There is an additional check in here to ensure that the
        # top item of the queue hasn't changed
        if delay < 10:
            time.sleep(delay)
        else:
            toptime = self._getqueuetoptime()
            endtime = time.time() + delay
            period = 5
            stoptime = endtime - period
            while (self.running
                    and stoptime > time.time()
                    and self._getqueuetoptime() == toptime):
                time.sleep(period)

            if self.running and self._getqueuetoptime() == toptime:
                now = time.time()
                if endtime > now:
                    time.sleep(endtime - now)

        if not self.running:
            raise SchedulerNotRunning()

    def _acquire_lock(self):
        pass

    def _release_lock(self):
        pass

    def add_interval_task(self, action, taskname, initialdelay, interval,
                          processmethod, args, kw, abs=False):
        """Add a new Interval Task to the schedule.

        A very short initialdelay or one of zero cannot be honored, you will
        see a slight delay before the task is first executed. This is because
        the scheduler needs to pick it up in its loop.

        """
        if initialdelay < 0 or interval < 1:
            raise ValueError("Delay or interval must be >0")
        # Select the correct IntervalTask class.
        # Not all types may be available!
        if processmethod == method.sequential:
            TaskClass = IntervalTask
        elif processmethod == method.threaded:
            TaskClass = ThreadedIntervalTask
        elif processmethod == method.forked:
            TaskClass = ForkedIntervalTask
        else:
            raise ValueError("Invalid processmethod")
        if not args:
            args = []
        if not kw:
            kw = {}
        task = TaskClass(taskname, interval, action, args, kw, abs)
        self.schedule_task(task, initialdelay)
        return task

    def add_single_task(self, action, taskname, initialdelay, processmethod,
                        args, kw):
        """Add a new task to the scheduler that will only be executed once."""
        if initialdelay < 0:
            raise ValueError("Delay must be >0")
        # Select the correct SingleTask class. Not all types may be available!
        if processmethod == method.sequential:
            TaskClass = SingleTask
        elif processmethod == method.threaded:
            TaskClass = ThreadedSingleTask
        elif processmethod == method.forked:
            TaskClass = ForkedSingleTask
        else:
            raise ValueError("Invalid processmethod")
        if not args:
            args = []
        if not kw:
            kw = {}
        task = TaskClass(taskname, action, args, kw)
        self.schedule_task(task, initialdelay)
        return task

    def add_daytime_task(self, action, taskname, weekdays, monthdays,
                         timeonday, processmethod, args, kw):
        """Add a new Day Task (Weekday or Monthday) to the schedule."""
        if weekdays and monthdays:
            raise ValueError("You can only specify weekdays or monthdays, "
                             "not both")
        if not args:
            args = []
        if not kw:
            kw = {}
        if weekdays:
            # Select the correct WeekdayTask class.
            # Not all types may be available!
            if processmethod == method.sequential:
                TaskClass = WeekdayTask
            elif processmethod == method.threaded:
                TaskClass = ThreadedWeekdayTask
            elif processmethod == method.forked:
                TaskClass = ForkedWeekdayTask
            else:
                raise ValueError("Invalid processmethod")
            task = TaskClass(taskname, weekdays, timeonday, action, args, kw)
        if monthdays:
            # Select the correct MonthdayTask class.
            # Not all types may be available!
            if processmethod == method.sequential:
                TaskClass = MonthdayTask
            elif processmethod == method.threaded:
                TaskClass = ThreadedMonthdayTask
            elif processmethod == method.forked:
                TaskClass = ForkedMonthdayTask
            else:
                raise ValueError("Invalid processmethod")
            task = TaskClass(taskname, monthdays, timeonday, action, args, kw)
        firsttime = task.get_schedule_time(True)
        self.schedule_task_abs(task, firsttime)
        return task

    def schedule_task(self, task, delay):
        """Add a new task to the scheduler with the given delay (seconds).

        Low-level method for internal use.

        """
        if self.running:
            # lock the sched queue, if needed
            self._acquire_lock()
            try:
                task.event = self.sched.enter(delay, 0, task,
                                             (weakref.ref(self),))
            finally:
                self._release_lock()
        else:
            task.event = self.sched.enter(delay, 0, task,
                                         (weakref.ref(self),))

    def schedule_task_abs(self, task, abstime):
        """Add a new task to the scheduler for the given absolute time value.

        Low-level method for internal use.

        """
        if self.running:
            # lock the sched queue, if needed
            self._acquire_lock()
            try:
                task.event = self.sched.enterabs(abstime, 0, task,
                                                (weakref.ref(self),))
            finally:
                self._release_lock()
        else:
            task.event = self.sched.enterabs(abstime, 0, task,
                                            (weakref.ref(self),))

    def start(self):
        """Start the scheduler."""
        self._run()

    def stop(self):
        """Remove all pending tasks and stop the Scheduler."""
        self.running = False
        # Pending tasks are removed in _run.

    def cancel(self, task):
        """Cancel given scheduled task."""
        self.sched.cancel(task.event)

    if sys.version_info >= (2, 6):
        # code for sched module of python 2.6+
        def _getqueuetoptime(self):
            return self.sched._queue[0].time

        def _clearschedqueue(self):
            self.sched._queue[:] = []

    else:
        # code for sched module of python 2.5 and older
        def _getqueuetoptime(self):
            return self.sched.queue[0][0]

        def _clearschedqueue(self):
            self.sched.queue[:] = []

    def _run(self):
        # Low-level run method to do the actual scheduling loop.
        while self.running:
            try:
                self.sched.run()
            except SchedulerNotRunning:
                self._clearschedqueue()
            except Exception, x:
                self.log.error(
                    "ERROR DURING SCHEDULER EXECUTION %s \n %s",
                    x,
                    "".join(traceback.format_exception(*sys.exc_info())))
            # queue is empty; sleep a short while before checking again
            if self.running:
                time.sleep(5)


class Task(object):
    """Abstract base class of all scheduler tasks"""

    def __init__(self, name, action, args, kw):
        """This is an abstract class!"""
        self.name = name
        self.action = action
        self.args = args
        self.kw = kw
        self.log = logging.getLogger('diamond')

    def __call__(self, schedulerref):
        """Execute the task action in the scheduler's thread."""
        try:
            self.execute()
        except Exception, x:
            self.handle_exception(x)
        self.reschedule(schedulerref())

    def reschedule(self, scheduler):
        """This method should be defined in one of the sub classes!"""
        raise NotImplementedError("You're using the abstract class 'Task',"
                                  " use a concrete class instead")

    def execute(self):
        """Execute the actual task."""
        self.action(*self.args, **self.kw)

    def handle_exception(self, exc):
        """Handle any exception that occured during task execution."""
        self.log.error("ERROR DURING TASK EXECUTION %s \n %s", exc,
                       "".join(traceback.format_exception(*sys.exc_info())))


class SingleTask(Task):
    """A task that only runs once."""

    def reschedule(self, scheduler):
        pass


class IntervalTask(Task):
    """A repeated task that occurs at certain intervals (in seconds)."""

    def __init__(self, name, interval, action, args=None, kw=None, abs=False):
        Task.__init__(self, name, action, args, kw)
        self.absolute = abs
        self.interval = interval
        self.duration = 0

    def execute(self):
        """ Execute the actual task."""
        start_time = time.time()
        self.action(*self.args, **self.kw)
        end_time = time.time()
        self.duration = int(end_time - start_time)

    def reschedule(self, scheduler):
        """Reschedule this task according to its interval (in seconds)."""
        if self.absolute and self.duration:
            if self.duration < self.interval:
                scheduler.schedule_task(self, self.interval - self.duration)
            else:
                scheduler.schedule_task(self, 0)
        else:
            scheduler.schedule_task(self, self.interval)


class DayTaskRescheduler:
    """A mixin class that contains the reschedule logic for the DayTasks."""

    def __init__(self, timeonday):
        self.timeonday = timeonday

    def get_schedule_time(self, today):
        """Calculate the time value at which this task is to be scheduled."""
        now = list(time.localtime())
        if today:
            # schedule for today. let's see if that is still possible
            if (now[3], now[4]) >= self.timeonday:
                # too bad, it will be tomorrow
                now[2] += 1
        else:
            # tomorrow
            now[2] += 1
        # set new time on day (hour,minute)
        now[3], now[4] = self.timeonday
        # seconds
        now[5] = 0
        return time.mktime(now)

    def reschedule(self, scheduler):
        """Reschedule this task according to the daytime for the task.

        The task is scheduled for tomorrow, for the given daytime.

        """
        # (The execute method in the concrete Task classes will check
        # if the current day is a day on which the task must run).
        abstime = self.get_schedule_time(False)
        scheduler.schedule_task_abs(self, abstime)


class WeekdayTask(DayTaskRescheduler, Task):
    """A task that is called at specific days in a week (1-7), at a fixed time
    on the day.

    """

    def __init__(self, name, weekdays, timeonday, action, args=None, kw=None):
        if type(timeonday) not in (list, tuple) or len(timeonday) != 2:
            raise TypeError("timeonday must be a 2-tuple (hour,minute)")
        if type(weekdays) not in (list, tuple):
            raise TypeError("weekdays must be a sequence of weekday numbers "
                            "1-7 (1 is Monday)")
        DayTaskRescheduler.__init__(self, timeonday)
        Task.__init__(self, name, action, args, kw)
        self.days = weekdays

    def execute(self):
        # This is called every day, at the correct time. We only need to
        # check if we should run this task today (this day of the week).
        weekday = time.localtime().tm_wday + 1
        if weekday in self.days:
            self.action(*self.args, **self.kw)


class MonthdayTask(DayTaskRescheduler, Task):
    """A task that is called at specific days in a month (1-31), at a fixed
    time on the day.

    """

    def __init__(self, name, monthdays, timeonday, action, args=None, kw=None):
        if type(timeonday) not in (list, tuple) or len(timeonday) != 2:
            raise TypeError("timeonday must be a 2-tuple (hour,minute)")
        if type(monthdays) not in (list, tuple):
            raise TypeError("monthdays must be a sequence of numbers 1-31")
        DayTaskRescheduler.__init__(self, timeonday)
        Task.__init__(self, name, action, args, kw)
        self.days = monthdays

    def execute(self):
        # This is called every day, at the correct time. We only need to
        # check if we should run this task today (this day of the month).
        if time.localtime().tm_mday in self.days:
            self.action(*self.args, **self.kw)


try:
    import threading

    class ThreadedScheduler(Scheduler):
        """A Scheduler that runs in its own thread."""

        def __init__(self):
            Scheduler.__init__(self)
            # we require a lock around the task queue
            self._lock = threading.Lock()

        def start(self):
            """Splice off a thread in which the scheduler will run."""
            self.thread = threading.Thread(target=self._run)
            self.thread.setDaemon(True)
            self.thread.start()

        def stop(self):
            """Stop the scheduler and wait for the thread to finish."""
            Scheduler.stop(self)
            try:
                self.thread.join()
            except AttributeError:
                pass

        def _acquire_lock(self):
            """Lock the thread's task queue."""
            self._lock.acquire()

        def _release_lock(self):
            """Release the lock on the thread's task queue."""
            self._lock.release()

    class ThreadedTaskMixin:
        """A mixin class to make a Task execute in a separate thread."""

        def __call__(self, schedulerref):
            """Execute the task action in its own thread."""
            threading.Thread(target=self.threadedcall).start()
            self.reschedule(schedulerref())

        def threadedcall(self):
            # This method is run within its own thread, so we have to
            # do the execute() call and exception handling here.
            try:
                self.execute()
            except Exception, x:
                self.handle_exception(x)

    class ThreadedIntervalTask(ThreadedTaskMixin, IntervalTask):
        """Interval Task that executes in its own thread."""

        def __init__(self, name, interval, action, args=None, kw=None,
                     abs=False):
            # Force abs to be False, as in threaded mode we reschedule
            # immediately.
            super(ThreadedIntervalTask, self).__init__(name, interval, action,
                                                       args=args, kw=kw,
                                                       abs=False)

    class ThreadedSingleTask(ThreadedTaskMixin, SingleTask):
        """Single Task that executes in its own thread."""
        pass

    class ThreadedWeekdayTask(ThreadedTaskMixin, WeekdayTask):
        """Weekday Task that executes in its own thread."""
        pass

    class ThreadedMonthdayTask(ThreadedTaskMixin, MonthdayTask):
        """Monthday Task that executes in its own thread."""
        pass

except ImportError:
    # threading is not available
    pass

if hasattr(os, "fork"):
    import signal

    class ForkedScheduler(Scheduler):
        """A Scheduler that runs in its own forked process."""

        def __del__(self):
            if hasattr(self, "childpid"):
                os.kill(self.childpid, signal.SIGKILL)

        def start(self):
            """Fork off a new process in which the scheduler will run."""
            pid = os.fork()
            if pid == 0:
                # we are the child
                signal.signal(signal.SIGUSR1, self.signalhandler)
                self._run()
                os._exit(0)
            else:
                # we are the parent
                self.childpid = pid
                # can no longer insert in the scheduler queue
                del self.sched

        def stop(self):
            """Stop the scheduler and wait for the process to finish."""
            os.kill(self.childpid, signal.SIGUSR1)
            os.waitpid(self.childpid, 0)

        def signalhandler(self, sig, stack):
            Scheduler.stop(self)

    class ForkedTaskMixin:
        """A mixin class to make a Task execute in a separate process."""

        def __call__(self, schedulerref):
            """Execute the task action in its own process."""
            pid = os.fork()
            if pid == 0:
                # we are the child
                try:
                    self.execute()
                except Exception, x:
                    self.handle_exception(x)
                os._exit(0)
            else:
                # we are the parent
                self.reschedule(schedulerref())

    class ForkedIntervalTask(ForkedTaskMixin, IntervalTask):
        """Interval Task that executes in its own process."""

        def __init__(self, name, interval, action, args=None, kw=None,
                     abs=False):
            # Force abs to be False, as in forked mode we reschedule
            # immediately.
            super(ForkedIntervalTask, self).__init__(name, interval, action,
                                                     args=args, kw=kw,
                                                     abs=False)

    class ForkedSingleTask(ForkedTaskMixin, SingleTask):
        """Single Task that executes in its own process."""
        pass

    class ForkedWeekdayTask(ForkedTaskMixin, WeekdayTask):
        """Weekday Task that executes in its own process."""
        pass

    class ForkedMonthdayTask(ForkedTaskMixin, MonthdayTask):
        """Monthday Task that executes in its own process."""
        pass

if __name__ == "__main__":
    def testaction(arg):
        print ">>>TASK", arg, "sleeping 3 seconds"
        time.sleep(3)
        print "<<<END_TASK", arg

    s = ThreadedScheduler()
    s.add_interval_task(testaction,
                        "test action 1",
                        0,
                        4,
                        method.threaded,
                        ["task 1"],
                        None)
    s.start()

    print "Scheduler started, waiting 15 sec...."
    time.sleep(15)

    print "STOP SCHEDULER"
    s.stop()

    print "EXITING"

########NEW FILE########
__FILENAME__ = server
# coding=utf-8

import os
import sys
import time
import logging
import traceback
import configobj
import inspect

# Path Fix
sys.path.append(
    os.path.abspath(
        os.path.join(
            os.path.dirname(__file__), "../")))

import diamond

from diamond.collector import Collector
from diamond.handler.Handler import Handler
from diamond.scheduler import ThreadedScheduler
from diamond.util import load_class_from_name


class Server(object):
    """
    Server class loads and starts Handlers and Collectors
    """

    def __init__(self, config):
        # Initialize Logging
        self.log = logging.getLogger('diamond')
        # Initialize Members
        self.config = config
        self.running = False
        self.handlers = []
        self.modules = {}
        self.tasks = {}
        # Initialize Scheduler
        self.scheduler = ThreadedScheduler()

    def load_config(self):
        """
        Load the full config
        """

        configfile = os.path.abspath(self.config['configfile'])
        config = configobj.ConfigObj(configfile)
        config['configfile'] = self.config['configfile']

        self.config = config

    def load_handler(self, fqcn):
        """
        Load Handler class named fqcn
        """
        # Load class
        cls = load_class_from_name(fqcn)
        # Check if cls is subclass of Handler
        if cls == Handler or not issubclass(cls, Handler):
            raise TypeError("%s is not a valid Handler" % fqcn)
        # Log
        self.log.debug("Loaded Handler: %s", fqcn)
        return cls

    def load_handlers(self):
        """
        Load handlers
        """
        if isinstance(self.config['server']['handlers'], basestring):
            handlers = [self.config['server']['handlers']]
            self.config['server']['handlers'] = handlers

        for h in self.config['server']['handlers']:
            try:
                # Load Handler Class
                cls = self.load_handler(h)

                # Initialize Handler config
                handler_config = configobj.ConfigObj()
                # Merge default Handler default config
                handler_config.merge(self.config['handlers']['default'])
                # Check if Handler config exists
                if cls.__name__ in self.config['handlers']:
                    # Merge Handler config section
                    handler_config.merge(self.config['handlers'][cls.__name__])

                # Check for config file in config directory
                configfile = os.path.join(
                    self.config['server']['handlers_config_path'],
                    cls.__name__) + '.conf'
                if os.path.exists(configfile):
                    # Merge Collector config file
                    handler_config.merge(configobj.ConfigObj(configfile))

                # Initialize Handler class
                self.handlers.append(cls(handler_config))

            except (ImportError, SyntaxError):
                # Log Error
                self.log.debug("Failed to load handler %s. %s", h,
                               traceback.format_exc())
                continue

    def load_collector(self, fqcn):
        """
        Load Collector class named fqcn
        """
        # Load class
        cls = load_class_from_name(fqcn)
        # Check if cls is subclass of Collector
        if cls == Collector or not issubclass(cls, Collector):
            raise TypeError("%s is not a valid Collector" % fqcn)
        # Log
        self.log.debug("Loaded Collector: %s", fqcn)
        return cls

    def load_include_path(self, path):
        """
        Scan for and add paths to the include path
        """
        # Verify the path is valid
        if not os.path.isdir(path):
            return
        # Add path to the system path, to avoid name clashes
        # with mysql-connector for example ...
        sys.path.insert(1, path)
        # Load all the files in path
        for f in os.listdir(path):
            # Are we a directory? If so process down the tree
            fpath = os.path.join(path, f)
            if os.path.isdir(fpath):
                self.load_include_path(fpath)

    def load_collectors(self, path, filter=None):
        """
        Scan for collectors to load from path
        """
        # Initialize return value
        collectors = {}

        # Get a list of files in the directory, if the directory exists
        if not os.path.exists(path):
            raise OSError("Directory does not exist: %s" % path)

        if path.endswith('tests') or path.endswith('fixtures'):
            return collectors

        # Log
        self.log.debug("Loading Collectors from: %s", path)

        # Load all the files in path
        for f in os.listdir(path):

            # Are we a directory? If so process down the tree
            fpath = os.path.join(path, f)
            if os.path.isdir(fpath):
                subcollectors = self.load_collectors(fpath)
                for key in subcollectors:
                    collectors[key] = subcollectors[key]

            # Ignore anything that isn't a .py file
            elif (os.path.isfile(fpath)
                  and len(f) > 3
                  and f[-3:] == '.py'
                  and f[0:4] != 'test'
                  and f[0] != '.'):

                # Check filter
                if filter and os.path.join(path, f) != filter:
                    continue

                modname = f[:-3]

                # Stat module file to get mtime
                st = os.stat(os.path.join(path, f))
                mtime = st.st_mtime
                # Check if module has been loaded before
                if modname in self.modules:
                    # Check if file mtime is newer then the last loaded verison
                    if mtime <= self.modules[modname]:
                        # Module hasn't changed
                        # Log
                        self.log.debug("Found %s, but it hasn't changed.",
                                       modname)
                        continue

                try:
                    # Import the module
                    mod = __import__(modname, globals(), locals(), ['*'])
                except (ImportError, SyntaxError):
                    # Log error
                    self.log.error("Failed to import module: %s. %s", modname,
                                   traceback.format_exc())
                    continue

                # Update module mtime
                self.modules[modname] = mtime
                # Log
                self.log.debug("Loaded Module: %s", modname)

                # Find all classes defined in the module
                for attrname in dir(mod):
                    attr = getattr(mod, attrname)
                    # Only attempt to load classes that are infact classes
                    # are Collectors but are not the base Collector class
                    if (inspect.isclass(attr)
                            and issubclass(attr, Collector)
                            and attr != Collector):
                        if attrname.startswith('parent_'):
                            continue
                        # Get class name
                        fqcn = '.'.join([modname, attrname])
                        try:
                            # Load Collector class
                            cls = self.load_collector(fqcn)
                            # Add Collector class
                            collectors[cls.__name__] = cls
                        except Exception:
                            # Log error
                            self.log.error("Failed to load Collector: %s. %s",
                                           fqcn, traceback.format_exc())
                            continue

        # Return Collector classes
        return collectors

    def init_collector(self, cls):
        """
        Initialize collector
        """
        collector = None
        try:
            # Initialize Collector
            collector = cls(self.config, self.handlers)
            # Log
            self.log.debug("Initialized Collector: %s", cls.__name__)
        except Exception:
            # Log error
            self.log.error("Failed to initialize Collector: %s. %s",
                           cls.__name__, traceback.format_exc())

        # Return collector
        return collector

    def schedule_collector(self, c, interval_task=True):
        """
        Schedule collector
        """
        # Check collector is for realz
        if c is None:
            self.log.warn("Skipped loading invalid Collector: %s",
                          c.__class__.__name__)
            return

        if c.config['enabled'] is not True:
            self.log.debug("Skipped loading disabled Collector: %s",
                           c.__class__.__name__)
            return

        # Get collector schedule
        for name, schedule in c.get_schedule().items():
            # Get scheduler args
            func, args, splay, interval = schedule

            # Check if Collecter with same name has already been scheduled
            if name in self.tasks:
                self.scheduler.cancel(self.tasks[name])
                # Log
                self.log.debug("Canceled task: %s", name)

            method = diamond.scheduler.method.sequential

            if 'method' in c.config:
                if c.config['method'] == 'Threaded':
                    method = diamond.scheduler.method.threaded
                elif c.config['method'] == 'Forked':
                    method = diamond.scheduler.method.forked

            # Schedule Collector
            if interval_task:
                task = self.scheduler.add_interval_task(func,
                                                        name,
                                                        splay,
                                                        interval,
                                                        method,
                                                        args,
                                                        None,
                                                        True)
            else:
                task = self.scheduler.add_single_task(func,
                                                      name,
                                                      splay,
                                                      method,
                                                      args,
                                                      None)

            # Log
            self.log.debug("Scheduled task: %s", name)
            # Add task to list
            self.tasks[name] = task

    def run(self):
        """
        Load handler and collector classes and then start collectors
        """

        # Set Running Flag
        self.running = True

        # Load handlers
        if 'handlers_path' in self.config['server']:
            handlers_path = self.config['server']['handlers_path']
            self.load_include_path(handlers_path)
        self.load_handlers()

        # Load config
        self.load_config()

        # Load collectors
        collectors_path = self.config['server']['collectors_path']
        self.load_include_path(collectors_path)
        collectors = self.load_collectors(collectors_path)

        # Setup Collectors
        for cls in collectors.values():
            # Initialize Collector
            c = self.init_collector(cls)
            # Schedule Collector
            self.schedule_collector(c)

        # Start main loop
        self.mainloop()

    def run_one(self, file):
        """
        Run given collector once and then exit
        """
        # Set Running Flag
        self.running = True

        # Load handlers
        if 'handlers_path' in self.config['server']:
            handlers_path = self.config['server']['handlers_path']
            self.load_include_path(handlers_path)
        self.load_handlers()

        # Overrides collector config dir
        collector_config_path = os.path.abspath(os.path.dirname(file))
        self.config['server']['collectors_config_path'] = collector_config_path

        # Load config
        self.load_config()

        # Load collectors
        if os.path.dirname(file) == '':
            tmp_path = self.config['server']['collectors_path']
            filter_out = True
        else:
            tmp_path = os.path.dirname(file)
            filter_out = False
        self.load_include_path(tmp_path)
        collectors = self.load_collectors(tmp_path, file)
        # if file is a full path, rather than a collector name, only the
        # collector(s) in that path are instantiated, and there's no need to
        # filter extraneous ones from the collectors dictionary
        if filter_out:
            for item in collectors.keys():
                if not item.lower() in file.lower():
                    del collectors[item]

        # Setup Collectors
        for cls in collectors.values():
            # Initialize Collector
            c = self.init_collector(cls)

            # Schedule collector
            self.schedule_collector(c, False)

        # Start main loop
        self.mainloop(False)

    def mainloop(self, reload=True):

        # Start scheduler
        self.scheduler.start()

        # Log
        self.log.info('Started task scheduler.')

        # Initialize reload timer
        time_since_reload = 0

        # Main Loop
        while self.running:
            time.sleep(1)
            time_since_reload += 1

            # Check if its time to reload collectors
            if (reload
                    and time_since_reload
                    > int(self.config['server']['collectors_reload_interval'])):
                self.log.debug("Reloading config.")
                self.load_config()
                # Log
                self.log.debug("Reloading collectors.")
                # Load collectors
                collectors_path = self.config['server']['collectors_path']
                collectors = self.load_collectors(collectors_path)
                # Setup any Collectors that were loaded
                for cls in collectors.values():
                    # Initialize Collector
                    c = self.init_collector(cls)
                    # Schedule Collector
                    self.schedule_collector(c)

                # Reset reload timer
                time_since_reload = 0

            # Is the queue empty and we won't attempt to reload it? Exit
            if not reload and len(self.scheduler.sched._queue) == 0:
                self.running = False

        # Log
        self.log.debug('Stopping task scheduler.')
        # Stop scheduler
        self.scheduler.stop()
        # Log
        self.log.info('Stopped task scheduler.')
        # Log
        self.log.debug("Exiting.")

    def stop(self):
        """
        Close all connections and terminate threads.
        """
        # Set Running Flag
        self.running = False

########NEW FILE########
__FILENAME__ = testcollector
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import unittest
import configobj

from diamond.collector import Collector


class BaseCollectorTest(unittest.TestCase):

    def test_SetCustomHostname(self):
        config = configobj.ConfigObj()
        config['server'] = {}
        config['server']['collectors_config_path'] = ''
        config['collectors'] = {}
        config['collectors']['default'] = {
            'hostname': 'custom.localhost',
        }
        c = Collector(config, [])
        self.assertEquals('custom.localhost', c.get_hostname())

########NEW FILE########
__FILENAME__ = testmetric
#!/usr/bin/python
# coding=utf-8
################################################################################

from test import unittest

from diamond.metric import Metric


class TestMetric(unittest.TestCase):

    def testgetPathPrefix(self):
        metric = Metric('servers.com.example.www.cpu.total.idle',
                        0,
                        host='com.example.www')

        actual_value = metric.getPathPrefix()
        expected_value = 'servers'

        message = 'Actual %s, expected %s' % (actual_value, expected_value)
        self.assertEqual(actual_value, expected_value, message)

    def testgetPathPrefixCustom(self):
        metric = Metric('custom.path.prefix.com.example.www.cpu.total.idle',
                        0,
                        host='com.example.www')

        actual_value = metric.getPathPrefix()
        expected_value = 'custom.path.prefix'

        message = 'Actual %s, expected %s' % (actual_value, expected_value)
        self.assertEqual(actual_value, expected_value, message)

    def testgetCollectorPath(self):
        metric = Metric('servers.com.example.www.cpu.total.idle',
                        0,
                        host='com.example.www')

        actual_value = metric.getCollectorPath()
        expected_value = 'cpu'

        message = 'Actual %s, expected %s' % (actual_value, expected_value)
        self.assertEqual(actual_value, expected_value, message)

    def testgetMetricPath(self):
        metric = Metric('servers.com.example.www.cpu.total.idle',
                        0,
                        host='com.example.www')

        actual_value = metric.getMetricPath()
        expected_value = 'total.idle'

        message = 'Actual %s, expected %s' % (actual_value, expected_value)
        self.assertEqual(actual_value, expected_value, message)

    # Test hostname of none
    def testgetPathPrefixHostNone(self):
        metric = Metric('servers.host.cpu.total.idle',
                        0)

        actual_value = metric.getPathPrefix()
        expected_value = 'servers'

        message = 'Actual %s, expected %s' % (actual_value, expected_value)
        self.assertEqual(actual_value, expected_value, message)

    def testgetCollectorPathHostNone(self):
        metric = Metric('servers.host.cpu.total.idle',
                        0)

        actual_value = metric.getCollectorPath()
        expected_value = 'cpu'

        message = 'Actual %s, expected %s' % (actual_value, expected_value)
        self.assertEqual(actual_value, expected_value, message)

    def testgetMetricPathHostNone(self):
        metric = Metric('servers.host.cpu.total.idle',
                        0)

        actual_value = metric.getMetricPath()
        expected_value = 'total.idle'

        message = 'Actual %s, expected %s' % (actual_value, expected_value)
        self.assertEqual(actual_value, expected_value, message)

########NEW FILE########
__FILENAME__ = util
# coding=utf-8

import os
import sys
import inspect


def get_diamond_version():
    try:
        from diamond.version import __VERSION__
        return __VERSION__
    except ImportError:
        return "Unknown"


def load_modules_from_path(path):
    """
    Import all modules from the given directory
    """
    # Check and fix the path
    if path[-1:] != '/':
        path += '/'

    # Get a list of files in the directory, if the directory exists
    if not os.path.exists(path):
        raise OSError("Directory does not exist: %s" % path)

    # Add path to the system path
    sys.path.append(path)
    # Load all the files in path
    for f in os.listdir(path):
        # Ignore anything that isn't a .py file
        if len(f) > 3 and f[-3:] == '.py':
            modname = f[:-3]
            # Import the module
            __import__(modname, globals(), locals(), ['*'])


def load_class_from_name(fqcn):
    # Break apart fqcn to get module and classname
    paths = fqcn.split('.')
    modulename = '.'.join(paths[:-1])
    classname = paths[-1]
    # Import the module
    __import__(modulename, globals(), locals(), ['*'])
    # Get the class
    cls = getattr(sys.modules[modulename], classname)
    # Check cls
    if not inspect.isclass(cls):
        raise TypeError("%s is not a class" % fqcn)
    # Return class
    return cls

########NEW FILE########
__FILENAME__ = test
#!/usr/bin/env python
# coding=utf-8
###############################################################################

import os
import sys
import unittest
import inspect
import traceback
import optparse
import logging
import configobj

try:
    import cPickle as pickle
    pickle  # workaround for pyflakes issue #13
except ImportError:
    import pickle as pickle

try:
    from cStringIO import StringIO
    StringIO  # workaround for pyflakes issue #13
except ImportError:
    from StringIO import StringIO

try:
    from setproctitle import setproctitle
    setproctitle  # workaround for pyflakes issue #13
except ImportError:
    setproctitle = None

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__))))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
                                             'src')))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
                                             'src', 'collectors')))


def run_only(func, predicate):
    if predicate():
        return func
    else:
        def f(arg):
            pass
        return f


def get_collector_config(key, value):
    config = configobj.ConfigObj()
    config['server'] = {}
    config['server']['collectors_config_path'] = ''
    config['collectors'] = {}
    config['collectors']['default'] = {}
    config['collectors']['default']['hostname_method'] = "uname_short"
    config['collectors'][key] = value
    return config


class CollectorTestCase(unittest.TestCase):

    def setDocExample(self, collector, metrics, defaultpath=None):
        if not len(metrics):
            return False

        filePath = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                'docs', 'collectors-' + collector + '.md')

        if not os.path.exists(filePath):
            return False

        if not os.access(filePath, os.W_OK):
            return False

        if not os.access(filePath, os.R_OK):
            return False

        try:
            fp = open(filePath, 'Ur')
            content = fp.readlines()
            fp.close()

            fp = open(filePath, 'w')
            for line in content:
                if line.strip() == '__EXAMPLESHERE__':
                    for metric in sorted(metrics.iterkeys()):

                        metricPath = 'servers.hostname.'

                        if defaultpath:
                            metricPath += defaultpath + '.'

                        metricPath += metric

                        metricPath = metricPath.replace('..', '.')
                        fp.write('%s %s\n' % (metricPath, metrics[metric]))
                else:
                    fp.write(line)
            fp.close()

        except IOError:
            return False
        return True

    def getFixtureDirPath(self):
        path = os.path.join(
            os.path.dirname(inspect.getfile(self.__class__)),
            'fixtures')
        return path

    def getFixturePath(self, fixture_name):
        path = os.path.join(self.getFixtureDirPath(),
                            fixture_name)
        if not os.access(path, os.R_OK):
            print "Missing Fixture " + path
        return path

    def getFixture(self, fixture_name):
        try:
            f = open(self.getFixturePath(fixture_name), 'r')
            data = StringIO(f.read())
            return data
        finally:
            f.close()

    def getFixtures(self):
        fixtures = []
        for root, dirnames, filenames in os.walk(self.getFixtureDirPath()):
            fixtures.append(os.path.join(root, dirnames, filenames))
        return fixtures

    def getPickledResults(self, results_name):
        try:
            f = open(self.getFixturePath(results_name), 'r')
            data = pickle.load(f)
            return data
        finally:
            f.close()

    def setPickledResults(self, results_name, data):
        pickle.dump(data, open(self.getFixturePath(results_name), "w+b"))

    def assertUnpublished(self, mock, key, value, expected_value=0):
        return self.assertPublished(mock, key, value, expected_value)

    def assertPublished(self, mock, key, value, expected_value=1):
        if type(mock) is list:
            for m in mock:
                calls = (filter(lambda x: x[0][0] == key, m.call_args_list))
                if len(calls) > 0:
                    break
        else:
            calls = filter(lambda x: x[0][0] == key, mock.call_args_list)

        actual_value = len(calls)
        message = '%s: actual number of calls %d, expected %d' % (
            key, actual_value, expected_value)

        self.assertEqual(actual_value, expected_value, message)

        if expected_value:
            actual_value = calls[0][0][1]
            expected_value = value
            precision = 0

            if isinstance(value, tuple):
                expected_value, precision = expected_value

            message = '%s: actual %r, expected %r' % (key,
                                                      actual_value,
                                                      expected_value)
            #print message

            if precision is not None:
                self.assertAlmostEqual(float(actual_value),
                                       float(expected_value),
                                       places=precision,
                                       msg=message)
            else:
                self.assertEqual(actual_value, expected_value, message)

    def assertUnpublishedMany(self, mock, dict, expected_value=0):
        return self.assertPublishedMany(mock, dict, expected_value)

    def assertPublishedMany(self, mock, dict, expected_value=1):
        for key, value in dict.iteritems():
            self.assertPublished(mock, key, value, expected_value)

        if type(mock) is list:
            for m in mock:
                m.reset_mock()
        else:
            mock.reset_mock()

    def assertUnpublishedMetric(self, mock, key, value, expected_value=0):
        return self.assertPublishedMetric(mock, key, value, expected_value)

    def assertPublishedMetric(self, mock, key, value, expected_value=1):
        calls = filter(lambda x: x[0][0].path.find(key) != -1,
                       mock.call_args_list)

        actual_value = len(calls)
        message = '%s: actual number of calls %d, expected %d' % (
            key, actual_value, expected_value)

        self.assertEqual(actual_value, expected_value, message)

        actual_value = calls[0][0][0].value
        expected_value = value
        precision = 0

        if isinstance(value, tuple):
            expected_value, precision = expected_value

        message = '%s: actual %r, expected %r' % (key,
                                                  actual_value,
                                                  expected_value)
        #print message

        if precision is not None:
            self.assertAlmostEqual(float(actual_value),
                                   float(expected_value),
                                   places=precision,
                                   msg=message)
        else:
            self.assertEqual(actual_value, expected_value, message)

    def assertUnpublishedMetricMany(self, mock, dict, expected_value=0):
        return self.assertPublishedMetricMany(mock, dict, expected_value)

    def assertPublishedMetricMany(self, mock, dict, expected_value=1):
        for key, value in dict.iteritems():
            self.assertPublishedMetric(mock, key, value, expected_value)

        mock.reset_mock()

collectorTests = {}


def getCollectorTests(path):
    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))

        if (os.path.isfile(cPath)
                and len(f) > 3
                and f[-3:] == '.py'
                and f[0:4] == 'test'):
            sys.path.append(os.path.dirname(cPath))
            sys.path.append(os.path.dirname(os.path.dirname(cPath)))
            modname = f[:-3]
            try:
                # Import the module
                collectorTests[modname] = __import__(modname,
                                                     globals(),
                                                     locals(),
                                                     ['*'])
                #print "Imported module: %s" % (modname)
            except Exception:
                print "Failed to import module: %s. %s" % (
                    modname, traceback.format_exc())
                continue

    for f in os.listdir(path):
        cPath = os.path.abspath(os.path.join(path, f))
        if os.path.isdir(cPath):
            getCollectorTests(cPath)

###############################################################################

if __name__ == "__main__":
    if setproctitle:
        setproctitle('test.py')

    # Disable log output for the unit tests
    log = logging.getLogger("diamond")
    log.addHandler(logging.StreamHandler(sys.stderr))
    log.disabled = True

    # Initialize Options
    parser = optparse.OptionParser()
    parser.add_option("-c",
                      "--collector",
                      dest="collector",
                      default="",
                      help="Run a single collector's unit tests")
    parser.add_option("-v",
                      "--verbose",
                      dest="verbose",
                      default=1,
                      action="count",
                      help="verbose")

    # Parse Command Line Args
    (options, args) = parser.parse_args()

    cPath = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                         'src',
                                         'collectors',
                                         options.collector))

    dPath = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                         'src',
                                         'diamond'))

    getCollectorTests(cPath)
    getCollectorTests(dPath)

    loader = unittest.TestLoader()
    tests = []
    for test in collectorTests:
        for name, c in inspect.getmembers(collectorTests[test],
                                          inspect.isclass):
            if not issubclass(c, unittest.TestCase):
                continue
            tests.append(loader.loadTestsFromTestCase(c))
    suite = unittest.TestSuite(tests)
    results = unittest.TextTestRunner(verbosity=options.verbose).run(suite)

    results = str(results)
    results = results.replace('>', '').split()[1:]
    resobj = {}
    for result in results:
        result = result.split('=')
        resobj[result[0]] = int(result[1])

    if resobj['failures'] > 0:
        sys.exit(1)
    if resobj['errors'] > 0:
        sys.exit(2)

    sys.exit(0)

########NEW FILE########
