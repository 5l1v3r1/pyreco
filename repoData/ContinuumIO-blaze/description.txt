Test directory
==============

This is a directory for automated tests. Only tests that must be
automatically executed should go in here.

For small testing code used while developing features and worth
keeping, consider using the directory
``[blaze repo]/samples/playground`` instead.

Blaze AIR (Array Intermediate Representation)
=============================================

 * [Blaze Execution System](blaze-execution.md)

Blaze AIR represents deferred computations for the purposes of transformation,
optimization and execution. These deferred operations are generated by the
user, through the application of blaze functions to data descriptions (arrays).

What follows is a description of the intermediate representation, the
transformations and the execution system.

We will start out in a local context and later describe how distributed
and out-of-core computation may fit in.


 * Transport blaze expressions across the network, aka "moving code to data".
 * Represent blaze's array programming abstractions explicitly. This means
   to convert implicit broadcasting to explicit, picking particular type
   signatures of kernels within blaze functions, etc.
 * Be understandable by third party systems, and participate in the
   development/discussion (https://groups.google.com/forum/#!topic/numfocus/tX7fRfwiFkI).

Intermediate Representation
---------------------------
The intermediate representation AIR uses is similar to a simple linear
three-address code. However, our operations may take an arbitrary number
of operands.

Each operation initially corresponds to a sub-expression in the expression
graph. The encoding is simply the result of a post-order traversal of the
graph, starting at the root. The root is determined by the expression that
was passed to blaze.eval:

```python
    root = a + (b * c)
    result = blaze.eval(root)
```

The initial AIR generated by the above expression would be similar to
the below:

```
    function expr(%a, %b, %c) {
        %0 = kernel(mul, %b, %c)
        %1 = kernel(add, %a, %0)
        ret %1
    }
```

As can be seen above, each operation produces a value in a virtual register,
e.g. `%0`. Such a register might be referenced multiple times without
re-evaluating the subexpression. This allows us to encode arbitrary DAGs.

The representation does not understand control flow, which is all internal to
the functions themselves. We merely represent a high-level function composition
and along with the data flow between computations. A (naive) evaluator could
simply walk through these instructions and apply these kernels successively:

```
    for each op in expr do
        kernel = op.args[0]
        args = [values[arg] for arg in op.args[1:]]
        values[op] = kernel(*args)
```

However, we want to apply optimizations such as fusion, and we want semantics
such as broadcasting, which complicates our story.

We first describe our IR in more detail.

Operations
----------
All operations are instances of `blaze.compute.air.ir.Operation`. This is a class
that tracks:

    * a result register (`result`)
    * a type for the result (`type`)
    * an arbitrary string opcode (`opcode`)
    * a (nested) list of arguments (`args`)

Types may be anything, in blaze we use datashape types.

All these operations are sequenced into basic blocks. We only ever use a single
basic block, since we do not support control flow at this level. These basic
blocks are arranged into functions, such as our function `expr` above.

We generate one such function for each compound expression that is evaluated
by the user.


Transformations
---------------
After correctness our main goal is efficiency. Hence we do not want to use
Python functions to execute over large amounts of data. Furthermore, we must
support open-ended extension where new backends may be registered externally.

The job of the transformations are to leverage the implementation kernels
that were registered for the blaze functions. E.g. to support local execution,
we may have to JIT a function, and to support an operation over data in a
relational database we may have to generate SQL.

This burden is placed on the kernels. This allows a clean separation of
function specification, kernel implementation and backend composition and
optimization.

Our goal is to have backend passes generate from these kernels efficient
compositions, encapsulated in backend-specific data. Passes consume sub-graphs
of the sub-expressions they want to handle, based on heuristics, cost models,
assumptions or simply because they can. Sub-graphs are contracted to single
nodes, which are either execution-ready or processed by some later pass.
Decisions are often based on on sub-expression specific metadata, such as type
or expected data location (covered below).


Pipeline and Environment
------------------------
Our transformations are composed in a single pipeline, which is a series of
transformations to be applied in succession. Each transformation takes two
arguments: the function (which encodes the expression) and the environment.

The function we have already discussed above, it holds the latest encoding
of the expression. The environment is a simple dictionary mapping string keys
to values. This allows passes to communicate any meta data down the pipeline.
This metadata can be local to each sub-expression, for instance by mapping
each Operation to some value, or global to the expression. What this data
means is specific to a sub-set of passes and is unrestricted by the system.
However, it is good practice to document environment keys and their purpose
in `air/environment.py`.


Backends
--------
Different backends perform different operations and need produce different
execution kernels. To allow mixing and matching of different backends, the
result of all transformations is a simple sequence of `"pykernel"` operations,
e.g. assuming we were able to fuse our `expr` example:

```
    function expr(%a, %b, %c) {
        %0 = pykernel(fma, %a, %b, %c)
        ret %0
    }
```

The result can be interpreted straightfowardly. Depending on the backend,
these kernels may be produced by a series of transformations or by a single
transformation. For instance, the JIT infrastructure currently produces
ckernels, which are composed in a later pass. A final pass can produce a
py-kernel from conglomerated ckernels.


Out-of-core
-----------
Out-of-core computations can be handled in many ways, depending on the
nature of the kernel. Notably, we can identify two approaches:

    * implement the OOC feature outside of the execution pipeline, wrapping
      the entire process
    * implement OOC feature inside the pipeline

In the simplest case trivially data-parallel kernels operating over
in-memory data can be wrapped using either mechanism, i.e. by wrapping
the entire pipeline or individual kernels with OOC kernels.

In the most general case however, wrapping the entire pipeline in an opaque
manner may not work, since have no knowledge of data access patterns.
In this case Blaze functions must be ready to accept a data descriptor for
consumption. How this is handled depends on how the kernel is implemented.
This may work well if kernels and data descriptors are jitted.


Distributed Computation
-----------------------
Distributed computation is again a different beast. We have the same problem
as with OOC computation, namely that we cannot assume knowlegde of data access
requirements of an arbitrary kernel. It makes sense to treat "out-of-core" and
"distributed" as their own separate "implementation kind", requiring a different
implementation to support execution over the respective kinds of data sources.
Of course, many cases can be generalized at the kernel implementation level,
such as OOC or distributed execution for elementwise ufuncs, reductions, etc.


Metadata
--------
To have some notion of kernel properties, such as commutativity or associativity,
we support per-op metadata. This can be represented directly on the operation
or in a mapping in the expression-global environment.

Such properties can be expressed in the blaze function, by passing in
additional keyword arguments which become part of the sub-expression's metadata.

We further allow kernel-specific metadata to be set for individual kernels.
This may be specific to the backend in question that is supposed to handle
the assemblage of kernels of that kind.

﻿Blaze Catalog
=============

The Blaze catalog is a component within Blaze which
provides a mechanism for finding arrays available
within the Blaze Cluster the machine is a part of.
This means locally on the current machine, if no
cluster has been explicitly configured, or within
the directory structure shared across the cluster
if one has.

Closely connected is the [Blaze Server](blaze-server.md),
which provides access to the catalog and other
Blaze functionality via a web service, and is how
members of a cluster communicate with each other.

The catalog is where most of the functionality
in the [Array Management
Repo](https://github.com/ContinuumIO/ArrayManagement)
goes.

Use Cases
---------

### Local Ad Hoc Data Analysis

A common pattern in analyzing data sets is to import
data from some files or other inputs, do some cleaning
on the data, then proceed with the analysis using scipy,
pandas, and other tools. The catalog can provide a way to
encapsulate the data import/cleaning.

In this case, extending and accessing the local
catalog without relying on any separate server
processes is important. It should be straightforward
to add an array to the server, as easy as loading
data using pandas, for example.

Additionally, it should be possible to access the
data in many common forms, including blaze, dynd,
numpy, pandas, and raw python objects (e.g. lists
of dictionaries or tuples). This functionality is
not specific to the catalog, rather a feature to
be supported by any Blaze array.

Desired Features
----------------

### Data Abstraction	

* Within a configured cluster, a single rooted
  namespace containing all persisted Blaze Arrays
  within the cluster.
  * Still want to be able to use URLs from other
    Blaze servers/catalogs directly without an import
    step.
* A catalog entry includes the following associated data.
  * Whether it produces a blaze.Array or a blaze.Table.
  * DataShape of the Array. Note that this scales from
    single values like a boolean or string, up through
    tables and multi-dimensional arrays.
  * The information needed to retrieve the Array's
    data. If a remote data source or file location
    is known to be fixed, this can be lazily loaded.
    Alternatively, an import step could snapshot the
    data when the it may change or disappear.
  * In the case of a deferred expression, the input
    arrays need to be tracked as a dependency graph,
    e.g. affecting how user management tools of the
    catalog warn about deleting arrays which others
    depend upon.
  * User-specified metadata about the array.
    Likely JSON or JSON-like key/value pairs of data.
* Local per-machine temporary namespace, `'/tmp'`.
* Per-user namespace, `'/home/<username>'`.

### Data Import

* Way to add data from local files (csv, tsv,
  other delimited text files, npy, json, netcdf, hdf5,
  etc) to the catalog.
* Data which is served publically via
  a [Blaze Server](blaze-server.md) should be
  importable into the cluster namespace with only
  the source array URL and the destination catalog
  path.
* Ability to insert data cleaning operations
  that work in a streaming fashion, per-element.
* Ability to insert data cleaning operations
  that work with the whole array at once, basically
  an arbitrary Python function manipulating the data.
* TODO: Add error handling info (mostly higher level).
* TODO: Cache invalidation when data changes via
        push, reevaluation in a lazy way via pull.

Peter - people like in ETL tools, rich error handling.

### Data Access

* Retrieve an Array object for any catalog entry
  with a convenient syntax.
    * `blaze.get("/path/to/array")`,
      `blaze.get("~/userarray")`
* Access permissions is not part of the catalog,
  just using the permissions of underlying FS for now.
* Create blaze command line/a series of ipython
  magics for exploring the catalog structure.

### Caching

* Caching of array data on disk in an efficient
  binary form, in cases where the original form
  is not efficient.
* Caching of array data in memory, with user-configurable
  memory limits.
* The cache needs tooling, users should be able to
  control how much space is in the cache, see the
  usage, etc.
* Caching is implicit, but with an explicit way
  to skip the cache if you'll just use data once.

### Implementation

* Implemented as pure Python code, using other Blaze
  components.
* Initial implementation is close to the style of
  Hugo's ArrayManagement repo, not using a sqlite
  database for now.

Interface
---------

### Importing Arrays Into Blaze

The Blaze Catalog is accessed from the `blaze.catalog`
namespace. Catalog entries live in a directory
structure, and may contain array data directly
or refer to it elsewhere. The configuration is
stored in the directory ~/.blaze/config.yaml,
which might look something like this:

```
catalog:
  local: ~/Arrays
  cluster: D:/BlazeCluster
  cache: D:/ArrayCache
  cachelimit: 20GB
```

To add an array to the catalog, one adds data
files or .yaml files describing how to interpret
the data to the catalog directories, describing the
data and how to import it.

One may, for example, drop a .npy or .csv into the
`~/Arrays` directory, and immediately have it be
picked up by Blaze. If the input file is not in
an efficient format, Blaze will cache it either
by making a copy of the data in an efficient format,
or an creating an index of the original file. 

In the case of a .yaml file, more control is afforded
over how the data gets loaded. For example with
a .csv file input, one can indicate whether there is
a header column, and provide a datashape to indicate
the exact types of the values, instead of relying
on heuristics to determine it.

### Accessing Arrays From Blaze

There are two mechanisms for accessing arrays
from the catalog. One is directly using the path
of an array, with the get function.

```python
from blaze import catalog as cat

userarray = cat.get('~/myarray')
userarray2 = cat.get('/home/<username>/myarray2')
sharedarray = cat.get('/tycho2')
```

These gives back Blaze Array objects.

﻿# Date/Time/DateTime

Blaze needs good date/time support for dates and
datetimes to support time series. This document
proposes some design ideas to accomplish that.

There are three main sources of inspiration for this
document, the datetime library in Python's standard
library, the datetime64 type in NumPy, and
the pandas library which uses NumPy's datetime64
storage using nanosecond as the unit together
with the dateutil library for support.

* http://docs.python.org/3.4/library/datetime.html
* http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html
* http://pandas.pydata.org/pandas-docs/stable/timeseries.html
* https://labix.org/python-dateutil / https://code.launchpad.net/dateutil
* http://userguide.icu-project.org/datetime/universaltimescale

The aim of this design is to roughly
model date and datetime as array-programming versions
of Python's standard datetime. While the Python
datetime library has known flaws, they are mostly
regarding completeness. It defines a time zone
object, but does not include standard time zone
implementations, leaving it to third party
libraries like ``pytz`` and ``dateutil`` to fill in
the gap. This does not appear to be a reasonable
approach for Blaze to take.

## Date

In NumPy, the choice was made to merge date and datetime
into a single object parameterized by unit. Keeping
these separate seems like a better idea, because
``date`` can have much more calendar logic, and doesn't
need to be concerned with time zones, whereas
``datetime`` can operate like the combination of a
``date`` and a ``time``.

### Attributes

```
a.year
a.month
a.day
```

These provide easy access to the components of
the date. They are read-only.

### Methods

``a.replace(year, month, day)``

Returns an array with any or all of year, month,
and day replaced in the dates.

``a.to_struct()``

Converts the dates to structs with type
"{year: int32, month: int16, day: int16}".

``a.strftime(format)``

Formats the dates using the C stdlib strftime function.

``a.weekday()``

Returns the zero-based weekday. Monday is 0, Sunday
is 6.

## DateTime

## No Unit Parameter

* Always store datetime as a 64-bit ticks
  (100*nanoseconds) offset from midnight of
  January 1, 0001. This is the "universal time
  scale" defined in the ICU library.

http://userguide.icu-project.org/datetime/universaltimescale

NumPy parameterizes its datetime64 type with a unit,
ranging from attoseconds up through hours. This
causes the implementation behind the scenes to have
a fair bit of complexity dealing with all the
details as a consequence.

Neither Python's datetime nor the Pandas library
use such a unit parameter. We will go with this
approach, storing all datetimes with a particular
fixed unit.

Pandas chooses nanoseconds as its unit, which means
that it can represent dates from 1678 to 2262. This
seems like a rather limiting choice of default if
the library is to be used for historical dates, it
can't even represent 1492! Microseconds (range of
600000 years) or Ticks (100*nanoseconds, range of
60000 years) seem more reasonable.

```
>>> import pandas as pd
>>> pd.date_range('1492-01-01', '1492-12-31', freq='D')
ValueError: Out of bounds nanosecond timestamp: 1492-01-01 00:00:00
```

## Time Zone

* Will use the Olson tz database, by importing the part which
  knows how to read a compiled database, tzcode, into
  DyND, then having a configuration to point at a build
  version of the database. Where there is an OS version,
  we can point at that, otherwise can point at the version
  inside the ``pytz`` library.

### Resources

* http://nodatime.org/
* http://www.joda.org/joda-time/
* http://www.boost.org/doc/libs/1_55_0/doc/html/date_time/local_time.html
* http://www.ibm.com/developerworks/aix/library/au-aix-posix/index.html
* https://www.gnu.org/software/libc/manual/html_node/TZ-Variable.html
* https://www.iana.org/time-zones
* http://www.cplusplus.com/reference/locale/time_get/get_date/
* https://github.com/ajaxorg/cygwin-builds/blob/master/usr/share/doc/tzcode/Theory
* http://www.timeanddate.com/library/abbreviations/timezones/
* .NET DateTime http://msdn.microsoft.com/en-us/library/bb384267.aspx

* http://pandas.pydata.org/pandas-docs/stable/timeseries.html#time-zone-handling
* http://www.stat.berkeley.edu/classes/s133/dates.html

### Discussion

Time zones are tricky to deal with, for a number of
reasons:

* They get changed over time. When or if daylight
  savings time happens can change, new time zones get
  added, etc. Libraries that support time zones must
  regularly be updated with fresh time zone databases.
* The ISO 8601 string format doesn't specify a
  time zone, it specifies a time zone offset. It's
  possible for such an offset to be the same for
  two different time zones, e.g. one that follows
  daylight savings and one that doesn't.
* There are two main conventions for time zones,
  short string codes like "CST" or "CDT", and
  location based names like "America/Chicago"
  for the central time zone. See the boost
  date_time time zone documentation linked
  above for one library's approach to this.
* The short string codes like "CST" are ambiguous
  in some cases. "CST" might mean "China Standard Time",
  "Central Standard Time", or "Cuba Standard Time".
* CST, for "Central Standard Time" might be for North or
  Central America, with differing daylight savings time
  conventions.
* For POSIX time zone specification, it looks like
  there is usage of GMT and UTC with opposite meanings.

The time zone could be attached to the data, similar
to how Python datetime objects work, or it could be
attached to the type. In Pandas, the time zone
is at the type level, as a property of the
DatetimeIndex object.

### OS-Managed Time Zone Data

The C and C++ libraries don't include time zone
information beyond the current locale, however many
operating systems do have such information accessible.
We would much prefer to use a database managed by
the OS than have to rely on update releases of Blaze
every time any time zone information changes.

On many UNIX systems, including OS X and most Linux
distributions, the Olson tz database is available
from ``/usr/share/zoneinfo``. On the system I looked
at, the files are in the TZif format, which are
generated by the ``zic`` utility included in
the Olson tz database code. As of the present, there
are three possible formats: TZif, TZif2, and TZif3.
The Olson tz database code comes with a reference
implementation for dealing with these zoneinfo files.

In Windows, starting with Windows Vista, there
is information in the registry. This only contains
current time zone information, not historical data,
so does not have the benefits of a full Olson tz
database. From the examples in the blog post linked
below and looking through the registry on a Windows
7 machine, it doesn't look like there's a trivial way
to match the time zone names used here with the POSIX
or Olson time zone names.

http://blogs.msdn.com/b/bclteam/archive/2007/06/07/exploring-windows-time-zones-with-system-timezoneinfo-josh-free.aspx

### Time Zone Attached To Data

* We're going to postpone considering this
  until the case with time zone attached to the dtype
  is functional.

Let's say we want to allow something where we parse
input strings and grab both the datetime value and
the time zone. Some possible formats we might be parsing
are as follows.

```
2001-02-03T04:05         # ISO 8601 with no time zone info
2001-02-03T04:05Z        # ISO 8601 Zulu (UTC) time
2001-02-03T04:05-0600    # ISO 8601 might be CST, GALT, etc.
Sat, 03-Feb-01 04:05 CST # CST
2005-10-21 18:47:22 PDT  # PST during daylight savings
```

If we use ISO 8601 strings as the standard string
representation, we cannot preserve the time zone of
a value when round tripping through a string.

One possibility for attaching time zones to data is to
add another 8 bytes to the value, with a NULL-terminated
string containing either a time zone code listed in
http://www.timeanddate.com/library/abbreviations/timezones/
or a UTC offset (interpreted as a time zone that does not
have daylight savings). A drawback of this is not handling
historical times well, as using the full Olson tz database
would allow. Time zone identifiers in this database are
by contrast relatively long strings, such as
"America/Dawson_Creek".

Systems that behave this way include:

* Python's standard library datetime.datetime.
* In Boost's date_time library, the local_date_time class does this. http://www.boost.org/doc/libs/1_55_0/doc/html/date_time/local_time.html#date_time.local_time.local_date_time
* In the .NET framework, DateTimes are represented as a 62
  bit unsigned integer offset from year 0001 in ticks
  (100 ns increments), with another 2 bits indicating
  whether the data is for UTC, local, or an unspecified
  time zone. This format can represent datetimes from the
  year 0001 to 9999.

### Time Zone Attached To Type

* This is what we will do first.

If we know all the datetimes are in the same time zone,
as is commonly the case in time series, we can attach
the time zone to the type instead of to the data.

Systems that behave this way include:

* Pandas
* Many systems where the time zone is implicit, and must
  be tracked separately by the programmer.

## Code Examples

The following code should work once the system is
completed.

```
>>> from datetime import date, time, datetime, timedelta
>>> import pytz
>>> import blaze, datashape
>>> from blaze import array
>>> from datashape import dshape
```

DataShape creation:

```
>>> dshape('date')
dshape("date")

>>> dshape('time')
dshape("time")
>>> dshape('time[tz="UTC"]')
dshape("time[tz='UTC']")
>>> dshape('time[tz="America/Vancouver"]')
dshape("time[tz='America/Vancouver']")

>>> dshape('datetime')
dshape("datetime")
>>> dshape('datetime[tz="UTC"]')
dshape("datetime[tz='UTC']")
>>> dshape('datetime[tz="America/Vancouver"]')
dshape("datetime[tz='America/Vancouver']")

>>> dshape('units["second"]')
dshape("units['second']")
>>> dshape('units["100*nanosecond", int64]')
dshape('units["100*nanosecond", int64]')
```

Array creation:

```
>>> array(date(2000, 1, 1))
array('2000-01-01',
      dshape='date')
>>> array(datetime(2000, 1, 1, 0, 0), dshape='date')
array('2000-01-01',
      dshape='date')
>>> array(datetime(2000, 1, 1, 5, 0), dshape='date')
ValueError: datetime cannot be converted to a date

>>> array(time(3, 45))
array('03:45',
      dshape='time')
>>> array(time(3, 45, 12, 345))
array('03:45:12.000345',
      dshape='time')
>>> array(time(3, 45, tzinfo=pytz.timezone('America/Vancouver'))
array('03:45',
      dshape='time[tz="America/Vancouver"]')
>>> array('03:45', dshape='time')
array('03:45',
      dshape='time')
>>> array('03:45:30', dshape='time[tz="America/Vancouver"]')
array('03:45:30',
      dshape='time[tz="America/Vancouver"]')

>>> array(datetime(2000, 1, 1))
array('2000-01-01T00:00',
      dshape='datetime')
>>> array(datetime(2000, 1, 1, 3, 45, 30))
array('2000-01-01T03:45:30',
      dshape='datetime')
>>> array(datetime(2000, 1, 1, 3, 45, tzinfo=pytz=timezone('America/Vancouver')))
array('2000-01-01T03:45',
      dshape='datetime[tz="America/Vancouver"]')
>>> array('2000-01-01T03:45', dshape='datetime')
array('2000-01-01T03:45',
      dshape='datetime')
>>> array('2000-01-01T03:45Z', dshape='datetime')
ValueError: Input for 'datetime' cannot specify a time zone
>>> array('2000-01-01T03:45Z', dshape='datetime[tz="America/Vancouver"]')
array('1999-12-31T19:45',
      dshape='datetime[tz="America/Vancouver"]')

>>> array(timedelta(seconds=3))
array(3000000,
      dshape='units["microsecond", int64]')
>>> 3 * blaze.units.second
array(3,
      dshape='units["second"]')
```

DateTime Arithmetic:

```
# NOTE: Python's datetime.time does not support this arithmetic
>>> a = array('2000-01-01', dshape='date')
>>> b = array('03:45', dshape='time')
>>> a + b
array('2000-01-01T03:45',
      dshape='datetime')

>>> a = array('2000-01-05', dshape='date')
>>> b = array('2000-01-01', dshape='date')
>>> a - b
array(4,
      dshape='units["day", int32]')

# NOTE: Python's datetime.time does not support this arithmetic
>>> a = array('03:45', dshape='time')
>>> b = array('02:00', dshape='time')
>>> a - b
array(63000000000,
      dshape='units['100*nanosecond', int64]')

>>> a = array('2000-01-01T03:45', dshape='datetime')
>>> b = array('2000-01-01T02:00', dshape='datetime')
>>> a - b
array(63000000000,
      dshape='units['100*nanosecond', int64]')

>>> a = array('2000-01-01', dshape='date')
>>> a + 100 * blaze.units.day
array('2000-04-10',
      dshape='date')

>>> a = array('2000-01-01T03:45', dshape='datetime')
>>> a + 12345 * blaze.units.millisecond
array('2000-01-01T03:45:12.345',
      dshape='datetime')
```

﻿Blaze Execution
===============

 * [Blaze Function Use Cases](blazefunc-usecases.md)
 * [Blaze NumPy-like API](blaze-numpy-api.md)
 * [Elementwise Reductions](elwise-reduction-ufuncs.md)
 * [Blaze AIR](blaze-air.md)
 * [Deferred CKernel Interface](deferred-ckernel-interface.md)
 * [CKernel](ckernel-interface.md)


The blaze execution system takes blaze expression, which are built up by
applying those functions over blaze arrays. Blaze arrays can describe data
or other (sub-)expression. The goal for the blaze execution system is to
determine a suitable evaluation strategy compatible with the input data
sources. This may include assembling something like an SQL query, composing
existing compiled code, or JIT-compiling kernel implementations written in
flypy.

The expressions themselves are simple nodes in a DAG, which are typed as
soon as they are created, which allows early error detection.
Nodes are created through function application:

    blaze.add(a, b) # return a new blaze array with a deferred expression
                    # Add(a.expr, b.expr)

High-level expressions may be sent over a network or passed on to the execution
engine to determine a suitable evaluation plan. The evaluation plan may
include C ABI constructs to compose code from low level libraries, JIT-compiled
code, assembled queries, etc.

This document describes the design of the execution system, how it goes from
expressions to execution.

At the highest level we have blaze expressions, in between we have an
intermediate representation of the expression along with execution details,
and at the lowest level we have a set of kernels linked together by their
arguments. This process works as follows:

    * operations over arrays are lazy and accumulate a directed acyclic graph
    * blaze.eval() starts the execution process, which involves conversion to
      AIR and executing the result


Blaze Expressions and Blaze Functions
-------------------------------------

 * [Blaze Function Use Cases](blazefunc-usecases.md)

Blaze expressions are generated by applying blaze functions.
Blaze functions are the user facing representation of functionality
in blaze. This is like the `ufunc`/`gufunc` of numpy, but more general since
we have more flexible ways to pattern match inputs. This happens through
datashape type signatures, to which arguments must conform.

These functions may be *implemented* using kernels of various sorts for various
backends.

We support open-ended extension through overloading at the blaze function level.
There are two forms of overloading at play:

    - function overloading
    - kernel overloading

The former overloads a logical blaze function, for instance for typing
purposes. The second form allows kernel (implementation) overloading,
where kernels are associated logically with blaze functions for a
certain implementation kind. Implementation kinds include flypy, SQL,
ckernel, and so forth.


Blaze Expression Lowering to Blaze AIR
--------------------------------------

The highest level of the blaze execution system is taking the interface
provided to users of blaze, which includes blaze functions and data
descriptions from concrete input arrays, and lowers it to blaze AIR.


Blaze AIR JIT Compilation
-------------------------

 * [Blaze AIR Documentation](blaze-air.md)

Once we are in blaze AIR (Array Intermediate Representation), we apply
successive passes in a pipeline to reduce the expressions to kernels which
can be applied to their arguments. This is further described in the link
above.


The Deferred CKernel Interface
------------------------------

 * [Deferred CKernel Interface Documentation](deferred-ckernel-interface.md)

At the lowest level
are primitive C ABI interfaces designed to be interoperable across
any library boundaries, including between systems using different
standard libraries. These low level interfaces are used by the
higher level systems as JIT compilation targets, and as a way to
import implementation kernels from outside of blaze.

One fundamental aspect of both blaze and dynd is deferred execution.
For supporting deferred and cached execution at the low level, just one
small step above the ckernel interface, is the deferred ckernel.
This object provides a simple interface to building a ckernel whose
structure has already been determined up to the dynd type level, and
just needs dynd metadata and a kernel type (i.e. single or strided) to
build a ckernel.

One of the use cases driving the deferred dynamic kernel is to provide
individual kernels to blaze and dynd function dispatch. While many of
the functions provided by blaze will be JIT compiled LLVM bitcode, there
needs to also be a way to expose functions to blaze from external systems
which know little or nothing about blaze and dynd.


The CKernel Builder
-------------------

 * [CKernel Documentation](ckernel-interface.md)

The lowest level execution interface in blaze is the ckernel.
Any time an operation gets executed in blaze, it is first reduced
down into a ckernel, either via JIT compilation or assembling together
other ckernels, and then executed as a ckernel.

When constructing a ckernel from JIT compilation or deferred ckernels,
the `ckernel_builder` object is used. This is a small C struct with a
static buffer which can hold small ckernels, and API functions for
dynamically growing the ckernel buffer for larger ones.

At the ckernel level, all information about types and possible variations
about memory layout has been baked into the code and data that make
up the ckernel. All that is left is the ability to call the kernel function
and to free the resources associated with the ckernel. This means that
code using a ckernel can be quite simple, it just needs to know the ckernel's
function prototype, and have data pointers that it knows conforms to the
types baked into the ckernel, and it can execute it.

Blaze Functions
===============

 * [Blaze Execution System](blaze-execution.md)

Blaze functions are like numpy's `ufunc` and `gufunc` objects.
A blaze function typically contains a "dictionary" mapping from
type signatures to kernel functions. These kernel functions may
be deferred ckernels or LLVM functions, and the LLVM functions
may be for a specific type or generic and able to compile down
to ckernels for patterns of types.

TODO: Expand/develop the blaze function design.
Fusion
======

This document addresses the problem of deforestation, or fusion, in blaze. We
shortly explain why fusion is important, after which we detail what the
current design allows, and what it's shortcoming are, with possible solutions.

Introduction
------------
Fusion is an optimization that eliminates temporary arrays, lists, trees, etc.
For instance, in the element-wise expression `a + b * c`, we want to loop
over all elements of a, b and c simulteneously, while writing to some output
array. We don't want to allocate a temporary in which we evaluate `b * c` only
to subsequently read the data back in to evaluate `a + temp`. This causes
unneccessary traffic to load data from memory into cache, and from cache into
registers (or even from disk into memory first).

Current State
-------------
We currently fuse expressions at their most inner levels. For instance, we
have a notion of "element-wise" functions which operate on scalars. These
operations can always be fused, such that all fused scalar functions can
be applied in succession:

```python
    for i in range(shape[0]):
        for j in range(shape[1]):
            t = b[i, j] * c[i, j]
            out[i, j] = a[i, j] + t
```

We support the notion of generalized ufuncs, which operate instead over
N dimensions, leaving broadcasting implicit in outer dimensions. For instance:

```python
def filter1d(array, conditions):
    return [item for item, cond in bzip(array, conditions) if cond]
```

Shortcomings
------------
A shortcoming of this design is that it is unclear how the computations inside
the generalized ufuncs in this system can be fused together without an
optimizing compiler. A problem is the mixture of languages and systems that
need to cooperate. For instance, in the following exmaple:

```python
sum(filter1d(arr, conds))
```

how can we avoid building all the intermediate filter lists or arrays at the
inner level? It seems the ability to fuse this expression depends on both
the nature of filter and sum: filter does not need to process all data in order
to yield an element or chunk, and sum doesn't need all data available in order
to perform any work.

It makes sense to allow pushing or pulling of data at a more fine-grained level.
In an optimizing compiler, this could be just elements. However, due to the overhead
of dynamic dispatch inherent to a static system implementing this, we need
a tradeoff between dynamic dispatch overhead and the cost of temporaries.
Instead, we can push around blocks of data. This can be done in several
ways:

    * tasks or coroutines
    * continuation passing style (CPS)
    * iterators

Tasks
-----
Write producers as cooperatively scheduled green-threads that push and pull
blocks of data. This correspconds directly to our expression graph:

```python
def filter(inputs, outputs):
    for (cond_block, item_block) in inputs.receive_all():
        result_block = [
        for cond, item in zip(cond_block, item_block):
            if cond:
                result_block.append(item)

        outputs.send(result_block)

def sum(inputs, outputs):
    sum = 0
    for block in inputs.receive_all():
        for item in block:
            sum += x

    outputs.send(sum)
```

This system terminates when all tasks have died. We leave implicit here the
closing of the channels that will allow `receive_all` to stop reading.

This system is elegant because it is modular and easy to extend with new
functions that cooperate under the same conditions. If someone along the chain
does need all input data, that task can simply perform buffering.

Continuation Passing Style
--------------------------
Instead of returning chunks, we can instead communicate them through
continuations. We can pass around blocks by default, and we can
support convenience code to perform buffering where necessary. On the other
hand you can annotate implementions with `element`, `block` or `full` buffering,
which automatically handles buffering/unbuffering, making the system more
powerful (it would be relatively straightforward for an optimizing compiler
to eliminate the dispatch on continuations).

Below we demonstrate how to make it work using just scalars. Instead, you can
imagine blocks being pushed through.

```python
def add(a, b, cont):
    cont(a + b)

def mul(a, b, cont):
    cont(a * b)

def expr(a, b, c, d, cont):
    mul(a, b, lambda ab: mul(c, d, lambda cd: add(ab, cd, cont)))

print((a * b) + (c * d))
expr(a, b, c, d, print)
```

Iterators
---------
Perhaps the easiest yet modular way to express the problem is in terms
of multi-dimensional iterators, where each iterator returns successive blocks
that are computed over:

```python
def filter(input, conds):
    for item, cond in zip(input, conds):
        if cond:
            yield item

def sum(input):
    result = input[0]
    for item in input[1:]:
        result += item
    return result
```

Multiple dimensions are simply expressed through nested iterators.

Iterators can be implemented as part of ckernels, where each dimension produces
an iterator of the dimension it wraps. The innermost kernel does some buffering
to accumulate results in blocks, which it returns. These iterators simply
wrap expressions:

```
template<typename Inputs..., typename Output, typename E>
class Iterator {
public:
    Output produce() {
        return expr.apply(iterators...);
    }
};

template<typename T>
class Add {
    T apply(T a, T b) {
        return a + b;
    }
};
```
﻿Blaze NumPy API
===============

This is a design document describing the philosophy
and details the Blaze API for supporting
NumPy functionality. While NumPy is an amazing
inspiration for how the API should look, we are not
matching it precisely. Differences arise from factors
such as the datashape type system, deferred evaluation,
and support of distributed arrays.

Blaze API Design Guidelines
---------------------------

When designing Blaze APIs, the following points
should guide the design:

 * An important target user is a domain expert who knows
   programming, not necessarily an expert programmer.
   Imagine how such a user might try to use Blaze,
   and design the public-facing API so as to match their
   expectations or guide them to the right answer.
 * Another important target user is the expert
   programmer. Don't dumb down the API so that it
   tries to "do the right thing" without providing
   a well-specified abstraction that can be relied on.
   This is a major drawing point of Python.
 * When making an API choice, always match
   NumPy, Pandas, SciPy, etc unless a difference
   is needed for:
   * using the datashape type system.
   * supporting distributed or out of core arrays.
   * self-consistency or a clean API. This kind of
     decision should be agreed upon within the Blaze
     team after a discussion.
 * Consider how a function generalizes from only
   fixed-size dimensions in NumPy to the `var`
   dimension type in Blaze. DyND does some things
   in this regard already, so it is worth looking
   there for some ideas.
 * Consider using keyword-only arguments. These
   are supported in the Python 3 syntax, but can
   be emulated using **kwargs in Python 2 as well.
   For adding many optional arguments, they are
   good because they make user's code more
   self-documenting.

Array Creation
--------------

In NumPy, there is a suite of functions for creating
arrays in various ways, such as `numpy.ones`,
`numpy.empty`, and `numpy.arange`. The main difference
Blaze has is that wherever a shape + dtype are
specified, a datashape needs to go instead.

Here are some of the NumPy array creation routines

 * `np.array` creates an array from data. NumPy
   has many related functions, like `np.asarray`,
   `np.asanyarray`, `np.ascontiguousarray`,
   `np.asmatrix`.
 * `np.empty` creates an array of uninitialized data.
 * `np.zeros`, `np.ones`, `np.full` create arrays
   initialized with a value.
 * `np.arange`, `np.linspace`, `np.logspace` create
   arrays with numerical ranges.
 * `np.eye`, `np.diag`, `np.tri`, etc provide
  linear algebra-oriented creations.

### Requires Discussion: Adopt DyND's nd.zeros function signature?

In DyND, the constructors like `nd.empty` accept
the type in a few different ways. First is as
a datashape string or `ndt.type` object containing
the full type. Second is a shape tuple followed by
a dtype, providing compatibility with NumPy.
Finally is the datashape specified with all its
dimensions as arguments. The following three
examples illustrate this:

```python
>>> from dynd import nd, ndt
>>> nd.zeros('2, 3, float32')
nd.array([[0, 0, 0], [0, 0, 0]], fixed_dim<2, fixed_dim<3, float32>>)

>>> nd.zeros((2, 3), ndt.float32)
nd.array([[0, 0, 0], [0, 0, 0]], strided_dim<strided_dim<float32>>)

>>> nd.zeros(2, 3, ndt.float32)
nd.array([[0, 0, 0], [0, 0, 0]], strided_dim<strided_dim<float32>>)
```
This is then partnered with keyword-only arguments
for any extra options, like `access='rw'`, or
`order='F'`. Do we want to adopt this idea from
DyND into the Blaze API?

Also to note here is that `nd.full` also uses a
keyword-only argument for the value, adopting
NumPy's choice here is incompatible with the
third signature above.

Array Reshape and Flatten
-------------------------

There is a common idiom in NumPy tutorials to
create test arrays as follows:

```python
>>> import numpy as np
>>> np.arange(6).reshape(2, 3)
array([[0, 1, 2],
       [3, 4, 5]])
```

One of the things we want to accomplish with Blaze
is increase the predictability of certain kinds of
operations. NumPy's reshape tries to be smart,
taking a view when possible. Whether a view is
taken or a copy is made thus depends on the strides
of the array, and cannot be reliably predicted in
all cases.

The following is an example of this,
where `a` and `b` are views of the same data,
but reshaping them to flat arrays produces a copy
in one case but not the other.

```python
>>> a = np.arange(6).reshape(2, 3)
>>> a.ctypes.data
127557280

>>> b = a[:,:2]
>>> b.ctypes.data
127557280

>>> a.reshape(6).ctypes.data
127557280

>>> b.reshape(4).ctypes.data
127557344
```

In Blaze, we would like both of these flattening
operations to behave "like a view". The way we
can do this for the latter is have the result
support iteration and indexing through appropriate
APIs or protocols.

Additionally, the case of reshaping a multidimensional
array to another multidimensional array has a hidden
flattening operation. Here's the kinds of things
you can get with just a two dimensional reshape:

```python
>>> np.arange(6).reshape(2,3).reshape(3,2)
array([[0, 1],
       [2, 3],
       [4, 5]])

>>> np.arange(6).reshape(2,3).reshape(3,2, order='F')
array([[0, 4],
       [3, 2],
       [1, 5]])

>>> np.arange(6).reshape(2,3,order='F').reshape(3,2, order='F')
array([[0, 3],
       [1, 4],
       [2, 5]])

>>> np.arange(6).reshape(2,3,order='F').reshape(3,2)
array([[0, 2],
       [4, 1],
       [3, 5]])
```

It seems likely that flatten and reshaping a one
dimensional array to a multidimensional array are
the only important cases, so we may consider
limiting reshape in Blaze this way.

Elementwise UFuncs
------------------

NumPy's large library of ufuncs is an essential
feature of the system. Blaze needs to provide the
same ufuncs, though with some different choices
in the general interface.

http://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs

The most important difference Blaze has is that
its ufuncs are always deferred. That means when
`blaze.sin(x)` is called, it does not compute
the sine of all the elements in `x`, but rather
creates an array representing the deferred operation
with an expression DAG (directed acyclic graph).

One consequence of deferred evaluation is that the
`out` parameter does not belong in the ufuncs,
but rather in the `blaze.eval` function, when
computation actually occurs.  Similar reasoning
can be applied to all the other keyword arguments
of NumPy ufuncs, for example the `dtype` argument
could be done as a manipulation to the deferred
expression DAG.

Blaze ufuncs encompass both NumPy ufuncs and
generalized ufuncs in one swoop, by allowing
the arguments to be typed with multi-dimensional
datashape types.

### Reductions Derived From UFuncs

NumPy automatically adds a few reduction methods
to all ufuncs. This means we can call
the nonsensical `np.sin.reduce([1,2,3])`, for
example, which raises an exception. It would be
better in Blaze to simply not add these reductions
in such cases.

Not all array programming systems agree on the
definition for these reductions, for example
NumPy uses an order of operations from left to
right, while J and other APL dialects use right
to left.

```python
>>> np.subtract.reduce([1, 2, 3])
-4
```

NumPy calculated `(1 - 2) - 3`, whereas
J calculates `1 - (2 - 3)`.

```
   -/ 1 2 3
2
```

Some features we do want when deriving reductions
from binary ufuncs are whether certain properties are
satisfied by the operation. Even in cases where a
property is only approximate (e.g. associativity
in floating point addition, try 1e-16 + 1 + -1),
we will usually want to indicate it is so that
calculations may be reordered for efficiency.

A previous incarnation of DyND included some
development towards this, with per-kernel customization
of the associativity flag, commutativity flag, and
identity element.
https://github.com/ContinuumIO/dynd-python/blob/master/doc/source/gfunc.rst#creating-elementwise-reduction-gfuncs

Elementwise Reductions
----------------------

Some reductions can be computed by visiting each
element exactly once, in sequence, using finite
state. The most common NumPy reduction
operations, `all`, `any`, `sum`, `product`,
`max`, and `min` all fit this pattern. Some
statistical functions like `mean`, `std`, and `var`
also fit this pattern, with slightly more sophisticated
choices for the accumulator state, as well as a finishing
operation.

NumPy has two keyword arguments that are worth
keeping in this kind of operation for Blaze,
`axis=` and `keepdims=`. The parameter to `axis`
may be a single integer, or, when the operation
is commutative and associative, a tuple of integers.
The `keepdims` parameter keeps the reduced dimensions
around as size 1 instead of removing them, so the
result still broadcasts appropriately against
the original.

To see how `mean` fits into this pattern, consider
a reduction with datashape '{sum: float64; count: int64}',
identity `(0, 0)`, and function

```python
def reduce(out, value):
    out.sum += value
    out.count += 1

# alternatively, chunked
def reduce_chunk(out, chunk):
	out.sum += blaze.sum(chunk)
	out.count += len(chunk)
```

A final division maps this to `float64`. When
defining this kind of kernel, it would also
be advantageous to provide a function to combine
two partial results, so as to allow Blaze to
parallelize the operation

```python
def combine(out, out_temp1, out_temp2):
    out.sum = out_temp1.sum + out_temp2.sum
    out.count = out_temp1.sum + out_temp2.count

def combine_destructive(out, out_temp):
    out.sum += out_temp.sum
    out.count += out_temp.count
```

Additional Discussion
---------------------

Some additional points for discussion, raised by Peter:

 * Do we move a lot of the top-level `numpy` functions into
   methods, and try to stick to more of a "Only one way to do it"
   API, so that chaining of methods is natural like this?
 * What does everyone think about a mechanism for hinting?
   This would be very useful for parallelism and distributed
   cases, and can be built in either as kwargs for reduction,
   filter, and join/merge funcs, or as an explicit method that
   is called in the chain:

```
ary.ufunc1().ufunc2().reduce_or_filter(array2, shapehint=(N,...)).ufunc3()
```

 * Similarly, what about specifying chunksize as a hint, for
   when we use these DAGs to process streams? So we could
   change up the processing chunksizes at certain points?

﻿Blaze Server
============

The Blaze server is a component of Blaze which
exposes array data from a local
[Blaze Catalog](blaze-catalog.md) to the network as
a web service. Its main goals are to provide a way
to serve array and table data in a rich way, and to
expose a blaze node's computation and storage to
a cluster.

Use Cases
---------

### Serving Rich Data On The Internet

The purpose of this mode of the server is to
be able to easily put datasets on a LAN or the
internet with the ability to slice and filter
server-side. An important inspiration for this
is [OPeNDAP](http://opendap.org/), discussed below.

The key points for this are:

1. Easy to import a dataset into a Blaze Catalog,
   and start a server for it. Should be similar
   in ease of use to starting an IPython Notebook.
2. Easy to connect to a server URL, explore the
   datasets available on it, and pull all or
   parts of the data into a Python context.

### Acting As a Node in a Blaze Cluster

The purpose of this mode of the server is to
expose a node's resources (compute, bandwidth,
and storage) to Blaze Cluster computations.
Initially, we are targetting embarrassingly
parallel jobs, such as elementwise execution
on large arrays, as well as some reductions.

Requirements
------------

### Serving Rich Data

* Expose the Blaze Catalog defined on the local server.
* Serve data in JSON, for clients that do not
  understand the binary form.
* Serve data in binary form. This requires definition
  of a serialization format for datashaped data.
* Server-side slicing of datasets.
* Server-side filtering of datasets.
* Server-side computed columns.
* Should support using POST for specifying all
  the data, so that queries can be done without
  leaving behind traces in web caching servers, etc.

### Supporting Blaze Cluster

* Support embarrassingly parallel computation
  via numpy-style broadcasting/loop fusion of deferred
  computations.
* Head node and compute node. Design needed here,
  like choosing a user-configured head node (i.e. configured
  to be optimal for the scheduling tasks)
  or arbitrarily selected, to avoid the single
  failure point.
* Ability to define a cluster, with a shared namespace
  for catalog access.
* Support a "compute context" which can receive
  code to execute on the local data.

Development Roadmap
-------------------

The first priority is building the capacity to serve
rich data using Blaze. The target client for this is
[Bokeh](https://github.com/ContinuumIO/Bokeh), which
is defining a Bokeh Server. The Bokeh Server would include
the Blaze Server as a subcomponent, providing direct access
to read the underlying data driving Bokeh plots.

Developing the cluster functionality is scheduled after
the catalog is functioning well for a single server
of rich data.

Inspiration from OPeNDAP/DAP
----------------------------

One of the inspirations for the Blaze Server is the
[OPeNDAP project](http://opendap.org/), which describes
the following reasons for its use:

* Allows you to access data over the internet.
* [DAP 2.0](https://earthdata.nasa.gov/our-community/esdswg/standards-process-spg/rfc/esds-rfc-004-dap-20) is a NASA community standard.
* Data is stored and transmitted in binary form.
* OPeNDAP provides sophisticated sub-sampling capabilities.

In DAP, the [DDS Format](http://docs.opendap.org/index.php/UserGuideDataModel)
is analogous to Blaze Datashape. The format for
storage and transmission of data is defined in
terms of [XDR](https://tools.ietf.org/html/rfc4506).
There is a Python implementation called
[PyDAP](http://www.pydap.org/).

Some limitations of DAP:

* No boolean, 8-bit integers, datetimes, enumerations,
  categorical types.
* The DAP v4 draft on the OPeNDAP site is from 2004,
  cannot tell if progress is being made.


SQL Support in Blaze
====================

The SQL support in blaze is provided through a set of implemented blaze
kernels. These kernels are SQL implementations of general blaze functions.
These stages the expressions go through are the same as usual, but we shall
take closer attention to the SQL aspects of it:

    * The user uses blaze functions, which builds up an expression graph
    * The user calls blaze.eval
        - this produces AIR
        - SQL queries are generated
        - SQL queries are executed

Below we will focus on these individual steps.

SQL Expression Generation
-------------------------
Queries are generated by the SQL interpreter. First, the general implementation
partioner assigns backends to operations (which are blaze function
applications). This happens in `blaze/compute/air/frontend/partitioning.py`:

    https://github.com/ContinuumIO/blaze/blob/master/blaze/compute/air/frontend/partitioning.py

See especially the `partition` function.

The SQL query generator then determines all operations that
should be handled in the SQL domain. This happens in `blaze/io/sql/air.py`:

    https://github.com/ContinuumIO/blaze/blob/master/blaze/io/sql/air.py

See especially the `rewrite_sql` function. This simply applies each SQL kernel,
passing in the arguments, generated by other operations.

The SQL kernel's job is then to compose larger queries from smaller queries.
Since SQL syntax expects a certain syntax which does not correspond cleanly
with the way blaze expressions are nested, kernels generate a little SQL
term tree, defined in `blaze/io/sql/syntax.py`:

    https://github.com/ContinuumIO/blaze/blob/master/blaze/io/sql/syntax.py

This syntax module supports a fairly simple and straightforward syntax:

```python
    Table   = qtuple('Table',   ['tablename'])
    Column  = qtuple('Column',  ['table', 'colname'])
    Select  = qtuple('Select',  ['exprs', 'from_expr', 'where',
                                 'groupby', 'order'])
    Where   = qtuple('Where',   ['expr'])
    GroupBy = qtuple('GroupBy', ['cols'])
    From    = qtuple('From',    ['exprs'])
    OrderBy = qtuple('OrderBy', ['exprs', 'ascending'])
    Call    = qtuple('Call',    ['name', 'args'])
    Expr    = qtuple('Expr',    ['args'])
```

This syntax is generated by SQL *expressions*, generated from kernels.
This looks somewhat like this:

```python
    QWhere      = namedtuple('QWhere',   ['arr', 'expr'])
    QGroupBy    = namedtuple('QGroupBy', ['arr', 'keys'])
    QOrderBy    = namedtuple('QOrderBy', ['arr', 'exprs', 'ascending'])
```

Selecting SQL Kernels
---------------------
After generating all the SQL expressions, `rewrite_sql` removes intermediate
SQL kernel nodes from the AIR that are no longer needed, and replaces the
"root" nodes with SQL implementations. These are the nodes that are consumed
by some other kernel that has no corresponding SQL counterpart, because the
parent node has other children which describe non-sql data sources, or simply
because it is the root of the expression.

The root is then replaced by `sql_pykernel` in `air.py`:

    https://github.com/ContinuumIO/blaze/blob/master/blaze/io/sql/air.py

Generating SQL Syntax
---------------------
The `sql_pykernel` function closes over the previously generated SQL
expression. It generates an SQL syntax query string from this using the
syntax module (`reorder_select` and `emit`). Remember that its job is to
return a new blaze array.

It first executes the query using pyodbc (or currently, sqlite). Then it
wraps the resulting cursor in an SQLDataDescriptor, along with its datashape.
The exact sizes of the columns may however be unknown in the resulting type.

SQL Data Descriptors
--------------------
The SQL data descriptors can be found here:

    https://github.com/ContinuumIO/blaze/blob/master/blaze/io/sql/datadescriptor.py

The `SQLDataDescriptor` describes a remote SQL column or table, ready for
retrieval or querying from the database. The `SQLResultDataDescriptor` is
a datadescriptor to describe the result of an executed SQL query, that lazily
pulls in the data when (and if) requested.

Other Features
--------------
There are still many pieces missing:

    * Load type information from existing SQL table or column
        - connect to catalog
    * Support groupby, orderby, etc
Blaze Function Use Cases
========================

 * [Blaze Execution System](blaze-execution.md)

This document goes through a number of blaze function use cases,
and is intended to be a representative sample. Included are notes
on how each use case affects the blaze datashape type system
and function dispatch.

Matrix Vector Multiplication
----------------------------

`matvec :: (M, N, T) (N, T) -> (M, T)`

T is a type for which the blaze functions `multiply(T, T)` and `add(T, T)` are implemented,
and satisfy the appropriate mathematical properties. (This leads to the question of
how precise we want to be about these things.)

Example types for `T`:
    real numbers, complex numbers: should dispatch to BLAS when reasonable
    integers
    integers modulo n
    matrices (e.g. 3x3 or 4x4 graphics transformation matrices)
    quaternions
    symbolic expressions (e.g. sympy)

There should be a general fallback implementation which works for
any T which implements multiplication and addition. For common types,
there should be BLAS or other library dispatch code which is optimized.

If someone registers a new type in Blaze, and has some code for efficiently
doing this operation, they should be able to add their kernel to the blaze
function and have it be called when the new type is used.

Sorting
-------

`sorted :: (N, T) -> (N, T)`

`T` is a type for which the blaze function `less(T, T)` is implemented, and
defines a strict weak ordering.

There should be a general sort implementation which uses the blaze function
`less`, along with the ability to provide optimized implementations for
specific common types.

We will need to choose whether `sorted` is a stable sort, like in Python,
or whether there are separate `sorted` and `stable_sorted` like in C++.

Partition Based On Indices
--------------------------

`partition_indexed :: (N, T) (M, integer) -> (M, var, T)`

`T` can be anything in this case, all it requires is to be copyable.

This function simply slices a one-dimensional array into a ragged
two dimensional array. In many cases, this may be implementable as
a view into the original data.

Eigenvalues
-----------

`eig :: (N, N, T) -> { eigval: (N, T); eigvec: (N, N, T) }`

Computes the eigenvalues and eigenvectors of the input matrix. This
is basically numpy.eig, but using a struct within the blaze type system
instead of a tuple from python's type system for the return value.

Noise Functions
---------------

`noise :: T -> T`
`         (2, T) -> T`
`         (3, T) -> T`

`curlnoise :: (2, T) -> (2, T)`
`             (3, T) -> (3, T)`

Here, `T` is a real type. It may make more sense to introduce a
vector type that doesn't act as an array dimension for broadcasting
purposes. In this case, the signatures may look more like the following.

`noise :: T -> T`
`         T[2] -> T[2]`
`         T[3] -> T[3]`

Gradient Function
-----------------

`gradient :: (N, T) -> (N, T)`
`            (M, N, T) -> (M, N, T[2])`
`            (M, N, R, T) -> (M, N, R, T[3])`

One version of this function would calculate the gradient of the function,
given a reconstruction filter. This function pokes at an ambiguity about
array dimensions and function dispatch vs broadcasting. Given an array
of datashape `(M, N, T)`, how should one choose between broadcasting the
`(N, T) -> (N, T)` signature, or using the signature `(M, N, T) -> (M, N, T[2])`?

Another version of this function might accept the gradient reconstruction function as
another blaze function, perhaps like this:

`gradient :: (N, T) ((T[3], real) -> T) -> (N, T)`
`            (M, N, T) ((T[3,3], real[2]) -> T[2]) -> (M, N, T[2]`
`            (M, N, R, T) ((T[3,3,3], real[3]) -> T[3]) -> (M, N, R, T[3]`

In this version of the function, the ambiguity is removed, because the dimensionality
of the reconstruction filter is provided in the type of the blaze function. On the other
hand, if the blaze function provided is itself a generic reconstruction filter which works
for any number of dimensions, this ambiguity still exists.

A big reason to prefer overloading this way instead of having multiple `gradient2`,
`gradient3`, etc. functions is that it is possible to write a lot of code using these
functions generically for both 2 and 3 dimensions.

Another possibility for separable reconstruction filters might be:

`gradient :: (N, T) (real -> real[3]) -> (N, T)`
`            (M, N, T) (real -> real[3]) (real -> real[3]) -> (M, N, T[2]`
`            (M, N, R, T) (real -> real[3]) (real -> real[3]) (real -> real[3]) -> (M, N, R, T[3]`

In this case, the number of filter coefficient functions disambiguates between
the different numbers of dimensions.

Hessian Function
----------------

`hess :: (M, N, T) -> (M, N, T[2,2])`

This function is like gradient, but now each element produced is a
matrix instead of a vector.

Date/Time Parsing
-----------------

`strptime :: string string -> datetime`

Struct Field Manipulation
-------------------------

These functions dive a bit more into some dynamic possibilities, where
the output type depends on the values of an input parameter instead of
its type

`fields :: {names...:types...} (N, string) -> {subsetnames...:subsettypes...}`

This function selects a subset of fields out of a struct.

This raises the question of how do we pattern match
variadically against input types like the struct `{names...:types...}`

`rename_fields :: {names...:types...} map<string, string> -> {newnames...:types...}`

This function renames a selection of fields, using the associative array
as the remapping. This could be an overload of `fields` as well, where the
type of the second parameter determines what happens.

There's a question here of how should we spell parameterized types
like the associative container `map<string, string>`.

Reductions With axis= Arguments
-------------------------------

Reductions which can be characterized elementwise are an important
class of functions for blaze. This includes min, max, average, standard
deviation, and similar functions. In numpy, these functions take two
optional parameters, `axis=` and `keepdims=`, which control how array
dimensions are handled.

One big difference for blaze over numpy is the addition of variable-sized
dimensions

Reshaping
---------

A function that maps a given array to another array with different dimensions:

`reshape :: {shapeD} (shapeS, T) -> (shapeD, T)`

Reshapes the original array into a different shape. In order for this to work
the source shape (shapeS) and the destination (shapeD) must be compatible.
This is basically the same concept as a NumpPy's reshape.

Note that a number of Blaze features will place new constraints in what it means
'compatible shapes'. Variable sized dimensions as well as the ability to grow an
array may place limits to reshaping.

ravel, flatten can be seen as related in NumPy as well.



Blaze CKernel Interface
=======================

 * [Blaze Execution System](blaze-execution.md)

This is a draft specification for a low level kernel
interface ABI. The purpose of this interface is for
third-party code to build kernels for blaze, and for
blaze to build kernels usable by third-party code.
This is a work in progress, if you would like to
discuss this design, please email the public blaze
development mailing list at blaze-dev@continuum.io

Interface Goals
---------------

The goals of this interface are:

 * To communicate low level executable kernels between
   Blaze, DyND, Numba, and other third-party library code.
   Third-party code shouldn't have to have a deep
   understanding of the blaze system to provide
   or consume kernels.
 * To allow safe, correct usage across any ABI boundary,
   i.e. to not rely on linking to a particular C/C++
   standard library, the Python API, a common blaze API, etc.
 * To allow thread-safe execution. An execution engine
   should be able to call any kernel following this
   specification at the same time on many different threads.

This interface provides a common denominator binary ABI
format for kernels, similar to how '.o' or '.obj' files
provide a format for compiled code before it gets linked
together.

CKernel Definition
------------------

A blaze ckernel is a chunk of memory which begins with
two function pointers, a ckernel function pointer and a
destructor. This is followed by an arbitrary amount of
additional memory owned by the ckernel.

The ckernel function prototype is context-specific,
it must be known by the caller from separate information.
It may be known because the ckernel is the result of
a request for a specific kind of assignment ckernel, or
for a binary predicate, for example.

The memory of a blaze ckernel must satisfy the following
restrictions:

 * Its alignment must be that of a pointer, i.e.
   4 on 32-bit platforms and 8 on 64-bit platforms.

 * Its total size must be divisible by the alignment.

 * It must be relocatable using a memcpy.

   - It cannot contain pointers to other
     locations within itself (must use offsets for this).

   - It cannot assume an address within its memory
     is aligned greater than pointer alignment, because
     other code could memcpy it to a location which
     changes that alignment.

As a C struct, this looks like

```cpp
struct ckernel_prefix;
typedef void (*destructor_fn_t)(ckernel_prefix *);

struct ckernel_prefix {
    void *function;
    destructor_fn_t destructor;
};

struct ckernel_data {
    ckernel_prefix base;
    /* Additional kernel data... */
};
```

An example ckernel function prototype is a single assignment ckernel,
which assigns one value from a source memory location
to a destination.

```cpp
typedef void (*unary_single_operation_t)(
                char *dst, const char *src,
                ckernel_prefix *extra);
```

Error Handling
--------------

Requires specification and implementation!

How errors are handled in ckernels needs to be defined. In DyND,
this is done with C++ exceptions, but this does not appear to be
reasonable in a cross-platform/cross-language way.

CKernel Builder
---------------

Blaze ckernels may be allocated on the stack or the heap,
and are usually contained within a ckernel_builder object.
This object is defined by libdynd, which exposes C constructor,
destructor, and operation functions for code using a ckernel.
The ckernel_builder object itself may be on the stack or on
the heap, as the code managing the object sees fit.

Note that this object is not copyable or relocatable, it must
be constructed and destructed from the same memory location.

```cpp
struct ckernel_builder {
    // Pointer to the kernel function pointers + data
    char *m_data;
    intptr_t m_capacity;
    // When the amount of data is small, this static data is used,
    // otherwise dynamic memory is allocated when it gets too big
    intptr_t m_static_data[16];
};
```

The pointer `m_data` points to a block of memory, guaranteed to
be aligned to `sizeof(void*)`, whose total size is `m_capacity`.
When initialized, `m_data` points to the data in `m_static_data`,
so small ckernels can be constructed with

CKernel Builder Functions From DyND
-----------------------------------

DyND's `dynd._lowlevel` namespace provides some functions to
construct and work with ckernel_builder objects. These functions
are in the form of `ctypes` function pointers.

When using the object, get the contained ckernel by directly
reading `m_data`, and its capacity by directly reading
`m_capacity`.

`void _lowlevel.ckernel_builder_construct(void *ckb)`

Given an appropriately sized (18 * sizeof(void *)) and
aligned (sizeof(void *)) buffer, this constructs a ckernel_builder
object in that memory.

`void _lowlevel.ckernel_builder_destruct(void *ckb)`

Given a memory pointer which was previously constructed with
`ckernel_builder_construct`, this destroys it, freeing any
memory it may own.

`void _lowlevel.ckernel_builder_reset(void *ckb)`

Given a `ckernel_builder` instance, this resets it to a state
equivalent to being newly constructed.

`int _lowlevel.ckernel_builder_ensure_capacity_leaf(void *ckb, intptr_t requested_capacity)`

Given a `ckernel_builder` instance, this ensures that the ckernel
builder has at least the requested capacity. Returns 0 on success and
-1 on failure. If it succeeds, it is guaranteed that the `m_capacity`
field of the ckernel builder is at least `requested_capacity`.

`int _lowlevel.ckernel_builder_ensure_capacity(void *ckb, intptr_t requested_capacity)`

This is just like the matching `*_leaf` function, but allocates extra
space of `sizeof(ckernel_prefix)`, intended for ckernel factories which
produce ckernels with children.

More Documentation
------------------

The proposed interface is also used in DyND.
The documentation for kernels there provide some
examples of how kernels can be constructed and how a kernel
factory API might look:

https://github.com/ContinuumIO/libdynd/blob/master/documents/ckernels.md

DataDescriptor as a 'Storage Descriptor' too
============================================

Rational
--------

The previous situation of having all the storage properties
centralized in the `Storage` class is getting the code unnecessarily
complicated (see https://github.com/ContinuumIO/blaze/pull/205).  Some
kind of specialization at the API level is highly desirable so that
the specific flags for each storage can be dealt separately.

This proposal advocates for improving the different `DataDescriptor`
classes for every supported format (currently DyND, BLZ, HDF5, CSV and
JSON) so that they can deal with the storage capabilities internally.
This will actually render the `Storage` class and different accessors
(`from_*()`) useless and will effectively change the API of several
important functions (array constructors and accessors mainly).

Also, besides of deprecating the `Storage` class, the different
`DataDescriptors` (renamed to `DDesc`, for brevity) are first
class citizens that the users will have to know about.

This document explains with detail the implications of the change.

The current API
---------------

The Blaze array constructors previously look like::

  array(obj, dshape=None, caps={'efficient-write': True}, storage=None)
  empty(dshape, caps={'efficient-write': True}, storage=None)
  zeros(dshape, caps={'efficient-write': True}, storage=None)
  ones(dshape, caps={'efficient-write': True}, storage=None)

And the Blaze accessors of existing, persistent datasets::

  from_blz(persist, **kwargs)
  from_csv(persist, **kwargs)
  from_json(persist, **kwargs)
  from_hdf5(persist, datapath, **kwargs)

where **kwargs refers to the specific parameters depending on each format.

The new API
-----------

With this proposal implementation, the array constructors look like::

  array(obj, dshape=None, ddesc=None)
  empty(dshape, ddesc=None)
  zeros(dshape, ddesc=None)
  ones(dshape, ddesc=None)

And for accessing an existing dataset::

  ddesc = BLZ_DDesc(path, **kwargs) # or
  ddesc = CSV_DDesc(path, **kwargs) # or
  ddesc = JSON_DDesc(path, **kwargs) # or
  ddesc = HDF5_DDesc(path, datapath, **kwargs) # followed by
  array(ddesc=ddesc)  # or just
  array(ddesc)

where most of the `**kwargs` in the data descriptors are passed to the
underlying libraries in charge of handling the format details (BLZ,
HDF5, CSV, JSON...).

Also, as the different `DDesc` interfaces are public, exposing it from
the `Array` object is possible now::

  Array.ddesc -> the DDesc associated with the Blaze `Array`

Pros and cons of this proposal
------------------------------

Pros:

* There is a specialized `DDesc` class per each storage format. This
  provides a better way to deal with the specifics for each format.

* The `caps` parameter is not there anymore, so the constructors API
  is simplified.

Cons:

* The user will need to know about the kind of the format he will
  need, and Blaze will not decide for her anymore.

* That's a hard change in public API.

Outcome
-------

This proposal has been approved and implemented following the terms
expressed here.

[TOC]

Blaze Deferred CKernel Interface
================================

 * [Blaze Execution System](blaze-execution.md)

This is a draft specification for a low level kernel
interface ABI. The purpose of this specification is
to provide a low level interface, just above the
[ckernel interface](ckernel-interface.md), which has
a little bit more flexibility to bind a given kernel
to dynd memory arrays with the same types but different
metadata. This also provides the mechanism to communicate
third party kernels to blaze and dynd.

Interface Goals
---------------

 * To provide a way to wrap third party functions, that
   can lower to different specific ckernel prototypes
   like strided or single, or with different dynd metadata,
   when the blaze execution system needs it.
 * Specific cases include wrapping numpy ufuncs/gufuncs,
   and being a target for blaze JIT compiling (to delay actual JIT
   until single or strided kernel is requested).

Deferred Kernel Definition
--------------------------

The deferred kernel is similar to the dynamic kernel instance
which wraps one particular ckernel. It is a by-value struct,
which owns an opaque allocation of memory. It has a little bit
of information about the kind of ckernel it generates, including
the size of the ckernel data,

```cpp
enum kernel_type_t {
    /** Kernel function unary_single_operation_t or expr_single_operation_t */
    kernel_request_single,
    /** Kernel function unary_strided_operation_t or expr_strided_operation_t*/
    kernel_request_strided
};

enum kernel_funcproto_t {
    unary_operation_funcproto_id,
    expr_operation_funcproto_id,
    binary_predicate_funcproto_id
};

/**
 * Function prototype for instantiating a ckernel from a
 * deferred_ckernel (dckernel). To use this function, the
 * caller should first allocate the appropriate
 * amount of memory (dckernel->ckernel_size) with the alignment
 * required (sizeof(void *)). When the data types of the kernel
 * require metadata, such as for 'strided' or 'var' dimension types,
 * the metadata must be provided as well.
 *
 * \param self_data_ptr  This is dckernel->data_ptr.
 * \param out_ckb  A ckernel_builder into which the ckernel is placed
 * \param ckb_offset  An offset within the out_ckb ckernel where to place it.
 * \param dynd_metadata  An array of dynd metadata pointers,
 *                       matching ckrenel->data_dynd_types.
 * \param kerntype  Either kernel_request_single or kernel_request_strided,
 *                  as required by the caller.
 *
 * \returns  The offset immediately after the created ckernel. This must
 *           be returned so ckernels with multiple children can place them
 *           one after another.
 */
typedef intptr_t (*instantiate_fn_t)(void *self_data_ptr,
                dynd::ckernel_builder *out_ckb, intptr_t ckb_offset,
                const char *const* dynd_metadata, uint32_t kerntype);

struct deferred_ckernel {
    // A value indicating what type of ckernel this instantiates,
    // an enumeration value from kernel_funcproto_t.
    // 0: unary_single_operation_t/unary_strided_operation_t
    // 1: expr_single_operation_t/expr_strided_operation_t
    // 2: binary_single_predicate_t
    size_t ckernel_funcproto;
    // The number of types in the data_types array
    size_t data_types_size;
    // An array of dynd types for the kernel's data pointers.
    // Note that the builtin dynd types are stored as
    // just the type ID, so cases like bool, int float
    // can be done very simply.
    // This array should be part of the memory for data_ptr.
    const dynd::base_type * const* data_dynd_types;
    // A pointer to typically heap-allocated memory for
    // the deferred ckernel. This is the value to be passed
    // in when calling instantiate_func and free_func.
    void *data_ptr;
    // The function which instantiates a ckernel
    instantiate_fn_t instantiate_func;
    // The function which deallocates the memory behind data_ptr.
    void (*free_func)(void *self_data_ptr);
};
```

Simple Deferred CKernel Example
-------------------------------

To illustrate what this interface allows, let's start with a really
simple example of a deferred ckernel that defines an operation
which triples an int32. This example is defined statically,
it does not require memory management.

```cpp
// Kernel function which processes a single element
void int32_triple_single(char *dst, const char * const *src,
                kernel_data_prefix *extra)
{
    int32_t val = *(const int32_t *)src[0];
    val *= 3;
    *(int32_t *)dst = val;
}

// Kernel function which processes a strided array of elements
void int32_triple_strided(char *dst, intptr_t dst_stride,
                const char * const *src, const intptr_t *src_stride,
                size_t count, kernel_data_prefix *extra)
{
    const char *src0 = src[0];
    intptr_t src0_stride = src_stride[0];

    for (size_t i = 0; i < count; ++i) {
        int32_t val = *(const int32_t *)src0;
        val *= 2;
        *(int32_t *)dst = val;

        dst += dst_stride;
        src0 += src0_stride;
    }
}

static intptr_t instantiate_triple_int32(void *self_data_ptr,
                dynd::ckernel_builder *out_ckb, intptr_t ckb_offset,
                const char *const* dynd_metadata, uint32_t kerntype)
{
    // Make sure the ckernel builder has enough space
    intptr_t ckb_end = ckb_offset + sizeof(ckernel_prefix);
    ckernel_builder_ensure_capacity_leaf(out_ckb, ckb_end);
    ckernel_prefix *out_ckernel = (ckernel_prefix *)(out_ckb->data + ckb_offset);
    if (kerntype == kernel_request_single) {
        out_ckernel->function = (void *)&int32_triple_single;
    } else if (kerntype == kernel_request_strided) {
        out_ckernel->function = (void *)&int32_triple_strided;
    } else {
        // raise an error...
    }

    return ckb_end;
}

static void empty_free_func(void *self_data_ptr)
{
}

static const dynd::base_type *triple_data_types = {
    (const dynd::base_type *)dynd::int32_type_id,
    (const dynd::base_type *)dynd::int32_type_id
};

static deferred_ckernel triple_kernel_int32 = {
    // ckernel_funcproto
    expr_operation_funcproto_id,
    // ckernel_size
    sizeof(dynd::kernel_data_prefix),
    // data_types_size
    2,
    // data_dynd_types
    &triple_data_types,
    // data_ptr
    NULL,
    // instantiate_func
    &instantiate_triple_int32,
    // free_func
    &empty_free_func
};
```

Deferred CKernel Example With Additional Data
---------------------------------------------

To take the `triple` kernel one step further, lets make the
factor of three a parameter instead of hardcoded in the kernel.
To do this, the deferred ckernel will need an additional data item,
which gets placed in the ckernel during instantiation.

```cpp
struct constant_int32_multiply_kernel_extra {
    // A typedef for the kernel data type
    typedef constant_int32_multiply_kernel_extra extra_type;

    // All ckernels are prefixed like this
    dynd::kernel_data_prefix base;
    // The extra data goes after
    intptr_t factor;

    // The kernel function which executes the operation once
    static void single(char *dst, const char * const *src,
                    kernel_data_prefix *extra)
    {
        extra_type *e = reinterpret_cast<extra_type *>(extra);
        int32_t val = *(const int32_t *)src[0];
        val *= (int32_t)e->factor;
        *(int32_t *)dst = val;
    }

    // The kernel function which executes the operation
    // across strided arrays.
    static void strided(char *dst, intptr_t dst_stride,
                const char * const *src, const intptr_t *src_stride,
                size_t count, kernel_data_prefix *extra)
    {
        extra_type *e = reinterpret_cast<extra_type *>(extra);
        int32_t factor = (int32_t)e->factor;
        const char *src0 = src[0];
        intptr_t src0_stride = src_stride[0];

        for (size_t i = 0; i < count; ++i) {
            int32_t val = *(const int32_t *)src0;
            val *= 2;
            *(int32_t *)dst = val;

            dst += dst_stride;
            src0 += src0_stride;
        }
    }
};

struct deferred_constant_int32_multiply_kernel_extra {
    intptr_t factor;
};

static intptr_t instantiate_deferred_constant_int32_multiply(void *self_data_ptr,
                dynd::ckernel_builder *out_ckb, intptr_t ckb_offset,
                const char *const* dynd_metadata, uint32_t kerntype)
{
    deferred_constant_int32_multiply_kernel_extra *self =
            (deferred_constant_int32_multiply_kernel_extra *)self_data_ptr;
    intptr_t ckb_end = ckb_offset + sizeof(constant_int32_multiply_kernel_extra);
    constant_int32_multiply_kernel_extra *out_ckernel =
            (constant_int32_multiply_kernel_extra *)(out_ckb->data + ckb_offset);
    if (kerntype == kernel_request_single) {
        out_ckernel->base.function = (void *)&constant_int32_multiply_kernel_extra::single;
    } else if (kerntype == kernel_request_strided) {
        out_ckernel->base.function = (void *)&constant_int32_multiply_kernel_extra::strided;
    } else {
        // raise an error...
    }

    // Copy the multiplication factor to the ckernel
    out_ckernel->factor = self->factor;
    return ckb_end;
}

static const dynd::base_type *constant_int32_multiply_data_types = {
    (const dynd::base_type *)dynd::int32_type_id,
    (const dynd::base_type *)dynd::int32_type_id
};

/**
 * This function builds a deferred ckernel for multiplying
 * int32 values by the specified factor.
 */
void make_deferred_constant_int32_multiply_kernel(
                deferred_ckernel *out,
                intptr_t factor)
{
    // Indicates which ckernel function prototype this provides
    out->ckernel_funcproto = expr_operation_funcproto_id;
    // How big the ckernel to be generated is
    out->ckernel_size = sizeof(constant_int32_multiply_kernel_extra);
    // How many data types the kernel refers to
    out->data_types_size = 2;
    // The array of data types. In a more dynamic example,
    // this can point inside of the out->data_ptr memory.
    out->data_dynd_types = constant_int32_multiply_data_types;
    // Pointer to data owned by the deferred ckernel
    out->data_ptr = malloc(deferred_constant_int32_multiply_kernel_extra);
    (deferred_constant_int32_multiply_kernel_extra *)out->data_ptr->factor = factor;
    // The function to instantiate a ckernel
    out->instantiate_func = &instantiate_deferred_constant_int32_multiply;
    // The destructor function for the deferred ckernel.
    out->free_func = &free;
}
```

Using a Deferred CKernel
------------------------

It is a bit more involved to use a deferred ckernel than a ckernel,
because one generally has to check which function prototype it uses,
as well as which types the arguments have. We're going do a simplified
example which constructs a deferred ckernel, then instantiates it
in both the `single` and `strided` variants.

```cpp
void call_single(ckernel_deferred *ckd)
{
    // This example is for a unary expr_function
    assert(ckd->ckernel_funcproto == expr_operation_funcproto);
    assert(ckd->data_types_size == 2);

    // Create a ckernel_builder object
    // (In C++ libdynd, just "ckernel_builder ckb;", a normal object).
    ckernel_builder_struct ckb;
    ckernel_builder_construct(&ckb);

    // Instantiate the ckernel
    const char *meta[2] = {0, 0}; // Pointers to dynd metadata for the operands
    ckd->instantiate_func(ckd->data_ptr, &ckb, 0, meta, kernel_request_single);

    ckernel_prefix *ck = (ckernel_prefix *)ckb->m_data;

    // Get the kernel function
    expr_single_operation_t kfunc;
    kfunc = (expr_single_operation_t)ck->function;

    // Set up some sample data for the kernel
    int32_t dst_val = 0, src_val = 12;
    char *dst = (char *)&dst_val;
    const char *src = (const char *)&src_val;
    // Call the kernel
    kfunc(dst, &src, ck);
    printf("called kernel: %d -> %d\n", (int)src_val, (int)dst_val);

    // Destroy the ckernel instance
    ckernel_builder_destruct(&ckb);
}

void call_strided(deferred_ckernel *ck)
{
    // This example is for a unary expr_function
    assert(ckd->ckernel_funcproto == expr_operation_funcproto);
    assert(ckd->data_types_size == 2);

    // Create a ckernel_builder object
    // (In C++ libdynd, just "ckernel_builder ckb;", a normal object).
    ckernel_builder_struct ckb;
    ckernel_builder_construct(&ckb);

    // Instantiate the ckernel
    const char *meta[2] = {0, 0}; // Pointers to dynd metadata for the operands
    ckd->instantiate_func(ckd->data_ptr, &ckb, 0, meta, kernel_request_strided);

    ckernel_prefix *ck = (ckernel_prefix *)ckb->m_data;

    // Get the kernel function
    expr_strided_operation_t kfunc;
    kfunc = (expr_strided_operation_t)ck->function;

    // Set up some sample data for the kernel
    int32_t dst_val[3] = {0, 0, 0}, src_val[3] = {12, -5, 3};
    char *dst = (char *)&dst_val[0];
    const char *src = (const char *)&src_val[0];
    intptr_t src_stride = sizeof(int32_t);
    // Call the kernel
    kfunc(dst, sizeof(int32_t), &src, &src_stride, ck);
    printf("called kernel:");
    for (int i = 0; i < 3; ++i)
        printf("[%d]: %d -> %d\n", i, (int)src_val[i], (int)dst_val[i]);

    // Destroy the ckernel instance
    ckernel_builder_destruct(&ckb);
}

void example_deferred_ckernel_usage()
{
    // Construct the constant int32 multiplication kernel (previous example)
    deferred_ckernel dc;
    make_deferred_constant_int32_multiply_kernel(&dc, 13)

    // Use it to get a single kernel
    call_single(&dc);

    // Use it to get a strided kernel
    call_strided(&dc);

    // Destroy the deferred ckernel
    dc->free_func(dc->data_ptr);
    memset(dc, 0, sizeof(dc));
}
```
﻿Elementwise Reduction UFuncs
=============================

As summarized in the [NumPy API Doc](blaze-numpy-api.md),
a number of useful reduction operations can be
computed in an elementwise fashion.

The particular functions we're most interested in are:

 * `all`, `any`: reduction on boolean `and`, `or`.
 * `sum`, `product`: reduction on `add`, `multiply`.
 * `max`, `min`: reduction on `maximum`, `minimum`.
 * `mean`, `std`, `var`: arithmetic mean, standard
   deviation, and variance.
 * `nansum`, `nanmin`, `nanmax`, `nanmean`, `nanstd`,
   `nanvar`: Versions of the above which map NaN values
   to "nonexistent", i.e. the array [1, NaN, 3] is
   considered to have length 2 instead of 3 like normal.

See also

* [Rolling Reduction UFuncs](rolling-reduction-ufuncs.md)

Keyword Arguments of Elementwise Reductions
-------------------------------------------

NumPy has two keyword arguments that are worth
keeping in this kind of operation for Blaze,
`axis=` and `keepdims=`. The parameter to `axis`
may be a single integer, or, when the operation
is commutative and associative, a tuple of integers.
The `keepdims` parameter keeps the reduced dimensions
around as size 1 instead of removing them, so the
result still broadcasts appropriately against
the original.

Properties of Elementwise Reductions
-------------------------------------

### Input Type

This is the type of the elements that are being reduced.

### Accumulation Type

The accumulation type may be different from the
input type in a reduction. For example, for `mean`,
it may be a struct `{sum: float64, count: int64}`,
where the input type is `float64`.

### Initial State (Identity Element)

When the input and accumulation types are the same,
a reduction may have an identity, which is an element
`x` such that `op(x, y) == y` for all `y`. The identity
may be used as the initial state of the accumulators.

When the input and accumulation types are different, this
algebraic identity does not make sense, but an initial
state such as `[0.0, 0]` in the `mean` case still
makes sense.

### Initialization Function

When there is no initial state that can be used,
an initialization function which sets the initial
state from the first element being accumulated may
be used instead. Most commonly, this will be a copy
operation, but when the input and accumulation types
are different, something different may be required.
For NumPy compatibility, only the copy operation is
needed.

In NumPy, the NDIter object has the ability to
report whether a reduction operand is being
visited for the first time. This exists for the purpose
of knowing when to call the initialization function.
[See the NumPy Doc for this.](http://docs.scipy.org/doc/numpy/reference/c-api.iterator.html#NpyIter_IsFirstVisit)

## Accumulator Combine Function

When a reduction kernel is associative, it may be
useful to provide a function that combines accumulator
values together. For example, in the case of `mean`,
it would add the `sum` and `count` fields of the
inputs together. This may be used by a multi-threaded
execution engine.

### Associativity and Commutativity

An elementwise reduction might be associative
and/or commutative. Associativity means the identity
`op(op(x, y), z) == op(x, op(y, z))` holds for all
`x`, `y`, and `z`. Commutativity means the identity
`op(x, y) == op(y, x)` holds for all `x` and `y`.
When both of these hold, the order in which elements
are visited can be arbitrarily rearranged, which
is useful for optimizing memory access patterns and
exploiting concurrency.

This information needs to get to the execution
engine somehow, to affect how the elementwise
reduction ckernel is lifted to an assignment
ckernel, or how distributed scheduling is controlled.

Per-Kernel vs Per-Function Properties
-------------------------------------

The identity and the associativity/commutativity
flags must be per-kernel properties. To see this,
we'll give a few examples.

First, consider `product`. For integers, floats,
complex, and quaternions, it has identity `1`.
For all but quaternions, it is both associative and
commutative. This means `blaze.product(complex_2d_array)`
makes sense, but `blaze.product(quat_2d_array)` does
not, because the lack of commutativity prevents an
unambiguous definition.

Similarly, consider `sum`. For numeric types, it's
both associative and commutative, but for strings,
it's not commutative. This results in the same situation
as `product` does for quaternions.

```
>>> blaze.sum([[1, 2], [3, 4]])
array(10,
      dshape='int32')

>>> blaze.sum([[1, 2], [3, 4]], axis=0)
array([4, 6],
      dshape='2 * int32')

>>> blaze.sum([[1, 2], [3, 4]], axis=1)
array([3, 7],
      dshape='2 * int32')

>>> blaze.sum([['this', 'is'], ['a', 'test']])
BlazeError: blaze.sum output of 2D reduction is ambiguous
due to lack of commutativity

>>> blaze.sum([['this', 'is'], ['a', 'test']], axis=0)
array(['thisa', 'istest'],
      dshape='2 * string')

>>> blaze.sum([['this', 'is'], ['a', 'test']], axis=1)
array(['thisis', 'atest'],
      dshape='2 * string')

>>> blaze.max([['this', 'is'], ['a', 'test']])
array('this',
      dshape='string')
```

Implementation Details
----------------------

### Reduction CKernel

A reduction ckernel uses the same prototype as
an assignment ckernel, except it both reads and writes
from the destination memory. When it is called,
the destination stride may be 0 (dot-product style)
or non-zero (saxpy style). For our `mean` function
example, we might code it like (pseudo-code like):

```
void kfunc(char *dst, intptr_t dst_stride,
           const char *src, intptr_t src_stride,
			 size_t count,
           ckernel_prefix *extra)
{
	if (dst_stride == 0) {
       // dot-product style
       dst->sum += sum(src, src_stride, count);
       dst->count += count;
   } else {
       // saxpy style
       for (i = 0; i < count; ++i) {
           dst->sum += *src;
           dst->count ++;
           dst += dst_stride;
           src += src_stride;
       }
   }
}
```

### Lifting a Reduction CKernel

Lifting a reduction ckernel means adding code which
handles the dimensions appropriately, matching the
output appropriately depending on whether the
`keepdims` argument was provided as True, and
processing dimensions differently based on which
values are in the `axis` argument.

In the more general case, we want to also be able
to lift a "map", i.e. a ckernel which takes any
number of inputs, and broadcasts them elementwise
through an expression, followed by a "reduce". For
the loops to be fused properly in the computation,
this combined "map-reduce" must be lifted as one
entity.

In the case of `mean`, we then want to tack on another
"map" which takes the sum divided by the count, but
that can be separate from the reduction lifting.

Reduction BlazeFunc signatures
------------------------------

Reduction function signatures operate via a dependent
type mechanism. The way this works is a reduction
function has a type signature like

```
(DimsIn... * InputDType) -> DimsOut... * OutputDType
```

and a Python function has an opportunity to fill
in the type variable values in the return type. The
ReductionBlazeFunc accepts ``axis=`` and ``keepdims=``
keyword arguments, which are used to control how
the dependent return type is computed. Here's
some example code:

```
>>> a = nd.array(..., dshape='3 * 5 * var * int32')
>>> b = sum(a, axis=1, keepdims=False)
>>> b.dshape
dshape('3 * 1 * var * int32')
```

In this example, the ``DimsIn...`` type variable matched
with ``3 * 5 * var *``, and ``InputDType`` matched with
``int32``. For ``sum``, the input and output dtypes are
the same, and the parameters ``axis=1, keepdims=False``
give the value ``3 * 1 * var *`` to ``DimsOut...``.

Here are some examples of what the type signatures
of various reductions look like for a `sum` example.

```
>>> a
array([...],
      dshape='3 * var * 5 * int32')
>>> blaze.sum(a).sig
'(3 * var * 5 * int32) -> int32'

>>> blaze.sum(a, keepdims=True).sig
'(3 * var * 5 * int32) -> 1 * 1 * 1 * int32'

>>> blaze.sum(a, axis=0).sig
'(3 * var * 5 * int32) -> var * 5 * int32'

>>> blaze.sum(a, axis=1).sig
'(3 * var * 5 * int32) -> 3 * 5 * int32'

>>> blaze.sum(a, axis=[0,2], keepdims=True).sig
'(3 * var * 5 * int32) -> 1 * var * 1 * int32'

>>> blaze.sum(a, axis=[0,2]).sig
'(3 * var * 5 * int32) -> var * int32'

>>> blaze.sum(a, axis=[1,2], keepdims=True).sig
'(3 * var * 5 * int32) -> 3 * 1 * 1 * int32'

>>> blaze.sum(a, axis=[1,2]).sig
'(3 * var * 5 * int32) -> 3 * int32'
```

Something not fully addressed yet is in the case of
`mean`, `std`, and `var`, where there needs to be a way
to tack on the final divide or expression that computes
the final value after the reduction accumulators
are finished.

﻿# Rolling Reduction UFuncs

## General Discussion

A rolling reduction is a function whose domain is
intervals of the source array as follows:

```
def rolling_reduce(op, a, winsize):
    """
    Apply `op` as a rolling reduction, with trailing
    windows.
    """
    # Start with NaN until we reach the window size
    res = [float('nan')] * (winsize - 1)
    for i in range(winsize - 1, len(a)):
        win = a[i - winsize + 1: i + 1]
        res.append(op(win))
    return res

def rolling_max(a, winsize):
    from functools import reduce # in Python 3
    def arrmax(win):
        return reduce(max, win)
    return rolling_reduce(arrmax, a, winsize)

>>> rolling_max([3, 2, -1, 0, 0, 5, 2, 2, 2], 3)
[nan, nan, 3, 2, 0, 5, 5, 5, 2]

>>> rolling_max([1, 3, 7, float('nan'), 6, 2, 7, float('inf')], 3)
[nan, nan, 7, 7, 7, nan, 7, inf]
```

Note that the handling of NaNs with this function is
incorrect, because NaN is not orderable.

```
>>> reduce(max, [float('nan'), 2, 1])
nan

>>> reduce(max, [1, float('nan'), 2])
2
```

Here's one way to fix this:

```
def rolling_max(a, winsize):
    from functools import reduce # in Python 3
    def arrmax(win):
        if all(x == x for x in win):
            return reduce(max, win)
        else:
            return float('nan')
    return rolling_reduce(arrmax, a, winsize)

>>> rolling_max([1, 3, 7, float('nan'), 6, 2, 7, float('inf')], 3)
[nan, nan, 7, nan, nan, nan, 7, inf]
```

Pandas adds some flexibility to handling of NaN values
by adding a 'minp'/'min_periods' parameter, which
is a minimum number of non-NaN values to require for
getting a finite result. Code for it might look
like this:

```
def rolling_max(a, winsize, minp):
    from functools import reduce # in Python 3
    def arrmax(win):
        present = sum(x == x for x in win)
        if present >= minp:
            return reduce(max, (x for x in win if x == x))
    return rolling_reduce(arrmax, a, winsize)

>>> rolling_max([1, 3, 7, float('nan'), 6, 2, 7, float('inf')], 3, 2)
[nan, nan, 7, 7, 7, 6, 7, inf]
```

## References

* https://en.wikipedia.org/wiki/Moving_average
* http://pandas.pydata.org/pandas-docs/stable/computation.html#moving-rolling-statistics-moments

## Pandas Implementation

The core of Pandas' rolling sum and rolling mean are
here in the codebase:

https://github.com/pydata/pandas/blob/master/pandas/algos.pyx#L840

Characteristics of the implementation include:

* Outputs NaNs until the window size is reached.
* Keeps track of the number of non-NaNs encountered
* Has parameter `minp` controlling the number of
  non-NaN values required to return a finite value.
* 
* The data is preprocessed with a 'kill_inf'
  option converting infinities to NaN.
  * This has implications, such as rolling_max
    incorrectly giving NaN instead of +inf
    where +inf appears, or NaN instead of the
    valid finite values where -inf appears.

## Miscellaneous Notes

* In science and engineering, a centred window is
  common, but in financial applications a trailing
  window is used. I assume the latter is because in
  you don't want the statistic to include information
  from the future in a time series analysis.

# Efficient Implementations

For a number of rolling reductions, it's possible to
create implementations that do not have a ``O(winsize)``
factor in them. Here we describe a few of these.

## Rolling Sum

We are computing the sequence

```
S_i = a_(i-k+1) + ... + a_i
```

and with a simple substraction, we get the ``O(1)`` update
formula

```
S_(i+1) = S_i + a_(i+1) - a_(i-k+1)
```

Both NaN and inf values cause some small computations,
because they don't satisfy ``(x - inf) + inf == x``.
Correct handling of these requires counting instances of
inf and NaN.

Additionally, this update formula will not produce the
exact same answer as recomputing the sum for each window,
because of floating point inaccuracy. Using a higher
precision float for the accumulator than the array values
is one way to mitigate this a bit. See the next rolling
mean section for a particular example of this which
was encountered in Pandas.

## Rolling Mean

Mean is a slight modification to sum, where the count of
non-NaN elements is used as the divisor.

In the Pandas implementation, an additional tweak is done,
where the number of negative values is tracked
separately, and the value is clamped to zero if the value
is negative when no negative values are in the sum. This
was introduced to deal with https://github.com/pydata/pandas/issues/2527,
where a sum of all non-negative numbers was producing a
negative value, causing an "impossible" square root failure.

Taking a look at the code for the particular failing example,
it looks like doing the subtract before the add would fix the
issue, though likely not fix it in general. Here's how
changing the order of computation affects the result in that
issue:

```
>>> ((0.00012456 + 0.0003) - 0.00012456) - 0.0003
-5.421010862427522e-20

>>> ((0.00012456 - 0.00012456) + 0.0003) - 0.0003
0.0
```

This issue affects both rolling sum and rolling
mean, so we should handle it the same in both.

## Rolling Min/Max

* http://people.cs.uct.ac.za/~ksmith/articles/sliding_window_minimum.html
* http://richardhartersworld.com/cri/2001/slidingmin.html

This algorithm is sometimes known as Ascending Minima.

The trick with rolling min/max is to keep track of the set of
all values which might affect a future result. Let's take
a look at what these sets look like for the example data we
used in the general discussion.

```
Value    Set
-----    ---
3        {3}
2        {3, 2}
-1       {3, 2, -1}
0        {2, 0}
0        {0}
5        {5}
2        {5, 2}
2        {5, 2}
2        {2}

Value    Set
-----    ---
1
3
7        {7}
nan      {nan}
6        {nan, 6}
2        {nan, 6, 2}
7        {7}
inf      {inf}
3        {inf, 3}
```

Since NaN is not part of the totally ordered set, it needs to
be tracked separately, but the rest of the values including
inf can be handled uniformly. We need a bounded deque, which
could be C++'s std::deque, or a circular buffer like boost has
(http://www.boost.org/doc/libs/1_55_0/doc/html/circular_buffer.html).
We store pairs ``(val, index)`` in this buffer, in sorted
order. When processing, we always discard pairs whose
``index`` is too old, or which can no longer affect a later
value because the new source value is dominant.

Let's work out the values seen in the buffer:

```
Index Value    Buffer
----- -----    ---
0     3        [(3, 0)]
1     2        [(3, 0), (2, 1)]
2     -1       [(3, 0), (2, 1), (-1, 2)]
3     0        [(2, 1), (0, 3)]
4     0        [(0, 4)]
5     5        [(5, 5)]
6     2        [(5, 5), (2, 6)]
7     2        [(5, 5), (2, 7)]
8     2        [(2, 8)]

Index Value    Buffer
----- -----    ------
0     1        [(1, 0)]
1     3        [(3, 1)]
2     7        [(7, 2)]
3     nan      [(nan, 3)]
4     6        [(nan, 3), (6, 4)]
5     2        [(nan, 3), (6, 4), (2, 5)]
6     7        [(7, 6)]
7     inf      [(inf, 7)]
8     3        [(inf, 7), (3, 8)]
```

And some Python code which includes tracking the
number of values present for `minp`:

```
def rolling_max(a, winsize, minp):
    result = []
    buffer = []
    present = 0
    nanindex = -1
    for i, el in enumerate(a):
        if el == el:
            present += 1
            # Remove any smaller values from the back of `buffer`
            while buffer and el >= buffer[-1][0]:
                del buffer[-1]
            # Add this value and index to the back of `buffer`
            buffer.append((el, i))

        # Remove any expired indices from the front of `buffer`
        while buffer and buffer[0][1] + winsize <= i:
            del buffer[0]

        # Append a value to the result
        if i >= winsize - 1 and present >= minp:
            result.append(buffer[0][0])
        else:
            result.append(float('nan'))

        # For illustrative purposes
        print(present, buffer)

        if i >= winsize - 1:
            # Keep track of # of non-NaN values
            el = a[i - winsize + 1]
            if el == el:
                present -= 1
    print()
    return result

>>> rolling_max([3, 2, -1, 0, 0, 5, 2, 2, 2], 3, 3)
1 [(3, 0)]
2 [(3, 0), (2, 1)]
3 [(3, 0), (2, 1), (-1, 2)]
3 [(2, 1), (0, 3)]
3 [(0, 4)]
3 [(5, 5)]
3 [(5, 5), (2, 6)]
3 [(5, 5), (2, 7)]
3 [(2, 8)]

[nan, nan, 3, 2, 0, 5, 5, 5, 2]

<<< rolling_max([1, 3, 7, float('nan'), 6, 2, 7, float('inf')], 3, 3)
1 [(1, 0)]
2 [(3, 1)]
3 [(7, 2)]
2 [(7, 2)]
2 [(7, 2), (6, 4)]
2 [(6, 4), (2, 5)]
3 [(7, 6)]
3 [(inf, 7)]

[nan, nan, 7, nan, nan, nan, 7, inf]

>>> rolling_max([1, 3, 7, float('nan'), 6, 2, 7, float('inf')], 3, 2)
1 [(1, 0)]
2 [(3, 1)]
3 [(7, 2)]
2 [(7, 2)]
2 [(7, 2), (6, 4)]
2 [(6, 4), (2, 5)]
3 [(7, 6)]
3 [(inf, 7)]

[nan, nan, 7, 7, 7, 6, 7, inf]

>>> rolling_max([1, 0, float('nan'), float('nan'), float('nan'), 2, 3], 3, 2)
1 [(1, 0)]
2 [(1, 0), (0, 1)]
2 [(1, 0), (0, 1)]
1 [(0, 1)]
0 []
1 [(2, 5)]
2 [(3, 6)]

[nan, nan, 1, nan, nan, nan, 3]

```

See also

* [Elementwise Reduction UFuncs](elwise-reduction-ufuncs.md)

Blaze Documentation
===================

The blaze documentation is split over two repositories:

    The user and developer documentation:

         https://github.com/ContinuumIO/blaze/tree/master/docs (the 'docs' directory)

    The webpage, blaze.pydata.org:

        https://github.com/ContinuumIO/blaze-webpage/tree/gh-pages

User and Developer Documentation
================================
The documentation is under the 'docs' directory of the blaze repo (this directory).

To build the documentation, you need the basicstrap theme and sphinxjp.themecore:

    $ pip install sphinxjp.themes.basicstrap

You can edit the source files under docs/source/, after which you can build and
check the documentation:

    $ make html
    $ open build/html/index.html

This documentation can be uploaded to http://blaze.pydata.org/dev/index.html
using the gh-pages.py script under docs/:

    $ python gh-pages.py

then verify the repository under the 'gh-pages' directory and use 'git push'.

Webpage
=======
The blaze webpage on blaze.pydata.org can be fetched from here: https://github.com/ContinuumIO/blaze-webpage/tree/gh-pages
The documentation is linked to in the 'dev' subdirectory.

================
Data Descriptors
================

Data Descriptors are unadorned blaze arrays, objects which
expose data with a dshape but have no math or other facilities
attached. The data descriptor interface is analogous to the
Python buffer interface described in PEP 3118, but with some
more flexibility.

An object signals it is a data descriptor by subclassing from
the blaze.datadescriptor.DataDescriptor abstract base class,
and implementing the necessary methods.

Python-Style Iteration
======================

A data descriptor must expose the Python __iter__ method,
which should iterate over the outermost dimension of the data.
If the data is a scalar, it should raise an IndexError.

This iterator must return only data descriptors for the
iteration, and if the data is writable, the returned data descriptors
must point into the same data, not contain copies. This can
be tricky in some cases, for example NumPy returns a copy if
it is returning a scalar.

This style of iteration can handle variable-sized/ragged arrays,
because access is handled in a nested fashion one dimension at
a time. Element iteration cannot handle this kind of dimension,
because it requires that the data conform to a C storage layout
that does not include variable-sized arrays presently.

Note that this style of iteration cannot be used to get at
actual data values. For this, you must use the element
access mechanisms.

Python-Style GetItem
====================

A data descriptor must expose the Python __getitem__ method,
which should at a minimum support tuples of integers as
input.

Calling the [] operator on a data descriptor must
return another data descriptor. Just as for the iterator,
when the data is writable the returned data descriptors
must point into the same data.

Note that this style of access cannot be used to get at
actual data values. For this, you must use the element
access mechanisms.

Memory Access
=============

To get data from the data descriptors in a raw memory form,
there is a way to retrieve it as a DyND array via the
``ddesc.dynd_arr()`` method

<p align="center" style="padding: 20px">
<img src="https://raw.github.com/ContinuumIO/blaze/master/docs/source/svg/blaze_med.png">
</p>

**Blaze** is the next-generation of NumPy. It is designed as a
foundational set of abstractions on which to build out-of-core and
distributed algorithms over a wide variety of data sources and to extend
the structure of NumPy itself.

<p align="center" style="padding: 20px">
<img src="https://raw.github.com/ContinuumIO/blaze/master/docs/source/svg/numpy_plus.png">
</p>

Blaze allows easy composition of low level computation kernels
( C, Fortran, Numba ) to form complex data transformations on large
datasets.

In Blaze, computations are described in a high-level language
(Python) but executed on a low-level runtime (outside of Python),
enabling the easy mapping of high-level expertise to data without sacrificing
low-level performance. Blaze aims to bring Python and NumPy into the
massively-multicore arena, allowing it to able to leverage many CPU and
GPU cores across computers, virtual machines and cloud services.

<p align="center" style="padding: 20px">
<img src="https://raw.github.com/ContinuumIO/blaze/master/docs/source/svg/codepush.png">
</p>

Continuum Analytics' vision is to provide open technologies for data
integration on a massive scale based on a vision of a structured,
universal "data web". In the same way that URL, HTML, and HTTP form
the basis of the World Wide Web for documents, Blaze could
be a fabric for structured and numerical data spearheading
innovations in data management, analytics, and distributed computation.

Blaze aims to be a foundational project allowing many different users of
other PyData projects (Pandas, Theano, Numba, SciPy, Scikit-Learn)
to interoperate at the application level and at the library level with
the goal of being able to to lift their existing functionality into a
distributed context.

<p align="center" style="padding: 20px">
<img src="https://raw.github.com/ContinuumIO/blaze/master/docs/source/svg/sources.png">
</p>

Status
------

Blaze is a work in progress at the moment, currently at release 0.5.
Take a look at the [release notes](docs/source/releases.rst).

Documentation
-------------

* [Dev Docs](http://blaze.pydata.org/docs/)

Trying out Blaze
----------------

The easiest way to try out Blaze is through the Anaconda
distribution. The latest release includes a version of Blaze.

http://continuum.io/downloads

To make sure you're running the latest released version
of Blaze, use the
[conda package manager](http://docs.continuum.io/conda/index.html)
to update.

```bash
$ conda update blaze
```

Source code for the latest development version of blaze can
be obtained [from Github](https://github.com/ContinuumIO/blaze).

Dependencies
------------

The Blaze project itself is spread out over multiple projects,
in addition to the main `blaze` repo. These dependencies
are

  * [blz][blz]
  * [datashape][datashape]
  * [dynd-python][dynd-python]
  * [pykit][pykit]

To see a full list of dependencies, please see `requirements.txt`

[blz]: https://github.com/ContinuumIO/blz
[datashape]: https://github.com/ContinuumIO/datashape
[dynd-python]: https://github.com/ContinuumIO/dynd-python
[pykit]: https://github.com/pykit/pykit

Installing from Source
----------------------

Install all the pre-requisites using conda or another mechanism,
then run:

```bash
$ python setup.py install
```

Documentation is generated using sphinx from the docs directory.

Contributing
------------

Anyone wishing to discuss on Blaze should join the
[blaze-dev](https://groups.google.com/a/continuum.io/forum/#!forum/blaze-dev)
mailing list. To get started contributing, read through the
[Developer Workflow](docs/source/dev_workflow.rst) documentation.

License
-------

Blaze development is sponsored by Continuum Analytics.

Released under BSD license. See [LICENSE.txt](LICENSE.txt) for details.

﻿Blaze Release Procedure
=======================

This document describes the steps to follow to release
a new version of Blaze.

1. Update version numbers in the following locations:

 * /setup.py, in the setup(...) call.
 * /README.md where it mentions the current release.
 * /blaze/__init__.py

1. Confirm the dependencies and their version numbers in
   /docs/installing
   /requirements.txt

   In particular, `blz`, `dynd-python`, etc
   will typically be released concurrently with `blaze`,
   so they need to be updated to match.

1. Update the release notes /docs/source/releases.rst
   You may use a github URL like https://github.com/ContinuumIO/blaze/compare/0.4.2...master for assistance.

1. Build and update the documentation in gh-pages.

1. Verify build is working on all platforms. The
   jenkins builder internal to Continuum can assist
   with this.

1. Tag the release version.

1. Release email to blaze-dev@continuum.io.

1. Update this release procedure document to reflect
   what needed to be done for the release.

Samples
=======

Code samples

basics/	Some examples on blaze basics.

This is a tiny subset of a kiva JSON snapshot, as downloadable here:

    http://build.kiva.org/docs/data/snapshots

It's for testing the server on a small dataset, with identical
data shape to the full data set.
﻿Vagrant Images
==============

This directory contains a few vagrant images which
can be used to build and develop on the latest Blaze
and its related dependencies. To get started, you'll
first want to install
[Vagrant](http://www.vagrantup.com/downloads.html)
and [VirtualBox](https://www.virtualbox.org/wiki/Downloads).

These images work on Linux, Mac OS X, and Windows, and
are an easy way to get a consistent, isolated environment
for Blaze development and experimentation.

Starting a Vagrant Image
------------------------

To provision and start one of these images, use
the following commands:

```
$ cd saucy64-py33
$ vagrant up
```

To connect with ssh, use:

```
$ vagrant ssh
```

On Windows,
[PuTTY](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)
is a good way to ssh into these images. If you run
`vagrant ssh-config`, you will see output similar to
the following:

```
Host default
  HostName 127.0.0.1
  User vagrant
  Port 2222
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile C:/Users/[name]/.vagrant.d/insecure_private_key
  IdentitiesOnly yes
  LogLevel FATAL
```

You can use the PuTTYgen tool to convert the IdentityFile
private key from ssh format to the PuTTY .ppk format.
Then create a PuTTY configuration using the settings
and converted .ppk file.

Updating VirtualBox Guest Additions
-----------------------------------

You may find it useful to install the vagrant-vbguest
plugin, which automatically keeps the VirtualBox guest
additions up to date. When these are out of date,
features such as access to the `/vagrant` folder
may not work reliably. The command is:

```
$ vagrant plugin install vagrant-vbguest
```
