__FILENAME__ = bootstrap
#!/usr/bin/env python
## WARNING: This file is generated
#!/usr/bin/env python
"""Create a "virtual" Python installation
"""

__version__ = "1.10.1"
virtualenv_version = __version__  # legacy

import base64
import sys
import os
import codecs
import optparse
import re
import shutil
import logging
import tempfile
import zlib
import errno
import glob
import distutils.sysconfig
from distutils.util import strtobool
import struct
import subprocess
import tarfile

if sys.version_info < (2, 6):
    print('ERROR: %s' % sys.exc_info()[1])
    print('ERROR: this script requires Python 2.6 or greater.')
    sys.exit(101)

try:
    set
except NameError:
    from sets import Set as set
try:
    basestring
except NameError:
    basestring = str

try:
    import ConfigParser
except ImportError:
    import configparser as ConfigParser

join = os.path.join
py_version = 'python%s.%s' % (sys.version_info[0], sys.version_info[1])

is_jython = sys.platform.startswith('java')
is_pypy = hasattr(sys, 'pypy_version_info')
is_win = (sys.platform == 'win32')
is_cygwin = (sys.platform == 'cygwin')
is_darwin = (sys.platform == 'darwin')
abiflags = getattr(sys, 'abiflags', '')

user_dir = os.path.expanduser('~')
if is_win:
    default_storage_dir = os.path.join(user_dir, 'virtualenv')
else:
    default_storage_dir = os.path.join(user_dir, '.virtualenv')
default_config_file = os.path.join(default_storage_dir, 'virtualenv.ini')

if is_pypy:
    expected_exe = 'pypy'
elif is_jython:
    expected_exe = 'jython'
else:
    expected_exe = 'python'

# Return a mapping of version -> Python executable
# Only provided for Windows, where the information in the registry is used
if not is_win:
    def get_installed_pythons():
        return {}
else:
    try:
        import winreg
    except ImportError:
        import _winreg as winreg

    def get_installed_pythons():
        python_core = winreg.CreateKey(winreg.HKEY_LOCAL_MACHINE,
                "Software\\Python\\PythonCore")
        i = 0
        versions = []
        while True:
            try:
                versions.append(winreg.EnumKey(python_core, i))
                i = i + 1
            except WindowsError:
                break
        exes = dict()
        for ver in versions:
            path = winreg.QueryValue(python_core, "%s\\InstallPath" % ver)
            exes[ver] = join(path, "python.exe")

        winreg.CloseKey(python_core)

        # Add the major versions
        # Sort the keys, then repeatedly update the major version entry
        # Last executable (i.e., highest version) wins with this approach
        for ver in sorted(exes):
            exes[ver[0]] = exes[ver]

        return exes

REQUIRED_MODULES = ['os', 'posix', 'posixpath', 'nt', 'ntpath', 'genericpath',
                    'fnmatch', 'locale', 'encodings', 'codecs',
                    'stat', 'UserDict', 'readline', 'copy_reg', 'types',
                    're', 'sre', 'sre_parse', 'sre_constants', 'sre_compile',
                    'zlib']

REQUIRED_FILES = ['lib-dynload', 'config']

majver, minver = sys.version_info[:2]
if majver == 2:
    if minver >= 6:
        REQUIRED_MODULES.extend(['warnings', 'linecache', '_abcoll', 'abc'])
    if minver >= 7:
        REQUIRED_MODULES.extend(['_weakrefset'])
    if minver <= 3:
        REQUIRED_MODULES.extend(['sets', '__future__'])
elif majver == 3:
    # Some extra modules are needed for Python 3, but different ones
    # for different versions.
    REQUIRED_MODULES.extend(['_abcoll', 'warnings', 'linecache', 'abc', 'io',
                             '_weakrefset', 'copyreg', 'tempfile', 'random',
                             '__future__', 'collections', 'keyword', 'tarfile',
                             'shutil', 'struct', 'copy', 'tokenize', 'token',
                             'functools', 'heapq', 'bisect', 'weakref',
                             'reprlib'])
    if minver >= 2:
        REQUIRED_FILES[-1] = 'config-%s' % majver
    if minver == 3:
        import sysconfig
        platdir = sysconfig.get_config_var('PLATDIR')
        REQUIRED_FILES.append(platdir)
        # The whole list of 3.3 modules is reproduced below - the current
        # uncommented ones are required for 3.3 as of now, but more may be
        # added as 3.3 development continues.
        REQUIRED_MODULES.extend([
            #"aifc",
            #"antigravity",
            #"argparse",
            #"ast",
            #"asynchat",
            #"asyncore",
            "base64",
            #"bdb",
            #"binhex",
            #"bisect",
            #"calendar",
            #"cgi",
            #"cgitb",
            #"chunk",
            #"cmd",
            #"codeop",
            #"code",
            #"colorsys",
            #"_compat_pickle",
            #"compileall",
            #"concurrent",
            #"configparser",
            #"contextlib",
            #"cProfile",
            #"crypt",
            #"csv",
            #"ctypes",
            #"curses",
            #"datetime",
            #"dbm",
            #"decimal",
            #"difflib",
            #"dis",
            #"doctest",
            #"dummy_threading",
            "_dummy_thread",
            #"email",
            #"filecmp",
            #"fileinput",
            #"formatter",
            #"fractions",
            #"ftplib",
            #"functools",
            #"getopt",
            #"getpass",
            #"gettext",
            #"glob",
            #"gzip",
            "hashlib",
            #"heapq",
            "hmac",
            #"html",
            #"http",
            #"idlelib",
            #"imaplib",
            #"imghdr",
            "imp",
            "importlib",
            #"inspect",
            #"json",
            #"lib2to3",
            #"logging",
            #"macpath",
            #"macurl2path",
            #"mailbox",
            #"mailcap",
            #"_markupbase",
            #"mimetypes",
            #"modulefinder",
            #"multiprocessing",
            #"netrc",
            #"nntplib",
            #"nturl2path",
            #"numbers",
            #"opcode",
            #"optparse",
            #"os2emxpath",
            #"pdb",
            #"pickle",
            #"pickletools",
            #"pipes",
            #"pkgutil",
            #"platform",
            #"plat-linux2",
            #"plistlib",
            #"poplib",
            #"pprint",
            #"profile",
            #"pstats",
            #"pty",
            #"pyclbr",
            #"py_compile",
            #"pydoc_data",
            #"pydoc",
            #"_pyio",
            #"queue",
            #"quopri",
            #"reprlib",
            "rlcompleter",
            #"runpy",
            #"sched",
            #"shelve",
            #"shlex",
            #"smtpd",
            #"smtplib",
            #"sndhdr",
            #"socket",
            #"socketserver",
            #"sqlite3",
            #"ssl",
            #"stringprep",
            #"string",
            #"_strptime",
            #"subprocess",
            #"sunau",
            #"symbol",
            #"symtable",
            #"sysconfig",
            #"tabnanny",
            #"telnetlib",
            #"test",
            #"textwrap",
            #"this",
            #"_threading_local",
            #"threading",
            #"timeit",
            #"tkinter",
            #"tokenize",
            #"token",
            #"traceback",
            #"trace",
            #"tty",
            #"turtledemo",
            #"turtle",
            #"unittest",
            #"urllib",
            #"uuid",
            #"uu",
            #"wave",
            #"weakref",
            #"webbrowser",
            #"wsgiref",
            #"xdrlib",
            #"xml",
            #"xmlrpc",
            #"zipfile",
        ])

if is_pypy:
    # these are needed to correctly display the exceptions that may happen
    # during the bootstrap
    REQUIRED_MODULES.extend(['traceback', 'linecache'])

class Logger(object):

    """
    Logging object for use in command-line script.  Allows ranges of
    levels, to avoid some redundancy of displayed information.
    """

    DEBUG = logging.DEBUG
    INFO = logging.INFO
    NOTIFY = (logging.INFO+logging.WARN)/2
    WARN = WARNING = logging.WARN
    ERROR = logging.ERROR
    FATAL = logging.FATAL

    LEVELS = [DEBUG, INFO, NOTIFY, WARN, ERROR, FATAL]

    def __init__(self, consumers):
        self.consumers = consumers
        self.indent = 0
        self.in_progress = None
        self.in_progress_hanging = False

    def debug(self, msg, *args, **kw):
        self.log(self.DEBUG, msg, *args, **kw)
    def info(self, msg, *args, **kw):
        self.log(self.INFO, msg, *args, **kw)
    def notify(self, msg, *args, **kw):
        self.log(self.NOTIFY, msg, *args, **kw)
    def warn(self, msg, *args, **kw):
        self.log(self.WARN, msg, *args, **kw)
    def error(self, msg, *args, **kw):
        self.log(self.ERROR, msg, *args, **kw)
    def fatal(self, msg, *args, **kw):
        self.log(self.FATAL, msg, *args, **kw)
    def log(self, level, msg, *args, **kw):
        if args:
            if kw:
                raise TypeError(
                    "You may give positional or keyword arguments, not both")
        args = args or kw
        rendered = None
        for consumer_level, consumer in self.consumers:
            if self.level_matches(level, consumer_level):
                if (self.in_progress_hanging
                    and consumer in (sys.stdout, sys.stderr)):
                    self.in_progress_hanging = False
                    sys.stdout.write('\n')
                    sys.stdout.flush()
                if rendered is None:
                    if args:
                        rendered = msg % args
                    else:
                        rendered = msg
                    rendered = ' '*self.indent + rendered
                if hasattr(consumer, 'write'):
                    consumer.write(rendered+'\n')
                else:
                    consumer(rendered)

    def start_progress(self, msg):
        assert not self.in_progress, (
            "Tried to start_progress(%r) while in_progress %r"
            % (msg, self.in_progress))
        if self.level_matches(self.NOTIFY, self._stdout_level()):
            sys.stdout.write(msg)
            sys.stdout.flush()
            self.in_progress_hanging = True
        else:
            self.in_progress_hanging = False
        self.in_progress = msg

    def end_progress(self, msg='done.'):
        assert self.in_progress, (
            "Tried to end_progress without start_progress")
        if self.stdout_level_matches(self.NOTIFY):
            if not self.in_progress_hanging:
                # Some message has been printed out since start_progress
                sys.stdout.write('...' + self.in_progress + msg + '\n')
                sys.stdout.flush()
            else:
                sys.stdout.write(msg + '\n')
                sys.stdout.flush()
        self.in_progress = None
        self.in_progress_hanging = False

    def show_progress(self):
        """If we are in a progress scope, and no log messages have been
        shown, write out another '.'"""
        if self.in_progress_hanging:
            sys.stdout.write('.')
            sys.stdout.flush()

    def stdout_level_matches(self, level):
        """Returns true if a message at this level will go to stdout"""
        return self.level_matches(level, self._stdout_level())

    def _stdout_level(self):
        """Returns the level that stdout runs at"""
        for level, consumer in self.consumers:
            if consumer is sys.stdout:
                return level
        return self.FATAL

    def level_matches(self, level, consumer_level):
        """
        >>> l = Logger([])
        >>> l.level_matches(3, 4)
        False
        >>> l.level_matches(3, 2)
        True
        >>> l.level_matches(slice(None, 3), 3)
        False
        >>> l.level_matches(slice(None, 3), 2)
        True
        >>> l.level_matches(slice(1, 3), 1)
        True
        >>> l.level_matches(slice(2, 3), 1)
        False
        """
        if isinstance(level, slice):
            start, stop = level.start, level.stop
            if start is not None and start > consumer_level:
                return False
            if stop is not None and stop <= consumer_level:
                return False
            return True
        else:
            return level >= consumer_level

    #@classmethod
    def level_for_integer(cls, level):
        levels = cls.LEVELS
        if level < 0:
            return levels[0]
        if level >= len(levels):
            return levels[-1]
        return levels[level]

    level_for_integer = classmethod(level_for_integer)

# create a silent logger just to prevent this from being undefined
# will be overridden with requested verbosity main() is called.
logger = Logger([(Logger.LEVELS[-1], sys.stdout)])

def mkdir(path):
    if not os.path.exists(path):
        logger.info('Creating %s', path)
        os.makedirs(path)
    else:
        logger.info('Directory %s already exists', path)

def copyfileordir(src, dest, symlink=True):
    if os.path.isdir(src):
        shutil.copytree(src, dest, symlink)
    else:
        shutil.copy2(src, dest)

def copyfile(src, dest, symlink=True):
    if not os.path.exists(src):
        # Some bad symlink in the src
        logger.warn('Cannot find file %s (bad symlink)', src)
        return
    if os.path.exists(dest):
        logger.debug('File %s already exists', dest)
        return
    if not os.path.exists(os.path.dirname(dest)):
        logger.info('Creating parent directories for %s', os.path.dirname(dest))
        os.makedirs(os.path.dirname(dest))
    if not os.path.islink(src):
        srcpath = os.path.abspath(src)
    else:
        srcpath = os.readlink(src)
    if symlink and hasattr(os, 'symlink') and not is_win:
        logger.info('Symlinking %s', dest)
        try:
            os.symlink(srcpath, dest)
        except (OSError, NotImplementedError):
            logger.info('Symlinking failed, copying to %s', dest)
            copyfileordir(src, dest, symlink)
    else:
        logger.info('Copying to %s', dest)
        copyfileordir(src, dest, symlink)

def writefile(dest, content, overwrite=True):
    if not os.path.exists(dest):
        logger.info('Writing %s', dest)
        f = open(dest, 'wb')
        f.write(content.encode('utf-8'))
        f.close()
        return
    else:
        f = open(dest, 'rb')
        c = f.read()
        f.close()
        if c != content.encode("utf-8"):
            if not overwrite:
                logger.notify('File %s exists with different content; not overwriting', dest)
                return
            logger.notify('Overwriting %s with new content', dest)
            f = open(dest, 'wb')
            f.write(content.encode('utf-8'))
            f.close()
        else:
            logger.info('Content %s already in place', dest)

def rmtree(dir):
    if os.path.exists(dir):
        logger.notify('Deleting tree %s', dir)
        shutil.rmtree(dir)
    else:
        logger.info('Do not need to delete %s; already gone', dir)

def make_exe(fn):
    if hasattr(os, 'chmod'):
        oldmode = os.stat(fn).st_mode & 0xFFF # 0o7777
        newmode = (oldmode | 0x16D) & 0xFFF # 0o555, 0o7777
        os.chmod(fn, newmode)
        logger.info('Changed mode of %s to %s', fn, oct(newmode))

def _find_file(filename, dirs):
    for dir in reversed(dirs):
        files = glob.glob(os.path.join(dir, filename))
        if files and os.path.isfile(files[0]):
            return True, files[0]
    return False, filename

def file_search_dirs():
    here = os.path.dirname(os.path.abspath(__file__))
    dirs = ['.', here,
            join(here, 'virtualenv_support')]
    if os.path.splitext(os.path.dirname(__file__))[0] != 'virtualenv':
        # Probably some boot script; just in case virtualenv is installed...
        try:
            import virtualenv
        except ImportError:
            pass
        else:
            dirs.append(os.path.join(os.path.dirname(virtualenv.__file__), 'virtualenv_support'))
    return [d for d in dirs if os.path.isdir(d)]


class UpdatingDefaultsHelpFormatter(optparse.IndentedHelpFormatter):
    """
    Custom help formatter for use in ConfigOptionParser that updates
    the defaults before expanding them, allowing them to show up correctly
    in the help listing
    """
    def expand_default(self, option):
        if self.parser is not None:
            self.parser.update_defaults(self.parser.defaults)
        return optparse.IndentedHelpFormatter.expand_default(self, option)


class ConfigOptionParser(optparse.OptionParser):
    """
    Custom option parser which updates its defaults by checking the
    configuration files and environmental variables
    """
    def __init__(self, *args, **kwargs):
        self.config = ConfigParser.RawConfigParser()
        self.files = self.get_config_files()
        self.config.read(self.files)
        optparse.OptionParser.__init__(self, *args, **kwargs)

    def get_config_files(self):
        config_file = os.environ.get('VIRTUALENV_CONFIG_FILE', False)
        if config_file and os.path.exists(config_file):
            return [config_file]
        return [default_config_file]

    def update_defaults(self, defaults):
        """
        Updates the given defaults with values from the config files and
        the environ. Does a little special handling for certain types of
        options (lists).
        """
        # Then go and look for the other sources of configuration:
        config = {}
        # 1. config files
        config.update(dict(self.get_config_section('virtualenv')))
        # 2. environmental variables
        config.update(dict(self.get_environ_vars()))
        # Then set the options with those values
        for key, val in config.items():
            key = key.replace('_', '-')
            if not key.startswith('--'):
                key = '--%s' % key  # only prefer long opts
            option = self.get_option(key)
            if option is not None:
                # ignore empty values
                if not val:
                    continue
                # handle multiline configs
                if option.action == 'append':
                    val = val.split()
                else:
                    option.nargs = 1
                if option.action == 'store_false':
                    val = not strtobool(val)
                elif option.action in ('store_true', 'count'):
                    val = strtobool(val)
                try:
                    val = option.convert_value(key, val)
                except optparse.OptionValueError:
                    e = sys.exc_info()[1]
                    print("An error occured during configuration: %s" % e)
                    sys.exit(3)
                defaults[option.dest] = val
        return defaults

    def get_config_section(self, name):
        """
        Get a section of a configuration
        """
        if self.config.has_section(name):
            return self.config.items(name)
        return []

    def get_environ_vars(self, prefix='VIRTUALENV_'):
        """
        Returns a generator with all environmental vars with prefix VIRTUALENV
        """
        for key, val in os.environ.items():
            if key.startswith(prefix):
                yield (key.replace(prefix, '').lower(), val)

    def get_default_values(self):
        """
        Overridding to make updating the defaults after instantiation of
        the option parser possible, update_defaults() does the dirty work.
        """
        if not self.process_default_values:
            # Old, pre-Optik 1.5 behaviour.
            return optparse.Values(self.defaults)

        defaults = self.update_defaults(self.defaults.copy())  # ours
        for option in self._get_all_options():
            default = defaults.get(option.dest)
            if isinstance(default, basestring):
                opt_str = option.get_opt_string()
                defaults[option.dest] = option.check_value(opt_str, default)
        return optparse.Values(defaults)


def main():
    parser = ConfigOptionParser(
        version=virtualenv_version,
        usage="%prog [OPTIONS] DEST_DIR",
        formatter=UpdatingDefaultsHelpFormatter())

    parser.add_option(
        '-v', '--verbose',
        action='count',
        dest='verbose',
        default=0,
        help="Increase verbosity")

    parser.add_option(
        '-q', '--quiet',
        action='count',
        dest='quiet',
        default=0,
        help='Decrease verbosity')

    parser.add_option(
        '-p', '--python',
        dest='python',
        metavar='PYTHON_EXE',
        help='The Python interpreter to use, e.g., --python=python2.5 will use the python2.5 '
        'interpreter to create the new environment.  The default is the interpreter that '
        'virtualenv was installed with (%s)' % sys.executable)

    parser.add_option(
        '--clear',
        dest='clear',
        action='store_true',
        help="Clear out the non-root install and start from scratch")

    parser.set_defaults(system_site_packages=False)
    parser.add_option(
        '--no-site-packages',
        dest='system_site_packages',
        action='store_false',
        help="Don't give access to the global site-packages dir to the "
             "virtual environment (default)")

    parser.add_option(
        '--system-site-packages',
        dest='system_site_packages',
        action='store_true',
        help="Give access to the global site-packages dir to the "
             "virtual environment")

    parser.add_option(
        '--always-copy',
        dest='symlink',
        action='store_false',
        default=True,
        help="Always copy files rather than symlinking")

    parser.add_option(
        '--unzip-setuptools',
        dest='unzip_setuptools',
        action='store_true',
        help="Unzip Setuptools when installing it")

    parser.add_option(
        '--relocatable',
        dest='relocatable',
        action='store_true',
        help='Make an EXISTING virtualenv environment relocatable.  '
        'This fixes up scripts and makes all .pth files relative')

    parser.add_option(
        '--no-setuptools',
        dest='no_setuptools',
        action='store_true',
        help='Do not install setuptools (or pip) '
        'in the new virtualenv.')

    parser.add_option(
        '--no-pip',
        dest='no_pip',
        action='store_true',
        help='Do not install pip in the new virtualenv.')

    default_search_dirs = file_search_dirs()
    parser.add_option(
        '--extra-search-dir',
        dest="search_dirs",
        action="append",
        default=default_search_dirs,
        help="Directory to look for setuptools/pip distributions in. "
        "You can add any number of additional --extra-search-dir paths.")

    parser.add_option(
        '--never-download',
        dest="never_download",
        action="store_true",
        default=True,
        help="Never download anything from the network. This is now always "
        "the case. The option is only retained for backward compatibility, "
        "and does nothing. Virtualenv will fail if local distributions "
        "of setuptools/pip are not present.")

    parser.add_option(
        '--prompt',
        dest='prompt',
        help='Provides an alternative prompt prefix for this environment')

    parser.add_option(
        '--setuptools',
        dest='setuptools',
        action='store_true',
        help="Backward compatibility. Does nothing.")

    parser.add_option(
        '--distribute',
        dest='distribute',
        action='store_true',
        help="Backward compatibility. Does nothing.")

    if 'extend_parser' in globals():
        extend_parser(parser)

    options, args = parser.parse_args()

    global logger

    if 'adjust_options' in globals():
        adjust_options(options, args)

    verbosity = options.verbose - options.quiet
    logger = Logger([(Logger.level_for_integer(2 - verbosity), sys.stdout)])

    if options.python and not os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):
        env = os.environ.copy()
        interpreter = resolve_interpreter(options.python)
        if interpreter == sys.executable:
            logger.warn('Already using interpreter %s' % interpreter)
        else:
            logger.notify('Running virtualenv with interpreter %s' % interpreter)
            env['VIRTUALENV_INTERPRETER_RUNNING'] = 'true'
            file = __file__
            if file.endswith('.pyc'):
                file = file[:-1]
            popen = subprocess.Popen([interpreter, file] + sys.argv[1:], env=env)
            raise SystemExit(popen.wait())

    if not args:
        print('You must provide a DEST_DIR')
        parser.print_help()
        sys.exit(2)
    if len(args) > 1:
        print('There must be only one argument: DEST_DIR (you gave %s)' % (
            ' '.join(args)))
        parser.print_help()
        sys.exit(2)

    home_dir = args[0]

    if os.environ.get('WORKING_ENV'):
        logger.fatal('ERROR: you cannot run virtualenv while in a workingenv')
        logger.fatal('Please deactivate your workingenv, then re-run this script')
        sys.exit(3)

    if 'PYTHONHOME' in os.environ:
        logger.warn('PYTHONHOME is set.  You *must* activate the virtualenv before using it')
        del os.environ['PYTHONHOME']

    if options.relocatable:
        make_environment_relocatable(home_dir)
        return

    if not options.never_download:
        logger.warn('The --never-download option is for backward compatibility only.')
        logger.warn('Setting it to false is no longer supported, and will be ignored.')

    create_environment(home_dir,
                       site_packages=options.system_site_packages,
                       clear=options.clear,
                       unzip_setuptools=options.unzip_setuptools,
                       prompt=options.prompt,
                       search_dirs=options.search_dirs,
                       never_download=True,
                       no_setuptools=options.no_setuptools,
                       no_pip=options.no_pip,
                       symlink=options.symlink)
    if 'after_install' in globals():
        after_install(options, home_dir)

def call_subprocess(cmd, show_stdout=True,
                    filter_stdout=None, cwd=None,
                    raise_on_returncode=True, extra_env=None,
                    remove_from_env=None):
    cmd_parts = []
    for part in cmd:
        if len(part) > 45:
            part = part[:20]+"..."+part[-20:]
        if ' ' in part or '\n' in part or '"' in part or "'" in part:
            part = '"%s"' % part.replace('"', '\\"')
        if hasattr(part, 'decode'):
            try:
                part = part.decode(sys.getdefaultencoding())
            except UnicodeDecodeError:
                part = part.decode(sys.getfilesystemencoding())
        cmd_parts.append(part)
    cmd_desc = ' '.join(cmd_parts)
    if show_stdout:
        stdout = None
    else:
        stdout = subprocess.PIPE
    logger.debug("Running command %s" % cmd_desc)
    if extra_env or remove_from_env:
        env = os.environ.copy()
        if extra_env:
            env.update(extra_env)
        if remove_from_env:
            for varname in remove_from_env:
                env.pop(varname, None)
    else:
        env = None
    try:
        proc = subprocess.Popen(
            cmd, stderr=subprocess.STDOUT, stdin=None, stdout=stdout,
            cwd=cwd, env=env)
    except Exception:
        e = sys.exc_info()[1]
        logger.fatal(
            "Error %s while executing command %s" % (e, cmd_desc))
        raise
    all_output = []
    if stdout is not None:
        stdout = proc.stdout
        encoding = sys.getdefaultencoding()
        fs_encoding = sys.getfilesystemencoding()
        while 1:
            line = stdout.readline()
            try:
                line = line.decode(encoding)
            except UnicodeDecodeError:
                line = line.decode(fs_encoding)
            if not line:
                break
            line = line.rstrip()
            all_output.append(line)
            if filter_stdout:
                level = filter_stdout(line)
                if isinstance(level, tuple):
                    level, line = level
                logger.log(level, line)
                if not logger.stdout_level_matches(level):
                    logger.show_progress()
            else:
                logger.info(line)
    else:
        proc.communicate()
    proc.wait()
    if proc.returncode:
        if raise_on_returncode:
            if all_output:
                logger.notify('Complete output from command %s:' % cmd_desc)
                logger.notify('\n'.join(all_output) + '\n----------------------------------------')
            raise OSError(
                "Command %s failed with error code %s"
                % (cmd_desc, proc.returncode))
        else:
            logger.warn(
                "Command %s had error code %s"
                % (cmd_desc, proc.returncode))

def filter_install_output(line):
    if line.strip().startswith('running'):
        return Logger.INFO
    return Logger.DEBUG

def install_sdist(project_name, sdist, py_executable, search_dirs=None):

    if search_dirs is None:
        search_dirs = file_search_dirs()
    found, sdist_path = _find_file(sdist, search_dirs)
    if not found:
        # Manual edit of bootstrap.py to make this error clearer.
        logger.fatal("Cannot find sdist %s" % (sdist,))
        logger.fatal("Installing virtualenv in the system python, or downloading setuptools/pip packages to the "
                     "bootstrap.py folder will fix this problem.")
        logger.fatal("")
        logger.fatal("https://pypi.python.org/packages/source/p/pip/pip-1.4.1.tar.gz")
        logger.fatal("https://pypi.python.org/packages/source/s/setuptools/setuptools-1.1.7.tar.gz")
        sys.exit(100)

    tmpdir = tempfile.mkdtemp()
    try:
        tar = tarfile.open(sdist_path)
        tar.extractall(tmpdir)
        tar.close()
        srcdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])
        cmd = [py_executable, 'setup.py', 'install',
            '--single-version-externally-managed',
            '--record', 'record']
        logger.start_progress('Installing %s...' % project_name)
        logger.indent += 2
        try:
            call_subprocess(cmd, show_stdout=False, cwd=srcdir,
                    filter_stdout=filter_install_output)
        finally:
            logger.indent -= 2
            logger.end_progress()
    finally:
        shutil.rmtree(tmpdir)

def create_environment(home_dir, site_packages=False, clear=False,
                       unzip_setuptools=False,
                       prompt=None, search_dirs=None, never_download=False,
                       no_setuptools=False, no_pip=False, symlink=True):
    """
    Creates a new environment in ``home_dir``.

    If ``site_packages`` is true, then the global ``site-packages/``
    directory will be on the path.

    If ``clear`` is true (default False) then the environment will
    first be cleared.
    """
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)

    py_executable = os.path.abspath(install_python(
        home_dir, lib_dir, inc_dir, bin_dir,
        site_packages=site_packages, clear=clear, symlink=symlink))

    install_distutils(home_dir)

    if not no_setuptools:
        install_sdist('Setuptools', 'setuptools-*.tar.gz', py_executable, search_dirs)
        if not no_pip:
            install_sdist('Pip', 'pip-*.tar.gz', py_executable, search_dirs)

    install_activate(home_dir, bin_dir, prompt)

def is_executable_file(fpath):
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

def path_locations(home_dir):
    """Return the path locations for the environment (where libraries are,
    where scripts go, etc)"""
    # XXX: We'd use distutils.sysconfig.get_python_inc/lib but its
    # prefix arg is broken: http://bugs.python.org/issue3386
    if is_win:
        # Windows has lots of problems with executables with spaces in
        # the name; this function will remove them (using the ~1
        # format):
        mkdir(home_dir)
        if ' ' in home_dir:
            import ctypes
            GetShortPathName = ctypes.windll.kernel32.GetShortPathNameW
            size = max(len(home_dir)+1, 256)
            buf = ctypes.create_unicode_buffer(size)
            try:
                u = unicode
            except NameError:
                u = str
            ret = GetShortPathName(u(home_dir), buf, size)
            if not ret:
                print('Error: the path "%s" has a space in it' % home_dir)
                print('We could not determine the short pathname for it.')
                print('Exiting.')
                sys.exit(3)
            home_dir = str(buf.value)
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'Scripts')
    if is_jython:
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'bin')
    elif is_pypy:
        lib_dir = home_dir
        inc_dir = join(home_dir, 'include')
        bin_dir = join(home_dir, 'bin')
    elif not is_win:
        lib_dir = join(home_dir, 'lib', py_version)
        multiarch_exec = '/usr/bin/multiarch-platform'
        if is_executable_file(multiarch_exec):
            # In Mageia (2) and Mandriva distros the include dir must be like:
            # virtualenv/include/multiarch-x86_64-linux/python2.7
            # instead of being virtualenv/include/python2.7
            p = subprocess.Popen(multiarch_exec, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdout, stderr = p.communicate()
            # stdout.strip is needed to remove newline character
            inc_dir = join(home_dir, 'include', stdout.strip(), py_version + abiflags)
        else:
            inc_dir = join(home_dir, 'include', py_version + abiflags)
        bin_dir = join(home_dir, 'bin')
    return home_dir, lib_dir, inc_dir, bin_dir


def change_prefix(filename, dst_prefix):
    prefixes = [sys.prefix]

    if is_darwin:
        prefixes.extend((
            os.path.join("/Library/Python", sys.version[:3], "site-packages"),
            os.path.join(sys.prefix, "Extras", "lib", "python"),
            os.path.join("~", "Library", "Python", sys.version[:3], "site-packages"),
            # Python 2.6 no-frameworks
            os.path.join("~", ".local", "lib","python", sys.version[:3], "site-packages"),
            # System Python 2.7 on OSX Mountain Lion
            os.path.join("~", "Library", "Python", sys.version[:3], "lib", "python", "site-packages")))

    if hasattr(sys, 'real_prefix'):
        prefixes.append(sys.real_prefix)
    if hasattr(sys, 'base_prefix'):
        prefixes.append(sys.base_prefix)
    prefixes = list(map(os.path.expanduser, prefixes))
    prefixes = list(map(os.path.abspath, prefixes))
    # Check longer prefixes first so we don't split in the middle of a filename
    prefixes = sorted(prefixes, key=len, reverse=True)
    filename = os.path.abspath(filename)
    for src_prefix in prefixes:
        if filename.startswith(src_prefix):
            _, relpath = filename.split(src_prefix, 1)
            if src_prefix != os.sep: # sys.prefix == "/"
                assert relpath[0] == os.sep
                relpath = relpath[1:]
            return join(dst_prefix, relpath)
    assert False, "Filename %s does not start with any of these prefixes: %s" % \
        (filename, prefixes)

def copy_required_modules(dst_prefix, symlink):
    import imp
    # If we are running under -p, we need to remove the current
    # directory from sys.path temporarily here, so that we
    # definitely get the modules from the site directory of
    # the interpreter we are running under, not the one
    # virtualenv.py is installed under (which might lead to py2/py3
    # incompatibility issues)
    _prev_sys_path = sys.path
    if os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):
        sys.path = sys.path[1:]
    try:
        for modname in REQUIRED_MODULES:
            if modname in sys.builtin_module_names:
                logger.info("Ignoring built-in bootstrap module: %s" % modname)
                continue
            try:
                f, filename, _ = imp.find_module(modname)
            except ImportError:
                logger.info("Cannot import bootstrap module: %s" % modname)
            else:
                if f is not None:
                    f.close()
                # special-case custom readline.so on OS X, but not for pypy:
                if modname == 'readline' and sys.platform == 'darwin' and not (
                        is_pypy or filename.endswith(join('lib-dynload', 'readline.so'))):
                    dst_filename = join(dst_prefix, 'lib', 'python%s' % sys.version[:3], 'readline.so')
                elif modname == 'readline' and sys.platform == 'win32':
                    # special-case for Windows, where readline is not a
                    # standard module, though it may have been installed in
                    # site-packages by a third-party package
                    pass
                else:
                    dst_filename = change_prefix(filename, dst_prefix)
                copyfile(filename, dst_filename, symlink)
                if filename.endswith('.pyc'):
                    pyfile = filename[:-1]
                    if os.path.exists(pyfile):
                        copyfile(pyfile, dst_filename[:-1], symlink)
    finally:
        sys.path = _prev_sys_path


def subst_path(prefix_path, prefix, home_dir):
    prefix_path = os.path.normpath(prefix_path)
    prefix = os.path.normpath(prefix)
    home_dir = os.path.normpath(home_dir)
    if not prefix_path.startswith(prefix):
        logger.warn('Path not in prefix %r %r', prefix_path, prefix)
        return
    return prefix_path.replace(prefix, home_dir, 1)


def install_python(home_dir, lib_dir, inc_dir, bin_dir, site_packages, clear, symlink=True):
    """Install just the base environment, no distutils patches etc"""
    if sys.executable.startswith(bin_dir):
        print('Please use the *system* python to run this script')
        return

    if clear:
        rmtree(lib_dir)
        ## FIXME: why not delete it?
        ## Maybe it should delete everything with #!/path/to/venv/python in it
        logger.notify('Not deleting %s', bin_dir)

    if hasattr(sys, 'real_prefix'):
        logger.notify('Using real prefix %r' % sys.real_prefix)
        prefix = sys.real_prefix
    elif hasattr(sys, 'base_prefix'):
        logger.notify('Using base prefix %r' % sys.base_prefix)
        prefix = sys.base_prefix
    else:
        prefix = sys.prefix
    mkdir(lib_dir)
    fix_lib64(lib_dir, symlink)
    stdlib_dirs = [os.path.dirname(os.__file__)]
    if is_win:
        stdlib_dirs.append(join(os.path.dirname(stdlib_dirs[0]), 'DLLs'))
    elif is_darwin:
        stdlib_dirs.append(join(stdlib_dirs[0], 'site-packages'))
    if hasattr(os, 'symlink'):
        logger.info('Symlinking Python bootstrap modules')
    else:
        logger.info('Copying Python bootstrap modules')
    logger.indent += 2
    try:
        # copy required files...
        for stdlib_dir in stdlib_dirs:
            if not os.path.isdir(stdlib_dir):
                continue
            for fn in os.listdir(stdlib_dir):
                bn = os.path.splitext(fn)[0]
                if fn != 'site-packages' and bn in REQUIRED_FILES:
                    copyfile(join(stdlib_dir, fn), join(lib_dir, fn), symlink)
        # ...and modules
        copy_required_modules(home_dir, symlink)
    finally:
        logger.indent -= 2
    mkdir(join(lib_dir, 'site-packages'))
    import site
    site_filename = site.__file__
    if site_filename.endswith('.pyc'):
        site_filename = site_filename[:-1]
    elif site_filename.endswith('$py.class'):
        site_filename = site_filename.replace('$py.class', '.py')
    site_filename_dst = change_prefix(site_filename, home_dir)
    site_dir = os.path.dirname(site_filename_dst)
    writefile(site_filename_dst, SITE_PY)
    writefile(join(site_dir, 'orig-prefix.txt'), prefix)
    site_packages_filename = join(site_dir, 'no-global-site-packages.txt')
    if not site_packages:
        writefile(site_packages_filename, '')

    if is_pypy or is_win:
        stdinc_dir = join(prefix, 'include')
    else:
        stdinc_dir = join(prefix, 'include', py_version + abiflags)
    if os.path.exists(stdinc_dir):
        copyfile(stdinc_dir, inc_dir, symlink)
    else:
        logger.debug('No include dir %s' % stdinc_dir)

    platinc_dir = distutils.sysconfig.get_python_inc(plat_specific=1)
    if platinc_dir != stdinc_dir:
        platinc_dest = distutils.sysconfig.get_python_inc(
            plat_specific=1, prefix=home_dir)
        if platinc_dir == platinc_dest:
            # Do platinc_dest manually due to a CPython bug;
            # not http://bugs.python.org/issue3386 but a close cousin
            platinc_dest = subst_path(platinc_dir, prefix, home_dir)
        if platinc_dest:
            # PyPy's stdinc_dir and prefix are relative to the original binary
            # (traversing virtualenvs), whereas the platinc_dir is relative to
            # the inner virtualenv and ignores the prefix argument.
            # This seems more evolved than designed.
            copyfile(platinc_dir, platinc_dest, symlink)

    # pypy never uses exec_prefix, just ignore it
    if sys.exec_prefix != prefix and not is_pypy:
        if is_win:
            exec_dir = join(sys.exec_prefix, 'lib')
        elif is_jython:
            exec_dir = join(sys.exec_prefix, 'Lib')
        else:
            exec_dir = join(sys.exec_prefix, 'lib', py_version)
        for fn in os.listdir(exec_dir):
            copyfile(join(exec_dir, fn), join(lib_dir, fn), symlink)

    if is_jython:
        # Jython has either jython-dev.jar and javalib/ dir, or just
        # jython.jar
        for name in 'jython-dev.jar', 'javalib', 'jython.jar':
            src = join(prefix, name)
            if os.path.exists(src):
                copyfile(src, join(home_dir, name), symlink)
        # XXX: registry should always exist after Jython 2.5rc1
        src = join(prefix, 'registry')
        if os.path.exists(src):
            copyfile(src, join(home_dir, 'registry'), symlink=False)
        copyfile(join(prefix, 'cachedir'), join(home_dir, 'cachedir'),
                 symlink=False)

    mkdir(bin_dir)
    py_executable = join(bin_dir, os.path.basename(sys.executable))
    if 'Python.framework' in prefix:
        # OS X framework builds cause validation to break
        # https://github.com/pypa/virtualenv/issues/322
        if os.environ.get('__PYVENV_LAUNCHER__'):
          os.unsetenv('__PYVENV_LAUNCHER__')
        if re.search(r'/Python(?:-32|-64)*$', py_executable):
            # The name of the python executable is not quite what
            # we want, rename it.
            py_executable = os.path.join(
                    os.path.dirname(py_executable), 'python')

    logger.notify('New %s executable in %s', expected_exe, py_executable)
    pcbuild_dir = os.path.dirname(sys.executable)
    pyd_pth = os.path.join(lib_dir, 'site-packages', 'virtualenv_builddir_pyd.pth')
    if is_win and os.path.exists(os.path.join(pcbuild_dir, 'build.bat')):
        logger.notify('Detected python running from build directory %s', pcbuild_dir)
        logger.notify('Writing .pth file linking to build directory for *.pyd files')
        writefile(pyd_pth, pcbuild_dir)
    else:
        pcbuild_dir = None
        if os.path.exists(pyd_pth):
            logger.info('Deleting %s (not Windows env or not build directory python)' % pyd_pth)
            os.unlink(pyd_pth)

    if sys.executable != py_executable:
        ## FIXME: could I just hard link?
        executable = sys.executable
        shutil.copyfile(executable, py_executable)
        make_exe(py_executable)
        if is_win or is_cygwin:
            pythonw = os.path.join(os.path.dirname(sys.executable), 'pythonw.exe')
            if os.path.exists(pythonw):
                logger.info('Also created pythonw.exe')
                shutil.copyfile(pythonw, os.path.join(os.path.dirname(py_executable), 'pythonw.exe'))
            python_d = os.path.join(os.path.dirname(sys.executable), 'python_d.exe')
            python_d_dest = os.path.join(os.path.dirname(py_executable), 'python_d.exe')
            if os.path.exists(python_d):
                logger.info('Also created python_d.exe')
                shutil.copyfile(python_d, python_d_dest)
            elif os.path.exists(python_d_dest):
                logger.info('Removed python_d.exe as it is no longer at the source')
                os.unlink(python_d_dest)
            # we need to copy the DLL to enforce that windows will load the correct one.
            # may not exist if we are cygwin.
            py_executable_dll = 'python%s%s.dll' % (
                sys.version_info[0], sys.version_info[1])
            py_executable_dll_d = 'python%s%s_d.dll' % (
                sys.version_info[0], sys.version_info[1])
            pythondll = os.path.join(os.path.dirname(sys.executable), py_executable_dll)
            pythondll_d = os.path.join(os.path.dirname(sys.executable), py_executable_dll_d)
            pythondll_d_dest = os.path.join(os.path.dirname(py_executable), py_executable_dll_d)
            if os.path.exists(pythondll):
                logger.info('Also created %s' % py_executable_dll)
                shutil.copyfile(pythondll, os.path.join(os.path.dirname(py_executable), py_executable_dll))
            if os.path.exists(pythondll_d):
                logger.info('Also created %s' % py_executable_dll_d)
                shutil.copyfile(pythondll_d, pythondll_d_dest)
            elif os.path.exists(pythondll_d_dest):
                logger.info('Removed %s as the source does not exist' % pythondll_d_dest)
                os.unlink(pythondll_d_dest)
        if is_pypy:
            # make a symlink python --> pypy-c
            python_executable = os.path.join(os.path.dirname(py_executable), 'python')
            if sys.platform in ('win32', 'cygwin'):
                python_executable += '.exe'
            logger.info('Also created executable %s' % python_executable)
            copyfile(py_executable, python_executable, symlink)

            if is_win:
                for name in 'libexpat.dll', 'libpypy.dll', 'libpypy-c.dll', 'libeay32.dll', 'ssleay32.dll', 'sqlite.dll':
                    src = join(prefix, name)
                    if os.path.exists(src):
                        copyfile(src, join(bin_dir, name), symlink)

    if os.path.splitext(os.path.basename(py_executable))[0] != expected_exe:
        secondary_exe = os.path.join(os.path.dirname(py_executable),
                                     expected_exe)
        py_executable_ext = os.path.splitext(py_executable)[1]
        if py_executable_ext.lower() == '.exe':
            # python2.4 gives an extension of '.4' :P
            secondary_exe += py_executable_ext
        if os.path.exists(secondary_exe):
            logger.warn('Not overwriting existing %s script %s (you must use %s)'
                        % (expected_exe, secondary_exe, py_executable))
        else:
            logger.notify('Also creating executable in %s' % secondary_exe)
            shutil.copyfile(sys.executable, secondary_exe)
            make_exe(secondary_exe)

    if '.framework' in prefix:
        if 'Python.framework' in prefix:
            logger.debug('MacOSX Python framework detected')
            # Make sure we use the the embedded interpreter inside
            # the framework, even if sys.executable points to
            # the stub executable in ${sys.prefix}/bin
            # See http://groups.google.com/group/python-virtualenv/
            #                              browse_thread/thread/17cab2f85da75951
            original_python = os.path.join(
                prefix, 'Resources/Python.app/Contents/MacOS/Python')
        if 'EPD' in prefix:
            logger.debug('EPD framework detected')
            original_python = os.path.join(prefix, 'bin/python')
        shutil.copy(original_python, py_executable)

        # Copy the framework's dylib into the virtual
        # environment
        virtual_lib = os.path.join(home_dir, '.Python')

        if os.path.exists(virtual_lib):
            os.unlink(virtual_lib)
        copyfile(
            os.path.join(prefix, 'Python'),
            virtual_lib,
            symlink)

        # And then change the install_name of the copied python executable
        try:
            mach_o_change(py_executable,
                          os.path.join(prefix, 'Python'),
                          '@executable_path/../.Python')
        except:
            e = sys.exc_info()[1]
            logger.warn("Could not call mach_o_change: %s. "
                        "Trying to call install_name_tool instead." % e)
            try:
                call_subprocess(
                    ["install_name_tool", "-change",
                     os.path.join(prefix, 'Python'),
                     '@executable_path/../.Python',
                     py_executable])
            except:
                logger.fatal("Could not call install_name_tool -- you must "
                             "have Apple's development tools installed")
                raise

    if not is_win:
        # Ensure that 'python', 'pythonX' and 'pythonX.Y' all exist
        py_exe_version_major = 'python%s' % sys.version_info[0]
        py_exe_version_major_minor = 'python%s.%s' % (
            sys.version_info[0], sys.version_info[1])
        py_exe_no_version = 'python'
        required_symlinks = [ py_exe_no_version, py_exe_version_major,
                         py_exe_version_major_minor ]

        py_executable_base = os.path.basename(py_executable)

        if py_executable_base in required_symlinks:
            # Don't try to symlink to yourself.
            required_symlinks.remove(py_executable_base)

        for pth in required_symlinks:
            full_pth = join(bin_dir, pth)
            if os.path.exists(full_pth):
                os.unlink(full_pth)
            if symlink:
                os.symlink(py_executable_base, full_pth)
            else:
                shutil.copyfile(py_executable_base, full_pth)

    if is_win and ' ' in py_executable:
        # There's a bug with subprocess on Windows when using a first
        # argument that has a space in it.  Instead we have to quote
        # the value:
        py_executable = '"%s"' % py_executable
    # NOTE: keep this check as one line, cmd.exe doesn't cope with line breaks
    cmd = [py_executable, '-c', 'import sys;out=sys.stdout;'
        'getattr(out, "buffer", out).write(sys.prefix.encode("utf-8"))']
    logger.info('Testing executable with %s %s "%s"' % tuple(cmd))
    try:
        proc = subprocess.Popen(cmd,
                            stdout=subprocess.PIPE)
        proc_stdout, proc_stderr = proc.communicate()
    except OSError:
        e = sys.exc_info()[1]
        if e.errno == errno.EACCES:
            logger.fatal('ERROR: The executable %s could not be run: %s' % (py_executable, e))
            sys.exit(100)
        else:
            raise e

    proc_stdout = proc_stdout.strip().decode("utf-8")
    proc_stdout = os.path.normcase(os.path.abspath(proc_stdout))
    norm_home_dir = os.path.normcase(os.path.abspath(home_dir))
    if hasattr(norm_home_dir, 'decode'):
        norm_home_dir = norm_home_dir.decode(sys.getfilesystemencoding())
    if proc_stdout != norm_home_dir:
        logger.fatal(
            'ERROR: The executable %s is not functioning' % py_executable)
        logger.fatal(
            'ERROR: It thinks sys.prefix is %r (should be %r)'
            % (proc_stdout, norm_home_dir))
        logger.fatal(
            'ERROR: virtualenv is not compatible with this system or executable')
        if is_win:
            logger.fatal(
                'Note: some Windows users have reported this error when they '
                'installed Python for "Only this user" or have multiple '
                'versions of Python installed. Copying the appropriate '
                'PythonXX.dll to the virtualenv Scripts/ directory may fix '
                'this problem.')
        sys.exit(100)
    else:
        logger.info('Got sys.prefix result: %r' % proc_stdout)

    pydistutils = os.path.expanduser('~/.pydistutils.cfg')
    if os.path.exists(pydistutils):
        logger.notify('Please make sure you remove any previous custom paths from '
                      'your %s file.' % pydistutils)
    ## FIXME: really this should be calculated earlier

    fix_local_scheme(home_dir, symlink)

    if site_packages:
        if os.path.exists(site_packages_filename):
            logger.info('Deleting %s' % site_packages_filename)
            os.unlink(site_packages_filename)

    return py_executable


def install_activate(home_dir, bin_dir, prompt=None):
    home_dir = os.path.abspath(home_dir)
    if is_win or is_jython and os._name == 'nt':
        files = {
            'activate.bat': ACTIVATE_BAT,
            'deactivate.bat': DEACTIVATE_BAT,
            'activate.ps1': ACTIVATE_PS,
        }

        # MSYS needs paths of the form /c/path/to/file
        drive, tail = os.path.splitdrive(home_dir.replace(os.sep, '/'))
        home_dir_msys = (drive and "/%s%s" or "%s%s") % (drive[:1], tail)

        # Run-time conditional enables (basic) Cygwin compatibility
        home_dir_sh = ("""$(if [ "$OSTYPE" "==" "cygwin" ]; then cygpath -u '%s'; else echo '%s'; fi;)""" %
                       (home_dir, home_dir_msys))
        files['activate'] = ACTIVATE_SH.replace('__VIRTUAL_ENV__', home_dir_sh)

    else:
        files = {'activate': ACTIVATE_SH}

        # suppling activate.fish in addition to, not instead of, the
        # bash script support.
        files['activate.fish'] = ACTIVATE_FISH

        # same for csh/tcsh support...
        files['activate.csh'] = ACTIVATE_CSH

    files['activate_this.py'] = ACTIVATE_THIS
    if hasattr(home_dir, 'decode'):
        home_dir = home_dir.decode(sys.getfilesystemencoding())
    vname = os.path.basename(home_dir)
    for name, content in files.items():
        content = content.replace('__VIRTUAL_PROMPT__', prompt or '')
        content = content.replace('__VIRTUAL_WINPROMPT__', prompt or '(%s)' % vname)
        content = content.replace('__VIRTUAL_ENV__', home_dir)
        content = content.replace('__VIRTUAL_NAME__', vname)
        content = content.replace('__BIN_NAME__', os.path.basename(bin_dir))
        writefile(os.path.join(bin_dir, name), content)

def install_distutils(home_dir):
    distutils_path = change_prefix(distutils.__path__[0], home_dir)
    mkdir(distutils_path)
    ## FIXME: maybe this prefix setting should only be put in place if
    ## there's a local distutils.cfg with a prefix setting?
    home_dir = os.path.abspath(home_dir)
    ## FIXME: this is breaking things, removing for now:
    #distutils_cfg = DISTUTILS_CFG + "\n[install]\nprefix=%s\n" % home_dir
    writefile(os.path.join(distutils_path, '__init__.py'), DISTUTILS_INIT)
    writefile(os.path.join(distutils_path, 'distutils.cfg'), DISTUTILS_CFG, overwrite=False)

def fix_local_scheme(home_dir, symlink=True):
    """
    Platforms that use the "posix_local" install scheme (like Ubuntu with
    Python 2.7) need to be given an additional "local" location, sigh.
    """
    try:
        import sysconfig
    except ImportError:
        pass
    else:
        if sysconfig._get_default_scheme() == 'posix_local':
            local_path = os.path.join(home_dir, 'local')
            if not os.path.exists(local_path):
                os.mkdir(local_path)
                for subdir_name in os.listdir(home_dir):
                    if subdir_name == 'local':
                        continue
                    cp_or_ln = (os.symlink if symlink else copyfile)
                    cp_or_ln(os.path.abspath(os.path.join(home_dir, subdir_name)), \
                                                            os.path.join(local_path, subdir_name))

def fix_lib64(lib_dir, symlink=True):
    """
    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y
    instead of lib/pythonX.Y.  If this is such a platform we'll just create a
    symlink so lib64 points to lib
    """
    if [p for p in distutils.sysconfig.get_config_vars().values()
        if isinstance(p, basestring) and 'lib64' in p]:
        # PyPy's library path scheme is not affected by this.
        # Return early or we will die on the following assert.
        if is_pypy:
            logger.debug('PyPy detected, skipping lib64 symlinking')
            return

        logger.debug('This system uses lib64; symlinking lib64 to lib')

        assert os.path.basename(lib_dir) == 'python%s' % sys.version[:3], (
            "Unexpected python lib dir: %r" % lib_dir)
        lib_parent = os.path.dirname(lib_dir)
        top_level = os.path.dirname(lib_parent)
        lib_dir = os.path.join(top_level, 'lib')
        lib64_link = os.path.join(top_level, 'lib64')
        assert os.path.basename(lib_parent) == 'lib', (
            "Unexpected parent dir: %r" % lib_parent)
        if os.path.lexists(lib64_link):
            return
        cp_or_ln = (os.symlink if symlink else copyfile)
        cp_or_ln('lib', lib64_link)

def resolve_interpreter(exe):
    """
    If the executable given isn't an absolute path, search $PATH for the interpreter
    """
    # If the "executable" is a version number, get the installed executable for
    # that version
    python_versions = get_installed_pythons()
    if exe in python_versions:
        exe = python_versions[exe]

    if os.path.abspath(exe) != exe:
        paths = os.environ.get('PATH', '').split(os.pathsep)
        for path in paths:
            if os.path.exists(os.path.join(path, exe)):
                exe = os.path.join(path, exe)
                break
    if not os.path.exists(exe):
        logger.fatal('The executable %s (from --python=%s) does not exist' % (exe, exe))
        raise SystemExit(3)
    if not is_executable(exe):
        logger.fatal('The executable %s (from --python=%s) is not executable' % (exe, exe))
        raise SystemExit(3)
    return exe

def is_executable(exe):
    """Checks a file is executable"""
    return os.access(exe, os.X_OK)

############################################################
## Relocating the environment:

def make_environment_relocatable(home_dir):
    """
    Makes the already-existing environment use relative paths, and takes out
    the #!-based environment selection in scripts.
    """
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)
    activate_this = os.path.join(bin_dir, 'activate_this.py')
    if not os.path.exists(activate_this):
        logger.fatal(
            'The environment doesn\'t have a file %s -- please re-run virtualenv '
            'on this environment to update it' % activate_this)
    fixup_scripts(home_dir, bin_dir)
    fixup_pth_and_egg_link(home_dir)
    ## FIXME: need to fix up distutils.cfg

OK_ABS_SCRIPTS = ['python', 'python%s' % sys.version[:3],
                  'activate', 'activate.bat', 'activate_this.py']

def fixup_scripts(home_dir, bin_dir):
    if is_win:
        new_shebang_args = (
            '%s /c' % os.path.normcase(os.environ.get('COMSPEC', 'cmd.exe')),
            '', '.exe')
    else:
        new_shebang_args = ('/usr/bin/env', sys.version[:3], '')

    # This is what we expect at the top of scripts:
    shebang = '#!%s' % os.path.normcase(os.path.join(
        os.path.abspath(bin_dir), 'python%s' % new_shebang_args[2]))
    # This is what we'll put:
    new_shebang = '#!%s python%s%s' % new_shebang_args

    for filename in os.listdir(bin_dir):
        filename = os.path.join(bin_dir, filename)
        if not os.path.isfile(filename):
            # ignore subdirs, e.g. .svn ones.
            continue
        f = open(filename, 'rb')
        try:
            try:
                lines = f.read().decode('utf-8').splitlines()
            except UnicodeDecodeError:
                # This is probably a binary program instead
                # of a script, so just ignore it.
                continue
        finally:
            f.close()
        if not lines:
            logger.warn('Script %s is an empty file' % filename)
            continue

        old_shebang = lines[0].strip()
        old_shebang = old_shebang[0:2] + os.path.normcase(old_shebang[2:])

        if not old_shebang.startswith(shebang):
            if os.path.basename(filename) in OK_ABS_SCRIPTS:
                logger.debug('Cannot make script %s relative' % filename)
            elif lines[0].strip() == new_shebang:
                logger.info('Script %s has already been made relative' % filename)
            else:
                logger.warn('Script %s cannot be made relative (it\'s not a normal script that starts with %s)'
                            % (filename, shebang))
            continue
        logger.notify('Making script %s relative' % filename)
        script = relative_script([new_shebang] + lines[1:])
        f = open(filename, 'wb')
        f.write('\n'.join(script).encode('utf-8'))
        f.close()

def relative_script(lines):
    "Return a script that'll work in a relocatable environment."
    activate = "import os; activate_this=os.path.join(os.path.dirname(os.path.realpath(__file__)), 'activate_this.py'); exec(compile(open(activate_this).read(), activate_this, 'exec'), dict(__file__=activate_this)); del os, activate_this"
    # Find the last future statement in the script. If we insert the activation
    # line before a future statement, Python will raise a SyntaxError.
    activate_at = None
    for idx, line in reversed(list(enumerate(lines))):
        if line.split()[:3] == ['from', '__future__', 'import']:
            activate_at = idx + 1
            break
    if activate_at is None:
        # Activate after the shebang.
        activate_at = 1
    return lines[:activate_at] + ['', activate, ''] + lines[activate_at:]

def fixup_pth_and_egg_link(home_dir, sys_path=None):
    """Makes .pth and .egg-link files use relative paths"""
    home_dir = os.path.normcase(os.path.abspath(home_dir))
    if sys_path is None:
        sys_path = sys.path
    for path in sys_path:
        if not path:
            path = '.'
        if not os.path.isdir(path):
            continue
        path = os.path.normcase(os.path.abspath(path))
        if not path.startswith(home_dir):
            logger.debug('Skipping system (non-environment) directory %s' % path)
            continue
        for filename in os.listdir(path):
            filename = os.path.join(path, filename)
            if filename.endswith('.pth'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .pth file %s, skipping' % filename)
                else:
                    fixup_pth_file(filename)
            if filename.endswith('.egg-link'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .egg-link file %s, skipping' % filename)
                else:
                    fixup_egg_link(filename)

def fixup_pth_file(filename):
    lines = []
    prev_lines = []
    f = open(filename)
    prev_lines = f.readlines()
    f.close()
    for line in prev_lines:
        line = line.strip()
        if (not line or line.startswith('#') or line.startswith('import ')
            or os.path.abspath(line) != line):
            lines.append(line)
        else:
            new_value = make_relative_path(filename, line)
            if line != new_value:
                logger.debug('Rewriting path %s as %s (in %s)' % (line, new_value, filename))
            lines.append(new_value)
    if lines == prev_lines:
        logger.info('No changes to .pth file %s' % filename)
        return
    logger.notify('Making paths in .pth file %s relative' % filename)
    f = open(filename, 'w')
    f.write('\n'.join(lines) + '\n')
    f.close()

def fixup_egg_link(filename):
    f = open(filename)
    link = f.readline().strip()
    f.close()
    if os.path.abspath(link) != link:
        logger.debug('Link in %s already relative' % filename)
        return
    new_link = make_relative_path(filename, link)
    logger.notify('Rewriting link %s in %s as %s' % (link, filename, new_link))
    f = open(filename, 'w')
    f.write(new_link)
    f.close()

def make_relative_path(source, dest, dest_is_directory=True):
    """
    Make a filename relative, where the filename is dest, and it is
    being referred to from the filename source.

        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/usr/share/another-place/src/Directory')
        '../another-place/src/Directory'
        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/home/user/src/Directory')
        '../../../home/user/src/Directory'
        >>> make_relative_path('/usr/share/a-file.pth', '/usr/share/')
        './'
    """
    source = os.path.dirname(source)
    if not dest_is_directory:
        dest_filename = os.path.basename(dest)
        dest = os.path.dirname(dest)
    dest = os.path.normpath(os.path.abspath(dest))
    source = os.path.normpath(os.path.abspath(source))
    dest_parts = dest.strip(os.path.sep).split(os.path.sep)
    source_parts = source.strip(os.path.sep).split(os.path.sep)
    while dest_parts and source_parts and dest_parts[0] == source_parts[0]:
        dest_parts.pop(0)
        source_parts.pop(0)
    full_parts = ['..']*len(source_parts) + dest_parts
    if not dest_is_directory:
        full_parts.append(dest_filename)
    if not full_parts:
        # Special case for the current directory (otherwise it'd be '')
        return './'
    return os.path.sep.join(full_parts)



############################################################
## Bootstrap script creation:

def create_bootstrap_script(extra_text, python_version=''):
    """
    Creates a bootstrap script, which is like this script but with
    extend_parser, adjust_options, and after_install hooks.

    This returns a string that (written to disk of course) can be used
    as a bootstrap script with your own customizations.  The script
    will be the standard virtualenv.py script, with your extra text
    added (your extra text should be Python code).

    If you include these functions, they will be called:

    ``extend_parser(optparse_parser)``:
        You can add or remove options from the parser here.

    ``adjust_options(options, args)``:
        You can change options here, or change the args (if you accept
        different kinds of arguments, be sure you modify ``args`` so it is
        only ``[DEST_DIR]``).

    ``after_install(options, home_dir)``:

        After everything is installed, this function is called.  This
        is probably the function you are most likely to use.  An
        example would be::

            def after_install(options, home_dir):
                subprocess.call([join(home_dir, 'bin', 'easy_install'),
                                 'MyPackage'])
                subprocess.call([join(home_dir, 'bin', 'my-package-script'),
                                 'setup', home_dir])

        This example immediately installs a package, and runs a setup
        script from that package.

    If you provide something like ``python_version='2.5'`` then the
    script will start with ``#!/usr/bin/env python2.5`` instead of
    ``#!/usr/bin/env python``.  You can use this when the script must
    be run with a particular Python version.
    """
    filename = __file__
    if filename.endswith('.pyc'):
        filename = filename[:-1]
    f = codecs.open(filename, 'r', encoding='utf-8')
    content = f.read()
    f.close()
    py_exe = 'python%s' % python_version
    content = (('#!/usr/bin/env %s\n' % py_exe)
               + '## WARNING: This file is generated\n'
               + content)
    return content.replace('##EXT' 'END##', extra_text)

def adjust_options(options, args):
    args[:] = ['.']


def after_install(options, home_dir):
    if sys.platform == 'win32':
        bin_dir = join(home_dir, 'Scripts')
    else:
        bin_dir = join(home_dir, 'bin')

    try:
        subprocess.call(
            [join(bin_dir, 'pip'), 'install']
            + []
            + ['paver==1.2.1']
        )
    except OSError:
        subprocess.call(
            [join(bin_dir, 'easy_install')]
            + []
            + ['paver==1.2.1']
        )
    subprocess.call([join(bin_dir, 'paver'),'develop'])

def convert(s):
    b = base64.b64decode(s.encode('ascii'))
    return zlib.decompress(b).decode('utf-8')

##file site.py
SITE_PY = convert("""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""")

##file activate.sh
ACTIVATE_SH = convert("""
eJytVVFvokAQfudXTLEPtTlLeo9tvMSmJpq02hSvl7u2wRUG2QR2DSxSe7n/frOACEVNLlceRHa+
nfl25pvZDswCnoDPQ4QoTRQsENIEPci4CsBMZBq7CAsuLOYqvmYKTTj3YxnBgiXBudGBjUzBZUJI
BXEqgCvweIyuCjeG4eF2F5x14bcB9KQiQQWrjSddI1/oQIx6SYYeoFjzWIoIhYI1izlbhJjkKO7D
M/QEmKfO9O7WeRo/zr4P7pyHwWxkwitcgwpQ5Ej96OX+PmiFwLeVjFUOrNYKaq1Nud3nR2n8nI2m
k9H0friPTGVsUdptaxGrTEfpNVFEskxpXtUkkCkl1UNF9cgLBkx48J4EXyALuBtAwNYIjF5kcmUU
abMKmMq1ULoiRbgsDEkTSsKSGFCJ6Z8vY/2xYiSacmtyAfCDdCNTVZoVF8vSTQOoEwSnOrngBkws
MYGMBMg8/bMBLSYKS7pYEXP0PqT+ZmBT0Xuy+Pplj5yn4aM9nk72JD8/Wi+Gr98sD9eWSMOwkapD
BbUv91XSvmyVkICt2tmXR4tWmrcUCsjWOpw87YidEC8i0gdTSOFhouJUNxR+4NYBG0MftoCTD9F7
2rTtxG3oPwY1b2HncYwhrlmj6Wq924xtGDWqfdNxap+OYxplEurnMVo9RWks+rH8qKEtx7kZT5zJ
4H7oOFclrN6uFe+d+nW2aIUsSgs/42EIPuOhXq+jEo3S6tX6w2ilNkDnIpHCWdEQhFgwj9pkk7FN
l/y5eQvRSIQ5+TrL05lewxWpt/Lbhes5cJF3mLET1MGhcKCF+40tNWnUulxrpojwDo2sObdje3Bz
N3QeHqf3D7OjEXMVV8LN3ZlvuzoWHqiUcNKHtwNd0IbvPGKYYM31nPKCgkUILw3KL+Y8l7aO1ArS
Ad37nIU0fCj5NE5gQCuC5sOSu+UdI2NeXg/lFkQIlFpdWVaWZRfvqGiirC9o6liJ9FXGYrSY9mI1
D/Ncozgn13vJvsznr7DnkJWXsyMH7e42ljdJ+aqNDF1bFnKWFLdj31xtaJYK6EXFgqmV/ymD/ROG
+n8O9H8f5vsGOWXsL1+1k3g=
""")

##file activate.fish
ACTIVATE_FISH = convert("""
eJyVVWFv2jAQ/c6vuBoqQVWC9nVSNVGVCaS2VC2rNLWVZZILWAs2s52wVvvxsyEJDrjbmgpK7PP5
3bt3d22YLbmGlGcIq1wbmCPkGhPYcLMEEsGciwGLDS+YwSjlekngLFVyBe73GXSXxqw/DwbuTS8x
yyKpFr1WG15lDjETQhpQuQBuIOEKY5O9tlppLqxHKSDByjVAPwEy+mXtCq5MzjIUBTCRgEKTKwFG
gpBqxTLYXgN2myspVigMaYF92tZSowGZJf4mFExxNs9Qb614CgZtmH0BpEOn11f0cXI/+za8pnfD
2ZjA1sg9zlV/8QvcMhxbNu0QwgYokn/d+n02nt6Opzcjcnx1vXcIoN74O4ymWQXmHURfJw9jenc/
vbmb0enj6P5+cuVhqlKm3S0u2XRtRbA2QQAhV7VhBF0rsgUX9Ur1rBUXJgVSy8O751k8mzY5OrKH
RW3eaQhYGTr8hrXO59ALhxQ83mCsDLAid3T72CCSdJhaFE+fXgicXAARUiR2WeVO37gH3oYHzFKo
9k7CaPZ1UeNwH1tWuXA4uFKYYcEa8vaKqXl7q1UpygMPhFLvlVKyNzsSM3S2km7UBOl4xweUXk5u
6e3wZmQ9leY1XE/Ili670tr9g/5POBBpGIJXCCF79L1siarl/dbESa8mD8PL61GpzqpzuMS7tqeB
1YkALrRBloBMbR9yLcVx7frQAgUqR7NZIuzkEu110gbNit1enNs82Rx5utq7Z3prU78HFRgulqNC
OTwbqJa9vkJFclQgZSjbKeBgSsUtCtt9D8OwAbIVJuewQdfvQRaoFE9wd1TmCuRG7OgJ1bVXGHc7
z5WDL/WW36v2oi37CyVBak61+yPBA9C1qqGxzKQqZ0oPuocU9hpud0PIp8sDHkXR1HKkNlzjuUWA
a0enFUyzOWZA4yXGP+ZMI3Tdt2OuqU/SO4q64526cPE0A7ZyW2PMbWZiZ5HamIZ2RcCKLXhcDl2b
vXL+eccQoRzem80mekPDEiyiWK4GWqZmwxQOmPM0eIfgp1P9cqrBsewR2p/DPMtt+pfcYM+Ls2uh
hALufTAdmGl8B1H3VPd2af8fQAc4PgqjlIBL9cGQqNpXaAwe3LrtVn8AkZTUxg==
""")

##file activate.csh
ACTIVATE_CSH = convert("""
eJx9VG1P2zAQ/u5fcYQKNgTNPtN1WxlIQ4KCUEGaxuQ6yYVYSuzKdhqVX7+zk3bpy5YPUXL3PPfc
ne98DLNCWshliVDV1kGCUFvMoJGugMjq2qQIiVSxSJ1cCofD1BYRnOVGV0CfZ0N2DD91DalQSjsw
tQLpIJMGU1euvPe7QeJlkKzgWixlhnAt4aoUVsLnLBiy5NtbJWQ5THX1ZciYKKWwkOFaE04dUm6D
r/zh7pq/3D7Nnid3/HEy+wFHY/gEJydg0aFaQrBFgz1c5DG1IhTs+UZgsBC2GMFBlaeH+8dZXwcW
VPvCjXdlAvCfQsE7al0+07XjZvrSCUevR5dnkVeKlFYZmUztG4BdzL2u9KyLVabTU0bdfg7a0hgs
cSmUg6UwUiQl2iHrcbcVGNvPCiLOe7+cRwG13z9qRGgx2z6DHjfm/Op2yqeT+xvOLzs0PTKHDz2V
tkckFHoQfQRXoGJAj9el0FyJCmEMhzgMS4sB7KPOE2ExoLcSieYwDvR+cP8cg11gKkVJc2wRcm1g
QhYFlXiTaTfO2ki0fQoiFM4tLuO4aZrhOzqR4dIPcWx17hphMBY+Srwh7RTyN83XOWkcSPh1Pg/k
TXX/jbJTbMtUmcxZ+/bbqOsy82suFQg/BhdSOTRhMNBHlUarCpU7JzBhmkKmRejKOQzayQe6MWoa
n1wqWmuh6LZAaHxcdeqIlVLhIBJdO9/kbl0It2oEXQj+eGjJOuvOIR/YGRqvFhttUB2XTvLXYN2H
37CBdbW2W7j2r2+VsCn0doVWcFG1/4y1VwBjfwAyoZhD
""")

##file activate.bat
ACTIVATE_BAT = convert("""
eJx9UdEKgjAUfW6wfxjiIH+hEDKUFHSKLCMI7kNOEkIf9P9pTJ3OLJ/03HPPPed4Es9XS9qqwqgT
PbGKKOdXL4aAFS7A4gvAwgijuiKlqOpGlATS2NeMLE+TjJM9RkQ+SmqAXLrBo1LLIeLdiWlD6jZt
r7VNubWkndkXaxg5GO3UaOOKS6drO3luDDiO5my3iA0YAKGzPRV1ack8cOdhysI0CYzIPzjSiH5X
0QcvC8Lfaj0emsVKYF2rhL5L3fCkVjV76kShi59NHwDniAHzkgDgqBcwOgTMx+gDQQqXCw==
""")

##file deactivate.bat
DEACTIVATE_BAT = convert("""
eJxzSE3OyFfIT0vj4ipOLVEI8wwKCXX0iXf1C7Pl4spMU0hJTcvMS01RiPf3cYmHyQYE+fsGhCho
cCkAAUibEkTEVhWLMlUlLk6QGixStlyaeCyJDPHw9/Pw93VFsQguim4ZXAJoIUw5DhX47XUM8UCx
EchHtwsohN1bILUgw61c/Vy4AJYPYm4=
""")

##file activate.ps1
ACTIVATE_PS = convert("""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""")

##file distutils-init.py
DISTUTILS_INIT = convert("""
eJytV1uL4zYUfvevOE0ottuMW9q3gVDa3aUMXXbLMlDKMBiNrSTqOJKRlMxkf33PkXyRbGe7Dw2E
UXTu37lpxLFV2oIyifAncxmOL0xLIfcG+gv80x9VW6maw7o/CANSWWBwFtqeWMPlGY6qPjV8A0bB
C4eKSTgZ5LRgFeyErMEeOBhbN+Ipgeizhjtnhkn7DdyjuNLPoCS0l/ayQTG0djwZC08cLXozeMss
aG5EzQ0IScpnWtHSTXuxByV/QCmxE7y+eS0uxWeoheaVVfqSJHiU7Mhhi6gULbOHorshkrEnKxpT
0n3A8Y8SMpuwZx6aoix3ouFlmW8gHRSkeSJ2g7hU+kiHLDaQw3bmRDaTGfTnty7gPm0FHbIBg9U9
oh1kZzAFLaue2R6htPCtAda2nGlDSUJ4PZBgCJBGVcwKTAMz/vJiLD+Oin5Z5QlvDPdulC6EsiyE
NFzb7McNTKJzbJqzphx92VKRFY1idenzmq3K0emRcbWBD0ryqc4NZGmKOOOX9Pz5x+/l27tP797c
f/z0d+4NruGNai8uAM0bfsYaw8itFk8ny41jsfpyO+BWlpqfhcG4yxLdi/0tQqoT4a8Vby382mt8
p7XSo7aWGdPBc+b6utaBmCQ7rQKQoWtAuthQCiold2KfJIPTT8xwg9blPumc+YDZC/wYGdAyHpJk
vUbHbHWAp5No6pK/WhhLEWrFjUwtPEv1Agf8YmnsuXUQYkeZoHm8ogP16gt2uHoxcEMdf2C6pmbw
hUMsWGhanboh4IzzmsIpWs134jVPqD/c74bZHdY69UKKSn/+KfVhxLgUlToemayLMYQOqfEC61bh
cbhwaqoGUzIyZRFHPmau5juaWqwRn3mpWmoEA5nhzS5gog/5jbcFQqOZvmBasZtwYlG93k5GEiyw
buHhMWLjDarEGpMGB2LFs5nIJkhp/nUmZneFaRth++lieJtHepIvKgx6PJqIlD9X2j6pG1i9x3pZ
5bHuCPFiirGHeO7McvoXkz786GaKVzC9DSpnOxJdc4xm6NSVq7lNEnKdVlnpu9BNYoKX2Iq3wvgh
gGEUM66kK6j4NiyoneuPLSwaCWDxczgaolEWpiMyDVDb7dNuLAbriL8ig8mmeju31oNvQdpnvEPC
1vAXbWacGRVrGt/uXN/gU0CDDwgooKRrHfTBb1/s9lYZ8ZqOBU0yLvpuP6+K9hLFsvIjeNhBi0KL
MlOuWRn3FRwx5oHXjl0YImUx0+gLzjGchrgzca026ETmYJzPD+IpuKzNi8AFn048Thd63OdD86M6
84zE8yQm0VqXdbbgvub2pKVnS76icBGdeTHHXTKspUmr4NYo/furFLKiMdQzFjHJNcdAnMhltBJK
0/IKX3DVFqvPJ2dLE7bDBkH0l/PJ29074+F0CsGYOxsb7U3myTUncYfXqnLLfa6sJybX4g+hmcjO
kMRBfA1JellfRRKJcyRpxdS4rIl6FdmQCWjo/o9Qz7yKffoP4JHjOvABcRn4CZIT2RH4jnxmfpVG
qgLaAvQBNfuO6X0/Ux02nb4FKx3vgP+XnkX0QW9pLy/NsXgdN24dD3LxO2Nwil7Zlc1dqtP3d7/h
kzp1/+7hGBuY4pk0XD/0Ao/oTe/XGrfyM773aB7iUhgkpy+dwAMalxMP0DrBcsVw/6p25+/hobP9
GBknrWExDhLJ1bwt1NcCNblaFbMKCyvmX0PeRaQ=
""")

##file distutils.cfg
DISTUTILS_CFG = convert("""
eJxNj00KwkAMhfc9xYNuxe4Ft57AjYiUtDO1wXSmNJnK3N5pdSEEAu8nH6lxHVlRhtDHMPATA4uH
xJ4EFmGbvfJiicSHFRzUSISMY6hq3GLCRLnIvSTnEefN0FIjw5tF0Hkk9Q5dRunBsVoyFi24aaLg
9FDOlL0FPGluf4QjcInLlxd6f6rqkgPu/5nHLg0cXCscXoozRrP51DRT3j9QNl99AP53T2Q=
""")

##file activate_this.py
ACTIVATE_THIS = convert("""
eJyNU01v2zAMvetXEB4K21jmDOstQA4dMGCHbeihlyEIDMWmG62yJEiKE//7kXKdpN2KzYBt8euR
fKSyLPs8wiEo8wh4wqZTGou4V6Hm0wJa1cSiTkJdr8+GsoTRHuCotBayiWqQEYGtMCgfD1KjGYBe
5a3p0cRKiAe2NtLADikftnDco0ko/SFEVgEZ8aRC5GLux7i3BpSJ6J1H+i7A2CjiHq9z7JRZuuQq
siwTIvpxJYCeuWaBpwZdhB+yxy/eWz+ZvVSU8C4E9FFZkyxFsvCT/ZzL8gcz9aXVE14Yyp2M+2W0
y7n5mp0qN+avKXvbsyyzUqjeWR8hjGE+2iCE1W1tQ82hsCZN9UzlJr+/e/iab8WfqsmPI6pWeUPd
FrMsd4H/55poeO9n54COhUs+sZNEzNtg/wanpjpuqHJaxs76HtZryI/K3H7KJ/KDIhqcbJ7kI4ar
XL+sMgXnX0D+Te2Iy5xdP8yueSlQB/x/ED2BTAtyE3K4SYUN6AMNfbO63f4lBW3bUJPbTL+mjSxS
PyRfJkZRgj+VbFv+EzHFi5pKwUEepa4JslMnwkowSRCXI+m5XvEOvtuBrxHdhLalG0JofYBok6qj
YdN2dEngUlbC4PG60M1WEN0piu7Nq7on0mgyyUw3iV1etLo6r/81biWdQ9MWHFaePWZYaq+nmp+t
s3az+sj7eA0jfgPfeoN1
""")

MH_MAGIC = 0xfeedface
MH_CIGAM = 0xcefaedfe
MH_MAGIC_64 = 0xfeedfacf
MH_CIGAM_64 = 0xcffaedfe
FAT_MAGIC = 0xcafebabe
BIG_ENDIAN = '>'
LITTLE_ENDIAN = '<'
LC_LOAD_DYLIB = 0xc
maxint = majver == 3 and getattr(sys, 'maxsize') or getattr(sys, 'maxint')


class fileview(object):
    """
    A proxy for file-like objects that exposes a given view of a file.
    Modified from macholib.
    """

    def __init__(self, fileobj, start=0, size=maxint):
        if isinstance(fileobj, fileview):
            self._fileobj = fileobj._fileobj
        else:
            self._fileobj = fileobj
        self._start = start
        self._end = start + size
        self._pos = 0

    def __repr__(self):
        return '<fileview [%d, %d] %r>' % (
            self._start, self._end, self._fileobj)

    def tell(self):
        return self._pos

    def _checkwindow(self, seekto, op):
        if not (self._start <= seekto <= self._end):
            raise IOError("%s to offset %d is outside window [%d, %d]" % (
                op, seekto, self._start, self._end))

    def seek(self, offset, whence=0):
        seekto = offset
        if whence == os.SEEK_SET:
            seekto += self._start
        elif whence == os.SEEK_CUR:
            seekto += self._start + self._pos
        elif whence == os.SEEK_END:
            seekto += self._end
        else:
            raise IOError("Invalid whence argument to seek: %r" % (whence,))
        self._checkwindow(seekto, 'seek')
        self._fileobj.seek(seekto)
        self._pos = seekto - self._start

    def write(self, bytes):
        here = self._start + self._pos
        self._checkwindow(here, 'write')
        self._checkwindow(here + len(bytes), 'write')
        self._fileobj.seek(here, os.SEEK_SET)
        self._fileobj.write(bytes)
        self._pos += len(bytes)

    def read(self, size=maxint):
        assert size >= 0
        here = self._start + self._pos
        self._checkwindow(here, 'read')
        size = min(size, self._end - here)
        self._fileobj.seek(here, os.SEEK_SET)
        bytes = self._fileobj.read(size)
        self._pos += len(bytes)
        return bytes


def read_data(file, endian, num=1):
    """
    Read a given number of 32-bits unsigned integers from the given file
    with the given endianness.
    """
    res = struct.unpack(endian + 'L' * num, file.read(num * 4))
    if len(res) == 1:
        return res[0]
    return res


def mach_o_change(path, what, value):
    """
    Replace a given name (what) in any LC_LOAD_DYLIB command found in
    the given binary with a new name (value), provided it's shorter.
    """

    def do_macho(file, bits, endian):
        # Read Mach-O header (the magic number is assumed read by the caller)
        cputype, cpusubtype, filetype, ncmds, sizeofcmds, flags = read_data(file, endian, 6)
        # 64-bits header has one more field.
        if bits == 64:
            read_data(file, endian)
        # The header is followed by ncmds commands
        for n in range(ncmds):
            where = file.tell()
            # Read command header
            cmd, cmdsize = read_data(file, endian, 2)
            if cmd == LC_LOAD_DYLIB:
                # The first data field in LC_LOAD_DYLIB commands is the
                # offset of the name, starting from the beginning of the
                # command.
                name_offset = read_data(file, endian)
                file.seek(where + name_offset, os.SEEK_SET)
                # Read the NUL terminated string
                load = file.read(cmdsize - name_offset).decode()
                load = load[:load.index('\0')]
                # If the string is what is being replaced, overwrite it.
                if load == what:
                    file.seek(where + name_offset, os.SEEK_SET)
                    file.write(value.encode() + '\0'.encode())
            # Seek to the next command
            file.seek(where + cmdsize, os.SEEK_SET)

    def do_file(file, offset=0, size=maxint):
        file = fileview(file, offset, size)
        # Read magic number
        magic = read_data(file, BIG_ENDIAN)
        if magic == FAT_MAGIC:
            # Fat binaries contain nfat_arch Mach-O binaries
            nfat_arch = read_data(file, BIG_ENDIAN)
            for n in range(nfat_arch):
                # Read arch header
                cputype, cpusubtype, offset, size, align = read_data(file, BIG_ENDIAN, 5)
                do_file(file, offset, size)
        elif magic == MH_MAGIC:
            do_macho(file, 32, BIG_ENDIAN)
        elif magic == MH_CIGAM:
            do_macho(file, 32, LITTLE_ENDIAN)
        elif magic == MH_MAGIC_64:
            do_macho(file, 64, BIG_ENDIAN)
        elif magic == MH_CIGAM_64:
            do_macho(file, 64, LITTLE_ENDIAN)

    assert(len(what) >= len(value))
    do_file(open(path, 'r+b'))


if __name__ == '__main__':
    main()

## TODO:
## Copy python.exe.manifest
## Monkeypatch distutils.sysconfig

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# FlexGet documentation build configuration file, created by
# sphinx-quickstart on Thu Dec 22 21:49:19 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.coverage', 'sphinx.ext.extlinks', 'sphinx.ext.doctest']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'FlexGet'
copyright = u'2011, FlexGet'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.2'
# The full version, including alpha/beta/rc tags.
release = '1.2'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'FlexGetDocs'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'FlexGet.tex', u'FlexGet Documentation',
   u'FlexGet', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'flexget', u'FlexGet Documentation',
     [u'FlexGet'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'FlexGet', u'FlexGet Technical Documentation',
   u'FlexGet', 'FlexGet', 'Automation tool.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# -- Extlinks for Trac ---------------------------------------------------------

# sphinx.ext.extlinks
extlinks = {'ticket': ('http://flexget.com/ticket/%s', 'ticket '),
            'wiki': ('http://flexget.com/wiki/%s', 'wiki ')}

########NEW FILE########
__FILENAME__ = config_schema
from __future__ import unicode_literals, division, absolute_import
from collections import defaultdict
from datetime import datetime
import os
import re
import urlparse

import jsonschema
from jsonschema.compat import str_types

from flexget.event import fire_event
from flexget.utils import qualities, template
from flexget.utils.tools import parse_timedelta

schema_paths = {}


# TODO: Rethink how config key and schema registration work
def register_schema(path, schema):
    """
    Register `schema` to be available at `path` for $refs

    :param path: Path to make schema available
    :param schema: The schema, or function which returns the schema
    """
    schema_paths[path] = schema


# Validator that handles root structure of config.
_root_config_schema = None


def register_config_key(key, schema, required=False):
    """ Registers a valid root level key for the config.

    :param string key:
      Name of the root level key being registered.
    :param dict schema:
      Schema for the key.
    :param bool required:
      Specify whether this is a mandatory key.
    """
    _root_config_schema['properties'][key] = schema
    if required:
        _root_config_schema.setdefault('required', []).append(key)
    register_schema('/schema/config/%s' % key, schema)


def get_schema():
    global _root_config_schema
    if _root_config_schema is None:
        _root_config_schema = {'type': 'object', 'properties': {}, 'additionalProperties': False}
        fire_event('config.register')
        # TODO: Is /schema/root this the best place for this?
        register_schema('/schema/config', _root_config_schema)
    return _root_config_schema


def one_or_more(schema):
    """
    Helper function to construct a schema that validates items matching `schema` or an array
    containing items matching `schema`.

    """

    schema.setdefault('title', 'single value')
    return {
        'oneOf': [
            {'title': 'multiple values', 'type': 'array', 'items': schema, 'minItems': 1},
            schema
        ]
    }


def resolve_ref(uri):
    """
    Finds and returns a schema pointed to by `uri` that has been registered in the register_schema function.
    """
    parsed = urlparse.urlparse(uri)
    if parsed.path in schema_paths:
        schema = schema_paths[parsed.path]
        if callable(schema):
            return schema(**dict(urlparse.parse_qsl(parsed.query)))
        return schema
    raise jsonschema.RefResolutionError("%s could not be resolved" % uri)


def process_config(config, schema=None, set_defaults=True):
    """
    Validates the config, and sets defaults within it if `set_defaults` is set.
    If schema is not given, uses the root config schema.

    :returns: A list with :class:`jsonschema.ValidationError`s if any

    """
    if schema is None:
        schema = get_schema()
    resolver = RefResolver.from_schema(schema)
    validator = SchemaValidator(schema, resolver=resolver, format_checker=format_checker)
    if set_defaults:
        validator.VALIDATORS['properties'] = validate_properties_w_defaults
    try:
        errors = list(validator.iter_errors(config))
    finally:
        validator.VALIDATORS['properties'] = jsonschema.Draft4Validator.VALIDATORS['properties']
    # Customize the error messages
    for e in errors:
        set_error_message(e)
        e.json_pointer = '/' + '/'.join(map(unicode, e.path))
    return errors


def parse_time(time_string):
    """Parse a time string from the config into a :class:`datetime.time` object."""
    formats = ['%I:%M %p', '%H:%M', '%H:%M:%S']
    for f in formats:
        try:
            return datetime.strptime(time_string, f).time()
        except ValueError:
            continue
    raise ValueError('invalid time `%s`' % time_string)


def parse_interval(interval_string):
    """Takes an interval string from the config and turns it into a :class:`datetime.timedelta` object."""
    regexp = r'^\d+ (second|minute|hour|day|week)s?$'
    if not re.match(regexp, interval_string):
        raise ValueError("should be in format 'x (seconds|minutes|hours|days|weeks)'")
    return parse_timedelta(interval_string)


## Public API end here, the rest should not be used outside this module


class RefResolver(jsonschema.RefResolver):
    def __init__(self, *args, **kwargs):
        kwargs.setdefault('handlers', {'': resolve_ref})
        super(RefResolver, self).__init__(*args, **kwargs)


format_checker = jsonschema.FormatChecker(('email',))


@format_checker.checks('quality', raises=ValueError)
def is_quality(instance):
    if not isinstance(instance, str_types):
        return True
    return qualities.get(instance)


@format_checker.checks('quality_requirements', raises=ValueError)
def is_quality_req(instance):
    if not isinstance(instance, str_types):
        return True
    return qualities.Requirements(instance)



@format_checker.checks('time', raises=ValueError)
def is_time(time_string):
    if not isinstance(time_string, str_types):
        return True
    return parse_time(time_string) is not None


@format_checker.checks('interval', raises=ValueError)
def is_interval(interval_string):
    if not isinstance(interval_string, str_types):
        return True
    return parse_interval(interval_string) is not None


@format_checker.checks('regex', raises=ValueError)
def is_regex(instance):
    if not isinstance(instance, str_types):
        return True
    try:
        return re.compile(instance)
    except re.error as e:
        raise ValueError('Error parsing regex: %s' % e)


@format_checker.checks('file', raises=ValueError)
def is_file(instance):
    if not isinstance(instance, str_types):
        return True
    if os.path.isfile(os.path.expanduser(instance)):
        return True
    raise ValueError('`%s` does not exist' % instance)


@format_checker.checks('path', raises=ValueError)
def is_path(instance):
    if not isinstance(instance, str_types):
        return True
    # Only validate the part of the path before the first identifier to be replaced
    pat = re.compile(r'{[{%].*[}%]}')
    result = pat.search(instance)
    if result:
        instance = os.path.dirname(instance[0:result.start()])
    if os.path.isdir(os.path.expanduser(instance)):
        return True
    raise ValueError('`%s` does not exist' % instance)


#TODO: jsonschema has a format checker for uri if rfc3987 is installed, perhaps we should use that
@format_checker.checks('url')
def is_url(instance):
    if not isinstance(instance, str_types):
        return True
    regexp = ('(' + '|'.join(['ftp', 'http', 'https', 'file', 'udp']) +
              '):\/\/(\w+:{0,1}\w*@)?(\S+)(:[0-9]+)?(\/|\/([\w#!:.?+=&%@!\-\/]))?')
    return re.match(regexp, instance)


def set_error_message(error):
    """
    Create user facing error message from a :class:`jsonschema.ValidationError` `error`

    """
    # First, replace default error messages with our custom ones
    if error.validator == 'type':
        if isinstance(error.validator_value, basestring):
            valid_types = [error.validator_value]
        else:
            valid_types = list(error.validator_value)
        # Replace some types with more pythony ones
        replace = {'object': 'dict', 'array': 'list'}
        valid_types = [replace.get(t, t) for t in valid_types]
        # Make valid_types into an english list, with commas and 'or'
        valid_types = ', '.join(valid_types[:-2] + ['']) + ' or '.join(valid_types[-2:])
        if isinstance(error.instance, dict):
            error.message = 'Got a dict, expected: %s' % valid_types
        if isinstance(error.instance, list):
            error.message = 'Got a list, expected: %s' % valid_types
        error.message = 'Got `%s`, expected: %s' % (error.instance, valid_types)
    elif error.validator == 'format':
        if error.cause:
            error.message = unicode(error.cause)
    elif error.validator == 'enum':
        error.message = 'Must be one of the following: %s' % ', '.join(map(unicode, error.validator_value))
    elif error.validator == 'additionalProperties':
        if error.validator_value is False:
            extras = set(jsonschema._utils.find_additional_properties(error.instance, error.schema))
            if len(extras) == 1:
                error.message = 'The key `%s` is not valid here.' % extras.pop()
            else:
                error.message = 'The keys %s are not valid here.' % ', '.join('`%s`' % e for e in extras)
    else:
        # Remove u'' string representation from jsonschema error messages
        error.message = re.sub('u\'(.*?)\'', '`\\1`', error.message)

    # Then update with any custom error message supplied from the schema
    custom_error = error.schema.get('error_%s' % error.validator, error.schema.get('error'))
    if custom_error:
        error.message = template.render(custom_error, error.__dict__)


def select_child_errors(validator, errors):
    """
    Looks through subschema errors, if any subschema is determined to be the intended one,
    (based on 'type' keyword errors,) errors from its branch will be released instead of the parent error.
    """
    for error in errors:
        if not error.context:
            yield error
            continue
        # Split the suberrors up by which subschema they are from
        subschema_errors = defaultdict(list)
        for sube in error.context:
            subschema_errors[sube.schema_path[0]].append(sube)
        # Find the subschemas that did not have a 'type' error validating the instance at this path
        no_type_errors = dict(subschema_errors)
        valid_types = set()
        for i, errors in subschema_errors.iteritems():
            for e in errors:
                if e.validator == 'type' and not e.path:
                    # Remove from the no_type_errors dict
                    no_type_errors.pop(i, None)
                    # Add the valid types to the list of all valid types
                    if validator.is_type(e.validator_value, 'string'):
                        valid_types.add(e.validator_value)
                    else:
                        valid_types.update(e.validator_value)
        if not no_type_errors:
            # If all of the branches had a 'type' error, create our own virtual type error with all possible types
            for e in validator.descend(error.instance, {'type': valid_types}):
                yield e
        elif len(no_type_errors) == 1:
            # If one of the possible schemas did not have a 'type' error, assume that is the intended one and issue
            # all errors from that subschema
            for e in no_type_errors.values()[0]:
                e.schema_path.extendleft(reversed(error.schema_path))
                e.path.extendleft(reversed(error.path))
                yield e
        else:
            yield error


def validate_properties_w_defaults(validator, properties, instance, schema):

    if not validator.is_type(instance, 'object'):
        return
    for key, subschema in properties.iteritems():
        if 'default' in subschema:
            instance.setdefault(key, subschema['default'])
    for error in jsonschema.Draft4Validator.VALIDATORS["properties"](validator, properties, instance, schema):
        yield error


def validate_anyOf(validator, anyOf, instance, schema):
    errors = jsonschema.Draft4Validator.VALIDATORS["anyOf"](validator, anyOf, instance, schema)
    for e in select_child_errors(validator, errors):
        yield e


def validate_oneOf(validator, oneOf, instance, schema):
    errors = jsonschema.Draft4Validator.VALIDATORS["oneOf"](validator, oneOf, instance, schema)
    for e in select_child_errors(validator, errors):
        yield e


validators = {
    'anyOf': validate_anyOf,
    'oneOf': validate_oneOf
}

SchemaValidator = jsonschema.validators.extend(jsonschema.Draft4Validator, validators)

########NEW FILE########
__FILENAME__ = db_schema
from __future__ import unicode_literals, division, absolute_import
import logging

from sqlalchemy import Column, Integer, String

from flexget.manager import Base, Session
from flexget.event import event
from flexget.utils.database import with_session
from flexget.utils.sqlalchemy_utils import table_schema

log = logging.getLogger('schema')

# Stores a mapping of {plugin: {'version': version, 'tables': ['table_names'])}
plugin_schemas = {}


class PluginSchema(Base):

    __tablename__ = 'plugin_schema'

    id = Column(Integer, primary_key=True)
    plugin = Column(String)
    version = Column(Integer)

    def __init__(self, plugin, version=0):
        self.plugin = plugin
        self.version = version

    def __str__(self):
        return '<PluginSchema(plugin=%s,version=%i)>' % (self.plugin, self.version)


def get_version(plugin):
    session = Session()
    try:
        schema = session.query(PluginSchema).filter(PluginSchema.plugin == plugin).first()
        if not schema:
            log.debug('No schema version stored for %s' % plugin)
            return None
        else:
            return schema.version
    finally:
        session.close()


def set_version(plugin, version):
    if plugin not in plugin_schemas:
        raise ValueError('Tried to set schema version for %s plugin with no versioned_base.' % plugin)
    base_version = plugin_schemas[plugin]['version']
    if version != base_version:
        raise ValueError('Tried to set %s plugin schema version to %d when '
                         'it should be %d as defined in versioned_base.' % (plugin, version, base_version))
    session = Session()
    try:
        schema = session.query(PluginSchema).filter(PluginSchema.plugin == plugin).first()
        if not schema:
            log.debug('Initializing plugin %s schema version to %i' % (plugin, version))
            schema = PluginSchema(plugin, version)
            session.add(schema)
        else:
            if version < schema.version:
                raise ValueError('Tried to set plugin %s schema version to lower value' % plugin)
            if version != schema.version:
                log.debug('Updating plugin %s schema version to %i' % (plugin, version))
                schema.version = version
        session.commit()
    finally:
        session.close()


def upgrade_required():
    """Returns true if an upgrade of the database is required."""
    session = Session()
    try:
        for old_schema in session.query(PluginSchema).all():
            if old_schema.plugin in plugin_schemas and old_schema.version < plugin_schemas[old_schema.plugin]['version']:
                return True
        return False
    finally:
        session.close()


class UpgradeImpossible(Exception):
    """
    Exception to be thrown during a db upgrade function which will cause the old tables to be removed and recreated from
    the new model.
    """


def upgrade(plugin):
    """Used as a decorator to register a schema upgrade function.

    The wrapped function will be passed the current schema version and a session object.
    The function should return the new version of the schema after the upgrade.

    There is no need to commit the session, it will commit automatically if an upgraded
    schema version is returned.

    Example::

      from flexget import schema
      @schema.upgrade('your_plugin')
      def upgrade(ver, session):
           if ver == 2:
               # upgrade
               ver = 3
           return ver
    """

    def upgrade_decorator(func):

        @event('manager.upgrade')
        def upgrade_wrapper(manager):
            ver = get_version(plugin)
            session = Session()
            try:
                new_ver = func(ver, session)
                if new_ver > ver:
                    log.info('Plugin `%s` schema upgraded successfully' % plugin)
                    set_version(plugin, new_ver)
                    session.commit()
                    manager.db_upgraded = True
                elif new_ver < ver:
                    log.critical('A lower schema version was returned (%s) from the %s upgrade function '
                                 'than passed in (%s)' % (new_ver, plugin, ver))
                    manager.shutdown(finish_queue=False)
            except UpgradeImpossible:
                log.info('Plugin %s database is not upgradable. Flushing data and regenerating.' % plugin)
                reset_schema(plugin)
                session.commit()
            except Exception as e:
                log.exception('Failed to upgrade database for plugin %s: %s' % (plugin, e))
                manager.shutdown(finish_queue=False)
            finally:
                session.close()

        return upgrade_wrapper
    return upgrade_decorator


@with_session
def reset_schema(plugin, session=None):
    """
    Removes all tables from given plugin from the database, as well as removing current stored schema number.

    :param plugin: The plugin whose schema should be reset
    """
    if plugin not in plugin_schemas:
        raise ValueError('The plugin %s has no stored schema to reset.' % plugin)
    table_names = plugin_schemas[plugin].get('tables', [])
    tables = [table_schema(name, session) for name in table_names]
    # Remove the plugin's tables
    for table in tables:
        table.drop()
    # Remove the plugin from schema table
    session.query(PluginSchema).filter(PluginSchema.plugin == plugin).delete()
    # Create new empty tables
    Base.metadata.create_all(bind=session.bind)
    session.commit()


def register_plugin_table(tablename, plugin, version):
    plugin_schemas.setdefault(plugin, {'version': version, 'tables': []})
    if plugin_schemas[plugin]['version'] != version:
        raise Exception('Two different schema versions received for plugin %s' % plugin)
    plugin_schemas[plugin]['tables'].append(tablename)


class Meta(type):
    """Metaclass for objects returned by versioned_base factory"""

    def __new__(mcs, metaname, bases, dict_):
        """This gets called when a class that subclasses VersionedBase is defined."""
        new_bases = []
        for base in bases:
            # Check if we are creating a subclass of VersionedBase
            if base.__name__ == 'VersionedBase':
                # Register this table in plugin_schemas
                register_plugin_table(dict_['__tablename__'], base.plugin, base.version)
                # Make sure the resulting class also inherits from Base
                if not any(isinstance(base, type(Base)) for base in bases):
                    # We are not already subclassing Base, add it in to the list of bases instead of VersionedBase
                    new_bases.append(Base)
                    # Since Base and VersionedBase have 2 different metaclasses, a class that subclasses both of them
                    # must have a metaclass that subclasses both of their metaclasses.

                    class mcs(type(Base), mcs):
                        pass
            else:
                new_bases.append(base)

        return type.__new__(mcs, str(metaname), tuple(new_bases), dict_)

    def __getattr__(self, item):
        """Transparently return attributes of Base instead of our own."""
        return getattr(Base, item)


def versioned_base(plugin, version):
    """Returns a class which can be used like Base, but automatically stores schema version when tables are created."""

    return Meta('VersionedBase', (object,), {'__metaclass__': Meta, 'plugin': plugin, 'version': version})


def after_table_create(event, target, bind, tables=None, **kw):
    """Sets the schema version to most recent for a plugin when it's tables are freshly created."""
    from flexget.manager import manager
    if tables:
        # TODO: Detect if any database upgrading is needed and acquire the lock only in one place
        with manager.acquire_lock(event=False):
            tables = [table.name for table in tables]
            for plugin, info in plugin_schemas.iteritems():
                # Only set the version if all tables for a given plugin are being created
                if all(table in tables for table in info['tables']):
                    set_version(plugin, info['version'])

# Register a listener to call our method after tables are created
Base.metadata.append_ddl_listener('after-create', after_table_create)

########NEW FILE########
__FILENAME__ = entry
from __future__ import unicode_literals, division, absolute_import
from exceptions import Exception, UnicodeDecodeError, TypeError, KeyError
import logging
import copy
import functools

from flexget.plugin import PluginError
from flexget.utils.imdb import extract_id, make_url
from flexget.utils.template import render_from_entry

log = logging.getLogger('entry')


class EntryUnicodeError(Exception):
    """This exception is thrown when trying to set non-unicode compatible field value to entry."""

    def __init__(self, key, value):
        self.key = key
        self.value = value

    def __str__(self):
        return 'Field %s is not unicode-compatible (%r)' % (self.key, self.value)


class LazyField(object):
    """
    LazyField is a type of :class:`Entry` field which is evaluated only
    when it's value is requested. This way FlexGet can avoid doing heavy
    lookups from the internet or database for details that may not be needed
    ever.

    Stores callback function(s) to which populates :class:`Entry` fields.
    Callback is ran when it's called or to get a string representation."""

    def __init__(self, entry, field, func):
        self.entry = entry
        self.field = field
        self.funcs = [func]

    def __call__(self):
        # Return a result from the first lookup function which succeeds
        for func in self.funcs[:]:
            result = func(self.entry, self.field)
            if result is not None:
                return result

    def __str__(self):
        return str(self())

    def __repr__(self):
        return '<LazyField(field=%s)>' % self.field

    def __unicode__(self):
        return unicode(self())


class Entry(dict):
    """
    Represents one item in task. Must have `url` and *title* fields.

    Stores automatically *original_url* key, which is necessary because
    plugins (eg. urlrewriters) may change *url* into something else
    and otherwise that information would be lost.

    Entry will also transparently convert all ascii strings into unicode
    and raises :class:`EntryUnicodeError` if conversion fails on any value
    being set. Such failures are caught by :class:`~flexget.task.Task`
    and trigger :meth:`~flexget.task.Task.abort`.
    """

    def __init__(self, *args, **kwargs):
        self.traces = []
        self.snapshots = {}
        self._state = 'undecided'
        self._hooks = {'accept': [], 'reject': [], 'fail': [], 'complete': []}
        self.task = None

        if len(args) == 2:
            kwargs['title'] = args[0]
            kwargs['url'] = args[1]
            args = []

        # Make sure constructor does not escape our __setitem__ enforcement
        self.update(*args, **kwargs)

    def trace(self, message, operation=None, plugin=None):
        """
        Adds trace message to the entry which should contain useful information about why
        plugin did not operate on entry. Accept and Reject messages are added to trace automatically.

        :param string message: Message to add into entry trace.
        :param string operation: None, reject, accept or fail
        :param plugin: Uses task.current_plugin by default, pass value to override
        """
        if operation not in (None, 'accept', 'reject', 'fail'):
            raise ValueError('Unknown operation %s' % operation)
        item = (plugin, operation, message)
        if item not in self.traces:
            self.traces.append(item)

    def run_hooks(self, action, **kwargs):
        """
        Run hooks that have been registered for given ``action``.

        :param action: Name of action to run hooks for
        :param kwargs: Keyword arguments that should be passed to the registered functions
        """
        for func in self._hooks[action]:
            func(self, **kwargs)

    def add_hook(self, action, func, **kwargs):
        """
        Add a hook for ``action`` to this entry.

        :param string action: One of: 'accept', 'reject', 'fail', 'complete'
        :param func: Function to execute when event occurs
        :param kwargs: Keyword arguments that should be passed to ``func``
        :raises: ValueError when given an invalid ``action``
        """
        try:
            self._hooks[action].append(functools.partial(func, **kwargs))
        except KeyError:
            raise ValueError('`%s` is not a valid entry action' % action)

    def on_accept(self, func, **kwargs):
        """
        Register a function to be called when this entry is accepted.

        :param func: The function to call
        :param kwargs: Keyword arguments that should be passed to the registered function
        """
        self.add_hook('accept', func, **kwargs)

    def on_reject(self, func, **kwargs):
        """
        Register a function to be called when this entry is rejected.

        :param func: The function to call
        :param kwargs: Keyword arguments that should be passed to the registered function
        """
        self.add_hook('reject', func, **kwargs)

    def on_fail(self, func, **kwargs):
        """
        Register a function to be called when this entry is failed.

        :param func: The function to call
        :param kwargs: Keyword arguments that should be passed to the registered function
        """
        self.add_hook('fail', func, **kwargs)

    def on_complete(self, func, **kwargs):
        """
        Register a function to be called when a :class:`Task` has finished processing this entry.

        :param func: The function to call
        :param kwargs: Keyword arguments that should be passed to the registered function
        """
        self.add_hook('complete', func, **kwargs)

    def accept(self, reason=None, **kwargs):
        if self.rejected:
            log.debug('tried to accept rejected %r' % self)
        elif not self.accepted:
            self._state = 'accepted'
            self.trace(reason, operation='accept')
            # Run entry on_accept hooks
            self.run_hooks('accept', reason=reason, **kwargs)

    def reject(self, reason=None, **kwargs):
        # ignore rejections on immortal entries
        if self.get('immortal'):
            reason_str = '(%s)' % reason if reason else ''
            log.info('Tried to reject immortal %s %s' % (self['title'], reason_str))
            self.trace('Tried to reject immortal %s' % reason_str)
            return
        if not self.rejected:
            self._state = 'rejected'
            self.trace(reason, operation='reject')
            # Run entry on_reject hooks
            self.run_hooks('reject', reason=reason, **kwargs)

    def fail(self, reason=None, **kwargs):
        log.debug('Marking entry \'%s\' as failed' % self['title'])
        if not self.failed:
            self._state = 'failed'
            self.trace(reason, operation='fail')
            log.error('Failed %s (%s)' % (self['title'], reason))
            # Run entry on_fail hooks
            self.run_hooks('fail', reason=reason, **kwargs)

    def complete(self, **kwargs):
        # Run entry on_complete hooks
        self.run_hooks('complete', **kwargs)

    @property
    def accepted(self):
        return self._state == 'accepted'

    @property
    def rejected(self):
        return self._state == 'rejected'

    @property
    def failed(self):
        return self._state == 'failed'

    @property
    def undecided(self):
        return self._state == 'undecided'

    def __setitem__(self, key, value):
        # Enforce unicode compatibility. Check for all subclasses of basestring, so that NavigableStrings are also cast
        if isinstance(value, basestring) and not type(value) == unicode:
            try:
                value = unicode(value)
            except UnicodeDecodeError:
                raise EntryUnicodeError(key, value)

        # url and original_url handling
        if key == 'url':
            if not isinstance(value, basestring):
                raise PluginError('Tried to set %r url to %r' % (self.get('title'), value))
            self.setdefault('original_url', value)

        # title handling
        if key == 'title':
            if not isinstance(value, basestring):
                raise PluginError('Tried to set title to %r' % value)

        # TODO: HACK! Implement via plugin once #348 (entry events) is implemented
        # enforces imdb_url in same format
        if key == 'imdb_url' and isinstance(value, basestring):
            imdb_id = extract_id(value)
            if imdb_id:
                value = make_url(imdb_id)
            else:
                log.debug('Tried to set imdb_id to invalid imdb url: %s' % value)
                value = None

        try:
            log.trace('ENTRY SET: %s = %r' % (key, value))
        except Exception as e:
            log.debug('trying to debug key `%s` value threw exception: %s' % (key, e))

        dict.__setitem__(self, key, value)

    def update(self, *args, **kwargs):
        """Overridden so our __setitem__ is not avoided."""
        if args:
            if len(args) > 1:
                raise TypeError("update expected at most 1 arguments, got %d" % len(args))
            other = dict(args[0])
            for key in other:
                self[key] = other[key]
        for key in kwargs:
            self[key] = kwargs[key]

    def setdefault(self, key, value=None):
        """Overridden so our __setitem__ is not avoided."""
        if key not in self:
            self[key] = value
        return self[key]

    def __getitem__(self, key):
        """Supports lazy loading of fields. If a stored value is a :class:`LazyField`, call it, return the result."""
        result = dict.__getitem__(self, key)
        if isinstance(result, LazyField):
            log.trace('evaluating lazy field %s' % key)
            return result()
        else:
            return result

    def get(self, key, default=None, eval_lazy=True, lazy=None):
        """
        Overridden so that our __getitem__ gets used for :class:`LazyFields`

        :param string key: Name of the key
        :param object default: Value to be returned if key does not exists
        :param bool eval_lazy: Allow evaluating LazyFields or not
        :param bool lazy: Provided for backwards compatibility
        :return: Value or given *default*
        """
        if lazy is not None:
            log.warning('deprecated lazy kwarg used')
            eval_lazy = lazy
        if not eval_lazy and self.is_lazy(key):
            return default
        try:
            return self[key]
        except KeyError:
            return default

    def __contains__(self, key):
        """Will cause lazy field lookup to occur and will return false if a field exists but is None."""
        return self.get(key) is not None

    def register_lazy_fields(self, fields, func):
        """Register a list of fields to be lazily loaded by callback func.

        :param list fields:
          List of field names that are registered as lazy fields
        :param func:
          Callback function which is called when lazy field needs to be evaluated.
          Function call will get params (entry, field).
          See :class:`LazyField` class for more details.
        """
        for field in fields:
            if self.is_lazy(field):
                # If the field is already a lazy field, append this function to it's list of functions
                dict.get(self, field).funcs.append(func)
            elif not self.get(field, eval_lazy=False):
                # If it is not a lazy field, and isn't already populated, make it a lazy field
                self[field] = LazyField(self, field, func)

    def unregister_lazy_fields(self, fields, func):
        """
        :param list fields: List of field names to unregister.
          If given field is not lazy loading, value is set to None
        :param function func: Function to be removed from registered.
        :return: Number of removed functions
        :rtype: int
        """
        removed = 0
        for field in fields:
            if self.is_lazy(field):
                lazy_funcs = dict.get(self, field).funcs
                if func in lazy_funcs:
                    removed += 1
                    lazy_funcs.remove(func)
                if not lazy_funcs:
                    self[field] = None
        return removed

    def is_lazy(self, field):
        """
        :param string field: Name of the field to check
        :return: True if field is lazy loading.
        :rtype: bool
        """
        return isinstance(dict.get(self, field), LazyField)

    def safe_str(self):
        return '%s | %s' % (self['title'], self['url'])

    # TODO: this is too manual, maybe we should somehow check this internally and throw some exception if
    # application is trying to operate on invalid entry
    def isvalid(self):
        """
        :return: True if entry is valid. Return False if this cannot be used.
        :rtype: bool
        """
        if not 'title' in self:
            return False
        if not 'url' in self:
            return False
        if not isinstance(self['url'], basestring):
            return False
        if not isinstance(self['title'], basestring):
            return False
        return True

    def take_snapshot(self, name):
        """
        Takes a snapshot of the entry under *name*. Snapshots can be accessed via :attr:`.snapshots`.
        :param string name: Snapshot name
        """
        snapshot = {}
        for field, value in self.iteritems():
            try:
                snapshot[field] = copy.deepcopy(value)
            except TypeError:
                log.warning('Unable to take `%s` snapshot for field `%s` in `%s`' % (name, field, self['title']))
        if snapshot:
            if name in self.snapshots:
                log.warning('Snapshot `%s` is being overwritten for `%s`' % (name, self['title']))
            self.snapshots[name] = snapshot

    def update_using_map(self, field_map, source_item, ignore_none=False):
        """
        Populates entry fields from a source object using a dictionary that maps from entry field names to
        attributes (or keys) in the source object.

        :param dict field_map:
          A dictionary mapping entry field names to the attribute in source_item (or keys,
          if source_item is a dict)(nested attributes/dicts are also supported, separated by a dot,)
          or a function that takes source_item as an argument
        :param source_item:
          Source of information to be used by the map
        :param ignore_none:
          Ignore any None values, do not record it to the Entry
        """
        func = dict.get if isinstance(source_item, dict) else getattr
        for field, value in field_map.iteritems():
            if isinstance(value, basestring):
                v = functools.reduce(func, value.split('.'), source_item)
            else:
                v = value(source_item)
            if ignore_none and v is None:
                continue
            self[field] = v

    def render(self, template):
        """
        Renders a template string based on fields in the entry.

        :param string template: A template string that uses jinja2 or python string replacement format.
        :return: The result of the rendering.
        :rtype: string
        :raises RenderError: If there is a problem.
        """
        if not isinstance(template, basestring):
            raise ValueError('Trying to render non string template, got %s' % repr(template))
        log.trace('rendering: %s' % template)
        return render_from_entry(template, self)

    def __eq__(self, other):
        return self.get('title') == other.get('title') and self.get('original_url') == other.get('original_url')

    def __hash__(self):
        return hash(self.get('title', '') + self.get('original_url', ''))

    def __repr__(self):
        return '<Entry(title=%s,state=%s)>' % (self['title'], self._state)

########NEW FILE########
__FILENAME__ = event
"""
Provides small event framework
"""
from __future__ import unicode_literals, division, absolute_import
import logging

log = logging.getLogger('event')

_events = {}


class Event(object):
    """Represents one registered event."""

    def __init__(self, name, func, priority=128):
        self.name = name
        self.func = func
        self.priority = priority

    def __call__(self, *args, **kwargs):
        return self.func(*args, **kwargs)

    def __eq__(self, other):
        return self.priority == other.priority

    def __lt__(self, other):
        return self.priority < other.priority

    def __gt__(self, other):
        return self.priority > other.priority

    def __str__(self):
        return '<Event(name=%s,func=%s,priority=%s)>' % (self.name, self.func.__name__, self.priority)

    __repr__ = __str__


def event(name, priority=128):
    """Register event to function with a decorator"""

    def decorator(func):
        add_event_handler(name, func, priority)
        return func

    return decorator


def get_events(name):
    """
    :param String name: event name
    :return: List of :class:`Event` for *name* ordered by priority
    """
    if not name in _events:
        raise KeyError('No such event %s' % name)
    _events[name].sort(reverse=True)
    return _events[name]


def add_event_handler(name, func, priority=128):
    """
    :param string name: Event name
    :param function func: Function that acts as event handler
    :param priority: Priority for this hook
    :return: Event created
    :rtype: Event
    :raises Exception: If *func* is already registered in an event
    """
    events = _events.setdefault(name, [])
    for event in events:
        if event.func == func:
            raise ValueError('%s has already been registered as event listener under name %s' % (func.__name__, name))
    log.trace('registered function %s to event %s' % (func.__name__, name))
    event = Event(name, func, priority)
    events.append(event)
    return event


def remove_event_handlers(name):
    """Removes all handlers for given event `name`."""
    _events.pop(name, None)


def remove_event_handler(name, func):
    """Remove `func` from the handlers for event `name`."""
    for e in list(_events.get(name, [])):
        if e.func is func:
            _events[name].remove(e)


def fire_event(name, *args, **kwargs):
    """
    Trigger an event with *name*. If event is not hooked by anything nothing happens.

    :param name: Name of event to be called
    :param args: List of arguments passed to handler function
    :param kwargs: Key Value arguments passed to handler function
    """
    if not name in _events:
        return
    for event in get_events(name):
        event(*args, **kwargs)

########NEW FILE########
__FILENAME__ = ipc
from __future__ import unicode_literals, division, absolute_import
import logging
import random
import string
import threading

import rpyc
from rpyc.utils.server import ThreadedServer

from flexget.scheduler import BufferQueue
from flexget.utils.tools import console

log = logging.getLogger('ipc')

# Allow some attributes from dict interface to be called over the wire
rpyc.core.protocol.DEFAULT_CONFIG['safe_attrs'].update(['items'])
rpyc.core.protocol.DEFAULT_CONFIG['allow_pickle'] = True

IPC_VERSION = 1
AUTH_ERROR = 'authentication error'
AUTH_SUCCESS = 'authentication success'


class DaemonService(rpyc.Service):
    # This will be populated when the server is started
    manager = None

    def exposed_version(self):
        return IPC_VERSION

    def exposed_execute(self, options=None):
        # Dictionaries are pass by reference with rpyc, turn this into a real dict on our side
        if options:
            options = rpyc.utils.classic.obtain(options)
        if self.manager.scheduler.run_queue.qsize() > 0:
            self.client_console('There is already a task executing. This task will execute next.')
        log.info('Executing for client.')
        cron = options and options.get('cron')
        output = None if cron else BufferQueue()
        tasks_finished = self.manager.scheduler.execute(options=options, output=output)
        if output:
            # Send back any output until all tasks have finished
            while any(not t.is_set() for t in tasks_finished) or output.qsize():
                try:
                    self.client_console(output.get(True, 0.5).rstrip())
                except BufferQueue.Empty:
                    continue

    def exposed_reload(self):
        try:
            self.manager.load_config()
        except ValueError as e:
            self.client_console('Error loading config: %s' % e.args[0])
        else:
            self.client_console('Config successfully reloaded from disk.')

    def exposed_shutdown(self, finish_queue=False):
        log.info('Shutdown requested over ipc.')
        self.client_console('Daemon shutdown requested.')
        self.manager.scheduler.shutdown(finish_queue=finish_queue)

    def client_console(self, text):
        self._conn.root.console(text)


class ClientService(rpyc.Service):
    def on_connect(self):
        """Make sure the client version matches our own."""
        daemon_version = self._conn.root.version()
        if IPC_VERSION != daemon_version:
            self._conn.close()
            raise ValueError('Daemon is different version than client.')

    def exposed_version(self):
        return IPC_VERSION

    def exposed_console(self, text):
        console(text)


class IPCServer(threading.Thread):
    def __init__(self, manager, port=None):
        super(IPCServer, self).__init__(name='ipc_server')
        self.daemon = True
        self.manager = manager
        self.host = '127.0.0.1'
        self.port = port or 0
        self.password = ''.join(random.choice(string.letters + string.digits) for x in range(15))
        self.server = None

    def authenticator(self, sock):
        channel = rpyc.Channel(rpyc.SocketStream(sock))
        password = channel.recv()
        if password != self.password:
            channel.send(AUTH_ERROR)
            raise rpyc.utils.authenticators.AuthenticationError('Invalid password from client.')
        channel.send(AUTH_SUCCESS)
        return sock, self.password

    def run(self):
        DaemonService.manager = self.manager
        self.server = ThreadedServer(
            DaemonService, hostname=self.host, port=self.port, authenticator=self.authenticator, logger=log
        )
        # If we just chose an open port, write save the chosen one
        self.port = self.server.listener.getsockname()[1]
        self.manager.write_lock(ipc_info={'port': self.port, 'password': self.password})
        self.server.start()

    def shutdown(self):
        self.server.close()


class IPCClient(object):
    def __init__(self, port, password):
        channel = rpyc.Channel(rpyc.SocketStream.connect('127.0.0.1', port))
        channel.send(password)
        response = channel.recv()
        if response == AUTH_ERROR:
            # TODO: What to raise here. I guess we create a custom error
            raise ValueError('Invalid password for daemon')
        self.conn = rpyc.utils.factory.connect_channel(channel, service=ClientService)

    def close(self):
        self.conn.close()

    def __getattr__(self, item):
        """Proxy all other calls to the exposed daemon service."""
        return getattr(self.conn.root, item)

########NEW FILE########
__FILENAME__ = logger
from __future__ import unicode_literals, division, absolute_import
import logging
import logging.handlers
import re
import sys
import threading
import string
import warnings

# A level more detailed than DEBUG
TRACE = 5
# A level more detailed than INFO
VERBOSE = 15


class FlexGetLogger(logging.Logger):
    """Custom logger that adds task and execution info to log records."""
    local = threading.local()

    def makeRecord(self, name, level, fn, lno, msg, args, exc_info, func=None, extra=None):
        extra = {'task': getattr(FlexGetLogger.local, 'task', '')}
        return logging.Logger.makeRecord(self, name, level, fn, lno, msg, args, exc_info, func, extra)

    def trace(self, msg, *args, **kwargs):
        """Log at TRACE level (more detailed than DEBUG)."""
        self.log(TRACE, msg, *args, **kwargs)

    def verbose(self, msg, *args, **kwargs):
        """Log at VERBOSE level (displayed when FlexGet is run interactively.)"""
        self.log(VERBOSE, msg, *args, **kwargs)


class FlexGetFormatter(logging.Formatter):
    """Custom formatter that can handle both regular log records and those created by FlexGetLogger"""
    plain_fmt = '%(asctime)-15s %(levelname)-8s %(name)-29s %(message)s'
    flexget_fmt = '%(asctime)-15s %(levelname)-8s %(name)-13s %(task)-15s %(message)s'

    def __init__(self):
        logging.Formatter.__init__(self, self.plain_fmt, '%Y-%m-%d %H:%M')

    def format(self, record):
        if hasattr(record, 'task'):
            self._fmt = self.flexget_fmt
        else:
            self._fmt = self.plain_fmt
        record.message = record.getMessage()
        if string.find(self._fmt, "%(asctime)") >= 0:
            record.asctime = self.formatTime(record, self.datefmt)
        s = self._fmt % record.__dict__
        # Replace newlines in log messages with \n
        s = s.replace('\n', '\\n')
        if record.exc_info:
            # Cache the traceback text to avoid converting it multiple times
            # (it's constant anyway)
            if not record.exc_text:
                record.exc_text = self.formatException(record.exc_info)
        if record.exc_text:
            if s[-1:] != "\n":
                s += "\n"
            s += record.exc_text
        return s


def set_execution(execution):
    FlexGetLogger.local.execution = execution


def set_task(task):
    FlexGetLogger.local.task = task


class PrivacyFilter(logging.Filter):
    """Edits log messages and <hides> obviously private information."""

    def __init__(self):
        self.replaces = []

        def hide(name):
            s = '([?&]%s=)\w+' % name
            p = re.compile(s)
            self.replaces.append(p)

        for param in ['passwd', 'password', 'pw', 'pass', 'passkey',
                      'key', 'apikey', 'user', 'username', 'uname', 'login', 'id']:
            hide(param)

    def filter(self, record):
        if not isinstance(record.msg, basestring):
            return False
        for p in self.replaces:
            record.msg = p.sub(r'\g<1><hidden>', record.msg)
            record.msg = record.msg
        return False

_logging_configured = False
_mem_handler = None
_logging_started = False


def initialize(unit_test=False):
    """Prepare logging.
    """
    global _logging_configured, _mem_handler

    if _logging_configured:
        return

    warnings.simplefilter('once')
    logging.addLevelName(TRACE, 'TRACE')
    logging.addLevelName(VERBOSE, 'VERBOSE')
    _logging_configured = True

    # with unit test we want a bit simpler setup
    if unit_test:
        logging.basicConfig()
        return

    # root logger
    logger = logging.getLogger()
    formatter = FlexGetFormatter()

    _mem_handler = logging.handlers.MemoryHandler(1000 * 1000, 100)
    _mem_handler.setFormatter(formatter)
    logger.addHandler(_mem_handler)

    #
    # Process commandline options, unfortunately we need to do it before argparse is available
    #

    # turn on debug level
    if '--debug' in sys.argv:
        logger.setLevel(logging.DEBUG)
    elif '--debug-trace' in sys.argv:
        logger.setLevel(TRACE)

    # without --cron we log to console
    # this must be done at initialize because otherwise there will be too much delay (user feedback) (see #1113)
    if not '--cron' in sys.argv:
        console = logging.StreamHandler()
        console.setFormatter(formatter)
        logger.addHandler(console)


def start(filename=None, level=logging.INFO, debug=False):
    """After initialization, start file logging.
    """
    global _logging_started

    assert _logging_configured
    if _logging_started:
        return

    if debug:
        handler = logging.StreamHandler()
    else:
        handler = logging.handlers.RotatingFileHandler(filename, maxBytes=1000 * 1024, backupCount=9)

    handler.setFormatter(_mem_handler.formatter)

    _mem_handler.setTarget(handler)

    # root logger
    logger = logging.getLogger()
    logger.removeHandler(_mem_handler)
    logger.addHandler(handler)
    logger.addFilter(PrivacyFilter())
    logger.setLevel(level)

    # flush what we have stored from the plugin initialization
    _mem_handler.flush()
    _logging_started = True


def flush_logging_to_console():
    """Flushes memory logger to console"""
    console = logging.StreamHandler()
    console.setFormatter(_mem_handler.formatter)
    logger = logging.getLogger()
    logger.addHandler(console)
    if len(_mem_handler.buffer) > 0:
        for record in _mem_handler.buffer:
            console.handle(record)
    _mem_handler.flush()

# Set our custom logger class as default
logging.setLoggerClass(FlexGetLogger)

########NEW FILE########
__FILENAME__ = manager
from __future__ import unicode_literals, division, absolute_import, print_function
import atexit
import codecs
from contextlib import contextmanager
import signal
import os
import sys
import shutil
import logging
import threading
import pkg_resources
import yaml
from datetime import datetime, timedelta

import sqlalchemy
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.pool import SingletonThreadPool
from sqlalchemy.exc import OperationalError

# These need to be declared before we start importing from other flexget modules, since they might import them
Base = declarative_base()
Session = sessionmaker()

from flexget import config_schema, db_schema
from flexget.event import fire_event
from flexget.ipc import IPCServer, IPCClient
from flexget.scheduler import Scheduler
from flexget.utils.tools import pid_exists

log = logging.getLogger('manager')

manager = None
DB_CLEANUP_INTERVAL = timedelta(days=7)


@sqlalchemy.event.listens_for(Session, 'before_commit')
def before_commit(session):
    if not manager.has_lock and session.dirty:
        log.debug('BUG?: Database writes should not be tried when there is no database lock.')


class Manager(object):

    """Manager class for FlexGet

    Fires events:

    * manager.initialize

      The first time the manager is initialized, before config is loaded

    * manager.before_config_load

      Before the config file is loaded from disk

    * manager.before_config_validate

      When updating the config, before the validator is run on it

    * manager.config_updated

      After a configuration file has been loaded or changed (and validated) this event is fired

    * manager.startup

      After manager has been initialized. This is when application becomes ready to use, however no database lock is
      present, so the database must not be modified on this event.

    * manager.lock_acquired

      The manager does not always require a lock on startup, if one is requested, this event will run when it has been
      acquired successfully

    * manager.upgrade

      If any plugins have declared a newer schema version than exists in the database, this event will be fired to
      allow plugins to upgrade their tables

    * manager.shutdown

      When the manager is exiting

    * manager.execute.completed

      If execution in current process was completed

    * manager.daemon.started
    * manager.daemon.completed
    * manager.db_cleanup
    """

    unit_test = False
    options = None

    def __init__(self, options):
        """
        :param options: argparse parsed options object
        """
        global manager
        assert not manager, 'Only one instance of Manager should be created at a time!'
        self.options = options
        self.config_base = None
        self.config_name = None
        self.config_path = None
        self.db_filename = None
        self.engine = None
        self.lockfile = None
        self.database_uri = None
        self.db_upgraded = False
        self._has_lock = False

        self.config = {}

        self.scheduler = Scheduler(self)
        self.ipc_server = IPCServer(self, options.ipc_port)
        manager = self
        self.initialize()

        # cannot be imported at module level because of circular references
        from flexget.utils.simple_persistence import SimplePersistence
        self.persist = SimplePersistence('manager')

        log.debug('sys.defaultencoding: %s' % sys.getdefaultencoding())
        log.debug('sys.getfilesystemencoding: %s' % sys.getfilesystemencoding())
        log.debug('os.path.supports_unicode_filenames: %s' % os.path.supports_unicode_filenames)

        if db_schema.upgrade_required():
            log.info('Database upgrade is required. Attempting now.')
            # Make sure not to fire the lock-acquired event yet
            # TODO: Detect if any database upgrading is needed and acquire the lock only in one place
            with self.acquire_lock(event=False):
                fire_event('manager.upgrade', self)
                if manager.db_upgraded:
                    fire_event('manager.db_upgraded', self)
        fire_event('manager.startup', self)

    def __del__(self):
        global manager
        manager = None

    def initialize(self):
        """Separated from __init__ so that unit tests can modify options before loading config."""
        self.setup_yaml()
        self.find_config(create=(self.options.cli_command == 'webui'))
        self.init_sqlalchemy()
        fire_event('manager.initialize', self)
        try:
            self.load_config()
        except ValueError as e:
            log.critical('Failed to load config file: %s' % e.args[0])
            self.shutdown(finish_queue=False)
            sys.exit(1)

    @property
    def tasks(self):
        """A list of tasks in the config"""
        if not self.config:
            return []
        return self.config.get('tasks', {}).keys()

    @property
    def has_lock(self):
        return self._has_lock

    def run_cli_command(self):
        """
        Starting point when executing from commandline, dispatch execution
        to correct destination.

        * :meth:`.execute_command`
        * :meth:`.daemon_command`
        * :meth:`.webui_command`
        * CLI plugin callback function
        """
        command = self.options.cli_command
        options = getattr(self.options, command)
        # First check for built-in commands
        if command == 'execute':
            self.execute_command(options)
        elif command == 'daemon':
            self.daemon_command(options)
        elif command == 'webui':
            self.webui_command(options)
        else:
            # Otherwise dispatch the command to the callback function
            options.cli_command_callback(self, options)

    def execute_command(self, options):
        """
        Send execute command to daemon through IPC or perform execution
        on current process.

        Fires events:

        * manager.execute.completed

        :param options: argparse options
        """
        # If a daemon is started, send the execution to the daemon
        ipc_info = self.check_ipc_info()
        if ipc_info:
            try:
                log.info('There is a daemon running for this config. Sending execution to running daemon.')
                client = IPCClient(ipc_info['port'], ipc_info['password'])
            except ValueError as e:
                log.error(e)
            else:
                client.execute(dict(options))
            self.shutdown()
            return
        # Otherwise we run the execution ourselves
        with self.acquire_lock():
            fire_event('manager.execute.started', self)
            self.scheduler.start(run_schedules=False)
            self.scheduler.execute(options)
            self.scheduler.shutdown(finish_queue=True)
            try:
                self.scheduler.wait()
            except KeyboardInterrupt:
                log.error('Got ctrl-c exiting after this task completes. Press ctrl-c again to abort this task.')
            else:
                fire_event('manager.execute.completed', self)
            self.shutdown(finish_queue=False)

    def daemon_command(self, options):
        """
        Fires events:

        * manager.daemon.started
        * manager.daemon.completed

        :param options: argparse options
        """
        if options.action == 'start':
            if options.daemonize:
                self.daemonize()
            with self.acquire_lock():
                try:
                    signal.signal(signal.SIGTERM, self._handle_sigterm)
                except ValueError as e:
                    # If flexget is being called from another script, e.g. windows service helper, and we are not the
                    # main thread, this error will occur.
                    log.debug('Error registering sigterm handler: %s' % e)
                self.ipc_server.start()
                fire_event('manager.daemon.started', self)
                self.scheduler.start()
                try:
                    self.scheduler.wait()
                except KeyboardInterrupt:
                    log.info('Got ctrl-c, shutting down.')
                    fire_event('manager.daemon.completed', self)
                    self.shutdown(finish_queue=False)
        elif options.action == 'status':
            ipc_info = self.check_ipc_info()
            if ipc_info:
                log.info('Daemon running. (PID: %s)' % ipc_info['pid'])
            else:
                log.info('No daemon appears to be running for this config.')
        elif options.action in ['stop', 'reload']:
            ipc_info = self.check_ipc_info()
            if ipc_info:
                try:
                    client = IPCClient(ipc_info['port'], ipc_info['password'])
                except ValueError as e:
                    log.error(e)
                else:
                    if options.action == 'stop':
                        client.shutdown()
                    elif options.action == 'reload':
                        client.reload()
                self.shutdown()
            else:
                log.error('There does not appear to be a daemon running.')

    def webui_command(self, options):
        """
        :param options: argparse options
        """
        try:
            pkg_resources.require('flexget[webui]')
        except pkg_resources.DistributionNotFound as e:
            log.error('Dependency not met. %s' % e)
            log.error('Webui dependencies not installed. You can use `pip install flexget[webui]` to install them.')
            self.shutdown()
            return
        if options.daemonize:
            self.daemonize()
        from flexget.ui import webui
        with self.acquire_lock():
            webui.start(self)

    def _handle_sigterm(self, signum, frame):
        log.info('Got SIGTERM. Shutting down.')
        self.shutdown(finish_queue=False)

    def setup_yaml(self):
        """Sets up the yaml loader to return unicode objects for strings by default"""

        def construct_yaml_str(self, node):
            # Override the default string handling function
            # to always return unicode objects
            return self.construct_scalar(node)
        yaml.Loader.add_constructor(u'tag:yaml.org,2002:str', construct_yaml_str)
        yaml.SafeLoader.add_constructor(u'tag:yaml.org,2002:str', construct_yaml_str)

        # Set up the dumper to not tag every string with !!python/unicode
        def unicode_representer(dumper, uni):
            node = yaml.ScalarNode(tag=u'tag:yaml.org,2002:str', value=uni)
            return node
        yaml.add_representer(unicode, unicode_representer)

        # Set up the dumper to increase the indent for lists
        def increase_indent_wrapper(func):

            def increase_indent(self, flow=False, indentless=False):
                func(self, flow, False)
            return increase_indent

        yaml.Dumper.increase_indent = increase_indent_wrapper(yaml.Dumper.increase_indent)
        yaml.SafeDumper.increase_indent = increase_indent_wrapper(yaml.SafeDumper.increase_indent)

    def find_config(self, create=False):
        """
        Find the configuration file.

        :param bool create: If a config file is not found, and create is True, one will be created in the home folder
        """
        config = None
        home_path = os.path.join(os.path.expanduser('~'), '.flexget')
        options_config = os.path.expanduser(self.options.config)

        possible = []
        if os.path.isabs(options_config):
            # explicit path given, don't try anything
            config = options_config
            possible = [config]
        else:
            log.debug('Figuring out config load paths')
            try:
                possible.append(os.getcwdu())
            except OSError:
                log.debug('current directory invalid, not searching for config there')
            # for virtualenv / dev sandbox
            if hasattr(sys, 'real_prefix'):
                log.debug('Adding virtualenv path')
                possible.append(sys.prefix.decode(sys.getfilesystemencoding()))
            # normal lookup locations
            possible.append(home_path)
            if sys.platform.startswith('win'):
                # On windows look in ~/flexget as well, as explorer does not let you create a folder starting with a dot
                home_path = os.path.join(os.path.expanduser('~'), 'flexget')
                possible.append(home_path)
            else:
                # The freedesktop.org standard config location
                xdg_config = os.environ.get('XDG_CONFIG_HOME', os.path.join(os.path.expanduser('~'), '.config'))
                possible.append(os.path.join(xdg_config, 'flexget'))

            for path in possible:
                config = os.path.join(path, options_config)
                if os.path.exists(config):
                    log.debug('Found config: %s' % config)
                    break
            else:
                config = None

        if not (config and os.path.exists(config)):
            if not create:
                log.info('Tried to read from: %s' % ', '.join(possible))
                log.critical('Failed to find configuration file %s' % options_config)
                sys.exit(1)
            config = os.path.join(home_path, options_config)
            log.info('Config file %s not found. Creating new config %s' % (options_config, config))
            with open(config, 'w') as newconfig:
                # Write empty tasks to the config
                newconfig.write(yaml.dump({'tasks': {}}))

        log.debug('Config file %s selected' % config)
        self.config_path = config
        self.config_name = os.path.splitext(os.path.basename(config))[0]
        self.config_base = os.path.normpath(os.path.dirname(config))
        self.lockfile = os.path.join(self.config_base, '.%s-lock' % self.config_name)

    def load_config(self):
        """
        Loads the config file from disk, validates and activates it.

        :raises: `ValueError` if there is a problem loading the config file
        """
        fire_event('manager.before_config_load', self)
        with codecs.open(self.config_path, 'rb', 'utf-8') as f:
            try:
                raw_config = f.read()
            except UnicodeDecodeError:
                log.critical('Config file must be UTF-8 encoded.')
                raise ValueError('Config file is not UTF-8 encoded')
        try:
            config = yaml.safe_load(raw_config) or {}
        except Exception as e:
            msg = str(e).replace('\n', ' ')
            msg = ' '.join(msg.split())
            log.critical(msg)
            print('')
            print('-' * 79)
            print(' Malformed configuration file (check messages above). Common reasons:')
            print('-' * 79)
            print('')
            print(' o Indentation error')
            print(' o Missing : from end of the line')
            print(' o Non ASCII characters (use UTF8)')
            print(' o If text contains any of :[]{}% characters it must be single-quoted ' \
                  '(eg. value{1} should be \'value{1}\')\n')

            # Not very good practice but we get several kind of exceptions here, I'm not even sure all of them
            # At least: ReaderError, YmlScannerError (or something like that)
            if hasattr(e, 'problem') and hasattr(e, 'context_mark') and hasattr(e, 'problem_mark'):
                lines = 0
                if e.problem is not None:
                    print(' Reason: %s\n' % e.problem)
                    if e.problem == 'mapping values are not allowed here':
                        print(' ----> MOST LIKELY REASON: Missing : from end of the line!')
                        print('')
                if e.context_mark is not None:
                    print(' Check configuration near line %s, column %s' % (e.context_mark.line, e.context_mark.column))
                    lines += 1
                if e.problem_mark is not None:
                    print(' Check configuration near line %s, column %s' % (e.problem_mark.line, e.problem_mark.column))
                    lines += 1
                if lines:
                    print('')
                if lines == 1:
                    print(' Fault is almost always in this or previous line\n')
                if lines == 2:
                    print(' Fault is almost always in one of these lines or previous ones\n')

            # When --debug escalate to full stacktrace
            if self.options.debug:
                raise
            raise ValueError('Config file is not valid YAML')

        # config loaded successfully
        log.debug('config_name: %s' % self.config_name)
        log.debug('config_base: %s' % self.config_base)
        # Install the newly loaded config
        self.update_config(config)

    def update_config(self, config):
        """
        Provide a new config for the manager to use.

        :raises: `ValueError` and rolls back to previous config if the provided config is not valid.
        """
        old_config = self.config
        self.config = config
        errors = self.validate_config()
        if errors:
            for error in errors:
                log.critical("[%s] %s", error.json_pointer, error.message)
            log.debug('invalid config, rolling back')
            self.config = old_config
            raise ValueError('Config did not pass schema validation')
        log.debug('New config data loaded.')
        fire_event('manager.config_updated', self)

    def save_config(self):
        """Dumps current config to yaml config file"""
        # Back up the user's current config before overwriting
        backup_path = os.path.join(self.config_base,
                                   '%s-%s.bak' % (self.config_name, datetime.now().strftime('%y%m%d%H%M%S')))
        log.debug('backing up old config to %s before new save' % backup_path)
        shutil.copy(self.config_path, backup_path)
        with open(self.config_path, 'w') as config_file:
            config_file.write(yaml.dump(self.config, default_flow_style=False))

    def config_changed(self):
        """Makes sure that all tasks will have the config_modified flag come out true on the next run.
        Useful when changing the db and all tasks need to be completely reprocessed."""
        from flexget.task import config_changed
        for task in self.tasks:
            config_changed(task)

    def validate_config(self):
        """
        Check all root level keywords are valid.

        :returns: A list of `ValidationError`s
        """
        fire_event('manager.before_config_validate', self)
        return config_schema.process_config(self.config)

    def init_sqlalchemy(self):
        """Initialize SQLAlchemy"""
        try:
            if [int(part) for part in sqlalchemy.__version__.split('.')] < [0, 7, 0]:
                print('FATAL: SQLAlchemy 0.7.0 or newer required. Please upgrade your SQLAlchemy.', file=sys.stderr)
                sys.exit(1)
        except ValueError as e:
            log.critical('Failed to check SQLAlchemy version, you may need to upgrade it')

        # SQLAlchemy
        if self.database_uri is None:
            self.db_filename = os.path.join(self.config_base, 'db-%s.sqlite' % self.config_name)
            if self.options.test:
                db_test_filename = os.path.join(self.config_base, 'test-%s.sqlite' % self.config_name)
                log.info('Test mode, creating a copy from database ...')
                if os.path.exists(self.db_filename):
                    shutil.copy(self.db_filename, db_test_filename)
                self.db_filename = db_test_filename
                # Different database, different lock file
                self.lockfile = os.path.join(self.config_base, '.test-%s-lock' % self.config_name)
                log.info('Test database created')

            # in case running on windows, needs double \\
            filename = self.db_filename.replace('\\', '\\\\')
            self.database_uri = 'sqlite:///%s' % filename

        if self.db_filename and not os.path.exists(self.db_filename):
            log.verbose('Creating new database %s ...' % self.db_filename)

        # fire up the engine
        log.debug('Connecting to: %s' % self.database_uri)
        try:
            self.engine = sqlalchemy.create_engine(self.database_uri,
                                                   echo=self.options.debug_sql,
                                                   poolclass=SingletonThreadPool,
                                                   connect_args={'check_same_thread': False})  # assert_unicode=True
        except ImportError:
            print('FATAL: Unable to use SQLite. Are you running Python 2.5 - 2.7 ?\n'
                  'Python should normally have SQLite support built in.\n'
                  'If you\'re running correct version of Python then it is not equipped with SQLite.\n'
                  'You can try installing `pysqlite`. If you have compiled python yourself, '
                  'recompile it with SQLite support.', file=sys.stderr)
            sys.exit(1)
        Session.configure(bind=self.engine)
        # create all tables, doesn't do anything to existing tables
        try:
            def before_table_create(event, target, bind, tables=None, **kw):
                if tables:
                    # We need to acquire a lock if we are creating new tables
                    # TODO: Detect if any database upgrading is needed and acquire the lock only in one place
                    self.acquire_lock(event=False).__enter__()

            Base.metadata.append_ddl_listener('before-create', before_table_create)
            Base.metadata.create_all(bind=self.engine)
        except OperationalError as e:
            if os.path.exists(self.db_filename):
                print('%s - make sure you have write permissions to file %s' %
                      (e.message, self.db_filename), file=sys.stderr)
            else:
                print('%s - make sure you have write permissions to directory %s' %
                      (e.message, self.config_base), file=sys.stderr)
            raise

    def _read_lock(self):
        """
        Read the values from the lock file. Returns None if there is no current lock file.
        """
        if self.lockfile and os.path.exists(self.lockfile):
            result = {}
            with open(self.lockfile) as f:
                lines = [l for l in f.readlines() if l]
            for line in lines:
                try:
                    key, value = line.split(b':', 1)
                except ValueError:
                    log.debug('Invalid line in lock file: %s' % line)
                    continue
                result[key.strip().lower()] = value.strip()
            for key in result:
                if result[key].isdigit():
                    result[key] = int(result[key])
            result.setdefault('pid', None)
            if not result['pid']:
                log.error('Invalid lock file. Make sure FlexGet is not running, then delete it.')
            elif not pid_exists(result['pid']):
                return None
            return result
        return None

    def check_lock(self):
        """Returns True if there is a lock on the database."""
        lock_info = self._read_lock()
        if not lock_info:
            return False
        # Don't count it if we hold the lock
        if os.getpid() == lock_info['pid']:
            return False
        return True

    def check_ipc_info(self):
        """If a daemon has a lock on the database, return info to connect to IPC."""
        lock_info = self._read_lock()
        if lock_info and 'port' in lock_info:
            return lock_info
        return None

    @contextmanager
    def acquire_lock(self, event=True):
        """
        :param bool event: If True, the 'manager.lock_acquired' event will be fired after a lock is obtained
        """
        acquired = False
        try:
            # Don't do anything if we already have a lock. This means only the outermost call will release the lock file
            if not self._has_lock:
                # Exit if there is an existing lock.
                if self.check_lock():
                    with open(self.lockfile) as f:
                        pid = f.read()
                    print('Another process (%s) is running, will exit.' % pid.split('\n')[0], file=sys.stderr)
                    print('If you\'re sure there is no other instance running, delete %s' % self.lockfile,
                          file=sys.stderr)
                    sys.exit(1)

                self._has_lock = True
                self.write_lock()
                acquired = True
                if event:
                    fire_event('manager.lock_acquired', self)
            yield
        finally:
            if acquired:
                self.release_lock()
                self._has_lock = False

    def write_lock(self, ipc_info=None):
        assert self._has_lock
        with open(self.lockfile, 'w') as f:
            f.write(b'PID: %s\n' % os.getpid())
            if ipc_info:
                for key in sorted(ipc_info):
                    f.write(b'%s: %s\n' % (key, ipc_info[key]))

    def release_lock(self):
        if os.path.exists(self.lockfile):
            os.remove(self.lockfile)
            log.debug('Removed %s' % self.lockfile)
        else:
            log.debug('Lockfile %s not found' % self.lockfile)

    def daemonize(self):
        """Daemonizes the current process. Returns the new pid"""
        if sys.platform.startswith('win'):
            log.error('Cannot daemonize on windows')
            return
        if threading.activeCount() != 1:
            log.critical('There are %r active threads. '
                         'Daemonizing now may cause strange failures.' % threading.enumerate())

        log.info('Daemonizing...')

        try:
            pid = os.fork()
            if pid > 0:
                # Don't run the exit handlers on the parent
                atexit._exithandlers = []
                # exit first parent
                sys.exit(0)
        except OSError as e:
            sys.stderr.write('fork #1 failed: %d (%s)\n' % (e.errno, e.strerror))
            sys.exit(1)

        # decouple from parent environment
        os.chdir('/')
        os.setsid()
        os.umask(0)

        # do second fork
        try:
            pid = os.fork()
            if pid > 0:
                # Don't run the exit handlers on the parent
                atexit._exithandlers = []
                # exit from second parent
                sys.exit(0)
        except OSError as e:
            sys.stderr.write('fork #2 failed: %d (%s)\n' % (e.errno, e.strerror))
            sys.exit(1)

        log.info('Daemonize complete. New PID: %s' % os.getpid())
        # redirect standard file descriptors
        sys.stdout.flush()
        sys.stderr.flush()
        si = file('/dev/null', 'r')
        so = file('/dev/null', 'a+')
        se = file('/dev/null', 'a+', 0)
        os.dup2(si.fileno(), sys.stdin.fileno())
        os.dup2(so.fileno(), sys.stdout.fileno())
        os.dup2(se.fileno(), sys.stderr.fileno())

    def db_cleanup(self, force=False):
        """
        Perform database cleanup if cleanup interval has been met.

        Fires events:

        * manager.db_cleanup

          If interval was met. Gives session to do the cleanup as a parameter.

        :param bool force: Run the cleanup no matter whether the interval has been met.
        """
        expired = self.persist.get('last_cleanup', datetime(1900, 1, 1)) < datetime.now() - DB_CLEANUP_INTERVAL
        if force or expired:
            log.info('Running database cleanup.')
            session = Session()
            try:
                fire_event('manager.db_cleanup', session)
                session.commit()
            finally:
                session.close()
            # Just in case some plugin was overzealous in its cleaning, mark the config changed
            self.config_changed()
            self.persist['last_cleanup'] = datetime.now()
        else:
            log.debug('Not running db cleanup, last run %s' % self.persist.get('last_cleanup'))

    def shutdown(self, finish_queue=True):
        """
        Application is being exited

        :param bool finish_queue: Should scheduler finish the task queue
        """
        # Wait for scheduler to finish
        self.scheduler.shutdown(finish_queue=finish_queue)
        try:
            self.scheduler.wait()
        except KeyboardInterrupt:
            log.debug('Not waiting for scheduler shutdown due to ctrl-c')
            # show real stack trace in debug mode
            if manager.options.debug:
                raise
            print('**** Keyboard Interrupt ****')
        fire_event('manager.shutdown', self)
        if not self.unit_test:  # don't scroll "nosetests" summary results when logging is enabled
            log.debug('Shutting down')
        self.engine.dispose()
        # remove temporary database used in test mode
        if self.options.test:
            if not 'test' in self.db_filename:
                raise Exception('trying to delete non test database?')
            if self._has_lock:
                os.remove(self.db_filename)
                log.info('Removed test database')
        if not self.unit_test:  # don't scroll "nosetests" summary results when logging is enabled
            log.debug('Shutdown completed')

########NEW FILE########
__FILENAME__ = options
from __future__ import unicode_literals, division, absolute_import
import copy
import pkg_resources
import random
import socket
import string
import sys
from argparse import (ArgumentParser as ArgParser, Action, ArgumentError, SUPPRESS, PARSER, REMAINDER, _VersionAction,
                      Namespace, ArgumentTypeError)

import flexget
from flexget.utils.tools import console
from flexget.utils import requests
from flexget.entry import Entry
from flexget.event import fire_event


_UNSET = object()

core_parser = None


def get_parser(command=None):
    global core_parser
    if not core_parser:
        core_parser = CoreArgumentParser()
        # Add all plugin options to the parser
        fire_event('options.register')
    if command:
        return core_parser.get_subparser(command)
    return core_parser


def register_command(command, callback, **kwargs):
    """
    Register a callback function to be executed when flexget is launched with the given `command`.

    :param command: The command being defined.
    :param callback: Callback function executed when this command is invoked from the CLI. Should take manager instance
        and parsed argparse namespace as parameters.
    :param kwargs: Other keyword arguments will be passed to the :class:`arparse.ArgumentParser` constructor
    :returns: An :class:`argparse.ArgumentParser` instance ready to be configured with the options for this command.
    """
    subparser = get_parser().add_subparser(command, **kwargs)
    subparser.set_defaults(cli_command_callback=callback)
    return subparser


def required_length(nmin, nmax):
    """Generates a custom Action to validate an arbitrary range of arguments."""
    class RequiredLength(Action):
        def __call__(self, parser, args, values, option_string=None):
            if not nmin <= len(values) <= nmax:
                raise ArgumentError(self, 'requires between %s and %s arguments' % (nmin, nmax))
            setattr(args, self.dest, values)
    return RequiredLength


class VersionAction(_VersionAction):
    """
    Action to print the current version.
    Also attempts to get more information from git describe if on git checkout.
    """
    def __call__(self, parser, namespace, values, option_string=None):
        self.version = flexget.__version__
        # Print the version number
        console('%s' % self.version)
        if self.version == '{git}':
            console('To check the latest released version you have run:')
            console('`git fetch --tags` then `git describe`')
        else:
            # Check for latest version from server
            try:
                page = requests.get('http://download.flexget.com/latestversion')
            except requests.RequestException:
                console('Error getting latest version number from download.flexget.com')
            else:
                ver = page.text.strip()
                if self.version == ver:
                    console('You are on the latest release.')
                else:
                    console('Latest release: %s' % ver)
        parser.exit()


class DebugAction(Action):
    def __call__(self, parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, True)
        namespace.loglevel = 'debug'


class DebugTraceAction(Action):
    def __call__(self, parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, True)
        namespace.debug = True
        namespace.log_level = 'trace'


class CronAction(Action):
    def __call__(self, parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, True)
        # Only set loglevel if it has not already explicitly been set
        if not hasattr(namespace, 'loglevel'):
            namespace.loglevel = 'info'


# This makes the old --inject form forwards compatible
class InjectAction(Action):
    def __call__(self, parser, namespace, values, option_string=None):
        kwargs = {'title': values.pop(0)}
        if values:
            kwargs['url'] = values.pop(0)
        else:
            kwargs['url'] = 'http://localhost/inject/%s' % ''.join(random.sample(string.letters + string.digits, 30))
        if 'force' in [v.lower() for v in values]:
            kwargs['immortal'] = True
        entry = Entry(**kwargs)
        if 'accept' in [v.lower() for v in values]:
            entry.accept(reason='accepted by --inject')
        setattr(namespace, self.dest, [entry])


class ParseExtrasAction(Action):
    """This action will take extra arguments, and parser them with a different parser."""
    def __init__(self, option_strings, parser, help=None, metavar=None, dest=None, required=False):
        if metavar is None:
            metavar = '<%s arguments>' % parser.prog
        if help is None:
            help = 'arguments for the `%s` command are allowed here' % parser.prog
        self._parser = parser
        super(ParseExtrasAction, self).__init__(option_strings=option_strings, dest=SUPPRESS, help=help,
                                                metavar=metavar, nargs=REMAINDER, required=required)

    def __call__(self, parser, namespace, values, option_string=None):
        namespace, extras = self._parser.parse_known_args(values, namespace)
        if extras:
            parser.error('unrecognized arguments: %s' % ' '.join(extras))


class ScopedNamespace(Namespace):
    def __init__(self, **kwargs):
        super(ScopedNamespace, self).__init__(**kwargs)
        self.__parent__ = None

    def __getattr__(self, key):
        if '.' in key:
            scope, key = key.split('.', 1)
            return getattr(getattr(self, scope), key)

        if self.__parent__:
            return getattr(self.__parent__, key)
        raise AttributeError("'%s' object has no attribute '%s'" % (type(self).__name__, key))

    def __setattr__(self, key, value):
        if '.' in key:
            scope, key = key.split('.', 1)
            if not hasattr(self, scope):
                setattr(self, scope, type(self)())
            sub_ns = getattr(self, scope, None)
            return object.__setattr__(sub_ns, key, value)
        # Let child namespaces keep track of us
        if key != '__parent__' and isinstance(value, ScopedNamespace):
            value.__parent__ = self
        return object.__setattr__(self, key, value)

    def __iter__(self):
        return (i for i in self.__dict__.iteritems() if i[0] != '__parent__')

    def __copy__(self):
        new = self.__class__()
        new.__dict__.update(self.__dict__)
        # Make copies of any nested namespaces
        for key, value in self:
            if isinstance(value, ScopedNamespace):
                setattr(new, key, copy.copy(value))
        return new


class ParserError(Exception):
    def __init__(self, message, parser):
        self.message = message
        self.parser = parser

    def __unicode__(self):
        return self.message

    def __repr__(self):
        return 'ParserError(%s, %s)' % self.message, self.parser


class ArgumentParser(ArgParser):
    """
    Mimics the default :class:`argparse.ArgumentParser` class, with a few distinctions, mostly to ease subparser usage:

    - Adds the `add_subparser` method. After `add_subparsers` has been called, the `add_subparser` method can be used
      instead of the `add_parser` method of the object returned by the `add_subparsers` call.
    - If `add_subparser` is called with the `scoped_namespace` kwarg, all subcommand options will be stored in a
      nested namespace scope based on the command name for this subparser
    - The `get_subparser` method will get the :class:`ArgumentParser` instance for an existing subparser on this parser
    - For any arguments defined both in this parser and one of its subparsers, the selected subparser default will
      override the main one.
    - Command shortening: If the command for a subparser is abbreviated unambiguously, it will still be accepted.
    - The add_argument `nargs` keyword argument supports a range of arguments, e.g. `"2-4"
    - If the `raise_errors` keyword argument to `parse_args` is True, a `ValueError` will be raised instead of sys.exit
    """

    def __init__(self, nested_namespace_name=None, **kwargs):
        """
        :param nested_namespace_name: When used as a subparser, options from this parser will be stored nested under
            this attribute name in the root parser's namespace
        """
        # Do this early, so even option processing stuff is caught
        if '--bugreport' in sys.argv:
            self._debug_tb_callback()

        self.subparsers = None
        self.nested_namespace_name = nested_namespace_name
        self.raise_errors = None
        ArgParser.__init__(self, **kwargs)

    def add_argument(self, *args, **kwargs):
        if isinstance(kwargs.get('nargs'), basestring) and '-' in kwargs['nargs']:
            # Handle a custom range of arguments
            min, max = kwargs['nargs'].split('-')
            min, max = int(min), int(max)
            kwargs['action'] = required_length(min, max)
            # Make the usage string a bit better depending on whether the first argument is optional
            if min == 0:
                kwargs['nargs'] = '*'
            else:
                kwargs['nargs'] = '+'
        result = super(ArgumentParser, self).add_argument(*args, **kwargs)
        if self.nested_namespace_name:
            # If metavar hasn't already been set, set it without the nested scope name
            if not result.metavar:
                result.metavar = result.dest
                if result.option_strings and result.option_strings[0].startswith('-'):
                    result.metavar = result.dest.upper()
            result.dest = self.nested_namespace_name + '.' + result.dest
        return result

    def print_help(self, file=None):
        self.restore_defaults()
        super(ArgumentParser, self).print_help(file)

    def stash_defaults(self):
        """Remove all the defaults and store them in a temporary location."""
        self.real_defaults, self._defaults = self._defaults, {}
        for action in self._actions:
            action.real_default, action.default = action.default, SUPPRESS

    def restore_defaults(self):
        """Restore all stashed defaults."""
        if hasattr(self, 'real_defaults'):
            self._defaults = self.real_defaults
            del self.real_defaults
        for action in self._actions:
            if hasattr(action, 'real_default'):
                action.default = action.real_default
                del action.real_default

    def error(self, msg):
        raise ParserError(msg, self)

    def parse_args(self, args=None, namespace=None, raise_errors=False):
        """
        :param raise_errors: If this is true, errors will be raised as `ParserError`s instead of calling sys.exit
        """
        try:
            return super(ArgumentParser, self).parse_args(args, namespace)
        except ParserError as e:
            if raise_errors:
                raise
            sys.stderr.write('error: %s\n' % e.message)
            e.parser.print_help()
            sys.exit(2)

    def parse_known_args(self, args=None, namespace=None):
        if args is None:
            # Decode all arguments to unicode before parsing
            args = [unicode(arg, sys.getfilesystemencoding()) for arg in sys.argv[1:]]
        # Remove all of our defaults, to give subparsers and custom actions first priority at setting them
        self.stash_defaults()
        try:
            namespace, _ = super(ArgumentParser, self).parse_known_args(args, namespace or ScopedNamespace())
        except ParserError:
            pass
        finally:
            # Restore the defaults
            self.restore_defaults()
        # Parse again with subparser and custom action defaults already in the namespace
        return super(ArgumentParser, self).parse_known_args(args, namespace)

    def add_subparsers(self, scoped_namespaces=False, **kwargs):
        # Set the parser class so subparsers don't end up being an instance of a subclass, like CoreArgumentParser
        kwargs.setdefault('parser_class', ArgumentParser)
        self.subparsers = super(ArgumentParser, self).add_subparsers(**kwargs)
        self.subparsers.scoped_namespaces = scoped_namespaces
        return self.subparsers

    def add_subparser(self, name, **kwargs):
        """
        Adds a parser for a new subcommand and returns it.

        :param name: Name of the subcommand
        :param require_lock: Whether this subcommand should require a database lock
        """
        if not self.subparsers:
            raise TypeError('This parser does not have subparsers')
        if self.subparsers.scoped_namespaces:
            kwargs.setdefault('nested_namespace_name', name)
        result = self.subparsers.add_parser(name, **kwargs)
        if self.subparsers.scoped_namespaces:
            result.set_defaults(**{name: ScopedNamespace()})
        return result

    def get_subparser(self, name, default=_UNSET):
        if not self.subparsers:
            raise TypeError('This parser does not have subparsers')
        p = self.subparsers.choices.get(name, default)
        if p is _UNSET:
            raise ValueError('%s is not an existing subparser name' % name)
        return p

    def _get_values(self, action, arg_strings):
        """Complete the full name for partial subcommands"""
        if action.nargs == PARSER and self.subparsers:
            subcommand = arg_strings[0]
            if subcommand not in self.subparsers.choices:
                matches = [x for x in self.subparsers.choices if x.startswith(subcommand)]
                if len(matches) == 1:
                    arg_strings[0] = matches[0]
        return super(ArgumentParser, self)._get_values(action, arg_strings)

    def _debug_tb_callback(self, *dummy):
        import cgitb
        cgitb.enable(format="text")


# This will hold just the arguments directly for Manager. Webui needs this clean, to build its parser.
manager_parser = ArgumentParser(add_help=False)
manager_parser.add_argument('-V', '--version', action=VersionAction, help='Print FlexGet version and exit.')
manager_parser.add_argument('--test', action='store_true', dest='test', default=0,
                            help='Verbose what would happen on normal execution.')
manager_parser.add_argument('-c', dest='config', default='config.yml',
                            help='Specify configuration file. Default: %(default)s')
manager_parser.add_argument('--logfile', '-l', default='flexget.log',
                            help='Specify a custom logfile name/location. '
                                 'Default: %(default)s in the config directory.')
manager_parser.add_argument('--loglevel', '-L', metavar='LEVEL',
                            default='verbose',
                            help='Set the verbosity of the logger. Levels: %(choices)s',
                            choices=['none', 'critical', 'error', 'warning', 'info', 'verbose', 'debug', 'trace'])
# This option is already handled above.
manager_parser.add_argument('--bugreport', action='store_true', dest='debug_tb',
                            help='Use this option to create a detailed bug report, '
                                 'note that the output might contain PRIVATE data, so edit that out')
manager_parser.add_argument('--profile', metavar='OUTFILE', nargs='?', const='flexget.profile',
                            help='Use the python profiler for this run to debug performance issues.')
manager_parser.add_argument('--debug', action=DebugAction, nargs=0, help=SUPPRESS)
manager_parser.add_argument('--debug-trace', action=DebugTraceAction, nargs=0, help=SUPPRESS)
manager_parser.add_argument('--debug-sql', action='store_true', default=False, help=SUPPRESS)
manager_parser.add_argument('--experimental', action='store_true', default=False, help=SUPPRESS)
manager_parser.add_argument('--ipc-port', type=int, help=SUPPRESS)


class CoreArgumentParser(ArgumentParser):
    """
    The core argument parser, contains the manager arguments, command parsers, and plugin arguments.

    Warning: Only gets plugin arguments if instantiated after plugins have been loaded.

    """
    def __init__(self, **kwargs):
        kwargs.setdefault('parents', [manager_parser])
        kwargs.setdefault('prog', 'flexget')
        super(CoreArgumentParser, self).__init__(**kwargs)
        self.add_subparsers(title='commands', metavar='<command>', dest='cli_command', scoped_namespaces=True)

        # The parser for the execute command
        exec_parser = self.add_subparser('execute', help='execute tasks now')
        exec_parser.add_argument('--tasks', nargs='+', metavar='TASK',
                                 help='run only specified task(s), optionally using glob patterns ("tv-*"). '
                                      'matching is case-insensitive')
        exec_parser.add_argument('--learn', action='store_true', dest='learn', default=False,
                                 help='matches are not downloaded but will be skipped in the future')
        exec_parser.add_argument('--cron', action=CronAction, default=False, nargs=0,
                                 help='use when scheduling FlexGet with cron or other scheduler: allows background '
                                      'maintenance to run, disables stdout and stderr output, reduces logging level')
        exec_parser.add_argument('--profile', action='store_true', default=False, help=SUPPRESS)
        exec_parser.add_argument('--disable-phases', nargs='*', help=SUPPRESS)
        exec_parser.add_argument('--inject', nargs='+', action=InjectAction, help=SUPPRESS)
        # Plugins should respect these flags where appropriate
        exec_parser.add_argument('--retry', action='store_true', dest='retry', default=False, help=SUPPRESS)
        exec_parser.add_argument('--no-cache', action='store_true', dest='nocache', default=False,
                                 help='disable caches. works only in plugins that have explicit support')

        daemonize_help = SUPPRESS
        if not sys.platform.startswith('win'):
            daemonize_help = 'causes process to daemonize after starting'

        # The parser for the daemon command
        daemon_parser = self.add_subparser('daemon', help='run continuously, executing tasks according to schedules '
                                                          'defined in config')
        daemon_parser.add_subparsers(title='actions', metavar='<action>', dest='action')
        start_parser = daemon_parser.add_subparser('start', help='start the daemon')
        start_parser.add_argument('-d', '--daemonize', action='store_true', help=daemonize_help)
        daemon_parser.add_subparser('stop', help='shutdown the running daemon')
        daemon_parser.add_subparser('status', help='check if a daemon is running')
        daemon_parser.add_subparser('reload', help='causes a running daemon to reload the config from disk')
        daemon_parser.set_defaults(loglevel='info')

        # The parser for the webui
        # Hide the webui command if deps aren't available
        webui_kwargs = {}
        try:
            pkg_resources.require('flexget[webui]')
            webui_kwargs['help'] = 'run continuously, with a web interface to configure and interact with the daemon'
        except pkg_resources.DistributionNotFound:
            pass
        webui_parser = self.add_subparser('webui', **webui_kwargs)

        def ip_type(value):
            try:
                socket.inet_aton(value)
            except socket.error:
                raise ArgumentTypeError('must be a valid ip address to bind to')
            return value

        webui_parser.add_argument('--bind', type=ip_type, default='0.0.0.0', metavar='IP',
                                  help='IP address to bind to when serving the web interface [default: %(default)s]')
        webui_parser.add_argument('--port', type=int, default=5050,
                                  help='run FlexGet webui on port [default: %(default)s]')
        webui_parser.add_argument('-d', '--daemonize', action='store_true', help=daemonize_help)

        # TODO: move these to authentication plugin?
        webui_parser.add_argument('--no-auth', action='store_true',
                                  help='runs without authentication required (dangerous)')
        webui_parser.add_argument('--no-local-auth', action='store_true',
                                  help='runs without any authentication required when accessed from localhost')
        webui_parser.add_argument('--username', help='username needed to login [default: flexget]')
        webui_parser.add_argument('--password', help='password needed to login [default: flexget]')

        # enable flask autoreloading (development)
        webui_parser.add_argument('--autoreload', action='store_true', help=SUPPRESS)
        webui_parser.set_defaults(loglevel='info')

    def add_subparsers(self, **kwargs):
        # The subparsers should not be CoreArgumentParsers
        kwargs.setdefault('parser_class', ArgumentParser)
        return super(CoreArgumentParser, self).add_subparsers(**kwargs)

    def parse_args(self, *args, **kwargs):
        result = super(CoreArgumentParser, self).parse_args(*args, **kwargs)
        # Make sure we always have execute parser settings even when other commands called
        if not result.cli_command == 'execute':
            exec_options = get_parser('execute').parse_args([]).execute
            if hasattr(result, 'execute'):
                exec_options.__dict__.update(result.execute.__dict__)
            result.execute = exec_options
        return result

########NEW FILE########
__FILENAME__ = plugin
""" Plugin Loading & Management.
"""

from __future__ import unicode_literals, division, absolute_import
import sys
import os
import re
import logging
import time
import pkgutil
import warnings
from itertools import ifilter

from requests import RequestException

from flexget import config_schema
from flexget.event import add_event_handler as add_phase_handler, fire_event, remove_event_handlers
from flexget import plugins as plugins_pkg

log = logging.getLogger('plugin')

__all__ = ['PluginWarning', 'PluginError', 'register_plugin', 'register_parser_option', 'register_task_phase',
           'get_plugin_by_name', 'get_plugins_by_group', 'get_plugin_keywords', 'get_plugins_by_phase',
           'get_phases_by_plugin', 'internet', 'priority']


class DependencyError(Exception):
    """Plugin depends on other plugin, but it cannot be loaded.

    Args:
        issued_by: name of the plugin trying to do the import
        missing: name of the plugin or library that is missing
        message: user readable error message

    All args are optional.
    """

    def __init__(self, issued_by=None, missing=None, message=None, silent=False):
        super(DependencyError, self).__init__()
        self.issued_by = issued_by
        self.missing = missing
        self._message = message
        self.silent = silent

    def _get_message(self):
        if self._message:
            return self._message
        else:
            return 'Plugin `%s` requires dependency `%s`' % (self.issued_by, self.missing)

    def _set_message(self, message):
        self._message = message

    def has_message(self):
        return self._message is not None

    message = property(_get_message, _set_message)

    def __str__(self):
        return '<DependencyError(issued_by=%r,missing=%r,message=%r,silent=%r)>' % \
            (self.issued_by, self.missing, self.message, self.silent)


class RegisterException(Exception):

    def __init__(self, value):
        super(RegisterException, self).__init__()
        self.value = value

    def __str__(self):
        return repr(self.value)


class PluginWarning(Warning):

    def __init__(self, value, logger=log, **kwargs):
        super(PluginWarning, self).__init__()
        self.value = value
        self.log = logger
        self.kwargs = kwargs

    def __str__(self):
        return unicode(self).encode('utf-8')

    def __unicode__(self):
        return self.value


class PluginError(Exception):

    def __init__(self, value, logger=log, **kwargs):
        super(PluginError, self).__init__()
        # Value is expected to be a string
        if not isinstance(value, basestring):
            value = unicode(value)
        self.value = value
        self.log = logger
        self.kwargs = kwargs

    def __str__(self):
        return unicode(self).encode('utf-8')

    def __unicode__(self):
        return unicode(self.value)


# TODO: move to utils or somewhere more appropriate
class internet(object):
    """@internet decorator for plugin phase methods.

    Catches all internet related exceptions and raises PluginError with relevant message.
    Task handles PluginErrors by aborting the task.
    """

    def __init__(self, logger=None):
        if logger:
            self.log = logger
        else:
            self.log = logging.getLogger('@internet')

    def __call__(self, func):

        def wrapped_func(*args, **kwargs):
            from httplib import BadStatusLine
            import urllib2
            try:
                return func(*args, **kwargs)
            except RequestException as e:
                log.debug('decorator caught RequestException. handled traceback:', exc_info=True)
                raise PluginError('RequestException: %s' % e)
            except urllib2.HTTPError as e:
                raise PluginError('HTTPError %s' % e.code, self.log)
            except urllib2.URLError as e:
                log.debug('decorator caught urlerror. handled traceback:', exc_info=True)
                raise PluginError('URLError %s' % e.reason, self.log)
            except BadStatusLine:
                log.debug('decorator caught badstatusline. handled traceback:', exc_info=True)
                raise PluginError('Got BadStatusLine', self.log)
            except ValueError as e:
                log.debug('decorator caught ValueError. handled traceback:', exc_info=True)
                raise PluginError(e)
            except IOError as e:
                log.debug('decorator caught ioerror. handled traceback:', exc_info=True)
                if hasattr(e, 'reason'):
                    raise PluginError('Failed to reach server. Reason: %s' % e.reason, self.log)
                elif hasattr(e, 'code'):
                    raise PluginError('The server couldn\'t fulfill the request. Error code: %s' % e.code, self.log)
                raise PluginError('IOError when connecting to server: %s' % e, self.log)
        return wrapped_func


def priority(value):
    """Priority decorator for phase methods"""

    def decorator(target):
        target.priority = value
        return target
    return decorator

DEFAULT_PRIORITY = 128

plugin_contexts = ['task', 'root']

# task phases, in order of their execution; note that this can be extended by
# registering new phases at runtime
task_phases = ['start', 'input', 'metainfo', 'filter', 'download', 'modify', 'output', 'learn', 'exit']

# map phase names to method names
phase_methods = {
    # task
    'abort': 'on_task_abort'  # special; not a task phase that gets called normally
}
phase_methods.update((_phase, 'on_task_' + _phase) for _phase in task_phases)  # DRY

# Mapping of plugin name to PluginInfo instance (logical singletons)
plugins = {}

# Loading done?
plugins_loaded = False

_loaded_plugins = {}
_plugin_options = []
_new_phase_queue = {}


def register_task_phase(name, before=None, after=None):
    """Adds a new task phase to the available phases."""
    if before and after:
        raise RegisterException('You can only give either before or after for a phase.')
    if not before and not after:
        raise RegisterException('You must specify either a before or after phase.')
    if name in task_phases or name in _new_phase_queue:
        raise RegisterException('Phase %s already exists.' % name)

    def add_phase(phase_name, before, after):
        if not before is None and not before in task_phases:
            return False
        if not after is None and not after in task_phases:
            return False
        # add method name to phase -> method lookup table
        phase_methods[phase_name] = 'on_task_' + phase_name
        # place phase in phase list
        if before is None:
            task_phases.insert(task_phases.index(after) + 1, phase_name)
        if after is None:
            task_phases.insert(task_phases.index(before), phase_name)
        return True

    # if can't add yet (dependencies) queue addition
    if not add_phase(name, before, after):
        _new_phase_queue[name] = [before, after]

    for phase_name, args in _new_phase_queue.items():
        if add_phase(phase_name, *args):
            del _new_phase_queue[phase_name]


class PluginInfo(dict):
    """
    Allows accessing key/value pairs of this dictionary subclass via
    attributes. Also instantiates a plugin and initializes properties.
    """
    # Counts duplicate registrations
    dupe_counter = 0

    def __init__(self, plugin_class, name=None, groups=None, builtin=False, debug=False, api_ver=1,
                 contexts=None, category=None):
        """
        Register a plugin.

        :param plugin_class: The plugin factory.
        :param string name: Name of the plugin (if not given, default to factory class name in underscore form).
        :param list groups: Groups this plugin belongs to.
        :param bool builtin: Auto-activated?
        :param bool debug: True if plugin is for debugging purposes.
        :param int api_ver: Signature of callback hooks (1=task; 2=task,config).
        :param list contexts: List of where this plugin is configurable. Can be 'task', 'root', or None
        :param string category: The type of plugin. Can be one of the task phases.
            Defaults to the package name containing the plugin.
        """
        dict.__init__(self)

        if groups is None:
            groups = []
        if name is None:
            # Convention is to take camel-case class name and rewrite it to an underscore form,
            # e.g. 'PluginName' to 'plugin_name'
            name = re.sub('[A-Z]+', lambda i: '_' + i.group(0).lower(), plugin_class.__name__).lstrip('_')
        if contexts is None:
            contexts = ['task']
        elif isinstance(contexts, basestring):
            contexts = [contexts]
        if category is None and plugin_class.__module__.startswith('flexget.plugins'):
            # By default look at the containing package of the plugin.
            category = plugin_class.__module__.split('.')[-2]

        # Check for unsupported api versions
        if api_ver < 2:
            warnings.warn('Api versions <2 are no longer supported. Plugin %s' % name, DeprecationWarning, stacklevel=2)

        # Set basic info attributes
        self.api_ver = api_ver
        self.name = name
        self.groups = groups
        self.builtin = builtin
        self.debug = debug
        self.contexts = contexts
        self.category = category
        self.phase_handlers = {}

        self.plugin_class = plugin_class
        self.instance = None

        if self.name in plugins:
            PluginInfo.dupe_counter += 1
            log.critical('Error while registering plugin %s. A plugin with the same name is already registered' %
                         self.name)
        else:
            plugins[self.name] = self

    def initialize(self):
        if self.instance is not None:
            # We already initialized
            return
        # Create plugin instance
        self.instance = self.plugin_class()
        self.instance.plugin_info = self  # give plugin easy access to its own info
        self.instance.log = logging.getLogger(getattr(self.instance, "LOGGER_NAME", None) or self.name)
        if hasattr(self.instance, 'schema'):
            self.schema = self.instance.schema
        elif hasattr(self.instance, 'validator'):
            self.schema = self.instance.validator().schema()
        else:
            # TODO: I think plugins without schemas should not be allowed in config, maybe rethink this
            self.schema = {}

        if self.schema is not None:
            location = '/schema/plugin/%s' % self.name
            self.schema['id'] = location
            config_schema.register_schema(location, self.schema)

        self.build_phase_handlers()

    def reset_phase_handlers(self):
        """Temporary utility method"""
        self.phase_handlers = {}
        self.build_phase_handlers()
        # TODO: should unregister events (from flexget.event)
        # this method is not used at the moment anywhere ...
        raise NotImplementedError

    def build_phase_handlers(self):
        """(Re)build phase_handlers in this plugin"""
        for phase, method_name in phase_methods.iteritems():
            if phase in self.phase_handlers:
                continue
            if hasattr(self.instance, method_name):
                method = getattr(self.instance, method_name)
                if not callable(method):
                    continue
                # check for priority decorator
                if hasattr(method, 'priority'):
                    handler_prio = method.priority
                else:
                    handler_prio = DEFAULT_PRIORITY
                event = add_phase_handler('plugin.%s.%s' % (self.name, phase), method, handler_prio)
                # provides backwards compatibility
                event.plugin = self
                self.phase_handlers[phase] = event

    def __getattr__(self, attr):
        if attr in self:
            return self[attr]
        return dict.__getattribute__(self, attr)

    def __setattr__(self, attr, value):
        self[attr] = value

    def __str__(self):
        return '<PluginInfo(name=%s)>' % self.name

    __repr__ = __str__


register = PluginInfo


def _strip_trailing_sep(path):
    return path.rstrip("\\/")


def _get_standard_plugins_path():
    """
    :returns: List of directories where plugins should be tried to load from.
    """
    # Get basic path from environment
    env_path = os.environ.get('FLEXGET_PLUGIN_PATH')
    if env_path:
        # Get rid of trailing slashes, since Python can't handle them when
        # it tries to import modules.
        paths = map(_strip_trailing_sep, env_path.split(os.pathsep))
    else:
        # Use standard default
        paths = [os.path.join(os.path.expanduser('~'), '.flexget', 'plugins')]

    # Add flexget.plugins directory (core plugins)
    paths.append(os.path.abspath(os.path.dirname(plugins_pkg.__file__)))
    return paths


def _load_plugins_from_dirs(dirs):
    """
    :param list dirs: Directories from where plugins are loaded from
    """

    log.debug('Trying to load plugins from: %s' % dirs)
    # add all dirs to plugins_pkg load path so that plugins are loaded from flexget and from ~/.flexget/plugins/
    plugins_pkg.__path__ = map(_strip_trailing_sep, dirs)
    for importer, name, ispkg in pkgutil.walk_packages(dirs, plugins_pkg.__name__ + '.'):
        if ispkg:
            continue
        # Don't load any plugins again if they are already loaded
        # This can happen if one plugin imports from another plugin
        if name in sys.modules:
            continue
        loader = importer.find_module(name)
        # Don't load from pyc files
        if not loader.filename.endswith('.py'):
            continue
        try:
            loaded_module = loader.load_module(name)
        except DependencyError as e:
            if e.has_message():
                msg = e.message
            else:
                msg = 'Plugin `%s` requires `%s` to load.' % (e.issued_by or name, e.missing or 'N/A')
            if not e.silent:
                log.warning(msg)
            else:
                log.debug(msg)
        except ImportError as e:
            log.critical('Plugin `%s` failed to import dependencies' % name)
            log.exception(e)
        except Exception as e:
            log.critical('Exception while loading plugin %s' % name)
            log.exception(e)
            raise
        else:
            log.trace('Loaded module %s from %s' % (name, loaded_module.__file__))

    if _new_phase_queue:
        for phase, args in _new_phase_queue.iteritems():
            log.error('Plugin %s requested new phase %s, but it could not be created at requested '
                      'point (before, after). Plugin is not working properly.' % (args[0], phase))


def load_plugins():
    """Load plugins from the standard plugin paths."""
    global plugins_loaded

    start_time = time.time()
    # Import all the plugins
    _load_plugins_from_dirs(_get_standard_plugins_path())
    # Register them
    fire_event('plugin.register')
    # Plugins should only be registered once, remove their handlers after
    remove_event_handlers('plugin.register')
    # After they have all been registered, instantiate them
    for plugin in plugins.values():
        plugin.initialize()
    took = time.time() - start_time
    plugins_loaded = True
    log.debug('Plugins took %.2f seconds to load' % took)


def get_plugins(phase=None, group=None, context=None, category=None, min_api=None):
    """
    Query other plugins characteristics.

    :param string phase: Require phase
    :param string group: Plugin must belong to this group.
    :param string context: Where plugin is configured, eg. (root, task)
    :param string category: Type of plugin, phase names.
    :param int min_api: Minimum api version.
    :return: List of PluginInfo instances.
    :rtype: list
    """
    def matches(plugin):
        if phase is not None and phase not in phase_methods:
            raise ValueError('Unknown phase %s' % phase)
        if phase and not phase in plugin.phase_handlers:
            return False
        if group and not group in plugin.groups:
            return False
        if context and not context in plugin.contexts:
            return False
        if category and not category == plugin.category:
            return False
        if min_api is not None and plugin.api_ver < min_api:
            return False
        return True
    return ifilter(matches, plugins.itervalues())


def plugin_schemas(**kwargs):
    """Create a dict schema that matches plugins specified by `kwargs`"""
    return {'type': 'object',
            'properties': dict((p.name, {'$ref': p.schema['id']}) for p in get_plugins(**kwargs)),
            'additionalProperties': False,
            'error_additionalProperties': '{{message}} Only known plugin names are valid keys.',
            'patternProperties': {'^_': {'title': 'Disabled Plugin'}}}


config_schema.register_schema('/schema/plugins', plugin_schemas)


def get_plugins_by_phase(phase):
    """
    .. deprecated:: 1.0.3328
       Use :func:`get_plugins` instead

    Return an iterator over all plugins that hook :phase:
    """
    warnings.warn('Deprecated API', DeprecationWarning, stacklevel=2)
    if not phase in phase_methods:
        raise Exception('Unknown phase %s' % phase)
    return get_plugins(phase=phase)


def get_phases_by_plugin(name):
    """Return all phases plugin :name: hooks"""
    return list(get_plugin_by_name(name).phase_handlers)


def get_plugins_by_group(group):
    """
    .. deprecated:: 1.0.3328
       Use :func:`get_plugins` instead

    Return an iterator over all plugins with in specified group.
    """
    warnings.warn('Deprecated API', DeprecationWarning, stacklevel=2)
    return get_plugins(group=group)


def get_plugin_keywords():
    """Return iterator over all plugin keywords."""
    return plugins.iterkeys()


def get_plugin_by_name(name, issued_by='???'):
    """Get plugin by name, preferred way since this structure may be changed at some point."""
    if not name in plugins:
        raise DependencyError(issued_by=issued_by, missing=name, message='Unknown plugin %s' % name)
    return plugins[name]

########NEW FILE########
__FILENAME__ = api_rottentomatoes
from __future__ import unicode_literals, division, absolute_import
import time
import logging
import difflib
import random
from datetime import datetime, timedelta
from urllib2 import URLError

from sqlalchemy import Table, Column, Integer, String, DateTime, func, sql
from sqlalchemy.schema import ForeignKey, Index
from sqlalchemy.orm import relation

from flexget import db_schema
from flexget.plugin import internet, PluginError
from flexget.utils import requests
from flexget.utils.titles import MovieParser
from flexget.utils.database import text_date_synonym, with_session
from flexget.utils.sqlalchemy_utils import table_schema, table_add_column

log = logging.getLogger('api_rottentomatoes')
Base = db_schema.versioned_base('api_rottentomatoes', 2)
session = requests.Session()
# There is a 5 call per second rate limit per api key with multiple users on the same api key, this can be problematic
session.set_domain_delay('api.rottentomatoes.com', '0.4 seconds')

# This is developer Atlanta800's API key
API_KEY = 'rh8chjzp8vu6gnpwj88736uv'
API_VER = 'v1.0'
SERVER = 'http://api.rottentomatoes.com/api/public'

MIN_MATCH = 0.5
MIN_DIFF = 0.01


@db_schema.upgrade('api_rottentomatoes')
def upgrade(ver, session):
    if ver is 0:
        table_names = ['rottentomatoes_actors', 'rottentomatoes_alternate_ids',
                       'rottentomatoes_directors', 'rottentomatoes_genres', 'rottentomatoes_links',
                       'rottentomatoes_movie_actors', 'rottentomatoes_movie_directors',
                       'rottentomatoes_movie_genres', 'rottentomatoes_movies', 'rottentomatoes_posters',
                       'rottentomatoes_releasedates', 'rottentomatoes_search_results']
        tables = [table_schema(name, session) for name in table_names]
        for table in tables:
            session.execute(table.delete())
        table_add_column('rottentomatoes_actors', 'rt_id', String, session)
        ver = 1
    if ver is 1:
        table = table_schema('rottentomatoes_search_results', session)
        session.execute(sql.delete(table, table.c.movie_id == None))
        ver = 2
    return ver


# association tables
genres_table = Table('rottentomatoes_movie_genres', Base.metadata,
    Column('movie_id', Integer, ForeignKey('rottentomatoes_movies.id')),
    Column('genre_id', Integer, ForeignKey('rottentomatoes_genres.id')),
    Index('ix_rottentomatoes_movie_genres', 'movie_id', 'genre_id'))

actors_table = Table('rottentomatoes_movie_actors', Base.metadata,
    Column('movie_id', Integer, ForeignKey('rottentomatoes_movies.id')),
    Column('actor_id', Integer, ForeignKey('rottentomatoes_actors.id')),
    Index('ix_rottentomatoes_movie_actors', 'movie_id', 'actor_id'))

directors_table = Table('rottentomatoes_movie_directors', Base.metadata,
    Column('movie_id', Integer, ForeignKey('rottentomatoes_movies.id')),
    Column('director_id', Integer, ForeignKey('rottentomatoes_directors.id')),
    Index('ix_rottentomatoes_movie_directors', 'movie_id', 'director_id'))


# TODO: get rid of
class RottenTomatoesContainer(object):
    """Base class for RottenTomatoes objects"""

    def __init__(self, init_dict=None):
        if isinstance(init_dict, dict):
            self.update_from_dict(init_dict)

    def update_from_dict(self, update_dict):
        """Populates any simple (string or number) attributes from a dict"""
        for col in self.__table__.columns:
            if isinstance(update_dict.get(col.name), (basestring, int, float)):
                setattr(self, col.name, update_dict[col.name])


class RottenTomatoesMovie(RottenTomatoesContainer, Base):

    __tablename__ = 'rottentomatoes_movies'

    id = Column(Integer, primary_key=True, autoincrement=False, nullable=False)
    title = Column(String)
    year = Column(Integer)
    genres = relation('RottenTomatoesGenre', secondary=genres_table, backref='movies')
    mpaa_rating = Column(String)
    runtime = Column(Integer)
    critics_consensus = Column(String)
    release_dates = relation('ReleaseDate', backref='movie', cascade='all, delete, delete-orphan')
    critics_rating = Column(String)
    critics_score = Column(Integer)
    audience_rating = Column(String)
    audience_score = Column(Integer)
    synopsis = Column(String)
    posters = relation('RottenTomatoesPoster', backref='movie', cascade='all, delete, delete-orphan')
    cast = relation('RottenTomatoesActor', secondary=actors_table, backref='movies')
    directors = relation('RottenTomatoesDirector', secondary=directors_table, backref='movies')
    studio = Column(String)
    # NOTE: alternate_ids is not anymore used, it used to store imdb_id
    alternate_ids = relation('RottenTomatoesAlternateId', backref='movie', cascade='all, delete, delete-orphan')
    links = relation('RottenTomatoesLink', backref='movie', cascade='all, delete, delete-orphan')

    # updated time, so we can grab new rating counts after 48 hours
    # set a default, so existing data gets updated with a rating
    updated = Column(DateTime)

    @property
    def expired(self):
        """
        :return: True if movie details are considered to be expired, ie. need of update
        """
        if self.updated is None:
            log.debug('updated is None: %s' % self)
            return True
        refresh_interval = 2
        if self.year:
            age = (datetime.now().year - self.year)
            refresh_interval += age * 5
            log.debug('movie `%s` age %i expires in %i days' % (self.title, age, refresh_interval))
        return self.updated < datetime.now() - timedelta(days=refresh_interval)

    def __repr__(self):
        return '<RottenTomatoesMovie(title=%s,id=%s,year=%s)>' % (self.title, self.id, self.year)


class RottenTomatoesGenre(Base):

    __tablename__ = 'rottentomatoes_genres'

    id = Column(Integer, primary_key=True)
    name = Column(String)

    def __init__(self, name):
        self.name = name


class ReleaseDate(Base):

    __tablename__ = 'rottentomatoes_releasedates'

    db_id = Column(Integer, primary_key=True)
    movie_id = Column(Integer, ForeignKey('rottentomatoes_movies.id'))
    name = Column(String)
    date = text_date_synonym('_date')
    _date = Column('date', DateTime)

    def __init__(self, name, date):
        self.name = name
        self.date = date


class RottenTomatoesPoster(Base):

    __tablename__ = 'rottentomatoes_posters'

    db_id = Column(Integer, primary_key=True)
    movie_id = Column(Integer, ForeignKey('rottentomatoes_movies.id'))
    name = Column(String)
    url = Column(String)

    def __init__(self, name, url):
        self.name = name
        self.url = url


class RottenTomatoesActor(Base):

    __tablename__ = 'rottentomatoes_actors'

    id = Column(Integer, primary_key=True)
    rt_id = Column(String)
    name = Column(String)

    def __init__(self, name, rt_id):
        self.name = name
        self.rt_id = rt_id


class RottenTomatoesDirector(Base):

    __tablename__ = 'rottentomatoes_directors'

    id = Column(Integer, primary_key=True)
    name = Column(String)

    def __init__(self, name):
        self.name = name


class RottenTomatoesAlternateId(Base):

    __tablename__ = 'rottentomatoes_alternate_ids'

    db_id = Column(Integer, primary_key=True)
    movie_id = Column(Integer, ForeignKey('rottentomatoes_movies.id'))
    name = Column(String)
    id = Column(String)

    def __init__(self, name, id):
        self.name = name
        self.id = id


class RottenTomatoesLink(Base):

    __tablename__ = 'rottentomatoes_links'

    db_id = Column(Integer, primary_key=True)
    movie_id = Column(Integer, ForeignKey('rottentomatoes_movies.id'))
    name = Column(String)
    url = Column(String)

    def __init__(self, name, url):
        self.name = name
        self.url = url


class RottenTomatoesSearchResult(Base):

    __tablename__ = 'rottentomatoes_search_results'

    id = Column(Integer, primary_key=True)
    search = Column(String, nullable=False)
    movie_id = Column(Integer, ForeignKey('rottentomatoes_movies.id'), nullable=False)
    movie = relation(RottenTomatoesMovie, backref='search_strings')

    def __repr__(self):
        return '<RottenTomatoesSearchResult(search=%s,movie_id=%s,movie=%s)>' % (self.search, self.movie_id, self.movie)


@internet(log)
@with_session
def lookup_movie(title=None, year=None, rottentomatoes_id=None, smart_match=None,
                 only_cached=False, session=None, api_key=None):
    """
    Do a lookup from Rotten Tomatoes for the movie matching the passed arguments.
    Any combination of criteria can be passed, the most specific criteria specified will be used.

    :param rottentomatoes_id: rottentomatoes_id of desired movie
    :param string title: title of desired movie
    :param year: release year of desired movie
    :param smart_match: attempt to clean and parse title and year from a string
    :param only_cached: if this is specified, an online lookup will not occur if the movie is not in the cache
    :param session: optionally specify a session to use, if specified, returned Movie will be live in that session
    :param api_key: optionaly specify an API key to use
    :returns: The Movie object populated with data from Rotten Tomatoes
    :raises: PluginError if a match cannot be found or there are other problems with the lookup

    """

    if smart_match:
        # If smart_match was specified, and we don't have more specific criteria, parse it into a title and year
        title_parser = MovieParser()
        title_parser.parse(smart_match)
        title = title_parser.name
        year = title_parser.year
        if title == '' and not (rottentomatoes_id or title):
            raise PluginError('Failed to parse name from %s' % smart_match)

    if title:
        search_string = title.lower()
        if year:
            search_string = '%s %s' % (search_string, year)
    elif not rottentomatoes_id:
        raise PluginError('No criteria specified for rotten tomatoes lookup')

    def id_str():
        return '<title=%s,year=%s,rottentomatoes_id=%s>' % (title, year, rottentomatoes_id)

    log.debug('Looking up rotten tomatoes information for %s' % id_str())

    movie = None

    # Try to lookup from cache
    if rottentomatoes_id:
        movie = session.query(RottenTomatoesMovie).\
            filter(RottenTomatoesMovie.id == rottentomatoes_id).first()
    if not movie and title:
        movie_filter = session.query(RottenTomatoesMovie).filter(func.lower(RottenTomatoesMovie.title) == title.lower())
        if year:
            movie_filter = movie_filter.filter(RottenTomatoesMovie.year == year)
        movie = movie_filter.first()
        if not movie:
            log.debug('No matches in movie cache found, checking search cache.')
            found = session.query(RottenTomatoesSearchResult).\
                filter(func.lower(RottenTomatoesSearchResult.search) == search_string).first()
            if found and found.movie:
                log.debug('Movie found in search cache.')
                movie = found.movie
    if movie:
        # Movie found in cache, check if cache has expired.
        if movie.expired and not only_cached:
            log.debug('Cache has expired for %s, attempting to refresh from Rotten Tomatoes.' % id_str())
            try:
                result = movies_info(movie.id, api_key)
                movie = _set_movie_details(movie, session, result, api_key)
                session.merge(movie)
            except URLError:
                log.error('Error refreshing movie details from Rotten Tomatoes, cached info being used.')
        else:
            log.debug('Movie %s information restored from cache.' % id_str())
    else:
        if only_cached:
            raise PluginError('Movie %s not found from cache' % id_str())
        # There was no movie found in the cache, do a lookup from Rotten Tomatoes
        log.debug('Movie %s not found in cache, looking up from rotten tomatoes.' % id_str())
        try:
            if not movie and rottentomatoes_id:
                result = movies_info(rottentomatoes_id, api_key)
                if result:
                    movie = RottenTomatoesMovie()
                    movie = _set_movie_details(movie, session, result, api_key)
                    session.add(movie)

            if not movie and title:
                # TODO: Extract to method
                log.verbose('Searching from rt `%s`' % search_string)
                results = movies_search(search_string, api_key=api_key)
                if results:
                    results = results.get('movies')
                    if results:
                        for movie_res in results:
                            seq = difflib.SequenceMatcher(
                                lambda x: x == ' ', movie_res['title'].lower(), title.lower())
                            movie_res['match'] = seq.ratio()
                        results.sort(key=lambda x: x['match'], reverse=True)

                        # Remove all movies below MIN_MATCH, and different year
                        for movie_res in results[:]:

                            if year and movie_res.get('year'):
                                movie_res['year'] = int(movie_res['year'])
                                if movie_res['year'] != year:
                                    release_year = False
                                    if movie_res.get('release_dates', {}).get('theater'):
                                        log.debug('Checking year against theater release date')
                                        release_year = time.strptime(movie_res['release_dates'].get('theater'),
                                                                     '%Y-%m-%d').tm_year
                                    elif movie_res.get('release_dates', {}).get('dvd'):
                                        log.debug('Checking year against dvd release date')
                                        release_year = time.strptime(movie_res['release_dates'].get('dvd'),
                                                                     '%Y-%m-%d').tm_year
                                    if not (release_year and release_year == year):
                                        log.debug('removing %s - %s (wrong year: %s)' %
                                                  (movie_res['title'], movie_res['id'],
                                                   str(release_year or movie_res['year'])))
                                        results.remove(movie_res)
                                        continue
                            if movie_res['match'] < MIN_MATCH:
                                log.debug('removing %s (min_match)' % movie_res['title'])
                                results.remove(movie_res)
                                continue

                        if not results:
                            raise PluginError('no appropiate results')

                        if len(results) == 1:
                            log.debug('SUCCESS: only one movie remains')
                        else:
                            # Check min difference between best two hits
                            diff = results[0]['match'] - results[1]['match']
                            if diff < MIN_DIFF:
                                log.debug('unable to determine correct movie, min_diff too small'
                                          '(`%s (%s) - %s` <-?-> `%s (%s) - %s`)' %
                                          (results[0]['title'], results[0]['year'], results[0]['id'],
                                           results[1]['title'], results[1]['year'], results[1]['id']))
                                for r in results:
                                    log.debug('remain: %s (match: %s) %s' % (r['title'], r['match'], r['id']))
                                raise PluginError('min_diff')

                        result = movies_info(results[0].get('id'), api_key)

                        if not result:
                            result = results[0]

                        movie = session.query(RottenTomatoesMovie).filter(
                            RottenTomatoesMovie.id == result['id']).first()

                        if not movie:
                            movie = RottenTomatoesMovie()
                            movie = _set_movie_details(movie, session, result, api_key)
                            session.add(movie)
                            session.commit()


                        if title.lower() != movie.title.lower():
                            log.debug('Saving search result for \'%s\'' % search_string)
                            session.add(RottenTomatoesSearchResult(search=search_string, movie=movie))
        except URLError:
            raise PluginError('Error looking up movie from RottenTomatoes')

    if not movie:
        raise PluginError('No results found from rotten tomatoes for %s' % id_str())
    else:
        # Access attributes to force the relationships to eager load before we detach from session
        for attr in ['alternate_ids', 'cast', 'directors', 'genres', 'links', 'posters', 'release_dates']:
            getattr(movie, attr)
        session.commit()
        return movie


# TODO: get rid of or heavily refactor
def _set_movie_details(movie, session, movie_data=None, api_key=None):
    """
    Populate ``movie`` object from given data

    :param movie: movie object to update
    :param session: session to use, returned Movie will be live in that session
    :param api_key: optionally specify an API key to use
    :param movie_data: data to copy into the :movie:
    """

    if not movie_data:
        if not movie.id:
            raise PluginError('Cannot get rotten tomatoes details without rotten tomatoes id')
        movie_data = movies_info(movie.id, api_key)
    if movie_data:
        if movie.id:
            log.debug("Updating movie info (actually just deleting the old info and adding the new)")
            session.delete(movie)
            session.flush()
            movie = RottenTomatoesMovie()
        movie.update_from_dict(movie_data)
        movie.update_from_dict(movie_data.get('ratings'))
        genres = movie_data.get('genres')
        if genres:
            for name in genres:
                genre = session.query(RottenTomatoesGenre).filter(
                    func.lower(RottenTomatoesGenre.name) == name.lower()).first()
                if not genre:
                    genre = RottenTomatoesGenre(name)
                movie.genres.append(genre)
        release_dates = movie_data.get('release_dates')
        if release_dates:
            for name, date in release_dates.items():
                movie.release_dates.append(ReleaseDate(name, date))
        posters = movie_data.get('posters')
        if posters:
            for name, url in posters.items():
                movie.posters.append(RottenTomatoesPoster(name, url))
        cast = movie_data.get('abridged_cast')
        if cast:
            for res_actor in cast:
                actor = session.query(RottenTomatoesActor).filter(
                    func.lower(RottenTomatoesActor.rt_id) == res_actor['id']).first()
                if not actor:
                    actor = RottenTomatoesActor(res_actor['name'], res_actor['id'])
                movie.cast.append(actor)
        directors = movie_data.get('abridged_directors')
        if directors:
            for res_director in directors:
                director = session.query(RottenTomatoesDirector).filter(
                    func.lower(RottenTomatoesDirector.name) == res_director['name'].lower()).first()
                if not director:
                    director = RottenTomatoesDirector(res_director['name'])
                movie.directors.append(director)
        alternate_ids = movie_data.get('alternate_ids')
        if alternate_ids:
            for name, id in alternate_ids.items():
                movie.alternate_ids.append(RottenTomatoesAlternateId(name, id))
        links = movie_data.get('links')
        if links:
            for name, url in links.items():
                movie.links.append(RottenTomatoesLink(name, url))
        movie.updated = datetime.now()
    else:
        raise PluginError('No movie_data for rottentomatoes_id %s' % movie.id)

    return movie


def movies_info(id, api_key=None):
    if not api_key:
        api_key = API_KEY
    url = '%s/%s/movies/%s.json?apikey=%s' % (SERVER, API_VER, id, api_key)
    result = get_json(url)
    if isinstance(result, dict) and result.get('id'):
        return result


def lists(list_type, list_name, limit=20, page_limit=20, page=None, api_key=None):
    if isinstance(list_type, basestring):
        list_type = list_type.replace(' ', '_').encode('utf-8')
    if isinstance(list_name, basestring):
        list_name = list_name.replace(' ', '_').encode('utf-8')

    if not api_key:
        api_key = API_KEY

    url = '%s/%s/lists/%s/%s.json?apikey=%s' % (SERVER, API_VER, list_type, list_name, api_key)
    if limit:
        url += '&limit=%i' % limit
    if page_limit:
        url += '&page_limit=%i' % page_limit
    if page:
        url += '&page=%i' % page

    results = get_json(url)
    if isinstance(results, dict) and len(results.get('movies')):
        return results


def movies_search(q, page_limit=None, page=None, api_key=None):
    if isinstance(q, basestring):
        q = q.replace(' ', '+').encode('utf-8')

    if not api_key:
        api_key = API_KEY

    url = '%s/%s/movies.json?q=%s&apikey=%s' % (SERVER, API_VER, q, api_key)
    if page_limit:
        url += '&page_limit=%i' % page_limit
    if page:
        url += '&page=%i' % page

    results = get_json(url)
    if isinstance(results, dict) and results.get('total') and len(results.get('movies')):
        return results

def get_json(url):
    try:
        log.debug('fetching json at %s' % url)
        data = session.get(url)
        return data.json()
    except requests.RequestException as e:
        log.warning('Request failed %s: %s' % (url, e))
        return
    except ValueError:
        log.warning('Rotten Tomatoes returned invalid json at: %s' % url)
        return

########NEW FILE########
__FILENAME__ = api_tmdb
from __future__ import unicode_literals, division, absolute_import
from datetime import datetime, timedelta
import logging
import os
import posixpath
import socket
import sys
from urllib2 import URLError

from sqlalchemy import Table, Column, Integer, Float, String, Unicode, Boolean, DateTime, func
from sqlalchemy.schema import ForeignKey
from sqlalchemy.orm import relation

from flexget import db_schema, plugin
from flexget.event import event
from flexget.manager import Session
from flexget.utils import requests
from flexget.utils.database import text_date_synonym, year_property, with_session
from flexget.utils.sqlalchemy_utils import table_add_column, table_schema
from flexget.utils.titles import MovieParser

try:
    import tmdb3
except ImportError:
    raise plugin.DependencyError(issued_by='api_tmdb', missing='tmdb3',
                                 message='TMDB requires https://github.com/wagnerrp/pytmdb3')

log = logging.getLogger('api_tmdb')
Base = db_schema.versioned_base('api_tmdb', 0)

# This is a FlexGet API key
tmdb3.tmdb_api.set_key('bdfc018dbdb7c243dc7cb1454ff74b95')
tmdb3.locales.set_locale("en", "us", True)
# There is a bug in tmdb3 library, where it uses the system encoding for query parameters, tmdb expects utf-8 #2392
tmdb3.locales.syslocale.encoding = 'utf-8'
tmdb3.set_cache('null')


@db_schema.upgrade('api_tmdb')
def upgrade(ver, session):
    if ver is None:
        log.info('Adding columns to tmdb cache table, marking current cache as expired.')
        table_add_column('tmdb_movies', 'runtime', Integer, session)
        table_add_column('tmdb_movies', 'tagline', Unicode, session)
        table_add_column('tmdb_movies', 'budget', Integer, session)
        table_add_column('tmdb_movies', 'revenue', Integer, session)
        table_add_column('tmdb_movies', 'homepage', String, session)
        table_add_column('tmdb_movies', 'trailer', String, session)
        # Mark all cached movies as expired, so new fields get populated next lookup
        movie_table = table_schema('tmdb_movies', session)
        session.execute(movie_table.update(values={'updated': datetime(1970, 1, 1)}))
        ver = 0
    return ver


# association tables
genres_table = Table('tmdb_movie_genres', Base.metadata,
    Column('movie_id', Integer, ForeignKey('tmdb_movies.id')),
    Column('genre_id', Integer, ForeignKey('tmdb_genres.id')))


class TMDBContainer(object):
    """Base class for TMDb objects"""

    def __init__(self, init_object=None):
        if isinstance(init_object, dict):
            self.update_from_dict(init_object)
        elif init_object:
            self.update_from_object(init_object)

    def update_from_dict(self, update_dict):
        """Populates any simple (string or number) attributes from a dict"""
        for col in self.__table__.columns:
            if isinstance(update_dict.get(col.name), (basestring, int, float)):
                setattr(self, col.name, update_dict[col.name])

    def update_from_object(self, update_object):
        """Populates any simple (string or number) attributes from object attributes"""
        for col in self.__table__.columns:
            if hasattr(update_object, col.name) and isinstance(getattr(update_object, col.name), (basestring, int, float)):
                setattr(self, col.name, getattr(update_object, col.name))


class TMDBMovie(TMDBContainer, Base):
    __tablename__ = 'tmdb_movies'

    id = Column(Integer, primary_key=True, autoincrement=False, nullable=False)
    updated = Column(DateTime, default=datetime.now, nullable=False)
    popularity = Column(Integer)
    translated = Column(Boolean)
    adult = Column(Boolean)
    language = Column(String)
    original_name = Column(Unicode)
    name = Column(Unicode)
    alternative_name = Column(Unicode)
    movie_type = Column(String)
    imdb_id = Column(String)
    url = Column(String)
    votes = Column(Integer)
    rating = Column(Float)
    certification = Column(String)
    overview = Column(Unicode)
    runtime = Column(Integer)
    tagline = Column(Unicode)
    budget = Column(Integer)
    revenue = Column(Integer)
    homepage = Column(String)
    trailer = Column(String)
    _released = Column('released', DateTime)
    released = text_date_synonym('_released')
    year = year_property('released')
    posters = relation('TMDBPoster', backref='movie', cascade='all, delete, delete-orphan')
    genres = relation('TMDBGenre', secondary=genres_table, backref='movies')

    def update_from_object(self, update_object):
        TMDBContainer.update_from_object(self, update_object)
        self.translated = len(update_object.translations) > 0
        if len(update_object.languages) > 0:
            self.language = update_object.languages[0].code #.code or .name ?
        self.original_name = update_object.originaltitle
        self.name = update_object.title
        try:
            if len(update_object.alternate_titles) > 0:
                # maybe we could choose alternate title from movie country only
                self.alternative_name = update_object.alternate_titles[0].title
        except UnicodeEncodeError:
            # Bug in tmdb3 library, see #2437. Just don't set alternate_name when it fails
            pass
        self.imdb_id = update_object.imdb
        self.url = update_object.homepage
        self.rating = update_object.userrating
        if len(update_object.youtube_trailers) > 0:
            self.trailer = update_object.youtube_trailers[0].source # unicode: ooNSm6Uug3g
        elif len(update_object.apple_trailers) > 0:
            self.trailer = update_object.apple_trailers[0].source
        self.released = update_object.releasedate


class TMDBGenre(TMDBContainer, Base):

    __tablename__ = 'tmdb_genres'

    id = Column(Integer, primary_key=True, autoincrement=False)
    name = Column(String, nullable=False)


class TMDBPoster(TMDBContainer, Base):

    __tablename__ = 'tmdb_posters'

    db_id = Column(Integer, primary_key=True)
    movie_id = Column(Integer, ForeignKey('tmdb_movies.id'))
    size = Column(String)
    url = Column(String)
    file = Column(Unicode)

    def get_file(self, only_cached=False):
        """Makes sure the poster is downloaded to the local cache (in userstatic folder) and
        returns the path split into a list of directory and file components"""
        from flexget.manager import manager
        base_dir = os.path.join(manager.config_base, 'userstatic')
        if self.file and os.path.isfile(os.path.join(base_dir, self.file)):
            return self.file.split(os.sep)
        elif only_cached:
            return
        # If we don't already have a local copy, download one.
        log.debug('Downloading poster %s' % self.url)
        dirname = os.path.join('tmdb', 'posters', str(self.movie_id))
        # Create folders if they don't exist
        fullpath = os.path.join(base_dir, dirname)
        if not os.path.isdir(fullpath):
            os.makedirs(fullpath)
        filename = os.path.join(dirname, posixpath.basename(self.url))
        thefile = file(os.path.join(base_dir, filename), 'wb')
        thefile.write(requests.get(self.url).content)
        self.file = filename
        # If we are detached from a session, update the db
        if not Session.object_session(self):
            session = Session()
            try:
                poster = session.query(TMDBPoster).filter(TMDBPoster.db_id == self.db_id).first()
                if poster:
                    poster.file = filename
                    session.commit()
            finally:
                session.close()
        return filename.split(os.sep)


class TMDBSearchResult(Base):

    __tablename__ = 'tmdb_search_results'

    id = Column(Integer, primary_key=True)
    search = Column(Unicode, nullable=False)
    movie_id = Column(Integer, ForeignKey('tmdb_movies.id'), nullable=True)
    movie = relation(TMDBMovie, backref='search_strings')


class ApiTmdb(object):
    """Does lookups to TMDb and provides movie information. Caches lookups."""

    @staticmethod
    @with_session
    def lookup(title=None, year=None, tmdb_id=None, imdb_id=None, smart_match=None, only_cached=False, session=None):
        """Do a lookup from TMDb for the movie matching the passed arguments.

        Any combination of criteria can be passed, the most specific criteria specified will be used.

        Returns:
            The Movie object populated with data from tmdb

        Raises:
            LookupError if a match cannot be found or there are other problems with the lookup

        Args:
            tmdb_id: tmdb_id of desired movie
            imdb_id: imdb_id of desired movie
            title: title of desired movie
            year: release year of desired movie
            smart_match: attempt to clean and parse title and year from a string
            only_cached: if this is specified, an online lookup will not occur if the movie is not in the cache
            session: optionally specify a session to use, if specified, returned Movie will be live in that session
        """

        if not (tmdb_id or imdb_id or title) and smart_match:
            # If smart_match was specified, and we don't have more specific criteria, parse it into a title and year
            title_parser = MovieParser()
            title_parser.parse(smart_match)
            title = title_parser.name
            year = title_parser.year

        if title:
            search_string = title.lower()
            if year:
                search_string = '%s (%s)' % (search_string, year)
        elif not (tmdb_id or imdb_id):
            raise LookupError('No criteria specified for TMDb lookup')
        log.debug('Looking up TMDb information for %r' % {'title': title, 'tmdb_id': tmdb_id, 'imdb_id': imdb_id})

        movie = None

        def id_str():
            return '<title=%s,tmdb_id=%s,imdb_id=%s>' % (title, tmdb_id, imdb_id)
        if tmdb_id:
            movie = session.query(TMDBMovie).filter(TMDBMovie.id == tmdb_id).first()
        if not movie and imdb_id:
            movie = session.query(TMDBMovie).filter(TMDBMovie.imdb_id == imdb_id).first()
        if not movie and title:
            movie_filter = session.query(TMDBMovie).filter(func.lower(TMDBMovie.name) == title.lower())
            if year:
                movie_filter = movie_filter.filter(TMDBMovie.year == year)
            movie = movie_filter.first()
            if not movie:
                found = session.query(TMDBSearchResult). \
                    filter(func.lower(TMDBSearchResult.search) == search_string).first()
                if found and found.movie:
                    movie = found.movie
        if movie:
            # Movie found in cache, check if cache has expired.
            refresh_time = timedelta(days=2)
            if movie.released:
                if movie.released > datetime.now() - timedelta(days=7):
                    # Movie is less than a week old, expire after 1 day
                    refresh_time = timedelta(days=1)
                else:
                    age_in_years = (datetime.now() - movie.released).days / 365
                    refresh_time += timedelta(days=age_in_years * 5)
            if movie.updated < datetime.now() - refresh_time and not only_cached:
                log.debug('Cache has expired for %s, attempting to refresh from TMDb.' % id_str())
                try:
                    ApiTmdb.get_movie_details(movie, session)
                except URLError:
                    log.error('Error refreshing movie details from TMDb, cached info being used.')
            else:
                log.debug('Movie %s information restored from cache.' % id_str())
        else:
            if only_cached:
                raise LookupError('Movie %s not found from cache' % id_str())
            # There was no movie found in the cache, do a lookup from tmdb
            log.verbose('Searching from TMDb %s' % id_str())
            try:
                if imdb_id and not tmdb_id:
                    result = tmdb3.Movie.fromIMDB(imdb_id)
                    if result:
                        movie = session.query(TMDBMovie).filter(TMDBMovie.id == result.id).first()
                        if movie:
                            # Movie was in database, but did not have the imdb_id stored, force an update
                            ApiTmdb.get_movie_details(movie, session, result)
                        else:
                            tmdb_id = result.id
                if tmdb_id:
                    movie = TMDBMovie()
                    movie.id = tmdb_id
                    ApiTmdb.get_movie_details(movie, session)
                    if movie.name:
                        session.merge(movie)
                    else:
                        movie = None
                elif title:
                    try:
                        result = _first_result(tmdb3.tmdb_api.searchMovie(title.lower(), adult=True, year=year))
                    except socket.timeout:
                        raise LookupError('Timeout contacting TMDb')
                    if not result and year:
                        result = _first_result(tmdb3.tmdb_api.searchMovie(title.lower(), adult=True))
                    if result:
                        movie = session.query(TMDBMovie).filter(TMDBMovie.id == result.id).first()
                        if not movie:
                            movie = TMDBMovie(result)
                            ApiTmdb.get_movie_details(movie, session, result)
                            session.merge(movie)
                        if title.lower() != movie.name.lower():
                            session.merge(TMDBSearchResult(search=search_string, movie=movie))
            except tmdb3.TMDBError as e:
                raise LookupError('Error looking up movie from TMDb (%s)' % e)
            if movie:
                log.verbose("Movie found from TMDb: %s (%s)" % (movie.name, movie.year))

        if not movie:
            raise LookupError('No results found from tmdb for %s' % id_str())
        else:
            # Access attributes to force the relationships to eager load before we detach from session
            movie.genres
            movie.posters
            return movie

    @staticmethod
    def get_movie_details(movie, session, result=None):
        """Populate details for this :movie: from TMDb"""

        if not result and not movie.id:
            raise LookupError('Cannot get tmdb details without tmdb id')
        if not result:
            try:
                result = tmdb3.Movie(movie.id)
            except tmdb3.TMDBError:
                raise LookupError('No results for tmdb_id: %s (%s)' % (movie.id, sys.exc_info()[1]))
            try:
                movie.update_from_object(result)
            except tmdb3.TMDBRequestInvalid as e:
                log.debug('Error updating tmdb info: %s' % e)
                raise LookupError('Error getting tmdb info')
        posters = result.posters
        if posters:
            # Add any posters we don't already have
            # TODO: There are quite a few posters per movie, do we need to cache them all?
            poster_urls = [p.url for p in movie.posters]
            for item in posters:
                for size in item.sizes():
                    url = item.geturl(size)
                    if url not in poster_urls:
                        poster_data = {"movie_id":movie.id, "size":size, "url": url, "file": item.filename}
                        movie.posters.append(TMDBPoster(poster_data))
        genres = result.genres
        if genres:
            for genre in genres:
                if not genre.id:
                    continue
                db_genre = session.query(TMDBGenre).filter(TMDBGenre.id == genre.id).first()
                if not db_genre:
                    db_genre = TMDBGenre(genre)
                if db_genre not in movie.genres:
                    movie.genres.append(db_genre)
        movie.updated = datetime.now()

def _first_result(results):
    if results and len(results) >= 1:
        return results[0]


@event('plugin.register')
def register_plugin():
    plugin.register(ApiTmdb, 'api_tmdb', api_ver=2)

########NEW FILE########
__FILENAME__ = api_trakt
from __future__ import unicode_literals, division, absolute_import
from difflib import SequenceMatcher
import logging
import re
import urllib

import requests
from sqlalchemy import Table, Column, Integer, String, Unicode, Boolean, func
from sqlalchemy.orm import relation
from sqlalchemy.schema import ForeignKey

from flexget import db_schema as schema
from flexget import plugin
from flexget.event import event
from flexget.plugins.filter.series import normalize_series_name
from flexget.utils.database import with_session

api_key = '6c228565a45a302e49fb7d2dab066c9ab948b7be/'
search_show = 'http://api.trakt.tv/search/shows.json/'
episode_summary = 'http://api.trakt.tv/show/episode/summary.json/'
show_summary = 'http://api.trakt.tv/show/summary.json/'
Base = schema.versioned_base('api_trakt', 2)
log = logging.getLogger('api_trakt')


class TraktContainer(object):

    def __init__(self, init_dict=None):
        if isinstance(init_dict, dict):
            self.update_from_dict(init_dict)

    def update_from_dict(self, update_dict):
        for col in self.__table__.columns:
            if isinstance(update_dict.get(col.name), (basestring, int, float)):
                setattr(self, col.name, update_dict[col.name])

genres_table = Table('trakt_series_genres', Base.metadata,
                     Column('tvdb_id', Integer, ForeignKey('trakt_series.tvdb_id')),
                     Column('genre_id', Integer, ForeignKey('trakt_genres.id')))


class TraktGenre(TraktContainer, Base):

    __tablename__ = 'trakt_genres'

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String, nullable=True)

actors_table = Table('trakt_series_actors', Base.metadata,
                     Column('tvdb_id', Integer, ForeignKey('trakt_series.tvdb_id')),
                     Column('actors_id', Integer, ForeignKey('trakt_actors.id')))


class TraktActors(TraktContainer, Base):

    __tablename__ = 'trakt_actors'

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String, nullable=False)


class TraktEpisode(TraktContainer, Base):
    __tablename__ = 'trakt_episodes'

    tvdb_id = Column(Integer, primary_key=True, autoincrement=False)
    episode_name = Column(Unicode)
    season = Column(Integer)
    number = Column(Integer)
    overview = Column(Unicode)
    expired = Column(Boolean)
    first_aired = Column(Integer)
    first_aired_iso = Column(Unicode)
    first_aired_utc = Column(Integer)
    screen = Column(Unicode)

    series_id = Column(Integer, ForeignKey('trakt_series.tvdb_id'), nullable=False)


class TraktSeries(TraktContainer, Base):
    __tablename__ = 'trakt_series'

    tvdb_id = Column(Integer, primary_key=True, autoincrement=False)
    tvrage_id = Column(Unicode)
    imdb_id = Column(Unicode)
    title = Column(Unicode)
    year = Column(Integer)
    genre = relation('TraktGenre', secondary=genres_table, backref='series')
    network = Column(Unicode, nullable=True)
    certification = Column(Unicode)
    country = Column(Unicode)
    overview = Column(Unicode)
    first_aired = Column(Integer)
    first_aired_iso = Column(Unicode)
    first_aired_utc = Column(Integer)
    air_day = Column(Unicode)
    air_day_utc = Column(Unicode)
    air_time = Column(Unicode)
    air_time_utc = Column(Unicode)
    runtime = Column(Integer)
    last_updated = Column(Integer)
    poster = Column(String)
    fanart = Column(String)
    banner = Column(String)
    status = Column(String)
    url = Column(Unicode)
    episodes = relation('TraktEpisode', backref='series', cascade='all, delete, delete-orphan')
    actors = relation('TraktActors', secondary=actors_table, backref='series')

    def update(self, session):
        tvdb_id = self.tvdb_id
        url = ('%s%s%s' % (show_summary, api_key, tvdb_id))
        try:
            data = requests.get(url).json()
        except requests.RequestException as e:
            raise LookupError('Request failed %s' % url)
        if data:
            if data['title']:
                for i in data['images']:
                    data[i] = data['images'][i]
            if data['genres']:
                genres = {}
                for genre in data['genres']:
                    db_genre = session.query(TraktGenre).filter(TraktGenre.name == genre).first()
                    if not db_genre:
                        genres['name'] = genre
                        db_genre = TraktGenre(genres)
                    if db_genre not in self.genre:
                        self.genre.append(db_genre)
            if data['people']['actors']:
                series_actors = data['people']['actors']
                for i in series_actors:
                    if i['name']:
                        db_character = session.query(TraktActors).filter(TraktActors.name == i['name']).first()
                        if not db_character:
                            db_character = TraktActors(i)
                        if db_character not in self.actors:
                            self.actors.append(db_character)
            if data['title']:
                TraktContainer.update_from_dict(self, data)
            else:
                raise LookupError('Could not update information to database for Trakt on ')

    def __repr__(self):
        return '<Traktv Name=%s, TVDB_ID=%s>' % (self.title, self.tvdb_id)


class TraktSearchResult(Base):

    __tablename__ = 'trakt_search_results'

    id = Column(Integer, primary_key=True)
    search = Column(Unicode, nullable=False)
    series_id = Column(Integer, ForeignKey('trakt_series.tvdb_id'), nullable=True)
    series = relation(TraktSeries, backref='search_strings')


def get_series_id(title):
    norm_series_name = normalize_series_name(title)
    series_name = urllib.quote_plus(norm_series_name)
    url = search_show + api_key + series_name
    series = None
    try:
        response = requests.get(url)
    except requests.RequestException:
        log.warning('Request failed %s' % url)
        return
    try:
        data = response.json()
    except ValueError:
        log.debug('Error Parsing Traktv Json for %s' % title)
        return
    if 'status' in data:
        log.debug('Returned Status %s' % data['status'])
    else:
        for item in data:
            if normalize_series_name(item['title']) == norm_series_name:
                series = item['tvdb_id']
        if not series:
            for item in data:
                title_match = SequenceMatcher(lambda x: x in '\t',
                                              normalize_series_name(item['title']), norm_series_name).ratio()
                if not series and title_match > .9:
                    log.debug('Warning: Using lazy matching because title was not found exactly for %s' % title)
                    series = item['tvdb_id']
    if not series:
        log.debug('Trakt.tv Returns only EXACT Name Matching: %s' % title)
    return series


class ApiTrakt(object):

    @staticmethod
    @with_session
    def lookup_series(title=None, tvdb_id=None, only_cached=False, session=None):
        if not title and not tvdb_id:
            raise LookupError('No criteria specified for Trakt.tv Lookup')
        series = None

        def id_str():
            return '<name=%s, tvdb_id=%s>' % (title, tvdb_id)
        if tvdb_id:
            series = session.query(TraktSeries).filter(TraktSeries.tvdb_id == tvdb_id).first()
        if not series and title:
            series_filter = session.query(TraktSeries).filter(func.lower(TraktSeries.title) == title.lower())
            series = series_filter.first()
            if not series:
                found = session.query(TraktSearchResult).filter(func.lower(TraktSearchResult.search) ==
                                                                title.lower()).first()
                if found and found.series:
                    series = found.series
        if not series:
            if only_cached:
                raise LookupError('Series %s not found from cache' % id_str())
            log.debug('Series %s not found in cache, looking up from trakt.' % id_str())
            if tvdb_id is not None:
                series = TraktSeries()
                series.tvdb_id = tvdb_id
                series.update(session=session)
                if series.title:
                    session.add(series)
            if tvdb_id is None and title is not None:
                series_lookup = get_series_id(title)
                if series_lookup:
                    series = session.query(TraktSeries).filter(TraktSeries.tvdb_id == series_lookup).first()
                    if not series and series_lookup:
                        series = TraktSeries()
                        series.tvdb_id = series_lookup
                        series.update(session=session)
                        if series.title:
                            session.add(series)
                    if title.lower() != series.title.lower():
                        session.add(TraktSearchResult(search=title, series=series))
                else:
                    raise LookupError('Unknown Series title from Traktv: %s' % id_str())

        if not series:
            raise LookupError('No results found from traktv for %s' % id_str())
        if not series.title:
            raise LookupError('Nothing Found for %s' % id_str())
        if series:
            series.episodes
            series.genre
            series.actors
            return series

    @staticmethod
    @with_session
    def lookup_episode(title=None, seasonnum=None, episodenum=None, tvdb_id=None, session=None, only_cached=False):
        series = ApiTrakt.lookup_series(title=title, tvdb_id=tvdb_id, only_cached=only_cached, session=session)
        if not series:
            raise LookupError('Could not identify series')
        if series.tvdb_id:
            ep_description = '%s.S%sE%s' % (series.title, seasonnum, episodenum)
            episode = session.query(TraktEpisode).filter(TraktEpisode.series_id == series.tvdb_id).\
                filter(TraktEpisode.season == seasonnum).filter(TraktEpisode.number == episodenum).first()
            url = episode_summary + api_key + '%s/%s/%s' % (series.tvdb_id, seasonnum, episodenum)
        elif title:
            title = normalize_series_name(title)
            title = re.sub(' ', '-', title)
            ep_description = '%s.S%sE%s' % (series.title, seasonnum, episodenum)
            episode = session.query(TraktEpisode).filter(title == series.title).\
                filter(TraktEpisode.season == seasonnum).\
                filter(TraktEpisode.number == episodenum).first()
            url = episode_summary + api_key + '%s/%s/%s' % (title, seasonnum, episodenum)
        if not episode:
            if only_cached:
                raise LookupError('Episode %s not found in cache' % ep_description)

            log.debug('Episode %s not found in cache, looking up from trakt.' % ep_description)
            try:
                data = requests.get(url)
            except requests.RequestException:
                log.debug('Error Retrieving Trakt url: %s' % url)
            try:
                data = data.json()
            except ValueError:
                log.debug('Error parsing Trakt episode json for %s' % title)
            if data:
                if 'status' in data:
                    raise LookupError('Error looking up episode')
                ep_data = data['episode']
                if ep_data:
                    episode = session.query(TraktEpisode).filter(TraktEpisode.tvdb_id == ep_data['tvdb_id']).first()
                    if not episode:
                        ep_data['episode_name'] = ep_data.pop('title')
                        for i in ep_data['images']:
                            ep_data[i] = ep_data['images'][i]
                            del ep_data['images']
                        episode = TraktEpisode(ep_data)
                    series.episodes.append(episode)
                    session.merge(episode)
        if episode:
            return episode
        else:
            raise LookupError('No results found for (%s)' % episode)


@event('plugin.register')
def register_plugin():
    plugin.register(ApiTrakt, 'api_trakt', api_ver=2)

########NEW FILE########
__FILENAME__ = api_tvdb
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib
import os
import posixpath
from datetime import datetime, timedelta
import random

import xml.etree.ElementTree as ElementTree

from sqlalchemy import Column, Integer, Float, String, Unicode, Boolean, DateTime, func
from sqlalchemy.schema import ForeignKey
from sqlalchemy.orm import relation
from requests import RequestException

from flexget import db_schema
from flexget.utils.tools import decode_html
from flexget.utils.requests import Session as ReqSession
from flexget.utils.database import with_session, pipe_list_synonym, text_date_synonym
from flexget.utils.sqlalchemy_utils import table_add_column
from flexget.manager import Session
from flexget.utils.simple_persistence import SimplePersistence

SCHEMA_VER = 3

log = logging.getLogger('api_tvdb')
Base = db_schema.versioned_base('api_tvdb', SCHEMA_VER)
requests = ReqSession(timeout=25)

# This is a FlexGet API key
api_key = '4D297D8CFDE0E105'
language = 'en'
server = 'http://www.thetvdb.com/api/'
_mirrors = {}
persist = SimplePersistence('api_tvdb')


@db_schema.upgrade('api_tvdb')
def upgrade(ver, session):
    if ver is None:
        if 'last_updated' in persist:
            del persist['last_updated']
        ver = 0
    if ver == 0:
        table_add_column('tvdb_episodes', 'gueststars', Unicode, session)
        ver = 1
    if ver == 1:
        table_add_column('tvdb_episodes', 'absolute_number', Integer, session)
        ver = 2
    if ver == 2:
        table_add_column('tvdb_series', 'overview', Unicode, session)
        ver = 3

    return ver


def get_mirror(type='xml'):
    """Returns a random mirror for a given type 'xml', 'zip', or 'banner'"""
    global _mirrors
    if not _mirrors.get(type):
        # Get the list of mirrors from tvdb
        page = None
        try:
            page = requests.get(server + api_key + '/mirrors.xml').content
        except RequestException:
            pass
        # If there were problems getting the mirror list we'll just fall back to the main site.
        if page:
            data = ElementTree.fromstring(page)
            for mirror in data.findall('Mirror'):
                type_mask = int(mirror.find("typemask").text)
                mirrorpath = mirror.find("mirrorpath").text
                for t in [(1, 'xml'), (2, 'banner'), (4, 'zip')]:
                    if type_mask & t[0]:
                        _mirrors.setdefault(t[1], set()).add(mirrorpath)
        else:
            log.debug('Unable to get the mirrors list from thetvdb.')
    if _mirrors.get(type):
        return random.sample(_mirrors[type], 1)[0] + ('/banners/' if type == 'banner' else '/api/')
    else:
        # If nothing was populated from the server's mirror list, return the main site as fallback
        return 'http://thetvdb.com' + ('/banners/' if type == 'banner' else '/api/')


class TVDBContainer(object):
    """Base class for TVDb objects"""

    def __init__(self, init_xml=None):
        if init_xml is not None:
            self.update_from_xml(init_xml)

    def update_from_xml(self, update_xml):
        """Populates any simple (string or number) attributes from a dict"""
        for node in update_xml:
            if not node.text or not node.tag:
                continue

            # Have to iterate to get around the inability to do a case-insensitive find
            for col in self.__table__.columns:
                if node.tag.lower() == col.name.lower():
                    if isinstance(col.type, Integer):
                        value = int(node.text)
                    elif isinstance(col.type, Float):
                        value = float(node.text)
                    else:
                        # Make sure we always have unicode strings
                        value = node.text.decode('utf-8') if isinstance(node.text, str) else node.text
                        value = decode_html(value)
                    setattr(self, col.name, value)
        self.expired = False


class TVDBSeries(TVDBContainer, Base):

    __tablename__ = "tvdb_series"

    id = Column(Integer, primary_key=True, autoincrement=False)
    lastupdated = Column(Integer)
    expired = Column(Boolean)
    seriesname = Column(Unicode)
    language = Column(Unicode)
    rating = Column(Float)
    status = Column(Unicode)
    runtime = Column(Integer)
    airs_time = Column(Unicode)
    airs_dayofweek = Column(Unicode)
    contentrating = Column(Unicode)
    network = Column(Unicode)
    overview = Column(Unicode)
    imdb_id = Column(String)
    zap2it_id = Column(String)
    banner = Column(String)
    fanart = Column(String)
    poster = Column(String)
    poster_file = Column(Unicode)
    _genre = Column('genre', Unicode)
    genre = pipe_list_synonym('_genre')
    _firstaired = Column('firstaired', DateTime)
    firstaired = text_date_synonym('_firstaired')

    episodes = relation('TVDBEpisode', backref='series', cascade='all, delete, delete-orphan')

    def update(self):
        if not self.id:
            raise LookupError('Cannot update a series without a tvdb id.')
        url = get_mirror() + api_key + '/series/%s/%s.xml' % (self.id, language)
        try:
            data = requests.get(url).content
        except RequestException as e:
            raise LookupError('Request failed %s' % url)
        result = ElementTree.fromstring(data).find('Series')
        if result is not None:
            self.update_from_xml(result)
        else:
            raise LookupError('Could not retrieve information from thetvdb')

    def get_poster(self, only_cached=False):
        """Downloads this poster to a local cache and returns the path"""
        from flexget.manager import manager
        base_dir = os.path.join(manager.config_base, 'userstatic')
        if os.path.isfile(os.path.join(base_dir, self.poster_file or '')):
            return self.poster_file
        elif only_cached:
            return
        # If we don't already have a local copy, download one.
        url = get_mirror('banner') + self.poster
        log.debug('Downloading poster %s' % url)
        dirname = os.path.join('tvdb', 'posters')
        # Create folders if the don't exist
        fullpath = os.path.join(base_dir, dirname)
        if not os.path.isdir(fullpath):
            os.makedirs(fullpath)
        filename = os.path.join(dirname, posixpath.basename(self.poster))
        thefile = file(os.path.join(base_dir, filename), 'wb')
        thefile.write(requests.get(url).content)
        self.poster_file = filename
        # If we are detached from a session, update the db
        if not Session.object_session(self):
            session = Session()
            try:
                session.query(TVDBSeries).filter(TVDBSeries.id == self.id).update(values={'poster_file': filename})
            finally:
                session.close()
        return filename

    def __repr__(self):
        return '<TVDBSeries name=%s,tvdb_id=%s>' % (self.seriesname, self.id)


class TVDBEpisode(TVDBContainer, Base):
    __tablename__ = 'tvdb_episodes'

    id = Column(Integer, primary_key=True, autoincrement=False)
    expired = Column(Boolean)
    lastupdated = Column(Integer)
    seasonnumber = Column(Integer)
    episodenumber = Column(Integer)
    absolute_number = Column(Integer)
    episodename = Column(Unicode)
    overview = Column(Unicode)
    _director = Column('director', Unicode)
    director = pipe_list_synonym('_director')
    _writer = Column('writer', Unicode)
    writer = pipe_list_synonym('_writer')
    _gueststars = Column('gueststars', Unicode)
    gueststars = pipe_list_synonym('_gueststars')
    rating = Column(Float)
    filename = Column(Unicode)
    _firstaired = Column('firstaired', DateTime)
    firstaired = text_date_synonym('_firstaired')

    series_id = Column(Integer, ForeignKey('tvdb_series.id'), nullable=False)

    def update(self):
        if not self.id:
            raise LookupError('Cannot update an episode without an episode id.')
        url = get_mirror() + api_key + '/episodes/%s/%s.xml' % (self.id, language)
        try:
            data = requests.get(url).content
        except RequestException as e:
            raise LookupError('Request failed %s' % url)
        result = ElementTree.fromstring(data).find('Episode')
        if result is not None:
            self.update_from_xml(result)
        else:
            raise LookupError('Could not retrieve information from thetvdb')

    def __repr__(self):
        return '<TVDBEpisode series=%s,season=%s,episode=%s>' %\
               (self.series.seriesname, self.seasonnumber, self.episodenumber)


class TVDBSearchResult(Base):

    __tablename__ = 'tvdb_search_results'

    id = Column(Integer, primary_key=True)
    search = Column(Unicode, nullable=False)
    series_id = Column(Integer, ForeignKey('tvdb_series.id'), nullable=True)
    series = relation(TVDBSeries, backref='search_strings')


def find_series_id(name):
    """Looks up the tvdb id for a series"""
    url = server + 'GetSeries.php?seriesname=%s&language=%s' % (urllib.quote(name), language)
    try:
        page = requests.get(url).content
    except RequestException as e:
        raise LookupError("Unable to get search results for %s: %s" % (name, e))
    xmldata = ElementTree.fromstring(page)
    if xmldata is None:
        log.error("Didn't get a return from tvdb on the series search for %s" % name)
        return
    # See if there is an exact match
    # TODO: Check if there are multiple exact matches
    firstmatch = xmldata.find('Series')
    if firstmatch is not None and firstmatch.find("SeriesName").text.lower() == name.lower():
        return int(firstmatch.find("seriesid").text)
    # If there is no exact match, sort by airing date and pick the latest
    # TODO: Is there a better way to do this? Maybe weight name similarity and air date
    series_list = []
    for s in xmldata.findall('Series'):
        fa = s.find("FirstAired")
        if fa is not None and fa.text:
            series_list.append((fa.text, s.find("seriesid").text))

    if series_list:
        series_list.sort(key=lambda s: s[0], reverse=True)
        return int(series_list[0][1])
    else:
        raise LookupError('No results for `%s`' % name)


@with_session
def lookup_series(name=None, tvdb_id=None, only_cached=False, session=None):
    if not name and not tvdb_id:
        raise LookupError('No criteria specified for tvdb lookup')

    log.debug('Looking up tvdb information for %r' % {'name': name, 'tvdb_id': tvdb_id})

    series = None

    def id_str():
        return '<name=%s,tvdb_id=%s>' % (name, tvdb_id)

    if tvdb_id:
        series = session.query(TVDBSeries).filter(TVDBSeries.id == tvdb_id).first()
    if not series and name:
        series = session.query(TVDBSeries).filter(func.lower(TVDBSeries.seriesname) == name.lower()).first()
        if not series:
            found = session.query(TVDBSearchResult).filter(
                func.lower(TVDBSearchResult.search) == name.lower()).first()
            if found and found.series:
                series = found.series
    if series:
        # Series found in cache, update if cache has expired.
        if not only_cached:
            mark_expired(session=session)
        if series.expired and not only_cached:
            log.verbose('Data for %s has expired, refreshing from tvdb' % series.seriesname)
            try:
                series.update()
            except LookupError as e:
                log.warning('Error while updating from tvdb (%s), using cached data.' % e.args[0])
        else:
            log.debug('Series %s information restored from cache.' % id_str())
    else:
        if only_cached:
            raise LookupError('Series %s not found from cache' % id_str())
        # There was no series found in the cache, do a lookup from tvdb
        log.debug('Series %s not found in cache, looking up from tvdb.' % id_str())
        if tvdb_id:
            series = TVDBSeries()
            series.id = tvdb_id
            series.update()
            if series.seriesname:
                session.add(series)
        elif name:
            tvdb_id = find_series_id(name)
            if tvdb_id:
                series = session.query(TVDBSeries).filter(TVDBSeries.id == tvdb_id).first()
                if not series:
                    series = TVDBSeries()
                    series.id = tvdb_id
                    series.update()
                    session.add(series)
                if name.lower() != series.seriesname.lower():
                    session.add(TVDBSearchResult(search=name, series=series))

    if not series:
        raise LookupError('No results found from tvdb for %s' % id_str())
    if not series.seriesname:
        raise LookupError('Tvdb result for series does not have a title.')
    series.episodes
    return series


@with_session
def lookup_episode(name=None, seasonnum=None, episodenum=None, absolutenum=None, airdate=None,
                   tvdb_id=None, only_cached=False, session=None):
    # First make sure we have the series data
    series = lookup_series(name=name, tvdb_id=tvdb_id, only_cached=only_cached, session=session)
    if not series:
        raise LookupError('Could not identify series')
    # Set variables depending on what type of identifier we are looking up
    if airdate:
        airdatestring = airdate.strftime('%Y-%m-%d')
        ep_description = '%s.%s' % (series.seriesname, airdatestring)
        episode = session.query(TVDBEpisode).filter(TVDBEpisode.series_id == series.id).\
            filter(TVDBEpisode.firstaired == airdate).first()
        url = get_mirror() + ('GetEpisodeByAirDate.php?apikey=%s&seriesid=%d&airdate=%s&language=%s' %
                             (api_key, series.id, airdatestring, language))
    elif absolutenum:
        ep_description = '%s.%d' % (series.seriesname, absolutenum)
        episode = session.query(TVDBEpisode).filter(TVDBEpisode.series_id == series.id).\
            filter(TVDBEpisode.absolute_number == absolutenum).first()
        url = get_mirror() + api_key + '/series/%d/absolute/%s/%s.xml' % (series.id, absolutenum, language)
    else:
        ep_description = '%s.S%sE%s' % (series.seriesname, seasonnum, episodenum)
        # See if we have this episode cached
        episode = session.query(TVDBEpisode).filter(TVDBEpisode.series_id == series.id).\
            filter(TVDBEpisode.seasonnumber == seasonnum).\
            filter(TVDBEpisode.episodenumber == episodenum).first()
        url = get_mirror() + api_key + '/series/%d/default/%d/%d/%s.xml' % (series.id, seasonnum, episodenum, language)
    if episode:
        if episode.expired and not only_cached:
            log.info('Data for %r has expired, refreshing from tvdb' % episode)
            try:
                episode.update()
            except LookupError as e:
                log.warning('Error while updating from tvdb (%s), using cached data.' % e.args[0])
        else:
            log.debug('Using episode info for %s from cache.' % ep_description)
    else:
        if only_cached:
            raise LookupError('Episode %s not found from cache' % ep_description)
        # There was no episode found in the cache, do a lookup from tvdb
        log.debug('Episode %s not found in cache, looking up from tvdb.' % ep_description)
        try:
            raw_data = requests.get(url).content
            data = ElementTree.fromstring(raw_data)
            if data is not None:
                error = data.find('Error') # TODO: lowercase????
                if error is not None:
                    raise LookupError('Error looking up episode from TVDb (%s)' % error.text)
                ep_data = data.find('Episode')
                if ep_data is not None:
                    # Check if this episode id is already in our db
                    episode = session.query(TVDBEpisode).filter(TVDBEpisode.id == ep_data.find("id").text).first()
                    if episode is not None:
                        episode.update_from_xml(ep_data)
                    else:
                        episode = TVDBEpisode(ep_data)
                    series.episodes.append(episode)
                    session.merge(series)
        except RequestException as e:
            raise LookupError('Error looking up episode from TVDb (%s)' % e)
    if episode:
        # Access the series attribute to force it to load before returning
        episode.series
        return episode
    else:
        raise LookupError('No results found for ')


@with_session
def mark_expired(session=None):
    """Marks series and episodes that have expired since we cached them"""
    # Only get the expired list every hour
    last_server = persist.get('last_server')
    last_local = persist.get('last_local')

    if not last_local:
        # Never run before? Lets reset ALL series
        log.info('Setting all series to expire')
        session.query(TVDBSeries).update({'expired': True}, 'fetch')
        persist['last_local'] = datetime.now()
        return
    elif last_local + timedelta(hours=6) > datetime.now():
        # It has been less than an hour, don't check again yet
        return

    if not last_server:
        last_server = 0

    #Need to figure out what type of update file to use
    #Default of day
    get_update = 'day'
    last_update_days = (datetime.now() - last_local).days

    if 1 < last_update_days < 7:
        get_update = 'week'
    elif last_update_days > 7:
        get_update = 'month'

    try:
        # Get items that have changed since our last update
        log.debug("Getting %s worth of updates from thetvdb" % get_update)
        content = requests.get(server + api_key + '/updates/updates_%s.xml' % get_update).content
        if not isinstance(content, basestring):
            raise Exception('expected string, got %s' % type(content))
        updates = ElementTree.fromstring(content)
    except RequestException as e:
        log.error('Could not get update information from tvdb: %s' % e)
        return

    if updates is not None:
        new_server = int(updates.attrib['time'])

        if new_server < last_server:
            #nothing changed on the server, ignoring
            log.debug("Not checking for expired as nothing has changed on server")
            return

        # Make lists of expired series and episode ids
        expired_series = []
        expired_episodes = []

        for series in updates.findall('Series'):
            expired_series.append(int(series.find("id").text))

        for episode in updates.findall('Episode'):
            expired_series.append(int(episode.find("id").text))

        def chunked(seq):
            """Helper to divide our expired lists into sizes sqlite can handle in a query. (<1000)"""
            for i in xrange(0, len(seq), 900):
                yield seq[i:i + 900]

        # Update our cache to mark the items that have expired
        for chunk in chunked(expired_series):
            num = session.query(TVDBSeries).filter(TVDBSeries.id.in_(chunk)).update({'expired': True}, 'fetch')
            log.debug('%s series marked as expired' % num)
        for chunk in chunked(expired_episodes):
            num = session.query(TVDBEpisode).filter(TVDBEpisode.id.in_(chunk)).update({'expired': True}, 'fetch')
            log.debug('%s episodes marked as expired' % num)

        # Save the time of this update
        persist['last_local'] = datetime.now()
        persist['last_server'] = new_server

########NEW FILE########
__FILENAME__ = api_tvrage
from __future__ import unicode_literals, division, absolute_import
import logging
import datetime
import difflib
from socket import timeout

from sqlalchemy import Column, Integer, DateTime, String, Unicode, ForeignKey, select, update, func
from sqlalchemy.orm import relation
import tvrage.api
import tvrage.feeds

from flexget.event import event
from flexget.utils.database import with_session
from flexget import db_schema
from flexget.utils.database import pipe_list_synonym
from flexget.utils.sqlalchemy_utils import table_schema
from flexget.utils.tools import parse_timedelta

log = logging.getLogger('api_tvrage')

Base = db_schema.versioned_base('tvrage', 2)
UPDATE_INTERVAL = '7 days'


# Monkey patch tvrage library to work around https://github.com/ckreutzer/python-tvrage/pull/8
tvrage.feeds.BASE_URL = 'http://services.tvrage.com/feeds/%s.php?%s=%s'


@event('manager.db_cleanup')
def db_cleanup(session):
    value = datetime.datetime.now() - parse_timedelta('30 days')
    for de in session.query(TVRageSeries).filter(TVRageSeries.last_update <= value).all():
        log.debug('deleting %s' % de)
        session.delete(de)


@db_schema.upgrade('tvrage')
def upgrade(ver, session):
    if ver == 0:
        series_table = table_schema('tvrage_series', session)
        for row in session.execute(select([series_table.c.id, series_table.c.genres])):
            # Recalculate the proper_count from title for old episodes
            new_genres = row['genres']
            if new_genres:
                new_genres = row['genres'].replace(',', '|')
            session.execute(update(series_table, series_table.c.id == row['id'], {'genres': new_genres}))
        ver = 1
    if ver == 1:
        raise db_schema.UpgradeImpossible
    return ver


class TVRageLookup(Base):
    __tablename__ = 'tvrage_lookup'
    id = Column(Integer, primary_key=True)
    name = Column(Unicode, index=True)
    failed_time = Column(DateTime)
    series_id = Column(Integer, ForeignKey('tvrage_series.id'))
    series = relation('TVRageSeries', uselist=False, cascade='all, delete')

    def __init__(self, name, series, **kwargs):
        super(TVRageLookup, self).__init__(**kwargs)
        self.name = name.lower()
        self.series = series


class TVRageSeries(Base):
    __tablename__ = 'tvrage_series'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    episodes = relation('TVRageEpisodes', order_by='TVRageEpisodes.season, TVRageEpisodes.episode',
                        cascade='all, delete, delete-orphan')
    showid = Column(String)
    link = Column(String)
    classification = Column(String)
    _genres = Column('genres', String)
    genres = pipe_list_synonym('_genres')
    country = Column(String)
    started = Column(Integer)
    ended = Column(Integer)
    seasons = Column(Integer)
    last_update = Column(DateTime)  # last time we updated the db for the show

    def __init__(self, series):
        self.update(series)

    def update(self, series):
        self.name = series.name
        self.showid = series.showid
        self.link = series.link
        self.classification = series.classification
        self.genres = [g for g in series.genres if g]  # Sometimes tvrage has a None in the genres list
        self.country = series.country
        self.started = series.started
        self.ended = series.ended
        self.seasons = series.seasons
        self.last_update = datetime.datetime.now()

        # Clear old eps before repopulating
        del self.episodes[:]
        for i in range(1, series.seasons + 1):
            season = series.season(i)
            for j in season.keys():
                episode = TVRageEpisodes(season.episode(j))
                self.episodes.append(episode)

    @with_session
    def find_episode(self, season, episode, session=None):
        return (session.query(TVRageEpisodes).
                filter(TVRageEpisodes.tvrage_series_id == self.id).
                filter(TVRageEpisodes.season == season).
                filter(TVRageEpisodes.episode == episode).first())

    def __str__(self):
        return '<TvrageSeries(title=%s,id=%s,last_update=%s)>' % (self.name, self.id, self.last_update)

    def finished(self):
        return self.ended != 0


class TVRageEpisodes(Base):
    __tablename__ = 'tvrage_episode'
    id = Column(Integer, primary_key=True)
    tvrage_series_id = Column(Integer, ForeignKey('tvrage_series.id'), nullable=False)
    episode = Column(Integer, index=True)
    season = Column(Integer, index=True)
    prodnum = Column(Integer)
    airdate = Column(DateTime)
    link = Column(String)
    title = Column(String)

    def __init__(self, ep):
        self.update(ep)

    def update(self, ep):
        self.episode = ep.number
        self.season = ep.season
        self.prodnum = ep.prodnumber
        self.airdate = ep.airdate
        self.link = ep.link
        self.title = ep.title

    def __str__(self):
        return '<TVRageEpisodes(title=%s,id=%s,season=%s,episode=%s)>' % (self.title, self.id, self.season, self.episode)

    @with_session
    def next(self, session=None):
        """Returns the next episode after this episode"""
        res = session.query(TVRageEpisodes).\
            filter(TVRageEpisodes.tvrage_series_id == self.tvrage_series_id).\
            filter(TVRageEpisodes.season == self.season).\
            filter(TVRageEpisodes.episode == self.episode+1).first()
        if res is not None:
            return res
        return session.query(TVRageEpisodes).\
            filter(TVRageEpisodes.tvrage_series_id == self.tvrage_series_id).\
            filter(TVRageEpisodes.season == self.season+1).\
            filter(TVRageEpisodes.episode == 1).first()


def closest_airdate(series_id, session):
    """Returns the next upcoming show's airdate or last airdate."""
    sq = session.query(TVRageEpisodes).\
         filter(TVRageEpisodes.tvrage_series_id == series_id).\
         filter(TVRageEpisodes.airdate > datetime.datetime.now()).subquery()

    upcoming_episode = session.query(sq).\
                       filter(sq.c.airdate == func.min(sq.c.airdate).select()).first()

    if upcoming_episode is not None:
        return upcoming_episode.airdate

    sq = session.query(TVRageEpisodes).\
         filter(TVRageEpisodes.tvrage_series_id == series_id).\
         filter(TVRageEpisodes.airdate < datetime.datetime.now()).subquery()

    past_episode = session.query(sq).\
                   filter(sq.c.airdate == func.max(sq.c.airdate).select()).first()

    if past_episode is not None:
        return past_episode.airdate

    return datetime.datetime.max


@with_session
def lookup_series(name=None, session=None):
    series = None
    res = session.query(TVRageLookup).filter(TVRageLookup.name == name.lower()).first()

    if res and not res.series:
        # The lookup failed in the past for this series, retry every week
        # TODO: 1.2 this should also retry with --retry or whatever flag imdb lookup is using for that
        if res.failed_time and res.failed_time > datetime.datetime.now() - datetime.timedelta(days=7):
            raise LookupError('Could not find show %s' % name)
    elif res:
        series = res.series

        airdate = closest_airdate(series.id, session)
        now = datetime.datetime.now()
        interval = parse_timedelta(UPDATE_INTERVAL)

        # if too old result or no upcoming result, clean the db and refresh it
        if (series.last_update - datetime.timedelta(days=1)) < airdate < now and \
           now > series.last_update + datetime.timedelta(hours=4):
            # no upcoming episode and last check done before one day after last airdate; adding timedelta
            # because last_update has no time information. Left-hand and statement is here to ensure there is
            # 4 hours between two tvrage lookups
            log.debug('No next episode information for %s; refreshing now', name)
        elif now > series.last_update + interval:
            # too old result, refreshing
            log.debug('Refreshing tvrage info for %s', name)
        else:
            return series

    def store_failed_lookup():
        if res:
            res.series = None
            res.failed_time = datetime.datetime.now()
        else:
            session.add(TVRageLookup(name, None, failed_time=datetime.datetime.now()))
            session.commit()

    log.debug('Fetching tvrage info for %s' % name)
    try:
        fetched = tvrage.api.Show(name.encode('utf-8'))
    except tvrage.exceptions.ShowNotFound:
        store_failed_lookup()
        raise LookupError('Could not find show %s' % name)
    except (timeout, AttributeError):
        # AttributeError is due to a bug in tvrage package trying to access URLError.code
        raise LookupError('Timed out while connecting to tvrage')
    except TypeError:
        # TODO: There should be option to pass tvrage id directly from within series plugin via "set:" (like tvdb_id)
        # and search directly for tvrage id. This is problematic, because 3rd party TVRage API does not support this.
        raise LookupError('Returned invalid data for "%s". This is often caused when TVRage is missing episode info'
                          % name)
    # Make sure the result is close enough to the search
    if difflib.SequenceMatcher(a=name, b=fetched.name).ratio() < 0.7:
        log.debug('Show result `%s` was not a close enough match for `%s`' % (fetched.name, name))
        store_failed_lookup()
        raise LookupError('Could not find show %s' % name)
    if not series:
        series = session.query(TVRageSeries).filter(TVRageSeries.showid == fetched.showid).first()
    if not series:
        series = TVRageSeries(fetched)
        session.add(series)
        session.add(TVRageLookup(unicode(fetched.name), series))
    else:
        series.update(fetched)
    if name.lower() != fetched.name.lower():
        if res:
            res.series = series
        else:
            session.add(TVRageLookup(name, series))
    return series

########NEW FILE########
__FILENAME__ = check
from __future__ import unicode_literals, division, absolute_import
import codecs
import logging

from flexget import options
from flexget.event import event

log = logging.getLogger('check')


@event('manager.before_config_load')
def pre_check_config(manager):
    """Checks configuration file for common mistakes that are easily detectable"""
    if manager.options.cli_command == 'check':
        with codecs.open(manager.config_path, 'r', 'utf-8') as config_file:
            try:
                config = config_file.read()
            except UnicodeDecodeError:
                return

        def get_indentation(line):
            i, n = 0, len(line)
            while i < n and line[i] == ' ':
                i += 1
            return i

        def isodd(n):
            return bool(n % 2)

        line_num = 0
        duplicates = {}
        # flags
        prev_indentation = 0
        prev_mapping = False
        prev_list = True
        prev_scalar = True
        list_open = False  # multiline list with [

        for line in config.splitlines():
            if '# warnings off' in line.strip().lower():
                log.debug('config pre-check warnings off')
                break
            line_num += 1
            # remove linefeed
            line = line.rstrip()
            # empty line
            if line.strip() == '':
                continue
            # comment line
            if line.strip().startswith('#'):
                continue
            indentation = get_indentation(line)

            if prev_scalar:
                if indentation <= prev_indentation:
                    prev_scalar = False
                else:
                    continue

            cur_list = line.strip().startswith('-')

            # skipping lines as long as multiline compact list is open
            if list_open:
                if line.strip().endswith(']'):
                    list_open = False
#                    print 'closed list at line %s' % line
                continue
            else:
                list_open = line.strip().endswith(': [') or line.strip().endswith(':[')
                if list_open:
#                    print 'list open at line %s' % line
                    continue

#            print '#%i: %s' % (line_num, line)
#            print 'indentation: %s, prev_ind: %s, prev_mapping: %s, prev_list: %s, cur_list: %s' % \
#                  (indentation, prev_indentation, prev_mapping, prev_list, cur_list)

            if ':\t' in line:
                log.critical('Line %s has TAB character after : character. '
                             'DO NOT use tab key when editing config!' % line_num)
            elif '\t' in line:
                log.warning('Line %s has tabs, use only spaces!' % line_num)
            if isodd(indentation):
                log.warning('Config line %s has odd (uneven) indentation' % line_num)
            if indentation > prev_indentation and not prev_mapping:
                # line increases indentation, but previous didn't start mapping
                log.warning('Config line %s is likely missing \':\' at the end' % (line_num - 1))
            if indentation > prev_indentation + 2 and prev_mapping and not prev_list:
                # mapping value after non list indented more than 2
                log.warning('Config line %s is indented too much' % line_num)
            if indentation <= prev_indentation + (2 * (not cur_list)) and prev_mapping and prev_list:
                log.warning('Config line %s is not indented enough' % line_num)
            if prev_mapping and cur_list:
                # list after opening mapping
                if indentation < prev_indentation or indentation > prev_indentation + 2 + (2 * prev_list):
                    log.warning('Config line %s containing list element is indented incorrectly' % line_num)
            elif prev_mapping and indentation <= prev_indentation:
                # after opening a map, indentation doesn't increase
                log.warning('Config line %s is indented incorrectly (previous line ends with \':\')' % line_num)

            # notify if user is trying to set same key multiple times in a task (a common mistake)
            for level in duplicates.iterkeys():
                # when indentation goes down, delete everything indented more than that
                if indentation < level:
                    duplicates[level] = {}
            if ':' in line:
                name = line.split(':', 1)[0].strip()
                ns = duplicates.setdefault(indentation, {})
                if name in ns:
                    log.warning('Trying to set value for `%s` in line %s, but it is already defined in line %s!' %
                        (name, line_num, ns[name]))
                ns[name] = line_num

            prev_indentation = indentation
            # this line is a mapping (ends with :)
            prev_mapping = line[-1] == ':'
            prev_scalar = line[-1] in '|>'
            # this line is a list
            prev_list = line.strip()[0] == '-'
            if prev_list:
                # This line is in a list, so clear the duplicates,
                # as duplicates are not always wrong in a list. see #697
                duplicates[indentation] = {}

        log.debug('Pre-checked %s configuration lines' % line_num)

def check(manager, options):
    # If we got here, there aren't any errors. :P
    log.info('Config passed check.')
    manager.shutdown()


@event('options.register')
def register_options():
    options.register_command('check', check, help='validate configuration file and print errors')

########NEW FILE########
__FILENAME__ = cli_config
from __future__ import unicode_literals, division, absolute_import
import argparse
import functools
import logging

from flexget import options
from flexget.event import event

log = logging.getLogger('cli_config')


"""
Allows specifying yml configuration values from commandline parameters.

Yml variables are prefixed with dollar sign ($).
Commandline parameter must be comma separated list of variable=values.

Configuration example::

  tasks:
    my task:
      rss: $url
      download: $path

Commandline example::

  --cli-config url=http://some.url/ path=~/downloads

"""


def replace_in_item(replaces, item):
    replace = functools.partial(replace_in_item, replaces)
    if isinstance(item, basestring):
        # Do replacement in text objects
        for key, val in replaces.iteritems():
            item = item.replace('$%s' % key, val)
        return item
    elif isinstance(item, list):
        # Make a new list with replacements done on each item
        return map(replace, item)
    elif isinstance(item, dict):
        # Make a new dict with replacements done on keys and values
        return dict(map(replace, kv_pair) for kv_pair in item.iteritems())
    else:
        # We don't know how to do replacements on this item, just return it
        return item


@event('manager.before_config_validate')
def substitute_cli_variables(manager):
    if not manager.options.execute.cli_config:
        return
    manager.config = replace_in_item(dict(manager.options.execute.cli_config), manager.config)


def key_value_pair(text):
    if '=' not in text:
        raise argparse.ArgumentTypeError('arguments must be in VARIABLE=VALUE form')
    return text.split('=', 1)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--cli-config', nargs='+', type=key_value_pair, metavar='VARIABLE=VALUE',
                                               help='configuration parameters trough commandline')

########NEW FILE########
__FILENAME__ = database
from __future__ import unicode_literals, division, absolute_import

from flexget import options
from flexget.db_schema import reset_schema, plugin_schemas
from flexget.event import event
from flexget.manager import Base, Session
from flexget.utils.tools import console


def do_cli(manager, options):
    with manager.acquire_lock():
        if options.db_action == 'cleanup':
            cleanup(manager)
        elif options.db_action == 'vacuum':
            vacuum()
        elif options.db_action == 'reset':
            reset(manager)
        elif options.db_action == 'reset-plugin':
            reset_plugin(options)


def cleanup(manager):
    manager.db_cleanup(force=True)
    console('Database cleanup complete.')


def vacuum():
    console('Running VACUUM on sqlite database, this could take a while.')
    session = Session()
    try:
        session.execute('VACUUM')
        session.commit()
    finally:
        session.close()
    console('VACUUM complete.')


def reset(manager):
    Base.metadata.drop_all(bind=manager.engine)
    Base.metadata.create_all(bind=manager.engine)
    console('The FlexGet database has been reset.')


def reset_plugin(options):
    plugin = options.reset_plugin
    if not plugin:
        console('%-20s Ver Tables' % 'Name')
        console('-' * 79)
        for k, v in sorted(plugin_schemas.iteritems()):
            tables = ''
            line_len = 0
            for name in v['tables']:
                if line_len + len(name) + 2 >= 53:
                    tables += '\n'
                    tables += ' ' * 25
                    line_len = len(name) + 2
                else:
                    line_len += len(name) + 2
                tables += name + ', '
            tables = tables.rstrip(', ')
            console('%-20s %s   %s' % (k, v['version'], tables))
    else:
        try:
            reset_schema(plugin)
            console('The database for `%s` has been reset.' % plugin)
        except ValueError as e:
            console('Unable to reset %s: %s' % (plugin, e.message))


@event('options.register')
def register_parser_arguments():
    parser = options.register_command('database', do_cli, help='utilities to manage the FlexGet database')
    subparsers = parser.add_subparsers(title='Actions', metavar='<action>', dest='db_action')
    subparsers.add_parser('cleanup', help='make all plugins clean un-needed data from the database')
    subparsers.add_parser('vacuum', help='running vacuum can increase performance and decrease database size')
    reset_parser = subparsers.add_parser('reset', add_help=False, help='reset the entire database (DANGEROUS!)')
    reset_parser.add_argument('--sure', action='store_true', required=True,
                              help='you must use this flag to indicate you REALLY want to do this')
    reset_plugin_parser = subparsers.add_parser('reset-plugin', help='reset the database for a specific plugin')
    reset_plugin_parser.add_argument('reset_plugin', metavar='<plugin>', nargs='?',
                                 help='name of plugin to reset (if omitted, known plugins will be listed)')

########NEW FILE########
__FILENAME__ = doc
from __future__ import unicode_literals, division, absolute_import, print_function
import logging
import sys

from flexget import options
from flexget.event import event
from flexget.plugin import plugins


log = logging.getLogger('doc')


def trim(docstring):
    if not docstring:
        return ''
    # Convert tabs to spaces (following the normal Python rules)
    # and split into a list of lines:
    lines = docstring.expandtabs().splitlines()
    # Determine minimum indentation (first line doesn't count):
    indent = sys.maxsize
    for line in lines[1:]:
        stripped = line.lstrip()
        if stripped:
            indent = min(indent, len(line) - len(stripped))
    # Remove indentation (first line is special):
    trimmed = [lines[0].strip()]
    if indent < sys.maxsize:
        for line in lines[1:]:
            trimmed.append(line[indent:].rstrip())
    # Strip off trailing and leading blank lines:
    while trimmed and not trimmed[-1]:
        trimmed.pop()
    while trimmed and not trimmed[0]:
        trimmed.pop(0)
    # Return a single string:
    return '\n'.join(trimmed)


def print_doc(manager, options):
    plugin_name = options.doc
    plugin = plugins.get(plugin_name, None)
    if plugin:
        if not plugin.instance.__doc__:
            print('Plugin %s does not have documentation' % plugin_name)
        else:
            print('')
            print(trim(plugin.instance.__doc__))
            print('')
    else:
        print('Could not find plugin %s' % plugin_name)


@event('options.register')
def register_parser_arguments():
    parser = options.register_command('doc', print_doc, help='display plugin documentation')
    parser.add_argument('doc', metavar='<plugin name>', help='name of plugin to show docs for')

########NEW FILE########
__FILENAME__ = explain_sql
from __future__ import unicode_literals, division, absolute_import
import logging
from time import time
from argparse import SUPPRESS

from sqlalchemy.orm.query import Query
from sqlalchemy.ext.compiler import compiles
from sqlalchemy.sql.expression import Executable, ClauseElement, _literal_as_text

from flexget import manager, options
from flexget.event import event

log = logging.getLogger('explain_sql')


class Explain(Executable, ClauseElement):

    def __init__(self, stmt):
        self.statement = _literal_as_text(stmt)


@compiles(Explain)
def explain(element, compiler, **kw):
    text = 'EXPLAIN QUERY PLAN ' + compiler.process(element.statement)
    return text


class ExplainQuery(Query):

    def __iter__(self):
        log.info('Query:\n\t%s' % unicode(self).replace('\n', '\n\t'))
        explain = self.session.execute(Explain(self)).fetchall()
        text = '\n\t'.join('|'.join(str(x) for x in line) for line in explain)
        before = time()
        result = Query.__iter__(self)
        log.info('Query Time: %0.3f Explain Query Plan:\n\t%s' % (time() - before, text))
        return result


@event('manager.execute.started')
def register_sql_explain(man):
    if man.options.execute.explain_sql:
        manager.Session.kw['query_cls'] = ExplainQuery


@event('manager.execute.completed')
def deregister_sql_explain(man):
    if man.options.execute.explain_sql:
        manager.Session.kw.pop('query_cls', None)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--explain-sql', action='store_true', dest='explain_sql',
                                               default=False, help=SUPPRESS)

########NEW FILE########
__FILENAME__ = inject
from __future__ import unicode_literals, division, absolute_import
import argparse
import string
import random
import yaml

from flexget import options
from flexget.entry import Entry
from flexget.event import event


@event('manager.subcommand.inject')
def do_cli(manager, options):
    entry = Entry(title=options.title)
    if options.url:
        entry['url'] = options.url
    else:
        entry['url'] = 'http://localhost/inject/%s' % ''.join(random.sample(string.letters + string.digits, 30))
    if options.force:
        entry['immortal'] = True
    if options.accept:
        entry.accept(reason='accepted by CLI inject')
    exec_options = dict(options.execute)
    exec_options['inject'] = [entry]
    manager.execute_command(exec_options)


def key_equals_value(text):
    if not '=' in text:
        raise argparse.ArgumentTypeError('must be in the form: <field name>=<value>')
    key, value = text.split('=')
    return key, yaml.safe_load(value)


# Run after other plugins, so we can get all exec subcommand options
@event('options.register', priority=0)
def register_parser_arguments():
    exec_parser = options.get_parser('execute')
    inject_parser = options.register_command('inject', do_cli, add_help=False, parents=[exec_parser],
                                             help='inject an entry from command line into tasks',
                                             usage='%(prog)s title [url] [--accept] [--force] '
                                                   '[--fields NAME=VALUE [NAME=VALUE...]] [<execute arguments>]')
    inject_group = inject_parser.add_argument_group('inject arguments')
    inject_group.add_argument('title', help='title of the entry to inject')
    inject_group.add_argument('url', nargs='?', help='url of the entry to inject')
    inject_group.add_argument('--force', action='store_true', help='prevent any plugins from rejecting this entry')
    inject_group.add_argument('--accept', action='store_true', help='accept this entry immediately upon injection')
    inject_group.add_argument('--fields', metavar='NAME=VALUE', nargs='+', type=key_equals_value)
    # Hack the title of the exec options a bit (would be 'optional arguments' otherwise)
    inject_parser._action_groups[1].title = 'execute arguments'
    # The exec arguments show first... unless we switch them
    inject_parser._action_groups.remove(inject_group)
    inject_parser._action_groups.insert(0, inject_group)

########NEW FILE########
__FILENAME__ = movie_queue
from __future__ import unicode_literals, division, absolute_import
from argparse import ArgumentParser

from sqlalchemy.exc import OperationalError

from flexget import options
from flexget.event import event
from flexget.plugin import DependencyError
from flexget.utils import qualities
from flexget.utils.tools import console

try:
    from flexget.plugins.filter.movie_queue import QueueError, queue_add, queue_del, queue_get, queue_forget, parse_what
except ImportError:
    raise DependencyError(issued_by='cli_movie_queue', missing='movie_queue')


def do_cli(manager, options):
    """Handle movie-queue subcommand"""

    if options.queue_action == 'list':
        queue_list(options)
        return

    # If the action affects make sure all entries are processed again next run.
    manager.config_changed()

    if options.queue_action == 'clear':
        clear()
        return

    if options.queue_action == 'del':
        try:
            what = parse_what(options.movie_name, lookup=False)
            title = queue_del(**what)
        except QueueError as e:
            console('ERROR: %s' % e.message)
        else:
            console('Removed %s from queue' % title)
        return

    if options.queue_action == 'forget':
        try:
            what = parse_what(options.movie_name, lookup=False)
            title = queue_forget(**what)
        except QueueError as e:
            console('ERROR: %s' % e.message)
        else:
            console('Forgot that %s was downloaded. Movie will be downloaded again.' % title)
        return

    if options.queue_action == 'add':
        try:
            quality = qualities.Requirements(options.quality)
        except ValueError as e:
            console('`%s` is an invalid quality requirement string: %s' % (options.quality, e.message))
            return

        # Adding to queue requires a lookup for missing information
        what = {}
        try:
            what = parse_what(options.movie_name)
        except QueueError as e:
            console('ERROR: %s' % e.message)

        if not what.get('title') or not (what.get('imdb_id') or what.get('tmdb_id')):
            console('could not determine movie')  # TODO: Rethink errors
            return

        try:
            queue_add(quality=quality, **what)
        except QueueError as e:
            console(e.message)
            if e.errno == 1:
                # This is an invalid quality error, display some more info
                # TODO: Fix this error?
                #console('Recognized qualities are %s' % ', '.join([qual.name for qual in qualities.all()]))
                console('ANY is the default and can also be used explicitly to specify that quality should be ignored.')
        except OperationalError:
            console('OperationalError')
        return


def queue_list(options):
    """List movie queue"""
    items = queue_get(downloaded=(options.type == 'downloaded'))
    console('-' * 79)
    console('%-10s %-7s %-37s %s' % ('IMDB id', 'TMDB id', 'Title', 'Quality'))
    console('-' * 79)
    for item in items:
        console('%-10s %-7s %-37s %s' % (item.imdb_id, item.tmdb_id, item.title, item.quality))
    if not items:
        console('No results')
    console('-' * 79)


def clear():
    """Deletes waiting movies from queue"""
    items = queue_get(downloaded=False)
    console('Removing the following movies from movie queue:')
    console('-' * 79)
    for item in items:
        console(item.title)
        queue_del(title=item.title)
    if not items:
        console('No results')
    console('-' * 79)

@event('options.register')
def register_parser_arguments():
    # Common option to be used in multiple subparsers
    what_parser = ArgumentParser(add_help=False)
    what_parser.add_argument('movie_name', metavar='<movie>',
                             help='the movie (can be movie title, imdb id, or in the form `tmdb_id=XXXX`')
    # Register subcommand
    parser = options.register_command('movie-queue', do_cli, help='view and manage the movie queue')
    # Set up our subparsers
    subparsers = parser.add_subparsers(title='actions', metavar='<action>', dest='queue_action')
    list_parser = subparsers.add_parser('list', help='list movies from the queue')
    list_parser.add_argument('type', nargs='?', choices=['waiting', 'downloaded'], default='waiting',
                             help='choose to show waiting or already downloaded movies')
    add_parser = subparsers.add_parser('add', parents=[what_parser], help='add a movie to the queue')
    add_parser.add_argument('quality', metavar='<quality>', default='ANY', nargs='?',
                            help='the quality requirements for getting this movie (default: %(default)s)')
    subparsers.add_parser('del', parents=[what_parser], help='remove a movie from the queue')
    subparsers.add_parser('forget', parents=[what_parser], help='remove the downloaded flag from a movie')
    subparsers.add_parser('clear', help='remove all un-downloaded movies from the queue')

########NEW FILE########
__FILENAME__ = performance
from __future__ import unicode_literals, division, absolute_import
import logging

from argparse import SUPPRESS

from flexget import options
from flexget.event import event

log = logging.getLogger('performance')

performance = {}

_start = {}

query_count = 0


def log_query_count(name_point):
    """Debugging purposes, allows logging number of executed queries at :name_point:"""
    log.info('At point named `%s` total of %s queries were ran' % (name_point, query_count))


@event('manager.execute.started')
def startup(manager):
    if manager.options.execute.debug_perf:
        log.info('Enabling plugin and SQLAlchemy performance debugging')
        import time

        # Monkeypatch query counter for SQLAlchemy
        from sqlalchemy.engine import Connection
        if hasattr(Connection, 'execute'):
            orig_f = Connection.execute

            def monkeypatched(*args, **kwargs):
                global query_count
                query_count += 1
                return orig_f(*args, **kwargs)

            Connection.execute = monkeypatched
        else:
            log.critical('Unable to monkeypatch sqlalchemy')

        @event('task.execute.before_plugin')
        def before(task, keyword):
            fd = _start.setdefault(task.name, {})
            fd.setdefault('time', {})[keyword] = time.time()
            fd.setdefault('queries', {})[keyword] = query_count

        @event('task.execute.after_plugin')
        def after(task, keyword):
            took = time.time() - _start[task.name]['time'][keyword]
            queries = query_count - _start[task.name]['queries'][keyword]
            # Store results, increases previous values
            pd = performance.setdefault(task.name, {})
            data = pd.setdefault(keyword, {})
            data['took'] = data.get('took', 0) + took
            data['queries'] = data.get('queries', 0) + queries

        @event('manager.execute.completed')
        def results(manager):
            for name, data in performance.iteritems():
                log.info('Performance results for task %s:' % name)
                for keyword, results in data.iteritems():
                    took = results['took']
                    queries = results['queries']
                    if took > 0.1 or queries > 10:
                        log.info('%-15s took %0.2f sec (%s queries)' % (keyword, took, queries))


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--debug-perf', action='store_true', dest='debug_perf', default=False,
                                               help=SUPPRESS)

########NEW FILE########
__FILENAME__ = perf_tests
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import options
from flexget.event import event
from flexget.manager import Session
from flexget.utils.tools import console

log = logging.getLogger('perftests')

TESTS = ['imdb_query']


def cli_perf_test(manager, options):
    if options.test_name not in TESTS:
        console('Unknown performance test %s' % options.test_name)
        return
    session = Session()
    try:
        if options.test_name == 'imdb_query':
            imdb_query(session)
    finally:
        session.close()


def imdb_query(session):
    import time
    from flexget.plugins.metainfo.imdb_lookup import Movie
    from flexget.plugins.cli.performance import log_query_count
    from sqlalchemy.sql.expression import select
    from progressbar import ProgressBar, Percentage, Bar, ETA
    from sqlalchemy.orm import joinedload_all

    imdb_urls = []

    log.info('Getting imdb_urls ...')
    # query so that we avoid loading whole object (maybe cached?)
    for id, url in session.execute(select([Movie.id, Movie.url])):
        imdb_urls.append(url)
    log.info('Got %i urls from database' % len(imdb_urls))
    if not imdb_urls:
        log.info('so .. aborting')
        return

    # commence testing

    widgets = ['Benchmarking - ', ETA(), ' ', Percentage(), ' ', Bar(left='[', right=']')]
    bar = ProgressBar(widgets=widgets, maxval=len(imdb_urls)).start()

    log_query_count('test')
    start_time = time.time()
    for index, url in enumerate(imdb_urls):
        bar.update(index)

        #movie = session.query(Movie).filter(Movie.url == url).first()
        #movie = session.query(Movie).options(subqueryload(Movie.genres)).filter(Movie.url == url).one()

        movie = session.query(Movie).\
            options(joinedload_all(Movie.genres, Movie.languages,
            Movie.actors, Movie.directors)).\
            filter(Movie.url == url).first()

        # access it's members so they're loaded
        var = [x.name for x in movie.genres]
        var = [x.name for x in movie.directors]
        var = [x.name for x in movie.actors]
        var = [x.name for x in movie.languages]

    log_query_count('test')
    took = time.time() - start_time
    log.debug('Took %.2f seconds to query %i movies' % (took, len(imdb_urls)))


@event('options.register')
def register_parser_arguments():
    perf_parser = options.register_command('perf-test', cli_perf_test)
    perf_parser.add_argument('test_name', metavar='<test name>', choices=TESTS)

########NEW FILE########
__FILENAME__ = plugins
from __future__ import unicode_literals, division, absolute_import, print_function
import logging

from flexget import options
from flexget.event import event
from flexget.plugin import get_plugins

log = logging.getLogger('plugins')


@event('manager.subcommand.plugins')
def plugins_summary(manager, options):
    print('-' * 79)
    print('%-20s%-30s%s' % ('Name', 'Roles (priority)', 'Info'))
    print('-' * 79)

    # print the list
    for plugin in sorted(get_plugins(phase=options.phase, group=options.group)):
        # do not include test classes, unless in debug mode
        if plugin.get('debug_plugin', False) and not options.debug:
            continue
        flags = []
        if plugin.instance.__doc__:
            flags.append('--doc')
        if plugin.builtin:
            flags.append('builtin')
        if plugin.debug:
            flags.append('debug')
        handlers = plugin.phase_handlers
        roles = ', '.join('%s(%s)' % (phase, handlers[phase].priority) for phase in handlers)
        print('%-20s%-30s%s' % (plugin.name, roles, ', '.join(flags)))

    print('-' * 79)


@event('options.register')
def register_parser_arguments():
    plugins_subparser = options.register_command('plugins', plugins_summary, help='print registered plugin summaries')
    plugins_subparser.add_argument('--group', help='show plugins belonging to this group')
    plugins_subparser.add_argument('--phase', help='show plugins that act on this phase')

########NEW FILE########
__FILENAME__ = series
from __future__ import unicode_literals, division, absolute_import
import argparse
from datetime import datetime, timedelta
from sqlalchemy import func

from flexget import options, plugin
from flexget.event import event
from flexget.manager import Session
from flexget.utils.tools import console

try:
    from flexget.plugins.filter.series import (Series, Episode, Release, SeriesTask, forget_series,
                                               forget_series_episode, set_series_begin, normalize_series_name,
                                               new_eps_after, get_latest_release)
except ImportError:
    raise plugin.DependencyError(issued_by='cli_series', missing='series',
                                 message='Series commandline interface not loaded')


def do_cli(manager, options):
    if options.series_action == 'list':
        display_summary(options)
    elif options.series_action == 'show':
        display_details(options.series_name)
    elif options.series_action == 'forget':
        forget(manager, options)
    elif options.series_action == 'begin':
        begin(manager, options)


def display_summary(options):
    """
    Display series summary.
    :param options: argparse options from the CLI
    """
    formatting = ' %-30s %-10s %-10s %-20s'
    console(formatting % ('Name', 'Latest', 'Age', 'Downloaded'))
    console('-' * 79)

    session = Session()
    try:
        query = (session.query(Series).outerjoin(Series.episodes).outerjoin(Episode.releases).
                 outerjoin(Series.in_tasks).group_by(Series.id))
        if options.configured == 'configured':
            query = query.having(func.count(SeriesTask.id) >= 1)
        elif options.configured == 'unconfigured':
            query = query.having(func.count(SeriesTask.id) < 1)
        if options.premieres:
            query = (query.having(func.max(Episode.season) <= 1).having(func.max(Episode.number) <= 2).
                     having(func.count(SeriesTask.id) < 1)).filter(Release.downloaded == True)
        if options.new:
            query = query.having(func.max(Episode.first_seen) > datetime.now() - timedelta(days=options.new))
        if options.stale:
            query = query.having(func.max(Episode.first_seen) < datetime.now() - timedelta(days=options.stale))
        for series in query.order_by(Series.name).yield_per(10):
            series_name = series.name
            if len(series_name) > 30:
                series_name = series_name[:27] + '...'

            new_ep = ' '
            behind = 0
            status = 'N/A'
            age = 'N/A'
            episode_id = 'N/A'
            latest = get_latest_release(series)
            if latest:
                if latest.first_seen > datetime.now() - timedelta(days=2):
                    new_ep = '>'
                behind = new_eps_after(latest)
                status = get_latest_status(latest)
                age = latest.age
                episode_id = latest.identifier

            if behind:
                episode_id += ' +%s' % behind

            console(new_ep + formatting[1:] % (series_name, episode_id, age, status))
            if behind >= 3:
                console(' ! Latest download is %d episodes behind, this may require '
                        'manual intervention' % behind)

        console('-' * 79)
        console(' > = new episode ')
        console(' Use `flexget series show NAME` to get detailed information')
    finally:
        session.close()


def begin(manager, options):
    series_name = options.series_name
    ep_id = options.episode_id
    session = Session()
    try:
        series = session.query(Series).filter(Series.name == series_name).first()
        if not series:
            console('Series not yet in database, adding `%s`' % series_name)
            series = Series()
            series.name = series_name
            session.add(series)
        try:
            set_series_begin(series, ep_id)
        except ValueError as e:
            console(e)
        else:
            console('Episodes for `%s` will be accepted starting with `%s`' % (series.name, ep_id))
            session.commit()
    finally:
        session.close()
    manager.config_changed()


def forget(manager, options):
    name = options.series_name

    if options.episode_id:
        # remove by id
        identifier = options.episode_id
        try:
            forget_series_episode(name, identifier)
            console('Removed episode `%s` from series `%s`.' % (identifier, name.capitalize()))
        except ValueError:
            # Try upper casing identifier if we fail at first
            try:
                forget_series_episode(name, identifier.upper())
                console('Removed episode `%s` from series `%s`.' % (identifier, name.capitalize()))
            except ValueError as e:
                console(e.message)
    else:
        # remove whole series
        try:
            forget_series(name)
            console('Removed series `%s` from database.' % name.capitalize())
        except ValueError as e:
            console(e.message)

    manager.config_changed()


def get_latest_status(episode):
    """
    :param episode: Instance of Episode
    :return: Status string for given episode
    """
    status = ''
    for release in sorted(episode.releases, key=lambda r: r.quality):
        if not release.downloaded:
            continue
        status += release.quality.name
        if release.proper_count > 0:
            status += '-proper'
            if release.proper_count > 1:
                status += str(release.proper_count)
        status += ', '
    return status.rstrip(', ') if status else None


def display_details(name):
    """Display detailed series information, ie. series show NAME"""

    from flexget.manager import Session
    session = Session()

    name = normalize_series_name(name)
    # Sort by length of name, so that partial matches always show shortest matching title
    matches = (session.query(Series).filter(Series._name_normalized.contains(name)).
               order_by(func.char_length(Series.name)).all())
    if not matches:
        console('ERROR: Unknown series `%s`' % name)
        return
    # Pick the best matching series
    series = matches[0]
    console('Showing results for `%s`.' % series.name)
    if len(matches) > 1:
        console('WARNING: Multiple series match to `%s`.' % name)
        console('Be more specific to see the results of other matches:')
        for s in matches[1:]:
            console(' - %s' % s.name)

    console(' %-63s%-15s' % ('Identifier, Title', 'Quality'))
    console('-' * 79)

    # Query episodes in sane order instead of iterating from series.episodes
    episodes = session.query(Episode).filter(Episode.series_id == series.id)
    if series.identified_by == 'sequence':
        episodes = episodes.order_by(Episode.number).all()
    elif series.identified_by == 'ep':
        episodes = episodes.order_by(Episode.season, Episode.number).all()
    else:
        episodes = episodes.order_by(Episode.identifier).all()

    for episode in episodes:

        if episode.identifier is None:
            console(' None <--- Broken!')
        else:
            console(' %s (%s) - %s' % (episode.identifier, episode.identified_by or 'N/A', episode.age))

        for release in episode.releases:
            status = release.quality.name
            title = release.title
            if len(title) > 55:
                title = title[:55] + '...'
            if release.proper_count > 0:
                status += '-proper'
                if release.proper_count > 1:
                    status += str(release.proper_count)
            if release.downloaded:
                console('  * %-60s%-15s' % (title, status))
            else:
                console('    %-60s%-15s' % (title, status))

    console('-' * 79)
    console(' * = downloaded')
    if not series.identified_by:
        console('')
        console(' Series plugin is still learning which episode numbering mode is ')
        console(' correct for this series (identified_by: auto).')
        console(' Few duplicate downloads can happen with different numbering schemes')
        console(' during this time.')
    else:
        console(' Series uses `%s` mode to identify episode numbering (identified_by).' % series.identified_by)
    console(' See option `identified_by` for more information.')
    if series.begin:
        console(' Begin episode for this series set to `%s`.' % series.begin.identifier)
    session.close()


@event('options.register')
def register_parser_arguments():
    # Register the command
    parser = options.register_command('series', do_cli, help='view and manipulate the series plugin database')

    # Parent parser for subcommands that need a series name
    series_parser = argparse.ArgumentParser(add_help=False)
    series_parser.add_argument('series_name', help='the name of the series', metavar='<series name>')

    # Set up our subparsers
    subparsers = parser.add_subparsers(title='actions', metavar='<action>', dest='series_action')
    list_parser = subparsers.add_parser('list', help='list a summary of the different series being tracked')
    list_parser.add_argument('configured', nargs='?', choices=['configured', 'unconfigured', 'all'],
                             default='configured',
                             help='limit list to series that are currently in the config or not (default: %(default)s)')
    list_parser.add_argument('--premieres', action='store_true',
                             help='limit list to series which only have episode 1 (and maybe also 2) downloaded')
    list_parser.add_argument('--new', nargs='?', type=int, metavar='DAYS', const=7,
                             help='limit list to series with a release seen in last %(const)s days. number of days can '
                                  'be overridden with %(metavar)s')
    list_parser.add_argument('--stale', nargs='?', type=int, metavar='DAYS', const=365,
                             help='limit list to series which have not seen a release in %(const)s days. number of '
                                  'days can be overridden with %(metavar)s')
    show_parser = subparsers.add_parser('show', parents=[series_parser],
                                        help='show the releases FlexGet has seen for a given series ')
    begin_parser = subparsers.add_parser('begin', parents=[series_parser],
                                         help='set the episode to start getting a series from')
    begin_parser.add_argument('episode_id', metavar='<episode ID>',
                              help='episode ID to start getting the series from (e.g. S02E01, 2013-12-11, or 9, '
                                   'depending on how the series is numbered)')
    forget_parser = subparsers.add_parser('forget', parents=[series_parser],
                                          help='removes episodes or whole series from the series database')
    forget_parser.add_argument('episode_id', nargs='?', default=None, help='episode ID to forget (optional)')




########NEW FILE########
__FILENAME__ = win32_service
import argparse
import logging
import os
import socket
import sys

import flexget
from flexget import options
from flexget.event import event
from flexget.utils.tools import console

log = logging.getLogger('win32_service')


try:
    import servicemanager
    import win32event
    import win32service
    import win32serviceutil


    class AppServerSvc (win32serviceutil.ServiceFramework):
        _svc_name_ = 'FlexGet'
        _svc_display_name_ = 'FlexGet Daemon'
        _svc_description_ = 'Runs FlexGet tasks according to defined schedules'

        def __init__(self, args):
            win32serviceutil.ServiceFramework.__init__(self, args)
            self.hWaitStop = win32event.CreateEvent(None, 0, 0, None)
            socket.setdefaulttimeout(60)
            self.manager = None

        def SvcStop(self):
            self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
            from flexget.manager import manager
            manager.shutdown(finish_queue=False)
            self.ReportServiceStatus(win32service.SERVICE_STOPPED)

        def SvcDoRun(self):
            servicemanager.LogMsg(servicemanager.EVENTLOG_INFORMATION_TYPE,
                                  servicemanager.PYS_SERVICE_STARTED,
                                  (self._svc_name_, ''))

            flexget.main(['daemon', 'start'])

except ImportError:
    pass


def do_cli(manager, options):
    import win32file
    import win32serviceutil

    if hasattr(sys, 'real_prefix'):
        # We are in a virtualenv, there is some special setup
        if not os.path.exists(os.path.join(sys.prefix, 'python.exe')):
            console('Creating a hard link to virtualenv python.exe in root of virtualenv')
            win32file.CreateHardLink(os.path.join(sys.prefix, 'python.exe'),
                                     os.path.join(sys.prefix, 'Scripts', 'python.exe'))

    argv = options.args
    if options.help:
        argv = []

    # Hack sys.argv a bit so that we get a better usage message
    sys.argv[0] = 'flexget service'
    win32serviceutil.HandleCommandLine(AppServerSvc, argv=['flexget service'] + argv)


@event('options.register')
def register_parser_arguments():
    if not sys.platform.startswith('win'):
        return
    # Still not fully working. Hidden for now.
    parser = options.register_command('service', do_cli,  #help='set up or control a windows service for the daemon',
                                      add_help=False)
    parser.add_argument('--help', '-h', action='store_true')
    parser.add_argument('args', nargs=argparse.REMAINDER)

########NEW FILE########
__FILENAME__ = est_released
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('est_released')


class EstimateRelease(object):

    """
    Front-end for estimator plugins that estimate release times
    for various things (series, movies).
    """

    def estimate(self, entry):
        """
        Estimate release schedule for Entry

        :param entry:
        :return: estimated date of released for the entry, None if it can't figure it out
        """

        log.debug(entry['title'])
        estimators = [e.instance.estimate for e in plugin.get_plugins(group='estimate_release')]
        for estimator in sorted(
                estimators, key=lambda e: getattr(e, 'priority', plugin.DEFAULT_PRIORITY), reverse=True
        ):
            estimate = estimator(entry)
            # return first successful estimation
            if estimate is not None:
                return estimate

@event('plugin.register')
def register_plugin():
    plugin.register(EstimateRelease, 'estimate_release', api_ver=2)

########NEW FILE########
__FILENAME__ = est_released_movies
from __future__ import unicode_literals, division, absolute_import
from datetime import datetime
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('est_movies')


class EstimatesReleasedMovies(object):

    def estimate(self, entry):
        if 'tmdb_released' in entry:
            log.verbose('Querying release estimation for %s' % entry['title'])
            return entry['tmdb_released']
        elif 'movie_year' in entry:
            return datetime(year=entry['movie_year'], month=1, day=1)
        else:
            log.debug('Unable to check release for %s, tmdb_release field is not defined' %
                      entry['title'])

@event('plugin.register')
def register_plugin():
    plugin.register(EstimatesReleasedMovies, 'est_released_movies', groups=['estimate_release'], api_ver=2)

########NEW FILE########
__FILENAME__ = est_released_series
from __future__ import unicode_literals, division, absolute_import
from datetime import timedelta, datetime
import contextlib
import logging

from sqlalchemy import desc, func

from flexget import plugin
from flexget.event import event
from flexget.manager import Session
from flexget.utils.tools import multiply_timedelta
try:
    from flexget.plugins.api_tvrage import lookup_series
    api_tvrage = True
except ImportError as e:
    api_tvrage = False
try:
    from flexget.plugins.filter.series import Series, Episode
except ImportError:
    raise plugin.DependencyError(issued_by='est_released_series', missing='series plugin', silent=True)

log = logging.getLogger('est_series')


class EstimatesReleasedSeries(object):

    @plugin.priority(0)  # Run only if better online lookups fail
    def estimate(self, entry):
        if all(field in entry for field in ['series_name', 'series_season', 'series_episode']):
            # Try to get airdate from tvrage first
            if api_tvrage:
                season = entry['series_season']
                if entry.get('series_id_type') == 'sequence':
                    # Tvrage has absolute numbered shows under season 1
                    season = 1
                log.debug("Querying release estimation for %s S%02dE%02d ..." %
                          (entry['series_name'], season, entry['series_episode']))
                try:
                    series_info = lookup_series(name=entry['series_name'])
                except LookupError as e:
                    log.debug('tvrage lookup error: %s' % e)
                else:
                    if series_info:
                        try:
                            episode_info = series_info.find_episode(season, entry['series_episode'])
                            if episode_info:
                                return episode_info.airdate
                            else:
                                # If episode does not exist in tvrage database, we always return a future date
                                log.verbose('%s S%02dE%02d does not exist in tvrage database, assuming unreleased',
                                          series_info.name, season, entry['series_episode'])
                                return datetime.now() + timedelta(weeks=4)
                        except Exception as e:
                            log.exception(e)
                    else:
                        log.debug('No series info obtained from TVRage to %s' % entry['series_name'])

                log.debug('No episode info obtained from TVRage for %s season %s episode %s' %
                          (entry['series_name'], entry['series_season'], entry['series_episode']))

            # If no results from tvrage, estimate a date based on series history
            with contextlib.closing(Session()) as session:
                series = session.query(Series).filter(Series.name == entry['series_name']).first()
                if not series:
                    return
                episodes = (session.query(Episode).join(Episode.series).
                            filter(Episode.season != None).
                            filter(Series.id == series.id).
                            filter(Episode.season == func.max(Episode.season).select()).
                            order_by(desc(Episode.number)).limit(2).all())

                if len(episodes) < 2:
                    return
                # If last two eps were not contiguous, don't guess
                if episodes[0].number != episodes[1].number + 1:
                    return
                last_diff = episodes[0].first_seen - episodes[1].first_seen
                # If last eps were grabbed close together, we might be catching up, don't guess
                # Or, if last eps were too far apart, don't guess
                # TODO: What range?
                if last_diff < timedelta(days=2) or last_diff > timedelta(days=10):
                    return
                # Estimate next season somewhat more than a normal episode break
                if entry['series_season'] > episodes[0].season:
                    # TODO: How big should this be?
                    return episodes[0].first_seen + multiply_timedelta(last_diff, 2)
                # Estimate next episode comes out about same length as last ep span, with a little leeway
                return episodes[0].first_seen + multiply_timedelta(last_diff, 0.9)


@event('plugin.register')
def register_plugin():
    plugin.register(EstimatesReleasedSeries, 'est_released_series', groups=['estimate_release'], api_ver=2)

########NEW FILE########
__FILENAME__ = accept_all
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('accept_all')


class FilterAcceptAll(object):
    """
        Just accepts all entries.

        Example::

          accept_all: true
    """

    schema = {'type': 'boolean'}

    def on_task_filter(self, task, config):
        if config:
            for entry in task.entries:
                entry.accept()

@event('plugin.register')
def register_plugin():
    plugin.register(FilterAcceptAll, 'accept_all', api_ver=2)

########NEW FILE########
__FILENAME__ = all_series
from __future__ import unicode_literals, division, absolute_import

from flexget import plugin
from flexget.event import event
from flexget.plugins.filter.series import FilterSeriesBase, normalize_series_name


class FilterAllSeries(FilterSeriesBase):
    """
    Grabs all entries that appear to be series episodes in a task.

    This plugin just configures the series plugin dynamically with all series from the task.
    It can take any of the options of the series plugin.

    Example::
      all_series: yes

    ::
      all_series:
        quality: hdtv+
        propers: no
    """

    @property
    def schema(self):
        return {'oneOf': [{'type': 'boolean'}, self.settings_schema]}

    # Run after series and metainfo series plugins
    @plugin.priority(115)
    def on_task_metainfo(self, task, config):
        if not config:
            # Don't run when we are disabled
            return
        if task.is_rerun:
            # Since we are running after task start phase, make sure not to merge into the config again on reruns
            return
        # Generate the group settings for series plugin
        group_settings = {}
        if isinstance(config, dict):
            group_settings = config
        group_settings['identified_by'] = 'ep'
        # Generate a list of unique series that metainfo_series can parse for this task
        metainfo_series = plugin.get_plugin_by_name('metainfo_series')
        guess_entry = metainfo_series.instance.guess_entry
        guessed_series = {}
        for entry in task.entries:
            if guess_entry(entry):
                guessed_series.setdefault(normalize_series_name(entry['series_name']), entry['series_name'])
        # Combine settings and series into series plugin config format
        allseries = {'settings': {'all_series': group_settings}, 'all_series': guessed_series.values()}
        # Merge our config in to the main series config
        self.merge_config(task, allseries)


@event('plugin.register')
def register_plugin():
    plugin.register(FilterAllSeries, 'all_series', api_ver=2)

########NEW FILE########
__FILENAME__ = content_filter
from __future__ import unicode_literals, division, absolute_import
import logging
import posixpath
from fnmatch import fnmatch

from flexget import plugin
from flexget.event import event
from flexget.config_schema import one_or_more

log = logging.getLogger('content_filter')


class FilterContentFilter(object):
    """
    Rejects entries based on the filenames in the content. Torrent files only right now.

    Example::

      content_filter:
        require:
          - '*.avi'
          - '*.mkv'
    """

    schema = {
        'type': 'object',
        'properties': {
            # These two properties allow a string or list of strings
            'require': one_or_more({'type': 'string'}),
            'require_all': one_or_more({'type': 'string'}),
            'reject': one_or_more({'type': 'string'}),
            'require_mainfile': {'type': 'boolean', 'default': False},
            'strict': {'type': 'boolean', 'default': False}
        },
        'additionalProperties': False
    }

    def prepare_config(self, config):
        for key in ['require', 'require_all', 'reject']:
            if key in config:
                if isinstance(config[key], basestring):
                    config[key] = [config[key]]
        return config

    def process_entry(self, task, entry, config):
        """
        Process an entry and reject it if it doesn't pass filter.

        :param task: Task entry belongs to.
        :param entry: Entry to process
        :return: True, if entry was rejected.
        """
        if 'content_files' in entry:
            files = entry['content_files']
            log.debug('%s files: %s' % (entry['title'], files))

            def matching_mask(files, masks):
                """Returns matching mask if any files match any of the masks, false otherwise"""
                for file in files:
                    for mask in masks:
                        if fnmatch(file, mask):
                            return mask
                return False

            # Avoid confusion by printing a reject message to info log, as
            # download plugin has already printed a downloading message.
            if config.get('require'):
                if not matching_mask(files, config['require']):
                    log.info('Entry %s does not have any of the required filetypes, rejecting' % entry['title'])
                    entry.reject('does not have any of the required filetypes', remember=True)
                    return True
            if config.get('require_all'):
                # Make sure each mask matches at least one of the contained files
                if not all(any(fnmatch(file, mask) for file in files) for mask in config['require_all']):
                    log.info('Entry %s does not have all of the required filetypes, rejecting' % entry['title'])
                    entry.reject('does not have all of the required filetypes', remember=True)
                    return True
            if config.get('reject'):
                mask = matching_mask(files, config['reject'])
                if mask:
                    log.info('Entry %s has banned file %s, rejecting' % (entry['title'], mask))
                    entry.reject('has banned file %s' % mask, remember=True)
                    return True
            if config.get('require_mainfile') and len(files) > 1:
                best = None
                for f in entry['torrent'].get_filelist():
                    if not best or f['size'] > best:
                        best = f['size']
                if (100*float(best)/float(entry['torrent'].size)) < 90:
                    log.info('Entry %s does not have a main file, rejecting' % (entry['title']))
                    entry.reject('does not have a main file', remember=True)
                    return True

    def parse_torrent_files(self, entry):
        if 'torrent' in entry:
            files = [posixpath.join(item['path'], item['name']) for item in entry['torrent'].get_filelist()]
            if files:
                # TODO: should not add this to entry, this is a filter plugin
                entry['content_files'] = files

    @plugin.priority(150)
    def on_task_modify(self, task, config):
        if task.options.test or task.options.learn:
            log.info('Plugin is partially disabled with --test and --learn because content filename information may not be available')
            #return

        config = self.prepare_config(config)
        for entry in task.accepted:
            # TODO: I don't know if we can parse filenames from nzbs, just do torrents for now
            # possibly also do compressed files in the future
            self.parse_torrent_files(entry)
            if self.process_entry(task, entry, config):
                task.rerun()
            elif not 'content_files' in entry and config.get('strict'):
                entry.reject('no content files parsed for entry', remember=True)
                task.rerun()

@event('plugin.register')
def register_plugin():
    plugin.register(FilterContentFilter, 'content_filter', api_ver=2)

########NEW FILE########
__FILENAME__ = content_size
from __future__ import unicode_literals, division, absolute_import
import logging
from sys import maxsize

from flexget import plugin
from flexget.event import event
from flexget.utils.log import log_once

log = logging.getLogger('content_size')


class FilterContentSize(object):

    schema = {
        'type': 'object',
        'properties': {
            'min': {'type': 'number'},
            'max': {'type': 'number'},
            'strict': {'type': 'boolean', 'default': True}
        },
        'additionalProperties': False
    }

    def process_entry(self, task, entry, config, remember=True):
        """Rejects this entry if it does not pass content_size requirements. Returns true if the entry was rejected."""
        if 'content_size' in entry:
            size = entry['content_size']
            log.debug('%s size %s MB' % (entry['title'], size))
            # Avoid confusion by printing a reject message to info log, as
            # download plugin has already printed a downloading message.
            if size < config.get('min', 0):
                log_once('Entry `%s` too small, rejecting' % entry['title'], log)
                entry.reject('minimum size %s MB, got %s MB' % (config['min'], size), remember=remember)
                return True
            if size > config.get('max', maxsize):
                log_once('Entry `%s` too big, rejecting' % entry['title'], log)
                entry.reject('maximum size %s MB, got %s MB' % (config['max'], size), remember=remember)
                return True

    @plugin.priority(130)
    def on_task_filter(self, task, config):
        # Do processing on filter phase in case input plugin provided the size
        for entry in task.entries:
            self.process_entry(task, entry, config, remember=False)

    @plugin.priority(150)
    def on_task_modify(self, task, config):
        if task.options.test or task.options.learn:
            log.info('Plugin is partially disabled with --test and --learn because size information may not be '
                     'available')
            return

        num_rejected = len(task.rejected)
        for entry in task.accepted:
            if 'content_size' in entry:
                self.process_entry(task, entry, config)
            elif config['strict']:
                log.debug('Entry %s size is unknown, rejecting because of strict mode (default)' % entry['title'])
                log.info('No size information available for %s, rejecting' % entry['title'])
                if not 'file' in entry:
                    entry.reject('no size info available nor file to read it from', remember=True)
                else:
                    entry.reject('no size info available from downloaded file', remember=True)

        if len(task.rejected) > num_rejected:
            # Since we are rejecting after the filter event,
            # re-run this task to see if there is an alternate entry to accept
            task.rerun()


@event('plugin.register')
def register_plugin():
    plugin.register(FilterContentSize, 'content_size', api_ver=2)

########NEW FILE########
__FILENAME__ = crossmatch
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('crossmatch')


class CrossMatch(object):
    """
    Perform action based on item on current task and other inputs.

    Example::

      crossmatch:
        from:
          - rss: http://example.com/
        fields:
          - title
        action: reject
    """

    schema = {
        'type': 'object',
        'properties': {
            'fields': {'type': 'array', 'items': {'type': 'string'}},
            'action': {'enum': ['accept', 'reject']},
            'from': {'type': 'array', 'items': {'$ref': '/schema/plugins?phase=input'}}
        },
        'required': ['fields', 'action', 'from'],
        'additionalProperties': False
    }

    def on_task_filter(self, task, config):

        fields = config['fields']
        action = config['action']

        match_entries = []

        # TODO: xxx
        # we probably want to have common "run and combine inputs" function sometime soon .. this code is in
        # few places already (discover, inputs, ...)
        # code written so that this can be done easily ...
        for item in config['from']:
            for input_name, input_config in item.iteritems():
                input = plugin.get_plugin_by_name(input_name)
                if input.api_ver == 1:
                    raise plugin.PluginError('Plugin %s does not support API v2' % input_name)
                method = input.phase_handlers['input']
                try:
                    result = method(task, input_config)
                except plugin.PluginError as e:
                    log.warning('Error during input plugin %s: %s' % (input_name, e))
                    continue
                if result:
                    match_entries.extend(result)
                else:
                    log.warning('Input %s did not return anything' % input_name)
                    continue

        # perform action on intersecting entries
        for entry in task.entries:
            for generated_entry in match_entries:
                log.trace('checking if %s matches %s' % (entry['title'], generated_entry['title']))
                common = self.entry_intersects(entry, generated_entry, fields)
                if common:
                    msg = 'intersects with %s on field(s) %s' % \
                          (generated_entry['title'], ', '.join(common))
                    if action == 'reject':
                        entry.reject(msg)
                    if action == 'accept':
                        entry.accept(msg)

    def entry_intersects(self, e1, e2, fields=None):
        """
        :param e1: First :class:`flexget.entry.Entry`
        :param e2: Second :class:`flexget.entry.Entry`
        :param fields: List of fields which are checked
        :return: List of field names in common
        """

        if fields is None:
            fields = []

        common_fields = []

        for field in fields:
            # TODO: simplify if seems to work (useless debug)
            log.trace('checking field %s' % field)
            v1 = e1.get(field, object())
            v2 = e2.get(field, object())
            log.trace('v1: %r' % v1)
            log.trace('v2: %r' % v2)

            if v1 == v2:
                common_fields.append(field)
            else:
                log.trace('not matching')
        return common_fields


@event('plugin.register')
def register_plugin():
    plugin.register(CrossMatch, 'crossmatch', api_ver=2)

########NEW FILE########
__FILENAME__ = delay
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime
from sqlalchemy import Column, Integer, String, Unicode, DateTime, PickleType, Index

from flexget import db_schema, plugin
from flexget.event import event
from flexget.entry import Entry
from flexget.utils.database import safe_pickle_synonym
from flexget.utils.tools import parse_timedelta

log = logging.getLogger('delay')
Base = db_schema.versioned_base('delay', 1)


class DelayedEntry(Base):

    __tablename__ = 'delay'

    id = Column(Integer, primary_key=True)
    task = Column('feed', String)
    title = Column(Unicode)
    expire = Column(DateTime)
    _entry = Column('entry', PickleType)
    entry = safe_pickle_synonym('_entry')

    def __repr__(self):
        return '<DelayedEntry(title=%s)>' % self.title

Index('delay_feed_title', DelayedEntry.task, DelayedEntry.title)
# TODO: index "expire, task"


@db_schema.upgrade('delay')
def upgrade(ver, session):
    if ver is None:
        log.info('Fixing delay table from erroneous data ...')
        # TODO: Using the DelayedEntry object here is no good.
        all = session.query(DelayedEntry).all()
        for de in all:
            for key, value in de.entry.iteritems():
                if not isinstance(value, (basestring, bool, int, float, list, dict)):
                    log.warning('Removing `%s` with erroneous data' % de.title)
                    session.delete(de)
                    break
        ver = 1
    return ver


class FilterDelay(object):
    """
        Add delay to a task. This is useful for de-prioritizing expensive / bad-quality tasks.

        Format: n [minutes|hours|days|weeks]

        Example::

          delay: 2 hours
    """

    schema = {'type': 'string', 'format': 'interval'}

    def get_delay(self, config):
        log.debug('delay: %s' % config)
        try:
            return parse_timedelta(config)
        except ValueError:
            raise plugin.PluginError('Invalid time format', log)

    @plugin.priority(-1)
    def on_task_input(self, task, config):
        """Captures the current input then replaces it with entries that have passed the delay."""
        if task.entries:
            log.verbose('Delaying %s new entries for %s' % (len(task.entries), config))
            # Let details plugin know that it is ok if this task doesn't produce any entries
            task.no_entries_ok = True
        # First learn the current entries in the task to the database
        expire_time = datetime.now() + self.get_delay(config)
        for entry in task.entries:
            log.debug('Delaying %s' % entry['title'])
            # check if already in queue
            if not task.session.query(DelayedEntry).\
                filter(DelayedEntry.title == entry['title']).\
                    filter(DelayedEntry.task == task.name).first():
                delay_entry = DelayedEntry()
                delay_entry.title = entry['title']
                delay_entry.entry = entry
                delay_entry.task = task.name
                delay_entry.expire = expire_time
                task.session.add(delay_entry)

        # Clear the current entries from the task now that they are stored
        task.all_entries[:] = []

        # Generate the list of entries whose delay has passed
        passed_delay = task.session.query(DelayedEntry).\
            filter(datetime.now() > DelayedEntry.expire).\
            filter(DelayedEntry.task == task.name)
        delayed_entries = [Entry(item.entry) for item in passed_delay.all()]
        for entry in delayed_entries:
            entry['passed_delay'] = True
            log.debug('Releasing %s' % entry['title'])
        # Delete the entries from the db we are about to inject
        passed_delay.delete()

        if delayed_entries:
            log.verbose('Restoring %s entries that have passed delay.' % len(delayed_entries))
        # Return our delayed entries
        return delayed_entries


@event('plugin.register')
def register_plugin():
    plugin.register(FilterDelay, 'delay', api_ver=2)

########NEW FILE########
__FILENAME__ = exists
from __future__ import unicode_literals, division, absolute_import
import os
import logging

from flexget import plugin
from flexget.event import event
from flexget.config_schema import one_or_more

log = logging.getLogger('exists')


class FilterExists(object):

    """
        Reject entries that already exist in given path.

        Example::

          exists: /storage/movies/
    """

    schema = one_or_more({'type': 'string', 'format': 'path'})

    def prepare_config(self, config):
        # If only a single path is passed turn it into a 1 element list
        if isinstance(config, basestring):
            config = [config]
        return config

    @plugin.priority(-1)
    def on_task_filter(self, task, config):
        if not task.accepted:
            log.debug('No accepted entries, not scanning for existing.')
            return
        log.verbose('Scanning path(s) for existing files.')
        config = self.prepare_config(config)
        for path in config:
            # unicode path causes crashes on some paths
            path = str(os.path.expanduser(path))
            if not os.path.exists(path):
                raise plugin.PluginWarning('Path %s does not exist' % path, log)
            # scan through
            for root, dirs, files in os.walk(path, followlinks=True):
                # convert filelists into utf-8 to avoid unicode problems
                dirs = [x.decode('utf-8', 'ignore') for x in dirs]
                files = [x.decode('utf-8', 'ignore') for x in files]
                for entry in task.accepted:
                    name = entry['title']
                    if name in dirs or name in files:
                        log.debug('Found %s in %s' % (name, root))
                        entry.reject(os.path.join(root, name))

@event('plugin.register')
def register_plugin():
    plugin.register(FilterExists, 'exists', api_ver=2)

########NEW FILE########
__FILENAME__ = exists_movie
from __future__ import unicode_literals, division, absolute_import
import os
import logging

from flexget import plugin
from flexget.event import event
from flexget.config_schema import one_or_more
from flexget.utils.titles.movie import MovieParser
from flexget.utils.tools import TimedDict

log = logging.getLogger('exists_movie')


class FilterExistsMovie(object):

    """
    Reject existing movies.

    Example::

      exists_movie: /storage/movies/
    """

    schema = one_or_more({'type': 'string', 'format': 'path'})

    skip = ['cd1', 'cd2', 'subs', 'sample']

    def __init__(self):
        self.cache = TimedDict(cache_time='1 hour')

    def build_config(self, config):
        # if only a single path is passed turn it into a 1 element list
        if isinstance(config, basestring):
            config = [config]
        return config

    @plugin.priority(-1)
    def on_task_filter(self, task, config):
        if not task.accepted:
            log.debug('nothing accepted, aborting')
            return

        config = self.build_config(config)
        imdb_lookup = plugin.get_plugin_by_name('imdb_lookup').instance

        incompatible_dirs = 0
        incompatible_entries = 0
        count_entries = 0
        count_dirs = 0

        # list of imdb ids gathered from paths / cache
        imdb_ids = []

        for path in config:
            # see if this path has already been scanned
            if path in self.cache:
                log.verbose('Using cached scan for %s ...' % path)
                imdb_ids.extend(self.cache[path])
                continue

            path_ids = []

            # with unicode it crashes on some paths ..
            path = str(os.path.expanduser(path))
            if not os.path.exists(path):
                log.critical('Path %s does not exist' % path)
                continue

            log.verbose('Scanning path %s ...' % path)

            # Help debugging by removing a lot of noise
            #logging.getLogger('movieparser').setLevel(logging.WARNING)
            #logging.getLogger('imdb_lookup').setLevel(logging.WARNING)

            # scan through
            for root, dirs, files in os.walk(path, followlinks=True):
                # convert filelists into utf-8 to avoid unicode problems
                dirs = [x.decode('utf-8', 'ignore') for x in dirs]
                # files = [x.decode('utf-8', 'ignore') for x in files]

                # TODO: add also video files?
                for item in dirs:
                    if item.lower() in self.skip:
                        continue
                    count_dirs += 1

                    movie = MovieParser()
                    movie.parse(item)

                    try:
                        imdb_id = imdb_lookup.imdb_id_lookup(movie_title=movie.name,
                                                             raw_title=item,
                                                             session=task.session)
                        if imdb_id in path_ids:
                            log.trace('duplicate %s' % item)
                            continue
                        if imdb_id is not None:
                            log.trace('adding: %s' % imdb_id)
                            path_ids.append(imdb_id)
                    except plugin.PluginError as e:
                        log.trace('%s lookup failed (%s)' % (item, e.value))
                        incompatible_dirs += 1

            # store to cache and extend to found list
            self.cache[path] = path_ids
            imdb_ids.extend(path_ids)

        log.debug('-- Start filtering entries ----------------------------------')

        # do actual filtering
        for entry in task.accepted:
            count_entries += 1
            if not entry.get('imdb_id', eval_lazy=False):
                try:
                    imdb_lookup.lookup(entry)
                except plugin.PluginError as e:
                    log.trace('entry %s imdb failed (%s)' % (entry['title'], e.value))
                    incompatible_entries += 1
                    continue

            # actual filtering
            if entry['imdb_id'] in imdb_ids:
                entry.reject('movie exists')

        if incompatible_dirs or incompatible_entries:
            log.verbose('There were some incompatible items. %s of %s entries '
                        'and %s of %s directories could not be verified.' %
                (incompatible_entries, count_entries, incompatible_dirs, count_dirs))

        log.debug('-- Finished filtering entries -------------------------------')

@event('plugin.register')
def register_plugin():
    plugin.register(FilterExistsMovie, 'exists_movie', groups=['exists'], api_ver=2)

########NEW FILE########
__FILENAME__ = exists_series
from __future__ import unicode_literals, division, absolute_import
import copy
import os
import logging

from flexget import plugin
from flexget.event import event
from flexget.config_schema import one_or_more
from flexget.utils.log import log_once
from flexget.utils.template import RenderError
from flexget.utils.titles import ParseWarning

log = logging.getLogger('exists_series')


class FilterExistsSeries(object):
    """
    Intelligent series aware exists rejecting.

    Example::

      exists_series: /storage/series/
    """

    schema = {
        'anyOf': [
            one_or_more({'type': 'string', 'format': 'path'}),
            {
                'type': 'object',
                'properties': {
                    'path': one_or_more({'type': 'string', 'format': 'path'}),
                    'allow_different_qualities': {'enum': ['better', True, False], 'default': False}
                },
                'required': ['path'],
                'additionalProperties': False
            }
        ]
    }

    def prepare_config(self, config):
        # if config is not a dict, assign value to 'path' key
        if not isinstance(config, dict):
            config = {'path': config}
        # if only a single path is passed turn it into a 1 element list
        if isinstance(config['path'], basestring):
            config['path'] = [config['path']]
        return config

    @plugin.priority(-1)
    def on_task_filter(self, task, config):
        if not task.accepted:
            log.debug('Scanning not needed')
            return
        config = self.prepare_config(config)
        accepted_series = {}
        paths = set()
        for entry in task.accepted:
            if 'series_parser' in entry:
                if entry['series_parser'].valid:
                    accepted_series.setdefault(entry['series_parser'].name, []).append(entry)
                    for path in config['path']:
                        try:
                            paths.add(entry.render(path))
                        except RenderError as e:
                            log.error('Error rendering path `%s`: %s', path, e)
                else:
                    log.debug('entry %s series_parser invalid', entry['title'])
        if not accepted_series:
            log.warning('No accepted entries have series information. exists_series cannot filter them')
            return

        for path in paths:
            log.verbose('Scanning %s', path)
            # crashes on some paths with unicode
            path = str(os.path.expanduser(path))
            if not os.path.exists(path):
                raise plugin.PluginWarning('Path %s does not exist' % path, log)
            # scan through
            for root, dirs, files in os.walk(path, followlinks=True):
                # convert filelists into utf-8 to avoid unicode problems
                dirs = [x.decode('utf-8', 'ignore') for x in dirs]
                files = [x.decode('utf-8', 'ignore') for x in files]
                # For speed, only test accepted entries since our priority should be after everything is accepted.
                for series in accepted_series:
                    # make new parser from parser in entry
                    disk_parser = copy.copy(accepted_series[series][0]['series_parser'])
                    for name in files + dirs:
                        # run parser on filename data
                        disk_parser.data = name
                        try:
                            disk_parser.parse(data=name)
                        except ParseWarning as pw:
                            log_once(pw.value, logger=log)
                        if disk_parser.valid:
                            log.debug('name %s is same series as %s', name, series)
                            log.debug('disk_parser.identifier = %s', disk_parser.identifier)
                            log.debug('disk_parser.quality = %s', disk_parser.quality)
                            log.debug('disk_parser.proper_count = %s', disk_parser.proper_count)

                            for entry in accepted_series[series]:
                                log.debug('series_parser.identifier = %s', entry['series_parser'].identifier)
                                if disk_parser.identifier != entry['series_parser'].identifier:
                                    log.trace('wrong identifier')
                                    continue
                                log.debug('series_parser.quality = %s', entry['series_parser'].quality)
                                if config.get('allow_different_qualities') == 'better':
                                    if entry['series_parser'].quality > disk_parser.quality:
                                        log.trace('better quality')
                                        continue
                                elif config.get('allow_different_qualities'):
                                    if disk_parser.quality != entry['series_parser'].quality:
                                        log.trace('wrong quality')
                                        continue
                                log.debug('entry parser.proper_count = %s', entry['series_parser'].proper_count)
                                if disk_parser.proper_count >= entry['series_parser'].proper_count:
                                    entry.reject('proper already exists')
                                    continue
                                else:
                                    log.trace('new one is better proper, allowing')
                                    continue

@event('plugin.register')
def register_plugin():
    plugin.register(FilterExistsSeries, 'exists_series', groups=['exists'], api_ver=2)

########NEW FILE########
__FILENAME__ = if_condition
from __future__ import unicode_literals, division, absolute_import
import __builtin__
import logging
import re
import datetime
from copy import copy

from flexget import plugin
from flexget.event import event
from flexget.task import Task
from flexget.entry import Entry

log = logging.getLogger('if')


def safer_eval(statement, locals):
    """A safer eval function. Does not allow __ or try statements, only includes certain 'safe' builtins."""
    allowed_builtins = ['True', 'False', 'str', 'unicode', 'int', 'float', 'len', 'any', 'all', 'sorted']
    for name in allowed_builtins:
        locals[name] = getattr(__builtin__, name)
    if re.search(r'__|try\s*:|lambda', statement):
        raise ValueError('`__`, lambda or try blocks not allowed in if statements.')
    return eval(statement, {'__builtins__': None}, locals)


class FilterIf(object):
    """Can run actions on entries that satisfy a given condition.

    Actions include accept, reject, and fail, as well as the ability to run other filter plugins on the entries."""

    schema = {
        'type': 'array',
        'items': {
            'type': 'object',
            'additionalProperties': {
                'anyOf': [
                    {'$ref': '/schema/plugins'},
                    {'enum': ['accept', 'reject', 'fail']}
                ]
            }
        }
    }

    def check_condition(self, condition, entry):
        """Checks if a given `entry` passes `condition`"""
        # Make entry fields and other utilities available in the eval namespace
        # We need our namespace to be an Entry instance for lazy loading to work
        eval_locals = copy(entry)
        eval_locals.update({'has_field': lambda f: f in entry,
                            'timedelta': datetime.timedelta,
                            'now': datetime.datetime.now()})
        try:
            # Restrict eval namespace to have no globals and locals only from eval_locals
            passed = safer_eval(condition, eval_locals)
            if passed:
                log.debug('%s matched requirement %s' % (entry['title'], condition))
            return passed
        except NameError as e:
            # Extract the name that did not exist
            missing_field = e.args[0].split('\'')[1]
            log.debug('%s does not contain the field %s' % (entry['title'], missing_field))
        except Exception as e:
            log.error('Error occured while evaluating statement `%s`. (%s)' % (condition, e))

    def __getattr__(self, item):
        """Provides handlers for all phases."""
        for phase, method in plugin.phase_methods.iteritems():
            if item == method and phase not in ['accept', 'reject', 'fail', 'input']:
                break
        else:
            raise AttributeError(item)

        def handle_phase(task, config):
            entry_actions = {
                'accept': Entry.accept,
                'reject': Entry.reject,
                'fail': Entry.fail}
            for item in config:
                requirement, action = item.items()[0]
                passed_entries = [e for e in task.entries if self.check_condition(requirement, e)]
                if isinstance(action, basestring):
                    if not phase == 'filter':
                        continue
                    # Simple entry action (accept, reject or fail) was specified as a string
                    for entry in passed_entries:
                        entry_actions[action](entry, 'Matched requirement: %s' % requirement)
                else:
                    # Other plugins were specified to run on this entry
                    fake_task = Task(task.manager, task.name, config=action, options=task.options)
                    fake_task.session = task.session
                    # This entry still belongs to our feed, accept/reject etc. will carry through.
                    fake_task.all_entries[:] = passed_entries

                    methods = {}
                    for plugin_name, plugin_config in action.iteritems():
                        p = plugin.get_plugin_by_name(plugin_name)
                        method = p.phase_handlers.get(phase)
                        if method:
                            methods[method] = (fake_task, plugin_config)
                    # Run the methods in priority order
                    for method in sorted(methods, reverse=True):
                        method(*methods[method])

        handle_phase.priority = 80
        return handle_phase


@event('plugin.register')
def register_plugin():
    plugin.register(FilterIf, 'if', api_ver=2)

########NEW FILE########
__FILENAME__ = imdb
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.log import log_once

log = logging.getLogger('imdb')


class FilterImdb(object):
    """
    This plugin allows filtering based on IMDB score, votes and genres etc.

    Note: All parameters are optional. Some are mutually exclusive.

    Configuration::

      min_score: <num>
      min_votes: <num>
      min_year: <num>
      max_year: <num>

      # reject if genre contains any of these
      reject_genres:
        - genre1
        - genre2

      # reject if language contain any of these
      reject_languages:
        - language1

      # accept only these primary languages
      accept_languages:
        - language1

      # accept movies with any of these actors
      accept_actors:
        - nm0004695
        - nm0004754

      # reject movie if it has any of these actors
      reject_actors:
        - nm0001191
        - nm0002071

      # accept all movies by these directors
      accept_directors:
        - nm0000318

      # reject movies by these directors
      reject_directors:
        - nm0093051

      # reject movies/TV shows with any of these ratings
      reject_mpaa_ratings:
        - PG_13
        - R
        - X

      # accept movies/TV shows with only these ratings
      accept_mpaa_ratings:
        - PG
        - G
        - TV_Y
    """

    schema = {
        'type': 'object',
        'properties': {
            'min_year': {'type': 'integer'},
            'max_year': {'type': 'integer'},
            'min_votes': {'type': 'integer'},
            'min_score': {'type': 'number'},
            'reject_genres': {'type': 'array', 'items': {'type': 'string'}},
            'reject_languages': {'type': 'array', 'items': {'type': 'string'}},
            'accept_languages': {'type': 'array', 'items': {'type': 'string'}},
            'reject_actors': {'type': 'array', 'items': {'type': 'string'}},
            'accept_actors': {'type': 'array', 'items': {'type': 'string'}},
            'reject_directors': {'type': 'array', 'items': {'type': 'string'}},
            'accept_directors': {'type': 'array', 'items': {'type': 'string'}},
            'reject_mpaa_ratings': {'type': 'array', 'items': {'type': 'string'}},
            'accept_mpaa_ratings': {'type': 'array', 'items': {'type': 'string'}}
        },
        'additionalProperties': False
    }

    # Run later to avoid unnecessary lookups
    @plugin.priority(120)
    def on_task_filter(self, task, config):

        lookup = plugin.get_plugin_by_name('imdb_lookup').instance.lookup

        # since the plugin does not reject anything, no sense going trough accepted
        for entry in task.undecided:

            force_accept = False

            try:
                lookup(entry)
            except plugin.PluginError as e:
                # logs skip message once trough log_once (info) and then only when ran from cmd line (w/o --cron)
                msg = 'Skipping %s because of an error: %s' % (entry['title'], e.value)
                if not log_once(msg, logger=log):
                    log.verbose(msg)
                continue

            #for key, value in entry.iteritems():
            #    log.debug('%s = %s (type: %s)' % (key, value, type(value)))

            # Check defined conditions, TODO: rewrite into functions?
            reasons = []
            if 'min_score' in config:
                if entry.get('imdb_score', 0) < config['min_score']:
                    reasons.append('min_score (%s < %s)' % (entry.get('imdb_score'), config['min_score']))
            if 'min_votes' in config:
                if entry.get('imdb_votes', 0) < config['min_votes']:
                    reasons.append('min_votes (%s < %s)' % (entry.get('imdb_votes'), config['min_votes']))
            if 'min_year' in config:
                if entry.get('imdb_year', 0) < config['min_year']:
                    reasons.append('min_year (%s < %s)' % (entry.get('imdb_year'), config['min_year']))
            if 'max_year' in config:
                if entry.get('imdb_year', 0) > config['max_year']:
                    reasons.append('max_year (%s > %s)' % (entry.get('imdb_year'), config['max_year']))
            if 'reject_genres' in config:
                rejected = config['reject_genres']
                for genre in entry.get('imdb_genres', []):
                    if genre in rejected:
                        reasons.append('reject_genres')
                        break

            if 'reject_languages' in config:
                rejected = config['reject_languages']
                for language in entry.get('imdb_languages', []):
                    if language in rejected:
                        reasons.append('reject_languages')
                        break

            if 'accept_languages' in config:
                accepted = config['accept_languages']
                if entry.get('imdb_languages') and entry['imdb_languages'][0] not in accepted:
                    # Reject if the first (primary) language is not among acceptable languages
                    reasons.append('accept_languages')

            if 'reject_actors' in config:
                rejected = config['reject_actors']
                for actor_id, actor_name in entry.get('imdb_actors', {}).iteritems():
                    if actor_id in rejected or actor_name in rejected:
                        reasons.append('reject_actors %s' % actor_name or actor_id)
                        break

            # Accept if actors contains an accepted actor, but don't reject otherwise
            if 'accept_actors' in config:
                accepted = config['accept_actors']
                for actor_id, actor_name in entry.get('imdb_actors', {}).iteritems():
                    if actor_id in accepted or actor_name in accepted:
                        log.debug('Accepting because of accept_actors %s' % actor_name or actor_id)
                        force_accept = True
                        break

            if 'reject_directors' in config:
                rejected = config['reject_directors']
                for director_id, director_name in entry.get('imdb_directors', {}).iteritems():
                    if director_id in rejected or director_name in rejected:
                        reasons.append('reject_directors %s' % director_name or director_id)
                        break

            # Accept if the director is in the accept list, but do not reject if the director is unknown
            if 'accept_directors' in config:
                accepted = config['accept_directors']
                for director_id, director_name in entry.get('imdb_directors', {}).iteritems():
                    if director_id in accepted or director_name in accepted:
                        log.debug('Accepting because of accept_directors %s' % director_name or director_id)
                        force_accept = True
                        break

            if 'reject_mpaa_ratings' in config:
                rejected = config['reject_mpaa_ratings']
                if entry.get('imdb_mpaa_rating') in rejected:
                    reasons.append('reject_mpaa_ratings %s' % entry['imdb_mpaa_rating'])

            if 'accept_mpaa_ratings' in config:
                accepted = config['accept_mpaa_ratings']
                if entry.get('imdb_mpaa_rating') not in accepted:
                    reasons.append('accept_mpaa_ratings %s' % entry.get('imdb_mpaa_rating'))

            if reasons and not force_accept:
                msg = 'Didn\'t accept `%s` because of rule(s) %s' % \
                    (entry.get('imdb_name', None) or entry['title'], ', '.join(reasons))
                if task.options.debug:
                    log.debug(msg)
                else:
                    if task.options.cron:
                        log_once(msg, log)
                    else:
                        log.info(msg)
            else:
                log.debug('Accepting %s' % (entry['title']))
                entry.accept()

@event('plugin.register')
def register_plugin():
    plugin.register(FilterImdb, 'imdb', api_ver=2)

########NEW FILE########
__FILENAME__ = imdb_required
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('imdb_required')


class FilterImdbRequired(object):
    """
    Rejects entries without imdb_url or imdb_id.
    Makes imdb lookup / search if necessary.

    Example::

      imdb_required: yes
    """

    schema = {'type': 'boolean'}

    @plugin.priority(32)
    def on_task_filter(self, task, config):
        if not config:
            return
        for entry in task.entries:
            try:
                plugin.get_plugin_by_name('imdb_lookup').instance.lookup(entry)
            except plugin.PluginError:
                entry.reject('imdb required')
            if 'imdb_id' not in entry and 'imdb_url' not in entry:
                entry.reject('imdb required')

@event('plugin.register')
def register_plugin():
    plugin.register(FilterImdbRequired, 'imdb_required', api_ver=2)

########NEW FILE########
__FILENAME__ = limit_new
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('limit_new')


class FilterLimitNew(object):
    """
    Limit number of new items.

    Example::

      limit_new: 1

    This would allow only one new item to pass trough per execution.
    Useful for passing torrents slowly into download.

    Note that since this is per execution, actual rate depends how often
    FlexGet is executed.
    """

    schema = {
        'type': 'integer',
        'minimum': 1
    }

    def __init__(self):
        try:
            self.backlog = plugin.get_plugin_by_name('backlog')
        except plugin.DependencyError:
            log.warning('Unable utilize backlog plugin, entries may slip trough limit_new in some rare cases')

    @plugin.priority(-255)
    def on_task_filter(self, task, config):
        if task.options.learn:
            log.info('Plugin limit_new is disabled with --learn')
            return

        amount = config
        for index, entry in enumerate(task.accepted):
            if index < amount:
                log.verbose('Allowed %s (%s)' % (entry['title'], entry['url']))
            else:
                entry.reject('limit exceeded')
                # Also save this in backlog so that it can be accepted next time.
                if self.backlog:
                    self.backlog.instance.add_backlog(task, entry)

        log.debug('Rejected: %s Allowed: %s' % (len(task.accepted[amount:]), len(task.accepted[:amount])))


@event('plugin.register')
def register_plugin():
    plugin.register(FilterLimitNew, 'limit_new', api_ver=2)

########NEW FILE########
__FILENAME__ = magnets
from __future__ import unicode_literals, division, absolute_import

from flexget import plugin
from flexget.event import event


class Magnets(object):
    """Removes magnet urls form the urls list. Rejects entries that have nothing but magnet urls."""

    schema = {'type': 'boolean'}

    @plugin.priority(0)
    def on_task_urlrewrite(self, task, config):
        if config is not False:
            return
        for entry in task.accepted:
            if 'urls' in entry:
                entry['urls'] = [url for url in entry['urls'] if not url.startswith('magnet:')]

            if entry['url'].startswith('magnet:'):
                if entry.get('urls'):
                    entry['url'] = entry['urls'][0]
                else:
                    entry.reject('Magnet urls not allowed.', remember=True)


@event('plugin.register')
def register_plugin():
    plugin.register(Magnets, 'magnets', api_ver=2)

########NEW FILE########
__FILENAME__ = movie_queue
from __future__ import unicode_literals, division, absolute_import
import logging

from sqlalchemy import Column, Integer, String, ForeignKey, or_, and_, select, update
from sqlalchemy.orm.exc import NoResultFound

from flexget import db_schema, plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.manager import Session
from flexget.utils import qualities
from flexget.utils.imdb import extract_id
from flexget.utils.log import log_once
from flexget.utils.database import quality_requirement_property, with_session
from flexget.utils.sqlalchemy_utils import table_exists, table_schema

try:
    from flexget.plugins.filter import queue_base
except ImportError:
    raise plugin.DependencyError(issued_by='movie_queue', missing='queue_base',
                                 message='movie_queue requires the queue_base plugin')

log = logging.getLogger('movie_queue')
Base = db_schema.versioned_base('movie_queue', 2)


@event('manager.lock_acquired')
def migrate_imdb_queue(manager):
    """If imdb_queue table is found, migrate the data to movie_queue"""
    session = Session()
    try:
        if table_exists('imdb_queue', session):
            log.info('Migrating imdb_queue items to movie_queue')
            old_table = table_schema('imdb_queue', session)
            for row in session.execute(old_table.select()):
                try:
                    queue_add(imdb_id=row['imdb_id'], quality=row['quality'], session=session)
                except QueueError as e:
                    log.error('Unable to migrate %s from imdb_queue to movie_queue' % row['title'])
            old_table.drop()
            session.commit()
    finally:
        session.close()


@db_schema.upgrade('movie_queue')
def upgrade(ver, session):
    if ver == 0:
        # Translate old qualities into new quality requirements
        movie_table = table_schema('movie_queue', session)
        for row in session.execute(select([movie_table.c.id, movie_table.c.quality])):
            # Webdl quality no longer has dash
            new_qual = row['quality'].replace('web-dl', 'webdl')
            if new_qual.lower() != 'any':
                # Old behavior was to get specified quality or greater, approximate that with new system
                new_qual = ' '.join(qual + '+' for qual in new_qual.split(' '))
            session.execute(update(movie_table, movie_table.c.id == row['id'],
                                   {'quality': new_qual}))
        ver = 1
    if ver == 1:
        # Bad upgrade left some qualities as 'ANY+'
        movie_table = table_schema('movie_queue', session)
        for row in session.execute(select([movie_table.c.id, movie_table.c.quality])):
            if row['quality'].lower() == 'any+':
                session.execute(update(movie_table, movie_table.c.id == row['id'],
                                       {'quality': 'ANY'}))
        ver = 2
    return ver


class QueuedMovie(queue_base.QueuedItem, Base):
    __tablename__ = 'movie_queue'
    __mapper_args__ = {'polymorphic_identity': 'movie'}
    id = Column(Integer, ForeignKey('queue.id'), primary_key=True)
    imdb_id = Column(String)
    tmdb_id = Column(Integer)
    quality = Column('quality', String)
    quality_req = quality_requirement_property('quality')


class FilterMovieQueue(queue_base.FilterQueueBase):
    def matches(self, task, config, entry):
        # Tell tmdb_lookup to add lazy lookup fields if not already present
        try:
            plugin.get_plugin_by_name('imdb_lookup').instance.register_lazy_fields(entry)
        except plugin.DependencyError:
            log.debug('imdb_lookup is not available, queue will not work if movie ids are not populated')
        try:
            plugin.get_plugin_by_name('tmdb_lookup').instance.lookup(entry)
        except plugin.DependencyError:
            log.debug('tmdb_lookup is not available, queue will not work if movie ids are not populated')
    
        conditions = []
        # Check if a movie id is already populated before incurring a lazy lookup
        for lazy in [False, True]:
            if entry.get('imdb_id', eval_lazy=lazy):
                conditions.append(QueuedMovie.imdb_id == entry['imdb_id'])
            if entry.get('tmdb_id', eval_lazy=lazy and not conditions):
                conditions.append(QueuedMovie.tmdb_id == entry['tmdb_id'])
            if conditions:
                break
        if not conditions:
            log_once('IMDB and TMDB lookups failed for %s.' % entry['title'], log, logging.WARN)
            return

        quality = entry.get('quality', qualities.Quality())

        movie = task.session.query(QueuedMovie).filter(QueuedMovie.downloaded == None). \
            filter(or_(*conditions)).first()
        if movie and movie.quality_req.allows(quality):
            return movie


class QueueError(Exception):
    """Exception raised if there is an error with a queue operation"""

    # TODO: I think message was removed from exception baseclass and is now masked
    # some other custom exception (DependencyError) had to make so tweaks to make it work ..

    def __init__(self, message, errno=0):
        self.message = message
        self.errno = errno


@with_session
def parse_what(what, lookup=True, session=None):
    """
    Determines what information was provided by the search string `what`.
    If `lookup` is true, will fill in other information from tmdb.

    :param what: Can be one of:
      <Movie Title>: Search based on title
      imdb_id=<IMDB id>: search based on imdb id
      tmdb_id=<TMDB id>: search based on tmdb id
    :param bool lookup: Whether missing info should be filled in from tmdb.
    :param session: An existing session that will be used for lookups if provided.
    :rtype: dict
    :return: A dictionary with 'title', 'imdb_id' and 'tmdb_id' keys
    """

    tmdb_lookup = plugin.get_plugin_by_name('api_tmdb').instance.lookup

    result = {'title': None, 'imdb_id': None, 'tmdb_id': None}
    result['imdb_id'] = extract_id(what)
    if not result['imdb_id']:
        if what.startswith('tmdb_id='):
            result['tmdb_id'] = what[8:]
        else:
            result['title'] = what

    if not lookup:
        # If not doing an online lookup we can return here
        return result

    search_entry = Entry(title=result['title'] or '')
    for field in ['imdb_id', 'tmdb_id']:
        if result.get(field):
            search_entry[field] = result[field]
    # Put lazy lookup fields on the search entry
    plugin.get_plugin_by_name('imdb_lookup').instance.register_lazy_fields(search_entry)
    plugin.get_plugin_by_name('tmdb_lookup').instance.lookup(search_entry)

    try:
        # Both ids are optional, but if movie_name was populated at least one of them will be there
        return {'title': search_entry['movie_name'], 'imdb_id': search_entry.get('imdb_id'),
                'tmdb_id': search_entry.get('tmdb_id')}
    except KeyError as e:
        raise QueueError(e.message)


# API functions to edit queue
@with_session
def queue_add(title=None, imdb_id=None, tmdb_id=None, quality=None, session=None):
    """
    Add an item to the queue with the specified quality requirements.

    One or more of `title` `imdb_id` or `tmdb_id` must be specified when calling this function.

    :param title: Title of the movie. (optional)
    :param imdb_id: IMDB id for the movie. (optional)
    :param tmdb_id: TMDB id for the movie. (optional)
    :param quality: A QualityRequirements object defining acceptable qualities.
    :param session: Optional session to use for database updates
    """

    quality = quality or qualities.Requirements('any')

    if not title or not (imdb_id or tmdb_id):
        # We don't have all the info we need to add movie, do a lookup for more info
        result = parse_what(imdb_id or title, session=session)
        title = result['title']
        imdb_id = result['imdb_id']
        tmdb_id = result['tmdb_id']

    # check if the item is already queued
    item = session.query(QueuedMovie).filter(or_(and_(QueuedMovie.imdb_id != None, QueuedMovie.imdb_id == imdb_id),
                                                 and_(QueuedMovie.tmdb_id != None, QueuedMovie.tmdb_id == tmdb_id))). \
        first()
    if not item:
        item = QueuedMovie(title=title, imdb_id=imdb_id, tmdb_id=tmdb_id, quality=quality.text)
        session.add(item)
        log.info('Adding %s to movie queue with quality=%s.' % (title, quality))
        return {'title': title, 'imdb_id': imdb_id, 'tmdb_id': tmdb_id, 'quality': quality}
    else:
        if item.downloaded:
            raise QueueError('ERROR: %s has already been queued and downloaded' % title)
        else:
            raise QueueError('ERROR: %s is already in the queue' % title, errno=1)


@with_session
def queue_del(title=None, imdb_id=None, tmdb_id=None, session=None):
    """
    Delete the given item from the queue.

    :param title: Movie title
    :param imdb_id: Imdb id
    :param tmdb_id: Tmdb id
    :param session: Optional session to use, new session used otherwise
    :return: Title of forgotten movie
    :raises QueueError: If queued item could not be found with given arguments
    """
    log.debug('queue_del - title=%s, imdb_id=%s, tmdb_id=%s' % (title, imdb_id, tmdb_id))
    query = session.query(QueuedMovie)
    if imdb_id:
        query = query.filter(QueuedMovie.imdb_id == imdb_id)
    elif tmdb_id:
        query = query.filter(QueuedMovie.tmdb_id == tmdb_id)
    elif title:
        query = query.filter(QueuedMovie.title == title)
    try:
        item = query.one()
        title = item.title
        session.delete(item)
        return title
    except NoResultFound as e:
        raise QueueError('title=%s, imdb_id=%s, tmdb_id=%s not found from queue' % (title, imdb_id, tmdb_id))


@with_session
def queue_forget(title=None, imdb_id=None, tmdb_id=None, session=None):
    """
    Forget movie download  from the queue.

    :param title: Movie title
    :param imdb_id: Imdb id
    :param tmdb_id: Tmdb id
    :param session: Optional session to use, new session used otherwise
    :return: Title of forgotten movie
    :raises QueueError: If queued item could not be found with given arguments
    """
    log.debug('queue_forget - title=%s, imdb_id=%s, tmdb_id=%s' % (title, imdb_id, tmdb_id))
    query = session.query(QueuedMovie)
    if imdb_id:
        query = query.filter(QueuedMovie.imdb_id == imdb_id)
    elif tmdb_id:
        query = query.filter(QueuedMovie.tmdb_id == tmdb_id)
    elif title:
        query = query.filter(QueuedMovie.title == title)
    try:
        item = query.one()
        title = item.title
        if not item.downloaded:
            raise QueueError('%s is not marked as downloaded' % title)
        item.downloaded = None
        return title
    except NoResultFound as e:
        raise QueueError('title=%s, imdb_id=%s, tmdb_id=%s not found from queue' % (title, imdb_id, tmdb_id))


@with_session
def queue_edit(quality, imdb_id=None, tmdb_id=None, session=None):
    """
    :param quality: Change the required quality for a movie in the queue
    :param imdb_id: Imdb id
    :param tmdb_id: Tmdb id
    :param session: Optional session to use, new session used otherwise
    :return: Title of edited item
    :raises QueueError: If queued item could not be found with given arguments
    """
    # check if the item is queued
    try:
        item = session.query(QueuedMovie).filter(QueuedMovie.imdb_id == imdb_id).one()
        item.quality = quality
        return item.title
    except NoResultFound as e:
        raise QueueError('imdb_id=%s, tmdb_id=%s not found from queue' % (imdb_id, tmdb_id))


@with_session
def queue_get(session=None, downloaded=False):
    """
    Get the current movie queue.

    :param session: New session is used it not given
    :param bool downloaded: Whether or not to return only downloaded
    :return: List of QueuedMovie objects (detached from session)
    """
    if not downloaded:
        return session.query(QueuedMovie).filter(QueuedMovie.downloaded == None).all()
    else:
        return session.query(QueuedMovie).filter(QueuedMovie.downloaded != None).all()


@event('plugin.register')
def register_plugin():
    plugin.register(FilterMovieQueue, 'movie_queue', api_ver=2)

########NEW FILE########
__FILENAME__ = only_new
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('only_new')


class FilterOnlyNew(object):
    """Causes input plugins to only emit entries that haven't been seen on previous runs."""

    schema = {'type': 'boolean'}

    def on_task_start(self, task, config):
        """Make sure the remember_rejected plugin is available"""
        # Raises an error if plugin isn't available
        plugin.get_plugin_by_name('remember_rejected')

    def on_task_learn(self, task, config):
        """Reject all entries so remember_rejected will reject them next time"""
        if not config or not task.entries:
            return
        log.verbose('Rejecting entries after the task has run so they are not processed next time.')
        for entry in task.entries:
            entry.reject('Already processed entry', remember=True)


@event('plugin.register')
def register_plugin():
    plugin.register(FilterOnlyNew, 'only_new', api_ver=2)

########NEW FILE########
__FILENAME__ = private_torrents
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('priv_torrents')


class FilterPrivateTorrents(object):
    """How to handle private torrents.

    private_torrents: yes|no

    Example::

      private_torrents: no

    This would reject all torrent entries with private flag.

    Example::

      private_torrents: yes

    This would reject all public torrents.

    Non-torrent content is not interviened.
    """

    schema = {'type': 'boolean'}

    @plugin.priority(127)
    def on_task_modify(self, task, config):
        private_torrents = config

        for entry in task.accepted:
            if not 'torrent' in entry:
                log.debug('`%s` is not a torrent' % entry['title'])
                continue
            private = entry['torrent'].private

            if not private_torrents and private:
                entry.reject('torrent is marked as private', remember=True)
            elif private_torrents and not private:
                entry.reject('public torrent', remember=True)


@event('plugin.register')
def register_plugin():
    plugin.register(FilterPrivateTorrents, 'private_torrents', api_ver=2)

########NEW FILE########
__FILENAME__ = proper_movies
from __future__ import unicode_literals, division, absolute_import
from datetime import datetime
import logging

from sqlalchemy import Column, Integer, String, Unicode, DateTime
from sqlalchemy.schema import Index
from sqlalchemy.sql.expression import desc

from flexget import plugin
from flexget.event import fire_event, event
from flexget.manager import Base
from flexget.utils.log import log_once
from flexget.utils.titles.movie import MovieParser
from flexget.utils.tools import parse_timedelta

log = logging.getLogger('proper_movies')


class ProperMovie(Base):

    __tablename__ = 'proper_movies'

    id = Column(Integer, primary_key=True)
    title = Column(Unicode)
    task = Column('feed', Unicode)
    imdb_id = Column(String, index=True)
    quality = Column(String)
    proper_count = Column(Integer)
    added = Column(DateTime)

    def __init__(self):
        self.added = datetime.now()

    def __repr__(self):
        return '<ProperMovie(title=%s,task=%s,imdb_id=%s,quality=%s,proper_count=%s,added=%s)>' % \
            (self.title, self.task, self.imdb_id, self.quality, self.proper_count, self.added)


# create index
columns = Base.metadata.tables['proper_movies'].c
Index('proper_movies_imdb_id_quality_proper', columns.imdb_id, columns.quality, columns.proper_count)


class FilterProperMovies(object):
    """
        Automatically download proper movies.

        Configuration:

            proper_movies: n <minutes|hours|days|weeks>

        or permanently:

            proper_movies: yes

        Value no will disable plugin.
    """

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {'type': 'string', 'format': 'interval'}
        ]
    }

    def on_task_filter(self, task, config):
        log.debug('check for enforcing')

        # parse config
        if isinstance(config, bool):
            # configured a boolean false, disable plugin
            if not config:
                return
            # configured a boolean true, disable timeframe
            timeframe = None
        else:
            # parse time window
            log.debug('interval: %s' % config)
            try:
                timeframe = parse_timedelta(config)
            except ValueError:
                raise plugin.PluginError('Invalid time format', log)

        # throws DependencyError if not present aborting task
        imdb_lookup = plugin.get_plugin_by_name('imdb_lookup').instance

        for entry in task.entries:

            parser = MovieParser()
            parser.data = entry['title']
            parser.parse()

            # if we have imdb_id already evaluated
            if entry.get('imdb_id', None, eval_lazy=False) is None:
                try:
                    # TODO: fix imdb_id_lookup, cumbersome that it returns None and or throws exception
                    # Also it's crappy name!
                    imdb_id = imdb_lookup.imdb_id_lookup(movie_title=parser.name, raw_title=entry['title'])
                    if imdb_id is None:
                        continue
                    entry['imdb_id'] = imdb_id
                except plugin.PluginError as pe:
                    log_once(pe.value)
                    continue

            quality = parser.quality.name

            log.debug('quality: %s' % quality)
            log.debug('imdb_id: %s' % entry['imdb_id'])
            log.debug('current proper count: %s' % parser.proper_count)

            proper_movie = task.session.query(ProperMovie).\
                filter(ProperMovie.imdb_id == entry['imdb_id']).\
                filter(ProperMovie.quality == quality).\
                order_by(desc(ProperMovie.proper_count)).first()

            if not proper_movie:
                log.debug('no previous download recorded for %s' % entry['imdb_id'])
                continue

            highest_proper_count = proper_movie.proper_count
            log.debug('highest_proper_count: %i' % highest_proper_count)

            accept_proper = False
            if parser.proper_count > highest_proper_count:
                log.debug('proper detected: %s ' % proper_movie)

                if timeframe is None:
                    accept_proper = True
                else:
                    expires = proper_movie.added + timeframe
                    log.debug('propers timeframe: %s' % timeframe)
                    log.debug('added: %s' % proper_movie.added)
                    log.debug('propers ignore after: %s' % str(expires))
                    if datetime.now() < expires:
                        accept_proper = True
                    else:
                        log.verbose('Proper `%s` has past it\'s expiration time' % entry['title'])

            if accept_proper:
                log.info('Accepting proper version previously downloaded movie `%s`' % entry['title'])
                # TODO: does this need to be called?
                # fire_event('forget', entry['imdb_url'])
                fire_event('forget', entry['imdb_id'])
                entry.accept('proper version of previously downloaded movie')

    def on_task_learn(self, task, config):
        """Add downloaded movies to the database"""
        log.debug('check for learning')
        for entry in task.accepted:
            if 'imdb_id' not in entry:
                log.debug('`%s` does not have imdb_id' % entry['title'])
                continue

            parser = MovieParser()
            parser.data = entry['title']
            parser.parse()

            quality = parser.quality.name

            log.debug('quality: %s' % quality)
            log.debug('imdb_id: %s' % entry['imdb_id'])
            log.debug('proper count: %s' % parser.proper_count)

            proper_movie = task.session.query(ProperMovie).\
                filter(ProperMovie.imdb_id == entry['imdb_id']).\
                filter(ProperMovie.quality == quality).\
                filter(ProperMovie.proper_count == parser.proper_count).first()

            if not proper_movie:
                pm = ProperMovie()
                pm.title = entry['title']
                pm.task = task.name
                pm.imdb_id = entry['imdb_id']
                pm.quality = quality
                pm.proper_count = parser.proper_count
                task.session.add(pm)
                log.debug('added %s' % pm)
            else:
                log.debug('%s already exists' % proper_movie)

@event('plugin.register')
def register_plugin():
    plugin.register(FilterProperMovies, 'proper_movies', api_ver=2)

########NEW FILE########
__FILENAME__ = quality
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.config_schema import one_or_more
import flexget.utils.qualities as quals

log = logging.getLogger('quality')


class FilterQuality(object):
    """
    Rejects all entries that don't have one of the specified qualities

    Example::

      quality:
        - hdtv
    """

    schema = one_or_more({'type': 'string', 'format': 'quality_requirements'})

    # Run before series and imdb plugins, so correct qualities are chosen
    @plugin.priority(175)
    def on_task_filter(self, task, config):
        if not isinstance(config, list):
            config = [config]
        reqs = [quals.Requirements(req) for req in config]
        for entry in task.entries:
            if not entry.get('quality'):
                entry.reject('Entry doesn\'t have a quality')
                continue
            if not any(req.allows(entry['quality']) for req in reqs):
                entry.reject('%s does not match quality requirement %s' % (entry['quality'], reqs))

@event('plugin.register')
def register_plugin():
    plugin.register(FilterQuality, 'quality', api_ver=2)

########NEW FILE########
__FILENAME__ = queue_base
from __future__ import unicode_literals, division, absolute_import
from datetime import datetime
import logging

from sqlalchemy import Column, Integer, Boolean, String, Unicode, DateTime

from flexget import db_schema
from flexget.plugin import priority
from flexget.utils.sqlalchemy_utils import table_add_column

log = logging.getLogger('queue')
Base = db_schema.versioned_base('queue', 2)


@db_schema.upgrade('queue')
def upgrade(ver, session):
    if False:  # ver == 0: disable this, since we don't have a remove column function
        table_add_column('queue', 'last_emit', DateTime, session)
        ver = 1
    if ver < 2:
        # We don't have a remove column for 'last_emit', do nothing
        ver = 2
    return ver


class QueuedItem(Base):
    __tablename__ = 'queue'
    id = Column(Integer, primary_key=True)
    title = Column(Unicode)
    added = Column(DateTime)
    # These fields are populated when the queue item has been downloaded
    downloaded = Column(DateTime)
    entry_title = Column(Unicode)
    entry_url = Column(Unicode)
    entry_original_url = Column(Unicode)
    # Configuration for joined table inheritance
    discriminator = Column('type', String)
    __mapper_args__ = {'polymorphic_on': discriminator}

    def __init__(self, **kwargs):
        super(QueuedItem, self).__init__(**kwargs)
        self.added = datetime.now()


class FilterQueueBase(object):
    """Base class to handle general tasks of keeping a queue of wanted items."""

    schema = {'type': 'boolean'}

    def on_task_start(self, task, config):
        # Dict of entries accepted by this plugin {imdb_id: entry} format
        self.accepted_entries = {}

    def matches(self, task, config, entry):
        """This should return the QueueItem object for the match, if this entry is in the queue."""
        raise NotImplementedError

    @priority(127)
    def on_task_filter(self, task, config):
        if config is False:
            return

        for entry in task.entries:
            item = self.matches(task, config, entry)
            if item and item.id not in self.accepted_entries:
                # Accept this entry if it matches a queue item that has not been accepted this run yet
                entry.accept(reason='Matches %s queue item: %s' % (item.discriminator, item.title))
                # Keep track of entries we accepted, so they can be marked as downloaded on task_exit if successful
                self.accepted_entries[item.id] = entry

    def on_task_learn(self, task, config):
        if config is False:
            return

        for id, entry in self.accepted_entries.iteritems():
            if entry in task.accepted and entry not in task.failed:
                # If entry was not rejected or failed, mark it as downloaded
                update_values = {'downloaded': datetime.now(),
                                 'entry_title': entry['title'],
                                 'entry_url': entry['url'],
                                 'entry_original_url': entry['original_url']}
                task.session.query(QueuedItem).filter(QueuedItem.id == id).update(update_values)
                log.debug('%s was successful, removing from imdb-queue' % entry['title'])

########NEW FILE########
__FILENAME__ = regexp
from __future__ import unicode_literals, division, absolute_import
import urllib
import logging
import re

from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.entry import Entry
from flexget.event import event

log = logging.getLogger('regexp')


class FilterRegexp(object):

    """
        All possible forms.

        regexp:
          [operation]:           # operation to perform on matches
            - [regexp]           # simple regexp
            - [regexp]: <path>   # override path
            - [regexp]:
                [path]: <path>   # override path
                [not]: <regexp>  # not match
                [from]: <field>  # search from given entry field
            - [regexp]:
                [path]: <path>   # override path
                [not]:           # list of not match regexps
                  - <regexp>
                [from]:          # search only from these fields
                  - <field>
          [operation]:
            - <regexp>
          [rest]: <operation>    # non matching entries are
          [from]:                # search only from these fields for all regexps
            - <field>

        Possible operations: accept, reject, accept_excluding, reject_excluding
    """

    schema = {
        'type': 'object',
        'properties': {
            'accept': {'$ref': '#/definitions/regex_list'},
            'reject': {'$ref': '#/definitions/regex_list'},
            'accept_excluding': {'$ref': '#/definitions/regex_list'},
            'reject_excluding': {'$ref': '#/definitions/regex_list'},
            'rest': {'type': 'string', 'enum': ['accept', 'reject']},
            'from': one_or_more({'type': 'string'})
        },
        'additionalProperties': False,
        'definitions': {
            # The validator for a list of regexps, each with or without settings
            'regex_list': {
                'type': 'array',
                'items': {
                    'oneOf': [
                        # Plain regex string
                        {'type': 'string', 'format': 'regex'},
                        # Regex with options (regex is key, options are value)
                        {
                            'type': 'object',
                            'additionalProperties': {
                                'oneOf': [
                                    # Simple options, just path
                                    {'type': 'string', 'format': 'path'},
                                    # Dict style options
                                    {
                                        'type': 'object',
                                        'properties': {
                                            'path': {'type': 'string', 'format': 'path'},
                                            'set': {'type': 'object'},
                                            'not': one_or_more({'type': 'string', 'format': 'regex'}),
                                            'from': one_or_more({'type': 'string'})
                                        },
                                        'additionalProperties': False
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        }
    }

    def prepare_config(self, config):
        """Returns the config in standard format.

        All regexps are turned into dictionaries in the form of {compiled regexp: options}

        :param config: Dict that can optionally contain the following keys
            path: will be attached to entries that match
            set: a dict of values to be attached to entries that match via set plugin
            from: a list of fields in entry for the regexps to match against
            not: a list of compiled regexps that if matching, will disqualify the main match
        :return: New config dictionary
        """
        out_config = {}
        if 'rest' in config:
            out_config['rest'] = config['rest']
        # Turn all our regexps into advanced form dicts and compile them
        for operation, regexps in config.iteritems():
            if operation in ['rest', 'from']:
                continue
            for regexp_item in regexps:
                if not isinstance(regexp_item, dict):
                    regexp = regexp_item
                    regexp_item = {regexp: {}}
                regexp, opts = regexp_item.items()[0]
                # Parse custom settings for this regexp
                if not isinstance(opts, dict):
                    opts = {'path': opts}
                else:
                    # We don't want to modify original config
                    opts = opts.copy()
                # advanced configuration
                if config.get('from'):
                    opts.setdefault('from', config['from'])
                # Put plain strings into list form for `from` and `not` options
                if 'from' in opts and isinstance(opts['from'], basestring):
                    opts['from'] = [opts['from']]
                if 'not' in opts and isinstance(opts['not'], basestring):
                    opts['not'] = [opts['not']]

                # compile `not` option regexps
                if 'not' in opts:
                    opts['not'] = [re.compile(not_re, re.IGNORECASE | re.UNICODE) for not_re in opts['not']]

                # compile regexp and make sure regexp is a string for series like '24'
                regexp = re.compile(unicode(regexp), re.IGNORECASE | re.UNICODE)
                out_config.setdefault(operation, []).append({regexp: opts})
        return out_config

    @plugin.priority(172)
    def on_task_filter(self, task, config):
        # TODO: what if accept and accept_excluding configured? Should raise error ...
        config = self.prepare_config(config)
        rest = []
        for operation, regexps in config.iteritems():
            if operation == 'rest':
                continue
            leftovers = self.filter(task, operation, regexps)
            if not rest:
                rest = leftovers
            else:
                # If there is already something in rest, take the intersection with r (entries no operations matched)
                rest = [entry for entry in leftovers if entry in rest]

        if 'rest' in config:
            rest_method = Entry.accept if config['rest'] == 'accept' else Entry.reject
            for entry in rest:
                log.debug('Rest method %s for %s' % (config['rest'], entry['title']))
                rest_method(entry, 'regexp `rest`')

    def matches(self, entry, regexp, find_from=None, not_regexps=None):
        """
        Check if :entry: has any string fields or strings in a list field that match :regexp:

        :param entry: Entry instance
        :param regexp: Compiled regexp
        :param find_from: None or a list of fields to search from
        :param not_regexps: None or list of regexps that can NOT match
        :return: Field matching
        """
        unquote = ['url']
        for field in find_from or ['title', 'description']:
            # Only evaluate lazy fields if find_from has been explicitly specified
            if not entry.get(field, eval_lazy=find_from):
                continue
            # Make all fields into lists for search purposes
            values = entry[field]
            if not isinstance(values, list):
                values = [values]
            for value in values:
                if not isinstance(value, basestring):
                    continue
                if field in unquote:
                    value = urllib.unquote(value)
                    # If none of the not_regexps match
                if regexp.search(value):
                    # Make sure the not_regexps do not match for this field
                    for not_regexp in not_regexps or []:
                        if self.matches(entry, not_regexp, find_from=[field]):
                            entry.trace('Configured not_regexp %s matched, ignored' % not_regexp)
                            break
                    else:  # None of the not_regexps matched
                        return field

    def filter(self, task, operation, regexps):
        """
        :param task: Task instance
        :param operation: one of 'accept' 'reject' 'accept_excluding' and 'reject_excluding'
                          accept and reject will be called on the entry if any of the regxps match
                          *_excluding operations will be called if any of the regexps don't match
        :param regexps: list of {compiled_regexp: options} dictionaries
        :return: Return list of entries that didn't match regexps
        """
        rest = []
        method = Entry.accept if 'accept' in operation else Entry.reject
        match_mode = 'excluding' not in operation
        for entry in task.entries:
            log.trace('testing %i regexps to %s' % (len(regexps), entry['title']))
            for regexp_opts in regexps:
                regexp, opts = regexp_opts.items()[0]

                # check if entry matches given regexp configuration
                field = self.matches(entry, regexp, opts.get('from'), opts.get('not'))

                # Run if we are in match mode and have a hit, or are in non-match mode and don't have a hit
                if match_mode == bool(field):
                    # Creates the string with the reason for the hit
                    matchtext = 'regexp \'%s\' ' % regexp.pattern + ('matched field \'%s\'' %
                                                                     field if match_mode else 'didn\'t match')
                    log.debug('%s for %s' % (matchtext, entry['title']))
                    # apply settings to entry and run the method on it
                    if opts.get('path'):
                        entry['path'] = opts['path']
                    if opts.get('set'):
                        # invoke set plugin with given configuration
                        log.debug('adding set: info to entry:"%s" %s' % (entry['title'], opts['set']))
                        set = plugin.get_plugin_by_name('set')
                        set.instance.modify(entry, opts['set'])
                    method(entry, matchtext)
                    # We had a match so break out of the regexp loop.
                    break
            else:
                # We didn't run method for any of the regexps, add this entry to rest
                entry.trace('None of configured %s regexps matched' % operation)
                rest.append(entry)
        return rest

@event('plugin.register')
def register_plugin():
    plugin.register(FilterRegexp, 'regexp', api_ver=2)

########NEW FILE########
__FILENAME__ = remember_rejected
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime, timedelta

from sqlalchemy import Column, Integer, String, Unicode, DateTime, ForeignKey, and_, Index
from sqlalchemy.orm import relation

from flexget import db_schema, options, plugin
from flexget.event import event
from flexget.manager import Session
from flexget.utils.sqlalchemy_utils import table_columns, drop_tables, table_add_column
from flexget.utils.tools import console, parse_timedelta

log = logging.getLogger('remember_rej')
Base = db_schema.versioned_base('remember_rejected', 3)


@db_schema.upgrade('remember_rejected')
def upgrade(ver, session):
    if ver is None:
        columns = table_columns('remember_rejected_entry', session)
        if 'uid' in columns:
            raise db_schema.UpgradeImpossible
        ver = 0
    if ver == 0:
        log.info('Adding reason column to remember_rejected_entry table.')
        table_add_column('remember_rejected_entry', 'reason', String, session)
        ver = 1
    if ver == 1:
        log.info('Adding `added` column to remember_rejected_entry table.')
        table_add_column('remember_rejected_entry', 'added', DateTime, session, default=datetime.now)
        ver = 2
    if ver == 2:
        log.info('Adding expires column to remember_rejected_entry table.')
        table_add_column('remember_rejected_entry', 'expires', DateTime, session)
        ver = 3
    return ver


class RememberTask(Base):

    __tablename__ = 'remember_rejected_feeds'

    id = Column(Integer, primary_key=True)
    name = Column(Unicode)

    entries = relation('RememberEntry', backref='task', cascade='all, delete, delete-orphan')


class RememberEntry(Base):

    __tablename__ = 'remember_rejected_entry'

    id = Column(Integer, primary_key=True)
    added = Column(DateTime, default=datetime.now)
    expires = Column(DateTime)
    title = Column(Unicode)
    url = Column(String)
    rejected_by = Column(String)
    reason = Column(String)

    task_id = Column('feed_id', Integer, ForeignKey('remember_rejected_feeds.id'), nullable=False)

Index('remember_feed_title_url', RememberEntry.task_id, RememberEntry.title, RememberEntry.url)


class FilterRememberRejected(object):
    """Internal.
    Rejects entries which have been rejected in the past.

    This is enabled when item is rejected with remember=True flag.

    Example::
        entry.reject('message', remember=True)
    """

    @plugin.priority(0)
    def on_task_start(self, task, config):
        """Purge remembered entries if the config has changed."""
        # See if the task has changed since last run
        old_task = task.session.query(RememberTask).filter(RememberTask.name == task.name).first()
        if not task.is_rerun and old_task and task.config_modified:
            log.debug('Task config has changed since last run, purging remembered entries.')
            task.session.delete(old_task)
            old_task = None
        if not old_task:
            # Create this task in the db if not present
            task.session.add(RememberTask(name=task.name))
        elif not task.is_rerun:
            # Delete expired items if this is not a rerun
            deleted = task.session.query(RememberEntry).filter(RememberEntry.task_id == old_task.id).\
                filter(RememberEntry.expires < datetime.now()).delete()
            if deleted:
                log.debug('%s entries have expired from remember_rejected table.' % deleted)
                task.config_changed()
        task.session.commit()

    @plugin.priority(-255)
    def on_task_input(self, task, config):
        for entry in task.all_entries:
            entry.on_reject(self.on_entry_reject, task=task)

    @plugin.priority(255)
    def on_task_filter(self, task, config):
        """Reject any remembered entries from previous runs"""
        (task_id,) = task.session.query(RememberTask.id).filter(RememberTask.name == task.name).first()
        reject_entries = task.session.query(RememberEntry).filter(RememberEntry.task_id == task_id)
        if reject_entries.count():
            # Reject all the remembered entries
            for entry in task.entries:
                if not entry.get('url'):
                    # We don't record or reject any entries without url
                    continue
                reject_entry = reject_entries.filter(and_(RememberEntry.title == entry['title'],
                                                          RememberEntry.url == entry['original_url'])).first()
                if reject_entry:
                    entry.reject('Rejected on behalf of %s plugin: %s' %
                        (reject_entry.rejected_by, reject_entry.reason))

    def on_entry_reject(self, entry, task=None, remember=None, remember_time=None, **kwargs):
        # We only remember rejections that specify the remember keyword argument
        if not remember and not remember_time:
            return
        expires = None
        if remember_time:
            if isinstance(remember_time, basestring):
                remember_time = parse_timedelta(remember_time)
            expires = datetime.now() + remember_time
        if not entry.get('title') or not entry.get('original_url'):
            log.debug('Can\'t remember rejection for entry without title or url.')
            return
        message = 'Remembering rejection of `%s`' % entry['title']
        if remember_time:
            message += ' for %i minutes' % (remember_time.seconds / 60)
        log.info(message)
        (remember_task_id,) = task.session.query(RememberTask.id).filter(RememberTask.name == task.name).first()
        task.session.add(RememberEntry(title=entry['title'], url=entry['original_url'], task_id=remember_task_id,
                                       rejected_by=task.current_plugin, reason=kwargs.get('reason'), expires=expires))
        # The test stops passing when this is taken out for some reason...
        task.session.flush()

def do_cli(manager, options):
    if options.rejected_action == 'list':
        list_rejected()
    elif options.rejected_action == 'clear':
        clear_rejected(manager)

def list_rejected():
    session = Session()
    try:
        results = session.query(RememberEntry).all()
        if not results:
            console('No rejected entries recorded by remember_rejected')
        else:
            console('Rejections remembered by remember_rejected:')
        for entry in results:
            console('%s from %s by %s because %s' % (entry.title, entry.task.name, entry.rejected_by, entry.reason))
    finally:
        session.close()


def clear_rejected(manager):
    session = Session()
    try:
        results = session.query(RememberEntry).delete()
        console('Cleared %i items.' % results)
        session.commit()
        if results:
            manager.config_changed()
    finally:
        session.close()

@event('manager.db_cleanup')
def db_cleanup(session):
    # Remove entries older than 30 days
    result = session.query(RememberEntry).filter(RememberEntry.added < datetime.now() - timedelta(days=30)).delete()
    if result:
        log.verbose('Removed %d entries from remember rejected table.' % result)

@event('plugin.register')
def register_plugin():
    plugin.register(FilterRememberRejected, 'remember_rejected', builtin=True, api_ver=2)

@event('options.register')
def register_parser_arguments():
    parser = options.register_command('rejected', do_cli, help='list or clear remembered rejections')
    subparsers = parser.add_subparsers(dest='rejected_action', metavar='<action>')
    subparsers.add_parser('list', help='list all the entries that have been rejected')
    subparsers.add_parser('clear', help='clear all rejected entries from database, so they can be retried')

########NEW FILE########
__FILENAME__ = require_field
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.event import event

log = logging.getLogger('require_field')


class FilterRequireField(object):
    """
    Rejects entries without defined field.

    Example::

      require_field: imdb_url
    """

    schema = one_or_more({'type': 'string'})

    @plugin.priority(32)
    def on_task_filter(self, task, config):
        if isinstance(config, basestring):
            config = [config]
        for entry in task.entries:
            for field in config:
                if not entry.get(field):
                    entry.reject('Required field %s is not present' % field)
                    break


@event('plugin.register')
def register_plugin():
    plugin.register(FilterRequireField, 'require_field', api_ver=2)

########NEW FILE########
__FILENAME__ = retry_failed
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime, timedelta

from sqlalchemy import Column, Integer, String, Unicode, DateTime
from sqlalchemy.schema import Index

from flexget import db_schema, options, plugin
from flexget.event import event
from flexget.manager import Session
from flexget.utils.tools import console, parse_timedelta
from flexget.utils.sqlalchemy_utils import table_add_column, table_schema

SCHEMA_VER = 2

log = logging.getLogger('failed')
Base = db_schema.versioned_base('failed', SCHEMA_VER)


@db_schema.upgrade('failed')
def upgrade(ver, session):
    if ver is None:
        # add count column
        table_add_column('failed', 'count', Integer, session, default=1)
        ver = 0
    if ver == 0:
        # define an index
        log.info('Adding database index ...')
        failed = table_schema('failed', session)
        Index('failed_title_url', failed.c.title, failed.c.url, failed.c.count).create()
        ver = 1
    if ver == 1:
        table_add_column('failed', 'reason', Unicode, session)
        ver = 2
    return ver


class FailedEntry(Base):
    __tablename__ = 'failed'

    id = Column(Integer, primary_key=True)
    title = Column(Unicode)
    url = Column(String)
    tof = Column(DateTime)
    reason = Column(Unicode)
    count = Column(Integer, default=1)

    def __init__(self, title, url, reason=None):
        self.title = title
        self.url = url
        self.reason = reason
        self.tof = datetime.now()

    def __str__(self):
        return '<Failed(title=%s)>' % self.title

# create indexes, used when creating tables
columns = Base.metadata.tables['failed'].c
Index('failed_title_url', columns.title, columns.url, columns.count)


class PluginFailed(object):
    """
    Records entry failures and stores them for trying again after a certain interval.
    Rejects them after they have failed too many times.

    """

    schema = {
        "oneOf": [
            # Allow retry_failed: no form to turn off plugin altogether
            {"type": "boolean"},
            {
                "type": "object",
                "properties": {
                    "retry_time": {"type": "string", "format": "interval", "default": "1 hour"},
                    "max_retries": {"type": "integer", "minimum": 0, "default": 3},
                    "retry_time_multiplier": {
                        # Allow turning off the retry multiplier with 'no' as well as 1
                        "oneOf": [{"type": "number", "minimum": 0}, {"type": "boolean"}],
                        "default": 1.5
                    }
                },
                "additionalProperties": False
            }
        ]
    }

    def __init__(self):
        try:
            self.backlog = plugin.get_plugin_by_name('backlog')
        except plugin.DependencyError:
            log.warning('Unable utilize backlog plugin, failed entries may not be retried properly.')

    def prepare_config(self, config):
        if not isinstance(config, dict):
            config = {}
        config.setdefault('retry_time', '1 hour')
        config.setdefault('max_retries', 3)
        if config.get('retry_time_multiplier', True) is True:
            # If multiplier is not specified, or is specified as True, use the default
            config['retry_time_multiplier'] = 1.5
        else:
            # If multiplier is False, turn it off
            config['retry_time_multiplier'] = 1
        return config

    @plugin.priority(-255)
    def on_task_input(self, task, config):
        for entry in task.all_entries:
            entry.on_fail(self.add_failed)

    def add_failed(self, entry, reason=None, **kwargs):
        """Adds entry to internal failed list, displayed with --failed"""
        reason = reason or 'Unknown'
        failed = Session()
        try:
            # query item's existence
            item = failed.query(FailedEntry).filter(FailedEntry.title == entry['title']).\
                filter(FailedEntry.url == entry['original_url']).first()
            if not item:
                item = FailedEntry(entry['title'], entry['original_url'], reason)
            else:
                item.count += 1
                item.tof = datetime.now()
                item.reason = reason
            failed.merge(item)
            log.debug('Marking %s in failed list. Has failed %s times.' % (item.title, item.count))

            # limit item number to 25
            for row in failed.query(FailedEntry).order_by(FailedEntry.tof.desc())[25:]:
                failed.delete(row)
            failed.commit()
        finally:
            failed.close()

    @plugin.priority(255)
    def on_task_filter(self, task, config):
        if config is False:
            return
        config = self.prepare_config(config)
        max_count = config['max_retries']
        for entry in task.entries:
            item = task.session.query(FailedEntry).filter(FailedEntry.title == entry['title']).\
                filter(FailedEntry.url == entry['original_url']).\
                filter(FailedEntry.count > max_count).first()
            if item:
                entry.reject('Has already failed %s times in the past' % item.count)

    def on_task_learn(self, task, config):
        if config is False:
            return
        config = self.prepare_config(config)
        base_retry_time = parse_timedelta(config['retry_time'])
        retry_time_multiplier = config['retry_time_multiplier']
        for entry in task.failed:
            item = task.session.query(FailedEntry).filter(FailedEntry.title == entry['title']).\
                filter(FailedEntry.url == entry['original_url']).first()
            if item:
                # Do not count the failure on this run when adding additional retry time
                fail_count = item.count - 1
                # Don't bother saving this if it has met max retries
                if fail_count >= config['max_retries']:
                    continue
                # Timedeltas do not allow floating point multiplication. Convert to seconds and then back to avoid this.
                base_retry_secs = base_retry_time.days * 86400 + base_retry_time.seconds
                retry_secs = base_retry_secs * (retry_time_multiplier ** fail_count)
                retry_time = timedelta(seconds=retry_secs)
            else:
                retry_time = base_retry_time
            if self.backlog:
                self.backlog.instance.add_backlog(task, entry, amount=retry_time)
            if retry_time:
                fail_reason = item.reason if item else entry.get('reason', 'unknown')
                entry.reject(reason='Waiting before trying failed entry again. (failure reason: %s)' %
                    fail_reason, remember_time=retry_time)
                # Cause a task rerun, to look for alternate releases
                task.rerun()


def do_cli(manager, options):
    if options.failed_action == 'list':
        list_failed()
    elif options.failed_action == 'clear':
        clear_failed(manager)


def list_failed():
    session = Session()
    try:
        results = session.query(FailedEntry).all()
        if not results:
            console('No failed entries recorded')
        for entry in results:
            console('%16s - %s - %s times - %s' %
                    (entry.tof.strftime('%Y-%m-%d %H:%M'), entry.title, entry.count, entry.reason))
    finally:
        session.close()


def clear_failed(manager):
    session = Session()
    try:
        results = session.query(FailedEntry).delete()
        console('Cleared %i items.' % results)
        session.commit()
        if results:
            manager.config_changed()
    finally:
        session.close()


@event('plugin.register')
def register_plugin():
    plugin.register(PluginFailed, 'retry_failed', builtin=True, api_ver=2)

@event('options.register')
def register_parser_arguments():
    parser = options.register_command('failed', do_cli, help='list or clear remembered failures')
    subparsers = parser.add_subparsers(dest='failed_action', metavar='<action>')
    subparsers.add_parser('list', help='list all the entries that have had failures')
    subparsers.add_parser('clear', help='clear all failures from database, so they can be retried')

########NEW FILE########
__FILENAME__ = rottentomatoes
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.log import log_once

log = logging.getLogger('rt')


class FilterRottenTomatoes(object):
    """
        This plugin allows filtering based on Rotten Tomatoes score, votes and genres etc.

        Configuration:

        Note: All parameters are optional. Some are mutually exclusive.

        min_critics_score: <num>
        min_audience_score: <num>
        min_average_score: <num>
        min_critics_rating: <rotten, fresh, or certified fresh>
        min_audience_rating: <upright or spilled>
        min_year: <num>
        max_year: <num>

        # reject if genre contains any of these
        reject_genres:
            - genre1
            - genre2

        # accept movies with any of these actors
        accept_actors:
            - actor1
            - actor2

        # reject movie if it has any of these actors
        reject_actors:
            - actor3
            - actor4

        # accept all movies by these directors
        accept_directors:
            - director1

        # reject movies by these directors
        reject_directors:
            - director2

        # reject movies with any of these ratings
        reject_mpaa_ratings:
            - PG-13
            - R
            - X

        # accept movies with only these ratings
        accept_mpaa_ratings:
            - PG
            - G
    """

    def __init__(self):
        # We could pull these from the API through lists.json but that's extra web/API key usage
        self.critics_ratings = {'rotten': 0, 'fresh': 1, 'certified fresh': 2}
        self.audience_ratings = {'spilled': 0, 'upright': 1}

    def validator(self):
        from flexget import validator
        rt = validator.factory('dict')
        rt.accept('integer', key='min_year')
        rt.accept('integer', key='max_year')
        rt.accept('number', key='min_critics_score')
        rt.accept('number', key='min_audience_score')
        rt.accept('number', key='min_average_score')
        rt.accept('choice', key='min_critics_rating').accept_choices(self.critics_ratings.keys())
        rt.accept('choice', key='min_audience_rating').accept_choices(self.audience_ratings.keys())
        rt.accept('list', key='reject_genres').accept('text')
        rt.accept('list', key='reject_actors').accept('text')
        rt.accept('list', key='accept_actors').accept('text')
        rt.accept('list', key='reject_directors').accept('text')
        rt.accept('list', key='accept_directors').accept('text')
        rt.accept('list', key='reject_mpaa_ratings').accept('text')
        rt.accept('list', key='accept_mpaa_ratings').accept('text')
        return rt

    # Run later to avoid unnecessary lookups
    @plugin.priority(115)
    def on_task_filter(self, task, config):

        lookup = plugin.get_plugin_by_name('rottentomatoes_lookup').instance.lookup

        # since the plugin does not reject anything, no sense going trough accepted
        for entry in task.undecided:

            force_accept = False

            try:
                lookup(entry)
            except plugin.PluginError as e:
                # logs skip message once through log_once (info) and then only when ran from cmd line (w/o --cron)
                msg = 'Skipping %s because of an error: %s' % (entry['title'], e.value)
                if not log_once(msg, logger=log):
                    log.verbose(msg)
                continue

            #for key, value in entry.iteritems():
            #    log.debug('%s = %s (type: %s)' % (key, value, type(value)))

            # Check defined conditions, TODO: rewrite into functions?
            reasons = []
            if 'min_critics_score' in config:
                if entry.get('rt_critics_score', 0) < config['min_critics_score']:
                    reasons.append('min_critics_score (%s < %s)' % (entry.get('rt_critics_score'),
                        config['min_critics_score']))
            if 'min_audience_score' in config:
                if entry.get('rt_audience_score', 0) < config['min_audience_score']:
                    reasons.append('min_audience_score (%s < %s)' % (entry.get('rt_audience_score'),
                        config['min_audience_score']))
            if 'min_average_score' in config:
                if entry.get('rt_average_score', 0) < config['min_average_score']:
                    reasons.append('min_average_score (%s < %s)' % (entry.get('rt_average_score'),
                        config['min_average_score']))
            if 'min_critics_rating' in config:
                if not entry.get('rt_critics_rating'):
                    reasons.append('min_critics_rating (no rt_critics_rating)')
                elif self.critics_ratings.get(entry.get('rt_critics_rating').lower(), 0) < self.critics_ratings[config['min_critics_rating']]:
                    reasons.append('min_critics_rating (%s < %s)' % (entry.get('rt_critics_rating').lower(), config['min_critics_rating']))
            if 'min_audience_rating' in config:
                if not entry.get('rt_audience_rating'):
                    reasons.append('min_audience_rating (no rt_audience_rating)')
                elif self.audience_ratings.get(entry.get('rt_audience_rating').lower(), 0) < self.audience_ratings[config['min_audience_rating']]:
                    reasons.append('min_audience_rating (%s < %s)' % (entry.get('rt_audience_rating').lower(), config['min_audience_rating']))
            if 'min_year' in config:
                if entry.get('rt_year', 0) < config['min_year']:
                    reasons.append('min_year (%s < %s)' % (entry.get('rt_year'), config['min_year']))
            if 'max_year' in config:
                if entry.get('rt_year', 0) > config['max_year']:
                    reasons.append('max_year (%s > %s)' % (entry.get('rt_year'), config['max_year']))
            if 'reject_genres' in config:
                rejected = config['reject_genres']
                for genre in entry.get('rt_genres', []):
                    if genre in rejected:
                        reasons.append('reject_genres')
                        break

            if 'reject_actors' in config:
                rejected = config['reject_actors']
                for actor_name in entry.get('rt_actors', []):
                    if actor_name in rejected:
                        reasons.append('reject_actors %s' % actor_name)
                        break

            # Accept if actors contains an accepted actor, but don't reject otherwise
            if 'accept_actors' in config:
                accepted = config['accept_actors']
                for actor_name in entry.get('rt_actors', []):
                    if actor_name in accepted:
                        log.debug('Accepting because of accept_actors %s' % actor_name)
                        force_accept = True
                        break

            if 'reject_directors' in config:
                rejected = config['reject_directors']
                for director_name in entry.get('rt_directors', []):
                    if director_name in rejected:
                        reasons.append('reject_directors %s' % director_name)
                        break

            # Accept if the director is in the accept list, but do not reject if the director is unknown
            if 'accept_directors' in config:
                accepted = config['accept_directors']
                for director_name in entry.get('rt_directors', []):
                    if director_name in accepted:
                        log.debug('Accepting because of accept_directors %s' % director_name)
                        force_accept = True
                        break

            if 'reject_mpaa_ratings' in config:
                rejected = config['reject_mpaa_ratings']
                if entry.get('rt_mpaa_rating') in rejected:
                    reasons.append('reject_mpaa_ratings %s' % entry['rt_mpaa_rating'])

            if 'accept_mpaa_ratings' in config:
                accepted = config['accept_mpaa_ratings']
                if entry.get('rt_mpaa_rating') not in accepted:
                    reasons.append('accept_mpaa_ratings %s' % entry.get('rt_mpaa_rating'))

            if reasons and not force_accept:
                msg = 'Didn\'t accept `%s` because of rule(s) %s' % \
                    (entry.get('rt_name', None) or entry['title'], ', '.join(reasons))
                if task.options.debug:
                    log.debug(msg)
                else:
                    if task.options.cron:
                        log_once(msg, log)
                    else:
                        log.info(msg)
            else:
                log.debug('Accepting %s' % (entry['title']))
                entry.accept()


@event('plugin.register')
def register_plugin():
    plugin.register(FilterRottenTomatoes, 'rottentomatoes', api_ver=2)

########NEW FILE########
__FILENAME__ = seen
"""
Listens events:

forget (string)

    Given string can be task name, remembered field (url, imdb_url) or a title. If given value is a
    task name then everything in that task will be forgotten. With title all learned fields from it and the
    title will be forgotten. With field value only that particular field is forgotten.
"""

from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime, timedelta

from sqlalchemy import Column, Integer, DateTime, Unicode, Boolean, or_, select, update, Index
from sqlalchemy.orm import relation
from sqlalchemy.schema import ForeignKey

from flexget import db_schema, options, plugin
from flexget.event import event
from flexget.manager import Session
from flexget.utils.imdb import is_imdb_url, extract_id
from flexget.utils.sqlalchemy_utils import table_schema, table_add_column
from flexget.utils.tools import console

log = logging.getLogger('seen')
Base = db_schema.versioned_base('seen', 4)


@db_schema.upgrade('seen')
def upgrade(ver, session):
    if ver is None:
        log.info('Converting seen imdb_url to imdb_id for seen movies.')
        field_table = table_schema('seen_field', session)
        for row in session.execute(select([field_table.c.id, field_table.c.value], field_table.c.field == 'imdb_url')):
            new_values = {'field': 'imdb_id', 'value': extract_id(row['value'])}
            session.execute(update(field_table, field_table.c.id == row['id'], new_values))
        ver = 1
    if ver == 1:
        field_table = table_schema('seen_field', session)
        log.info('Adding index to seen_field table.')
        Index('ix_seen_field_seen_entry_id', field_table.c.seen_entry_id).create(bind=session.bind)
        ver = 2
    if ver == 2:
        log.info('Adding local column to seen_entry table')
        table_add_column('seen_entry', 'local', Boolean, session, default=False)
        ver = 3
    if ver == 3:
        # setting the default to False in the last migration was broken, fix the data
        log.info('Repairing seen table')
        entry_table = table_schema('seen_entry', session)
        session.execute(update(entry_table, entry_table.c.local == None, {'local': False}))
        ver = 4

    return ver


class SeenEntry(Base):

    __tablename__ = 'seen_entry'

    id = Column(Integer, primary_key=True)
    title = Column(Unicode)
    reason = Column(Unicode)
    task = Column('feed', Unicode)
    added = Column(DateTime)
    local = Column(Boolean)

    fields = relation('SeenField', backref='seen_entry', cascade='all, delete, delete-orphan')

    def __init__(self, title, task, reason=None, local=False):
        self.title = title
        self.reason = reason
        self.task = task
        self.added = datetime.now()
        self.local = local

    def __str__(self):
        return '<SeenEntry(title=%s,reason=%s,task=%s,added=%s)>' % (self.title, self.reason, self.task, self.added)


class SeenField(Base):

    __tablename__ = 'seen_field'

    id = Column(Integer, primary_key=True)
    seen_entry_id = Column(Integer, ForeignKey('seen_entry.id'), nullable=False, index=True)
    field = Column(Unicode)
    value = Column(Unicode, index=True)
    added = Column(DateTime)

    def __init__(self, field, value):
        self.field = field
        self.value = value
        self.added = datetime.now()

    def __str__(self):
        return '<SeenField(field=%s,value=%s,added=%s)>' % (self.field, self.value, self.added)


@event('forget')
def forget(value):
    """
    See module docstring
    :param string value: Can be task name, entry title or field value
    :return: count, field_count where count is number of entries removed and field_count number of fields
    """
    log.debug('forget called with %s' % value)
    session = Session()

    try:
        count = 0
        field_count = 0
        for se in session.query(SeenEntry).filter(or_(SeenEntry.title == value, SeenEntry.task == value)).all():
            field_count += len(se.fields)
            count += 1
            log.debug('forgetting %s' % se)
            session.delete(se)

        for sf in session.query(SeenField).filter(SeenField.value == value).all():
            se = session.query(SeenEntry).filter(SeenEntry.id == sf.seen_entry_id).first()
            field_count += len(se.fields)
            count += 1
            log.debug('forgetting %s' % se)
            session.delete(se)
        return count, field_count
    finally:
        session.commit()
        session.close()


class FilterSeen(object):
    """
        Remembers previously downloaded content and rejects them in
        subsequent executions. Without this plugin FlexGet would
        download all matching content on every execution.

        This plugin is enabled on all tasks by default.
        See wiki for more information.
    """

    def __init__(self):
        # remember and filter by these fields
        self.fields = ['title', 'url', 'original_url']
        self.keyword = 'seen'

    def validator(self):
        from flexget import validator
        root = validator.factory()
        root.accept('boolean')
        root.accept('choice').accept_choices(['global', 'local'])
        return root

    @plugin.priority(255)
    def on_task_filter(self, task, config, remember_rejected=False):
        """Filter seen entries"""
        if config is False:
            log.debug('%s is disabled' % self.keyword)
            return

        fields = self.fields
        local = config == 'local'

        for entry in task.entries:
            # construct list of values looked
            values = []
            for field in fields:
                if field not in entry:
                    continue
                if entry[field] not in values and entry[field]:
                    values.append(unicode(entry[field]))
            if values:
                log.trace('querying for: %s' % ', '.join(values))
                # check if SeenField.value is any of the values
                found = task.session.query(SeenField).join(SeenEntry).filter(SeenField.value.in_(values))
                if local:
                    found = found.filter(SeenEntry.task == task.name)
                else:
                    found = found.filter(SeenEntry.local == False)
                found = found.first()
                if found:
                    log.debug("Rejecting '%s' '%s' because of seen '%s'" % (entry['url'], entry['title'], found.value))
                    se = task.session.query(SeenEntry).filter(SeenEntry.id == found.seen_entry_id).one()
                    entry.reject('Entry with %s `%s` is already marked seen in the task %s at %s' %
                                 (found.field, found.value, se.task, se.added.strftime('%Y-%m-%d %H:%M')),
                                remember=remember_rejected)

    def on_task_learn(self, task, config):
        """Remember succeeded entries"""
        if config is False:
            log.debug('disabled')
            return

        fields = self.fields
        if isinstance(config, list):
            fields.extend(config)

        for entry in task.accepted:
            self.learn(task, entry, fields=fields, local=config == 'local')
            # verbose if in learning mode
            if task.options.learn:
                log.info("Learned '%s' (will skip this in the future)" % (entry['title']))

    def learn(self, task, entry, fields=None, reason=None, local=False):
        """Marks entry as seen"""
        # no explicit fields given, use default
        if not fields:
            fields = self.fields
        se = SeenEntry(entry['title'], unicode(task.name), reason, local)
        remembered = []
        for field in fields:
            if not field in entry:
                continue
            # removes duplicate values (eg. url, original_url are usually same)
            if entry[field] in remembered:
                continue
            remembered.append(entry[field])
            sf = SeenField(unicode(field), unicode(entry[field]))
            se.fields.append(sf)
            log.debug("Learned '%s' (field: %s)" % (entry[field], field))
        # Only add the entry to the session if it has one of the required fields
        if se.fields:
            task.session.add(se)

    def forget(self, task, title):
        """Forget SeenEntry with :title:. Return True if forgotten."""
        se = task.session.query(SeenEntry).filter(SeenEntry.title == title).first()
        if se:
            log.debug("Forgotten '%s' (%s fields)" % (title, len(se.fields)))
            task.session.delete(se)
            return True


@event('manager.db_cleanup')
def db_cleanup(session):
    log.debug('TODO: Disabled because of ticket #1321')
    return

    # Remove seen fields over a year old
    result = session.query(SeenField).filter(SeenField.added < datetime.now() - timedelta(days=365)).delete()
    if result:
        log.verbose('Removed %d seen fields older than 1 year.' % result)


def do_cli(manager, options):
    if options.seen_action == 'forget':
        seen_forget(manager, options)
    elif options.seen_action == 'add':
        seen_add(options)
    elif options.seen_action == 'search':
        seen_search(options)


def seen_forget(manager, options):
    forget_name = options.forget_value
    if is_imdb_url(forget_name):
        imdb_id = extract_id(forget_name)
        if imdb_id:
            forget_name = imdb_id

    count, fcount = forget(forget_name)
    console('Removed %s titles (%s fields)' % (count, fcount))
    manager.config_changed()


def seen_add(options):
    seen_name = options.add_value
    if is_imdb_url(seen_name):
        imdb_id = extract_id(seen_name)
        if imdb_id:
            seen_name = imdb_id

    session = Session()
    se = SeenEntry(seen_name, 'cli_seen')
    sf = SeenField('cli_seen', seen_name)
    se.fields.append(sf)
    session.add(se)
    session.commit()
    console('Added %s as seen. This will affect all tasks.' % seen_name)


def seen_search(options):
    session = Session()
    try:
        search_term = '%' + options.search_term + '%'
        seen_entries = (session.query(SeenEntry).join(SeenField).
                        filter(SeenField.value.like(search_term)).order_by(SeenField.added).all())

        for se in seen_entries:
            console('ID: %s Name: %s Task: %s Added: %s' % (se.id, se.title, se.task, se.added.strftime('%c')))
            for sf in se.fields:
                console(' %s: %s' % (sf.field, sf.value))
            console('')

        if not seen_entries:
            console('No results')
    finally:
        session.close()


@event('plugin.register')
def register_plugin():
    plugin.register(FilterSeen, 'seen', builtin=True, api_ver=2)


@event('options.register')
def register_parser_arguments():
    parser = options.register_command('seen', do_cli, help='view or forget entries remembered by the seen plugin')
    subparsers = parser.add_subparsers(dest='seen_action', metavar='<action>')
    forget_parser = subparsers.add_parser('forget', help='forget entry or entire task from seen plugin database')
    forget_parser.add_argument('forget_value', metavar='<value>',
                               help='title or url of entry to forget, or name of task to forget')
    add_parser = subparsers.add_parser('add', help='add a title or url to the seen database')
    add_parser.add_argument('add_value', metavar='<value>', help='the title or url to add')
    search_parser = subparsers.add_parser('search', help='search text from the seen database')
    search_parser.add_argument('search_term', metavar='<search term>')

########NEW FILE########
__FILENAME__ = seen_info_hash
from __future__ import unicode_literals, division, absolute_import

from flexget import plugin
from flexget.event import event
from flexget.plugins.filter.seen import FilterSeen


class FilterSeenInfoHash(FilterSeen):
    """Prevents the same torrent from being downloaded twice by remembering the infohash of all downloaded torrents."""

    def __init__(self):
        # remember and filter by these fields
        self.fields = ['torrent_info_hash']
        self.keyword = 'seen_info_hash'

    def validator(self):
        from flexget import validator
        return validator.factory('boolean')

    @plugin.priority(180)
    def on_task_filter(self, task, config):
        # Return if we are disabled.
        if config is False:
            return
        # First make sure all the torrent_info_hash fields are in upper case
        for entry in task.entries:
            if isinstance(entry.get('torrent_info_hash'), basestring):
                entry['torrent_info_hash'] = entry['torrent_info_hash'].upper()
        FilterSeen.on_task_filter(self, task, config, remember_rejected=True)

    def on_task_modify(self, task, config):
        # Return if we are disabled.
        if config is False:
            return
        # Run the filter again after the torrent plugin has populated the infohash
        self.on_task_filter(task, config)
        # Make sure no duplicates were accepted this run
        accepted_infohashes = set()
        for entry in task.accepted:
            if 'torrent_info_hash' in entry:
                infohash = entry['torrent_info_hash']
                if infohash in accepted_infohashes:
                    entry.reject('Already accepted torrent with this infohash once for this task')
                else:
                    accepted_infohashes.add(infohash)


@event('plugin.register')
def register_plugin():
    plugin.register(FilterSeenInfoHash, 'seen_info_hash', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = seen_movies
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.plugins.filter.seen import FilterSeen

log = logging.getLogger('seenmovies')


class FilterSeenMovies(FilterSeen):
    """
        Prevents movies being downloaded twice.
        Works only on entries which have imdb url available.

        How duplicate movie detection works:
        1) Remember all imdb urls from downloaded entries.
        2) If stored imdb url appears again, entry is rejected.
    """

    def __init__(self):
        # remember and filter by these fields
        self.fields = ['imdb_id', 'tmdb_id']
        self.keyword = 'seen_movies'

    def validator(self):
        from flexget import validator
        root = validator.factory('choice', message="must be one of the following: strict, loose")
        root.accept_choices(['strict', 'loose'])
        return root

    # We run last (-255) to make sure we don't reject duplicates before all the other plugins get a chance to reject.
    @plugin.priority(-255)
    def on_task_filter(self, task, config):
        # strict method
        if config == 'strict':
            for entry in task.entries:
                if 'imdb_id' not in entry and 'tmdb_id' not in entry:
                    log.info('Rejecting %s because of missing movie (imdb or tmdb) id' % entry['title'])
                    entry.reject('missing movie (imdb or tmdb) id, strict')
        # call super
        super(FilterSeenMovies, self).on_task_filter(task, True)
        # check that two copies of a movie have not been accepted this run
        imdb_ids = set()
        tmdb_ids = set()
        for entry in task.accepted:
            if 'imdb_id' in entry:
                if entry['imdb_id'] in imdb_ids:
                    entry.reject('already accepted once in task')
                    continue
                else:
                    imdb_ids.add(entry['imdb_id'])
            if 'tmdb_id' in entry:
                if entry['tmdb_id'] in tmdb_ids:
                    entry.reject('already accepted once in task')
                    continue
                else:
                    tmdb_ids.add(entry['tmdb_id'])

@event('plugin.register')
def register_plugin():
    plugin.register(FilterSeenMovies, 'seen_movies', api_ver=2)

########NEW FILE########
__FILENAME__ = series
from __future__ import unicode_literals, division, absolute_import
import argparse
import logging
import re
import time
from copy import copy
from datetime import datetime, timedelta

from sqlalchemy import (Column, Integer, String, Unicode, DateTime, Boolean,
                        desc, select, update, delete, ForeignKey, Index, func, and_, not_)
from sqlalchemy.orm import relation, backref
from sqlalchemy.ext.hybrid import Comparator, hybrid_property
from sqlalchemy.exc import OperationalError

from flexget import db_schema, options, plugin
from flexget.config_schema import one_or_more
from flexget.event import event
from flexget.manager import Session
from flexget.utils import qualities
from flexget.utils.log import log_once
from flexget.utils.titles import SeriesParser, ParseWarning, ID_TYPES
from flexget.utils.sqlalchemy_utils import (table_columns, table_exists, drop_tables, table_schema, table_add_column,
                                            create_index)
from flexget.utils.tools import merge_dict_from_to, parse_timedelta
from flexget.utils.database import quality_property

SCHEMA_VER = 11

log = logging.getLogger('series')
Base = db_schema.versioned_base('series', SCHEMA_VER)


@db_schema.upgrade('series')
def upgrade(ver, session):
    if ver is None:
        if table_exists('episode_qualities', session):
            log.info('Series database format is too old to upgrade, dropping and recreating tables.')
            # Drop the deprecated data
            drop_tables(['series', 'series_episodes', 'episode_qualities'], session)
            # Create new tables from the current models
            Base.metadata.create_all(bind=session.bind)
        # Upgrade episode_releases table to have a proper count and seed it with appropriate numbers
        columns = table_columns('episode_releases', session)
        if not 'proper_count' in columns:
            log.info('Upgrading episode_releases table to have proper_count column')
            table_add_column('episode_releases', 'proper_count', Integer, session)
            release_table = table_schema('episode_releases', session)
            for row in session.execute(select([release_table.c.id, release_table.c.title])):
                # Recalculate the proper_count from title for old episodes
                proper_count = len([part for part in re.split('[\W_]+', row['title'].lower())
                                    if part in SeriesParser.propers])
                session.execute(update(release_table, release_table.c.id == row['id'], {'proper_count': proper_count}))
        ver = 0
    if ver == 0:
        log.info('Migrating first_seen column from series_episodes to episode_releases table.')
        # Create the column in episode_releases
        table_add_column('episode_releases', 'first_seen', DateTime, session)
        # Seed the first_seen value for all the past releases with the first_seen of their episode.
        episode_table = table_schema('series_episodes', session)
        release_table = table_schema('episode_releases', session)
        for row in session.execute(select([episode_table.c.id, episode_table.c.first_seen])):
            session.execute(update(release_table, release_table.c.episode_id == row['id'],
                                   {'first_seen': row['first_seen']}))
        ver = 1
    if ver == 1:
        log.info('Adding `identified_by` column to series table.')
        table_add_column('series', 'identified_by', String, session)
        ver = 2
    if ver == 2:
        log.info('Creating index on episode_releases table.')
        create_index('episode_releases', session, 'episode_id')
        ver = 3
    if ver == 3:
        # Remove index on Series.name
        try:
            Index('ix_series_name').drop(bind=session.bind)
        except OperationalError:
            log.debug('There was no ix_series_name index to remove.')
        # Add Series.name_lower column
        log.info('Adding `name_lower` column to series table.')
        table_add_column('series', 'name_lower', Unicode, session)
        series_table = table_schema('series', session)
        create_index('series', session, 'name_lower')
        # Fill in lower case name column
        session.execute(update(series_table, values={'name_lower': func.lower(series_table.c.name)}))
        ver = 4
    if ver == 4:
        log.info('Adding `identified_by` column to episodes table.')
        table_add_column('series_episodes', 'identified_by', String, session)
        series_table = table_schema('series', session)
        # Clear out identified_by id series so that they can be auto detected again
        session.execute(update(series_table, series_table.c.identified_by != 'ep', {'identified_by': None}))
        # Warn users about a possible config change needed.
        log.warning('If you are using `identified_by: id` option for the series plugin for date, '
                    'or abolute numbered series, you will need to update your config. Two new identified_by modes have '
                    'been added, `date` and `sequence`. In addition, if you are using auto identified_by, it will'
                    'be relearned based on upcoming episodes.')
        ver = 5
    if ver == 5:
        # Episode advancement now relies on identified_by being filled for the episodes.
        # This action retroactively marks 'ep' mode for all episodes where the series is already in 'ep' mode.
        series_table = table_schema('series', session)
        ep_table = table_schema('series_episodes', session)
        ep_mode_series = select([series_table.c.id], series_table.c.identified_by == 'ep')
        where_clause = and_(ep_table.c.series_id.in_(ep_mode_series),
            ep_table.c.season != None, ep_table.c.number != None, ep_table.c.identified_by == None)
        session.execute(update(ep_table, where_clause, {'identified_by': 'ep'}))
        ver = 6
    if ver == 6:
        # Translate old qualities into new quality requirements
        release_table = table_schema('episode_releases', session)
        for row in session.execute(select([release_table.c.id, release_table.c.quality])):
            # Webdl quality no longer has dash
            new_qual = row['quality'].replace('web-dl', 'webdl')
            if row['quality'] != new_qual:
                session.execute(update(release_table, release_table.c.id == row['id'],
                                       {'quality': new_qual}))
        ver = 7
    # Normalization rules changed for 7 and 8, but only run this once
    if ver in [7, 8]:
        # Merge series that qualify as duplicates with new normalization scheme
        series_table = table_schema('series', session)
        ep_table = table_schema('series_episodes', session)
        all_series = session.execute(select([series_table.c.name, series_table.c.id]))
        unique_series = {}
        for row in all_series:
            unique_series.setdefault(normalize_series_name(row['name']), []).append(row['id'])
        for series, ids in unique_series.iteritems():
            session.execute(update(ep_table, ep_table.c.series_id.in_(ids), {'series_id': ids[0]}))
            if len(ids) > 1:
                session.execute(delete(series_table, series_table.c.id.in_(ids[1:])))
            session.execute(update(series_table, series_table.c.id == ids[0], {'name_lower': series}))
        ver = 9
    if ver == 9:
        table_add_column('series', 'begin_episode_id', Integer, session)
        ver = 10
    if ver == 10:
        # Due to bad db cleanups there may be invalid entries in series_tasks table
        series_tasks = table_schema('series_tasks', session)
        series_table = table_schema('series', session)
        log.verbose('Repairing series_tasks table data')
        session.execute(delete(series_tasks, ~series_tasks.c.series_id.in_(select([series_table.c.id]))))
        ver = 11

    return ver


@event('manager.db_cleanup')
def db_cleanup(session):
    # Clean up old undownloaded releases
    result = session.query(Release).\
        filter(Release.downloaded == False).\
        filter(Release.first_seen < datetime.now() - timedelta(days=120)).delete(False)
    if result:
        log.verbose('Removed %d undownloaded episode releases.', result)
    # Clean up episodes without releases
    result = session.query(Episode).filter(~Episode.releases.any()).filter(~Episode.begins_series.any()).delete(False)
    if result:
        log.verbose('Removed %d episodes without releases.', result)
    # Clean up series without episodes that aren't in any tasks
    result = session.query(Series).filter(~Series.episodes.any()).filter(~Series.in_tasks.any()).delete(False)
    if result:
        log.verbose('Removed %d series without episodes.', result)


@event('manager.lock_acquired')
def repair(manager):
    # Perform database repairing and upgrading at startup.
    if not manager.persist.get('series_repaired', False):
        session = Session()
        try:
            # For some reason at least I have some releases in database which don't belong to any episode.
            for release in session.query(Release).filter(Release.episode == None).all():
                log.info('Purging orphan release %s from database', release.title)
                session.delete(release)
            session.commit()
        finally:
            session.close()
        manager.persist['series_repaired'] = True

    # Run clean_series the first time we get a database lock, since we won't have had one the first time the config
    # got loaded.
    clean_series(manager)


@event('manager.config_updated')
def clean_series(manager):
    # Unmark series from tasks which have been deleted.
    if not manager.has_lock:
        return
    session = Session()
    try:
        deleted = (session.query(SeriesTask).filter(not_(SeriesTask.name.in_(manager.tasks))).
                   delete(synchronize_session=False))
        if deleted:
            session.commit()
    finally:
        session.close()


TRANSLATE_MAP = {ord(u'&'): u' and '}
for char in u'\'\\':
    TRANSLATE_MAP[ord(char)] = u''
for char in u'_./-,[]():':
    TRANSLATE_MAP[ord(char)] = u' '


def normalize_series_name(name):
    """Returns a normalized version of the series name."""
    name = name.lower()
    name = name.replace('&amp;', ' and ')
    name = name.translate(TRANSLATE_MAP)  # Replaced some symbols with spaces
    name = u' '.join(name.split())
    return name


class NormalizedComparator(Comparator):
    def operate(self, op, other):
        return op(self.__clause_element__(), normalize_series_name(other))


class Series(Base):

    """ Name is handled case insensitively transparently
    """

    __tablename__ = 'series'

    id = Column(Integer, primary_key=True)
    _name = Column('name', Unicode)
    _name_normalized = Column('name_lower', Unicode, index=True, unique=True)
    identified_by = Column(String)
    begin_episode_id = Column(Integer, ForeignKey('series_episodes.id', name='begin_episode_id', use_alter=True))
    begin = relation('Episode', uselist=False, primaryjoin="Series.begin_episode_id == Episode.id",
                     foreign_keys=[begin_episode_id], post_update=True, backref='begins_series')
    episodes = relation('Episode', backref='series', cascade='all, delete, delete-orphan',
                        primaryjoin='Series.id == Episode.series_id')
    in_tasks = relation('SeriesTask', backref=backref('series', uselist=False), cascade='all, delete, delete-orphan')

    # Make a special property that does indexed case insensitive lookups on name, but stores/returns specified case
    def name_getter(self):
        return self._name

    def name_setter(self, value):
        self._name = value
        self._name_normalized = normalize_series_name(value)

    def name_comparator(self):
        return NormalizedComparator(self._name_normalized)

    name = hybrid_property(name_getter, name_setter)
    name.comparator(name_comparator)

    def __unicode__(self):
        return '<Series(id=%s,name=%s)>' % (self.id, self.name)

    def __repr__(self):
        return unicode(self).encode('ascii', 'replace')


class Episode(Base):

    __tablename__ = 'series_episodes'

    id = Column(Integer, primary_key=True)
    identifier = Column(String)

    season = Column(Integer)
    number = Column(Integer)

    identified_by = Column(String)
    series_id = Column(Integer, ForeignKey('series.id'), nullable=False)
    releases = relation('Release', backref='episode', cascade='all, delete, delete-orphan')

    @hybrid_property
    def first_seen(self):
        if not self.releases:
            return None
        return min(release.first_seen for release in self.releases)

    @first_seen.expression
    def first_seen(cls):
        return select([func.min(Release.first_seen)]).where(Release.episode_id == cls.id).\
            correlate(Episode.__table__).label('first_seen')

    @property
    def age(self):
        """
        :return: Pretty string representing age of episode. eg "23d 12h" or "No releases seen"
        """
        if not self.first_seen:
            return 'No releases seen'
        diff = datetime.now() - self.first_seen
        age_days = diff.days
        age_hours = diff.seconds // 60 // 60
        age = ''
        if age_days:
            age += '%sd ' % age_days
        age += '%sh' % age_hours
        return age

    @property
    def is_premiere(self):
        if self.season == 1 and self.number in (0, 1):
            return 'Series Premiere'
        elif self.number in (0, 1):
            return 'Season Premiere'
        return False

    @property
    def downloaded_releases(self):
        return [release for release in self.releases if release.downloaded]

    def __unicode__(self):
        return '<Episode(id=%s,identifier=%s,season=%s,number=%s)>' % \
               (self.id, self.identifier, self.season, self.number)

    def __repr__(self):
        return unicode(self).encode('ascii', 'replace')

    def __eq__(self, other):
        if not isinstance(other, Episode):
            return NotImplemented
        if self.identified_by != other.identified_by:
            return NotImplemented
        return self.identifier == other.identifier

    def __lt__(self, other):
        if not isinstance(other, Episode):
            return NotImplemented
        if self.identified_by != other.identified_by:
            return NotImplemented
        if self.identified_by in ['ep', 'sequence']:
            return self.season < other.season or (self.season == other.season and self.number < other.number)
        if self.identified_by == 'date':
            return self.identifier < other.identifier
        # Can't compare id type identifiers
        return NotImplemented


Index('episode_series_identifier', Episode.series_id, Episode.identifier)


class Release(Base):

    __tablename__ = 'episode_releases'

    id = Column(Integer, primary_key=True)
    episode_id = Column(Integer, ForeignKey('series_episodes.id'), nullable=False, index=True)
    _quality = Column('quality', String)
    quality = quality_property('_quality')
    downloaded = Column(Boolean, default=False)
    proper_count = Column(Integer, default=0)
    title = Column(Unicode)
    first_seen = Column(DateTime)

    def __init__(self):
        self.first_seen = datetime.now()

    @property
    def proper(self):
        # TODO: TEMP
        import warnings
        warnings.warn("accessing deprecated release.proper, use release.proper_count instead")
        return self.proper_count > 0

    def __unicode__(self):
        return '<Release(id=%s,quality=%s,downloaded=%s,proper_count=%s,title=%s)>' % \
            (self.id, self.quality, self.downloaded, self.proper_count, self.title)

    def __repr__(self):
        return unicode(self).encode('ascii', 'replace')


class SeriesTask(Base):
    __tablename__ = 'series_tasks'

    id = Column(Integer, primary_key=True)
    series_id = Column(Integer, ForeignKey('series.id'), nullable=False)
    name = Column(Unicode, index=True)

    def __init__(self, name):
        self.name = name


def get_latest_episode(series):
    """Return latest known identifier in dict (season, episode, name) for series name"""
    session = Session.object_session(series)
    episode = session.query(Episode).join(Episode.series).\
        filter(Series.id == series.id).\
        filter(Episode.season != None).\
        order_by(desc(Episode.season)).\
        order_by(desc(Episode.number)).first()
    if not episode:
        # log.trace('get_latest_info: no info available for %s', name)
        return False
    # log.trace('get_latest_info, series: %s season: %s episode: %s' % \
    #    (name, episode.season, episode.number))
    return episode


def auto_identified_by(series):
    """
    Determine if series `name` should be considered identified by episode or id format

    Returns 'ep', 'sequence', 'date' or 'id' if enough history is present to identify the series' id type.
    Returns 'auto' if there is not enough history to determine the format yet
    """

    session = Session.object_session(series)
    type_totals = dict(session.query(Episode.identified_by, func.count(Episode.identified_by)).join(Episode.series).
                       filter(Series.id == series.id).group_by(Episode.identified_by).all())
    # Remove None and specials from the dict,
    # we are only considering episodes that we know the type of (parsed with new parser)
    type_totals.pop(None, None)
    type_totals.pop('special', None)
    if not type_totals:
        return 'auto'
    log.debug('%s episode type totals: %r', series.name, type_totals)
    # Find total number of parsed episodes
    total = sum(type_totals.itervalues())
    # See which type has the most
    best = max(type_totals, key=lambda x: type_totals[x])

    # Ep mode locks in faster than the rest. At 2 seen episodes.
    if type_totals.get('ep', 0) >= 2 and type_totals['ep'] > total / 3:
        log.info('identified_by has locked in to type `ep` for %s', series.name)
        return 'ep'
    # If we have over 3 episodes all of the same type, lock in
    if len(type_totals) == 1 and total >= 3:
        return best
    # Otherwise wait until 5 episodes to lock in
    if total >= 5:
        log.info('identified_by has locked in to type `%s` for %s', best, series.name)
        return best
    log.verbose('identified by is currently on `auto` for %s. '
                'Multiple id types may be accepted until it locks in on the appropriate type.', series.name)
    return 'auto'


def get_latest_release(series, downloaded=True, season=None):
    """
    :param Series series: SQLAlchemy session
    :param Downloaded: find only downloaded releases
    :param Season: season to find newest release for
    :return: Instance of Episode or None if not found.
    """
    session = Session.object_session(series)
    releases = session.query(Episode).join(Episode.releases, Episode.series).filter(Series.id == series.id)

    if downloaded:
        releases = releases.filter(Release.downloaded == True)

    if season is not None:
        releases = releases.filter(Episode.season == season)

    if series.identified_by and series.identified_by != 'auto':
        releases = releases.filter(Episode.identified_by == series.identified_by)

    if series.identified_by in ['ep', 'sequence']:
        latest_release = releases.order_by(desc(Episode.season), desc(Episode.number)).first()
    elif series.identified_by == 'date':
        latest_release = releases.order_by(desc(Episode.identifier)).first()
    else:
        latest_release = releases.order_by(desc(Episode.first_seen)).first()

    if not latest_release:
        log.debug('get_latest_release returning None, no downloaded episodes found for: %s', series.name)
        return

    return latest_release


def new_eps_after(since_ep):
    """
    :param since_ep: Episode instance
    :return: Number of episodes since then
    """
    session = Session.object_session(since_ep)
    series = since_ep.series
    series_eps = session.query(Episode).join(Episode.series).\
        filter(Series.id == series.id)
    if series.identified_by == 'ep':
        if since_ep.season is None or since_ep.number is None:
            log.debug('new_eps_after for %s falling back to timestamp because latest dl in non-ep format' %
                      series.name)
            return series_eps.filter(Episode.first_seen > since_ep.first_seen).count()
        return series_eps.filter((Episode.identified_by == 'ep') &
                                 (((Episode.season == since_ep.season) & (Episode.number > since_ep.number)) |
                                  (Episode.season > since_ep.season))).count()
    elif series.identified_by == 'seq':
        return series_eps.filter(Episode.number > since_ep.number).count()
    elif series.identified_by == 'id':
        return series_eps.filter(Episode.first_seen > since_ep.first_seen).count()
    else:
        log.debug('unsupported identified_by %s', series.identified_by)
        return 0


def store_parser(session, parser, series=None):
    """
    Push series information into database. Returns added/existing release.

    :param session: Database session to use
    :param parser: parser for release that should be added to database
    :param series: Series in database to add release to. Will be looked up if not provided.
    :return: List of Releases
    """
    if not series:
        # if series does not exist in database, add new
        series = session.query(Series).\
            filter(Series.name == parser.name).\
            filter(Series.id != None).first()
        if not series:
            log.debug('adding series %s into db', parser.name)
            series = Series()
            series.name = parser.name
            session.add(series)
            log.debug('-> added %s' % series)

    releases = []
    for ix, identifier in enumerate(parser.identifiers):
        # if episode does not exist in series, add new
        episode = session.query(Episode).filter(Episode.series_id == series.id).\
            filter(Episode.identifier == identifier).\
            filter(Episode.series_id != None).first()
        if not episode:
            log.debug('adding episode %s into series %s', identifier, parser.name)
            episode = Episode()
            episode.identifier = identifier
            episode.identified_by = parser.id_type
            # if episodic format
            if parser.id_type == 'ep':
                episode.season = parser.season
                episode.number = parser.episode + ix
            elif parser.id_type == 'sequence':
                episode.season = 0
                episode.number = parser.id + ix
            series.episodes.append(episode)  # pylint:disable=E1103
            log.debug('-> added %s' % episode)

        # if release does not exists in episode, add new
        #
        # NOTE:
        #
        # filter(Release.episode_id != None) fixes weird bug where release had/has been added
        # to database but doesn't have episode_id, this causes all kinds of havoc with the plugin.
        # perhaps a bug in sqlalchemy?
        release = session.query(Release).filter(Release.episode_id == episode.id).\
            filter(Release.title == parser.data).\
            filter(Release.quality == parser.quality).\
            filter(Release.proper_count == parser.proper_count).\
            filter(Release.episode_id != None).first()
        if not release:
            log.debug('adding release %s into episode', parser)
            release = Release()
            release.quality = parser.quality
            release.proper_count = parser.proper_count
            release.title = parser.data
            episode.releases.append(release)  # pylint:disable=E1103
            log.debug('-> added %s' % release)
        releases.append(release)
    return releases


def set_series_begin(series, ep_id):
    """
    Set beginning for series

    :param Series series: Series instance
    :param ep_id: Integer for sequence mode, SxxEyy for episodic and yyyy-mm-dd for date.
    :raises ValueError: If malformed ep_id or series in different mode
    """
    # If identified_by is not explicitly specified, auto-detect it based on begin identifier
    # TODO: use some method of series parser to do the identifier parsing
    session = Session.object_session(series)
    if isinstance(ep_id, int):
        identified_by = 'sequence'
    elif re.match(r'(?i)^S\d{1,4}E\d{1,2}$', ep_id):
        identified_by = 'ep'
        ep_id = ep_id.upper()
    elif re.match(r'\d{4}-\d{2}-\d{2}', ep_id):
        identified_by = 'date'
    else:
        # Check if a sequence identifier was passed as a string
        try:
            ep_id = int(ep_id)
            identified_by = 'sequence'
        except ValueError:
            raise ValueError('`%s` is not a valid episode identifier' % ep_id)
    if series.identified_by not in ['auto', '', None]:
        if identified_by != series.identified_by:
            raise ValueError('`begin` value `%s` does not match identifier type for identified_by `%s`' %
                              (ep_id, series.identified_by))
    series.identified_by = identified_by
    episode = (session.query(Episode).filter(Episode.series_id == series.id).
               filter(Episode.identified_by == series.identified_by).
               filter(Episode.identifier == str(ep_id)).first())
    if not episode:
        # TODO: Don't duplicate code from self.store method
        episode = Episode()
        episode.identifier = ep_id
        episode.identified_by = identified_by
        if identified_by == 'ep':
            match = re.match(r'S(\d+)E(\d+)', ep_id)
            episode.season = int(match.group(1))
            episode.number = int(match.group(2))
        elif identified_by == 'sequence':
            episode.season = 0
            episode.number = ep_id
        series.episodes.append(episode)
        # Need to flush to get an id on new Episode before assigning it as series begin
        session.flush()
    series.begin = episode


def forget_series(name):
    """Remove a whole series `name` from database."""
    session = Session()
    try:
        series = session.query(Series).filter(Series.name == name).all()
        if series:
            for s in series:
                session.delete(s)
            session.commit()
            log.debug('Removed series %s from database.', name)
        else:
            raise ValueError('Unknown series %s' % name)
    finally:
        session.close()


def forget_series_episode(name, identifier):
    """Remove all episodes by `identifier` from series `name` from database."""
    session = Session()
    try:
        series = session.query(Series).filter(Series.name == name).first()
        if series:
            episode = session.query(Episode).filter(Episode.identifier == identifier).\
                filter(Episode.series_id == series.id).first()
            if episode:
                series.identified_by = ''  # reset identified_by flag so that it will be recalculated
                session.delete(episode)
                session.commit()
                log.debug('Episode %s from series %s removed from database.', identifier, name)
            else:
                raise ValueError('Unknown identifier %s for series %s' % (identifier, name.capitalize()))
        else:
            raise ValueError('Unknown series %s' % name)
    finally:
        session.close()


def populate_entry_fields(entry, parser):
    entry['series_parser'] = copy(parser)
    # add series, season and episode to entry
    entry['series_name'] = parser.name
    if 'quality' in entry and entry['quality'] != parser.quality:
        log.verbose('Found different quality for %s. Was %s, overriding with %s.' %
                    (entry['title'], entry['quality'], parser.quality))
    entry['quality'] = parser.quality
    entry['proper'] = parser.proper
    entry['proper_count'] = parser.proper_count
    if parser.id_type == 'ep':
        entry['series_season'] = parser.season
        entry['series_episode'] = parser.episode
    elif parser.id_type == 'date':
        entry['series_date'] = parser.id
        entry['series_season'] = parser.id.year
    else:
        entry['series_season'] = time.gmtime().tm_year
    entry['series_episodes'] = parser.episodes
    entry['series_id'] = parser.pack_identifier
    entry['series_id_type'] = parser.id_type


class FilterSeriesBase(object):
    """
    Class that contains helper methods for both filter.series as well as plugins that configure it,
    such as all_series, series_premiere and configure_series.
    """

    @property
    def settings_schema(self):
        return {
            'title': 'series options',
            'type': 'object',
            'properties': {
                'path': {'type': 'string'},
                'set': {'type': 'object'},
                'alternate_name': one_or_more({'type': 'string'}),
                # Custom regexp options
                'name_regexp': one_or_more({'type': 'string', 'format': 'regex'}),
                'ep_regexp': one_or_more({'type': 'string', 'format': 'regex'}),
                'date_regexp': one_or_more({'type': 'string', 'format': 'regex'}),
                'sequence_regexp': one_or_more({'type': 'string', 'format': 'regex'}),
                'id_regexp': one_or_more({'type': 'string', 'format': 'regex'}),
                # Date parsing options
                'date_yearfirst': {'type': 'boolean'},
                'date_dayfirst': {'type': 'boolean'},
                # Quality options
                'quality': {'type': 'string', 'format': 'quality_requirements'},
                'qualities': {'type': 'array', 'items': {'type': 'string', 'format': 'quality_requirements'}},
                'timeframe': {'type': 'string', 'format': 'interval'},
                'upgrade': {'type': 'boolean'},
                'target': {'type': 'string', 'format': 'quality_requirements'},
                # Specials
                'specials': {'type': 'boolean'},
                # Propers (can be boolean, or an interval string)
                'propers': {'type': ['boolean', 'string'], 'format': 'interval'},
                # Identified by
                'identified_by': {
                    'type': 'string', 'enum': ['ep', 'date', 'sequence', 'id', 'auto']
                },
                # Strict naming
                'exact': {'type': 'boolean'},
                # Begin takes an ep, sequence or date identifier
                'begin': {
                    'oneOf': [
                        {'name': 'ep identifier', 'type': 'string', 'pattern': r'(?i)^S\d{2}E\d{2,3}$',
                         'error_pattern': 'episode identifiers should be in the form `SxxEyy`'},
                        {'name': 'date identifier', 'type': 'string', 'pattern': r'^\d{4}-\d{2}-\d{2}$',
                         'error_pattern': 'date identifiers must be in the form `YYYY-MM-DD`'},
                        {'name': 'sequence identifier', 'type': 'integer', 'minimum': 0}
                    ]
                },
                'from_group': one_or_more({'type': 'string'}),
                'parse_only': {'type': 'boolean'},
                'special_ids': one_or_more({'type': 'string'}),
                'prefer_specials': {'type': 'boolean'},
                'assume_special': {'type': 'boolean'},
                'tracking': {'type': ['boolean', 'string'], 'enum': [True, False, 'backfill']}
            },
            'additionalProperties': False
        }

    def make_grouped_config(self, config):
        """Turns a simple series list into grouped format with a empty settings dict"""
        if not isinstance(config, dict):
            # convert simplest configuration internally grouped format
            config = {'simple': config, 'settings': {}}
        else:
            # already in grouped format, just make sure there's settings
            config.setdefault('settings', {})
        return config

    def apply_group_options(self, config):
        """Applies group settings to each item in series group and removes settings dict."""

        # Make sure config is in grouped format first
        config = self.make_grouped_config(config)
        for group_name in config:
            if group_name == 'settings':
                continue
            group_series = []
            if isinstance(group_name, basestring):
                # if group name is known quality, convenience create settings with that quality
                try:
                    qualities.Requirements(group_name)
                    config['settings'].setdefault(group_name, {}).setdefault('target', group_name)
                except ValueError:
                    # If group name is not a valid quality requirement string, do nothing.
                    pass
            for series in config[group_name]:
                # convert into dict-form if necessary
                series_settings = {}
                group_settings = config['settings'].get(group_name, {})
                if isinstance(series, dict):
                    series, series_settings = series.items()[0]
                    if series_settings is None:
                        raise Exception('Series %s has unexpected \':\'' % series)
                # Make sure this isn't a series with no name
                if not series:
                    log.warning('Series config contains a series with no name!')
                    continue
                # make sure series name is a string to accommodate for "24"
                if not isinstance(series, basestring):
                    series = unicode(series)
                # if series have given path instead of dict, convert it into a dict
                if isinstance(series_settings, basestring):
                    series_settings = {'path': series_settings}
                # merge group settings into this series settings
                merge_dict_from_to(group_settings, series_settings)
                # Convert to dict if watched is in SXXEXX format
                if isinstance(series_settings.get('watched'), basestring):
                    season, episode = series_settings['watched'].upper().split('E')
                    season = season.lstrip('S')
                    series_settings['watched'] = {'season': int(season), 'episode': int(episode)}
                # Convert enough to target for backwards compatibility
                if 'enough' in series_settings:
                    log.warning('Series setting `enough` has been renamed to `target` please update your config.')
                    series_settings.setdefault('target', series_settings['enough'])
                # Add quality: 720p if timeframe is specified with no target
                if 'timeframe' in series_settings and 'qualities' not in series_settings:
                    series_settings.setdefault('target', '720p hdtv+')

                group_series.append({series: series_settings})
            config[group_name] = group_series
        del config['settings']
        return config

    def prepare_config(self, config):
        """Generate a list of unique series from configuration.
        This way we don't need to handle two different configuration formats in the logic.
        Applies group settings with advanced form."""

        config = self.apply_group_options(config)
        return self.combine_series_lists(*config.values())

    def combine_series_lists(self, *series_lists, **kwargs):
        """Combines the series from multiple lists, making sure there are no doubles.

        If keyword argument log_once is set to True, an error message will be printed if a series
        is listed more than once, otherwise log_once will be used."""
        unique_series = {}
        for series_list in series_lists:
            for series in series_list:
                series, series_settings = series.items()[0]
                if series not in unique_series:
                    unique_series[series] = series_settings
                else:
                    if kwargs.get('log_once'):
                        log_once('Series %s is already configured in series plugin' % series, log)
                    else:
                        log.warning('Series %s is configured multiple times in series plugin.', series)
                    # Combine the config dicts for both instances of the show
                    unique_series[series].update(series_settings)
        # Turn our all_series dict back into a list
        # sort by reverse alpha, so that in the event of 2 series with common prefix, more specific is parsed first
        return [{series: unique_series[series]} for series in sorted(unique_series, reverse=True)]

    def merge_config(self, task, config):
        """Merges another series config dict in with the current one."""

        # Make sure we start with both configs as a list of complex series
        native_series = self.prepare_config(task.config.get('series', {}))
        merging_series = self.prepare_config(config)
        task.config['series'] = self.combine_series_lists(merging_series, native_series, log_once=True)
        return task.config['series']


class FilterSeries(FilterSeriesBase):
    """
    Intelligent filter for tv-series.

    http://flexget.com/wiki/Plugins/series
    """

    @property
    def schema(self):
        return {
            'type': ['array', 'object'],
            # simple format:
            #   - series
            #   - another series
            'items': {
                'type': ['string', 'number', 'object'],
                'additionalProperties': self.settings_schema
            },
            # advanced format:
            #   settings:
            #     group: {...}
            #   group:
            #     {...}
            'properties': {
                'settings': {
                    'type': 'object',
                    'additionalProperties': self.settings_schema
                }
            },
            'additionalProperties': {
                'type': 'array',
                'items': {
                    'type': ['string', 'number', 'object'],
                    'additionalProperties': self.settings_schema
                }
            }
        }

    def __init__(self):
        try:
            self.backlog = plugin.get_plugin_by_name('backlog')
        except plugin.DependencyError:
            log.warning('Unable utilize backlog plugin, episodes may slip trough timeframe')

    def auto_exact(self, config):
        """Automatically enable exact naming option for series that look like a problem"""

        # generate list of all series in one dict
        all_series = {}
        for series_item in config:
            series_name, series_config = series_item.items()[0]
            all_series[series_name] = series_config

        # scan for problematic names, enable exact mode for them
        for series_name, series_config in all_series.iteritems():
            for name in all_series.keys():
                if (name.lower().startswith(series_name.lower())) and \
                   (name.lower() != series_name.lower()):
                    if not 'exact' in series_config:
                        log.verbose('Auto enabling exact matching for series %s (reason %s)', series_name, name)
                        series_config['exact'] = True

    # Run after metainfo_quality and before metainfo_series
    @plugin.priority(125)
    def on_task_metainfo(self, task, config):
        config = self.prepare_config(config)
        self.auto_exact(config)
        for series_item in config:
            series_name, series_config = series_item.items()[0]
            log.trace('series_name: %s series_config: %s', series_name, series_config)
            start_time = time.clock()
            self.parse_series(task.session, task.entries, series_name, series_config)
            took = time.clock() - start_time
            log.trace('parsing %s took %s', series_name, took)

    def on_task_filter(self, task, config):
        """Filter series"""
        # Parsing was done in metainfo phase, create the dicts to pass to process_series from the task entries
        # key: series episode identifier ie. S01E02
        # value: seriesparser

        config = self.prepare_config(config)
        found_series = {}
        for entry in task.entries:
            if entry.get('series_name') and entry.get('series_id') is not None and entry.get('series_parser'):
                found_series.setdefault(entry['series_name'], []).append(entry)

        for series_item in config:
            series_name, series_config = series_item.items()[0]
            if series_config.get('parse_only'):
                log.debug('Skipping filtering of series %s because of parse_only', series_name)
                continue
            # Make sure number shows (e.g. 24) are turned into strings
            series_name = unicode(series_name)
            db_series = task.session.query(Series).filter(Series.name == series_name).first()
            if not db_series:
                log.debug('adding series %s into db', series_name)
                db_series = Series()
                db_series.name = series_name
                db_series.identified_by = series_config.get('identified_by', 'auto')
                task.session.add(db_series)
                log.debug('-> added %s' % db_series)
            if not series_name in found_series:
                continue
            series_entries = {}
            for entry in found_series[series_name]:
                # store found episodes into database and save reference for later use
                releases = store_parser(task.session, entry['series_parser'], series=db_series)
                entry['series_releases'] = releases
                series_entries.setdefault(releases[0].episode, []).append(entry)

                # TODO: Unfortunately we are setting these again, even though they were set in metanifo. This is for the
                # benefit of all_series and series_premiere. Figure a better way.
                # set custom download path
                if 'path' in series_config:
                    log.debug('setting %s custom path to %s', entry['title'], series_config.get('path'))
                    # Just add this to the 'set' dictionary, so that string replacement is done cleanly
                    series_config.setdefault('set', {}).update(path=series_config['path'])

                # accept info from set: and place into the entry
                if 'set' in series_config:
                    set = plugin.get_plugin_by_name('set')
                    set.instance.modify(entry, series_config.get('set'))

            # If we didn't find any episodes for this series, continue
            if not series_entries:
                log.trace('No entries found for %s this run.', series_name)
                continue

            # configuration always overrides everything
            if series_config.get('identified_by', 'auto') != 'auto':
                db_series.identified_by = series_config['identified_by']
            # if series doesn't have identified_by flag already set, calculate one now that new eps are added to db
            if not db_series.identified_by or db_series.identified_by == 'auto':
                db_series.identified_by = auto_identified_by(db_series)
                log.debug('identified_by set to \'%s\' based on series history', db_series.identified_by)

            log.trace('series_name: %s series_config: %s', series_name, series_config)

            import time
            start_time = time.clock()

            self.process_series(task, series_entries, series_config)

            took = time.clock() - start_time
            log.trace('processing %s took %s', series_name, took)

    def parse_series(self, session, entries, series_name, config):
        """
        Search for `series_name` and populate all `series_*` fields in entries when successfully parsed

        :param session: SQLAlchemy session
        :param entries: List of entries to process
        :param series_name: Series name which is being processed
        :param config: Series config being processed
        """

        def get_as_array(config, key):
            """Return configuration key as array, even if given as a single string"""
            v = config.get(key, [])
            if isinstance(v, basestring):
                return [v]
            return v

        # set parser flags flags based on config / database
        identified_by = config.get('identified_by', 'auto')
        if identified_by == 'auto':
            series = session.query(Series).filter(Series.name == series_name).first()
            if series:
                # set flag from database
                identified_by = series.identified_by or 'auto'

        params = dict(name=series_name,
                      identified_by=identified_by,
                      alternate_names=get_as_array(config, 'alternate_name'),
                      name_regexps=get_as_array(config, 'name_regexp'),
                      strict_name=config.get('exact', False),
                      allow_groups=get_as_array(config, 'from_group'),
                      date_yearfirst=config.get('date_yearfirst'),
                      date_dayfirst=config.get('date_dayfirst'),
                      special_ids=get_as_array(config, 'special_ids'),
                      prefer_specials=config.get('prefer_specials'),
                      assume_special=config.get('assume_special'))
        for id_type in ID_TYPES:
            params[id_type + '_regexps'] = get_as_array(config, id_type + '_regexp')

        parser = SeriesParser(**params)

        for entry in entries:
            # skip processed entries
            if (entry.get('series_parser') and entry['series_parser'].valid and
                    entry['series_parser'].name.lower() != series_name.lower()):
                continue
            # scan from fields
            for field in ('title', 'description'):
                data = entry.get(field)
                # skip invalid fields
                if not isinstance(data, basestring) or not data:
                    continue
                # in case quality will not be found from title, set it from entry['quality'] if available
                quality = None
                if entry.get('quality'):
                    log.trace('Setting quality %s from entry field to parser', entry['quality'])
                    quality = entry['quality']
                try:
                    parser.parse(data, field=field, quality=quality)
                except ParseWarning as pw:
                    log_once(pw.value, logger=log)

                if parser.valid:
                    break
            else:
                continue  # next field

            log.debug('%s detected as %s, field: %s', entry['title'], parser, parser.field)
            populate_entry_fields(entry, parser)

            # set custom download path
            if 'path' in config:
                log.debug('setting %s custom path to %s', entry['title'], config.get('path'))
                # Just add this to the 'set' dictionary, so that string replacement is done cleanly
                config.setdefault('set', {}).update(path=config['path'])

            # accept info from set: and place into the entry
            if 'set' in config:
                set = plugin.get_plugin_by_name('set')
                set.instance.modify(entry, config.get('set'))

    def process_series(self, task, series_entries, config):
        """
        Accept or Reject episode from available releases, or postpone choosing.

        :param task: Current Task
        :param series_entries: dict mapping Episodes to entries for that episode
        :param config: Series configuration
        """

        for ep, entries in series_entries.iteritems():
            if not entries:
                continue

            reason = None

            # sort episodes in order of quality
            entries.sort(key=lambda e: e['series_parser'], reverse=True)

            log.debug('start with episodes: %s', [e['title'] for e in entries])

            # reject episodes that have been marked as watched in config file
            if ep.series.begin:
                if ep < ep.series.begin:
                    for entry in entries:
                        entry.reject('Episode `%s` is before begin value of `%s`' %
                                     (ep.identifier, ep.series.begin.identifier))
                    continue

            # skip special episodes if special handling has been turned off
            if not config.get('specials', True) and ep.identified_by == 'special':
                log.debug('Skipping special episode as support is turned off.')
                continue

            log.debug('current episodes: %s', [e['title'] for e in entries])

            # quality filtering
            if 'quality' in config:
                entries = self.process_quality(config, entries)
                if not entries:
                    continue
                reason = 'matches quality'

            # Many of the following functions need to know this info. Only look it up once.
            downloaded = ep.downloaded_releases
            downloaded_qualities = [rls.quality for rls in downloaded]

            # proper handling
            log.debug('-' * 20 + ' process_propers -->')
            entries = self.process_propers(config, ep, entries)
            if not entries:
                continue

            # Remove any eps we already have from the list
            for entry in reversed(entries):  # Iterate in reverse so we can safely remove from the list while iterating
                if entry['series_parser'].quality in downloaded_qualities:
                    entry.reject('quality already downloaded')
                    entries.remove(entry)
            if not entries:
                continue

            # Figure out if we need an additional quality for this ep
            if downloaded:
                if config.get('upgrade'):
                    # Remove all the qualities lower than what we have
                    for entry in reversed(entries):
                        if entry['series_parser'].quality < max(downloaded_qualities):
                            entry.reject('worse quality than already downloaded.')
                            entries.remove(entry)
                if not entries:
                    continue

                if 'target' in config and config.get('upgrade'):
                    # If we haven't grabbed the target yet, allow upgrade to it
                    self.process_timeframe_target(config, entries, downloaded)
                    continue
                if 'qualities' in config:
                    # Grab any additional wanted qualities
                    log.debug('-' * 20 + ' process_qualities -->')
                    self.process_qualities(config, entries, downloaded)
                    continue
                elif config.get('upgrade'):
                    entries[0].accept('is an upgrade to existing quality')
                    continue

                # Reject eps because we have them
                for entry in entries:
                    entry.reject('episode has already been downloaded')
                continue

            best = entries[0]
            log.debug('continuing w. episodes: %s', [e['title'] for e in entries])
            log.debug('best episode is: %s', best['title'])

            # episode tracking. used only with season and sequence based series
            if ep.identified_by in ['ep', 'sequence']:
                if task.options.disable_tracking or not config.get('tracking', True):
                    log.debug('episode tracking disabled')
                else:
                    log.debug('-' * 20 + ' episode tracking -->')
                    # Grace is number of distinct eps in the task for this series + 2
                    backfill = config.get('tracking') == 'backfill'
                    if self.process_episode_tracking(ep, entries, grace=len(series_entries)+2, backfill=backfill):
                        continue

            # quality
            if 'target' in config or 'qualities' in config:
                if 'target' in config:
                    if self.process_timeframe_target(config, entries, downloaded):
                        continue
                elif 'qualities' in config:
                    if self.process_qualities(config, entries, downloaded):
                        continue

                # We didn't make a quality target match, check timeframe to see
                # if we should get something anyway
                if 'timeframe' in config:
                    if self.process_timeframe(task, config, ep, entries):
                        continue
                    reason = 'Timeframe expired, choosing best available'
                else:
                    # If target or qualities is configured without timeframe, don't accept anything now
                    continue

            # Just pick the best ep if we get here
            reason = reason or 'choosing best available quality'
            best.accept(reason)

    def process_propers(self, config, episode, entries):
        """
        Accepts needed propers. Nukes episodes from which there exists proper.

        :returns: A list of episodes to continue processing.
        """

        pass_filter = []
        best_propers = []
        # Since eps is sorted by quality then proper_count we always see the highest proper for a quality first.
        (last_qual, best_proper) = (None, 0)
        for entry in entries:
            if entry['series_parser'].quality != last_qual:
                last_qual, best_proper = entry['series_parser'].quality, entry['series_parser'].proper_count
                best_propers.append(entry)
            if entry['series_parser'].proper_count < best_proper:
                # nuke qualities which there is a better proper available
                entry.reject('nuked')
            else:
                pass_filter.append(entry)

        # If propers support is turned off, or proper timeframe has expired just return the filtered eps list
        if isinstance(config.get('propers', True), bool):
            if not config.get('propers', True):
                return pass_filter
        else:
            # propers with timeframe
            log.debug('proper timeframe: %s', config['propers'])
            timeframe = parse_timedelta(config['propers'])

            first_seen = episode.first_seen
            expires = first_seen + timeframe
            log.debug('propers timeframe: %s', timeframe)
            log.debug('first_seen: %s', first_seen)
            log.debug('propers ignore after: %s', expires)

            if datetime.now() > expires:
                log.debug('propers timeframe expired')
                return pass_filter

        downloaded_qualities = dict((d.quality, d.proper_count) for d in episode.downloaded_releases)
        log.debug('propers - downloaded qualities: %s' % downloaded_qualities)

        # Accept propers we actually need, and remove them from the list of entries to continue processing
        for entry in best_propers:
            if (entry['series_parser'].quality in downloaded_qualities and
                    entry['series_parser'].proper_count > downloaded_qualities[entry['series_parser'].quality]):
                entry.accept('proper')
                pass_filter.remove(entry)

        return pass_filter

    def process_timeframe_target(self, config, entries, downloaded=None):
        """
        Accepts first episode matching the quality configured for the series.

        :return: True if accepted something
        """
        req = qualities.Requirements(config['target'])
        if downloaded:
            if any(req.allows(release.quality) for release in downloaded):
                log.debug('Target quality already achieved.')
                return True
        # scan for quality
        for entry in entries:
            if req.allows(entry['series_parser'].quality):
                log.debug('Series accepting. %s meets quality %s', entry['title'], req)
                entry.accept('target quality')
                return True

    def process_quality(self, config, entries):
        """
        Filters eps that do not fall between within our defined quality standards.

        :returns: A list of eps that are in the acceptable range
        """
        reqs = qualities.Requirements(config['quality'])
        log.debug('quality req: %s', reqs)
        result = []
        # see if any of the eps match accepted qualities
        for entry in entries:
            if reqs.allows(entry['series_parser'].quality):
                result.append(entry)
            else:
                log.verbose('Ignored `%s`. Does not meet quality requirement `%s`.', entry['title'], reqs)
        if not result:
            log.debug('no quality meets requirements')
        return result

    def process_episode_tracking(self, episode, entries, grace, backfill=False):
        """
        Rejects all episodes that are too old or new, return True when this happens.

        :param episode: Episode model
        :param list entries: List of entries for given episode.
        :param int grace: Number of episodes before or after latest download that are allowed.
        :param bool backfill: If this is True, previous episodes will be allowed,
            but forward advancement will still be restricted.
        """

        latest = get_latest_release(episode.series)
        if episode.series.begin and episode.series.begin > latest:
            latest = episode.series.begin
        log.debug('latest download: %s' % latest)
        log.debug('current: %s' % episode)

        if latest and latest.identified_by == episode.identified_by:
            # Allow any previous episodes this season, or previous episodes within grace if sequence mode
            if (not backfill and (episode.season < latest.season or
                    (episode.identified_by == 'sequence' and episode.number < (latest.number - grace)))):
                log.debug('too old! rejecting all occurrences')
                for entry in entries:
                    entry.reject('Too much in the past from latest downloaded episode %s' % latest.identifier)
                return True

            # Allow future episodes within grace, or first episode of next season
            if (episode.season > latest.season + 1 or (episode.season > latest.season and episode.number > 1) or
               (episode.season == latest.season and episode.number > (latest.number + grace))):
                log.debug('too new! rejecting all occurrences')
                for entry in entries:
                    entry.reject('Too much in the future from latest downloaded episode %s. '
                                 'See `--disable-tracking` if this should be downloaded.' % latest.identifier)
                return True

    def process_timeframe(self, task, config, episode, entries):
        """
        Runs the timeframe logic to determine if we should wait for a better quality.
        Saves current best to backlog if timeframe has not expired.

        :returns: True - if we should keep the quality (or qualities) restriction
                  False - if the quality restriction should be released, due to timeframe expiring
        """

        if 'timeframe' not in config:
            return True

        best = entries[0]

        # parse options
        log.debug('timeframe: %s', config['timeframe'])
        timeframe = parse_timedelta(config['timeframe'])

        releases = episode.releases
        if config.get('quality'):
            req = qualities.Requirements(config['quality'])
            first_seen = min(rls.first_seen for rls in releases if req.allows(rls.quality))
        else:
            first_seen = min(rls.first_seen for rls in releases)
        expires = first_seen + timeframe
        log.debug('timeframe: %s, first_seen: %s, expires: %s', timeframe, first_seen, expires)

        stop = task.options.stop_waiting.lower() == episode.series.name.lower()
        if expires <= datetime.now() or stop:
            # Expire timeframe, accept anything
            log.info('Timeframe expired, releasing quality restriction.')
            return False
        else:
            # verbose waiting, add to backlog
            diff = expires - datetime.now()

            hours, remainder = divmod(diff.seconds, 3600)
            hours += diff.days * 24
            minutes, seconds = divmod(remainder, 60)

            log.info('Timeframe waiting %s for %sh:%smin, currently best is %s' %
                     (episode.series.name, hours, minutes, best['title']))

            # add best entry to backlog (backlog is able to handle duplicate adds)
            if self.backlog:
                self.backlog.instance.add_backlog(task, best)
            return True

    def process_qualities(self, config, entries, downloaded):
        """
        Handles all modes that can accept more than one quality per episode. (qualities, upgrade)

        :returns: True - if at least one wanted quality has been downloaded or accepted.
                  False - if no wanted qualities have been accepted
        """

        # Get list of already downloaded qualities
        downloaded_qualities = [r.quality for r in downloaded]
        log.debug('downloaded_qualities: %s', downloaded_qualities)

        # If qualities key is configured, we only want qualities defined in it.
        wanted_qualities = set([qualities.Requirements(name) for name in config.get('qualities', [])])
        # Compute the requirements from our set that have not yet been fulfilled
        still_needed = [req for req in wanted_qualities if not any(req.allows(qual) for qual in downloaded_qualities)]
        log.debug('Wanted qualities: %s', wanted_qualities)

        def wanted(quality):
            """Returns True if we want this quality based on the config options."""
            wanted = not wanted_qualities or any(req.allows(quality) for req in wanted_qualities)
            if config.get('upgrade'):
                wanted = wanted and quality > max(downloaded_qualities or [qualities.Quality()])
            return wanted

        for entry in entries:
            quality = entry['series_parser'].quality
            log.debug('ep: %s quality: %s', entry['title'], quality)
            if not wanted(quality):
                log.debug('%s is unwanted quality', quality)
                continue
            if any(req.allows(quality) for req in still_needed):
                # Don't get worse qualities in upgrade mode
                if config.get('upgrade'):
                    if downloaded_qualities and quality < max(downloaded_qualities):
                        continue
                entry.accept('quality wanted')
                downloaded_qualities.append(quality)
                downloaded.append(entry)
                # Re-calculate what is still needed
                still_needed = [req for req in still_needed if not req.allows(quality)]
        return bool(downloaded_qualities)

    def on_task_learn(self, task, config):
        """Learn succeeded episodes"""
        log.debug('on_task_learn')
        for entry in task.accepted:
            if 'series_releases' in entry:
                for release in entry['series_releases']:
                    log.debug('marking %s as downloaded' % release)
                    release.downloaded = True
            else:
                log.debug('%s is not a series', entry['title'])


class SeriesDBManager(FilterSeriesBase):
    """Update in the database with series info from the config"""

    @plugin.priority(0)
    def on_task_start(self, task, config):
        if not task.config_modified:
            return
        # Clear all series from this task
        task.session.query(SeriesTask).filter(SeriesTask.name == task.name).delete()
        if not task.config.get('series'):
            return
        config = self.prepare_config(task.config['series'])
        for series_item in config:
            series_name, series_config = series_item.items()[0]
            # Make sure number shows (e.g. 24) are turned into strings
            series_name = unicode(series_name)
            db_series = task.session.query(Series).filter(Series.name == series_name).first()
            if db_series:
                # Update database with capitalization from config
                db_series.name = series_name
            else:
                log.debug('adding series %s into db', series_name)
                db_series = Series()
                db_series.name = series_name
                task.session.add(db_series)
                log.debug('-> added %s' % db_series)
            db_series.in_tasks.append(SeriesTask(task.name))
            if series_config.get('identified_by', 'auto') != 'auto':
                db_series.identified_by = series_config['identified_by']
            # Set the begin episode
            if series_config.get('begin'):
                try:
                    set_series_begin(db_series, series_config['begin'])
                except ValueError as e:
                    raise plugin.PluginError(e)


@event('plugin.register')
def register_plugin():
    plugin.register(FilterSeries, 'series', api_ver=2)
    # This is a builtin so that it can update the database for tasks that may have had series plugin removed
    plugin.register(SeriesDBManager, 'series_db', builtin=True, api_ver=2)


@event('options.register')
def register_parser_arguments():
    exec_parser = options.get_parser('execute')
    exec_parser.add_argument('--stop-waiting', action='store', dest='stop_waiting', default='',
                             metavar='NAME', help='stop timeframe for a given series')
    exec_parser.add_argument('--disable-tracking', action='store_true', default=False,
                             help='disable episode advancement for this run')
    # Backwards compatibility
    exec_parser.add_argument('--disable-advancement', action='store_true', dest='disable_tracking',
                             help=argparse.SUPPRESS)

########NEW FILE########
__FILENAME__ = series_premiere
from __future__ import unicode_literals, division, absolute_import

from flexget import plugin
from flexget.event import event
from flexget.plugins.filter.series import FilterSeriesBase, normalize_series_name, Series


class FilterSeriesPremiere(FilterSeriesBase):
    """
    Accept an entry that appears to be the first episode of any series.

    Can be configured with any of the options of series plugin
    Examples:

    series_premiere: yes

    series_premiere:
      path: ~/Media/TV/_NEW_/.
      quality: 720p
      timeframe: 12 hours

    NOTE: this plugin only looks in the entry title and expects the title
    format to start with the series name followed by the episode info. Use
    the manipulate plugin to modify the entry title to match this format, if
    necessary.

    TODO:
        - integrate thetvdb to allow refining by genres, etc.
    """

    @property
    def schema(self):
        settings = self.settings_schema
        settings['properties']['allow_seasonless'] = {'type': 'boolean'}
        settings['properties']['allow_teasers'] = {'type': 'boolean'}
        return {'anyOf': [{'type': 'boolean'}, settings]}

    # Run after series and metainfo series plugins
    @plugin.priority(115)
    def on_task_metainfo(self, task, config):
        if not config:
            # Don't run when we are disabled
            return
        if task.is_rerun:
            return
        # Generate the group settings for series plugin
        group_settings = {}
        allow_seasonless = False
        desired_eps = [0, 1]
        if isinstance(config, dict):
            allow_seasonless = config.pop('allow_seasonless', False)
            if not config.pop('allow_teasers', True):
                desired_eps = [1]
            group_settings = config
        group_settings['identified_by'] = 'ep'
        # Generate a list of unique series that have premieres
        metainfo_series = plugin.get_plugin_by_name('metainfo_series')
        guess_entry = metainfo_series.instance.guess_entry
        # Make a set of unique series according to series name normalization rules
        guessed_series = {}
        for entry in task.entries:
            if guess_entry(entry, allow_seasonless=allow_seasonless):
                if entry['series_season'] == 1 and entry['series_episode'] in desired_eps:
                    normalized_name = normalize_series_name(entry['series_name'])
                    db_series = task.session.query(Series).filter(Series.name == normalized_name).first()
                    if db_series and db_series.in_tasks:
                        continue
                    guessed_series.setdefault(normalized_name, entry['series_name'])
        # Reject any further episodes in those series
        for entry in task.entries:
            for series in guessed_series.itervalues():
                if entry.get('series_name') == series and not (
                        entry.get('series_season') == 1
                        and entry.get('series_episode') in desired_eps):
                    entry.reject('Non premiere episode in a premiere series')
        # Since we are running after task start phase, make sure not to merge into the config multiple times on reruns
        if not task.is_rerun:
            # Combine settings and series into series plugin config format
            allseries = {'settings': {'series_premiere': group_settings}, 'series_premiere': guessed_series.values()}
            # Merge the our config in to the main series config
            self.merge_config(task, allseries)


@event('plugin.register')
def register_plugin():
    plugin.register(FilterSeriesPremiere, 'series_premiere', api_ver=2)

########NEW FILE########
__FILENAME__ = thetvdb
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.log import log_once

log = logging.getLogger('thetvdb')


class FilterTvdb(object):
    """
        This plugin allows filtering based on thetvdb series rating,
        episode rating, status, genres, runtime, content-rating,
        languages, directors, writers, network, guest stars, episode
        rating, and actors

        Configuration:

        Note: All parameters are optional. Some are mutually exclusive.

        min_series_rating: <num>
        min_episode_rating: <num>
        min_episode_air_year: <num>
        max_episode_air_year: <num>
        min_episode_runtime: <num>
        max_episode_runtime: <num>

        # reject if genre contains any of these
        reject_content_rating:
            - TV-MA
        # accept only this content rating
        accept_content_rating:
            - TV-PG

        # accept only these networks
        accept_network:
            - NBC
        # reject if this network
        reject_network:
            - ABC

        # reject if genre contains any of these
        reject_genres:
            - drama
            - romance

        # reject if status contains any of these
        reject_status:
            - Ended

        # reject if language contain any of these
        reject_languages:
            - fr
        # accept only this language
        accept_languages:
            - en

        # Actors below take into account series actors, and guest stars
        # accept episode with any of these actors
        accept_actors:
            - Heidi Klum
            - Bruce Willis
        # reject episode if it has any of these actors
        reject_actors:
            - Cher
            - Tamala Jones

        # accept all episodes by these writers
        accept_writers:
            - Andrew W. Marlowe
        # reject episodes by these writers
        reject_writers:
            - Barry Schindel

        # accept all episodes by these directors
        accept_directors:
            - Rob Bowman
        # reject movies by these directors
        reject_directors:
            - John Terlesky
    """

    def validator(self):
        """Validate given configuration"""
        from flexget import validator
        thetvdb = validator.factory('dict')
        thetvdb.accept('number', key='min_series_rating')
        thetvdb.accept('number', key='min_episode_rating')
        thetvdb.accept('integer', key='min_episode_air_year')
        thetvdb.accept('integer', key='max_episode_air_year')
        thetvdb.accept('number', key='min_episode_runtime')
        thetvdb.accept('number', key='max_episode_runtime')
        thetvdb.accept('list', key='reject_content_rating').accept('text')
        thetvdb.accept('list', key='accept_content_rating').accept('text')
        thetvdb.accept('list', key='accept_network').accept('text')
        thetvdb.accept('list', key='reject_network').accept('text')
        thetvdb.accept('list', key='reject_genres').accept('text')
        thetvdb.accept('list', key='reject_status').accept('text')
        thetvdb.accept('list', key='reject_languages').accept('text')
        thetvdb.accept('list', key='accept_languages').accept('text')
        thetvdb.accept('list', key='accept_actors').accept('text')
        thetvdb.accept('list', key='reject_actors').accept('text')
        thetvdb.accept('list', key='accept_writers').accept('text')
        thetvdb.accept('list', key='reject_writers').accept('text')
        thetvdb.accept('list', key='accept_directors').accept('text')
        thetvdb.accept('list', key='reject_directors').accept('text')
        return thetvdb

    def is_in_set(self, config, configkey, entryitem,):
        '''
        this takes the config object, config key (to a list), and entry
        item so it can return True if the object matches,
        (be that a subset of the list, or if the entry item is contained
        within the config object list) or false if it does not.
        '''
        # will want to port this over to filter_imdb as well, for code
        # clarity in that module.
        if configkey in config:
            configlist = config[configkey]
            if isinstance(entryitem, list):
                for item in entryitem:
                    if item in configlist:
                        return True
            else:
                if entryitem in configlist:
                    return True
        return False

    @plugin.priority(126)
    def on_task_filter(self, task, config):

        lookup = plugin.get_plugin_by_name('thetvdb_lookup').instance.lookup

        for entry in task.entries:
            force_accept = False

            try:
                lookup(task, entry)
            except plugin.PluginError as e:
                log.error('Skipping %s because of an error: %s' % (entry['title'], e.value))
                continue

            # Check defined conditions
            reasons = []
            if 'min_series_rating' in config:
                if entry['tvdb_rating'] < config['min_series_rating']:
                    reasons.append('series_rating (%s < %s)' % (entry['tvdb_rating'], config['min_series_rating']))
            if 'min_episode_rating' in config:
                if entry['tvdb_ep_rating'] < config['min_episode_rating']:
                    reasons.append('tvdb_ep_rating (%s < %s)' % (entry['tvdb_ep_rating'], config['min_episode_rating']))
            if 'min_episode_air_year' in config:
                if entry['tvdb_ep_air_date'].strftime("%Y") < config['min_episode_air_year']:
                    reasons.append('tvdb_ep_air_date (%s < %s)' % (entry['tvdb_ep_air_date'].strftime("%Y"), config['min_episode_air_year']))
            if 'max_episode_air_year' in config:
                if entry['tvdb_ep_air_date'].strftime("%Y") > config['max_episode_air_year']:
                    reasons.append('tvdb_ep_air_date (%s < %s)' % (entry['tvdb_ep_air_date'].strftime("%Y"), config['max_episode_air_year']))

            if self.is_in_set(config, 'reject_content_rating', entry['tvdb_content_rating']):
                reasons.append('reject_content_rating')

            if not self.is_in_set(config, 'accept_content_rating', entry['tvdb_content_rating']):
                reasons.append('accept_content_rating')

            if self.is_in_set(config, 'reject_network', entry['tvdb_network']):
                reasons.append('reject_network')

            if not self.is_in_set(config, 'accept_network', entry['tvdb_network']):
                reasons.append('accept_network')

            if self.is_in_set(config, 'reject_genres', entry['tvdb_genres']):
                reasons.append('reject_genres')

            if self.is_in_set(config, 'reject_status', entry['tvdb_status']):
                reasons.append('reject_status')

            if self.is_in_set(config, 'reject_languages', entry['tvdb_language']):
                reasons.append('reject_languages')

            if not self.is_in_set(config, 'accept_languages', entry['tvdb_language']):
                reasons.append('accept_languages')

            # Accept if actors contains an accepted actor, but don't reject otherwise
            if self.is_in_set(config, 'accept_actors', entry['tvdb_actors'] + entry['tvdb_ep_guest_stars']):
                force_accept = True

            if self.is_in_set(config, 'reject_actors', entry['tvdb_actors'] + entry['tvdb_ep_guest_stars']):
                reasons.append('reject_genres')

            # Accept if writer is an accepted writer, but don't reject otherwise
            if self.is_in_set(config, 'accept_writers', entry['tvdb_ep_writer']):
                force_accept = True

            if self.is_in_set(config, 'reject_writers', entry['tvdb_ep_writer']):
                reasons.append('reject_writers')

            # Accept if director is an accepted director, but don't reject otherwise
            if self.is_in_set(config, 'accept_directors', entry['tvdb_ep_director']):
                force_accept = True

            if self.is_in_set(config, 'reject_directors', entry['tvdb_ep_director']):
                reasons.append('reject_directors')

            if reasons and not force_accept:
                msg = 'Skipping %s because of rule(s) %s' % \
                    (entry.get('series_name_thetvdb', None) or entry['title'], ', '.join(reasons))
                if task.options.debug:
                    log.debug(msg)
                else:
                    log_once(msg, log)
            else:
                log.debug('Accepting %s' % (entry))
                entry.accept()


@event('plugin.register')
def register_plugin():
    plugin.register(FilterTvdb, 'thetvdb', api_ver=2)

########NEW FILE########
__FILENAME__ = torrent_alive
from __future__ import unicode_literals, division, absolute_import
import logging
import threading
import socket
from urlparse import urlparse, SplitResult, urlsplit, urlunsplit
import struct
from random import randrange
from httplib import BadStatusLine
from urllib import quote
from urllib2 import URLError

from flexget import plugin
from flexget.event import event
from flexget.utils.tools import urlopener
from flexget.utils.bittorrent import bdecode

log = logging.getLogger('torrent_alive')


class TorrentAliveThread(threading.Thread):

    def __init__(self, tracker, info_hash):
        threading.Thread.__init__(self)
        self.tracker = tracker
        self.info_hash = info_hash
        self.tracker_seeds = 0

    def run(self):
        self.tracker_seeds = get_tracker_seeds(self.tracker, self.info_hash)
        log.debug('%s seeds found from %s' % (self.tracker_seeds, get_scrape_url(self.tracker, self.info_hash)))


def max_seeds_from_threads(threads):
    """
    Joins the threads and returns the maximum seeds found from any of them.

    :param threads: A list of started `TorrentAliveThread`
    :return: Maximum seeds found from any of the threads
    """
    seeds = 0
    for background in threads:
        log.debug('Coming up next: %s' % background.tracker)
        background.join()
        seeds = max(seeds, background.tracker_seeds)
        log.debug('Current hightest number of seeds found: %s' % seeds)
    return seeds


def get_scrape_url(tracker_url, info_hash):
    if 'announce' in tracker_url:
        v = urlsplit(tracker_url)
        sr = SplitResult(v.scheme, v.netloc, v.path.replace('announce', 'scrape'),
                         v.query, v.fragment)
        result = urlunsplit(sr)
    else:
        log.debug('`announce` not contained in tracker url, guessing scrape address.')
        result = tracker_url + '/scrape'

    result += '&' if '?' in result else '?'
    result += 'info_hash=%s' % quote(info_hash.decode('hex'))
    return result


def get_udp_seeds(url, info_hash):
    parsed_url = urlparse(url)
    port = None
    try:
        port = parsed_url.port
    except ValueError as ve:
        log.error('UDP Port Error, url was %s' % url)
        return 0

    log.debug('Checking for seeds from %s' % url)

    connection_id = 0x41727101980  # connection id is always this
    transaction_id = randrange(1, 65535)  # Random Transaction ID creation

    if port is None:
        log.error('UDP Port Error, port was None')
        return 0

    if port < 0 or port > 65535:
        log.error('UDP Port Error, port was %s' % port)
        return 0

    # Create the socket
    try:
        clisocket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        clisocket.settimeout(5.0)
        clisocket.connect((parsed_url.hostname, port))

        # build packet with connection_ID, using 0 value for action, giving our transaction ID for this packet
        packet = struct.pack(b">QLL", connection_id, 0, transaction_id)
        clisocket.send(packet)

        # set 16 bytes ["QLL" = 16 bytes] for the fmq for unpack
        res = clisocket.recv(16)
        # check recieved packet for response
        action, transaction_id, connection_id = struct.unpack(b">LLQ", res)

        #build packet hash out of decoded info_hash
        packet_hash = info_hash.decode('hex')

        # construct packet for scrape with decoded info_hash setting action byte to 2 for scape
        packet = struct.pack(b">QLL", connection_id, 2, transaction_id) + packet_hash

        clisocket.send(packet)
        # set recieve size of 8 + 12 bytes
        res = clisocket.recv(20)

    except IOError as e:
        log.warning('Socket Error: %s', e)
        return 0
    # Check for UDP error packet
    (action,) = struct.unpack(b">L", res[:4])
    if action == 3:
        log.error('There was a UDP Packet Error 3')
        return 0

    # first 8 bytes are followed by seeders, completed and leechers for requested torrent
    seeders, completed, leechers = struct.unpack(b">LLL", res[8:20])
    log.debug('get_udp_seeds is returning: %s', seeders)
    clisocket.close()
    return seeders


def get_http_seeds(url, info_hash):
    url = get_scrape_url(url, info_hash)
    if not url:
        log.debug('if not url is true returning 0')
        return 0
    log.debug('Checking for seeds from %s' % url)
    data = None
    try:
        data = bdecode(urlopener(url, log, retries=1, timeout=10).read()).get('files')
    except URLError as e:
        log.debug('Error scraping: %s' % e)
        return 0
    except SyntaxError as e:
        log.warning('Error decoding tracker response: %s' % e)
        return 0
    except BadStatusLine as e:
        log.warning('Error BadStatusLine: %s' % e)
        return 0
    except IOError as e:
        log.warning('Server error: %s' % e)
        return 0
    if not data:
        log.debug('No data received from tracker scrape.')
        return 0
    log.debug('get_http_seeds is returning: %s' % data.values()[0]['complete'])
    return data.values()[0]['complete']


def get_tracker_seeds(url, info_hash):
    if url.startswith('udp'):
        return get_udp_seeds(url, info_hash)
    elif url.startswith('http'):
        return get_http_seeds(url, info_hash)
    else:
        log.warning('There has beena problem with the get_tracker_seeds')
        return 0


class TorrentAlive(object):

    def validator(self):
        from flexget import validator
        root = validator.factory()
        root.accept('boolean')
        root.accept('integer')
        advanced = root.accept('dict')
        advanced.accept('integer', key='min_seeds')
        advanced.accept('interval', key='reject_for')
        return root

    def prepare_config(self, config):
        # Convert config to dict form
        if not isinstance(config, dict):
            config = {'min_seeds': int(config)}
        # Set the defaults
        config.setdefault('min_seeds', 1)
        config.setdefault('reject_for', '1 hour')
        return config

    @plugin.priority(150)
    def on_task_filter(self, task, config):
        if not config:
            return
        config = self.prepare_config(config)
        for entry in task.entries:
            if 'torrent_seeds' in entry and entry['torrent_seeds'] < config['min_seeds']:
                entry.reject(reason='Had < %d required seeds. (%s)' %
                            (config['min_seeds'], entry['torrent_seeds']))

    # Run on output phase so that we let torrent plugin output modified torrent file first
    @plugin.priority(250)
    def on_task_output(self, task, config):
        if not config:
            return
        config = self.prepare_config(config)
        min_seeds = config['min_seeds']

        for entry in task.accepted:
            # If torrent_seeds is filled, we will have already filtered in filter phase
            if entry.get('torrent_seeds'):
                log.debug('Not checking trackers for seeds, as torrent_seeds is already filled.')
                continue
            log.debug('Checking for seeds for %s:' % entry['title'])
            torrent = entry.get('torrent')
            if torrent:
                seeds = 0
                info_hash = torrent.info_hash
                announce_list = torrent.content.get('announce-list')
                if announce_list:
                    # Multitracker torrent
                    threadlist = []
                    for tier in announce_list:
                        for tracker in tier:
                            background = TorrentAliveThread(tracker, info_hash)
                            try:
                                background.start()
                                threadlist.append(background)
                            except threading.ThreadError:
                                # If we can't start a new thread, wait for current ones to complete and continue
                                log.debug('Reached max threads, finishing current threads.')
                                seeds = max(seeds, max_seeds_from_threads(threadlist))
                                background.start()
                                threadlist = [background]
                            log.debug('Started thread to scrape %s with info hash %s' % (tracker, info_hash))

                    seeds = max(seeds, max_seeds_from_threads(threadlist))
                    log.debug('Highest number of seeds found: %s' % seeds)
                else:
                    # Single tracker
                    tracker = torrent.content['announce']
                    try:
                        seeds = get_tracker_seeds(tracker, info_hash)
                    except URLError as e:
                        log.debug('Error scraping %s: %s' % (tracker, e))

                # Reject if needed
                if seeds < min_seeds:
                    entry.reject(reason='Tracker(s) had < %s required seeds. (%s)' % (min_seeds, seeds),
                                 remember_time=config['reject_for'])
                    # Maybe there is better match that has enough seeds
                    task.rerun()
                else:
                    log.debug('Found %i seeds from trackers' % seeds)


@event('plugin.register')
def register_plugin():
    plugin.register(TorrentAlive, 'torrent_alive', api_ver=2)

########NEW FILE########
__FILENAME__ = archive
from __future__ import unicode_literals, division, absolute_import
from collections import defaultdict
import logging
import re
from datetime import datetime

from sqlalchemy.orm import relationship
from sqlalchemy.orm.exc import NoResultFound
from sqlalchemy.schema import Table, ForeignKey
from sqlalchemy import Column, Integer, DateTime, Unicode, Index

from flexget import db_schema, options, plugin
from flexget.event import event
from flexget.entry import Entry
from flexget.options import ParseExtrasAction, get_parser
from flexget.utils.sqlalchemy_utils import table_schema, get_index_by_name
from flexget.utils.tools import console, strip_html
from flexget.manager import Session

log = logging.getLogger('archive')

SCHEMA_VER = 0

Base = db_schema.versioned_base('archive', SCHEMA_VER)

archive_tags_table = Table('archive_entry_tags', Base.metadata,
                           Column('entry_id', Integer, ForeignKey('archive_entry.id')),
                           Column('tag_id', Integer, ForeignKey('archive_tag.id')),
                           Index('ix_archive_tags', 'entry_id', 'tag_id'))

archive_sources_table = Table('archive_entry_sources', Base.metadata,
                              Column('entry_id', Integer, ForeignKey('archive_entry.id')),
                              Column('source_id', Integer, ForeignKey('archive_source.id')),
                              Index('ix_archive_sources', 'entry_id', 'source_id'))


class ArchiveEntry(Base):
    __tablename__ = 'archive_entry'
    __table_args__ = (Index('ix_archive_title_url', 'title', 'url'),)

    id = Column(Integer, primary_key=True)
    title = Column(Unicode, index=True)
    url = Column(Unicode, index=True)
    description = Column(Unicode)
    task = Column('feed', Unicode) # DEPRECATED, but SQLite does not support drop column
    added = Column(DateTime, index=True)

    tags = relationship("ArchiveTag", secondary=archive_tags_table)
    sources = relationship("ArchiveSource", secondary=archive_sources_table, backref='archive_entries')

    def __init__(self):
        self.added = datetime.now()

    def __str__(self):
        return '<ArchiveEntry(title=%s,url=%s,task=%s,added=%s)>' %\
               (self.title, self.url, self.task, self.added.strftime('%Y-%m-%d %H:%M'))


class ArchiveTag(Base):
    __tablename__ = 'archive_tag'

    id = Column(Integer, primary_key=True)
    name = Column(Unicode, index=True)

    def __init__(self, name):
        self.name = name

    def __str__(self):
        return '<ArchiveTag(id=%s,name=%s)>' % (self.id, self.name)


class ArchiveSource(Base):
    __tablename__ = 'archive_source'

    id = Column(Integer, primary_key=True)
    name = Column(Unicode, index=True)

    def __init__(self, name):
        self.name = name

    def __str__(self):
        return '<ArchiveSource(id=%s,name=%s)>' % (self.id, self.name)


def get_source(name, session):
    """
    :param string name: Source name
    :param session: SQLAlchemy session
    :return: ArchiveSource from db or new one
    """
    try:
        return session.query(ArchiveSource).filter(ArchiveSource.name == name).one()
    except NoResultFound:
        source = ArchiveSource(name)
        return source


def get_tag(name, session):
    """
    :param string name: Tag name
    :param session: SQLAlchemy session
    :return: ArchiveTag from db or new one
    """
    try:
        return session.query(ArchiveTag).filter(ArchiveTag.name == name).one()
    except NoResultFound:
        source = ArchiveTag(name)
        return source


@db_schema.upgrade('archive')
def upgrade(ver, session):
    if ver is None:
        # get rid of old index
        aet = table_schema('archive_entry', session)
        old_index = get_index_by_name(aet, 'archive_feed_title')
        if old_index is not None:
            log.info('Dropping legacy index (may take a while) ...')
            old_index.drop()
            # create new index by title, url
        new_index = get_index_by_name(Base.metadata.tables['archive_entry'], 'ix_archive_title_url')
        if new_index:
            log.info('Creating new index (may take a while) ...')
            new_index.create(bind=session.connection())
        else:
            # maybe removed from the model by later migrations?
            log.error('Unable to create index `ix_archive_title_url`, removed from the model?')
            # TODO: nag about this ?
        # This is safe as long as we don't delete the model completely :)
        # But generally never use Declarative Models in migrate!
        if session.query(ArchiveEntry).first():
            log.critical('----------------------------------------------')
            log.critical('You should run `--archive consolidate` ')
            log.critical('one time when you have time, it may take hours')
            log.critical('----------------------------------------------')
        ver = 0
    return ver


class Archive(object):
    """
    Archives all new items into database where they can be later searched and injected.
    Stores the entries in the state as they are at the exit phase, this way task cleanup for title
    etc is stored into the database. This may however make injecting them back to the original task work
    wrongly.
    """

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {'type': 'array', 'items': {'type': 'string'}}
        ]

    }

    def on_task_learn(self, task, config):
        """Add new entries into archive. We use learn phase in case the task corrects title or url via some plugins."""

        if isinstance(config, bool):
            tag_names = []
        else:
            tag_names = config

        tags = []
        for tag_name in set(tag_names):
            tags.append(get_tag(tag_name, task.session))

        count = 0
        processed = []
        for entry in task.entries + task.rejected + task.failed:
            # I think entry can be in multiple of those lists .. not sure though!
            if entry in processed:
                continue
            else:
                processed.append(entry)

            ae = task.session.query(ArchiveEntry).\
                filter(ArchiveEntry.title == entry['title']).\
                filter(ArchiveEntry.url == entry['url']).first()
            if ae:
                # add (missing) sources
                source = get_source(task.name, task.session)
                if not source in ae.sources:
                    log.debug('Adding `%s` into `%s` sources' % (task.name, ae))
                    ae.sources.append(source)
                # add (missing) tags
                for tag_name in tag_names:
                    atag = get_tag(tag_name, task.session)
                    if not atag in ae.tags:
                        log.debug('Adding tag %s into %s' % (tag_name, ae))
                        ae.tags.append(atag)
            else:
                # create new archive entry
                ae = ArchiveEntry()
                ae.title = entry['title']
                ae.url = entry['url']
                if 'description' in entry:
                    ae.description = entry['description']
                ae.task = task.name
                ae.sources.append(get_source(task.name, task.session))
                if tags:
                    # note, we're extending empty list
                    ae.tags.extend(tags)
                log.debug('Adding `%s` with %i tags to archive' % (ae, len(tags)))
                task.session.add(ae)
                count += 1
        if count:
            log.verbose('Added %i new entries to archive' % count)

    def on_task_abort(self, task, config):
        """
        Archive even on task abort, except if the abort has happened before session
        was started.
        """
        if task.session is not None:
            self.on_task_learn(task, config)


class UrlrewriteArchive(object):
    """
    Provides capability to rewrite urls from archive or make searches with discover.
    """

    entry_map = {'title': 'title',
                 'url': 'url',
                 'description': 'description'}

    schema = {'oneOf': [
        {'type': 'boolean'},
        {'type': 'array', 'items': {'type': 'string'}}
    ]}

    def search(self, entry, config=None):
        """Search plugin API method"""

        session = Session()
        entries = set()
        try:
            for query in entry.get('search_strings', [entry['title']]):
                log.debug('looking for `%s` config: %s' % (query, config))
                for archive_entry in search(session, query, desc=True):
                    log.debug('rewrite search result: %s' % archive_entry)
                    entry = Entry()
                    entry.update_using_map(self.entry_map, archive_entry, ignore_none=True)
                    if entry.isvalid():
                        entries.add(entry)
        finally:
            session.close()
        log.debug('found %i entries' % len(entries))
        return entries


def consolidate():
    """
    Converts previous archive data model to new one.
    """

    session = Session()
    try:
        log.verbose('Checking archive size ...')
        count = session.query(ArchiveEntry).count()
        log.verbose('Found %i items to migrate, this can be aborted with CTRL-C safely.' % count)

        # consolidate old data
        from progressbar import ProgressBar, Percentage, Bar, ETA

        widgets = ['Process - ', ETA(), ' ', Percentage(), ' ', Bar(left='[', right=']')]
        bar = ProgressBar(widgets=widgets, maxval=count).start()

        # id's for duplicates
        duplicates = []

        for index, orig in enumerate(session.query(ArchiveEntry).yield_per(5)):
            bar.update(index)

            # item already processed
            if orig.id in duplicates:
                continue

            # item already migrated
            if orig.sources:
                log.info('Database looks like it has already been consolidated, '
                         'item %s has already sources ...' % orig.title)
                session.rollback()
                return

            # add legacy task to the sources list
            orig.sources.append(get_source(orig.task, session))
            # remove task, deprecated .. well, let's still keep it ..
            #orig.task = None

            for dupe in session.query(ArchiveEntry).\
                filter(ArchiveEntry.id != orig.id).\
                filter(ArchiveEntry.title == orig.title).\
                    filter(ArchiveEntry.url == orig.url).all():
                orig.sources.append(get_source(dupe.task, session))
                duplicates.append(dupe.id)

        if duplicates:
            log.info('Consolidated %i items, removing duplicates ...' % len(duplicates))
            for id in duplicates:
                session.query(ArchiveEntry).filter(ArchiveEntry.id == id).delete()
        session.commit()
        log.info('Completed! This does NOT need to be ran again.')
    except KeyboardInterrupt:
        session.rollback()
        log.critical('Aborted, no changes saved')
    finally:
        session.close()


def tag_source(source_name, tag_names=None):
    """
    Tags all archived entries within a source with supplied tags

    :param string source_name: Source name
    :param list tag_names: List of tag names to add
    """

    if not tag_names or tag_names is None:
        return

    session = Session()
    try:
        # check that source exists
        source = session.query(ArchiveSource).filter(ArchiveSource.name == source_name).first()
        if not source:
            log.critical('Source `%s` does not exists' % source_name)
            srcs = ', '.join([s.name for s in session.query(ArchiveSource).order_by(ArchiveSource.name)])
            if srcs:
                log.info('Known sources: %s' % srcs)
            return

        # construct tags list
        tags = []
        for tag_name in tag_names:
            tags.append(get_tag(tag_name, session))

        # tag 'em
        log.verbose('Please wait while adding tags %s ...' % (', '.join(tag_names)))
        for a in session.query(ArchiveEntry).\
            filter(ArchiveEntry.sources.any(name=source_name)).yield_per(5):
            a.tags.extend(tags)
    finally:
        session.commit()
        session.close()


# API function, was also used from webui .. needs to be rethinked
def search(session, text, tags=None, sources=None, desc=False):
    """
    Search from the archive.

    :param string text: Search text, spaces and dots are tried to be ignored.
    :param Session session: SQLAlchemy session, should not be closed while iterating results.
    :param list tags: Optional list of acceptable tags
    :param list sources: Optional list of acceptable sources
    :param bool desc: Sort results descending
    :return: ArchiveEntries responding to query
    """
    keyword = unicode(text).replace(' ', '%').replace('.', '%')
    # clean the text from any unwanted regexp, convert spaces and keep dots as dots
    normalized_re = re.escape(text.replace('.', ' ')).replace('\\ ', ' ').replace(' ', '.')
    find_re = re.compile(normalized_re, re.IGNORECASE)
    query = session.query(ArchiveEntry).filter(ArchiveEntry.title.like('%' + keyword + '%'))
    if tags:
        query = query.filter(ArchiveEntry.tags.any(ArchiveTag.name.in_(tags)))
    if sources:
        query = query.filter(ArchiveEntry.sources.any(ArchiveSource.name.in_(sources)))
    if desc:
        query = query.order_by(ArchiveEntry.added.desc())
    else:
        query = query.order_by(ArchiveEntry.added.asc())
    for a in query.yield_per(5):
        if find_re.match(a.title):
            yield a
        else:
            log.trace('title %s is too wide match' % a.title)


def cli_search(options):
    search_term = ' '.join(options.keywords)
    tags = options.tags
    sources = options.sources

    def print_ae(ae):
        diff = datetime.now() - ae.added

        console('ID: %-6s | Title: %s\nAdded: %s (%d days ago)\nURL: %s' %
                (ae.id, ae.title, ae.added, diff.days, ae.url))
        source_names = ', '.join([s.name for s in ae.sources])
        tag_names = ', '.join([t.name for t in ae.tags])
        console('Source(s): %s | Tag(s): %s' % (source_names or 'N/A', tag_names or 'N/A'))
        if ae.description:
            console('Description: %s' % strip_html(ae.description))
        console('---')

    session = Session()
    try:
        console('Searching: %s' % search_term)
        if tags:
            console('Tags: %s' % ', '.join(tags))
        if sources:
            console('Sources: %s' % ', '.join(sources))
        console('Please wait...')
        console('')
        results = False
        for ae in search(session, search_term, tags=tags, sources=sources):
            print_ae(ae)
            results = True
        if not results:
            console('No results found.')
    finally:
        session.close()


def cli_inject(manager, options):
    log.debug('Finding inject content')
    inject_entries = defaultdict(list)
    session = Session()
    try:
        for id in options.ids:
            archive_entry = session.query(ArchiveEntry).get(id)

            # not found
            if not archive_entry:
                log.critical('There\'s no archived item with ID `%s`' % id)
                continue

            # find if there is no longer any task within sources
            if not any(source.name in manager.tasks for source in archive_entry.sources):
                log.error('None of sources (%s) exists anymore, cannot inject `%s` from archive!' %
                          (', '.join([s.name for s in archive_entry.sources]), archive_entry.title))
                continue

            # update list of tasks to be injected
            for source in archive_entry.sources:
                inject_entries[source.name].append(archive_entry)
    finally:
        session.close()

    for task_name in inject_entries:
        entries = []
        for inject_entry in inject_entries[task_name]:
            log.info('Injecting from archive `%s`' % inject_entry.title)
            entry = Entry(inject_entry.title, inject_entry.url)
            if inject_entry.description:
                entry['description'] = inject_entry.description
            if options.immortal:
                log.debug('Injecting as immortal')
                entry['immortal'] = True
            entry['accepted_by'] = 'archive inject'
            entry.accept('injected')
            entries.append(entry)

        manager.scheduler.execute(options={'inject': entries, 'tasks': [task_name]})

    with manager.acquire_lock():
        manager.scheduler.start(run_schedules=False)
        manager.shutdown()


def do_cli(manager, options):
    action = options.archive_action

    if action == 'tag-source':
        tag_source(options.source, tag_names=options.tags)
    elif action == 'consolidate':
        consolidate()
    elif action == 'search':
        cli_search(options)
    elif action == 'inject':
        cli_inject(manager, options)


@event('plugin.register')
def register_plugin():
    plugin.register(Archive, 'archive', api_ver=2)
    plugin.register(UrlrewriteArchive, 'flexget_archive', groups=['search'], api_ver=2)


@event('options.register')
def register_parser_arguments():
    archive_parser = options.register_command('archive', do_cli, help='search and manipulate the archive database')
    archive_parser.add_subparsers(title='Actions', metavar='<action>', dest='archive_action')
    # Default usage shows the positional arguments after the optional ones, override usage to fix it
    search_parser = archive_parser.add_subparser('search', help='search from the archive',
                                                 usage='%(prog)s [-h] <keyword> [<keyword> ...] [optional arguments]')
    search_parser.add_argument('keywords', metavar='<keyword>', nargs='+', help='keyword(s) to search for')
    search_parser.add_argument('--tags', metavar='TAG', nargs='+', default=[], help='tag(s) to search within')
    search_parser.add_argument('--sources', metavar='SOURCE', nargs='+', default=[], help='source(s) to search within')
    inject_parser = archive_parser.add_subparser('inject', help='inject entries from the archive back into tasks')
    inject_parser.add_argument('ids', nargs='+', type=int, metavar='ID', help='archive ID of an item to inject')
    inject_parser.add_argument('--immortal', action='store_true', help='injected entries will not be able to be '
                                                                       'rejected by any plugins')
    exec_group = inject_parser.add_argument_group('execute arguments')
    exec_group.add_argument('execute_options', action=ParseExtrasAction, parser=get_parser('execute'))
    tag_parser = archive_parser.add_subparser('tag-source', help='tag all archived entries within a given source')
    tag_parser.add_argument('source', metavar='<source>', help='the source whose entries you would like to tag')
    tag_parser.add_argument('tags', nargs='+', metavar='<tag>',
                            help='the tag(s) you would like to apply to the entries')
    archive_parser.add_subparser('consolidate', help='migrate old archive data to new model, may take a long time')

########NEW FILE########
__FILENAME__ = cron_env
from __future__ import unicode_literals, division, absolute_import
from flexget.utils.log import log_once

__author__ = 'paranoidi'

import logging
import sys

from flexget.event import event
from flexget.utils.simple_persistence import SimplePersistence

log = logging.getLogger('cron_env')


@event('manager.execute.started')
def check_env(manager):
    persistence = SimplePersistence(plugin='cron_env')
    encoding = sys.getfilesystemencoding()
    if manager.options.execute.cron:
        if 'terminal_encoding' in persistence:
            terminal_encoding = persistence['terminal_encoding']
            if terminal_encoding != encoding:
                log.warning('Your cron environment has different filesystem encoding '
                            '(%s) compared to your terminal environment (%s).' %
                            (encoding, terminal_encoding))
                if encoding == 'ANSI_X3.4-1968':
                    log.warning('Your current cron environment results filesystem encoding ANSI_X3.4-1968 '
                                'which supports only ASCII letters in filenames.')
            else:
                log_once('Good! Your crontab environment seems to be same as terminal.')
        else:
            log.info('Please run FlexGet manually once for environment verification purposes.')
    else:
        log.debug('Encoding %s stored' % encoding)
        persistence['terminal_encoding'] = encoding

########NEW FILE########
__FILENAME__ = db_analyze
from __future__ import unicode_literals, division, absolute_import
import logging
from flexget.event import event

log = logging.getLogger('db_analyze')


# Run after the cleanup is actually finished
@event('manager.db_cleanup', 0)
def on_cleanup(session):
    log.info('Running ANALYZE on database to improve performance.')
    session.execute('ANALYZE')

########NEW FILE########
__FILENAME__ = db_vacuum
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime, timedelta
from flexget.utils.simple_persistence import SimplePersistence
from flexget.event import event

log = logging.getLogger('db_vacuum')
VACUUM_INTERVAL = timedelta(weeks=24) # 6 months


# Run after the cleanup is actually finished, but before analyze
@event('manager.db_cleanup', 1)
def on_cleanup(session):
    # Vacuum can take a long time, and is not needed frequently
    persistence = SimplePersistence('db_vacuum')
    last_vacuum = persistence.get('last_vacuum')
    if not last_vacuum or last_vacuum < datetime.now() - VACUUM_INTERVAL:
        log.info('Running VACUUM on database to improve performance and decrease db size.')
        session.execute('VACUUM')
        persistence['last_vacuum'] = datetime.now()

########NEW FILE########
__FILENAME__ = log_start
from __future__ import unicode_literals, division, absolute_import
from argparse import SUPPRESS
import logging
import os


from flexget import options
from flexget.event import event

log = logging.getLogger('log_start')


@event('manager.startup')
def log_start(manager):
    if manager.options.log_start:
            log.info('FlexGet started (PID: %s)' % os.getpid())


@event('manager.shutdown')
def log_start(manager):
    if manager.options.log_start:
            log.info('FlexGet stopped (PID: %s)' % os.getpid())


@event('options.register')
def register_options():
    options.get_parser().add_argument('--log-start', action='store_true', help=SUPPRESS)

########NEW FILE########
__FILENAME__ = urlfix
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.log import log_once

log = logging.getLogger('urlfix')


class UrlFix(object):
    """
    Automatically fix broken urls.
    """

    schema = {'type': 'boolean'}

    @plugin.priority(-255)
    def on_task_input(self, task, config):
        if config is False:
            return
        for entry in task.entries:
            if '&amp;' in entry['url']:
                log_once('Corrected `%s` url (replaced &amp; with &)' % entry['title'], logger=log)
                entry['url'] = entry['url'].replace('&amp;', '&')


@event('plugin.register')
def register_plugin():
    plugin.register(UrlFix, 'urlfix', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = welcome
from __future__ import unicode_literals, division, absolute_import
import os

__author__ = 'paranoidi'

import logging
import sys

from flexget.event import event
from flexget.utils.simple_persistence import SimplePersistence

log = logging.getLogger('welcome')


@event('manager.lock_acquired')
def welcome_message(manager):
    # Only run for cli cron executions
    if manager.options.cli_command != 'execute' or not manager.options.execute.cron:
        return
    persistence = SimplePersistence(plugin='welcome')
    count = persistence.setdefault('count', 5)
    if not count:
        return

    # check for old users, assume old user if db larger than 2 MB
    if count == 5 and os.stat(manager.db_filename).st_size / 1024 / 1024 >= 2:
        log.debug('Looks like old user, skipping welcome message')
        persistence['count'] = 0
        return

    count -= 1
    scheduler = 'scheduler' if sys.platform.startswith('win') else 'crontab'
    if not count:
        log.info('FlexGet has been successfully started from %s (--cron). '
                 'I hope you have %s under control now. This message will not be repeated again.' %
                 (scheduler, scheduler))
    else:
        log.info('%sFlexGet has been successfully started from %s (--cron). '
                 'This message will be repeated %i times for your set up verification conveniences.' %
                 ('Congratulations! ' if count == 4 else '',
                  scheduler, count))
    persistence['count'] = count

########NEW FILE########
__FILENAME__ = apple_trailers
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached
from flexget.utils.requests import RequestException
from flexget.utils.soup import get_soup
try:
    from flexget.plugins.input.rss import InputRSS
except ImportError:
    raise plugin.DependencyError(issued_by='apple_trailers', missing='rss')

log = logging.getLogger('apple_trailers')


class AppleTrailers(InputRSS):
    """
        Adds support for Apple.com movie trailers.

        Configuration:
        quality: Set the desired resolution - 480p or 720p. default '720p'
        genres:  List of genres used to filter the entries. If set, the
        trailer must match at least one listed genre to be accepted. Genres
        that can be used: Action and Adventure, Comedy, Documentary, Drama,
        Family, Fantasy, Foreign, Horror, Musical, Romance, Science Fiction,
        Thriller. default '' (all)

        apple_trailers:
          quality: 720p
          genres: ['Action and Adventure']

        Alternatively, a simpler configuration format can be used. This uses
        the default genre filter, all:

        apple_trailers: 720p

        This plugin adds the following fields to the entry:
          movie_name, movie_year, genres, apple_trailers_name, movie_studio
        movie_name: Name of the movie
        movie_year: Year the movie was/will be released
        genres: Comma-separated list of genres that apply to the movie
        apple_trailers_name: Contains the Apple-supplied name of the clip,
        such as 'Clip 2', 'Trailer', 'Winter Olympic Preview'
        movie_studio: Name of the studio that makes the movie
    """

    rss_url = 'http://trailers.apple.com/trailers/home/rss/newtrailers.rss'
    qualities = ['480p', '720p']

    schema = {
        'oneOf': [
            {
                'type': 'object',
                'properties': {
                    'quality': {
                        'type': 'string',
                        'enum': qualities,
                        'default': '720p'
                    },
                    'genres': {'type': 'array', 'items': {'type': 'string'}}
                },
                'additionalProperties': False
            },
            {'title': 'justquality', 'type': 'string', 'enum': qualities}
            ]

    }

    # Run before headers plugin
    @plugin.priority(135)
    def on_task_start(self, task, config):
        # TODO: Resolve user-agent in a way that doesn't involve modifying the task config.
        # make sure we have dependencies available, will throw DependencyError if not
        plugin.get_plugin_by_name('headers')
        # configure them
        task.config['headers'] = {'User-Agent': 'Quicktime/7.7'}

    @plugin.priority(127)
    @cached('apple_trailers')
    def on_task_input(self, task, config):
        # use rss plugin
        # since we have to do 2 page lookups per trailer, use all_entries False to lighten load
        rss_config = {'url': self.rss_url, 'all_entries': False}
        rss_entries = super(AppleTrailers, self).on_task_input(task, rss_config)

        # Multiple entries can point to the same movie page (trailer 1, clip1, etc.)
        trailers = {}
        for entry in rss_entries:
            url = entry['original_url']
            trailers.setdefault(url, []).append(entry['title'])

        result = []
        for url, titles in trailers.iteritems():
            genre_url = url + '#gallery-film-info-details'
            try:
                page = task.requests.get(genre_url)
                soup = get_soup(page.text)
            except RequestException as err:
                log.warning("RequestsException when opening playlist page: %s" % err)

            genres = set()
            genre_head = soup.find(name='dt', text='Genre')
            if not genre_head:
                log.debug('genre(s) not found')
            for genre_name in genre_head.next_sibling.contents:
                if genre_name == ' ' or genre_name == ', ':
                    continue
                genres.add(genre_name.contents[0].string)
                log.debug('genre found: %s' % genre_name.contents[0].string)

            #Turn simple config into full config
            if isinstance(config, basestring):
                config = {'quality': config}

            if config.get('genres'):
                config_genres = set(config.get('genres'))
                good_genres = set.intersection(config_genres, genres)
                if not good_genres:
                    continue

            film_detail = soup.find(class_='film-detail')
            release_year = ''
            studio = ''
            if not film_detail:
                log.debug('film detail not found')
            else:
                release_c = film_detail.contents[1].string
                release_year = release_c[(release_c.find(', 20')+2):(release_c.find(', 20')+6)]
                log.debug('release year: %s' % release_year)
                studio_c = film_detail.contents[5].string
                studio = studio_c[7:]
                log.debug('studio: %s' % studio)

            # the HTML for the trailer gallery is stored in a "secret" location...let's see how long this lasts
            # the iPad version has direct links to the video files
            url = url + 'includes/playlists/ipad.inc'
            try:
                page = task.requests.get(url)
                soup = get_soup(page.text)
            except RequestException as err:
                log.warning("RequestsException when opening playlist page: %s" % err)
                continue

            for title in titles:
                log.debug('Searching for trailer title: %s' % title.split(' - ')[1])
                try:
                    trailer = soup.find(text=title.split(' - ')[1])
                except AttributeError:
                    log.debug('did not find %s listed' % title.split(' - ')[1])
                    continue
                try:
                    trailers_link = trailer.parent.next_sibling.next_sibling.contents[1].contents[0]
                except AttributeError:
                    log.debug('did not find trailer link tag')
                    continue
                try:
                    link = trailers_link['href'].replace('r640s', ''.join(['h',config.get('quality')]))
                except AttributeError:
                    log.debug('could not find download link')
                    continue
                entry = Entry(title, link)
                # Populate a couple entry fields for making pretty filenames
                entry['movie_name'], entry['apple_trailers_name'] = title.split(' - ')
                if genres:
                    entry['genres'] = ', '.join(list(genres))
                if release_year:
                    entry['movie_year'] = release_year
                if studio:
                    entry['movie_studio'] = studio
                result.append(entry)

        return result

@event('plugin.register')
def register_plugin():
    plugin.register(AppleTrailers, 'apple_trailers', api_ver=2)

########NEW FILE########
__FILENAME__ = backlog
from __future__ import unicode_literals, division, absolute_import
import logging
import pickle
from datetime import datetime

from sqlalchemy import Column, Integer, String, DateTime, PickleType, Index

from flexget import db_schema, plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.database import safe_pickle_synonym
from flexget.utils.sqlalchemy_utils import table_schema
from flexget.utils.tools import parse_timedelta

log = logging.getLogger('backlog')
Base = db_schema.versioned_base('backlog', 1)


@db_schema.upgrade('backlog')
def upgrade(ver, session):
    if ver is None:
        # Make sure there is no data we can't load in the backlog table
        backlog_table = table_schema('backlog', session)
        try:
            for item in session.query('entry').select_from(backlog_table).all():
                pickle.loads(item.entry)
        except (ImportError, TypeError):
            # If there were problems, we can drop the data.
            log.info('Backlog table contains unloadable data, clearing old data.')
            session.execute(backlog_table.delete())
        ver = 0
    if ver == 0:
        backlog_table = table_schema('backlog', session)
        log.info('Creating index on backlog table.')
        Index('ix_backlog_feed_expire', backlog_table.c.feed, backlog_table.c.expire).create(bind=session.bind)
        ver = 1
    return ver


class BacklogEntry(Base):

    __tablename__ = 'backlog'

    id = Column(Integer, primary_key=True)
    task = Column('feed', String)
    title = Column(String)
    expire = Column(DateTime)
    _entry = Column('entry', PickleType)
    entry = safe_pickle_synonym('_entry')

    def __repr__(self):
        return '<BacklogEntry(title=%s)>' % (self.title)

Index('ix_backlog_feed_expire', BacklogEntry.task, BacklogEntry.expire)


class InputBacklog(object):
    """
    Keeps task history for given amount of time.

    Example::

      backlog: 4 days

    Rarely useful for end users, mainly used by other plugins.
    """

    def validator(self):
        from flexget import validator
        return validator.factory('interval')

    @plugin.priority(-255)
    def on_task_input(self, task, config):
        # Get a list of entries to inject
        injections = self.get_injections(task)
        # Take a snapshot of the entries' states after the input event in case we have to store them to backlog
        for entry in task.entries + injections:
            entry.take_snapshot('after_input')
        if config:
            # If backlog is manually enabled for this task, learn the entries.
            self.learn_backlog(task, config)
        # Return the entries from backlog that are not already in the task
        return injections

    def on_task_abort(self, task, config):
        """Remember all entries until next execution when task gets aborted."""
        if task.entries:
            log.debug('Remembering all entries to backlog because of task abort.')
            self.learn_backlog(task)

    def add_backlog(self, task, entry, amount=''):
        """Add single entry to task backlog

        If :amount: is not specified, entry will only be injected on next execution."""
        snapshot = entry.snapshots.get('after_input')
        if not snapshot:
            if task.current_phase != 'input':
                # Not having a snapshot is normal during input phase, don't display a warning
                log.warning('No input snapshot available for `%s`, using current state' % entry['title'])
            snapshot = entry
        expire_time = datetime.now() + parse_timedelta(amount)
        backlog_entry = task.session.query(BacklogEntry).filter(BacklogEntry.title == entry['title']).\
            filter(BacklogEntry.task == task.name).first()
        if backlog_entry:
            # If there is already a backlog entry for this, update the expiry time if necessary.
            if backlog_entry.expire < expire_time:
                log.debug('Updating expiry time for %s' % entry['title'])
                backlog_entry.expire = expire_time
        else:
            log.debug('Saving %s' % entry['title'])
            backlog_entry = BacklogEntry()
            backlog_entry.title = entry['title']
            backlog_entry.entry = snapshot
            backlog_entry.task = task.name
            backlog_entry.expire = expire_time
            task.session.add(backlog_entry)

    def learn_backlog(self, task, amount=''):
        """Learn current entries into backlog. All task inputs must have been executed."""
        for entry in task.entries:
            self.add_backlog(task, entry, amount)

    def get_injections(self, task):
        """Insert missing entries from backlog."""
        entries = []
        task_backlog = task.session.query(BacklogEntry).filter(BacklogEntry.task == task.name)
        for backlog_entry in task_backlog.all():
            entry = Entry(backlog_entry.entry)

            # this is already in the task
            if task.find_entry(title=entry['title'], url=entry['url']):
                continue
            log.debug('Restoring %s' % entry['title'])
            entries.append(entry)
        if entries:
            log.verbose('Added %s entries from backlog' % len(entries))

        # purge expired
        for backlog_entry in task_backlog.filter(datetime.now() > BacklogEntry.expire).all():
            log.debug('Purging %s' % backlog_entry.title)
            task.session.delete(backlog_entry)

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(InputBacklog, 'backlog', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = betaseries_list
"""Input plugin for www.betaseries.com"""
from __future__ import unicode_literals, division, absolute_import
from hashlib import md5
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import requests
from flexget.utils.cached_input import cached


log = logging.getLogger('betaseries_list')

API_URL_PREFIX = 'http://api.betaseries.com/'


class BetaSeriesList(object):
    """
        Emits an entry for each serie followed by one or more BetaSeries account.
        See http://www.betaseries.com/

        Configuration examples:

        # will get all series followed by the account identified by your_user_name
        betaseries_list:
          username: your_user_name
          password: your_password
          api_key: your_api_key

        # will get all series followed by the account identified by some_other_guy
        betaseries_list:
          username: your_user_name
          password: your_password
          api_key: your_api_key
          members:
            - some_other_guy

        # will get all series followed by the accounts identified by guy1 and guy2
        betaseries_list:
          username: your_user_name
          password: your_password
          api_key: your_api_key
          members:
            - guy1
            - guy2


        Api key can be requested at http://www.betaseries.com/api.

        This plugin is meant to work with the import_series plugin as follow:

        import_series:
          from:
            betaseries_list:
              username: xxxxx
              password: xxxxx
              api_key: xxxxx

    """

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'api_key': {'type': 'string'},
            'members': {
                    'type': 'array',
                    'items': {
                        "title": "member name",
                        "type": "string"
                    }
            }
        },
        'required': ['username', 'password', 'api_key'],
        'additionalProperties': False
    }

    @cached('betaseries_list', persist='2 hours')
    def on_task_input(self, task, config):
        username = config['username']
        password = config['password']
        api_key = config['api_key']
        members = config.get('members', [username])

        titles = set()
        try:
            user_token = create_token(api_key, username, password)
            for member in members:
                titles.update(query_series(api_key, user_token, member))
        except (requests.RequestException, AssertionError) as err:
            log.critical('Failed to get series at BetaSeries.com: %s' % err.message, exc_info=err)

        log.verbose("series: " + ", ".join(titles))
        entries = []
        for t in titles:
            e = Entry()
            e['title'] = t
            entries.append(e)
        return entries


def create_token(api_key, login, password):
    """
    login in and request an new API token.
    http://www.betaseries.com/wiki/Documentation#cat-members

    :param string api_key: Api key requested at http://www.betaseries.com/api
    :param string login: Login name
    :param string password: Password
    :return: User token
    """
    r = requests.post(API_URL_PREFIX + 'members/auth', params={
        'login': login,
        'password': md5(password).hexdigest()
    }, headers={
        'Accept': 'application/json',
        'X-BetaSeries-Version': '2.1',
        'X-BetaSeries-Key': api_key,
    })
    assert r.status_code == 200, "Bad HTTP status code: %s" % r.status_code
    j = r.json()
    error_list = j['errors']
    for err in error_list:
        log.error(str(err))
    if not error_list:
        return j['token']


def query_member_id(api_key, user_token, login_name):
    """
    Get the member id of a member identified by its login name.

    :param string api_key: Api key requested at http://www.betaseries.com/api
    :param string user_token: obtained with a call to create_token()
    :param string login_name: The login name of the member
    :return: Id of the member identified by its login name or `None` if not found
    """
    r = requests.get(API_URL_PREFIX + 'members/search', params={
        'login': login_name
    }, headers={
        'Accept': 'application/json',
        'X-BetaSeries-Version': '2.1',
        'X-BetaSeries-Key': api_key,
        'X-BetaSeries-Token': user_token,
    })
    assert r.status_code == 200, "Bad HTTP status code: %s" % r.status_code
    j = r.json()
    error_list = j['errors']
    for err in error_list:
        log.error(str(err))
    found_id = None
    if not error_list:
        for candidate in j['users']:
            if candidate['login'] == login_name:
                found_id = candidate['id']
                break
    return found_id


def query_series(api_key, user_token, member_name=None):
    """
    Get the list of series followed by the authenticated user

    :param string api_key: Api key requested at http://www.betaseries.com/api
    :param string user_token: Obtained with a call to create_token()
    :param string member_name: [optional] A member name to get the list of series from. If None, will query the member
        for whom the user_token was for
    :return: List of serie titles or empty list
    """
    params = {}
    if member_name:
        member_id = query_member_id(api_key, user_token, member_name)
        if member_id:
            params = {'id': member_id}
        else:
            log.error("member %r not found" % member_name)
            return []
    r = requests.get(API_URL_PREFIX + 'members/infos', params=params, headers={
        'Accept': 'application/json',
        'X-BetaSeries-Version': '2.1',
        'X-BetaSeries-Key': api_key,
        'X-BetaSeries-Token': user_token,
    })
    assert r.status_code == 200, "Bad HTTP status code: %s" % r.status_code
    j = r.json()
    error_list = j['errors']
    for err in error_list:
        log.error(str(err))
    if not error_list:
        return [x['title'] for x in j['member']['shows'] if x['user']['archived'] is False]
    else:
        return []


@event('plugin.register')
def register_plugin():
    plugin.register(BetaSeriesList, 'betaseries_list', api_ver=2)

########NEW FILE########
__FILENAME__ = discover
from __future__ import unicode_literals, division, absolute_import
import datetime
import logging
import random

from sqlalchemy import Column, Integer, DateTime, Unicode, Index

from flexget import options, plugin
from flexget.event import event
from flexget.plugin import get_plugin_by_name, PluginError, PluginWarning
from flexget import db_schema
from flexget.utils.tools import parse_timedelta, multiply_timedelta

log = logging.getLogger('discover')
Base = db_schema.versioned_base('discover', 0)


class DiscoverEntry(Base):
    __tablename__ = 'discover_entry'

    id = Column(Integer, primary_key=True)
    title = Column(Unicode, index=True)
    task = Column(Unicode, index=True)
    last_execution = Column(DateTime)

    def __init__(self, title, task):
        self.title = title
        self.task = task
        self.last_execution = None

    def __str__(self):
        return '<DiscoverEntry(title=%s,task=%s,added=%s)>' % (self.title, self.task, self.last_execution)

Index('ix_discover_entry_title_task', DiscoverEntry.title, DiscoverEntry.task)


@event('manager.db_cleanup')
def db_cleanup(session):
    value = datetime.datetime.now() - parse_timedelta('7 days')
    for de in session.query(DiscoverEntry).filter(DiscoverEntry.last_execution <= value).all():
        log.debug('deleting %s' % de)
        session.delete(de)


class Discover(object):
    """
    Discover content based on other inputs material.

    Example::

      discover:
        what:
          - emit_series: yes
        from:
          - piratebay
        interval: [1 hours|days|weeks]
        ignore_estimations: [yes|no]
    """

    schema = {
        'type': 'object',
        'properties': {
            'what': {'type': 'array', 'items': {
                'allOf': [{'$ref': '/schema/plugins?phase=input'}, {'maxProperties': 1, 'minProperties': 1}]
            }},
            'from': {'type': 'array', 'items': {
                'allOf': [{'$ref': '/schema/plugins?group=search'}, {'maxProperties': 1, 'minProperties': 1}]
            }},
            'interval': {'type': 'string', 'format': 'interval', 'default': '5 hours'},
            'ignore_estimations': {'type': 'boolean', 'default': False},
            'limit': {'type': 'integer', 'minimum': 1}
        },
        'required': ['what', 'from'],
        'additionalProperties': False
    }

    def execute_inputs(self, config, task):
        """
        :param config: Discover config
        :param task: Current task
        :return: List of pseudo entries created by inputs under `what` configuration
        """
        entries = []
        entry_titles = set()
        entry_urls = set()
        # run inputs
        for item in config['what']:
            for input_name, input_config in item.iteritems():
                input = get_plugin_by_name(input_name)
                if input.api_ver == 1:
                    raise PluginError('Plugin %s does not support API v2' % input_name)
                method = input.phase_handlers['input']
                try:
                    result = method(task, input_config)
                except PluginError as e:
                    log.warning('Error during input plugin %s: %s' % (input_name, e))
                    continue
                if not result:
                    log.warning('Input %s did not return anything' % input_name)
                    continue

                for entry in result:
                    urls = ([entry['url']] if entry.get('url') else []) + entry.get('urls', [])
                    if any(url in entry_urls for url in urls):
                        log.debug('URL for `%s` already in entry list, skipping.' % entry['title'])
                        continue

                    if entry['title'] in entry_titles:
                        log.verbose('Ignored duplicate title `%s`' % entry['title'])    # TODO: should combine?
                        continue

                    entries.append(entry)
                    entry_titles.add(entry['title'])
                    entry_urls.update(urls)
        return entries

    def execute_searches(self, config, entries):
        """
        :param config: Discover plugin config
        :param entries: List of pseudo entries to search
        :return: List of entries found from search engines listed under `from` configuration
        """

        result = []
        for item in config['from']:
            if isinstance(item, dict):
                plugin_name, plugin_config = item.items()[0]
            else:
                plugin_name, plugin_config = item, None
            search = get_plugin_by_name(plugin_name).instance
            if not callable(getattr(search, 'search')):
                log.critical('Search plugin %s does not implement search method' % plugin_name)
            for index, entry in enumerate(entries):
                log.verbose('Searching for `%s` with plugin `%s` (%i of %i)' %
                            (entry['title'], plugin_name, index + 1, len(entries)))
                try:
                    search_results = search.search(entry, plugin_config)
                    if not search_results:
                        log.debug('No results from %s' % plugin_name)
                        entry.complete()
                        continue
                    log.debug('Discovered %s entries from %s' % (len(search_results), plugin_name))
                    if config.get('limit'):
                        search_results = sorted(search_results, reverse=True,
                                                key=lambda x: x.get('search_sort'))[:config['limit']]
                    for e in search_results:
                        e['discovered_from'] = entry['title']
                        e['discovered_with'] = plugin_name
                        e.on_complete(self.entry_complete, query=entry, search_results=search_results)

                    result.extend(search_results)

                except (PluginError, PluginWarning) as err:
                    log.debug('No results from %s: %s' % (plugin_name, err))
                    entry.complete()

        return sorted(result, reverse=True, key=lambda x: x.get('search_sort'))

    def entry_complete(self, entry, query=None, search_results=None, **kwargs):
        if entry.accepted:
            # One of the search results was accepted, transfer the acceptance back to the query entry which generated it
            query.accept()
        # Remove this entry from the list of search results yet to complete
        search_results.remove(entry)
        # When all the search results generated by a query entry are complete, complete the query which generated them
        if not search_results:
            query.complete()

    def estimated(self, entries):
        """
        :return: Entries that we have estimated to be available
        """
        estimator = get_plugin_by_name('estimate_release').instance
        result = []
        for entry in entries:
            est_date = estimator.estimate(entry)
            if est_date is None:
                log.debug('No release date could be determined for %s' % entry['title'])
                result.append(entry)
                continue
            if type(est_date) == datetime.date:
                # If we just got a date, add a time so we can compare it to now()
                est_date = datetime.datetime.combine(est_date, datetime.time())
            if datetime.datetime.now() >= est_date:
                log.debug('%s has been released at %s' % (entry['title'], est_date))
                result.append(entry)
            else:
                entry.reject('has not been released')
                entry.complete()
                log.debug("%s hasn't been released yet (Expected: %s)" % (entry['title'], est_date))
        return result

    def interval_total_seconds(self, interval):
        """
        Because python 2.6 doesn't have total_seconds()
        """
        return interval.seconds + interval.days * 24 * 3600

    def interval_expired(self, config, task, entries):
        """
        Maintain some limit levels so that we don't hammer search
        sites with unreasonable amount of queries.

        :return: Entries that are up for ``config['interval']``
        """
        config.setdefault('interval', '5 hour')
        interval = parse_timedelta(config['interval'])
        if task.options.discover_now:
            log.info('Ignoring interval because of --discover-now')
        result = []
        interval_count = 0
        for entry in entries:
            de = task.session.query(DiscoverEntry).\
                filter(DiscoverEntry.title == entry['title']).\
                filter(DiscoverEntry.task == task.name).first()

            if not de:
                log.debug('%s -> No previous run recorded' % entry['title'])
                de = DiscoverEntry(entry['title'], task.name)
                task.session.add(de)
            if task.options.discover_now or not de.last_execution:
                # First time we execute (and on --discover-now) we randomize time to avoid clumping
                delta = multiply_timedelta(interval, random.random())
                de.last_execution = datetime.datetime.now() - delta
            else:
                next_time = de.last_execution + interval
                log.debug('last_time: %r, interval: %s, next_time: %r, ',
                          de.last_execution, config['interval'], next_time)
                if datetime.datetime.now() < next_time:
                    log.debug('interval not met')
                    interval_count += 1
                    entry.reject('discover interval not met')
                    entry.complete()
                    continue
                de.last_execution = datetime.datetime.now()
            log.debug('interval passed')
            result.append(entry)
        if interval_count:
            log.verbose('Discover interval of %s not met for %s entries. Use --discover-now to override.' %
                        (config['interval'], interval_count))
        return result

    def on_task_input(self, task, config):
        task.no_entries_ok = True
        entries = self.execute_inputs(config, task)
        log.verbose('Discovering %i titles ...' % len(entries))
        if len(entries) > 500:
            log.critical('Looks like your inputs in discover configuration produced '
                         'over 500 entries, please reduce the amount!')
        # TODO: the entries that are estimated should be given priority over expiration
        entries = self.interval_expired(config, task, entries)
        if not config.get('ignore_estimations', False):
            entries = self.estimated(entries)
        return self.execute_searches(config, entries)


@event('plugin.register')
def register_plugin():
    plugin.register(Discover, 'discover', api_ver=2)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--discover-now', action='store_true', dest='discover_now',
                                               default=False, help='immediately try to discover everything')

########NEW FILE########
__FILENAME__ = emit_movie_queue
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.entry import Entry
from flexget.utils.imdb import make_url as make_imdb_url

try:
    from flexget.plugins.filter.movie_queue import queue_get
except ImportError:
    raise plugin.DependencyError(issued_by='emit_movie_queue', missing='movie_queue')

log = logging.getLogger('emit_movie_queue')


class EmitMovieQueue(object):
    """Use your movie queue as an input by emitting the content of it"""

    def validator(self):
        from flexget import validator
        root = validator.factory()
        root.accept('boolean')
        advanced = root.accept('dict')
        advanced.accept('boolean', key='year')
        advanced.accept('boolean', key='quality')
        return root

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {}
        config.setdefault('year', True)
        config.setdefault('quality', False)
        return config

    def on_task_input(self, task, config):
        if not config:
            return
        config = self.prepare_config(config)

        entries = []

        for queue_item in queue_get(session=task.session):
            entry = Entry()
            # make sure the entry has IMDB fields filled
            entry['url'] = ''
            if queue_item.imdb_id:
                entry['imdb_id'] = queue_item.imdb_id
                entry['imdb_url'] = make_imdb_url(queue_item.imdb_id)
            if queue_item.tmdb_id:
                entry['tmdb_id'] = queue_item.tmdb_id

            plugin.get_plugin_by_name('tmdb_lookup').instance.lookup(entry)
            # check if title is a imdb url (leftovers from old database?)
            # TODO: maybe this should be fixed at the queue_get ...
            if 'http://' in queue_item.title:
                log.debug('queue contains url instead of title')
                if entry.get('movie_name'):
                    entry['title'] = entry['movie_name']
                else:
                    log.error('Found imdb url in imdb queue, but lookup failed: %s' % entry['title'])
                    continue
            else:
                # normal title
                entry['title'] = queue_item.title

            # Add the year and quality if configured to
            if config.get('year') and entry.get('movie_year'):
                entry['title'] += ' %s' % entry['movie_year']
            # TODO: qualities can now be ranges.. how should we handle this?
            if config.get('quality') and queue_item.quality != 'ANY':
                log.info('quality option of emit_movie_queue is disabled while we figure out how to handle ranges')
                #entry['title'] += ' %s' % queue_item.quality
            entries.append(entry)
            log.debug('Added title and IMDB id to new entry: %s - %s' %
                     (entry['title'], entry['imdb_id']))

        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(EmitMovieQueue, 'emit_movie_queue', api_ver=2)

########NEW FILE########
__FILENAME__ = emit_series
from __future__ import unicode_literals, division, absolute_import
import logging

from sqlalchemy import desc, and_

from flexget import plugin
from flexget.event import event
from flexget.entry import Entry

log = logging.getLogger('emit_series')

try:
    from flexget.plugins.filter.series import SeriesTask, Series, Episode, Release, get_latest_release
except ImportError as e:
    log.error(e.message)
    raise plugin.DependencyError(issued_by='emit_series', missing='series')


class EmitSeries(object):
    """
    Emit next episode number from all series configured in this task.

    Supports only 'ep' and 'sequence' mode series.
    """

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {
                'type': 'object',
                'properties': {
                    'from_start': {'type': 'boolean', 'default': False},
                    'backfill': {'type': 'boolean', 'default': False}
                },
                'additionalProperties': False
            }
        ]
    }

    def ep_identifiers(self, season, episode):
        return ['S%02dE%02d' % (season, episode),
                '%dx%02d' % (season, episode)]

    def sequence_identifiers(self, episode):
        return ['%d' % episode]

    def search_entry(self, series, season, episode, task, rerun=True):
        if series.identified_by == 'ep':
            search_strings = ['%s %s' % (series.name, id) for id in self.ep_identifiers(season, episode)]
            series_id = 'S%02dE%02d' % (season, episode)
        else:
            search_strings = ['%s %s' % (series.name, id) for id in self.sequence_identifiers(episode)]
            series_id = episode
        entry = Entry(title=search_strings[0], url='',
                      search_strings=search_strings,
                      series_name=series.name,
                      series_season=season,
                      series_episode=episode,
                      series_id=series_id,
                      series_id_type=series.identified_by)
        if rerun:
            entry.on_complete(self.on_search_complete, task=task, identified_by=series.identified_by)
        return entry

    def on_task_input(self, task, config):
        if not config:
            return
        if isinstance(config, bool):
            config = {}
        if not task.is_rerun:
            self.try_next_season = {}
        entries = []
        for seriestask in task.session.query(SeriesTask).filter(SeriesTask.name == task.name).all():
            series = seriestask.series
            if not series:
                # TODO: How can this happen?
                log.debug('Found SeriesTask item without series specified. Cleaning up.')
                task.session.delete(seriestask)
                continue

            if series.identified_by not in ['ep', 'sequence']:
                log.verbose('Can only emit ep or sequence based series. `%s` is identified_by %s' %
                            (series.name, series.identified_by or 'auto'))
                continue

            low_season = 0 if series.identified_by == 'ep' else -1

            latest_season = get_latest_release(series)
            if latest_season:
                latest_season = latest_season.season
            else:
                latest_season = low_season + 1

            if self.try_next_season.get(series.name):
                entries.append(self.search_entry(series, latest_season + 1, 1, task))
            else:
                for season in xrange(latest_season, low_season, -1):
                    log.debug('Adding episodes for %d' % latest_season)
                    check_downloaded = not config.get('backfill')
                    latest = get_latest_release(series, season=season, downloaded=check_downloaded)
                    if series.begin and (not latest or latest < series.begin):
                        entries.append(self.search_entry(series, series.begin.season, series.begin.number, task))
                    elif latest:
                        start_at_ep = 1
                        episodes_this_season = (task.session.query(Episode).
                                                filter(Episode.series_id == series.id).
                                                filter(Episode.season == season))
                        if series.identified_by == 'sequence':
                            # Don't look for missing too far back with sequence shows
                            start_at_ep = max(latest.number - 10, 1)
                            episodes_this_season = episodes_this_season.filter(Episode.number >= start_at_ep)
                        latest_ep_this_season = episodes_this_season.order_by(desc(Episode.number)).first()
                        downloaded_this_season = (episodes_this_season.join(Episode.releases).
                                                filter(Release.downloaded == True).all())
                        # Calculate the episodes we still need to get from this season
                        if series.begin and series.begin.season == season:
                            start_at_ep = max(start_at_ep, series.begin.number)
                        eps_to_get = range(start_at_ep, latest_ep_this_season.number + 1)
                        for ep in downloaded_this_season:
                            try:
                                eps_to_get.remove(ep.number)
                            except ValueError:
                                pass
                        entries.extend(self.search_entry(series, season, x, task, rerun=False) for x in eps_to_get)
                        # If we have already downloaded the latest known episode, try the next episode
                        if latest_ep_this_season.releases:
                            entries.append(self.search_entry(series, season, latest_ep_this_season.number + 1, task))
                    else:
                        if config.get('from_start') or config.get('backfill'):
                            entries.append(self.search_entry(series, season, 1, task))
                        else:
                            log.verbose('Series `%s` has no history. Set begin option, or use CLI `series begin` '
                                        'subcommand to set first episode to emit' % series.name)
                            break

                    if not config.get('backfill'):
                        break

        return entries

    def on_search_complete(self, entry, task=None, identified_by=None, **kwargs):
        series = task.session.query(Series).filter(Series.name == entry['series_name']).first()
        latest = get_latest_release(series)
        episode = (task.session.query(Episode).join(Episode.series).
                   filter(Series.name == entry['series_name']).
                   filter(Episode.season == entry['series_season']).
                   filter(Episode.number == entry['series_episode']).
                   first())
        if entry.accepted or (episode and len(episode.releases) > 0):
            self.try_next_season.pop(entry['series_name'], None)
            task.rerun()
        elif latest and latest.season == entry['series_season']:
            if identified_by != 'ep':
                # Do not try next season if this is not an 'ep' show
                return
            if entry['series_name'] not in self.try_next_season:
                self.try_next_season[entry['series_name']] = True
                task.rerun()
            else:
                # Don't try a second time
                self.try_next_season[entry['series_name']] = False


@event('plugin.register')
def register_plugin():
    plugin.register(EmitSeries, 'emit_series', api_ver=2)

########NEW FILE########
__FILENAME__ = find
from __future__ import unicode_literals, division, absolute_import
from datetime import datetime
import logging
import os
import re
import sys

from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.event import event
from flexget.entry import Entry
from flexget.utils.cached_input import cached

log = logging.getLogger('find')
# Default to utf-8 if we get None from getfilesystemencoding()
FS_ENCODING = sys.getfilesystemencoding() or 'utf-8'


def fsencode(filename):
    """
    Prepares a filename for use with system calls.

    On Windows, keeps paths as native strings.
    On unixy systems encodes to a bytestring using the filesystem encoding.
    """
    if isinstance(filename, unicode):
        if sys.platform.startswith('win'):
            return filename
        else:
            # Linuxy systems can have trouble unless we give them a bytestring path
            return filename.encode(FS_ENCODING)
    elif isinstance(filename, str):
        return filename
    else:
        raise TypeError('expected bytes or str, not %s' % type(filename).__name__)


def fsdecode(filename, replace=False):
    """
    Makes sure a filename returned from a system call is converted back to a native string.

    :param bool replace: If replace is set to True, this function will mangle the path rather than throw an error.
        This can be useful for debug output, but not for accessing the file system any longer.
    """
    if isinstance(filename, str):
        return filename.decode(FS_ENCODING, 'replace' if replace else 'strict')
    elif isinstance(filename, unicode):
        return filename
    else:
        raise TypeError('expected bytes or str, not %s' % type(filename).__name__)


class InputFind(object):
    """
    Uses local path content as an input, recurses through directories and creates entries for files that match mask.

    You can specify either the mask key, in shell file matching format, (see python fnmatch module,) or regexp key.

    Example::

      find:
        path: /storage/movies/
        mask: *.avi

    Example::

      find:
        path:
          - /storage/movies/
          - /storage/tv/
        regexp: .*\.(avi|mkv)$
    """

    schema = {
        'type': 'object',
        'properties': {
            'path': one_or_more({'type': 'string', 'format': 'path'}),
            'mask': {'type': 'string'},
            'regexp': {'type': 'string', 'format': 'regex'},
            'recursive': {'type': 'boolean'}
        },
        'required': ['path'],
        'additionalProperties': False
    }

    def prepare_config(self, config):
        from fnmatch import translate
        # If only a single path is passed turn it into a 1 element list
        if isinstance(config['path'], basestring):
            config['path'] = [config['path']]
        config.setdefault('recursive', False)
        # If mask was specified, turn it in to a regexp
        if config.get('mask'):
            config['regexp'] = translate(config['mask'])
        # If no mask or regexp specified, accept all files
        if not config.get('regexp'):
            config['regexp'] = '.'

    @cached('find')
    def on_task_input(self, task, config):
        self.prepare_config(config)
        entries = []
        match = re.compile(config['regexp'], re.IGNORECASE).match
        for path in config['path']:
            log.debug('scanning %s' % path)
            # unicode causes problems in here on linux (#989)
            fs_path = fsencode(path)
            fs_path = os.path.expanduser(fs_path)
            for fs_item in os.walk(fs_path):
                # Make sure subfolder is decodable
                try:
                    fsdecode(fs_item[0])
                except UnicodeDecodeError as e:
                    log.warning('Directory `%s` in `%s` encoding broken? %s' %
                                (fsdecode(fs_item[0], replace=True), fsdecode(fs_path, replace=True), e))
                    continue
                for fs_name in fs_item[2]:
                    e = Entry()
                    # Make sure filename is decodable
                    try:
                        fsdecode(fs_name)
                    except UnicodeDecodeError as e:
                        log.warning('Filename `%s` in `%s` is not decodable by declared filesystem encoding `%s`. '
                                    'Either your environment does not declare the correct encoding, or this filename '
                                    'is incorrectly encoded.' %
                                    (fsdecode(fs_name, replace=True), fsdecode(fs_item[0], replace=True), FS_ENCODING))
                        continue

                    e['title'] = fsdecode(os.path.splitext(fs_name)[0])
                    # If mask fails continue
                    if not match(fsdecode(fs_name)):
                        continue
                    fs_filepath = os.path.join(fs_item[0], fs_name)
                    try:
                        e['timestamp'] = datetime.fromtimestamp(os.path.getmtime(fs_filepath))
                    except ValueError as e:
                        log.debug('Error setting timestamp for %s: %s' % (fsdecode(fs_filepath), e))
                    # We are done calling os.path functions, turn filepath back into a native string
                    filepath = fsdecode(fs_filepath)
                    e['location'] = filepath
                    # Windows paths need an extra / prepended to them for url
                    if not filepath.startswith('/'):
                        filepath = '/' + filepath
                    e['url'] = 'file://%s' % filepath
                    entries.append(e)
                # If we are not searching recursively, break after first (base) directory
                if not config['recursive']:
                    break
        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(InputFind, 'find', api_ver=2)

########NEW FILE########
__FILENAME__ = ftp_list
import logging
import ftplib
import os

from flexget import plugin
from flexget.event import event
from flexget.entry import Entry

log = logging.getLogger('ftp_list')


class InputFtpList(object):
    """
    Generate entries from a ftp listing

    Configuration:
      ftp_list:
        config:
          use-ssl: no
          name: <ftp name>
          username: <username>
          password: <password>
          host: <host to connect>
          port: <port>
        dirs:
          - <directory 1>
          - <directory 2>
          - ....
    """

    def validator(self):
        from flexget import validator
        root = validator.factory('dict')
        root.accept('list', key='dirs').accept('text')
        config = root.accept('dict', key='config', required=True)
        config.accept('text', key='name', required=True)
        config.accept('text', key='username', required=True)
        config.accept('text', key='password', required=True)
        config.accept('text', key='host', required=True)
        config.accept('integer', key='port', required=True)
        config.accept('boolean', key='use-ssl')
        return root

    def prepare_config(self, config):
        config.setdefault('use-ssl', False)
        return config

    def on_task_input(self, task, config):
        config = self.prepare_config(config)
        connection_config = config['config']

        if connection_config['use-ssl']:
            ftp = ftplib.FTP_TLS()
        else:
            ftp = ftplib.FTP()

        # ftp.set_debuglevel(2)
        log.debug('Trying connecting to: %s', (connection_config['host']))
        try: 
            ftp.connect(connection_config['host'], connection_config['port'])
            ftp.login(connection_config['username'], connection_config['password'])
        except ftplib.all_errors as e:
            raise plugin.PluginError(e)

        log.debug('Connected.')
            
        ftp.sendcmd('TYPE I')
        ftp.set_pasv(True)
        entries = []
        for path in config['dirs']:
            baseurl = "ftp://%s:%s@%s:%s/" % (connection_config['username'], connection_config['password'],
                                              connection_config['host'], connection_config['port'])

            try:
                dirs = ftp.nlst(path)
            except ftplib.error_perm as e:
                raise plugin.PluginWarning(str(e))

            if not dirs:
                log.verbose('Directory %s is empty', path)

            for p in dirs:
                url = baseurl + p
                title = os.path.basename(p)
                log.info('Accepting entry %s ' % title)
                entries.append(Entry(title, url))

        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(InputFtpList, 'ftp_list', api_ver=2)

########NEW FILE########
__FILENAME__ = generate
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.entry import Entry
from flexget import validator

log = logging.getLogger(__name__.rsplit('.')[-1])


class Generate(object):
    """Generates n number of random entries. Used for debugging purposes."""

    def validator(self):
        return validator.factory('integer')

    def on_task_input(self, task, config):
        amount = config or 0  # hackily makes sure it's an int value
        entries = []
        for i in range(amount):
            entry = Entry()
            import string
            import random
            entry['url'] = 'http://localhost/generate/%s/%s' % (i, ''.join([random.choice(string.letters + string.digits) for x in range(1, 30)]))
            entry['title'] = ''.join([random.choice(string.letters + string.digits) for x in range(1, 30)])
            entry['description'] = ''.join([random.choice(string.letters + string.digits) for x in range(1, 1000)])
            entries.append(entry)
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(Generate, 'generate', api_ver=2, debug=True)

########NEW FILE########
__FILENAME__ = gen_series
from __future__ import unicode_literals, division, absolute_import
import logging
import random
import string

from flexget import plugin
from flexget.event import event
from flexget.entry import Entry

log = logging.getLogger('gen_series')

PER_RUN = 50


class GenSeries(object):
    """
    Purely for debugging purposes. Not great quality :)

    gen_series_data:
        series: NUM
        seasons: NUM
        episodes: NUM
        qualities:
          - LIST

    This will also auto configure series plugin for testing
    """

    def __init__(self):
        self.entries = []

    def validator(self):
        from flexget import validator
        container = validator.factory('any')
        return container

    @plugin.priority(200)
    def on_task_start(self, task, config):
        log.info('Generating test data ...')
        series = []
        for num in range(config['series']):
            series.append('series %d name' % num)
            for season in range(int(config['seasons'])):
                for episode in range(int(config['episodes'])):
                    for quality in config['qualities']:
                        entry = Entry()
                        entry['title'] = 'series %d name - S%02dE%02d - %s' % \
                            (num, season + 1, episode + 1, quality)
                        entry['url'] = 'http://localhost/mock/%s' % \
                                       ''.join([random.choice(string.letters + string.digits) for x in range(1, 30)])
                        self.entries.append(entry)
        log.info('Generated %d entries' % len(self.entries))

        # configure series plugin, bad way but this is debug shit
        task.config['series'] = series

    def on_task_input(self, task, config):
        entries = []
        for num, entry in enumerate(self.entries):
            entries.append(entry)
            if num == PER_RUN - 1:
                break
        self.entries = self.entries[len(entries):]
        return entries

    def on_task_exit(self, task, config):
        if self.entries:
            log.info('There are still %d left to be processed!' % len(self.entries))
            # rerun ad infinitum, also commits session between them
            task._rerun = True
            task._rerun_count = 0


@event('plugin.register')
def register_plugin():
    plugin.register(GenSeries, 'gen_series_data', api_ver=2, debug=True)

########NEW FILE########
__FILENAME__ = html
from __future__ import unicode_literals, division, absolute_import
import urlparse
import logging
import urllib
import zlib
import re
from jinja2 import Template

from flexget import plugin
from flexget.event import event
from flexget.entry import Entry
from flexget.utils.soup import get_soup
from flexget.utils.cached_input import cached

log = logging.getLogger('html')


class InputHtml(object):
    """
        Parses urls from html page. Usefull on sites which have direct download
        links of any type (mp3, jpg, torrent, ...).

        Many anime-fansubbers do not provide RSS-feed, this works well in many cases.

        Configuration expects url parameter.

        Note: This returns ALL links on url so you need to configure filters
        to match only to desired content.
    """

    def validator(self):
        from flexget import validator
        root = validator.factory()
        root.accept('text')
        advanced = root.accept('dict')
        advanced.accept('url', key='url', required=True)
        advanced.accept('text', key='username')
        advanced.accept('text', key='password')
        advanced.accept('text', key='dump')
        advanced.accept('text', key='title_from')
        regexps = advanced.accept('list', key='links_re')
        regexps.accept('regexp')
        advanced.accept('boolean', key='increment')
        increment = advanced.accept('dict', key='increment')
        increment.accept('integer', key='from')
        increment.accept('integer', key='to')
        increment.accept('text', key='name')
        increment.accept('integer', key='step')
        increment.accept('boolean', key='stop_when_empty')
        increment.accept('integer', key='entries_count')
        return root

    def build_config(self, config):

        def get_auth_from_url():
            """Moves basic authentication from url to username and password fields"""
            parts = list(urlparse.urlsplit(config['url']))
            split = parts[1].split('@')
            if len(split) > 1:
                auth = split[0].split(':')
                if len(auth) == 2:
                    config['username'], config['password'] = auth[0], auth[1]
                else:
                    log.warning('Invalid basic authentication in url: %s' % config['url'])
                parts[1] = split[1]
                config['url'] = urlparse.urlunsplit(parts)

        if isinstance(config, basestring):
            config = {'url': config}
        get_auth_from_url()
        return config

    @cached('html')
    @plugin.internet(log)
    def on_task_input(self, task, config):
        config = self.build_config(config)

        auth = None
        if config.get('username') and config.get('password'):
            log.debug('Basic auth enabled. User: %s Password: %s' % (config['username'], config['password']))
            auth = (config['username'], config['password'])

        increment = config.get('increment')
        base_url = config['url']
        if increment:
            entries = None
            if not isinstance(increment, dict):
                increment = {}
            current = increment.get('from', 0)
            to = increment.get('to')
            step = increment.get('step', 1)
            base_url = config['url']
            entries_count = increment.get('entries_count', 500)
            stop_when_empty = increment.get('stop_when_empty', True)
            increment_name = increment.get('name', 'i')

            template_url = Template(base_url)
            template_dump = None
            if 'dump' in config:
                dump_name = config['dump']
                if dump_name:
                    template_dump = Template(dump_name)

            while to is None or current < to:
                render_ctx = {increment_name: current}
                url = template_url.render(**render_ctx)
                dump_name = None
                if template_dump:
                    dump_name = template_dump.render(**render_ctx)
                new_entries = self._request_url(task, config, url, auth, dump_name)
                if not entries:
                    entries = new_entries
                else:
                    entries.extend(new_entries)
                if stop_when_empty and not new_entries:
                    break
                if entries_count and len(entries) >= entries_count:
                    break
                current += step
            return entries
        else:
            return self._request_url(task, config, base_url, auth)

    def _request_url(self, task, config, url, auth, dump_name=None):
        log.verbose('Requesting: %s' % url)
        page = task.requests.get(url, auth=auth)
        log.verbose('Response: %s (%s)' % (page.status_code, page.reason))
        soup = get_soup(page.text)

        # dump received content into a file
        if dump_name:
            log.verbose('Dumping: %s' % dump_name)
            data = soup.prettify()
            with open(dump_name, 'w') as f:
                f.write(data)

        return self.create_entries(url, soup, config)

    def _title_from_link(self, link, log_link):
        title = link.text
        # longshot from next element (?)
        if not title:
            title = link.next.string
            if title is None:
                log.debug('longshot failed for %s' % log_link)
                return None
        return title or None

    def _title_from_url(self, url):
        parts = urllib.splitquery(url[url.rfind('/') + 1:])
        title = urllib.unquote_plus(parts[0])
        return title

    def create_entries(self, page_url, soup, config):

        queue = []
        duplicates = {}
        duplicate_limit = 4

        def title_exists(title):
            """Helper method. Return True if title is already added to entries"""
            for entry in queue:
                if entry['title'] == title:
                    return True

        for link in soup.find_all('a'):
            # not a valid link
            if not link.has_attr('href'):
                continue
            # no content in the link
            if not link.contents:
                continue

            url = link['href']
            log_link = url
            log_link = log_link.replace('\n', '')
            log_link = log_link.replace('\r', '')

            # fix broken urls
            if url.startswith('//'):
                url = 'http:' + url
            elif not url.startswith('http://') or not url.startswith('https://'):
                url = urlparse.urljoin(page_url, url)

            # get only links matching regexp
            regexps = config.get('links_re', None)
            if regexps:
                accept = False
                for regexp in regexps:
                    if re.search(regexp, url):
                        accept = True
                if not accept:
                    continue

            title_from = config.get('title_from', 'auto')
            if title_from == 'url':
                title = self._title_from_url(url)
                log.debug('title from url: %s' % title)
            elif title_from == 'title':
                if not link.has_attr('title'):
                    log.warning('Link `%s` doesn\'t have title attribute, ignored.' % log_link)
                    continue
                title = link['title']
                log.debug('title from title: %s' % title)
            elif title_from == 'auto':
                title = self._title_from_link(link, log_link)
                if title is None:
                    continue
                # automatic mode, check if title is unique
                # if there are too many duplicate titles, switch to title_from: url
                if title_exists(title):
                    # ignore index links as a counter
                    if 'index' in title and len(title) < 10:
                        log.debug('ignored index title %s' % title)
                        continue
                    duplicates.setdefault(title, 0)
                    duplicates[title] += 1
                    if duplicates[title] > duplicate_limit:
                        # if from url seems to be bad choice use title
                        from_url = self._title_from_url(url)
                        switch_to = 'url'
                        for ext in ('.html', '.php'):
                            if from_url.endswith(ext):
                                switch_to = 'title'
                        log.info('Link names seem to be useless, auto-configuring \'title_from: %s\'. '
                                 'This may not work well, you might need to configure it yourself.' % switch_to)
                        config['title_from'] = switch_to
                        # start from the beginning  ...
                        return self.create_entries(page_url, soup, config)
            elif title_from == 'link' or title_from == 'contents':
                # link from link name
                title = self._title_from_link(link, log_link)
                if title is None:
                    continue
                log.debug('title from link: %s' % title)
            else:
                raise plugin.PluginError('Unknown title_from value %s' % title_from)

            if not title:
                log.warning('title could not be determined for link %s' % log_link)
                continue

            # strip unicode white spaces
            title = title.replace(u'\u200B', u'').strip()

            # in case the title contains xxxxxxx.torrent - foooo.torrent clean it a bit (get up to first .torrent)
            # TODO: hack
            if title.lower().find('.torrent') > 0:
                title = title[:title.lower().find('.torrent')]

            if title_exists(title):
                # title link should be unique, add CRC32 to end if it's not
                hash = zlib.crc32(url.encode("utf-8"))
                crc32 = '%08X' % (hash & 0xFFFFFFFF)
                title = '%s [%s]' % (title, crc32)
                # truly duplicate, title + url crc already exists in queue
                if title_exists(title):
                    continue
                log.debug('uniqued title to %s' % title)

            entry = Entry()
            entry['url'] = url
            entry['title'] = title

            queue.append(entry)

        # add from queue to task
        return queue


@event('plugin.register')
def register_plugin():
    plugin.register(InputHtml, 'html', api_ver=2)

########NEW FILE########
__FILENAME__ = imdb_list
from __future__ import unicode_literals, division, absolute_import
import logging
import re

import feedparser

from flexget import plugin
from flexget.event import event
from flexget.utils.imdb import extract_id
from flexget.utils.cached_input import cached
from flexget.entry import Entry

log = logging.getLogger('imdb_list')

USER_ID_RE = r'^ur\d{7,8}$'


class ImdbList(object):
    """"Creates an entry for each movie in your imdb list."""

    schema = {
        'type': 'object',
        'properties': {
            'user_id': {
                'type': 'string',
                'pattern': USER_ID_RE,
                'error_pattern': 'user_id must be in the form urXXXXXXX'
            },
            'list': {'type': 'string'}
        },
        'required': ['list', 'user_id'],
        'additionalProperties': False
    }

    @cached('imdb_list', persist='2 hours')
    def on_task_input(self, task, config):
        log.verbose('Retrieving list %s ...' % config['list'])

        # Get the imdb list in RSS format
        if config['list'] in ['watchlist', 'ratings', 'checkins']:
            url = 'http://rss.imdb.com/user/%s/%s' % (config['user_id'], config['list'])
        else:
            url = 'http://rss.imdb.com/list/%s' % config['list']
        log.debug('Requesting %s' % url)
        try:
            rss = feedparser.parse(url)
        except LookupError as e:
            raise plugin.PluginError('Failed to parse RSS feed for list `%s` correctly: %s' % (config['list'], e))
        if rss.status == 404:
            raise plugin.PluginError('Unable to get imdb list. Either list is private or does not exist.')

        # Create an Entry for each movie in the list
        entries = []
        title_re = re.compile(r'(.*) \((\d{4})?.*?\)$')
        for entry in rss.entries:
            try:
                # IMDb puts some extra stuff in the titles, e.g. "Battlestar Galactica (2004 TV Series)"
                # Strip out everything but the date
                match = title_re.match(entry.title)
                title = match.group(1)
                if match.group(2):
                    title += ' (%s)' % match.group(2)
                entries.append(
                    Entry(title=title, url=entry.link, imdb_id=extract_id(entry.link), imdb_name=match.group(1)))
            except IndexError:
                log.critical('IndexError! Unable to handle RSS entry: %s' % entry)
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(ImdbList, 'imdb_list', api_ver=2)

########NEW FILE########
__FILENAME__ = inputs
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('inputs')


class PluginInputs(object):
    """
    Allows the same input plugin to be configured multiple times in a task.

    Example::

      inputs:
        - rss: http://feeda.com
        - rss: http://feedb.com
    """

    schema = {
        'type': 'array',
        'items': {'allOf': [{'$ref': '/schema/plugins?phase=input'}, {'maxProperties': 1, 'minProperties': 1}]}
    }

    def on_task_input(self, task, config):
        entries = []
        entry_titles = set()
        entry_urls = set()
        for item in config:
            for input_name, input_config in item.iteritems():
                input = plugin.get_plugin_by_name(input_name)
                if input.api_ver == 1:
                    raise plugin.PluginError('Plugin %s does not support API v2' % input_name)

                method = input.phase_handlers['input']
                try:
                    result = method(task, input_config)
                except plugin.PluginError as e:
                    log.warning('Error during input plugin %s: %s' % (input_name, e))
                    continue
                if not result:
                    msg = 'Input %s did not return anything' % input_name
                    if getattr(task, 'no_entries_ok', False):
                        log.verbose(msg)
                    else:
                        log.warning(msg)
                    continue
                for entry in result:
                    if entry['title'] in entry_titles:
                        log.debug('Title `%s` already in entry list, skipping.' % entry['title'])
                        continue
                    urls = ([entry['url']] if entry.get('url') else []) + entry.get('urls', [])
                    if any(url in entry_urls for url in urls):
                        log.debug('URL for `%s` already in entry list, skipping.' % entry['title'])
                        continue
                    entries.append(entry)
                    entry_titles.add(entry['title'])
                    entry_urls.update(urls)
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(PluginInputs, 'inputs', api_ver=2)

########NEW FILE########
__FILENAME__ = input_csv
from __future__ import unicode_literals, division, absolute_import
import logging
import csv

from requests import RequestException

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached

log = logging.getLogger('csv')


class InputCSV(object):
    """
        Adds support for CSV format. Configuration may seem a bit complex,
        but this has advantage of being universal solution regardless of CSV
        and internal entry fields.

        Configuration format:

        csv:
          url: <url>
          values:
            <field>: <number>

        Example DB-fansubs:

        csv:
          url: http://www.dattebayo.com/t/dump
          values:
            title: 3  # title is in 3th field
            url: 1    # download url is in 1st field

        Fields title and url are mandatory. First field is 1.
        List of other common (optional) fields can be found from wiki.
    """

    schema = {
        'type': 'object',
        'properties': {
            'url': {'type': 'string', 'format': 'url'},
            'values': {
                'type': 'object',
                'additionalProperties': {'type': 'integer'},
                'required': ['title', 'url']
            }
        },
        'required': ['url', 'values'],
        'additionalProperties': False
    }

    @cached('csv')
    def on_task_input(self, task, config):
        entries = []
        try:
            r = task.requests.get(config['url'])
        except RequestException as e:
            raise plugin.PluginError('Error fetching `%s`: %s' % (config['url'], e))
        # CSV module needs byte strings, we'll convert back to unicode later
        page = r.text.encode('utf-8').splitlines()
        for row in csv.reader(page):
            if not row:
                continue
            entry = Entry()
            for name, index in config.get('values', {}).items():
                try:
                    # Convert the value back to unicode
                    entry[name] = row[index - 1].decode('utf-8').strip()
                except IndexError:
                    raise plugin.PluginError('Field `%s` index is out of range' % name)

            entries.append(entry)
        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(InputCSV, 'csv', api_ver=2)

########NEW FILE########
__FILENAME__ = listdir
"""Plugin for filesystem tasks."""
from __future__ import unicode_literals, division, absolute_import
import os
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event

log = logging.getLogger('listdir')


class Listdir(object):
    """
    Uses local path content as an input.

    Example::

      listdir: /storage/movies/
    """

    def validator(self):
        from flexget import validator
        root = validator.factory()
        root.accept('path')
        bundle = root.accept('list')
        bundle.accept('path')
        return root

    def on_task_input(self, task, config):
        # If only a single path is passed turn it into a 1 element list
        if isinstance(config, basestring):
            config = [config]
        entries = []
        for path in config:
            path = os.path.expanduser(path)
            for name in os.listdir(unicode(path)):
                e = Entry()
                filepath = os.path.join(path, name)
                if os.path.isfile(filepath):
                    e['title'] = os.path.splitext(name)[0]
                else:
                    e['title'] = name
                e['location'] = filepath
                # Windows paths need an extra / preceded to them
                if not filepath.startswith('/'):
                    filepath = '/' + filepath
                e['url'] = 'file://%s' % filepath
                e['filename'] = name
                entries.append(e)
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(Listdir, 'listdir', api_ver=2)

########NEW FILE########
__FILENAME__ = mock
"""Plugin for mocking task data."""
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event

log = logging.getLogger('mock')


class Mock(object):
    """
    Allows adding mock input entries.

    Example::

      mock:
        - {title: foobar, url: http://some.com }
        - {title: mock, url: http://another.com }

    If url is not given a random url pointing to localhost will be generated.
    """

    schema = {
        'type': 'array',
        'items': {
            'type': 'object',
            'properties': {
                'title': {'type': 'string'},
                'url': {'type': 'string'}
            },
            'required': ['title']
        }
    }

    def on_task_input(self, task, config):
        entries = []
        for line in config:
            entry = Entry(line)
            # no url specified, add random one (ie. test)
            if not 'url' in entry:
                import string
                import random
                entry['url'] = 'http://localhost/mock/%s' % \
                               ''.join([random.choice(string.letters + string.digits) for x in range(1, 30)])
            entries.append(entry)
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(Mock, 'mock', api_ver=2)

########NEW FILE########
__FILENAME__ = plex
"""Plugin for plex media server (www.plexapp.com)."""
from xml.dom.minidom import parseString
import re
import logging
import os
from os.path import basename
from socket import gethostbyname
from string import find

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import requests

log = logging.getLogger('plex')

class InputPlex(object):
    """
    Uses a plex media server (www.plexapp.com) tv section as an input.

    'section'           Required parameter, numerical (/library/sections/<num>) or section name.
    'selection'         Can be set to different keys:
        - all                   : Default
        - unwatched             :
        - recentlyAdded         :
        - recentlyViewed        :
        - recentlyViewedShows   : Series only.
      'all' and 'recentlyViewedShows' will only produce a list of show names while the other three will produce 
      filename and download url.
    'username'          Myplex (http://my.plexapp.com) username, used to connect to shared PMS'.
    'password'          Myplex (http://my.plexapp.com) password, used to connect to shared PMS'. 
    'server'            Host/IP of PMS to connect to. 
    'lowercase_title'   Convert filename (title) to lower case.
    'strip_year'        Remove year from title, ex: Show Name (2012) 01x01 => Show Name 01x01.
                        Movies will have year added to their filename unless this is set.
    'original_filename' Use filename stored in PMS instead of transformed name. lowercase_title and strip_year
                        will be ignored.
    'unwatched_only'    Request only unwatched media from PMS.
    'fetch'             What to download, can be set to the following values:
        - file          The file itself, default.
        - art           Series or movie art as configured in PMS
        - cover         Series cover for series, movie cover for movies.
        - thumb         Episode thumbnail, series only.
        - season_cover  Season cover, series only. If used in movies, movie cover will be set.


    Default paramaters:
      server           : localhost
      port             : 32400
      selection        : all
      lowercase_title  : no
      strip_year       : yes
      original_filename: no
      unwatched_only   : no
      fetch            : file

    Example:

      plex:
        server: 192.168.1.23
        section: 3
        selection: recentlyAdded
        fetch: series_art
    """


    def validator(self):
        from flexget import validator
        config = validator.factory('dict')
        config.accept('text', key='server')
        config.accept('text', key='selection')
        config.accept('integer', key='port')
        config.accept('text', key='section', required=True)
        config.accept('integer', key='section', required=True)
        config.accept('text', key='username')
        config.accept('text', key='password')
        config.accept('boolean', key='lowercase_title')
        config.accept('boolean', key='strip_year')
        config.accept('boolean', key='original_filename')
        config.accept('boolean', key='unwatched_only')
        config.accept('text', key='fetch')
        return config

    def prepare_config(self, config):
        config.setdefault('server', '127.0.0.1')
        config.setdefault('port', 32400)
        config.setdefault('selection', 'all')
        config.setdefault('username', '')
        config.setdefault('password', '')
        config.setdefault('lowercase_title', False)
        config.setdefault('strip_year', True)
        config.setdefault('original_filename', False)
        config.setdefault('unwatched_only', False)
        config.setdefault('fetch', 'file')
        config['plexserver'] = config['server']
        config = self.plex_format_server(config)
        return config

    def plex_get_globalaccesstoken(self, config):
        header = {'X-Plex-Client-Identifier': 'flexget'}
        try:
            r = requests.post('https://my.plexapp.com/users/sign_in.xml',
                              auth=(config['username'], config['password']), headers=header)
        except requests.RequestException as error:
            raise plugin.PluginError('Could not log in to myplex! Error: %s' % error)
        if 'Ivalid email' in r.text:
            raise plugin.PluginError('Myplex: invalid username and/or password!')
        dom = parseString(r.text)
        globalaccesstoken = dom.getElementsByTagName('authentication-token')[0].firstChild.nodeValue
        if not globalaccesstoken:
            raise plugin.PluginError('Myplex: could not find a server!')
        else:
            log.debug('Myplex: Got global accesstoken: %s' % globalaccesstoken)
        return globalaccesstoken

    def plex_get_accesstoken(self, config, globalaccesstoken = ""):
        if not globalaccesstoken:
            globalaccesstoken = self.plex_get_globalaccesstoken(config)
        try:
            r = requests.get("https://my.plexapp.com/pms/servers?X-Plex-Token=%s" % globalaccesstoken)
        except requests.RequestException as e:
            raise plugin.PluginError("Could not get servers from my.plexapp.com using "
                                     "authentication-token: %s. (%s)" % (globalaccesstoken, e))
        dom = parseString(r.text)
        for node in dom.getElementsByTagName('Server'):
            if node.getAttribute('address') == config['server']:
                accesstoken = node.getAttribute('accessToken')
                log.debug("Got plextoken: %s" % accesstoken)
        if not accesstoken:
            raise plugin.PluginError('Could not retrieve accesstoken for %s.' % config['server'])
        else:
            return accesstoken

    def plex_format_server(self, config):
        if gethostbyname(config['server']) != config['server']:
            config['server'] = gethostbyname(config['server'])
        return config

    def plex_section_is_int(self, section):
        return isinstance(section, int)

    def on_task_input(self, task, config):
        config = self.prepare_config(config)
        accesstoken = ""
        urlconfig = {}
        urlappend = "?"
        entries = []
        data = {}
        if config['unwatched_only'] and config['section'] != 'recentlyViewedShows' and config['section'] != 'all':
            urlconfig['unwatched'] = '1'
        if config['username'] and config['password'] and config['server'] != '127.0.0.1':
            accesstoken = self.plex_get_accesstoken(config)
            log.debug("Got accesstoken: %s" % accesstoken)
            urlconfig['X-Plex-Token'] = accesstoken

        for key in urlconfig:
            urlappend += '%s=%s&' % (key, urlconfig[key])
        if not self.plex_section_is_int(config['section']):
            try:
                path = "/library/sections/"
                r = requests.get("http://%s:%d%s%s" %(config['plexserver'], config['port'], path, urlappend))
            except requests.RequestException as e:
                raise plugin.PluginError('Error retrieving source: %s' % e)
            dom = parseString(r.text.encode("utf-8"))
            for node in dom.getElementsByTagName('Directory'):
                if node.getAttribute('title') == config['section']:
                    config['section'] = int(node.getAttribute('key'))
        if not self.plex_section_is_int(config['section']):
            raise plugin.PluginError('Could not find section \'%s\'' % config['section'])

        log.debug("Fetching http://%s:%d/library/sections/%s/%s%s" %
                  (config['server'], config['port'], config['section'], config['selection'], urlappend))
        try:
            path = "/library/sections/%s/%s" % (config['section'], config['selection'])
            r = requests.get("http://%s:%d%s%s" %(config['plexserver'], config['port'], path, urlappend))
        except requests.RequestException as e:
            raise plugin.PluginError('There is no section with number %d. (%s)' % (config['section'], e) )
        dom = parseString(r.text.encode("utf-8"))
        plexsectionname = dom.getElementsByTagName('MediaContainer')[0].getAttribute('title1')
        viewgroup = dom.getElementsByTagName('MediaContainer')[0].getAttribute('viewGroup')


        log.debug("Plex section \"%s\" is a \"%s\" section" % (plexsectionname, viewgroup))
        if (viewgroup != "movie" and viewgroup != "show" and viewgroup != "episode"):
            raise plugin.PluginError("Section is neither a movie nor tv show section!")
        domroot = "Directory"
        titletag = "title"
        if viewgroup == "episode":
            domroot = "Video"
            titletag = "grandparentTitle"
            thumbtag = "thumb"
            arttag = "art"
            seasoncovertag = "parentThumb"
            covertag = "grandparentThumb"
        elif viewgroup == "movie":
            domroot = "Video"
            titletag = "title"
            arttag = "art"
            seasoncovertag = "thumb"
            covertag = "thumb"
            if config['fetch'] == "thumb":
                raise plugin.PluginError("Movie sections does not have any thumbnails to download!")
        for node in dom.getElementsByTagName(domroot):
            e = Entry()
            e['plex_server'] = config['plexserver']
            e['plex_port'] = config['port']
            e['plex_section'] = config['section']
            e['plex_section_name'] = plexsectionname
            e['plex_episode_thumb'] = ''

            title = node.getAttribute(titletag)
            if config['strip_year']:
                title = re.sub(r'^(.*)\(\d{4}\)(.*)', r'\1\2', title)
            title = re.sub(r'[\(\)]', r'', title)
            title = re.sub(r'&', r'And', title)
            title = re.sub(r'[^A-Za-z0-9- ]', r'', title)
            if config['lowercase_title']:
                title = title.lower()
            if viewgroup == "show":
                e['title'] = title
                e['url'] = 'NULL'
                entries.append(e)
#               show ends here.
                continue
            e['plex_art'] = "http://%s:%d%s%s" % (config['server'], config['port'],
                                                  node.getAttribute(arttag), urlappend)
            e['plex_cover'] = "http://%s:%d%s%s" % (config['server'], config['port'],
                                                    node.getAttribute(covertag), urlappend)
            e['plex_season_cover'] = "http://%s:%d%s%s" % (config['server'], config['port'],
                                                           node.getAttribute(seasoncovertag), urlappend)
            if viewgroup == "episode":
                e['plex_thumb'] = "http://%s:%d%s%s" % (config['server'], config['port'],                                                        node.getAttribute('thumb'), urlappend)
                season = int(node.getAttribute('parentIndex'))
                if node.getAttribute('parentIndex') == node.getAttribute('year'):
                    season = node.getAttribute('originallyAvailableAt')
                    filenamemap = "%s_%s%s_%s_%s_%s.%s"
                    episode = ""
                elif node.getAttribute('index'):
                    episode = int(node.getAttribute('index'))
                    filenamemap = "%s_%02dx%02d_%s_%s_%s.%s"
                else:
                    log.debug("Could not get episode number for '%s' (Hint, ratingKey: %s)"
                              % (title, node.getAttribute('ratingKey')))
                    break
            elif viewgroup == "movie":
                filenamemap = "%s_%s_%s_%s.%s"

            e['plex_duration'] = node.getAttribute('duration')
            year = node.getAttribute('year')
            e['plex_summary'] = node.getAttribute('summary')
            count = node.getAttribute('viewCount')
            offset = node.getAttribute('viewOffset')
            if count:
                e['plex_status'] = "seen"
            elif offset:
                e['plex_status'] = "inprogress"
            else:
                e['plex_status'] = "unwatched"
            for media in node.getElementsByTagName('Media'):
                    vcodec = media.getAttribute('videoCodec')
                    acodec = media.getAttribute('audioCodec')
                    if config['fetch'] == "file" or not config['fetch']:
                        container = media.getAttribute('container')
                    else:
                        container = "jpg"
                    resolution = media.getAttribute('videoResolution') + "p"
                    for part in media.getElementsByTagName('Part'):
                        if config['fetch'] == "file" or not config['fetch']:
                            key = part.getAttribute('key')
                        elif config['fetch'] == "art":
                            key = node.getAttribute(arttag)
                        elif config['fetch'] == "cover":
                            key = node.getAttribute(arttag)
                        elif config['fetch'] == "season_cover":
                            key = node.getAttribute(seasoncovertag)
                        elif config['fetch'] == "thumb":
                            key = node.getAttribute(thumbtag)
#                        key = part.getAttribute('key')
                        duration = part.getAttribute('duration')
                        if viewgroup == "show":
                            e['plex_title'] = episodetitle
                        elif viewgroup == "movie":
                            e['plex_title'] = title
                        if config['original_filename']:
                            filename, fileext = os.path.splitext(basename(part.getAttribute('file')))
                            if config['fetch'] != 'file':
                                filename += ".jpg"
                            else:
                                filename = "%s.%s" % (filename, fileext)
                        else:
                            if viewgroup == "episode":
                                filename = filenamemap % (title.replace(" ", "."), season, episode, resolution, vcodec,
                                                          acodec, container)
                                title = filename
                            elif viewgroup == "movie":
                                filename = filenamemap % (title.replace(" ", "."), resolution, vcodec,
                                                          acodec, container)
                        e['plex_url'] = "http://%s:%d%s%s" % (config['server'], config['port'], key, urlappend)
                        e['plex_path'] = key
                        e['url'] = "http://%s:%d%s%s" % (config['server'], config['port'], key, urlappend)
                        e['plex_duration'] = duration
                        e['filename'] = filename
                        e['title'] = title
            if key == "":
                log.debug("Could not find anything in PMS to download. Next!")
            else:
               entries.append(e)
        return entries
@event('plugin.register')
def register_plugin():
    plugin.register(InputPlex, 'plex', api_ver=2)

########NEW FILE########
__FILENAME__ = pogcal
from __future__ import unicode_literals, division, absolute_import
import logging

from bs4 import BeautifulSoup

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import requests

log = logging.getLogger('pogcal')


class InputPogDesign(object):

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'}
        },
        'required': ['username', 'password'],
        'additionalProperties': False
    }

    name_map = {'The Tonight Show [Leno]': 'The Tonight Show With Jay Leno',
                'Late Show [Letterman]': 'David Letterman'}

    def on_task_input(self, task, config):
        session = requests.Session()
        data = {'username': config['username'], 'password': config['password'], 'sub_login': 'Account Login'}
        try:
            r = session.post('http://www.pogdesign.co.uk/cat/', data=data)
            if 'U / P Invalid' in r.text:
                raise plugin.PluginError('Invalid username/password for pogdesign.')
            page = session.get('http://www.pogdesign.co.uk/cat/showselect.php')
        except requests.RequestException as e:
            raise plugin.PluginError('Error retrieving source: %s' % e)
        soup = BeautifulSoup(page.text)
        entries = []
        for row in soup.find_all('label', {'class': 'label_check'}):
            if row.find(attrs={'checked': 'checked'}):
                t = row.find('strong').text
                if t.endswith('[The]'):
                    t = 'The ' + t[:-6]

                # Make certain names friendlier
                if t in self.name_map:
                    t = self.name_map[t]

                e = Entry()
                e['title'] = t
                url = row.find_next('a', {'class': 'slink'})
                e['url'] = 'http://www.pogdesign.co.uk' + url['href']
                entries.append(e)
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(InputPogDesign, 'pogcal', api_ver=2)

########NEW FILE########
__FILENAME__ = regexp_parse
from __future__ import unicode_literals, division, absolute_import
import codecs
import re
import logging
import os

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached

log = logging.getLogger('regexp_parse')


class RegexpParse(object):
    """This plugin is designed to take input from a web resource or a file.
    It then parses the text via regexps supplied in the config file.

    source: is a file or url to get the data from. You can specify a username:password

    sections: Takes a list of dicts that contain regexps to split the data up into sections.
    The regexps listed here are used by find all so every matching string in the data will be
    a valid section.

    keys: hold the keys that will be set in the entries

    key:
      regexps: a list of dicts that hold regexps. The key is set to the first string that matches
      any of the regexps listed. The regexps are evaluated in the order they are supplied so if a
      string matches the first regexp none of the others in the list will be used.

      required: a boolean that when set to true will only allow entries that contain this key
      onto the next stage. url and title are always required no matter what you do (part of flexget)

      #TODO: consider adding a set field that will allow you to set the field if no regexps match

      #TODO: consider a mode field that allows a growing list for a field instead of just setting to
            # first match

    Example config

    regexp_parse:
      source: http://username:password@ezrss.it/feed/
      sections:
        - {regexp: "(?<=<item>).*?(?=</item>)", flags: "DOTALL,IGNORECASE"}

      keys:
        title:
          regexps:
            - {regexp: '(?<=<title><!\[CDATA\[).*?(?=\]\]></title>)'} #comment
        url:
          regexps:
            - {regexp: "magnet:.*?(?=])"}
        custom_field:
          regexps:
            - {regexp: "custom regexps", flags: "comma seperated list of flags (see python regex docs)"}
          required: False
        custom_field2:
          regexps:
            - {regexp: 'first custom regexps'}
            - {regexp: 'can't find first regexp so try this one'}
    """

    #dict used to convert string values of regexp flags to int
    FLAG_VALUES = {'DEBUG': re.DEBUG,
                   'I': re.I,
                   'IGNORECASE': re.IGNORECASE,
                   'L': re.L,
                   'LOCALE': re.LOCALE,
                   'M': re.M,
                   'MULTILINE': re.MULTILINE,
                   'S': re.S,
                   'DOTALL': re.DOTALL,
                   'U': re.U,
                   'UNICODE': re.UNICODE,
                   'X': re.X,
                   'VERBOSE': re.VERBOSE
                   }

    def __init__(self):
        self.required = []

    def validator(self):
        from flexget import validator
        root = validator.factory('dict')

        root.accept('url', key='source', required=True)
        root.accept('file', key='source', required=True)

        #sections to divied source into
        sections_regexp_lists = root.accept('list', key='sections')
        section_regexp_list = sections_regexp_lists.accept('dict', required=True)
        section_regexp_list.accept('regexp', key='regexp', required=True)
        section_regexp_list.accept('text', key='flags')

        keys = root.accept('dict', key='keys', required=True)

        #required key need to specify for validator
        title = keys.accept('dict', key='title', required=True)
        title.accept('boolean', key='required')
        regexp_list = title.accept('list', key='regexps', required=True)
        regexp = regexp_list.accept('dict', required=True)
        regexp.accept('regexp', key='regexp', required=True)
        regexp.accept('text', key='flags')

        #required key need to specify for validator
        url = keys.accept_any_key('dict', key='url', required=True)
        url.accept('boolean', key='required')
        regexp_list = url.accept('list', key='regexps', required=True)
        regexp = regexp_list.accept('dict', required=True)
        regexp.accept('regexp', key='regexp', required=True)
        regexp.accept('text', key='flags')

        #accept any other key the user wants to use
        key = keys.accept_any_key('dict')
        key.accept('boolean', key='required')
        regexp_list = key.accept('list', key='regexps', required=True)
        regexp = regexp_list.accept('dict', required=True)
        regexp.accept('regexp', key='regexp', required=True)
        regexp.accept('text', key='flags')

        return root

    def flagstr_to_flags(self, flag_str):
        """turns a comma seperated list of flags into the int value."""
        COMBIND_FLAGS = 0
        split_flags = flag_str.split(',')
        for flag in split_flags:
            COMBIND_FLAGS = COMBIND_FLAGS | RegexpParse.FLAG_VALUES[flag.strip()]
        return COMBIND_FLAGS

    def compile_regexp_dict_list(self, re_list):
        """turns a list of dicts containing regexps information into a list of compiled regexps."""
        compiled_regexps = []
        for dic in re_list:
            flags = 0
            if 'flags' in dic:
                flags = self.flagstr_to_flags(dic['flags'])
            compiled_regexps.append(re.compile(dic['regexp'], flags))
        return compiled_regexps

    def isvalid(self, entry):
        """checks to make sure that all required fields are present in the entry."""
        for key in self.required:
            if key not in entry:
                return False
        return entry.isvalid()

    @cached('regexp_parse')
    @plugin.internet(log)
    def on_task_input(self, task, config):
        url = config['source']

        #if it's a file open it and read into content (assume utf-8 encoding)
        if os.path.isfile(os.path.expanduser(url)):
            content = codecs.open(url, 'r', encoding='utf-8').read()
        #else use requests to get the data
        else:
            content = task.requests.get(url).text

        sections = []
        seperators = config.get('sections')
        if seperators:
            for sep in seperators:
                flags = 0
                if 'flags' in sep:
                    flags = self.flagstr_to_flags(sep['flags'])
                sections.extend(re.findall(sep['regexp'], content, flags))

        #no seperators just do work on the whole content
        else:
            sections.append(content)

        #holds all the regex in a dict for the field they are trying to fill
        key_to_regexps = {}

        #put every key in keys into the rey_to_regexps list
        for key, value in config['keys'].iteritems():
            key_to_regexps[key] = self.compile_regexp_dict_list(value['regexps'])
            if 'required' in value and value['required']:
                self.required.append(key)

        entries = []
        for section in sections:
            entry = Entry()
            for key, regexps in key_to_regexps.iteritems():
                for regexp in regexps:
                    m = regexp.search(section)
                    if m:
                        entry[key] = m.group(0)
                        break
            if self.isvalid(entry):
                entries.append(entry)

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(RegexpParse, 'regexp_parse', api_ver=2)

########NEW FILE########
__FILENAME__ = rlslog
from __future__ import unicode_literals, division, absolute_import
import logging
import re
import time

from requests import RequestException

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.imdb import extract_id
from flexget.utils.log import log_once
from flexget.utils.soup import get_soup
from flexget.utils.cached_input import cached

log = logging.getLogger('rlslog')


class RlsLog(object):
    """
    Adds support for rlslog.net as a feed.
    """

    def validator(self):
        from flexget import validator
        return validator.factory('url')

    def parse_rlslog(self, rlslog_url, task):
        """
        :param rlslog_url: Url to parse from
        :param task: Task instance
        :return: List of release dictionaries
        """

        # BeautifulSoup doesn't seem to work if data is already decoded to unicode :/
        soup = get_soup(task.requests.get(rlslog_url, timeout=25).content)

        releases = []
        for entry in soup.find_all('div', attrs={'class': 'entry'}):
            release = {}
            h3 = entry.find('h3', attrs={'class': 'entrytitle'})
            if not h3:
                log.debug('FAIL: No h3 entrytitle')
                continue
            release['title'] = h3.a.contents[0].strip()
            entrybody = entry.find('div', attrs={'class': 'entrybody'})
            if not entrybody:
                log.debug('FAIL: No entrybody')
                continue

            log.trace('Processing title %s' % (release['title']))

            # find imdb url
            link_imdb = entrybody.find('a', text=re.compile(r'imdb', re.IGNORECASE))
            if link_imdb:
                release['imdb_id'] = extract_id(link_imdb['href'])
                release['imdb_url'] = link_imdb['href']

            # find google search url
            google = entrybody.find('a', href=re.compile(r'google', re.IGNORECASE))
            if google:
                release['url'] = google['href']
                releases.append(release)
            else:
                log_once('%s skipped due to missing or unsupported download link' % (release['title']), log)

        return releases

    @cached('rlslog')
    @plugin.internet(log)
    def on_task_input(self, task, config):
        url = config
        if url.endswith('feed/'):
            raise plugin.PluginError('Invalid URL. Remove trailing feed/ from the url.')

        releases = []
        entries = []

        # retry rlslog (badly responding) up to 4 times (requests tries 2 times per each of our tries here)
        for number in range(2):
            try:
                releases = self.parse_rlslog(url, task)
                break
            except RequestException as e:
                if number == 1:
                    raise
                else:
                    log.verbose('Error receiving content, retrying in 5s. Try [%s of 2]. Error: %s' % (number + 1, e))
                    time.sleep(5)

        # Construct entry from release
        for release in releases:
            entry = Entry()

            def apply_field(d_from, d_to, f):
                if f in d_from:
                    if d_from[f] is None:
                        return  # None values are not wanted!
                    d_to[f] = d_from[f]

            for field in ('title', 'url', 'imdb_url'):
                apply_field(release, entry, field)

            entries.append(entry)

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(RlsLog, 'rlslog', api_ver=2)

########NEW FILE########
__FILENAME__ = rottentomatoes_list
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached

try:
        from flexget.plugins.api_rottentomatoes import lists
except ImportError:
        raise plugin.DependencyError(issued_by='rottentomatoes_lookup', missing='api_rottentomatoes',
                                     message='rottentomatoes_lookup requires the `api_rottentomatoes` plugin')

log = logging.getLogger('rottentomatoes_list')


class RottenTomatoesList(object):
    """
        Emits an entry for each movie in a Rotten Tomatoes list.

        Configuration:

        dvds:
          - top_rentals
          - upcoming

        movies:
          - box_office

        Possible lists are
          * dvds: top_rentals, current_releases, new_releases, upcoming
          * movies: box_office, in_theaters, opening, upcoming

    """

    def __init__(self):
        # We could pull these from the API through lists.json but that's extra web/API key usage
        self.dvd_lists = ['top_rentals', 'current_releases', 'new_releases', 'upcoming']
        self.movie_lists = ['box_office', 'in_theaters', 'opening', 'upcoming']

    def validator(self):
        from flexget import validator
        root = validator.factory('dict')
        root.accept('list', key='dvds').accept('choice').accept_choices(self.dvd_lists)
        root.accept('list', key='movies').accept('choice').accept_choices(self.movie_lists)
        return root

    @cached('rottentomatoes_list', persist='2 hours')
    def on_task_input(self, task, config):
        entries = []
        for l_type, l_names in config.items():
            for l_name in l_names:
                results = lists(list_type=l_type, list_name=l_name)
                if results:
                    for movie in results['movies']:
                        if [entry for entry in entries if movie['title'] == entry.get('title')]:
                            continue
                        imdb_id = movie.get('alternate_ids', {}).get('imdb')
                        if imdb_id:
                            imdb_id = 'tt' + str(imdb_id)
                        entries.append(Entry(title=movie['title'], rt_id=movie['id'],
                            imdb_id=imdb_id,
                            rt_name=movie['title'],
                            url=movie['links']['alternate']))
                else:
                    log.critical('Failed to fetch Rotten tomatoes %s list: %s. List doesn\'t exist?' %
                            (l_type, l_name))
        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(RottenTomatoesList, 'rottentomatoes_list', api_ver=2)

########NEW FILE########
__FILENAME__ = rss
from __future__ import unicode_literals, division, absolute_import
import os
import logging
import urlparse
import xml.sax
import posixpath
import httplib
from datetime import datetime

import feedparser
from requests import RequestException

from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached
from flexget.utils.tools import decode_html
from flexget.utils.pathscrub import pathscrub

log = logging.getLogger('rss')


class InputRSS(object):
    """
    Parses RSS feed.

    Hazzlefree configuration for public rss feeds::

      rss: <url>

    Configuration with basic http authentication::

      rss:
        url: <url>
        username: <name>
        password: <password>

    Advanced usages:

    You may wish to clean up the entry by stripping out all non-ascii characters.
    This can be done by setting ascii value to yes.

    Example::

      rss:
        url: <url>
        ascii: yes

    In case RSS-feed uses some nonstandard field for urls and automatic detection fails
    you can configure plugin to use url from any feedparser entry attribute.

    Example::

      rss:
        url: <url>
        link: guid

    If you want to keep information in another rss field attached to the flexget entry,
    you can use the other_fields option.

    Example::

      rss:
        url: <url>
        other_fields: [date]

    You can disable few possibly annoying warnings by setting silent value to
    yes on feeds where there are frequently invalid items.

    Example::

      rss:
        url: <url>
        silent: yes

    You can group all the links of an item, to make the download plugin tolerant
    to broken urls: it will try to download each url until one works.
    Links are enclosures plus item fields given by the link value, in that order.
    The value to set is "group_links".

    Example::

      rss:
        url: <url>
        group_links: yes
    """

    schema = {
        'type': ['string', 'object'],
        # Simple form, just url or file
        'anyOf': [{'format': 'url'}, {'format': 'file'}],
        # Advanced form, with options
        'properties': {
            'url': {'type': 'string', 'anyOf': [{'format': 'url'}, {'format': 'file'}]},
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'title': {'type': 'string'},
            'link': one_or_more({'type': 'string'}),
            'silent': {'type': 'boolean', 'default': False},
            'ascii': {'type': 'boolean', 'default': False},
            'filename': {'type': 'boolean'},
            'group_links': {'type': 'boolean', 'default': False},
            'all_entries': {'type': 'boolean', 'default': True},
            'other_fields': {'type': 'array', 'items': {
                # Items can be a string, or a dict with a string value
                'type': ['string', 'object'], 'additionalProperties': {'type': 'string'}
            }}
        },
        'required': ['url'],
        'additionalProperties': False
    }

    def build_config(self, config):
        """Set default values to config"""
        if isinstance(config, basestring):
            config = {'url': config}
        # set the default link value to 'auto'
        config.setdefault('link', 'auto')
        # Replace : with _ and lower case other fields so they can be found in rss
        if config.get('other_fields'):
            other_fields = []
            for item in config['other_fields']:
                if isinstance(item, basestring):
                    key, val = item, item
                else:
                    key, val = item.items()[0]
                other_fields.append({key.replace(':', '_').lower(): val.lower()})
            config['other_fields'] = other_fields
        # set default value for group_links as deactivated
        config.setdefault('group_links', False)
        # set default for all_entries
        config.setdefault('all_entries', True)
        return config

    def process_invalid_content(self, task, data, url):
        """If feedparser reports error, save the received data and log error."""

        if data is None:
            log.critical('Received empty page - no content')
            return
        ext = 'xml'
        if '<html>' in data.lower():
            log.critical('Received content is HTML page, not an RSS feed')
            ext = 'html'
        if 'login' in data.lower() or 'username' in data.lower():
            log.critical('Received content looks a bit like login page')
        if 'error' in data.lower():
            log.critical('Received content looks a bit like error page')
        received = os.path.join(task.manager.config_base, 'received')
        if not os.path.isdir(received):
            os.mkdir(received)
        filename = task.name
        sourcename = urlparse.urlparse(url).netloc
        if sourcename:
            filename += '-' + sourcename
        filename = pathscrub(filename, filename=True)
        filepath = os.path.join(received, '%s.%s' % (filename, ext))
        with open(filepath, 'w') as f:
            f.write(data)
        log.critical('I have saved the invalid content to %s for you to view', filepath)

    def add_enclosure_info(self, entry, enclosure, filename=True, multiple=False):
        """Stores information from an rss enclosure into an Entry."""
        entry['url'] = enclosure['href']
        # get optional meta-data
        if 'length' in enclosure:
            try:
                entry['size'] = int(enclosure['length'])
            except:
                entry['size'] = 0
        if 'type' in enclosure:
            entry['type'] = enclosure['type']
        # TODO: better and perhaps join/in download plugin?
        # Parse filename from enclosure url
        basename = posixpath.basename(urlparse.urlsplit(entry['url']).path)
        # If enclosure has size OR there are multiple enclosures use filename from url
        if (entry.get('size') or multiple and basename) and filename:
            entry['filename'] = basename
            log.trace('filename `%s` from enclosure', entry['filename'])

    @cached('rss')
    @plugin.internet(log)
    def on_task_input(self, task, config):
        config = self.build_config(config)

        log.debug('Requesting task `%s` url `%s`', task.name, config['url'])

        # Used to identify which etag/modified to use
        url_hash = str(hash(config['url']))

        # set etag and last modified headers if config has not changed since
        # last run and if caching wasn't disabled with --no-cache argument.
        all_entries = (config['all_entries'] or task.config_modified or
                       task.options.nocache or task.options.retry)
        headers = {}
        if not all_entries:
            etag = task.simple_persistence.get('%s_etag' % url_hash, None)
            if etag:
                log.debug('Sending etag %s for task %s', etag, task.name)
                headers['If-None-Match'] = etag
            modified = task.simple_persistence.get('%s_modified' % url_hash, None)
            if modified:
                if not isinstance(modified, basestring):
                    log.debug('Invalid date was stored for last modified time.')
                else:
                    headers['If-Modified-Since'] = modified
                    log.debug('Sending last-modified %s for task %s', headers['If-Modified-Since'], task.name)

        # Get the feed content
        if config['url'].startswith(('http', 'https', 'ftp', 'file')):
            # Get feed using requests library
            auth = None
            if 'username' in config and 'password' in config:
                auth = (config['username'], config['password'])
            try:
                # Use the raw response so feedparser can read the headers and status values
                response = task.requests.get(config['url'], timeout=60, headers=headers, raise_status=False, auth=auth)
                content = response.content
            except RequestException as e:
                raise plugin.PluginError('Unable to download the RSS for task %s (%s): %s' %
                                  (task.name, config['url'], e))
            if config.get('ascii'):
                # convert content to ascii (cleanup), can also help with parsing problems on malformed feeds
                content = response.text.encode('ascii', 'ignore')

            # status checks
            status = response.status_code
            if status == 304:
                log.verbose('%s hasn\'t changed since last run. Not creating entries.', config['url'])
                # Let details plugin know that it is ok if this feed doesn't produce any entries
                task.no_entries_ok = True
                return []
            elif status == 401:
                raise plugin.PluginError('Authentication needed for task %s (%s): %s' %
                                         (task.name, config['url'], response.headers['www-authenticate']), log)
            elif status == 404:
                raise plugin.PluginError('RSS Feed %s (%s) not found' % (task.name, config['url']), log)
            elif status == 500:
                raise plugin.PluginError('Internal server exception on task %s (%s)' % (task.name, config['url']), log)
            elif status != 200:
                raise plugin.PluginError('HTTP error %s received from %s' % (status, config['url']), log)

            # update etag and last modified
            if not config['all_entries']:
                etag = response.headers.get('etag')
                if etag:
                    task.simple_persistence['%s_etag' % url_hash] = etag
                    log.debug('etag %s saved for task %s', etag, task.name)
                if response.headers.get('last-modified'):
                    modified = response.headers['last-modified']
                    task.simple_persistence['%s_modified' % url_hash] = modified
                    log.debug('last modified %s saved for task %s', modified, task.name)
        else:
            # This is a file, open it
            with open(config['url'], 'rb') as f:
                content = f.read()
            if config.get('ascii'):
                # Just assuming utf-8 file in this case
                content = content.decode('utf-8', 'ignore').encode('ascii', 'ignore')

        if not content:
            log.error('No data recieved for rss feed.')
            return
        try:
            rss = feedparser.parse(content)
        except LookupError as e:
            raise plugin.PluginError('Unable to parse the RSS (from %s): %s' % (config['url'], e))

        # check for bozo
        ex = rss.get('bozo_exception', False)
        if ex or rss.get('bozo'):
            if rss.entries:
                msg = 'Bozo error %s while parsing feed, but entries were produced, ignoring the error.' % type(ex)
                if config.get('silent', False):
                    log.debug(msg)
                else:
                    log.verbose(msg)
            else:
                if isinstance(ex, feedparser.NonXMLContentType):
                    # see: http://www.feedparser.org/docs/character-encoding.html#advanced.encoding.nonxml
                    log.debug('ignoring feedparser.NonXMLContentType')
                elif isinstance(ex, feedparser.CharacterEncodingOverride):
                    # see: ticket 88
                    log.debug('ignoring feedparser.CharacterEncodingOverride')
                elif isinstance(ex, UnicodeEncodeError):
                    raise plugin.PluginError('Feed has UnicodeEncodeError while parsing...')
                elif isinstance(ex, (xml.sax._exceptions.SAXParseException, xml.sax._exceptions.SAXException)):
                    # save invalid data for review, this is a bit ugly but users seem to really confused when
                    # html pages (login pages) are received
                    self.process_invalid_content(task, content, config['url'])
                    if task.options.debug:
                        log.exception(ex)
                    raise plugin.PluginError('Received invalid RSS content from task %s (%s)' % (task.name, config['url']))
                elif isinstance(ex, httplib.BadStatusLine) or isinstance(ex, IOError):
                    raise ex  # let the @internet decorator handle
                else:
                    # all other bozo errors
                    self.process_invalid_content(task, content, config['url'])
                    raise plugin.PluginError('Unhandled bozo_exception. Type: %s (task: %s)' %
                                      (ex.__class__.__name__, task.name), log)

        log.debug('encoding %s', rss.encoding)

        last_entry_id = ''
        if not all_entries:
            # Test to make sure entries are in descending order
            if rss.entries and rss.entries[0].get('published_parsed') and rss.entries[-1].get('published_parsed'):
                if rss.entries[0]['published_parsed'] < rss.entries[-1]['published_parsed']:
                    # Sort them if they are not
                    rss.entries.sort(key=lambda x: x['published_parsed'], reverse=True)
            last_entry_id = task.simple_persistence.get('%s_last_entry' % url_hash)

        # new entries to be created
        entries = []

        # field name for url can be configured by setting link.
        # default value is auto but for example guid is used in some feeds
        ignored = 0
        for entry in rss.entries:

            # Check if title field is overridden in config
            title_field = config.get('title', 'title')
            # ignore entries without title
            if not entry.get(title_field):
                log.debug('skipping entry without title')
                ignored += 1
                continue

            # Set the title from the source field
            entry.title = entry[title_field]

            # Check we haven't already processed this entry in a previous run
            if last_entry_id == entry.title + entry.get('guid', ''):
                log.verbose('Not processing entries from last run.')
                # Let details plugin know that it is ok if this task doesn't produce any entries
                task.no_entries_ok = True
                break

            # remove annoying zero width spaces
            entry.title = entry.title.replace(u'\u200B', u'')

            # Dict with fields to grab mapping from rss field name to FlexGet field name
            fields = {'guid': 'guid',
                      'author': 'author',
                      'description': 'description',
                      'infohash': 'torrent_info_hash'}
            # extend the dict of fields to grab with other_fields list in config
            for field_map in config.get('other_fields', []):
                fields.update(field_map)

            # helper
            # TODO: confusing? refactor into class member ...

            def add_entry(ea):
                ea['title'] = entry.title

                for rss_field, flexget_field in fields.iteritems():
                    if rss_field in entry:
                        if not isinstance(getattr(entry, rss_field), basestring):
                            # Error if this field is not a string
                            log.error('Cannot grab non text field `%s` from rss.', rss_field)
                            # Remove field from list of fields to avoid repeated error
                            config['other_fields'].remove(rss_field)
                            continue
                        if not getattr(entry, rss_field):
                            log.debug('Not grabbing blank field %s from rss for %s.', rss_field, ea['title'])
                            continue
                        try:
                            ea[flexget_field] = decode_html(entry[rss_field])
                            if rss_field in config.get('other_fields', []):
                                # Print a debug message for custom added fields
                                log.debug('Field `%s` set to `%s` for `%s`', rss_field, ea[rss_field], ea['title'])
                        except UnicodeDecodeError:
                            log.warning('Failed to decode entry `%s` field `%s`', ea['title'], rss_field)
                # Also grab pubdate if available
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    ea['rss_pubdate'] = datetime(*entry.published_parsed[:6])
                # store basic auth info
                if 'username' in config and 'password' in config:
                    ea['download_auth'] = (config['username'], config['password'])
                entries.append(ea)

            # create from enclosures if present
            enclosures = entry.get('enclosures', [])

            if len(enclosures) > 1 and not config.get('group_links'):
                # There is more than 1 enclosure, create an Entry for each of them
                log.debug('adding %i entries from enclosures', len(enclosures))
                for enclosure in enclosures:
                    if not 'href' in enclosure:
                        log.debug('RSS-entry `%s` enclosure does not have URL', entry.title)
                        continue
                    # There is a valid url for this enclosure, create an Entry for it
                    ee = Entry()
                    self.add_enclosure_info(ee, enclosure, config.get('filename', True), True)
                    add_entry(ee)
                # If we created entries for enclosures, we should not create an Entry for the main rss item
                continue

            # create flexget entry
            e = Entry()

            if not isinstance(config.get('link'), list):
                # If the link field is not a list, search for first valid url
                if config['link'] == 'auto':
                    # Auto mode, check for a single enclosure url first
                    if len(entry.get('enclosures', [])) == 1 and entry['enclosures'][0].get('href'):
                        self.add_enclosure_info(e, entry['enclosures'][0], config.get('filename', True))
                    else:
                        # If there is no enclosure url, check link, then guid field for urls
                        for field in ['link', 'guid']:
                            if entry.get(field):
                                e['url'] = entry[field]
                                break
                else:
                    if entry.get(config['link']):
                        e['url'] = entry[config['link']]
            else:
                # If link was passed as a list, we create a list of urls
                for field in config['link']:
                    if entry.get(field):
                        e.setdefault('url', entry[field])
                        if entry[field] not in e.setdefault('urls', []):
                            e['urls'].append(entry[field])

            if config.get('group_links'):
                # Append a list of urls from enclosures to the urls field if group_links is enabled
                e.setdefault('urls', [e['url']]).extend(
                    [enc.href for enc in entry.get('enclosures', []) if enc.get('href') not in e['urls']])

            if not e.get('url'):
                log.debug('%s does not have link (%s) or enclosure', entry.title, config['link'])
                ignored += 1
                continue

            add_entry(e)

        # Save last spot in rss
        if rss.entries:
            log.debug('Saving location in rss feed.')
            task.simple_persistence['%s_last_entry' % url_hash] = rss.entries[0].title + rss.entries[0].get('guid', '')

        if ignored:
            if not config.get('silent'):
                log.warning('Skipped %s RSS-entries without required information (title, link or enclosures)', ignored)

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(InputRSS, 'rss', api_ver=2)

########NEW FILE########
__FILENAME__ = sceper
from __future__ import unicode_literals, division, absolute_import
import logging

from bs4 import NavigableString
from requests import RequestException

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached
from flexget.utils.imdb import extract_id
from flexget.utils.soup import get_soup

log = logging.getLogger('sceper')


class InputSceper(object):
    """
    Uses sceper.ws category url as input.

    Example::

      sceper: http://sceper.ws/category/movies/movies-dvd-rip
    """

    schema = {'type': 'string', 'format': 'url'}

    def parse_site(self, url, task):
        """Parse configured url and return releases array"""

        try:
            page = task.requests.get(url).content
        except RequestException as e:
            raise plugin.PluginError('Error getting input page: %e' % e)
        soup = get_soup(page)

        releases = []
        for entry in soup.find_all('div', attrs={'class': 'entry'}):
            release = {}
            title = entry.find('h2')
            if not title:
                log.debug('No h2 entrytitle')
                continue
            release['title'] = title.a.contents[0].strip()

            log.debug('Processing title %s' % (release['title']))

            for link in entry.find_all('a'):
                # no content in the link
                if not link.contents:
                    continue
                link_name = link.contents[0]
                if link_name is None:
                    continue
                if not isinstance(link_name, NavigableString):
                    continue
                link_name = link_name.strip().lower()
                if link.has_attr('href'):
                    link_href = link['href']
                else:
                    continue
                log.debug('found link %s -> %s' % (link_name, link_href))
                # handle imdb link
                if link_name.lower() == 'imdb':
                    log.debug('found imdb link %s' % link_href)
                    release['imdb_id'] = extract_id(link_href)

                # test if entry with this url would be rewritable by known plugins (ie. downloadable)
                temp = {}
                temp['title'] = release['title']
                temp['url'] = link_href
                urlrewriting = plugin.get_plugin_by_name('urlrewriting')
                if urlrewriting['instance'].url_rewritable(task, temp):
                    release['url'] = link_href
                    log.trace('--> accepting %s (resolvable)' % link_href)
                else:
                    log.trace('<-- ignoring %s (non-resolvable)' % link_href)

            # reject if no torrent link
            if not 'url' in release:
                from flexget.utils.log import log_once
                log_once('%s skipped due to missing or unsupported (unresolvable) download link' % (release['title']), log)
            else:
                releases.append(release)

        return releases

    @cached('sceper')
    @plugin.internet(log)
    def on_task_input(self, task, config):
        releases = self.parse_site(config, task)
        return [Entry(release) for release in releases]


@event('plugin.register')
def register_plugin():
    plugin.register(InputSceper, 'sceper', api_ver=2)

########NEW FILE########
__FILENAME__ = tail
from __future__ import unicode_literals, division, absolute_import
import os
import re
import logging

from flexget import options, plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached

log = logging.getLogger('tail')


class InputTail(object):

    """
    Parse any text for entries using regular expression.

    ::

      file: <file>
      entry:
        <field>: <regexp to match value>
      format:
        <field>: <python string formatting>

    Note: each entry must have atleast two fields, title and url

    You may wish to specify encoding used by file so file can be properly
    decoded. List of encodings
    at http://docs.python.org/library/codecs.html#standard-encodings.

    Example::

      tail:
        file: ~/irclogs/some/log
        entry:
          title: 'TITLE: (.*) URL:'
          url: 'URL: (.*)'
        encoding: utf8
    """

    def validator(self):
        from flexget import validator
        root = validator.factory('dict')
        root.accept('file', key='file', required=True)
        root.accept('text', key='encoding')
        entry = root.accept('dict', key='entry', required=True)
        entry.accept('regexp', key='url', required=True)
        entry.accept('regexp', key='title', required=True)
        entry.accept_any_key('regexp')
        format = root.accept('dict', key='format')
        format.accept_any_key('text')
        return root

    def format_entry(self, entry, d):
        for k, v in d.iteritems():
            entry[k] = v % entry

    @cached('tail')
    def on_task_input(self, task, config):

        # Let details plugin know that it is ok if this task doesn't produce any entries
        task.no_entries_ok = True

        filename = os.path.expanduser(config['file'])
        encoding = config.get('encoding', None)
        with open(filename, 'r') as file:
            last_pos = task.simple_persistence.setdefault(filename, 0)
            if task.options.tail_reset == filename or task.options.tail_reset == task.name:
                if last_pos == 0:
                    log.info('Task %s tail position is already zero' % task.name)
                else:
                    log.info('Task %s tail position (%s) reset to zero' % (task.name, last_pos))
                    last_pos = 0

            if os.path.getsize(filename) < last_pos:
                log.info('File size is smaller than in previous execution, reseting to beginning of the file')
                last_pos = 0

            file.seek(last_pos)

            log.debug('continuing from last position %s' % last_pos)

            entry_config = config.get('entry')
            format_config = config.get('format', {})

            # keep track what fields have been found
            used = {}
            entries = []
            entry = Entry()

            # now parse text

            while True:
                line = file.readline()
                if encoding:
                    try:
                        line = line.decode(encoding)
                    except UnicodeError:
                        raise plugin.PluginError('Failed to decode file using %s. Check encoding.' % encoding)

                if not line:
                    task.simple_persistence[filename] = file.tell()
                    break

                for field, regexp in entry_config.iteritems():
                    #log.debug('search field: %s regexp: %s' % (field, regexp))
                    match = re.search(regexp, line)
                    if match:
                        # check if used field detected, in such case start with new entry
                        if field in used:
                            if entry.isvalid():
                                log.info('Found field %s again before entry was completed. \
                                          Adding current incomplete, but valid entry and moving to next.' % field)
                                self.format_entry(entry, format_config)
                                entries.append(entry)
                            else:
                                log.info('Invalid data, entry field %s is already found once. Ignoring entry.' % field)
                            # start new entry
                            entry = Entry()
                            used = {}

                        # add field to entry
                        entry[field] = match.group(1)
                        used[field] = True
                        log.debug('found field: %s value: %s' % (field, entry[field]))

                    # if all fields have been found
                    if len(used) == len(entry_config):
                        # check that entry has at least title and url
                        if not entry.isvalid():
                            log.info('Invalid data, constructed entry is missing mandatory fields (title or url)')
                        else:
                            self.format_entry(entry, format_config)
                            entries.append(entry)
                            log.debug('Added entry %s' % entry)
                            # start new entry
                            entry = Entry()
                            used = {}
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(InputTail, 'tail', api_ver=2)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--tail-reset', action='store', dest='tail_reset', default=False,
                                               metavar='FILE|TASK', help='reset tail position for a file')

########NEW FILE########
__FILENAME__ = text
"""Plugin for text file or URL feeds via regex."""
from __future__ import unicode_literals, division, absolute_import
import re
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached

log = logging.getLogger('text')


class Text(object):

    """
    Parse any text for entries using regular expression.

    Example::

      url: <url>
      entry:
        <field>: <regexp to match value>
      format:
        <field>: <python string formatting>

    Note: each entry must have atleast two fields, title and url

    Example::

      text:
        url: http://www.nbc.com/Heroes/js/novels.js
        entry:
          title: novelTitle = "(.*)"
          url: novelPrint = "(.*)"
        format:
          url: http://www.nbc.com%(url)s
    """

    def validator(self):
        from flexget import validator
        root = validator.factory('dict')
        root.accept('url', key='url')
        root.accept('file', key='url')
        root.require_key('url')
        entry = root.accept('dict', key='entry', required=True)
        entry.accept('regexp', key='url', required=True)
        entry.accept('regexp', key='title', required=True)
        entry.accept_any_key('regexp')
        format = root.accept('dict', key='format')
        format.accept_any_key('text')
        return root

    def format_entry(self, entry, d):
        for k, v in d.iteritems():
            entry[k] = v % entry

    @cached('text')
    @plugin.internet(log)
    def on_task_input(self, task, config):
        url = config['url']
        if '://' in url:
            lines = task.requests.get(url).iter_lines()
        else:
            lines = open(url, 'rb').readlines()

        entry_config = config.get('entry')
        format_config = config.get('format', {})

        entries = []
        # keep track what fields have been found
        used = {}
        entry = Entry()

        # now parse text
        for line in lines:
            for field, regexp in entry_config.iteritems():
                #log.debug('search field: %s regexp: %s' % (field, regexp))
                match = re.search(regexp, line)
                if match:
                    # check if used field detected, in such case start with new entry
                    if field in used:
                        if entry.isvalid():
                            log.info('Found field %s again before entry was completed. \
                                      Adding current incomplete, but valid entry and moving to next.' % field)
                            self.format_entry(entry, format_config)
                            entries.append(entry)
                        else:
                            log.info('Invalid data, entry field %s is already found once. Ignoring entry.' % field)
                        # start new entry
                        entry = Entry()
                        used = {}

                    # add field to entry
                    try:
                        entry[field] = match.group(1)
                    except IndexError:
                        log.error('regex for field `%s` must contain a capture group' % field)
                        raise plugin.PluginError('Your text plugin config contains errors, please correct them.')
                    used[field] = True
                    log.debug('found field: %s value: %s' % (field, entry[field]))

                # if all fields have been found
                if len(used) == len(entry_config):
                    # check that entry has atleast title and url
                    if not entry.isvalid():
                        log.info('Invalid data, constructed entry is missing mandatory fields (title or url)')
                    else:
                        self.format_entry(entry, format_config)
                        entries.append(entry)
                        log.debug('Added entry %s' % entry)
                        # start new entry
                        entry = Entry()
                        used = {}
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(Text, 'text', api_ver=2)

########NEW FILE########
__FILENAME__ = thetvdb_favorites
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib2
import re
from datetime import datetime, timedelta

from xml.etree import ElementTree
from sqlalchemy import Column, Integer, Unicode, DateTime, String

from flexget import db_schema, plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.cached_input import cached
from flexget.utils.database import pipe_list_synonym, with_session
from flexget.utils.sqlalchemy_utils import drop_tables, table_columns
from flexget.utils.tools import urlopener

try:
    from flexget.plugins.api_tvdb import lookup_series
except ImportError:
    raise plugin.DependencyError(issued_by='thetvdb_favorites', missing='api_tvdb',
                                 message='thetvdb_lookup requires the `api_tvdb` plugin')

log = logging.getLogger('thetvdb_favorites')
Base = db_schema.versioned_base('thetvdb_favorites', 0)


@db_schema.upgrade('thetvdb_favorites')
def upgrade(ver, session):
    if ver is None:
        columns = table_columns('thetvdb_favorites', session)
        if not 'series_ids' in columns:
            # Drop the old table
            log.info('Dropping old version of thetvdb_favorites table from db')
            drop_tables(['thetvdb_favorites'], session)
            # Create new table from the current model
            Base.metadata.create_all(bind=session.bind)
        ver = 0
    return ver


class ThetvdbFavorites(Base):

    __tablename__ = 'thetvdb_favorites'

    id = Column(Integer, primary_key=True)
    account_id = Column(String, index=True)
    _series_ids = Column('series_ids', Unicode)
    series_ids = pipe_list_synonym('_series_ids')
    updated = Column(DateTime)

    def __init__(self, account_id, series_ids):
        self.account_id = account_id
        self.series_ids = series_ids
        self.updated = datetime.now()

    def __repr__(self):
        return '<series_favorites(account_id=%s, series_id=%s)>' % (self.account_id, self.series_ids)


class InputThetvdbFavorites(object):
    """
    Creates a list of entries for your series marked as favorites at thetvdb.com for use in configure_series.

    Example::

      configure_series:
        from:
          thetvdb_favorites:
            account_id: 23098230
    """

    def validator(self):
        from flexget import validator
        root = validator.factory('dict')
        root.accept('text', key='account_id', required=True)
        root.accept('boolean', key='strip_dates')
        return root

    @cached('thetvdb_favorites')
    @plugin.internet(log)
    @with_session
    def on_task_input(self, task, config, session=None):
        account_id = str(config['account_id'])
        # Get the cache for this user
        user_favorites = session.query(ThetvdbFavorites).filter(ThetvdbFavorites.account_id == account_id).first()
        if user_favorites and user_favorites.updated > datetime.now() - timedelta(minutes=10):
            log.debug('Using cached thetvdb favorite series information for account ID %s' % account_id)
        else:
            try:
                url = 'http://thetvdb.com/api/User_Favorites.php?accountid=%s' % account_id
                log.debug('requesting %s' % url)
                data = ElementTree.fromstring(urlopener(url, log).read())
                favorite_ids = []
                for i in data.findall('Series'):
                    if i.text:
                        favorite_ids.append(i.text)
            except (urllib2.URLError, IOError, AttributeError):
                import traceback
                # If there are errors getting the favorites or parsing the xml, fall back on cache
                log.error('Error retrieving favorites from thetvdb, using cache.')
                log.debug(traceback.format_exc())
            else:
                # Successfully updated from tvdb, update the database
                log.debug('Successfully updated favorites from thetvdb.com')
                if not user_favorites:
                    user_favorites = ThetvdbFavorites(account_id, favorite_ids)
                else:
                    user_favorites.series_ids = favorite_ids
                    user_favorites.updated = datetime.now()
                session.merge(user_favorites)
        if not user_favorites.series_ids:
            log.warning('Didn\'t find any thetvdb.com favorites.')
            return

        # Construct list of entries with our series names
        entries = []
        for series_id in user_favorites.series_ids:
            # Lookup the series name from the id
            try:
                series = lookup_series(tvdb_id=series_id)
            except LookupError as e:
                log.error('Error looking up %s from thetvdb: %s' % (series_id, e.args[0]))
            else:
                series_name = series.seriesname
                if config.get('strip_dates'):
                    # Remove year from end of series name if present
                    series_name = re.sub(r'\s+\(\d{4}\)$', '', series_name)
                entries.append(Entry(series_name, '', tvdb_id=series.id))
        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(InputThetvdbFavorites, 'thetvdb_favorites', api_ver=2)

########NEW FILE########
__FILENAME__ = trakt_emit
from __future__ import unicode_literals, division, absolute_import
import hashlib
import logging

from requests import RequestException

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import json

log = logging.getLogger('trakt_emit')


def make_list_slug(name):
    """Return the slug for use in url for given list name."""
    slug = name.lower()
    # These characters are just stripped in the url
    for char in '!@#$%^*()[]{}/=?+\\|-_':
        slug = slug.replace(char, '')
    # These characters get replaced
    slug = slug.replace('&', 'and')
    slug = slug.replace(' ', '-')
    return slug


class TraktEmit(object):
    """
    Creates an entry for the latest or the next item in your watched or collected
    episodes in your trakt account.

    Syntax:

    trakt_emit:
      username: <value>
      api_key: <value>
      position: <last|next>
      context: <collect|collected|watch|watched>
      list: <value>

    Options username, password and api_key are required.

    """

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'api_key': {'type': 'string'},
            'position': {'type': 'string', 'enum': ['last', 'next'], 'default': 'next'},
            'context': {'type': 'string', 'enum': ['watched', 'collected'], 'default': 'watched'},
            'list': {'type': 'string'}
        },
        'required': ['username', 'password', 'api_key'],
        'additionalProperties': False
    }

    def get_trakt_data(self, task, config, url, null_data=None):
        log.debug('Opening %s' % url)
        auth = {'username': config['username'],
                'password': hashlib.sha1(config['password']).hexdigest()}
        try:
            data = task.requests.get(url, data=json.dumps(auth)).json()
        except RequestException as e:
            raise plugin.PluginError('Unable to get data from trakt.tv: %s' % e)

        def check_auth():
            auth_url = 'http://api.trakt.tv/account/test/' + config['api_key']
            if task.requests.post(auth_url, data=json.dumps(auth), raise_status=False).status_code != 200:
                raise plugin.PluginError('Authentication to trakt failed.')

        if not data:
            check_auth()
            log.warning('No data returned from trakt.')
            return null_data
        if 'error' in data:
            check_auth()
            raise plugin.PluginError('Error getting trakt list: %s' % data['error'])
        return data

    def on_task_input(self, task, config):
        listed_series = {}
        if config.get('list'):
            url = ('http://api.trakt.tv/user/list.json/%s/%s/%s' %
                   (config['api_key'], config['username'], make_list_slug(config['list'])))
            data = self.get_trakt_data(task, config, url, null_data={})
            if not data.get('items') or len(data['items']) <= 0:
                log.warning('The list "%s" is empty.' % config['list'])
                return
            for item in data['items']:
                if item['type'] == 'show':
                    tvdb_id = int(item['show']['tvdb_id'])
                    listed_series[tvdb_id] = item['show']['title']
        url = ('http://api.trakt.tv/user/progress/%s.json/%s/%s' %
               (config['context'], config['api_key'], config['username']))
        if listed_series:
            url += '/' + ','.join(unicode(s) for s in listed_series)
        data = self.get_trakt_data(task, config, url, null_data=[])
        entries = []

        for item in data:
            if item['show']['tvdb_id'] == 0:  # (sh)it happens with filtered queries
                continue
            eps, epn = None, None
            if config['position'] == 'next' and item.get('next_episode'):
                # If the next episode is already in the trakt database, we'll get it here
                eps = item['next_episode']['season']
                epn = item['next_episode']['number']
            else:
                # If we need last ep, or next_episode was not provided, search for last ep
                for seas in reversed(item['seasons']):
                    # Find the first season with collected/watched episodes
                    if seas['completed'] > 0:
                        eps = seas['season']
                        # Pick the highest collected/watched episode
                        epn = max(int(num) for (num, seen) in seas['episodes'].iteritems() if seen)
                        # If we are in next episode mode, we have to increment this number
                        if config['position'] == 'next':
                            if seas['percentage'] >= 100:
                                # If there are more episodes to air this season, next_episode handled it above
                                eps += 1
                                epn = 1
                            else:
                                epn += 1
                        break
            if eps and epn:
                entry = self.make_entry(item['show']['tvdb_id'], item['show']['title'], eps, epn,
                                        item['show']['imdb_id'])
                entries.append(entry)
                if entry['tvdb_id'] in listed_series:
                    del listed_series[entry['tvdb_id']]
        # If we were given an explicit list in next mode, fill in any missing series with S01E01 entries
        if config['position'] == 'next':
            for tvdb_id in listed_series:
                entries.append(self.make_entry(tvdb_id, listed_series[tvdb_id], 1, 1))
        return entries

    def make_entry(self, tvdb_id, name, season, episode, imdb_id=None):
        entry = Entry()
        entry['tvdb_id'] = int(tvdb_id)
        entry['series_name'] = name
        entry['series_season'] = season
        entry['series_episode'] = episode
        entry['series_id_type'] = 'ep'
        entry['series_id'] = 'S%02dE%02d' % (season, episode)
        entry['title'] = entry['series_name'] + ' ' + entry['series_id']
        entry['url'] = 'http://thetvdb.com/?tab=series&id=%s' % tvdb_id
        if imdb_id:
            entry['imdb_id'] = imdb_id
        return entry


@event('plugin.register')
def register_plugin():
    plugin.register(TraktEmit, 'trakt_emit', api_ver=2)

########NEW FILE########
__FILENAME__ = trakt_list
from __future__ import unicode_literals, division, absolute_import
import hashlib
import logging
import re

from requests import RequestException

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import json
from flexget.utils.cached_input import cached

log = logging.getLogger('trakt_list')


def make_list_slug(name):
    """Return the slug for use in url for given list name."""
    slug = name.lower()
    # These characters are just stripped in the url
    for char in '!@#$%^*()[]{}/=?+\\|-_':
        slug = slug.replace(char, '')
    # These characters get replaced
    slug = slug.replace('&', 'and')
    slug = slug.replace(' ', '-')
    return slug


class TraktList(object):
    """Creates an entry for each item in your trakt list.

    Syntax:

    trakt_list:
      username: <value>
      api_key: <value>
      strip_dates: <yes|no>
      movies: <all|loved|hated|collection|watchlist|watched>
      series: <all|loved|hated|collection|watchlist|watched>
      custom: <value>

    Options username and api_key are required.
    """

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'api_key': {'type': 'string'},
            'password': {'type': 'string'},
            'movies': {'enum': ['all', 'loved', 'hated', 'collection', 'watched', 'watchlist']},
            'series': {'enum': ['all', 'loved', 'hated', 'collection', 'watched', 'watchlist']},
            'custom': {'type': 'string'},
            'strip_dates': {'type': 'boolean', 'default': False}
        },
        'required': ['username', 'api_key'],
        'error_oneOf': 'Must specify one and only one of `movies`, `series` or `custom`',
        'oneOf': [
            {'title': 'movie list', 'required': ['movies']},
            {'title': 'series list', 'required': ['series']},
            {'title': 'custom list', 'required': ['custom']}
        ],
        'additionalProperties': False
    }

    movie_map = {
        'title': 'title',
        'url': 'url',
        'imdb_id': 'imdb_id',
        'tmdb_id': 'tmdb_id',
        # Generic fields filled by all movie lookup plugins:
        'movie_name': 'title',
        'movie_year': 'year'}

    series_map = {
        'title': 'title',
        'url': 'url',
        'imdb_id': 'imdb_id',
        'tvdb_id': lambda x: int(x['tvdb_id']),
        'tvrage_id': 'tvrage_id'}

    @cached('trakt_list', persist='2 hours')
    def on_task_input(self, task, config):
        # Don't edit the config, or it won't pass validation on rerun
        url_params = config.copy()
        if 'movies' in config and 'series' in config:
            raise plugin.PluginError('Cannot use both series list and movies list in the same task.')
        if 'movies' in config:
            url_params['data_type'] = 'movies'
            url_params['list_type'] = config['movies']
            map = self.movie_map
        elif 'series' in config:
            url_params['data_type'] = 'shows'
            url_params['list_type'] = config['series']
            map = self.series_map
        elif 'custom' in config:
            url_params['data_type'] = 'custom'
            url_params['list_type'] = make_list_slug(config['custom'])
            # Map type is per item in custom lists
        else:
            raise plugin.PluginError('Must define movie or series lists to retrieve from trakt.')

        url = 'http://api.trakt.tv/user/'
        auth = None
        if url_params['data_type'] == 'custom':
            url += 'list.json/%(api_key)s/%(username)s/%(list_type)s'
        elif url_params['list_type'] == 'watchlist':
            url += 'watchlist/%(data_type)s.json/%(api_key)s/%(username)s'
        else:
            url += 'library/%(data_type)s/%(list_type)s.json/%(api_key)s/%(username)s'
        url = url % url_params

        if 'password' in config:
            auth = {'username': config['username'],
                    'password': hashlib.sha1(config['password']).hexdigest()}

        entries = []
        log.verbose('Retrieving list %s %s...' % (url_params['data_type'], url_params['list_type']))

        try:
            result = task.requests.post(url, data=json.dumps(auth))
        except RequestException as e:
            raise plugin.PluginError('Could not retrieve list from trakt (%s)' % e.args[0])
        try:
            data = result.json()
        except ValueError:
            log.debug('Could not decode json from response: %s', data.text)
            raise plugin.PluginError('Error getting list from trakt.')

        def check_auth():
            if task.requests.post(
                    'http://api.trakt.tv/account/test/' + config['api_key'],
                    data=json.dumps(auth), raise_status=False
            ).status_code != 200:
                raise plugin.PluginError('Authentication to trakt failed.')

        if 'error' in data:
            check_auth()
            raise plugin.PluginError('Error getting trakt list: %s' % data['error'])
        if not data:
            check_auth()
            log.warning('No data returned from trakt.')
            return
        if url_params['data_type'] == 'custom':
            if not isinstance(data['items'], list):
                raise plugin.PluginError('Faulty custom items in response: %s' % data['items'])
            data = data['items']
        for item in data:
            entry = Entry()
            if url_params['data_type'] == 'custom':
                if 'rating' in item:
                    entry['trakt_in_collection'] = item['in_collection']
                    entry['trakt_in_watchlist'] = item['in_watchlist']
                    entry['trakt_rating'] = item['rating']
                    entry['trakt_rating_advanced'] = item['rating_advanced']
                    entry['trakt_watched'] = item['watched']
                if item['type'] == 'movie':
                    map = self.movie_map
                    item = item['movie']
                else:
                    map = self.series_map
                    item = item['show']
            entry.update_using_map(map, item)
            if entry.isvalid():
                if config.get('strip_dates'):
                    # Remove year from end of name if present
                    entry['title'] = re.sub('\s+\(\d{4}\)$', '', entry['title'])
                entries.append(entry)

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(TraktList, 'trakt_list', api_ver=2)

########NEW FILE########
__FILENAME__ = twitterfeed
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from flexget import options, plugin
from flexget.entry import Entry
from flexget.event import event

log = logging.getLogger('twitterfeed')

# Size of the chunks when fetching a timeline
CHUNK_SIZE = 200

# Maximum number of tweets to fetch no matter how (if there is no
# since_id for example or a too old since_id)
MAX_TWEETS = 1000


class TwitterFeed(object):
    """Parses a twitter feed

    Example::

      twitterfeed:
        account: <account>
        consumer_key: <consumer_key>
        consumer_secret: <consumer_secret>
        access_token_key: <access_token_key>
        access_token_secret: <access_token_secret>

    By default, the 50 last tweets are fetched corresponding to the option:
      all_entries: yes

    To change that default number:
      tweets: 75

    Beware that Twitter only allows 300 requests during a 15 minutes
    window.

    If you want to process only new tweets:
      all_entries: no

    That option's behaviour is changed if the corresponding task's
    configuration has been changed. In that case, new tweets are
    fetched and if there are no more than `tweets`, older ones are
    fetched to have `tweets` of them in total.
    """

    schema = {
        'type': 'object',
        'properties': {
            'account': {'type': 'string'},
            'consumer_key': {'type': 'string'},
            'consumer_secret': {'type': 'string'},
            'access_token_key': {'type': 'string'},
            'access_token_secret': {'type': 'string'},
            'all_entries': {'type': 'boolean', 'default': True},
            'tweets': {'type': 'number', 'default': 50}
        },
        'required': ['account', 'consumer_key', 'consumer_secret', 'access_token_secret', 'access_token_key'],
        'additionalProperties': False
    }

    def on_task_start(self, task, config):
        try:
            import twitter
        except ImportError:
            raise plugin.PluginError('twitter module required', logger=log)

    def on_task_input(self, task, config):
        import twitter

        account = config['account']
        log.debug('Looking at twitter account `%s`', account)

        try:
            self.api = twitter.Api(consumer_key=config['consumer_key'],
                                   consumer_secret=config['consumer_secret'],
                                   access_token_key=config['access_token_key'],
                                   access_token_secret=config['access_token_secret'])
        except twitter.TwitterError as e:
            raise plugin.PluginError('Unable to authenticate to twitter for task %s: %s' %
                                     (task.name, e))

        if config['all_entries']:
            log.debug('Fetching %d last tweets from %s timeline' %
                      (config['tweets'], config['account']))
            tweets = self.get_tweets(account, number=config['tweets'])
        else:
            # Fetching from where we left off last time
            since_id = task.simple_persistence.get('since_id', None)
            if since_id:
                log.debug('Fetching from tweet id %d from %s timeline' % (since_id, config['account']))
                kwargs = {'since_id': since_id}
            else:
                log.debug('No since_id, fetching last %d tweets' % config['tweets'])
                kwargs = {'number': config['tweets']}

            tweets = self.get_tweets(account, **kwargs)
            if task.config_modified and len(tweets) < config['tweets']:
                log.debug('Configuration modified; fetching at least %d tweets' % config['tweets'])
                max_id = tweets[-1].id if tweets else None
                remaining_tweets = config['tweets'] - len(tweets)
                tweets = tweets + self.get_tweets(account, max_id=max_id, number=remaining_tweets)
            if tweets:
                last_tweet = tweets[0]
                log.debug('New last tweet id: %d' % last_tweet.id)
                task.simple_persistence['since_id'] = last_tweet.id

        log.debug('%d tweets fetched' % len(tweets))
        for t in tweets:
            log.debug('id:%d' % t.id)

        return [self.entry_from_tweet(e) for e in tweets]

    def get_tweets(self, account, number=MAX_TWEETS, since_id=None, max_id=None):
        """Fetch tweets from twitter account `account`."""
        import twitter

        all_tweets = []
        while number > 0:
            try:
                tweets = self.api.GetUserTimeline(screen_name=account,
                                                  include_rts=False,
                                                  exclude_replies=True,
                                                  count=min(number, CHUNK_SIZE),
                                                  since_id=since_id,
                                                  max_id=max_id)
            except twitter.TwitterError as e:
                raise plugin.PluginError('Unable to fetch timeline %s for task %s: %s' % (account, task.name, e))

            if not tweets:
                break

            all_tweets += tweets
            number -= len(tweets)
            max_id = tweets[-1].id - 1

        return all_tweets

    def entry_from_tweet(self, tweet):
        new_entry = Entry()
        new_entry['title'] = tweet.text
        urls = re.findall(r'(https?://\S+)', tweet.text)
        new_entry['urls'] = urls
        if urls:
            new_entry['url'] = urls[0]
        return new_entry


@event('plugin.register')
def register_plugin():
    plugin.register(TwitterFeed, 'twitterfeed', api_ver=2)

########NEW FILE########
__FILENAME__ = whatcd
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.entry import Entry
from flexget.event import event
from flexget.plugin import PluginError
from flexget.utils.cached_input import cached
from flexget.utils.requests import Session

log = logging.getLogger('whatcd')


class InputWhatCD(object):
    """A plugin that searches what.cd

    == Usage:

    All parameters except `username` and `password` are optional.

    whatcd:
        username:
        password:

        user_agent: (A custom user-agent for the client to report.
                     It is NOT A GOOD IDEA to spoof a browser with
                     this. You are responsible for your account.)

        search: (general search filter)

        artist: (artist name)
        album: (album name)
        year: (album year)

        encoding: (encoding specifics - 192, 320, lossless, etc.)
        format: (MP3, FLAC, AAC, etc.)
        media: (CD, DVD, vinyl, Blu-ray, etc.)
        release_type: (album, soundtrack, EP, etc.)

        log: (log specification - true, false, '100%', or '<100%')
        hascue: (has a cue file - true or false)
        scene: (is a scene release - true or false)
        vanityhouse: (is a vanity house release - true or false)
        leech_type: ('freeleech', 'neutral', 'either', or 'normal')

        tags: (a list of tags to match - drum.and.bass, new.age, blues, etc.)
        tag_type: (match 'any' or 'all' of the items in `tags`)
    """

    # Aliases for config -> api params
    ALIASES = {
        "artist": "artistname",
        "album": "groupname",
        "leech_type": "freetorrent",
        "release_type": "releaseType",
        "tags": "taglist",
        "tag_type": "tags_type",
        "search": "searchstr",
        "log": "haslog",
    }

    # API parameters
    # None means a raw value entry (no validation)
    # A dict means a choice with a mapping for the API
    # A list is just a choice with no mapping
    PARAMS = {
        "searchstr": None,
        "taglist": None,
        "artistname": None,
        "groupname": None,
        "year": None,
        "tags_type": {
            "any": 0,
            "all": 1,
        },
        "encoding": [
            "192", "APS (VBR)", "V2 (VBR)", "V1 (VBR)", "256", "APX (VBR)",
            "V0 (VBR)", "320", "lossless", "24bit lossless", "V8 (VBR)"
        ],
        "format": [
            "MP3", "FLAC", "AAC", "AC3", "DTS"
        ],
        "media": [
            "CD", "DVD", "vinyl", "soundboard", "SACD", "DAT", "cassette",
            "WEB", "Blu-ray"
        ],
        "releaseType": {
            "album": 1,
            "soundtrack": 3,
            "EP": 5,
            "anthology": 6,
            "compilation": 7,
            "DJ mix": 8,
            "single": 9,
            "live album": 11,
            "remix": 13,
            "bootleg": 14,
            "interview": 15,
            "mixtape": 16,
            "unknown": 21,
            "concert recording": 22,
            "demo": 23
        },
        "haslog": {
            "False": 0,
            "True": 1,
            "100%": 100,
            "<100%": -1
        },
        "freetorrent": {
            "freeleech": 1,
            "neutral": 2,
            "either": 3,
            "normal": 0,
        },
        "hascue": {
            "False": 0,
            "True": 1,
        },
        "scene": {
            "False": 0,
            "True": 1,
        },
        "vanityhouse": {
            "False": 0,
            "True": 1,
        }
    }

    def _key(self, key):
        """Gets the API key name from the entered key"""
        try:
            if key in self.ALIASES:
                return self.ALIASES[key]
            elif key in self.PARAMS:
                return key
            return None
        except KeyError:
            return None

    def _opts(self, key):
        """Gets the options for the specified key"""
        temp = self._key(key)
        try:
            return self.PARAMS[temp]
        except KeyError:
            return None

    def _getval(self, key, val):
        """Gets the value for the specified key"""
        # No alias or param by that name
        if self._key(key) is None:
            return None

        opts = self._opts(key)
        if opts is None:
            if isinstance(val, list):
                return ",".join(val)
            return val
        elif isinstance(opts, dict):
            # Options, translate the input to output
            # The str cast converts bools to 'True'/'False' for use as keys
            return opts[str(val)]
        else:
            # List of options, check it's in the list
            if val not in opts:
                return None
            return val

    def __init__(self):
        """Set up the schema"""

        self.schema = {
            'type': 'object',
            'properties': {
                'username': {'type': 'string'},
                'password': {'type': 'string'},
                'user_agent': {'type': 'string'},
                'search': {'type': 'string'},
                'artist': {'type': 'string'},
                'album': {'type': 'string'},
                'year': {'type': ['string', 'integer']},
                'tags': one_or_more({'type': 'string'}),
                'tag_type': {'type': 'string', 'enum': self._opts('tag_type').keys()},
                'encoding': {'type': 'string', 'enum': self._opts('encoding')},
                'format': {'type': 'string', 'enum': self._opts('format')},
                'media': {'type': 'string', 'enum': self._opts('media')},
                'release_type': {'type': 'string', 'enum': self._opts('release_type').keys()},
                'log': {'oneOf': [{'type': 'string', 'enum': self._opts('log').keys()}, {'type': 'boolean'}]},
                'leech_type': {'type': 'string', 'enum': self._opts('leech_type').keys()},
                'hascue': {'type': 'boolean'},
                'scene': {'type': 'boolean'},
                'vanityhouse': {'type': 'boolean'},
            },
            'required': ['username', 'password'],
            'additionalProperties': False
        }

    def _login(self, config):
        """
        Log in and store auth data from the server
        Adapted from https://github.com/isaaczafuta/whatapi
        """

        data = {
            'username': config['username'],
            'password': config['password'],
            'keeplogged': 1,
        }

        r = self.session.post("https://ssl.what.cd/login.php", data=data,
                              allow_redirects=False)
        if r.status_code != 302 or r.headers.get('location') != "index.php":
            raise PluginError("Failed to log in to What.cd")

        accountinfo = self._request("index")

        self.authkey = accountinfo["authkey"]
        self.passkey = accountinfo["passkey"]
        log.info("Logged in to What.cd")

    def _request(self, action, **kwargs):
        """
        Make an AJAX request to a given action page
        Adapted from https://github.com/isaaczafuta/whatapi
        """

        ajaxpage = 'https://ssl.what.cd/ajax.php'

        params = {}

        # Filter params and map config values -> api values
        for k, v in kwargs.iteritems():
            key = self._key(k)
            if key is not None:
                params[key] = self._getval(k, v)

        # Params other than the searching ones
        params['action'] = action
        if 'page' in kwargs:
            params['page'] = kwargs['page']

        r = self.session.get(ajaxpage, params=params, allow_redirects=False)
        if r.status_code != 200:
            raise PluginError("What.cd returned a non-200 status code")

        try:
            json_response = r.json()
            if json_response['status'] != "success":
                raise PluginError("What.cd gave a 'failure' response: "
                                  "'{0}'".format(json_response['error']))
            return json_response['response']
        except (ValueError, TypeError) as e:
            raise PluginError("What.cd returned an invalid response")

    @cached('whatcd')
    @plugin.internet(log)
    def on_task_input(self, task, config):
        """Search on What.cd"""

        self.session = Session()
        self.session.headers.update({"User-Agent": config.get('user_agent', "Flexget (What.cd plugin)")})

        # From the API docs: "Refrain from making more than five (5) requests every ten (10) seconds"
        self.session.set_domain_delay('ssl.what.cd', '2 seconds')

        # Login
        self._login(config)

        # Perform the query
        results = []
        page = 1
        while True:
            result = self._request("browse", page=page, **config)
            if not result['results']:
                break
            results.extend(result["results"])
            pages = result['pages']
            page = result['currentPage']
            log.info("Got {0} of {1} pages".format(page, pages))
            if page >= pages:
                break
            page += 1

        # Logged in and made a request successfully, it's ok if nothing matches
        task.no_entries_ok = True

        # Parse the needed information out of the response
        entries = []
        for result in results:
            # Get basic information on the release
            info = dict((k, result[k]) for k in ('artist', 'groupName', 'groupYear'))

            # Releases can have multiple download options
            for tor in result['torrents']:
                temp = info.copy()
                temp.update(dict((k, tor[k]) for k in ('media', 'encoding', 'format', 'torrentId')))

                entries.append(Entry(
                    title="{artist} - {groupName} - {groupYear} "
                          "({media} - {format} - {encoding})-{torrentId}.torrent".format(**temp),
                    url="https://what.cd/torrents.php?action=download&"
                        "id={0}&authkey={1}&torrent_pass={2}".format(temp['torrentId'], self.authkey, self.passkey),
                    torrent_seeds=tor['seeders'],
                    torrent_leeches=tor['leechers'],
                    # Size is given in bytes, convert it
                    content_size=int(tor['size']/(1024**2)*100)/100
                ))

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(InputWhatCD, 'whatcd', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = assume_quality
from __future__ import unicode_literals, division, absolute_import
from collections import namedtuple
import logging
import flexget.utils.qualities as qualities
from flexget import plugin
from flexget.event import event

log = logging.getLogger('assume_quality')


class AssumeQuality(object):
    """
    Applies quality components to entries that match specified quality requirements.
    When a quality is applied, any components which are unknown in the entry are filled from the applied quality.
    Quality requirements are tested in order of increasing precision (ie "720p h264" is more precise than "1080p"
    so gets tested first), and applied as matches are found. Using the simple configuration is the same as specifying
    an "any" rule.

    Examples::
    assume_quality: 1080p webdl 10bit truehd

    assume_quality:
      hdtv: 720p
      720p hdtv: 10bit
      '!ac3 !mp3': flac
      any: 720p h264
    """

    schema = {
        'oneOf': [
            {'title':'simple config', 'type': 'string', 'format': 'quality'},
            {'title':'advanced config', 'type': 'object',
                   #Can't validate dict keys, so allow any
                   'additionalProperties': {'type': 'string', 'format': 'quality'}
            }
        ]
    }

    def precision(self, qualityreq):
        p = 0
        for component in qualityreq.components:
            if component.acceptable: p += 8
            if component.min: p += 4
            if component.max: p += 4
            if component.none_of: p += len(component.none_of)
            #Still a long way from perfect, but probably good enough.
        return p

    def assume(self, entry, quality):
        newquality = qualities.Quality()
        log.debug('Current qualities: %s', entry.get('quality'))
        for component in entry.get('quality').components:
            qualitycomponent = getattr(quality, component.type)
            log.debug('\t%s: %s vs %s', component.type, component.name, qualitycomponent.name)
            if component.name != 'unknown':
                log.debug('\t%s: keeping %s', component.type, component.name)
                setattr(newquality, component.type, component)
            elif qualitycomponent.name != 'unknown':
                log.debug('\t%s: assuming %s', component.type, qualitycomponent.name)
                setattr(newquality, component.type, qualitycomponent)
                entry['assumed_quality'] = True
            elif component.name == 'unknown' and qualitycomponent.name == 'unknown':
                log.debug('\t%s: got nothing', component.type)
        entry['quality'] = newquality
        log.debug('Quality updated: %s', entry.get('quality'))

    def on_task_start(self, task, config):
        if isinstance(config, basestring): config = {'any': config}
        assume = namedtuple('assume', ['target', 'quality'])
        self.assumptions = []
        for target, quality in config.items():
            log.verbose('New assumption: %s is %s' % (target, quality))
            try: target = qualities.Requirements(target)
            except: raise plugin.PluginError('%s is not a valid quality. Forgetting assumption.' % target)
            try: quality = qualities.get(quality)
            except: raise plugin.PluginError('%s is not a valid quality. Forgetting assumption.' % quality)
            self.assumptions.append(assume(target, quality))
        self.assumptions.sort(key=lambda assumption: self.precision(assumption.target), reverse=True)
        for assumption in self.assumptions:
            log.debug('Target %s - Priority %s' % (assumption.target, self.precision(assumption.target)))

    @plugin.priority(127)  #run after metainfo_quality@128
    def on_task_metainfo(self, task, config):
        for entry in task.entries:
            log.verbose('%s' % entry.get('title'))
            for assumption in self.assumptions:
                log.debug('Trying %s - %s' % (assumption.target, assumption.quality))
                if assumption.target.allows(entry.get('quality')):
                    log.debug('Match: %s' % assumption.target)
                    self.assume(entry, assumption.quality)
            log.verbose('New quality: %s', entry.get('quality'))

@event('plugin.register')
def register_plugin():
    plugin.register(AssumeQuality, 'assume_quality', api_ver=2)

########NEW FILE########
__FILENAME__ = content_size
from __future__ import unicode_literals, division, absolute_import
import logging
import re
import math
import os.path

from flexget import plugin
from flexget.event import event

log = logging.getLogger('metanfo_csize')

SIZE_RE = re.compile(r'Size[^\d]{0,7}(\d*\.?\d+).{0,5}(MB|GB)', re.IGNORECASE)


class MetainfoContentSize(object):
    """
    Utility:

    Check if content size is mentioned in description and set content_size attribute for entries if it is.
    Also sets content_size for entries with local files from input_listdir.
    """

    schema = {'type': 'boolean', 'default': False}

    def on_task_metainfo(self, task, config):
        # check if disabled (value set to false)
        if config is False:
            return

        count = 0
        for entry in task.entries:
            if entry.get('content_size'):
                # Don't override if already set
                log.trace('skipping content size check because it is already set for %r' % entry['title'])
                continue
            # Try to parse size from description
            match = SIZE_RE.search(entry.get('description', ''))
            if match:
                try:
                    amount = float(match.group(1).replace(',', '.'))
                except Exception:
                    log.error('BUG: Unable to convert %s into float (%s)' % (match.group(1), entry['title']))
                    continue
                unit = match.group(2).lower()
                count += 1
                if unit == 'gb':
                    amount = math.ceil(amount * 1024)
                log.trace('setting content size to %s' % amount)
                entry['content_size'] = int(amount)
                continue
            # If this entry has a local file, (it was added by listdir) grab the size.
            elif 'location' in entry:
                # If it is a .torrent or .nzb, don't bother getting the size as it will not be the content's size
                if entry['location'].endswith('.torrent') or entry['location'].endswith('.nzb'):
                    continue
                if os.path.isfile(entry['location']):
                    amount = os.path.getsize(entry['location'])
                    amount = int(amount / (1024 * 1024))
                    log.trace('setting content size to %s' % amount)
                    entry['content_size'] = amount
                    continue

        if count:
            log.debug('Found content size information from %s entries' % count)


@event('plugin.register')
def register_plugin():
    plugin.register(MetainfoContentSize, 'metainfo_content_size', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = imdb_lookup
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime, timedelta

from sqlalchemy import Table, Column, Integer, Float, String, Unicode, Boolean, DateTime, delete
from sqlalchemy.schema import ForeignKey, Index
from sqlalchemy.orm import relation, joinedload

from flexget import db_schema, plugin
from flexget.event import event
from flexget.entry import Entry
from flexget.manager import Session
from flexget.utils.log import log_once
from flexget.utils.imdb import ImdbSearch, ImdbParser, extract_id, make_url
from flexget.utils.sqlalchemy_utils import table_add_column
from flexget.utils.database import with_session
from flexget.utils.sqlalchemy_utils import table_columns, get_index_by_name, table_schema

SCHEMA_VER = 4

Base = db_schema.versioned_base('imdb_lookup', SCHEMA_VER)


# association tables
genres_table = Table('imdb_movie_genres', Base.metadata,
    Column('movie_id', Integer, ForeignKey('imdb_movies.id')),
    Column('genre_id', Integer, ForeignKey('imdb_genres.id')),
    Index('ix_imdb_movie_genres', 'movie_id', 'genre_id'))

actors_table = Table('imdb_movie_actors', Base.metadata,
    Column('movie_id', Integer, ForeignKey('imdb_movies.id')),
    Column('actor_id', Integer, ForeignKey('imdb_actors.id')),
    Index('ix_imdb_movie_actors', 'movie_id', 'actor_id'))

directors_table = Table('imdb_movie_directors', Base.metadata,
    Column('movie_id', Integer, ForeignKey('imdb_movies.id')),
    Column('director_id', Integer, ForeignKey('imdb_directors.id')),
    Index('ix_imdb_movie_directors', 'movie_id', 'director_id'))


class Movie(Base):

    __tablename__ = 'imdb_movies'

    id = Column(Integer, primary_key=True)
    title = Column(Unicode)
    original_title = Column(Unicode)
    url = Column(String, index=True)

    # many-to-many relations
    genres = relation('Genre', secondary=genres_table, backref='movies')
    actors = relation('Actor', secondary=actors_table, backref='movies')
    directors = relation('Director', secondary=directors_table, backref='movies')
    languages = relation('MovieLanguage', order_by='MovieLanguage.prominence')

    score = Column(Float)
    votes = Column(Integer)
    year = Column(Integer)
    plot_outline = Column(Unicode)
    mpaa_rating = Column(String, default='')
    photo = Column(String)

    # updated time, so we can grab new rating counts after 48 hours
    # set a default, so existing data gets updated with a rating
    updated = Column(DateTime)

    @property
    def imdb_id(self):
        return extract_id(self.url)

    @property
    def expired(self):
        """
        :return: True if movie details are considered to be expired, ie. need of update
        """
        if self.updated is None:
            log.debug('updated is None: %s' % self)
            return True
        refresh_interval = 2
        if self.year:
            age = (datetime.now().year - self.year)
            refresh_interval += age * 5
            log.debug('movie `%s` age %i expires in %i days' % (self.title, age, refresh_interval))
        return self.updated < datetime.now() - timedelta(days=refresh_interval)

    def __repr__(self):
        return '<Movie(name=%s,votes=%s,year=%s)>' % (self.title, self.votes, self.year)


class MovieLanguage(Base):

    __tablename__ = 'imdb_movie_languages'

    movie_id = Column(Integer, ForeignKey('imdb_movies.id'), primary_key=True)
    language_id = Column(Integer, ForeignKey('imdb_languages.id'), primary_key=True)
    prominence = Column(Integer)

    language = relation('Language')

    def __init__(self, language, prominence=None):
        self.language = language
        self.prominence = prominence


class Language(Base):

    __tablename__ = 'imdb_languages'

    id = Column(Integer, primary_key=True)
    name = Column(Unicode)

    def __init__(self, name):
        self.name = name


class Genre(Base):

    __tablename__ = 'imdb_genres'

    id = Column(Integer, primary_key=True)
    name = Column(String)

    def __init__(self, name):
        self.name = name


class Actor(Base):

    __tablename__ = 'imdb_actors'

    id = Column(Integer, primary_key=True)
    imdb_id = Column(String)
    name = Column(Unicode)

    def __init__(self, imdb_id, name=None):
        self.imdb_id = imdb_id
        self.name = name


class Director(Base):

    __tablename__ = 'imdb_directors'

    id = Column(Integer, primary_key=True)
    imdb_id = Column(String)
    name = Column(Unicode)

    def __init__(self, imdb_id, name=None):
        self.imdb_id = imdb_id
        self.name = name


class SearchResult(Base):

    __tablename__ = 'imdb_search'

    id = Column(Integer, primary_key=True)
    title = Column(Unicode, index=True)
    url = Column(String)
    fails = Column(Boolean, default=False)
    queried = Column(DateTime)

    @property
    def imdb_id(self):
        return extract_id(self.url)

    def __init__(self, title, url=None):
        self.title = title
        self.url = url
        self.queried = datetime.now()

    def __repr__(self):
        return '<SearchResult(title=%s,url=%s,fails=%s)>' % (self.title, self.url, self.fails)

log = logging.getLogger('imdb_lookup')


@db_schema.upgrade('imdb_lookup')
def upgrade(ver, session):
    if ver is None:
        columns = table_columns('imdb_movies', session)
        if not 'photo' in columns:
            log.info('Adding photo column to imdb_movies table.')
            table_add_column('imdb_movies', 'photo', String, session)
        if not 'updated' in columns:
            log.info('Adding updated column to imdb_movies table.')
            table_add_column('imdb_movies', 'updated', DateTime, session)
        if not 'mpaa_rating' in columns:
            log.info('Adding mpaa_rating column to imdb_movies table.')
            table_add_column('imdb_movies', 'mpaa_rating', String, session)
        ver = 0
    if ver == 0:
        # create indexes retrospectively (~r2563)
        log.info('Adding imdb indexes delivering up to 20x speed increase \o/ ...')
        indexes = [get_index_by_name(actors_table, 'ix_imdb_movie_actors'),
                   get_index_by_name(genres_table, 'ix_imdb_movie_genres'),
                   get_index_by_name(directors_table, 'ix_imdb_movie_directors')]
        for index in indexes:
            if index is None:
                log.critical('Index adding failure!')
                continue
            log.info('Creating index %s ...' % index.name)
            index.create(bind=session.connection())
        ver = 1
    if ver == 1:
        # http://flexget.com/ticket/1399
        log.info('Adding prominence column to imdb_movie_languages table.')
        table_add_column('imdb_movie_languages', 'prominence', Integer, session)
        ver = 2
    if ver == 2:
        log.info('Adding search result timestamp and clearing all previous results.')
        table_add_column('imdb_search', 'queried', DateTime, session)
        search_table = table_schema('imdb_search', session)
        session.execute(delete(search_table, search_table.c.fails))
        ver = 3
    if ver == 3:
        log.info('Adding original title column, cached data will not have this information')
        table_add_column('imdb_movies', 'original_title', Unicode, session)
        ver = 4
    return ver


class ImdbLookup(object):
    """
        Retrieves imdb information for entries.

        Example:

        imdb_lookup: yes

        Also provides imdb lookup functionality to all other imdb related plugins.
    """

    field_map = {
        'imdb_url': 'url',
        'imdb_id': lambda movie: extract_id(movie.url),
        'imdb_name': 'title',
        'imdb_original_name': 'original_title',
        'imdb_photo': 'photo',
        'imdb_plot_outline': 'plot_outline',
        'imdb_score': 'score',
        'imdb_votes': 'votes',
        'imdb_year': 'year',
        'imdb_genres': lambda movie: [genre.name for genre in movie.genres],
        'imdb_languages': lambda movie: [lang.language.name for lang in movie.languages],
        'imdb_actors': lambda movie: dict((actor.imdb_id, actor.name) for actor in movie.actors),
        'imdb_directors': lambda movie: dict((director.imdb_id, director.name) for director in movie.directors),
        'imdb_mpaa_rating': 'mpaa_rating',
        # Generic fields filled by all movie lookup plugins:
        'movie_name': 'title',
        'movie_year': 'year'}

    def validator(self):
        from flexget import validator
        return validator.factory('boolean')

    @plugin.priority(130)
    def on_task_metainfo(self, task, config):
        if not config:
            return
        for entry in task.entries:
            self.register_lazy_fields(entry)

    def register_lazy_fields(self, entry):
        entry.register_lazy_fields(self.field_map, self.lazy_loader)

    def lazy_loader(self, entry, field):
        """Does the lookup for this entry and populates the entry fields."""
        try:
            self.lookup(entry)
        except plugin.PluginError as e:
            log_once(unicode(e.value).capitalize(), logger=log)
            # Set all of our fields to None if the lookup failed
            entry.unregister_lazy_fields(self.field_map, self.lazy_loader)
        return entry[field]

    @with_session
    def imdb_id_lookup(self, movie_title=None, raw_title=None, session=None):
        """
        Perform faster lookup providing just imdb_id.
        Falls back to using basic lookup if data cannot be found from cache.

        .. note::

           API will be changed, it's dumb to return None on errors AND
           raise PluginError on some else

        :param movie_title: Name of the movie
        :param raw_title: Raw entry title
        :return: imdb id or None
        :raises PluginError: Failure reason
        """
        if movie_title:
            log.debug('imdb_id_lookup: trying with title: %s' % movie_title)
            movie = session.query(Movie).filter(Movie.title == movie_title).first()
            if movie:
                log.debug('--> success! got %s returning %s' % (movie, movie.imdb_id))
                return movie.imdb_id
        if raw_title:
            log.debug('imdb_id_lookup: trying cache with: %s' % raw_title)
            result = session.query(SearchResult).filter(SearchResult.title == raw_title).first()
            if result:
                # this title is hopeless, give up ..
                if result.fails:
                    return None
                log.debug('--> success! got %s returning %s' % (result, result.imdb_id))
                return result.imdb_id
        if raw_title:
            # last hope with hacky lookup
            fake_entry = Entry(raw_title, '')
            self.lookup(fake_entry)
            return fake_entry['imdb_id']

    @plugin.internet(log)
    def lookup(self, entry, search_allowed=True):
        """
        Perform imdb lookup for entry.

        :param entry: Entry instance
        :param search_allowed: Allow fallback to search
        :raises PluginError: Failure reason
        """

        from flexget.manager import manager

        if entry.get('imdb_id', eval_lazy=False):
            log.debug('No title passed. Lookup for %s' % entry['imdb_id'])
        elif entry.get('imdb_url', eval_lazy=False):
            log.debug('No title passed. Lookup for %s' % entry['imdb_url'])
        elif entry.get('title', eval_lazy=False):
            log.debug('lookup for %s' % entry['title'])
        else:
            raise plugin.PluginError('looking up IMDB for entry failed, no title, imdb_url or imdb_id passed.')

        session = Session()

        try:
            # entry sanity checks
            for field in ['imdb_votes', 'imdb_score']:
                if entry.get(field, eval_lazy=False):
                    value = entry[field]
                    if not isinstance(value, (int, float)):
                        raise plugin.PluginError('Entry field %s should be a number!' % field)

            # if imdb_id is included, build the url.
            if entry.get('imdb_id', eval_lazy=False) and not entry.get('imdb_url', eval_lazy=False):
                entry['imdb_url'] = make_url(entry['imdb_id'])

            # make sure imdb url is valid
            if entry.get('imdb_url', eval_lazy=False):
                imdb_id = extract_id(entry['imdb_url'])
                if imdb_id:
                    entry['imdb_url'] = make_url(imdb_id)
                else:
                    log.debug('imdb url %s is invalid, removing it' % entry['imdb_url'])
                    del(entry['imdb_url'])

            # no imdb_url, check if there is cached result for it or if the
            # search is known to fail
            if not entry.get('imdb_url', eval_lazy=False):
                result = session.query(SearchResult).\
                    filter(SearchResult.title == entry['title']).first()
                if result:
                    # TODO: 1.2 this should really be checking task.options.retry
                    if result.fails and not manager.options.execute.retry:
                        # this movie cannot be found, not worth trying again ...
                        log.debug('%s will fail lookup' % entry['title'])
                        raise plugin.PluginError('IMDB lookup failed for %s' % entry['title'])
                    else:
                        if result.url:
                            log.trace('Setting imdb url for %s from db' % entry['title'])
                            entry['imdb_url'] = result.url

            # no imdb url, but information required, try searching
            if not entry.get('imdb_url', eval_lazy=False) and search_allowed:
                log.verbose('Searching from imdb `%s`' % entry['title'])

                search = ImdbSearch()
                search_name = entry.get('movie_name', entry['title'], eval_lazy=False)
                search_result = search.smart_match(search_name)
                if search_result:
                    entry['imdb_url'] = search_result['url']
                    # store url for this movie, so we don't have to search on
                    # every run
                    result = SearchResult(entry['title'], entry['imdb_url'])
                    session.add(result)
                    log.verbose('Found %s' % (entry['imdb_url']))
                else:
                    log_once('IMDB lookup failed for %s' % entry['title'], log, logging.WARN)
                    # store FAIL for this title
                    result = SearchResult(entry['title'])
                    result.fails = True
                    session.add(result)
                    raise plugin.PluginError('Title `%s` lookup failed' % entry['title'])

            # check if this imdb page has been parsed & cached
            movie = session.query(Movie).filter(Movie.url == entry['imdb_url']).first()

            # determine whether or not movie details needs to be parsed
            req_parse = False
            if not movie:
                req_parse = True
            elif movie.expired:
                req_parse = True

            if req_parse:
                if movie is not None:
                    if movie.expired:
                        log.verbose('Movie `%s` details expired, refreshing ...' % movie.title)
                    # Remove the old movie, we'll store another one later.
                    session.query(MovieLanguage).filter(MovieLanguage.movie_id == movie.id).delete()
                    session.query(Movie).filter(Movie.url == entry['imdb_url']).delete()

                # search and store to cache
                if 'title' in entry:
                    log.verbose('Parsing imdb for `%s`' % entry['title'])
                else:
                    log.verbose('Parsing imdb for `%s`' % entry['imdb_id'])
                try:
                    movie = self._parse_new_movie(entry['imdb_url'], session)
                except UnicodeDecodeError:
                    log.error('Unable to determine encoding for %s. Installing chardet library may help.' %
                              entry['imdb_url'])
                    # store cache so this will not be tried again
                    movie = Movie()
                    movie.url = entry['imdb_url']
                    session.add(movie)
                    raise plugin.PluginError('UnicodeDecodeError')
                except ValueError as e:
                    # TODO: might be a little too broad catch, what was this for anyway? ;P
                    if manager.options.debug:
                        log.exception(e)
                    raise plugin.PluginError('Invalid parameter: %s' % entry['imdb_url'], log)

            for att in ['title', 'score', 'votes', 'year', 'genres', 'languages', 'actors', 'directors', 'mpaa_rating']:
                log.trace('movie.%s: %s' % (att, getattr(movie, att)))

            # store to entry
            entry.update_using_map(self.field_map, movie)
        finally:
            log.trace('committing session')
            session.commit()

    def _parse_new_movie(self, imdb_url, session):
        """
        Get Movie object by parsing imdb page and save movie into the database.

        :param imdb_url: IMDB url
        :param session: Session to be used
        :return: Newly added Movie
        """
        parser = ImdbParser()
        parser.parse(imdb_url)
        # store to database
        movie = Movie()
        movie.photo = parser.photo
        movie.title = parser.name
        movie.original_title = parser.original_name
        movie.score = parser.score
        movie.votes = parser.votes
        movie.year = parser.year
        movie.mpaa_rating = parser.mpaa_rating
        movie.plot_outline = parser.plot_outline
        movie.url = imdb_url
        for name in parser.genres:
            genre = session.query(Genre).filter(Genre.name == name).first()
            if not genre:
                genre = Genre(name)
            movie.genres.append(genre)  # pylint:disable=E1101
        for index, name in enumerate(parser.languages):
            language = session.query(Language).filter(Language.name == name).first()
            if not language:
                language = Language(name)
            movie.languages.append(MovieLanguage(language, prominence=index))
        for imdb_id, name in parser.actors.iteritems():
            actor = session.query(Actor).filter(Actor.imdb_id == imdb_id).first()
            if not actor:
                actor = Actor(imdb_id, name)
            movie.actors.append(actor)  # pylint:disable=E1101
        for imdb_id, name in parser.directors.iteritems():
            director = session.query(Director).filter(Director.imdb_id == imdb_id).first()
            if not director:
                director = Director(imdb_id, name)
            movie.directors.append(director)  # pylint:disable=E1101
            # so that we can track how long since we've updated the info later
        movie.updated = datetime.now()
        session.add(movie)
        return movie

@event('plugin.register')
def register_plugin():
    plugin.register(ImdbLookup, 'imdb_lookup', api_ver=2)

########NEW FILE########
__FILENAME__ = imdb_url
from __future__ import unicode_literals, division, absolute_import
import re
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('metainfo_imdb_url')


class MetainfoImdbUrl(object):
    """
        Scan entry information for imdb url.
    """

    schema = {'type': 'boolean'}

    def on_task_metainfo(self, task, config):
        # check if disabled (value set to false)
        if 'scan_imdb' in task.config:
            if not task.config['scan_imdb']:
                return

        for entry in task.entries:
            if not 'description' in entry:
                continue
            urls = re.findall(r'\bimdb.com/title/tt\d+\b', entry['description'])
            if not urls:
                continue

            # Uniquify the list of urls.
            urls = list(set(urls))
            if 1 < len(urls):
                log.debug('Found multiple imdb urls; not using any of: %s' %
                    ' '.join(urls))
                continue

            url = ''.join(['http://www.', urls[0]])
            entry['imdb_url'] = url
            log.debug('Found imdb url in description %s' % url)

@event('plugin.register')
def register_plugin():
    plugin.register(MetainfoImdbUrl, 'scan_imdb', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = magnet_info_hash
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from flexget import plugin
from flexget.event import event

log = logging.getLogger('magnet_btih')


class MagnetBtih(object):
    """Sets torrent_info_hash from magnet url."""

    schema = {'type': 'boolean'}

    def on_task_metainfo(self, task, config):
        if config is False:
            return
        for entry in task.all_entries:
            if entry.get('torrent_info_hash'):
                continue
            for url in [entry['url']] + entry.get('urls', []):
                if url.startswith('magnet:'):
                    info_hash_search = re.search('btih:([0-9a-f]+)', url, re.IGNORECASE)
                    if info_hash_search:
                        entry['torrent_info_hash'] = info_hash_search.group(1).upper()
                        break


@event('plugin.register')
def register_plugin():
    plugin.register(MagnetBtih, 'magnet_btih', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = nzb_size
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('nzb_size')

# a bit hacky, add nzb as a known mimetype
import mimetypes
mimetypes.add_type('application/x-nzb', '.nzb')


class NzbSize(object):

    """
    Provides entry size information when dealing with nzb files
    """

    @plugin.priority(200)
    def on_task_modify(self, task, config):
        """
        The downloaded file is accessible in modify phase
        """
        try:
            from pynzb import nzb_parser
        except ImportError:
            # TODO: remove builtin status so this won't get repeated on every task execution
            # TODO: this will get loaded even without any need for nzb
            raise plugin.DependencyError(issued_by='nzb_size', missing='lib pynzb')

        for entry in task.accepted:
            if entry.get('mime-type', None) in [u'text/nzb', u'application/x-nzb'] or \
                    entry.get('filename', '').endswith('.nzb'):

                if 'file' not in entry:
                    log.warning('`%s` does not have a `file` that could be used to get size information' %
                                entry['title'])
                    continue

                filename = entry['file']
                log.debug('reading %s' % filename)
                xmldata = file(filename).read()

                try:
                    nzbfiles = nzb_parser.parse(xmldata)
                except:
                    log.debug('%s is not a valid nzb' % entry['title'])
                    continue

                size = 0
                for nzbfile in nzbfiles:
                    for segment in nzbfile.segments:
                        size += segment.bytes

                size_mb = size / 1024 / 1024
                log.debug('%s content size: %s MB' % (entry['title'], size_mb))
                entry['content_size'] = size_mb
            else:
                log.trace('%s does not seem to be nzb' % entry['title'])


@event('plugin.register')
def register_plugin():
    plugin.register(NzbSize, 'nzb_size', api_ver=2, builtin=True)

########NEW FILE########
__FILENAME__ = quality
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils import qualities

log = logging.getLogger('metainfo_quality')


class MetainfoQuality(object):
    """
    Utility:

    Set quality attribute for entries.
    """

    schema = {'type': 'boolean'}

    def on_task_metainfo(self, task, config):
        # check if disabled (value set to false)
        if config is False:
            return
        for entry in task.entries:
            entry.register_lazy_fields(['quality'], self.lazy_loader)

    def lazy_loader(self, entry, field):
        self.get_quality(entry)
        return entry.get(field)

    def get_quality(self, entry):
        if entry.get('quality', eval_lazy=False):
            log.debug('Quality is already set to %s for %s, skipping quality detection.' %
                      (entry['quality'], entry['title']))
            return
        quality = qualities.Quality()
        for field_name in ['title', 'description']:
            if field_name not in entry:
                continue
            quality = qualities.Quality(entry[field_name])
            if quality:
                # if we find a quality in this field, stop searching
                break
        entry['quality'] = quality
        if quality:
            log.trace('Found quality %s (%s) for %s from field %s' %
                (entry['quality'], quality, entry['title'], field_name))


@event('plugin.register')
def register_plugin():
    plugin.register(MetainfoQuality, 'metainfo_quality', api_ver=2, builtin=True)

########NEW FILE########
__FILENAME__ = rottentomatoes_lookup
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.log import log_once

try:
    from flexget.plugins.api_rottentomatoes import lookup_movie, API_KEY
except ImportError:
    raise plugin.DependencyError(issued_by='rottentomatoes_lookup', missing='api_rottentomatoes',
                          message='rottentomatoes_lookup requires the `api_rottentomatoes` plugin')

log = logging.getLogger('rottentomatoes_lookup')


def get_rt_url(movie):
    for link in movie.links:
        if link.name == 'alternate':
            return link.url


class PluginRottenTomatoesLookup(object):
    """
    Retrieves Rotten Tomatoes information for entries.

    Example::
        rottentomatoes_lookup: yes
    """

    field_map = {
        'rt_name': 'title',
        'rt_id': 'id',
        'rt_year': 'year',
        'rt_genres': lambda movie: [genre.name for genre in movie.genres],
        'rt_mpaa_rating': 'mpaa_rating',
        'rt_runtime': 'runtime',
        'rt_critics_consensus': 'critics_consensus',
        'rt_releases': lambda movie: dict((release.name, release.date) for
            release in movie.release_dates),
        'rt_critics_rating': 'critics_rating',
        'rt_critics_score': 'critics_score',
        'rt_audience_rating': 'audience_rating',
        'rt_audience_score': 'audience_score',
        'rt_average_score': lambda movie: (movie.critics_score + movie.audience_score) / 2,
        'rt_synopsis': 'synopsis',
        'rt_posters': lambda movie: dict((poster.name, poster.url) for poster in movie.posters),
        'rt_actors': lambda movie: [actor.name for actor in movie.cast],
        'rt_directors': lambda movie: [director.name for director in movie.directors],
        'rt_studio': 'studio',
        'rt_alternate_ids': lambda movie: dict((alt_id.name, alt_id.id)
            for alt_id in movie.alternate_ids),
        'rt_url': get_rt_url,
        # Generic fields filled by all movie lookup plugins:
        'movie_name': 'title',
        'movie_year': 'year'}

    schema = {'oneOf': [
        {'type': 'boolean'},
        {'type': 'string', 'description': 'provide a custom api key'}
    ]}

    def lazy_loader(self, entry, field):
        """Does the lookup for this entry and populates the entry fields.

        :param entry: entry to perform lookup on
        :param field: the field to be populated (others may be populated as well)
        :returns: the field value

        """
        try:
            self.lookup(entry, key=self.key)
        except plugin.PluginError as e:
            log_once(e.value.capitalize(), logger=log)
            # Set all of our fields to None if the lookup failed
            entry.unregister_lazy_fields(self.field_map, self.lazy_loader)
        return entry[field]

    def lookup(self, entry, search_allowed=True, key=None):
        """
        Perform Rotten Tomatoes lookup for entry.

        :param entry: Entry instance
        :param search_allowed: Allow fallback to search
        :param key: optionally specify an API key to use
        :raises PluginError: Failure reason
        """
        if not key:
            key = API_KEY
        movie = lookup_movie(smart_match=entry['title'],
                             rottentomatoes_id=entry.get('rt_id', eval_lazy=False),
                             only_cached=(not search_allowed),
                             api_key=key
                             )
        log.debug(u'Got movie: %s' % movie)
        entry.update_using_map(self.field_map, movie)
        
        if not entry.get('imdb_id', eval_lazy=False):
            for alt_id in movie.alternate_ids:
                if alt_id.name == 'imdb':
                    entry['imdb_id'] = 'tt' + alt_id.id
                    break

    def on_task_metainfo(self, task, config):
        if not config:
            return

        if isinstance(config, basestring):
            self.key = config.lower()
        else:
            self.key = None

        for entry in task.entries:
            entry.register_lazy_fields(self.field_map, self.lazy_loader)

@event('plugin.register')
def register_plugin():
    plugin.register(PluginRottenTomatoesLookup, 'rottentomatoes_lookup', api_ver=2)

########NEW FILE########
__FILENAME__ = series
from __future__ import unicode_literals, division, absolute_import
import logging
from string import capwords
import re

from flexget import plugin
from flexget.event import event
from flexget.plugins.filter.series import populate_entry_fields
from flexget.utils.titles import SeriesParser
from flexget.utils.titles.parser import ParseWarning

log = logging.getLogger('metanfo_series')


class MetainfoSeries(object):
    """
    Check if entry appears to be a series, and populate series info if so.
    """

    schema = {'type': 'boolean'}

    # Run after series plugin so we don't try to re-parse it's entries
    @plugin.priority(120)
    def on_task_metainfo(self, task, config):
        # Don't run if we are disabled
        if config is False:
            return
        for entry in task.entries:
            # If series plugin already parsed this, don't touch it.
            if entry.get('series_name'):
                continue
            self.guess_entry(entry)

    def guess_entry(self, entry, allow_seasonless=False):
        """Populates series_* fields for entries that are successfully parsed."""
        if entry.get('series_parser') and entry['series_parser'].valid:
            # Return true if we already parsed this, false if series plugin parsed it
            return entry.get('series_guessed')
        parser = self.guess_series(entry['title'], allow_seasonless=allow_seasonless, quality=entry.get('quality'))
        if parser:
            populate_entry_fields(entry, parser)
            entry['series_guessed'] = True
            return True
        return False

    def guess_series(self, title, allow_seasonless=False, quality=None):
        """Returns a valid series parser if this `title` appears to be a series"""

        parser = SeriesParser(identified_by='auto', allow_seasonless=allow_seasonless)
        # We need to replace certain characters with spaces to make sure episode parsing works right
        # We don't remove anything, as the match positions should line up with the original title
        clean_title = re.sub('[_.,\[\]\(\):]', ' ', title)
        if parser.parse_unwanted(clean_title):
            return
        match = parser.parse_date(clean_title)
        if match:
            parser.identified_by = 'date'
        else:
            match = parser.parse_episode(clean_title)
            if match and parser.parse_unwanted(clean_title):
                return
            parser.identified_by = 'ep'
        if not match:
            return
        if match['match'].start() > 1:
            # We start using the original title here, so we can properly ignore unwanted prefixes.
            # Look for unwanted prefixes to find out where the series title starts
            start = 0
            prefix = re.match('|'.join(parser.ignore_prefixes), title)
            if prefix:
                start = prefix.end()
            # If an episode id is found, assume everything before it is series name
            name = title[start:match['match'].start()]
            # Remove possible episode title from series name (anything after a ' - ')
            name = name.split(' - ')[0]
            # Replace some special characters with spaces
            name = re.sub('[\._\(\) ]+', ' ', name).strip(' -')
            # Normalize capitalization to title case
            name = capwords(name)
            # If we didn't get a series name, return
            if not name:
                return
            parser.name = name
            parser.data = title
            try:
                parser.parse(data=title, quality=quality)
            except ParseWarning as pw:
                log.debug('ParseWarning: %s' % pw.value)
            if parser.valid:
                return parser


@event('plugin.register')
def register_plugin():
    plugin.register(MetainfoSeries, 'metainfo_series', api_ver=2)

########NEW FILE########
__FILENAME__ = subtitles_check
from __future__ import unicode_literals, division, absolute_import
import logging
import os
import tempfile

from flexget import plugin
from flexget.event import event

log = logging.getLogger('check_subtitles')


class MetainfoSubs(object):
    """
    Set 'subtitles' field for entries, if they are local video files with subs.
    The field is a list of language codes (3-letter ISO-639-3) for each subtitles 
    file found on disk and/or subs track found inside video (for MKVs).
    Special "und" code is for unidentified language (i.e. files without language 
    code before extension).
    """

    schema = {'type': 'boolean'}
    
    def on_task_start(self, task, config):
        try:
            import subliminal
        except ImportError as e:
            log.debug('Error importing Subliminal: %s' % e)
            raise plugin.DependencyError('subliminal', 'subliminal', 
                'Subliminal module required. ImportError: %s' % e)
        from dogpile.cache.exception import RegionAlreadyConfigured
        try:
            subliminal.cache_region.configure('dogpile.cache.dbm', 
                arguments={'filename': os.path.join(tempfile.gettempdir(), 'cachefile.dbm'), 
                           'lock_factory': subliminal.MutexLock})
        except RegionAlreadyConfigured:
            pass
        logging.getLogger("subliminal").setLevel(logging.CRITICAL)
        logging.getLogger("enzyme").setLevel(logging.WARNING)

    def on_task_metainfo(self, task, config):
        # check if explicitly disabled (value set to false)
        if config is False:
            return
        for entry in task.entries:
            entry.register_lazy_fields(['subtitles'], self.lazy_loader)

    def lazy_loader(self, entry, field):
        self.get_subtitles(entry)
        return entry.get(field)

    def get_subtitles(self, entry):
        if entry.get('subtitles', eval_lazy=False) or not ('location' in entry) or \
            ('$RECYCLE.BIN' in entry['location']) or not os.path.exists(entry['location']):
            return
        import subliminal
        try:
            video = subliminal.scan_video(entry['location'])
            lst = [l.alpha3 for l in video.subtitle_languages]
            if lst:
                entry['subtitles'] = lst
                log.trace('Found subtitles %s for %s' % ('/'.join(lst), entry['title']))
            else:
                entry.unregister_lazy_fields(['subtitles'], self.lazy_loader)
        except Exception as e:
            log.debug('Error checking local subtitles for %s: %s' % (entry['title'], e))


@event('plugin.register')
def register_plugin():
    plugin.register(MetainfoSubs, 'check_subtitles', api_ver=2)

########NEW FILE########
__FILENAME__ = task
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('metainfo_task')


class MetainfoTask(object):
    """
    Set 'task' field for entries.
    """

    schema = {'type': 'boolean'}

    def on_task_metainfo(self, task, config):
        # check if explicitly disabled (value set to false)
        if config is False:
            return

        for entry in task.entries:
            entry['task'] = task.name


@event('plugin.register')
def register_plugin():
    plugin.register(MetainfoTask, 'metainfo_task', api_ver=2, builtin=True)

########NEW FILE########
__FILENAME__ = thetvdb_lookup
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

try:
    from flexget.plugins.api_tvdb import lookup_series, lookup_episode, get_mirror
except ImportError:
    raise plugin.DependencyError(issued_by='thetvdb_lookup', missing='api_tvdb',
                                 message='thetvdb_lookup requires the `api_tvdb` plugin')

log = logging.getLogger('thetvdb_lookup')


class PluginThetvdbLookup(object):
    """Retrieves TheTVDB information for entries. Uses series_name,
    series_season, series_episode from series plugin.

    Example:

    thetvdb_lookup: yes

    Primarily used for passing thetvdb information to other plugins.
    Among these is the IMDB url for the series.

    This information is provided (via entry):
    series info:
      tvdb_series_name
      tvdb_rating
      tvdb_status (Continuing or Ended)
      tvdb_runtime (show runtime in minutes)
      tvdb_first_air_date
      tvdb_air_time
      tvdb_content_rating
      tvdb_genres
      tvdb_network
      tvdb_overview
      tvdb_banner_url
      tvdb_fanart_url
      tvdb_poster_url
      tvdb_airs_day_of_week
      tvdb_actors
      tvdb_language (en, fr, etc.)
      imdb_url (if available)
      zap2it_id (if available)
    episode info: (if episode is found)
      tvdb_ep_name
      tvdb_ep_overview
      tvdb_ep_directors
      tvdb_ep_writers
      tvdb_ep_air_date
      tvdb_ep_rating
      tvdb_ep_guest_stars
      tvdb_ep_image_url
    """

    # Series info
    series_map = {
        'tvdb_series_name': 'seriesname',
        'tvdb_rating': 'rating',
        'tvdb_status': 'status',
        'tvdb_runtime': 'runtime',
        'tvdb_first_air_date': 'firstaired',
        'tvdb_air_time': 'airs_time',
        'tvdb_content_rating': 'contentrating',
        'tvdb_genres': 'genre',
        'tvdb_network': 'network',
        'tvdb_overview': 'overview',
        'tvdb_banner_url': lambda series: series.banner and get_mirror('banner') + series.banner,
        'tvdb_fanart_url': lambda series: series.fanart and get_mirror('banner') + series.fanart,
        'tvdb_poster_url': lambda series: series.poster and get_mirror('banner') + series.poster,
        'tvdb_airs_day_of_week': 'airs_dayofweek',
        'tvdb_language': 'language',
        'imdb_url': lambda series: series.imdb_id and 'http://www.imdb.com/title/%s' % series.imdb_id,
        'imdb_id': 'imdb_id',
        'zap2it_id': 'zap2it_id',
        'tvdb_id': 'id'}
    # Episode info
    episode_map = {
        'tvdb_ep_name': 'episodename',
        'tvdb_ep_air_date': 'firstaired',
        'tvdb_ep_rating': 'rating',
        'tvdb_ep_image_url': lambda ep: ep.filename and get_mirror('banner') + ep.filename,
        'tvdb_ep_overview': 'overview',
        'tvdb_ep_writers': 'writer',
        'tvdb_ep_directors': 'director',
        'tvdb_ep_guest_stars': 'gueststars',
        'tvdb_absolute_number': 'absolute_number',
        'tvdb_season': 'seasonnumber',
        'tvdb_episode': 'episodenumber',
        'tvdb_ep_id': lambda ep: 'S%02dE%02d' % (ep.seasonnumber, ep.episodenumber)}

    def validator(self):
        from flexget import validator
        return validator.factory('boolean')

    def lazy_series_lookup(self, entry, field):
        """Does the lookup for this entry and populates the entry fields."""
        try:
            series = lookup_series(entry.get('series_name', eval_lazy=False), tvdb_id=entry.get('tvdb_id', eval_lazy=False))
            entry.update_using_map(self.series_map, series)
        except LookupError as e:
            log.debug('Error looking up tvdb series information for %s: %s' % (entry['title'], e.args[0]))
            entry.unregister_lazy_fields(self.series_map, self.lazy_series_lookup)
            # Also clear episode fields, since episode lookup cannot succeed without series lookup
            entry.unregister_lazy_fields(self.episode_map, self.lazy_episode_lookup)

        return entry[field]

    def lazy_episode_lookup(self, entry, field):
        try:
            season_offset = entry.get('thetvdb_lookup_season_offset', 0)
            episode_offset = entry.get('thetvdb_lookup_episode_offset', 0)
            if not isinstance(season_offset, int):
                log.error('thetvdb_lookup_season_offset must be an integer')
                season_offset = 0
            if not isinstance(episode_offset, int):
                log.error('thetvdb_lookup_episode_offset must be an integer')
                episode_offset = 0
            if season_offset != 0 or episode_offset != 0:
                log.debug('Using offset for tvdb lookup: season: %s, episode: %s' % (season_offset, episode_offset))

            lookupargs = {'name': entry.get('series_name', eval_lazy=False),
                          'tvdb_id': entry.get('tvdb_id', eval_lazy=False)}
            if entry['series_id_type'] == 'ep':
                lookupargs['seasonnum'] = entry['series_season'] + season_offset
                lookupargs['episodenum'] = entry['series_episode'] + episode_offset
            elif entry['series_id_type'] == 'sequence':
                lookupargs['absolutenum'] = entry['series_id'] + episode_offset
            elif entry['series_id_type'] == 'date':
                lookupargs['airdate'] = entry['series_date']
            episode = lookup_episode(**lookupargs)
            entry.update_using_map(self.episode_map, episode)
        except LookupError as e:
            log.debug('Error looking up tvdb episode information for %s: %s' % (entry['title'], e.args[0]))
            entry.unregister_lazy_fields(self.episode_map, self.lazy_episode_lookup)

        return entry[field]

    # Run after series and metainfo series
    @plugin.priority(110)
    def on_task_metainfo(self, task, config):
        if not config:
            return

        for entry in task.entries:
            # If there is information for a series lookup, register our series lazy fields
            if entry.get('series_name') or entry.get('tvdb_id', eval_lazy=False):
                entry.register_lazy_fields(self.series_map, self.lazy_series_lookup)

                # If there is season and ep info as well, register episode lazy fields
                if entry.get('series_id_type') in ('ep', 'sequence', 'date'):
                    entry.register_lazy_fields(self.episode_map, self.lazy_episode_lookup)
                # TODO: lookup for 'seq' and 'date' type series


@event('plugin.register')
def register_plugin():
    plugin.register(PluginThetvdbLookup, 'thetvdb_lookup', api_ver=2)

########NEW FILE########
__FILENAME__ = tmdb_lookup
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils import imdb
from flexget.utils.log import log_once

try:
    # TODO: Fix this after api_tmdb has module level functions
    from flexget.plugins.api_tmdb import ApiTmdb
    lookup = ApiTmdb.lookup
except ImportError:
    raise plugin.DependencyError(issued_by='tmdb_lookup', missing='api_tmdb')

log = logging.getLogger('tmdb_lookup')


class PluginTmdbLookup(object):
    """Retrieves tmdb information for entries.

    Example:
        tmdb_lookup: yes
    """

    field_map = {
        'tmdb_name': 'name',
        'tmdb_id': 'id',
        'imdb_id': 'imdb_id',
        'tmdb_year': 'year',
        'tmdb_popularity': 'popularity',
        'tmdb_rating': 'rating',
        'tmdb_genres': lambda movie: [genre.name for genre in movie.genres],
        'tmdb_released': 'released',
        'tmdb_votes': 'votes',
        'tmdb_certification': 'certification',
        'tmdb_posters': lambda movie: [poster.url for poster in movie.posters],
        'tmdb_runtime': 'runtime',
        'tmdb_tagline': 'tagline',
        'tmdb_budget': 'budget',
        'tmdb_revenue': 'revenue',
        'tmdb_homepage': 'homepage',
        'tmdb_trailer': 'trailer',
        # Generic fields filled by all movie lookup plugins:
        'movie_name': 'name',
        'movie_year': 'year'}

    def validator(self):
        from flexget import validator
        return validator.factory('boolean')

    def lazy_loader(self, entry, field):
        """Does the lookup for this entry and populates the entry fields."""
        imdb_id = (entry.get('imdb_id', eval_lazy=False) or
                   imdb.extract_id(entry.get('imdb_url', eval_lazy=False)))
        try:
            movie = lookup(smart_match=entry['title'],
                           tmdb_id=entry.get('tmdb_id', eval_lazy=False),
                           imdb_id=imdb_id)
            entry.update_using_map(self.field_map, movie)
        except LookupError:
            log_once('TMDB lookup failed for %s' % entry['title'], log, logging.WARN)
            # Set all of our fields to None if the lookup failed
            entry.unregister_lazy_fields(self.field_map, self.lazy_loader)
        return entry[field]

    def lookup(self, entry):
        """
        Populates all lazy fields to an Entry. May be called by other plugins
        requiring tmdb info on an Entry

        :param entry: Entry instance
        """
        entry.register_lazy_fields(self.field_map, self.lazy_loader)

    def on_task_metainfo(self, task, config):
        if not config:
            return
        for entry in task.entries:
            self.lookup(entry)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginTmdbLookup, 'tmdb_lookup', api_ver=2)

########NEW FILE########
__FILENAME__ = torrent_size
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('torrent_size')


class TorrentSize(object):
    """
    Provides file size information when dealing with torrents
    """

    @plugin.priority(200)
    def on_task_modify(self, task, config):
        for entry in task.entries:
            if 'torrent' in entry:
                size = entry['torrent'].size / 1024 / 1024
                log.debug('%s size: %s MB' % (entry['title'], size))
                entry['content_size'] = size


@event('plugin.register')
def register_plugin():
    plugin.register(TorrentSize, 'torrent_size', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = trakt_collected_lookup
from __future__ import unicode_literals, division, absolute_import
import hashlib
import logging

from requests import RequestException

from flexget import plugin
from flexget.event import event
from flexget.utils import json

log = logging.getLogger('trakt_collected')


class TraktCollected(object):
    """
    Query trakt.tv for episodes in the user collection to set the trakt_in_collection flag on entries.
    Uses tvdb_id or imdb_id or series_name, plus series_season and series_episode fields (metainfo_series and 
    thetvdb_lookup or trakt_lookup plugins will do).
    """

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'api_key': {'type': 'string'}
        },
        'required': ['username', 'api_key'],
        'additionalProperties': False
    }
    
    # Run after metainfo_series and thetvdb_lookup
    @plugin.priority(100)
    def on_task_metainfo(self, task, config):
        if not task.entries:
            return
        url = 'http://api.trakt.tv/user/library/shows/collection.json/%s/%s' % \
            (config['api_key'], config['username'])
        auth = None
        if 'password' in config:
            auth = {'username': config['username'],
                    'password': hashlib.sha1(config['password']).hexdigest()}
        try:
            log.debug('Opening %s' % url)
            data = task.requests.get(url, data=json.dumps(auth)).json()
        except RequestException as e:
            raise plugin.PluginError('Unable to get data from trakt.tv: %s' % e)

        def check_auth():
            if task.requests.post('http://api.trakt.tv/account/test/' + config['api_key'],
                                  data=json.dumps(auth), raise_status=False).status_code != 200:
                raise plugin.PluginError('Authentication to trakt failed.')

        if not data:
            check_auth()
            self.log.warning('No data returned from trakt.')
            return
        if 'error' in data:
            check_auth()
            raise plugin.PluginError('Error getting trakt list: %s' % data['error'])
        log.verbose('Received %d series records from trakt.tv' % len(data))
        # the index will speed the work if we have a lot of entries to check
        index = {}
        for idx, val in enumerate(data):
            index[val['title']] = index[int(val['tvdb_id'])] = index[val['imdb_id']] = idx
        for entry in task.entries:
            if not (entry.get('series_name') and entry.get('series_season') and entry.get('series_episode')):
                continue
            entry['trakt_in_collection'] = False
            if 'tvdb_id' in entry and entry['tvdb_id'] in index:
                series = data[index[entry['tvdb_id']]]
            elif 'imdb_id' in entry and entry['imdb_id'] in index:
                series = data[index[entry['imdb_id']]]
            elif 'series_name' in entry and entry['series_name'] in index:
                series = data[index[entry['series_name']]]
            else:
                continue
            for s in series['seasons']:
                if s['season'] == entry['series_season']:
                    entry['trakt_in_collection'] = entry['series_episode'] in s['episodes']
                    break
            log.debug('The result for entry "%s" is: %s' % (entry['title'], 
                'Owned' if entry['trakt_in_collection'] else 'Not owned'))


@event('plugin.register')
def register_plugin():
    plugin.register(TraktCollected, 'trakt_collected_lookup', api_ver=2)

########NEW FILE########
__FILENAME__ = trakt_lookup
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

try:
    from flexget.plugins.api_trakt import ApiTrakt
    lookup_series = ApiTrakt.lookup_series
    lookup_episode = ApiTrakt.lookup_episode
except ImportError:
    raise plugin.DependencyError(issued_by='trakt_lookup', missing='api_trakt',
                                 message='trakt_lookup requires the `api_trakt` plugin')

from flexget import plugin

log = logging.getLogger('trakt_lookup')


class PluginTraktLookup(object):
    """Retrieves trakt information for entries. Uses series_name,
    series_season, series_episode from series plugin.

    Example:

    trakt_lookup: yes

    Primarily used for passing trakt information to other plugins.
    Among these is the IMDB url for the series.

    This information is provided (via entry):
    series info:
    trakt_series_name
    trakt_series_runtime
    trakt_series_first_aired_epoch
    trakt_series_first_aired_iso
    trakt_series_air_time
    trakt_series_content_rating
    trakt_series_genres
    trakt_sereis_banner_url
    trakt_sereis_fanart_url
    trakt_series_imdb_url
    trakt_series_trakt_url
    trakt_series_imdb_id
    trakt_series_tvdb_id
    trakt_series_actors
    trakt_series_country
    trakt_series_year
    trakt_series_tvrage_id
    trakt_series_status
    trakt_series_overview

    trakt_ep_name
    trakt_ep_season
    trakt_ep_number
    trakt_ep_overview
    trakt_ep_first_aired_epoch
    trakt_ep_first_aired_iso
    trakt_ep_image_url
    trakt_ep_id
    trakt_ep_tvdb_id

  """

  # Series info
    series_map = {
        'trakt_series_name': 'title',
        'trakt_series_runtime': 'runtime',
        'trakt_series_first_aired_epoch': 'first_aired',
        'trakt_series_first_aired_iso': 'first_aired_iso',
        'trakt_series_air_time': 'air_time',
        'trakt_series_content_rating': 'certification',
        'trakt_series_genres': lambda series: [genre.name for genre in series.genre],
        'trakt_series_network': 'network',
        'trakt_series_banner_url': 'banner',
        'trakt_series_fanart_url': 'fanart',
        'trakt_series_poster_url': 'poster',
        'imdb_url': lambda series: series.imdb_id and 'http://www.imdb.com/title/%s' % series.imdb_id,
        'trakt_series_url': 'url',
        'trakt_series_imdb_id': 'imdb_id',
        'trakt_series_tvdb_id': 'tvdb_id',
        'trakt_series_actors': lambda series: [actors.name for actors in series.actors],
        'trakt_series_country': 'country',
        'trakt_series_year': 'year',
        'trakt_series_tvrage_id': 'tvrage_id',
        'trakt_series_status': 'status',
        'trakt_series_overview': 'overview'}

  # Episode info
    episode_map = {
        'trakt_ep_name': 'episode_name',
        'trakt_ep_first_aired_epoch': 'first_aired',
        'trakt_ep_first_aired_iso': 'first_aired_iso',
        'trakt_ep_image_url': 'screen',
        'trakt_ep_overview': 'overview',
        'trakt_season': 'season',
        'trakt_episode': 'number',
        'trakt_ep_id': lambda ep: 'S%02dE%02d' % (ep.season, ep.number),
        'trakt_ep_tvdb_id': 'tvdb_id'}

    def validator(self):
        from flexget import validator
        return validator.factory('boolean')

    def lazy_series_lookup(self, entry, field):
        """Does the lookup for this entry and populates the entry fields."""
        try:
            series = lookup_series(entry.get('series_name', eval_lazy=False),
                                   tvdb_id=entry.get('tvdb_id', eval_lazy=False))
            entry.update_using_map(self.series_map, series)
        except LookupError as e:
            log.debug(e.message)
            entry.unregister_lazy_fields(self.series_map, self.lazy_series_lookup)
            # Also clear episode fields, since episode lookup cannot succeed without series lookup
            entry.unregister_lazy_fields(self.episode_map, self.lazy_episode_lookup)
        return entry[field]

    def lazy_episode_lookup(self, entry, field):
        try:

            lookupargs = {'title': entry.get('series_name', eval_lazy=False),
                          'tvdb_id': entry.get('tvdb_id', eval_lazy=False)}
            if entry['series_id_type'] == 'ep':
                lookupargs['seasonnum'] = entry['series_season']
                lookupargs['episodenum'] = entry['series_episode']
            elif entry['series_id_type'] == 'sequence':
                log.error('Trakt only accepts episode sequences in the format of Season/Episode')
            elif entry['series_id_type'] == 'date':
                log.error('Trakt only accepts episode sequences in the format of Season/Episode')
            episode = lookup_episode(**lookupargs)
            entry.update_using_map(self.episode_map, episode)
        except LookupError as e:
            log.debug('Error looking up trakt episode information for %s: %s' % (entry['title'], e.args[0]))
            entry.unregister_lazy_fields(self.episode_map, self.lazy_episode_lookup)
        return entry[field]

    # Run after series and metainfo series
    @plugin.priority(110)
    def on_task_metainfo(self, task, config):
        if not config:
            return

        for entry in task.entries:

            if entry.get('series_name') or entry.get('tvdb_id', eval_lazy=False):
                entry.register_lazy_fields(self.series_map, self.lazy_series_lookup)

                if entry.get('series_id_type') in ('ep', 'sequence', 'date'):
                    entry.register_lazy_fields(self.episode_map, self.lazy_episode_lookup)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginTraktLookup, 'trakt_lookup', api_ver=2)

########NEW FILE########
__FILENAME__ = trakt_watched_lookup
from __future__ import unicode_literals, division, absolute_import
import hashlib
import logging

from requests import RequestException

from flexget import plugin
from flexget.event import event
from flexget.utils import json

log = logging.getLogger('trakt_watched')


class TraktWatched(object):
    """
    Query trakt.tv for watched episodes to set the trakt_watched flag on entries.
    Uses tvdb_id or imdb_id or series_name, plus series_season and series_episode 
    fields (metainfo_series and thetvdb_lookup or trakt_lookup plugins will do).
    
    Example task:
    
      Purge watched episodes:
        find:
          path:
            - D:\Media\Incoming\series
          regexp: '.*\.(avi|mkv|mp4)$'
          recursive: yes
        metainfo_series: yes
        thetvdb_lookup: yes
        trakt_watched_lookup:
          username: xxx
          password: xxx
          api_key: xxx
        if:
          - trakt_watched: accept
        move:
          to: "D:\\Media\\Purge\\{{ tvdb_series_name|default(series_name) }}"
          clean_source: 10
    """

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'api_key': {'type': 'string'}
        },
        'required': ['username', 'api_key'],
        'additionalProperties': False
    }
    
    # Run after metainfo_series and thetvdb_lookup
    @plugin.priority(100)
    def on_task_metainfo(self, task, config):
        if not task.entries:
            return
        url = 'http://api.trakt.tv/user/library/shows/watched.json/%s/%s' % \
            (config['api_key'], config['username'])
        auth = None
        if 'password' in config:
            auth = {'username': config['username'],
                    'password': hashlib.sha1(config['password']).hexdigest()}
        try:
            log.debug('Opening %s' % url)
            data = task.requests.get(url, data=json.dumps(auth)).json()
        except RequestException as e:
            raise plugin.PluginError('Unable to get data from trakt.tv: %s' % e)

        def check_auth():
            if task.requests.post('http://api.trakt.tv/account/test/' + config['api_key'],
                                  data=json.dumps(auth), raise_status=False).status_code != 200:
                raise plugin.PluginError('Authentication to trakt failed.')

        if not data:
            check_auth()
            self.log.warning('No data returned from trakt.')
            return
        if 'error' in data:
            check_auth()
            raise plugin.PluginError('Error getting trakt list: %s' % data['error'])
        log.verbose('Received %d series records from trakt.tv' % len(data))
        # the index will speed the work if we have a lot of entries to check
        index = {}
        for idx, val in enumerate(data):
            index[val['title']] = index[int(val['tvdb_id'])] = index[val['imdb_id']] = idx
        for entry in task.entries:
            if not (entry.get('series_name') and entry.get('series_season') and entry.get('series_episode')):
                continue
            entry['trakt_watched'] = False
            if 'tvdb_id' in entry and entry['tvdb_id'] in index:
                series = data[index[entry['tvdb_id']]]
            elif 'imdb_id' in entry and entry['imdb_id'] in index:
                series = data[index[entry['imdb_id']]]
            elif 'series_name' in entry and entry['series_name'] in index:
                series = data[index[entry['series_name']]]
            else:
                continue
            for s in series['seasons']:
                if s['season'] == entry['series_season']:
                    entry['trakt_watched'] = entry['series_episode'] in s['episodes']
                    break
            log.debug('The result for entry "%s" is: %s' % (entry['title'], 
                'Watched' if entry['trakt_watched'] else 'Unwatched'))


@event('plugin.register')
def register_plugin():
    plugin.register(TraktWatched, 'trakt_watched_lookup', api_ver=2)

########NEW FILE########
__FILENAME__ = extension
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('extension')


class ModifyExtension(object):

    """
        Allows specifying file extension explicitly when all other built-in detection mechanisms fail.

        Example:

        extension: nzb
    """

    schema = {'type': ['string', 'number']}

    def on_task_modify(self, task, config):
        ext = unicode(config)
        if ext.startswith('.'):
            ext = ext[1:]

        for entry in task.entries:
            log.debug('`%s` filename is `%s`' % (entry['title'], entry.get('filename', 'N/A')))
            entry['filename'] = '%s.%s' % (entry.get('filename', entry['title']), ext)
            log.debug('filename is now `%s`' % entry['filename'])

@event('plugin.register')
def register_plugin():
    plugin.register(ModifyExtension, 'extension', api_ver=2)

########NEW FILE########
__FILENAME__ = headers
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib2

from flexget import plugin
from flexget.event import event

log = logging.getLogger('headers')


class HTTPHeadersProcessor(urllib2.BaseHandler):

    # run first
    handler_order = urllib2.HTTPHandler.handler_order - 10

    def __init__(self, headers=None):
        if headers:
            self.headers = headers
        else:
            self.headers = {}

    def http_request(self, request):
        for name, value in self.headers.iteritems():
            if not request.has_header(name):
                log.debug('Adding %s: %s' % (name, value))
                request.add_unredirected_header(name.capitalize(), value.strip())
        return request

    def http_response(self, request, response):
        return response

    https_request = http_request
    https_response = http_response


class PluginHeaders(object):
    """Allow setting up any headers in all requests (which use urllib2)

    Example:

    headers:
      cookie: uid=<YOUR UID>; pass=<YOUR PASS>
    """

    schema = {'type': 'object', 'additionalProperties': {'type': 'string'}}

    @plugin.priority(130)
    def on_task_start(self, task, config):
        """Task starting"""
        # Set the headers for this task's request session
        if task.requests.headers:
            task.requests.headers.update(config)
        else:
            task.requests.headers = config
        # Set the headers in urllib2 for backwards compatibility
        if urllib2._opener:
            log.debug('Adding HTTPHeadersProcessor to default opener')
            urllib2._opener.add_handler(HTTPHeadersProcessor(config))
        else:
            log.debug('Creating new opener and installing it')
            opener = urllib2.build_opener(HTTPHeadersProcessor(config))
            urllib2.install_opener(opener)

    def on_task_exit(self, task, config):
        """Task exiting, remove additions"""
        if urllib2._opener:
            log.debug('Removing urllib2 default opener')
            # TODO: this uninstalls all other handlers as well, but does it matter?
            urllib2.install_opener(None)

    on_task_abort = on_task_exit


@event('plugin.register')
def register_plugin():
    plugin.register(PluginHeaders, 'headers', api_ver=2)

########NEW FILE########
__FILENAME__ = manipulate
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from flexget import plugin
from flexget.event import event

log = logging.getLogger('manipulate')


class Manipulate(object):
    """
    Usage:

      manipulate:
        - <destination field>:
            [phase]: <phase>
            [from]: <source field>
            [extract]: <regexp>
            [separator]: <text>
            [replace]:
              regexp: <regexp>
              format: <regexp>
            [remove]: <boolean>

    Example:

      manipulate:
        - title:
            extract: \[\d\d\d\d\](.*)
    """

    def validator(self):
        from flexget import validator
        root = validator.factory()
        bundle = root.accept('list').accept('dict')
        # prevent invalid indentation level
        bundle.reject_keys(['from', 'extract', 'replace', 'phase'],
            'Option \'$key\' has invalid indentation level. It needs 2 more spaces.')
        edit = bundle.accept_any_key('dict')
        edit.accept('choice', key='phase').accept_choices(['metainfo', 'filter'], ignore_case=True)
        edit.accept('text', key='from')
        edit.accept('regexp', key='extract')
        edit.accept('text', key='separator')
        edit.accept('boolean', key='remove')
        replace = edit.accept('dict', key='replace')
        replace.accept('regexp', key='regexp', required=True)
        replace.accept('text', key='format', required=True)
        return root

    def on_task_start(self, task, config):
        """
        Separates the config into a dict with a list of jobs per phase.
        Allows us to skip phases without any jobs in them.
        """
        self.phase_jobs = {'filter': [], 'metainfo': []}
        for item in config:
            for item_config in item.itervalues():
                # Get the phase specified for this item, or use default of metainfo
                phase = item_config.get('phase', 'metainfo')
                self.phase_jobs[phase].append(item)

    @plugin.priority(255)
    def on_task_metainfo(self, task, config):
        if not self.phase_jobs['metainfo']:
            # return if no jobs for this phase
            return
        modified = sum(self.process(entry, self.phase_jobs['metainfo']) for entry in task.entries)
        log.verbose('Modified %d entries.' % modified)

    @plugin.priority(255)
    def on_task_filter(self, task, config):
        if not self.phase_jobs['filter']:
            # return if no jobs for this phase
            return
        modified = sum(self.process(entry, self.phase_jobs['filter']) for entry in task.entries + task.rejected)
        log.verbose('Modified %d entries.' % modified)

    def process(self, entry, jobs):
        """Process given jobs from config for an entry.

        :param entry: Entry to modify
        :param jobs: Config items to run on this entry
        :return: True if any fields were modified
        """

        modified = False
        for item in jobs:
            for field, config in item.iteritems():
                from_field = field
                if 'from' in config:
                    from_field = config['from']
                field_value = entry.get(from_field)
                log.debug('field: `%s` from_field: `%s` field_value: `%s`' % (field, from_field, field_value))

                if config.get('remove'):
                    if field in entry:
                        del entry[field]
                        modified = True
                    continue

                if 'extract' in config:
                    if not field_value:
                        log.warning('Cannot extract, field `%s` is not present' % from_field)
                        continue
                    match = re.search(config['extract'], field_value, re.I | re.U)
                    if match:
                        groups = [x for x in match.groups() if x is not None]
                        log.debug('groups: %s' % groups)
                        field_value = config.get('separator', ' ').join(groups).strip()
                        log.debug('field `%s` after extract: `%s`' % (field, field_value))

                if 'replace' in config:
                    if not field_value:
                        log.warning('Cannot replace, field `%s` is not present' % from_field)
                        continue
                    replace_config = config['replace']
                    regexp = re.compile(replace_config['regexp'], flags=re.I | re.U)
                    field_value = regexp.sub(replace_config['format'], field_value).strip()
                    log.debug('field `%s` after replace: `%s`' % (field, field_value))

                if from_field != field or entry[field] != field_value:
                    log.verbose('Field `%s` is now `%s`' % (field, field_value))
                    modified = True
                entry[field] = field_value
        return modified

@event('plugin.register')
def register_plugin():
    plugin.register(Manipulate, 'manipulate', api_ver=2)

########NEW FILE########
__FILENAME__ = path_by_ext
from __future__ import unicode_literals, division, absolute_import
import logging
import mimetypes

from flexget import plugin
from flexget.event import event

log = logging.getLogger('path_by_ext')


class PluginPathByExt(object):
    """
        Allows specifying path based on content-type

        Example:

        path_by_ext:
          torrent: ~/watch/torrent/
          nzb: ~/watch/nzb/
    """

    schema = {'type': 'object'}

    def on_task_modify(self, task, config):
        self.ext(task, config, self.set_path)

    def set_path(self, entry, path):
        log.debug('Setting %s path to %s' % (entry['title'], path))
        entry['path'] = path

    def ext(self, task, config, callback):
        for entry in task.entries:
            if 'mime-type' in entry:
                # check if configuration has mimetype that entry has
                if entry['mime-type'] in config:
                    callback(entry, config[entry['mime-type']])
                # check if entry mimetype extension matches in config
                ext = mimetypes.types_map.get(entry['mime-type'])
                path = config.get(ext) or config.get(ext[1:])
                if path:
                    callback(entry, path)
                else:
                    log.debug('Unknown mimetype %s' % entry['mime-type'])
            else:
                # try to find from url
                for ext, path in config.iteritems():
                    if entry['url'].endswith('.' + ext):
                        callback(entry, path)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginPathByExt, 'path_by_ext', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_priority
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('p_priority')


class PluginPriority(object):

    """
        Allows modifying plugin priorities from default values.

        Example:

        plugin_priority:
          ignore: 50
          series: 100
    """

    schema = {'type': 'object', 'additionalProperties': {'type': 'integer'}}

    def __init__(self):
        self.priorities = {}

    def on_task_start(self, task, config):
        self.priorities = {}
        names = []
        for name, priority in config.iteritems():
            names.append(name)
            originals = self.priorities.setdefault(name, {})
            for phase, event in plugin.plugins[name].phase_handlers.iteritems():
                originals[phase] = event.priority
                log.debug('stored %s original value %s' % (phase, event.priority))
                event.priority = priority
                log.debug('set %s new value %s' % (phase, priority))
        log.debug('Changed priority for: %s' % ', '.join(names))

    def on_task_exit(self, task, config):
        if not self.priorities:
            log.debug('nothing changed, aborting restore')
            return
        names = []
        for name in config.keys():
            names.append(name)
            originals = self.priorities[name]
            for phase, priority in originals.iteritems():
                plugin.plugins[name].phase_handlers[phase].priority = priority
        log.debug('Restored priority for: %s' % ', '.join(names))
        self.priorities = {}

    on_task_abort = on_task_exit

@event('plugin.register')
def register_plugin():
    plugin.register(PluginPriority, 'plugin_priority', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_rutracker
# coding=utf-8
from __future__ import unicode_literals, division, absolute_import
import json
import logging
import itertools
from time import sleep
from datetime import datetime, timedelta

from sqlalchemy import Column, Unicode, Integer, DateTime
from sqlalchemy.types import TypeDecorator, VARCHAR

import re
from flexget import plugin
from flexget.event import event
from flexget.db_schema import versioned_base
from flexget.plugin import PluginError
from flexget.manager import Session
from requests import post
from requests.auth import AuthBase
from requests.cookies import cookiejar_from_dict
from requests.utils import dict_from_cookiejar


__author__ = 'asm0dey'

log = logging.getLogger('rutracker_auth')
Base = versioned_base('rutracker_auth', 0)


class JSONEncodedDict(TypeDecorator):

    """Represents an immutable structure as a json-encoded string.

    Usage:

        JSONEncodedDict(255)

    """

    impl = VARCHAR

    def process_bind_param(self, value, dialect):
        if value is not None:
            value = json.dumps(value)

        return value

    def process_result_value(self, value, dialect):
        if value is not None:
            value = json.loads(value)
        return value


class RutrackerAccount(Base):
    __tablename__ = 'rutracker_accoounts'
    id = Column(Integer, primary_key=True, autoincrement=True, nullable=False)
    login = Column(Unicode, index=True)
    cookies = Column(JSONEncodedDict)
    expiry_time = Column(DateTime)


class RutrackerAuth(AuthBase):

    """Supports downloading of torrents from 'rutracker' tracker
       if you pass cookies (CookieJar) to constructor then authentication will be bypassed and cookies will be just set
    """

    def try_authenticate(self, payload):
        for _ in itertools.repeat(None, 5):
            auth_response = post("http://login.rutracker.org/forum/login.php", data=payload,
                                 cookies=cookiejar_from_dict({'spylog_test': '1'}))
            if auth_response.cookies and len(auth_response.cookies) > 0:
                return auth_response
            else:
                sleep(3)
        raise PluginError('unable to obtain cookies from rutracker')

    def __init__(self, login, password, cookies=None, db_session=None):
        if cookies is None:
            log.debug('rutracker cookie not found. Requesting new one')
            payload_ = {'login_username': login,
                        'login_password': password, 'login': 'Вход'}
            auth_response = self.try_authenticate(payload_)
            self.cookies_ = auth_response.cookies
            if db_session:
                db_session.add(
                    RutrackerAccount(
                        login=login, cookies=dict_from_cookiejar(
                            self.cookies_),
                        expiry_time=datetime.now() + timedelta(days=1)))
                db_session.commit()
            else:
                raise ValueError(
                    'db_session can not be None if cookies is None')
        else:
            log.debug('Using previously saved cookie')
            self.cookies_ = cookies

    def __call__(self, r):
        url = r.url
        id = re.findall(r'\d+', url)[0]
        data = 't=' + id
        headers = {
            'referer': "http://rutracker.org/forum/viewtopic.php?t=" + id,
            "Content-Type": "application/x-www-form-urlencoded", "t": id, 'Origin': 'http://rutracker.org',
            'Accept-Encoding': 'gzip,deflate,sdch'}
        r.prepare_body(data=data, files=None)
        r.prepare_method('POST')
        r.prepare_url(
            url='http://dl.rutracker.org/forum/dl.php?t=' + id, params=None)
        r.prepare_headers(headers)
        r.prepare_cookies(self.cookies_)
        return r


class RutrackerModify(object):

    """Usage:

    rutracker_auth:
      username: 'username_here'
      password: 'password_here'
    """
    schema = {'type': 'object',
              'properties': {
                  'username': {'type': 'string'},
                  'password': {'type': 'string'}
              },
              "additionalProperties": False}

    auth_cache = {}

    @plugin.priority(127)
    def on_task_urlrewrite(self, task, config):
        username = config['username']
        db_session = Session()
        cookies = self.try_find_cookie(db_session, username)
        if not username in self.auth_cache:
            auth_handler = RutrackerAuth(
                username, config['password'], cookies, db_session)
            self.auth_cache[username] = auth_handler
        else:
            auth_handler = self.auth_cache[username]
        for entry in task.accepted:
            if entry['url'].startswith('http://rutracker.org/forum/viewtopic.php'):
                entry['download_auth'] = auth_handler

    def try_find_cookie(self, db_session, username):
        account = db_session.query(RutrackerAccount).filter(
            RutrackerAccount.login == username).first()
        if account:
            if account.expiry_time < datetime.now():
                db_session.delete(account)
                db_session.commit()
                return None
            return account.cookies
        else:
            return None


@event('plugin.register')
def register_plugin():
    plugin.register(RutrackerModify, 'rutracker_auth', api_ver=2)

########NEW FILE########
__FILENAME__ = set_field
from __future__ import unicode_literals, division, absolute_import
from copy import copy
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.template import RenderError

log = logging.getLogger('set')

PRIORITY_LAST = -255


class ModifySet(object):

    """Allows adding information to a task entry for use later.

    Example:

    set:
      path: ~/download/path/
    """

    def __init__(self):
        self.keys = {}
        try:
            from jinja2 import Environment
        except ImportError:
            self.jinja = False
        else:
            self.jinja = True

    def validator(self):
        from flexget import validator
        v = validator.factory('dict')
        v.accept_any_key('any')
        return v

    def on_task_start(self, task, config):
        """Checks that jinja2 is available"""
        if not self.jinja:
            log.warning("jinja2 module is not available, set plugin will only work with python string replacement.")

    # Filter priority is -255 so we run after all filters are finished
    @plugin.priority(PRIORITY_LAST)
    def on_task_filter(self, task, config):
        """Adds the set dict to all accepted entries."""
        # TODO: This is ugly, maybe we should only run on accepted entries all the time, or have an option to run on all
        if plugin.get_plugin_by_name('set').phase_handlers['filter'].priority == PRIORITY_LAST:
            # If priority is last only run on accepted entries to prevent unneeded lazy lookups
            log.debug('Set plugin at default priority, only running on accepted entries.')
            entries = task.accepted
        else:
        # If the priority has been modified to run before last, run on all entries
            log.debug('Set plugin\'s priority has been altered, running on all entries.')
            entries = task.entries

        for entry in entries:
            self.modify(entry, config)

    def modify(self, entry, config, errors=True):
        """This can be called from a plugin to add set values to an entry"""

        # Create a new dict so we don't overwrite the set config with string replaced values.
        conf = copy(config)

        # Do jinja2 rendering/string replacement
        for field, value in conf.items():
            if isinstance(value, basestring):
                logger = log.error if errors else log.debug
                try:
                    conf[field] = entry.render(value)
                except RenderError as e:
                    logger('Could not set %s for %s: %s' % (field, entry['title'], e))
                    # If the replacement failed, remove this key from the update dict
                    del conf[field]

        # If there are valid items in the config, apply to entry.
        if conf:
            log.debug('adding set: info to entry:\'%s\' %s' % (entry['title'], conf))
            entry.update(conf)

@event('plugin.register')
def register_plugin():
    plugin.register(ModifySet, 'set', api_ver=2)

########NEW FILE########
__FILENAME__ = torrent
from __future__ import unicode_literals, division, absolute_import
import logging
import os

from flexget import plugin
from flexget.event import event
from flexget.utils.bittorrent import Torrent, is_torrent_file

log = logging.getLogger('modif_torrent')


class TorrentFilename(object):
    """
        Makes sure that entries containing torrent-file have .torrent
        extension. This is enabled always by default (builtins).
    """
    TORRENT_PRIO = 255

    @plugin.priority(TORRENT_PRIO)
    def on_task_modify(self, task, config):
        # Only scan through accepted entries, as the file must have been downloaded in order to parse anything
        for entry in task.accepted:
            # skip if entry does not have file assigned
            if not 'file' in entry:
                log.trace('%s doesn\'t have a file associated' % entry['title'])
                continue
            if not os.path.exists(entry['file']):
                entry.fail('File %s does not exists' % entry['file'])
                continue
            if os.path.getsize(entry['file']) == 0:
                entry.fail('File %s is 0 bytes in size' % entry['file'])
                continue
            if not is_torrent_file(entry['file']):
                continue
            log.debug('%s seems to be a torrent' % entry['title'])

            # create torrent object from torrent
            try:
                with open(entry['file'], 'rb') as f:
                    # NOTE: this reads entire file into memory, but we're pretty sure it's
                    # a small torrent file since it starts with TORRENT_RE
                    data = f.read()

                if 'content-length' in entry:
                    if len(data) != entry['content-length']:
                        entry.fail('Torrent file length doesn\'t match to the one reported by the server')
                        self.purge(entry)
                        continue

                # construct torrent object
                try:
                    torrent = Torrent(data)
                except SyntaxError as e:
                    entry.fail('%s - broken or invalid torrent file received' % e.message)
                    self.purge(entry)
                    continue

                entry['torrent'] = torrent
                entry['torrent_info_hash'] = torrent.info_hash
                # if we do not have good filename (by download plugin)
                # for this entry, try to generate one from torrent content
                if entry.get('filename'):
                    if not entry['filename'].lower().endswith('.torrent'):
                        # filename present but without .torrent extension, add it
                        entry['filename'] += '.torrent'
                else:
                    # generate filename from torrent or fall back to title plus extension
                    entry['filename'] = self.make_filename(torrent, entry)
            except Exception as e:
                log.exception(e)

    @plugin.priority(TORRENT_PRIO)
    def on_task_output(self, task, config):
        for entry in task.entries:
            if 'torrent' in entry:
                if entry['torrent'].modified:
                    # re-write data into a file
                    log.debug('Writing modified torrent file for %s' % entry['title'])
                    with open(entry['file'], 'wb+') as f:
                        f.write(entry['torrent'].encode())

    def make_filename(self, torrent, entry):
        """Build a filename for this torrent"""

        title = entry['title']
        files = torrent.get_filelist()
        if len(files) == 1:
            # single file, if filename is longer than title use it
            fn = files[0]['name']
            if len(fn) > len(title):
                title = fn[:fn.rfind('.')]

        # neatify title
        title = title.replace('/', '_')
        title = title.replace(' ', '_')
        title = title.replace('\u200b', '')

        # title = title.encode('iso8859-1', 'ignore') # Damn \u200b -character, how I loathe thee
        # TODO: replace only zero width spaces, leave unicode alone?

        fn = '%s.torrent' % title
        log.debug('make_filename made %s' % fn)
        return fn

    def purge(self, entry):
        if os.path.exists(entry['file']):
            log.debug('removing temp file %s from %s' % (entry['file'], entry['title']))
            os.remove(entry['file'])
        del(entry['file'])


@event('plugin.register')
def register_plugin():
    plugin.register(TorrentFilename, 'torrent', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = torrent_scrub
""" Torrent Scrubber Plugin.
"""
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin, validator
from flexget.event import event
from flexget.plugins.modify.torrent import TorrentFilename
from flexget.utils import bittorrent

log = logging.getLogger('torrent_scrub')


class TorrentScrub(object):
    """ Scrubs torrents from unwanted keys.

        Example:
            tasks:
              rutorrent-fast-resume-infected-task:
                torrent_scrub: resume
    """
    # Scrub at high level, but BELOW "torrent"
    SCRUB_PRIO = TorrentFilename.TORRENT_PRIO - 10

    # Scrubbing modes
    SCRUB_MODES = ("off", "on", "all", "resume", "rtorrent",)

    # Keys of rTorrent / ruTorrent session data
    RT_KEYS = ("libtorrent_resume", "log_callback", "err_callback", "rtorrent")

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {'type': 'string', 'enum': list(SCRUB_MODES)},
            {'type': 'array', 'items': {'type': 'string'}}  # list of keys to scrub
        ]
    }

    @plugin.priority(SCRUB_PRIO)
    def on_task_modify(self, task, config):
        """ Scrub items that are torrents, if they're affected.
        """
        if isinstance(config, list):
            mode = "fields"
        else:
            mode = str(config).lower()
            if mode in ("off", "false"):
                log.debug("Plugin configured, but disabled")
                return

        for entry in task.entries:
            # Skip non-torrents
            if "torrent" not in entry:
                continue

            # Scrub keys as configured
            modified = set()
            metainfo = entry["torrent"].content
            infohash = entry["torrent"].info_hash

            if mode in ("on", "all", "true"):
                modified = bittorrent.clean_meta(metainfo, including_info=(mode == "all"), logger=log.debug)
            elif mode in ("resume", "rtorrent"):
                if mode == "resume":
                    self.RT_KEYS = self.RT_KEYS[:1]

                for key in self.RT_KEYS:
                    if key in metainfo:
                        log.debug("Removing key '%s'..." % (key,))
                        del metainfo[key]
                        modified.add(key)
            elif mode == "fields":
                # Scrub all configured fields
                for key in config:
                    fieldname = key  # store for logging
                    key = bittorrent.Torrent.KEY_TYPE(key)
                    field = metainfo

                    while field and '.' in key:
                        name, key = key.split('.', 1)
                        try:
                            field = field[name]
                        except KeyError:
                            # Key not found in this entry
                            field = None
                        log.trace((key, field))

                    if field and key in field:
                        log.debug("Removing key '%s'..." % (fieldname,))
                        del field[key]
                        modified.add(fieldname)
            else:
                raise ValueError("INTERNAL ERROR: Unknown mode %r" % mode)

            # Commit any changes back into entry
            if modified:
                entry["torrent"].content = metainfo
                entry["torrent"].modified = True
                log.info((("Key %s was" if len(modified) == 1 else "Keys %s were")
                          + " scrubbed from torrent '%s'!") % (", ".join(sorted(modified)), entry['title']))
                new_infohash = entry["torrent"].info_hash
                if infohash != new_infohash:
                    log.warn("Info hash changed from #%s to #%s in '%s'" %
                             (infohash, new_infohash, entry['filename']))


@event('plugin.register')
def register_plugin():
    plugin.register(TorrentScrub, groups=["torrent"], api_ver=2)

########NEW FILE########
__FILENAME__ = trackers
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from flexget import plugin
from flexget.event import event

log = logging.getLogger('modify_torrents')


class AddTrackers(object):

    """
        Adds tracker URL to torrent files.

        Configuration example:

        add_trackers:
          - uri://tracker_address:port/

        This will add all tracker URL uri://tracker_address:port/.
    """

    schema = {'type': 'array', 'items': {'type': 'string', 'format': 'url'}}

    @plugin.priority(127)
    def on_task_modify(self, task, config):
        for entry in task.entries:
            if 'torrent' in entry:
                for url in config:
                    if not url in entry['torrent'].trackers:
                        entry['torrent'].add_multitracker(url)
                        log.info('Added %s tracker to %s' % (url, entry['title']))
            if entry['url'].startswith('magnet:'):
                entry['url'] += ''.join(['&tr=' + url for url in config])


class RemoveTrackers(object):

    """
        Removes trackers from torrent files using regexp matching.

        Configuration example:

        remove_trackers:
          - moviex

        This will remove all trackers that contain text moviex in their url.
    """

    schema = {'type': 'array', 'items': {'type': 'string', 'format': 'regex'}}

    @plugin.priority(127)
    def on_task_modify(self, task, config):
        for entry in task.entries:
            if 'torrent' in entry:
                for tracker in entry['torrent'].trackers:
                    for regexp in config or []:
                        if re.search(regexp, tracker, re.IGNORECASE | re.UNICODE):
                            log.debug('remove_trackers removing %s because of %s' % (tracker, regexp))
                            # remove tracker
                            entry['torrent'].remove_multitracker(tracker)
                            log.info('Removed %s' % tracker)
            if entry['url'].startswith('magnet:'):
                for regexp in config:
                    # Replace any tracker strings that match the regexp with nothing
                    tr_search = r'&tr=([^&]*%s[^&]*)' % regexp
                    entry['url'] = re.sub(tr_search, '', entry['url'], re.IGNORECASE | re.UNICODE)


class ModifyTrackers(object):

    """
    Modify tracker URL to torrent files.

    Configuration example::

        modify_trackers:
          - SearchAndReplace:
              from: string_to_search
              to: string_to_replace

    """

    schema = {
        'type': 'array',
        'items': {
            'type': 'object',
            'additionalProperties': {
                'type': 'object',
                'properties': {'from': {'type': 'string'}, 'to': {'type': 'string'}},
                'additionalProperties': False
            },
            'maxProperties': 1
        }
    }

    @plugin.priority(127)
    def on_task_modify(self, task, config):
        for entry in task.entries:
            if 'torrent' in entry:
                torrent = entry['torrent']
                trackers = torrent.trackers
                for item in config:
                    for replace in item.itervalues():
                        for tracker in trackers:
                            if replace.get('from') in tracker:
                                torrent.remove_multitracker(tracker)
                                trackernew = tracker.replace(replace.get('from'), replace.get('to'))
                                torrent.add_multitracker(trackernew)
                                log.info('Modify %s in %s' % (tracker, trackernew))


@event('plugin.register')
def register_plugin():
    plugin.register(AddTrackers, 'add_trackers', api_ver=2)
    plugin.register(RemoveTrackers, 'remove_trackers', api_ver=2)
    plugin.register(ModifyTrackers, 'modify_trackers', api_ver=2)

########NEW FILE########
__FILENAME__ = debug_warnings
from __future__ import unicode_literals, division, absolute_import
import logging
import warnings

from flexget import options
from flexget.event import event

log = logging.getLogger('debug_warnings')


@event('manager.startup')
def debug_warnings(manager):
    if manager.options.debug_warnings:
        log.info('All warnings will be raised as errors for debugging purposes.')
        warnings.simplefilter('error')


@event('options.register')
def register_parser_arguments():
    options.get_parser().add_argument('--debug-warnings', action='store_true',
                                      help='elevate warnings to errors for debugging purposes, so a traceback is shown')

########NEW FILE########
__FILENAME__ = disable_builtins
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('builtins')


def all_builtins():
    """Helper function to return an iterator over all builtin plugins."""
    return (plugin for plugin in plugin.plugins.itervalues() if plugin.builtin)


class PluginDisableBuiltins(object):
    """Disables all (or specific) builtin plugins from a task."""

    def __init__(self):
        # cannot trust that on_task_start would have been executed
        self.disabled = []

    @property
    def schema(self):
        return {
            'oneOf': [
                {'type': 'boolean'},
                {'type': 'array', 'items': {'type': 'string', 'enum': [p.name for p in all_builtins()]}}
            ]
        }

    def debug(self):
        log.debug('Builtin plugins: %s' % ', '.join(plugin.name for plugin in all_builtins()))

    @plugin.priority(255)
    def on_task_start(self, task, config):
        self.disabled = []
        if not config:
            return

        for plugin in all_builtins():
            if config is True or plugin.name in config:
                plugin.builtin = False
                self.disabled.append(plugin.name)
        log.debug('Disabled builtin plugin(s): %s' % ', '.join(self.disabled))

    @plugin.priority(-255)
    def on_task_exit(self, task, config):
        if not self.disabled:
            return

        for name in self.disabled:
            plugin.plugins[name].builtin = True
        log.debug('Enabled builtin plugin(s): %s' % ', '.join(self.disabled))
        self.disabled = []

    on_task_abort = on_task_exit

@event('plugin.register')
def register_plugin():
    plugin.register(PluginDisableBuiltins, 'disable_builtins', api_ver=2)

########NEW FILE########
__FILENAME__ = disable_phases
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('disable_phases')


class PluginDisablePhases(object):
    """Disables phases from task execution.

    Mainly meant for advanced users and development.

    Example:

    disable_phases:
      - download
    """

    @property
    def schema(self):
        return {'type': 'array', 'items': {'type': 'string', 'enum': plugin.task_phases}}

    def on_task_start(self, task, config):
        map(task.disable_phase, config)

@event('plugin.register')
def register_plugin():
    plugin.register(PluginDisablePhases, 'disable_phases', api_ver=2)

########NEW FILE########
__FILENAME__ = domain_delay
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('domain_delay')


class DomainDelay(object):
    """
    Sets a minimum interval between requests to specific domains.

    Example::
      domain_delay:
        mysite.com: 5 seconds
    """

    schema = {'type': 'object', 'additionalProperties': {'type': 'string', 'format': 'interval'}}

    def on_task_start(self, task, config):
        for domain, delay in config.iteritems():
            log.debug('Adding minimum interval of %s between requests to %s' % (delay, domain))
            task.requests.set_domain_delay(domain, delay)


@event('plugin.register')
def register_plugin():
    plugin.register(DomainDelay, 'domain_delay', api_ver=2)

########NEW FILE########
__FILENAME__ = feed_priority
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('priority')


# TODO: 1.2 figure out replacement for this
# Currently the manager reads this value directly out of the config when the 'execute' command is run, and this plugin
# does nothing but make the config key valid.
# In daemon mode, schedules should be made which run tasks in the proper order instead of using this.
class TaskPriority(object):
    """Set task priorities"""

    schema = {'type': 'integer'}


@event('plugin.register')
def register_plugin():
    plugin.register(TaskPriority, 'priority', api_ver=2)

########NEW FILE########
__FILENAME__ = free_space
from __future__ import unicode_literals, division, absolute_import
import logging
import os

from flexget import plugin
from flexget.event import event

log = logging.getLogger('free_space')


def get_free_space(folder):
    """ Return folder/drive free space (in megabytes)"""
    if os.name == 'nt':
        import ctypes
        free_bytes = ctypes.c_ulonglong(0)
        ctypes.windll.kernel32.GetDiskFreeSpaceExW(ctypes.c_wchar_p(folder), None, None, ctypes.pointer(free_bytes))
        return free_bytes.value / (1024 * 1024)
    else:
        stats = os.statvfs(folder)
        return (stats.f_bavail * stats.f_frsize) / (1024 * 1024)


class PluginFreeSpace(object):
    """Aborts a task if an entry is accepted and there is less than a certain amount of space free on a drive."""

    schema = {
        'oneOf': [
            {'type': 'number'},
            {
                'type': 'object',
                'properties': {
                    'space': {'type': 'number'},
                    'path': {'type': 'string', 'format': 'path'}
                },
                'required': ['space'],
                'additionalProperties': False
            }
        ]
    }

    def prepare_config(self, config):
        if isinstance(config, (float, int)):
            config = {'space': config}
        # Use config path if none is specified
        if not config.get('path'):
            config['path'] = task.manager.config_base
        return config

    @plugin.priority(255)
    def on_task_download(self, task, config):
        config = self.prepare_config(config)
        # Only bother aborting if there were accepted entries this run.
        if task.accepted:
            if get_free_space(config['path']) < config['space']:
                log.error('Less than %d MB of free space in %s aborting task.' % (config['space'], config['path']))
                # backlog plugin will save and restore the task content, if available
                task.abort('Less than %d MB of free space in %s' % (config['space'], config['path']))


@event('plugin.register')
def register_plugin():
    plugin.register(PluginFreeSpace, 'free_space', api_ver=2)

########NEW FILE########
__FILENAME__ = interval
from __future__ import unicode_literals, division, absolute_import
import datetime
import logging

from flexget import options, plugin
from flexget.config_schema import parse_interval
from flexget.event import event

log = logging.getLogger('interval')


class PluginInterval(object):
    """
        Allows specifying minimum interval for task execution.

        Format: [n] [minutes|hours|days|weeks]

        Example:

        interval: 7 days
    """

    schema = {'type': 'string', 'format': 'interval'}

    @plugin.priority(255)
    def on_task_start(self, task, config):
        if task.options.learn:
            log.info('Ignoring task %s interval for --learn' % task.name)
            return
        last_time = task.simple_persistence.get('last_time')
        if not last_time:
            log.info('No previous run recorded, running now')
        elif task.options.interval_ignore:
            log.info('Ignoring interval because of --now')
        else:
            log.debug('last_time: %r' % last_time)
            log.debug('interval: %s' % config)
            next_time = last_time + parse_interval(config)
            log.debug('next_time: %r' % next_time)
            if datetime.datetime.now() < next_time:
                log.verbose('Interval %s not met on task %s. Use --now to override.' % (config, task.name))
                task.abort('Interval not met', silent=True)
                return
        log.debug('interval passed')
        task.simple_persistence['last_time'] = datetime.datetime.now()


@event('plugin.register')
def register_plugin():
    plugin.register(PluginInterval, 'interval', api_ver=2)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--now', action='store_true', dest='interval_ignore', default=False,
                                               help='run task(s) even if the interval plugin would normally prevent it')

########NEW FILE########
__FILENAME__ = manual
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('manual')


class ManualTask(object):
    """Only execute task when specified with --tasks"""

    schema = {'type': 'boolean'}

    @plugin.priority(255)
    def on_task_start(self, task, config):
        # Make sure we need to run
        if not config:
            return
        # If --task hasn't been specified disable this plugin
        if not task.options.tasks or task.name not in task.options.tasks:
            log.debug('Disabling task %s' % task.name)
            task.abort('manual task not specified in --tasks', silent=True)


@event('plugin.register')
def register_plugin():
    plugin.register(ManualTask, 'manual', api_ver=2)



########NEW FILE########
__FILENAME__ = max_reruns
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.task import Task

log = logging.getLogger('max_reruns')


class MaxReRuns(object):
    """Overrides the maximum amount of re-runs allowed by a task."""

    schema = {'type': 'integer'}

    def __init__(self):
        self.default = Task.max_reruns

    def on_task_start(self, task, config):
        self.default = task.max_reruns
        task.max_reruns = int(config)
        log.debug('changing max task rerun variable to: %s' % config)

    def on_task_exit(self, task, config):
        log.debug('restoring max task rerun variable to: %s' % self.default)
        task.max_reruns = self.default

    on_task_abort = on_task_exit


@event('plugin.register')
def register_plugin():
    plugin.register(MaxReRuns, 'max_reruns', api_ver=2)

########NEW FILE########
__FILENAME__ = pathscrub
from __future__ import unicode_literals, division, absolute_import

from flexget import plugin
from flexget.event import event
from flexget.utils import pathscrub


class PathScrub(object):
    """
    Plugin that will clear illegal characters from paths. Other plugins should use this if available when
    creating paths. User can specify what os if filenames must be compatible with an os other than current.

    Example::
      pathscrub: windows
    """

    schema = {'type': 'string', 'enum': ['windows', 'linux', 'mac']}

    def on_task_start(self, task, config):
        # Change path scrub os mode
        pathscrub.os_mode = config

    def on_task_exit(self, task, config):
        # Reset os mode when task has finished
        pathscrub.os_mode = None

    on_task_abort = on_task_exit


@event('plugin.register')
def register_plugin():
    plugin.register(PathScrub, 'pathscrub', api_ver=2)

########NEW FILE########
__FILENAME__ = proxy
from __future__ import unicode_literals, division, absolute_import
import logging
import os

from flexget import plugin
from flexget import validator
from flexget.event import event

log = logging.getLogger('proxy')

PROTOCOLS = ['http', 'https', 'ftp']


class Proxy(object):
    """Adds a proxy to the requests session."""

    schema = {
        'oneOf': [
            {'type': 'string', 'format': 'url'},
            {
                'type': 'object',
                'properties': dict((prot, {'type': 'string', 'format': 'url'}) for prot in PROTOCOLS),
                'additionalProperties': False
            }
        ]
    }

    @plugin.priority(255)
    def on_task_start(self, task, config):
        if not config:
            # If no configuration is provided, see if there are any proxy env variables
            proxies = {}
            for prot in PROTOCOLS:
                if os.environ.get(prot + '_proxy'):
                    proxies[prot] = os.environ[prot + '_proxy']
            if not proxies:
                # If there were no environment variables set, do nothing
                return
        elif isinstance(config, dict):
            proxies = config
        else:
            # Map all protocols to the configured proxy
            proxies = dict((prot, config) for prot in PROTOCOLS)
        log.verbose('Setting proxy to %s' % proxies)
        task.requests.proxies = proxies


@event('plugin.register')
def register_plugin():
    plugin.register(Proxy, 'proxy', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = rerun
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('rerun')


class MaxReRuns(object):
    """Force a task to rerun for debugging purposes."""

    schema = {'type': ['boolean', 'integer']}

    def on_task_start(self, task, config):
        task.max_reruns = int(config)

    def on_task_input(self, task, config):
        task.rerun()


@event('plugin.register')
def register_plugin():
    plugin.register(MaxReRuns, 'rerun', api_ver=2, debug=True)

########NEW FILE########
__FILENAME__ = run_task
from __future__ import unicode_literals, division, absolute_import
import copy
import functools
import logging

from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.event import event

log = logging.getLogger('run_task')


class RunTask(object):
    schema = {
        'type': 'object',
        'properties': {
            'task': {'type': 'string'},
            'when': one_or_more({'type': 'string',
                                 'enum': ['accepted', 'rejected', 'failed', 'no_entries', 'aborted', 'always']})
        },
        'required': ['task'],
        'additionalProperties': False
    }

    def on_task_exit(self, task, config):
        config.setdefault('when', 'always')
        conditions = [
            task.accepted and 'accepted' in config['when'],
            task.rejected and 'rejected' in config['when'],
            not task.all_entries and 'no_entries' in config['when'],
            'always' in config['when']
        ]
        if any(conditions):
            self.run_task(task, config['task'])

    def on_task_abort(self, task, config):
        if 'aborted' in config:
            self.run_task(task, config['task'])

    def run_task(self, current_task, run_task):
        log.info('Scheduling %s task to run' % run_task)
        options = copy.copy(current_task.options)
        options.tasks = [run_task]
        current_task.manager.scheduler.execute(options=options)


@event('plugin.register')
def register_plugin():
    plugin.register(RunTask, 'run_task', api_ver=2)

########NEW FILE########
__FILENAME__ = sequence
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('sequence')


class PluginSequence(object):
    """ Allows the same plugin to be configured multiple times in a task.

    Example:
    sequence:
      - rss: http://feeda.com
      - rss: http://feedb.com
    """

    schema = {
        'type': 'array',
        'items': {'$ref': '/schema/plugins'}
    }

    def __getattr__(self, item):
        """Returns a function for all on_task_* events, that runs all the configured plugins."""
        for phase, method in plugin.phase_methods.iteritems():
            if item == method and phase not in ['accept', 'reject', 'fail']:
                break
        else:
            raise AttributeError(item)

        def handle_phase(task, config):
            """Function that runs all of the configured plugins which act on the current phase."""
            # Keep a list of all results, for input plugin combining
            results = []
            for item in config:
                for plugin_name, plugin_config in item.iteritems():
                    if phase in plugin.get_phases_by_plugin(plugin_name):
                        method = plugin.get_plugin_by_name(plugin_name).phase_handlers[phase]
                        log.debug('Running plugin %s' % plugin_name)
                        result = method(task, plugin_config)
                        if isinstance(result, list):
                            results.extend(result)
            return results

        return handle_phase


@event('plugin.register')
def register_plugin():
    plugin.register(PluginSequence, 'sequence', api_ver=2, debug=True)

########NEW FILE########
__FILENAME__ = sleep
from __future__ import unicode_literals, division, absolute_import
import logging
import time

from flexget import plugin
from flexget.event import event

log = logging.getLogger('sleep')


class PluginSleep(object):
    """Causes a pause to occur before execution of a task"""

    def validator(self):
        from flexget import validator
        return validator.factory('number')

    @plugin.priority(255)
    def on_task_start(self, task, config):
        if config:
            log.verbose('Sleeping for %d seconds.' % config)
            time.sleep(config)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginSleep, 'sleep', api_ver=2)

########NEW FILE########
__FILENAME__ = verify_ssl_certificates
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin, validator
from flexget.event import event

log = logging.getLogger('verify_ssl')


class VerifySSLCertificates(object):
    """
    Plugin that can off SSL certificate verification.

    Example::
      verify_ssl_certificates: no
    """

    def validator(self):
        return validator.factory('boolean')

    @plugin.priority(255)
    def on_task_start(self, task, config):
        if config is False:
            task.requests.verify = False


@event('plugin.register')
def register_plugin():
    plugin.register(VerifySSLCertificates, 'verify_ssl_certificates', api_ver=2)

########NEW FILE########
__FILENAME__ = download
from __future__ import unicode_literals, division, absolute_import
import hashlib
import logging
import mimetypes
import os
import shutil
import socket
import sys
import tempfile
import urllib
import urllib2
from cgi import parse_header
from httplib import BadStatusLine

from requests import RequestException

from flexget import options, plugin
from flexget.event import event
from flexget.utils.tools import decode_html
from flexget.utils.template import RenderError
from flexget.utils.pathscrub import pathscrub

log = logging.getLogger('download')


class PluginDownload(object):

    """
    Downloads content from entry url and writes it into a file.

    Example::

      download: ~/torrents/

    Allow HTML content:

    By default download plugin reports failure if received content
    is a html. Usually this is some sort of custom error page without
    proper http code and thus entry is assumed to be downloaded
    incorrectly.

    In the rare case you actually need to retrieve html-pages you must
    disable this feature.

    Example::

      download:
        path: ~/something/
        fail_html: no

    You may use commandline parameter --dl-path to temporarily override
    all paths to another location.
    """

    schema = {
        'oneOf': [
            {
                'title': 'specify options',
                'type': 'object',
                'properties': {
                    'path': {'type': 'string', 'format': 'path'},
                    'fail_html': {'type': 'boolean', 'default': True},
                    'overwrite': {'type': 'boolean', 'default': False},
                    'temp': {'type': 'string', 'format': 'path'}
                },
                'additionalProperties': False
            },
            {'title': 'specify path', 'type': 'string', 'format': 'path'},
            {'title': 'no options', 'type': 'boolean', 'enum': [True]}
        ]
    }

    def process_config(self, config):
        """Return plugin configuration in advanced form"""
        if isinstance(config, basestring):
            config = {'path': config}
        if not isinstance(config, dict):
            config = {}
        if not config.get('path'):
            config['require_path'] = True
        config.setdefault('fail_html', True)
        return config

    def on_task_download(self, task, config):
        config = self.process_config(config)

        # set temporary download path based on user's config setting or use fallback
        tmp = config.get('temp', os.path.join(task.manager.config_base, 'temp'))

        self.get_temp_files(task, require_path=config.get('require_path', False), fail_html=config['fail_html'],
                            tmp_path=tmp)

    def get_temp_file(self, task, entry, require_path=False, handle_magnets=False, fail_html=True,
                      tmp_path=tempfile.gettempdir()):
        """
        Download entry content and store in temporary folder.
        Fails entry with a reason if there was problem.

        :param bool require_path:
          whether or not entries without 'path' field are ignored
        :param bool handle_magnets:
          when used any of urls containing magnet link will replace url,
          otherwise warning is printed.
        :param fail_html:
          fail entries which url respond with html content
        :param tmp_path:
          path to use for temporary files while downloading
        """
        if entry.get('urls'):
            urls = entry.get('urls')
        else:
            urls = [entry['url']]
        errors = []
        for url in urls:
            if url.startswith('magnet:'):
                if handle_magnets:
                    # Set magnet link as main url, so a torrent client plugin can grab it
                    log.debug('Accepting magnet url for %s' % entry['title'])
                    entry['url'] = url
                    break
                else:
                    log.warning('Can\'t download magnet url')
                    errors.append('Magnet URL')
                    continue
            if require_path and 'path' not in entry:
                # Don't fail here, there might be a magnet later in the list of urls
                log.debug('Skipping url %s because there is no path for download' % url)
                continue
            error = self.process_entry(task, entry, url, tmp_path)

            # disallow html content
            html_mimes = ['html', 'text/html']
            if entry.get('mime-type') in html_mimes and fail_html:
                error = 'Unexpected html content received from `%s` - maybe a login page?' % entry['url']
                self.cleanup_temp_file(entry)

            if not error:
                # Set the main url, so we know where this file actually came from
                log.debug('Successfully retrieved %s from %s' % (entry['title'], url))
                entry['url'] = url
                break
            else:
                errors.append(error)
        else:
            # check if entry must have a path (download: yes)
            if require_path and 'path' not in entry:
                log.error('%s can\'t be downloaded, no path specified for entry' % entry['title'])
                entry.fail('no path specified for entry')
            else:
                entry.fail(', '.join(errors))

    def save_error_page(self, entry, task, page):
        received = os.path.join(task.manager.config_base, 'received', task.name)
        if not os.path.isdir(received):
            os.makedirs(received)
        filename = os.path.join(received, '%s.error' % entry['title'].encode(sys.getfilesystemencoding(), 'replace'))
        log.error('Error retrieving %s, the error page has been saved to %s' % (entry['title'], filename))
        with open(filename, 'w') as outfile:
            outfile.write(page)

    def get_temp_files(self, task, require_path=False, handle_magnets=False, fail_html=True,
                       tmp_path=tempfile.gettempdir()):
        """Download all task content and store in temporary folder.

        :param bool require_path:
          whether or not entries without 'path' field are ignored
        :param bool handle_magnets:
          when used any of urls containing magnet link will replace url,
          otherwise warning is printed.
        :param fail_html:
          fail entries which url respond with html content
        :param tmp_path:
          path to use for temporary files while downloading
        """
        for entry in task.accepted:
            self.get_temp_file(task, entry, require_path, handle_magnets, fail_html, tmp_path)

    # TODO: a bit silly method, should be get rid of now with simplier exceptions ?
    def process_entry(self, task, entry, url, tmp_path):
        """
        Processes `entry` by using `url`. Does not use entry['url'].
        Does not fail the `entry` if there is a network issue, instead just logs and returns a string error.

        :param task: Task
        :param entry: Entry
        :param url: Url to try download
        :param tmp_path: Path to store temporary files
        :return: String error, if failed.
        """
        try:
            if task.options.test:
                log.info('Would download: %s' % entry['title'])
            else:
                if not task.manager.unit_test:
                    log.info('Downloading: %s' % entry['title'])
                self.download_entry(task, entry, url, tmp_path)
        except RequestException as e:
            # TODO: Improve this error message?
            log.warning('RequestException %s' % e)
            return 'Network error during request: %s' % e
        # TODO: I think these exceptions will not be thrown by requests library.
        except urllib2.HTTPError as e:
            log.warning('HTTPError %s' % e.code)
            return 'HTTP error'
        except urllib2.URLError as e:
            log.warning('URLError %s' % e.reason)
            return 'URL Error'
        except BadStatusLine as e:
            log.warning('Failed to reach server. Reason: %s' % getattr(e, 'message', 'N/A'))
            return 'BadStatusLine'
        except IOError as e:
            if hasattr(e, 'reason'):
                log.warning('Failed to reach server. Reason: %s' % e.reason)
            elif hasattr(e, 'code'):
                log.warning('The server couldn\'t fulfill the request. Error code: %s' % e.code)
            log.debug('IOError', exc_info=True)
            return 'IOError'
        except ValueError as e:
            # Probably unknown url type
            msg = 'ValueError %s' % e
            log.warning(msg)
            log.debug(msg, exc_info=True)
            return msg

    def download_entry(self, task, entry, url, tmp_path):
        """Downloads `entry` by using `url`.

        :raises: Several types of exceptions ...
        :raises: PluginWarning
        """

        # see http://bugs.python.org/issue1712522
        # note, url is already unicode ...
        try:
            url = url.encode('latin1')
        except UnicodeEncodeError:
            log.debug('URL for `%s` could not be encoded in latin1' % entry['title'])
            try:
                url = url.encode('utf-8')
            except Exception:
                log.warning('Unable to URL-encode URL for `%s`' % entry['title'])
        if not isinstance(url, unicode):
            url = urllib.quote(url, safe=b':/~?=&%;')
        log.debug('Downloading url \'%s\'' % url)

        # get content
        auth = None
        if 'download_auth' in entry:
            auth = entry['download_auth']
            log.debug('Custom auth enabled for %s download: %s' % (entry['title'], entry['download_auth']))

        response = task.requests.get(url, auth=auth, raise_status=False)
        if response.status_code != 200:
            log.debug('Got %s response from server. Saving error page.' % response.status_code)
            # Save the error page
            if response.content:
                self.save_error_page(entry, task, response.content)
            # Raise the error
            response.raise_for_status()
            return

        # expand ~ in temp path
        #TODO jinja?
        try:
            tmp_path = os.path.expanduser(tmp_path)
        except RenderError as e:
            entry.fail('Could not set temp path. Error during string replacement: %s' % e)
            return

        # Clean illegal characters from temp path name
        tmp_path = pathscrub(tmp_path)

        # create if missing
        if not os.path.isdir(tmp_path):
            log.debug('creating tmp_path %s' % tmp_path)
            os.mkdir(tmp_path)

        # check for write-access
        if not os.access(tmp_path, os.W_OK):
            raise plugin.PluginError('Not allowed to write to temp directory `%s`' % tmp_path)

        # download and write data into a temp file
        tmp_dir = tempfile.mkdtemp(dir=tmp_path)
        fname = hashlib.md5(url).hexdigest()
        datafile = os.path.join(tmp_dir, fname)
        outfile = open(datafile, 'wb')
        try:
            for chunk in response.iter_content(chunk_size=150 * 1024, decode_unicode=False):
                outfile.write(chunk)
        except Exception as e:
            # don't leave futile files behind
            # outfile has to be closed before we can delete it on Windows
            outfile.close()
            log.debug('Download interrupted, removing datafile')
            os.remove(datafile)
            if isinstance(e, socket.timeout):
                log.error('Timeout while downloading file')
            else:
                raise
        else:
            outfile.close()
            # Do a sanity check on downloaded file
            if os.path.getsize(datafile) == 0:
                entry.fail('File %s is 0 bytes in size' % datafile)
                os.remove(datafile)
                return
            # store temp filename into entry so other plugins may read and modify content
            # temp file is moved into final destination at self.output
            entry['file'] = datafile
            log.debug('%s field file set to: %s' % (entry['title'], entry['file']))

        if 'content-type' in response.headers:
            entry['mime-type'] = parse_header(response.headers['content-type'])[0]
        else:
            entry['mime-type'] = "unknown/unknown"

        content_encoding = response.headers.get('content-encoding', '')
        decompress = 'gzip' in content_encoding or 'deflate' in content_encoding
        if 'content-length' in response.headers and not decompress:
            entry['content-length'] = int(response.headers['content-length'])

        # prefer content-disposition naming, note: content-disposition can be disabled completely
        # by setting entry field `content-disposition` to False
        if entry.get('content-disposition', True):
            self.filename_from_headers(entry, response)
        else:
            log.info('Content-disposition disabled for %s' % entry['title'])
        self.filename_ext_from_mime(entry)

        if not entry.get('filename'):
            filename = os.path.basename(url)
            log.debug('No filename - setting from url: %s' % filename)
            entry['filename'] = filename
        log.debug('Finishing download_entry() with filename %s' % entry.get('filename'))

    def filename_from_headers(self, entry, response):
        """Checks entry filename if it's found from content-disposition"""
        if not response.headers.get('content-disposition'):
            # No content disposition header, nothing we can do
            return
        filename = parse_header(response.headers['content-disposition'])[1].get('filename')

        if filename:
            # try to decode to unicode, specs allow latin1, some may do utf-8 anyway
            try:
                filename = filename.decode('latin1')
                log.debug('filename header latin1 decoded')
            except UnicodeError:
                try:
                    filename = filename.decode('utf-8')
                    log.debug('filename header UTF-8 decoded')
                except UnicodeError:
                    pass
            filename = decode_html(filename)
            log.debug('Found filename from headers: %s' % filename)
            if 'filename' in entry:
                log.debug('Overriding filename %s with %s from content-disposition' % (entry['filename'], filename))
            entry['filename'] = filename

    def filename_ext_from_mime(self, entry):
        """Tries to set filename extension from mime-type"""
        extensions = mimetypes.guess_all_extensions(entry['mime-type'], strict=False)
        if extensions:
            log.debug('Mimetype guess for %s is %s ' % (entry['mime-type'], extensions))
            if entry.get('filename'):
                if any(entry['filename'].endswith(extension) for extension in extensions):
                    log.debug('Filename %s extension matches to mime-type' % entry['filename'])
                else:
                    # mimetypes library has no concept of a 'prefered' extension when there are multiple possibilites
                    # this causes the first to be used which is not always desirable, e.g. 'ksh' for 'text/plain'
                    extension = mimetypes.guess_extension(entry['mime-type'], strict=False)
                    log.debug('Adding mime-type extension %s to %s' % (extension, entry['filename']))
                    entry['filename'] = entry['filename'] + extension
        else:
            log.debug('Python doesn\'t know extension for mime-type: %s' % entry['mime-type'])

    def on_task_output(self, task, config):
        """Move downloaded content from temp folder to final destination"""
        config = self.process_config(config)
        for entry in task.accepted:
            try:
                self.output(task, entry, config)
            except plugin.PluginWarning as e:
                entry.fail()
                log.error('Plugin error while writing: %s' % e)
            except Exception as e:
                entry.fail()
                log.exception('Exception while writing: %s' % e)

    def output(self, task, entry, config):
        """Moves temp-file into final destination

        Raises:
            PluginError if operation fails
        """

        if 'file' not in entry and not task.options.test:
            log.debug('file missing, entry: %s' % entry)
            raise plugin.PluginError('Entry `%s` has no temp file associated with' % entry['title'])

        try:
            # use path from entry if has one, otherwise use from download definition parameter
            path = entry.get('path', config.get('path'))
            if not isinstance(path, basestring):
                raise plugin.PluginError('Invalid `path` in entry `%s`' % entry['title'])

            # override path from command line parameter
            if task.options.dl_path:
                path = task.options.dl_path

            # expand variables in path
            try:
                path = os.path.expanduser(entry.render(path))
            except RenderError as e:
                entry.fail('Could not set path. Error during string replacement: %s' % e)
                return

            # Clean illegal characters from path name
            path = pathscrub(path)

            # If we are in test mode, report and return
            if task.options.test:
                log.info('Would write `%s` to `%s`' % (entry['title'], path))
                # Set a fake location, so the exec plugin can do string replacement during --test #1015
                entry['output'] = os.path.join(path, 'TEST_MODE_NO_OUTPUT')
                return

            # make path
            if not os.path.isdir(path):
                log.debug('Creating directory %s' % path)
                try:
                    os.makedirs(path)
                except:
                    raise plugin.PluginError('Cannot create path %s' % path, log)

            # check that temp file is present
            if not os.path.exists(entry['file']):
                log.debug('entry: %s' % entry)
                raise plugin.PluginWarning('Downloaded temp file `%s` doesn\'t exist!?' % entry['file'])

            # if we still don't have a filename, try making one from title (last resort)
            if not entry.get('filename'):
                entry['filename'] = entry['title']
                log.debug('set filename from title %s' % entry['filename'])
                if not 'mime-type' in entry:
                    log.warning('Unable to figure proper filename for %s. Using title.' % entry['title'])
                else:
                    guess = mimetypes.guess_extension(entry['mime-type'])
                    if not guess:
                        log.warning('Unable to guess extension with mime-type %s' % guess)
                    else:
                        self.filename_ext_from_mime(entry)

            name = entry.get('filename', entry['title'])
            # Remove illegal characters from filename #325, #353
            name = pathscrub(name)
            # Remove directory separators from filename #208
            name = name.replace('/', ' ')
            if sys.platform.startswith('win'):
                name = name.replace('\\', ' ')
            # remove duplicate spaces
            name = ' '.join(name.split())
            # combine to full path + filename
            destfile = os.path.join(path, name)
            log.debug('destfile: %s' % destfile)

            if os.path.exists(destfile):
                import filecmp
                if filecmp.cmp(entry['file'], destfile):
                    log.debug("Identical destination file '%s' already exists", destfile)
                elif config.get('overwrite'):
                    log.debug("Overwriting already existing file %s" % destfile)
                else:
                    log.info('File `%s` already exists and is not identical, download failed.' % destfile)
                    entry.fail('File `%s` already exists and is not identical.' % destfile)
                    return
            else:
                # move temp file
                log.debug('moving %s to %s' % (entry['file'], destfile))

                try:
                    shutil.move(entry['file'], destfile)
                except OSError as err:
                    # ignore permission errors, see ticket #555
                    import errno
                    if not os.path.exists(destfile):
                        raise plugin.PluginError('Unable to write %s' % destfile)
                    if err.errno != errno.EPERM:
                        raise

            # store final destination as output key
            entry['output'] = destfile

        finally:
            self.cleanup_temp_file(entry)

    def on_task_learn(self, task, config):
        """Make sure all temp files are cleaned up after output phase"""
        self.cleanup_temp_files(task)

    def on_task_abort(self, task, config):
        """Make sure all temp files are cleaned up when task is aborted."""
        self.cleanup_temp_files(task)

    def cleanup_temp_file(self, entry):
        if 'file' in entry:
            if os.path.exists(entry['file']):
                log.debug('removing temp file %s from %s' % (entry['file'], entry['title']))
                os.remove(entry['file'])
            shutil.rmtree(os.path.dirname(entry['file']))
            del(entry['file'])

    def cleanup_temp_files(self, task):
        """Checks all entries for leftover temp files and deletes them."""
        for entry in task.entries + task.rejected + task.failed:
            self.cleanup_temp_file(entry)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginDownload, 'download', api_ver=2)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--dl-path', dest='dl_path', default=False, metavar='PATH',
                                               help='override path for download plugin, applies to all executed tasks')

########NEW FILE########
__FILENAME__ = dump
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import options, plugin
from flexget.event import event
from flexget.utils.tools import console

log = logging.getLogger('dump')


def dump(entries, debug=False, eval_lazy=False, trace=False):
    """
    Dump *entries* to stdout

    :param list entries: Entries to be dumped.
    :param bool debug: Print non printable fields as well.
    :param bool eval_lazy: Evaluate lazy fields.
    :param bool trace: Display trace information.
    """
    def sort_key(field):
        # Sort certain fields above the rest
        if field == 'title':
            return 0
        if field == 'url':
            return 1
        if field == 'original_url':
            return 2
        return field

    for entry in entries:
        for field in sorted(entry, key=sort_key):
            if entry.is_lazy(field) and not eval_lazy:
                value = '<LazyField - value will be determined when it is accessed>'
            else:
                value = entry[field]
            if isinstance(value, basestring):
                try:
                    console('%-17s: %s' % (field, value.replace('\r', '').replace('\n', '')))
                except Exception:
                    console('%-17s: %r (warning: unable to print)' % (field, value))
            elif isinstance(value, list):
                console('%-17s: %s' % (field, '[%s]' % ', '.join(unicode(v) for v in value)))
            elif isinstance(value, (int, float, dict)):
                console('%-17s: %s' % (field, value))
            elif value is None:
                console('%-17s: %s' % (field, value))
            else:
                try:
                    value = str(entry[field])
                    console('%-17s: %s' % (field, value.replace('\r', '').replace('\n', '')))
                except Exception:
                    if debug:
                        console('%-17s: [not printable] (%r)' % (field, value))
        if trace:
            console('-- Processing trace:')
            for item in entry.traces:
                console('%-10s %-7s %s' % (item[0], '' if item[1] is None else item[1], item[2]))
        console('')


class OutputDump(object):
    """
    Outputs all entries to console
    """

    schema = {'type': 'boolean'}

    @plugin.priority(0)
    def on_task_output(self, task, config):
        if not config and task.options.dump_entries is None:
            return

        eval_lazy = 'eval' in task.options.dump_entries
        trace = 'trace' in task.options.dump_entries
        states = ['accepted', 'rejected', 'failed', 'undecided']
        dumpstates = [s for s in states if s in task.options.dump_entries]
        specificstates = dumpstates
        if not dumpstates: dumpstates = states
        undecided = [entry for entry in task.all_entries if entry.undecided]
        if 'undecided' in dumpstates:
            if undecided:
                console('-- Undecided: --------------------------')
                dump(undecided, task.options.debug, eval_lazy, trace)
            elif specificstates:
                console('No undecided entries')
        if 'accepted' in dumpstates:
            if task.accepted:
                console('-- Accepted: ---------------------------')
                dump(task.accepted, task.options.debug, eval_lazy, trace)
            elif specificstates:
                console('No accepted entries')
        if 'rejected' in dumpstates:
            if task.rejected:
                console('-- Rejected: ---------------------------')
                dump(task.rejected, task.options.debug, eval_lazy, trace)
            elif specificstates:
                console('No rejected entries')


@event('plugin.register')
def register_plugin():
    plugin.register(OutputDump, 'dump', builtin=True, api_ver=2)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--dump', nargs='*', choices=['eval', 'trace', 'accepted', 'rejected',
        'undecided'], dest='dump_entries', help='display all entries in task with fields they contain, '
        'use `--dump eval` to evaluate all lazy fields. Specify an entry state/states to only dump matching entries.')

########NEW FILE########
__FILENAME__ = dump_config
from __future__ import unicode_literals, division, absolute_import, print_function
import logging

from argparse import SUPPRESS

from flexget import options, plugin
from flexget.event import event

log = logging.getLogger('dump_config')


class OutputDumpConfig(object):
    """
        Dumps task config in STDOUT in yaml at exit or abort event.
    """

    @plugin.priority(-255)
    def on_task_start(self, task, config):
        if task.options.dump_config:
            import yaml
            print('--- config from task: %s' % task.name)
            print(yaml.safe_dump(task.config))
            print('---')
            task.abort(silent=True)
        if task.options.dump_config_python:
            print(task.config)
            task.abort(silent=True)


@event('plugin.register')
def register_plugin():
    plugin.register(OutputDumpConfig, 'dump_config', debug=True, builtin=True, api_ver=2)


@event('options.register')
def register_parser_arguments():
    exec_parser = options.get_parser('execute')
    exec_parser.add_argument('--dump-config', action='store_true', dest='dump_config', default=False,
                             help='display the config of each feed after template merging/config generation occurs')
    exec_parser.add_argument('--dump-config-python', action='store_true', dest='dump_config_python', default=False,
                             help=SUPPRESS)

########NEW FILE########
__FILENAME__ = exec
from __future__ import unicode_literals, division, absolute_import
from collections import Mapping
import logging
import subprocess


from flexget import plugin
from flexget.event import event
from flexget.utils.template import render_from_entry, render_from_task, RenderError
from flexget.utils.tools import io_encoding

log = logging.getLogger('exec')


class EscapingDict(Mapping):
    """Helper class, same as a dict, but returns all string value with quotes escaped."""

    def __init__(self, mapping):
        self._data = mapping

    def __len__(self):
        return len(self._data)

    def __iter__(self):
        return iter(self._data)

    def __getitem__(self, key):
        value = self._data[key]
        if isinstance(value, basestring):
            # TODO: May need to be different depending on OS
            value = value.replace('"', '\\"')
            #value = re.escape(value)
        return value


class PluginExec(object):
    """
    Execute commands

    Simple example, xecute command for entries that reach output::

      exec: echo 'found {{title}} at {{url}}' > file

    Advanced Example::

      exec:
        on_start:
          phase: echo "Started"
        on_input:
          for_entries: echo 'got {{title}}'
        on_output:
          for_accepted: echo 'accepted {{title}} - {{url}} > file

    You can use all (available) entry fields in the command.
    """

    NAME = 'exec'
    HANDLED_PHASES = ['start', 'input', 'filter', 'output', 'exit']

    schema = {
        'oneOf': [
            {'type': 'string'},
            {
                'type': 'object',
                'properties': {
                    'on_start': {'$ref': '#/definitions/phaseSettings'},
                    'on_input': {'$ref': '#/definitions/phaseSettings'},
                    'on_filter': {'$ref': '#/definitions/phaseSettings'},
                    'on_output': {'$ref': '#/definitions/phaseSettings'},
                    'on_exit': {'$ref': '#/definitions/phaseSettings'},
                    'fail_entries': {'type': 'boolean'},
                    'auto_escape': {'type': 'boolean'},
                    'encoding': {'type': 'string'},
                    'allow_background': {'type': 'boolean'}
                },
                'additionalProperties': False
            }
        ],
        'definitions': {
            'phaseSettings': {
                'type': 'object',
                'properties': {
                    'phase': {'type': 'string'},
                    'for_entries': {'type': 'string'},
                    'for_accepted': {'type': 'string'},
                    'for_rejected': {'type': 'string'},
                    'for_failed': {'type': 'string'}
                },
                'additionalProperties': False
            }
        }
    }

    def prepare_config(self, config):
        if isinstance(config, basestring):
            config = {'on_output': {'for_accepted': config}}
        if not config.get('encoding'):
            config['encoding'] = io_encoding
        return config

    def execute_cmd(self, cmd, allow_background, encoding):
        log.verbose('Executing: %s' % cmd)
        p = subprocess.Popen(cmd.encode(encoding), shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE,
                             stderr=subprocess.STDOUT, close_fds=False)
        if not allow_background:
            (r, w) = (p.stdout, p.stdin)
            response = r.read().decode(encoding, 'replace')
            r.close()
            w.close()
            if response:
                log.info('Stdout: %s' % response)
        return p.wait()

    def execute(self, task, phase_name, config):
        config = self.prepare_config(config)
        if not phase_name in config:
            log.debug('phase %s not configured' % phase_name)
            return

        name_map = {'for_entries': task.entries, 'for_accepted': task.accepted,
                    'for_rejected': task.rejected, 'for_failed': task.failed}

        allow_background = config.get('allow_background')
        for operation, entries in name_map.iteritems():
            if not operation in config[phase_name]:
                continue

            log.debug('running phase_name: %s operation: %s entries: %s' % (phase_name, operation, len(entries)))

            for entry in entries:
                cmd = config[phase_name][operation]
                entrydict = EscapingDict(entry) if config.get('auto_escape') else entry
                # Do string replacement from entry, but make sure quotes get escaped
                try:
                    cmd = render_from_entry(cmd, entrydict)
                except RenderError as e:
                    log.error('Could not set exec command for %s: %s' % (entry['title'], e))
                    # fail the entry if configured to do so
                    if config.get('fail_entries'):
                        entry.fail('Entry `%s` does not have required fields for string replacement.' % entry['title'])
                    continue

                log.debug('phase_name: %s operation: %s cmd: %s' % (phase_name, operation, cmd))
                if task.options.test:
                    log.info('Would execute: %s' % cmd)
                else:
                    # Make sure the command can be encoded into appropriate encoding, don't actually encode yet,
                    # so logging continues to work.
                    try:
                        cmd.encode(config['encoding'])
                    except UnicodeEncodeError:
                        log.error('Unable to encode cmd `%s` to %s' % (cmd, config['encoding']))
                        if config.get('fail_entries'):
                            entry.fail('cmd `%s` could not be encoded to %s.' % (cmd, config['encoding']))
                        continue
                    # Run the command, fail entries with non-zero return code if configured to
                    if self.execute_cmd(cmd, allow_background, config['encoding']) != 0 and config.get('fail_entries'):
                        entry.fail('exec return code was non-zero')

        # phase keyword in this
        if 'phase' in config[phase_name]:
            cmd = config[phase_name]['phase']
            try:
                cmd = render_from_task(cmd, task)
            except RenderError as e:
                log.error('Error rendering `%s`: %s' % (cmd, e))
            else:
                log.debug('phase cmd: %s' % cmd)
                if task.options.test:
                    log.info('Would execute: %s' % cmd)
                else:
                    self.execute_cmd(cmd, allow_background, config['encoding'])

    def __getattr__(self, item):
        """Creates methods to handle task phases."""
        for phase in self.HANDLED_PHASES:
            if item == plugin.phase_methods[phase]:
                # A phase method we handle has been requested
                break
        else:
            # We don't handle this phase
            raise AttributeError(item)

        def phase_handler(task, config):
            self.execute(task, 'on_' + phase, config)

        # Make sure we run after other plugins so exec can use their output
        phase_handler.priority = 100
        return phase_handler


@event('plugin.register')
def register_plugin():
    plugin.register(PluginExec, 'exec', api_ver=2)

########NEW FILE########
__FILENAME__ = ftp_download
import logging
import os
import ftplib
from urlparse import urlparse

from flexget import plugin
from flexget.event import event

log = logging.getLogger('ftp')


class OutputFtp(object):
    """
        Ftp Download plugin

        input-url: ftp://<user>:<password>@<host>:<port>/<path to file>
        Example: ftp://anonymous:anon@my-ftp-server.com:21/torrent-files-dir

        config:
            ftp_download:
              tls: False
              ftp_tmp_path: /tmp

        TODO:
          - Resume downloads
          - create banlists files
          - validate connection parameters

    """

    schema = {
        'type': 'object',
        'properties': {
            'use-ssl': {'type': 'boolean', 'default': False},
            'ftp_tmp_path': {'type': 'string', 'format': 'path'},
            'delete_origin': {'type': 'boolean', 'default' : False}
        },
        'additionalProperties': False
    }

    def prepare_config(self, config, task):
        config.setdefault('use-ssl', False)
        config.setdefault('delete_origin', False)
        config.setdefault('ftp_tmp_path', os.path.join(task.manager.config_base, 'temp'))
        return config

    def ftp_connect(self, config, ftp_url, current_path):
        if config['use-ssl']:
            ftp = ftplib.FTP_TLS()
        else:
            ftp = ftplib.FTP()
        log.debug("Connecting to " + ftp_url.hostname)
        ftp.connect(ftp_url.hostname, ftp_url.port)
        ftp.login(ftp_url.username, ftp_url.password)
        ftp.sendcmd('TYPE I')
        ftp.set_pasv(True)
        ftp.cwd(current_path)

        return ftp

    def check_connection(self, ftp, config, ftp_url, current_path):
        try:
            ftp.voidcmd("NOOP")
        except:
            ftp = self.ftp_connect(config, ftp_url, current_path)
        return ftp

    def on_task_download(self, task, config):
        config = self.prepare_config(config, task)
        for entry in task.accepted:
            ftp_url = urlparse(entry.get('url'))
            current_path = os.path.dirname(ftp_url.path)
            try:
                ftp = self.ftp_connect(config, ftp_url, current_path)
            except:
                entry.failed("Unable to connect to server")
                break

            if not os.path.isdir(config['ftp_tmp_path']):
                log.debug('creating base path: %s' % config['ftp_tmp_path'])
                os.mkdir(config['ftp_tmp_path'])

            file_name = os.path.basename(ftp_url.path)

            try:
                # Directory
                ftp = self.check_connection(ftp, config, ftp_url, current_path)
                ftp.cwd(file_name)
                self.ftp_walk(ftp, os.path.join(config['ftp_tmp_path'], file_name), config, ftp_url, ftp_url.path)
                ftp = self.check_connection(ftp, config, ftp_url, current_path)
                ftp.cwd('..')
                if config['delete_origin']:
                    ftp.rmd(file_name)
            except ftplib.error_perm:
                # File
                self.ftp_down(ftp, file_name, config['ftp_tmp_path'], config, ftp_url, current_path)

            ftp.close()

    def ftp_walk(self, ftp, tmp_path, config, ftp_url, current_path):
        log.debug("DIR->" + ftp.pwd())
        log.debug("FTP tmp_path : " + tmp_path)
        try:
            ftp = self.check_connection(ftp, config, ftp_url, current_path)
            dirs = ftp.nlst(ftp.pwd())
        except ftplib.error_perm as ex:
            log.info("Error %s" % ex)
            return ftp

        if not dirs:
            return ftp

        for file_name in (path for path in dirs if path not in ('.', '..')):
            file_name = os.path.basename(file_name)
            try:
                ftp = self.check_connection(ftp, config, ftp_url, current_path)
                ftp.cwd(file_name)
                if not os.path.isdir(tmp_path):
                    os.mkdir(tmp_path)
                    log.debug("Directory %s created" % tmp_path)
                ftp = self.ftp_walk(ftp,
                                    os.path.join(tmp_path, os.path.basename(file_name)),
                                    config,
                                    ftp_url,
                                    os.path.join(current_path, os.path.basename(file_name)))
                ftp = self.check_connection(ftp, config, ftp_url, current_path)
                ftp.cwd('..')
                if config['delete_origin']:
                    ftp.rmd(os.path.basename(file_name))
            except ftplib.error_perm:
                ftp = self.ftp_down(ftp, os.path.basename(file_name), tmp_path, config, ftp_url, current_path)
        ftp = self.check_connection(ftp, config, ftp_url, current_path)
        return ftp

    def ftp_down(self, ftp, file_name, tmp_path, config, ftp_url, current_path):
        log.debug("Downloading %s into %s" % (file_name, tmp_path))

        if not os.path.exists(tmp_path):
            os.makedirs(tmp_path)

        local_file = open(os.path.join(tmp_path, file_name), 'a+b')
        ftp = self.check_connection(ftp, config, ftp_url, current_path)
        try:
            ftp.sendcmd("TYPE I")
            file_size = ftp.size(file_name)
        except Exception as e:
            file_size = 1

        max_attempts = 5

        log.info("Starting download of %s into %s" % (file_name, tmp_path))

        while file_size > local_file.tell():
            try:
                if local_file.tell() != 0:
                    ftp = self.check_connection(ftp, config, ftp_url, current_path)
                    ftp.retrbinary('RETR %s' % file_name, local_file.write, local_file.tell())
                else:
                    ftp = self.check_connection(ftp, config, ftp_url, current_path)
                    ftp.retrbinary('RETR %s' % file_name, local_file.write)
            except Exception as error:
                if max_attempts != 0:
                    log.debug("Retrying download after error %s" % error);
                else:
                    log.error("Too many errors downloading %s. Aborting." % file_name)
                    break

        local_file.close()
        if config['delete_origin']:
            ftp = self.check_connection(ftp, config, ftp_url, current_path)
            ftp.delete(file_name)

        return ftp


@event('plugin.register')
def register_plugin():
    plugin.register(OutputFtp, 'ftp_download', api_ver=2)

########NEW FILE########
__FILENAME__ = history
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime

from sqlalchemy import Column, String, Integer, DateTime, Unicode, desc

from flexget import options, plugin
from flexget.event import event
from flexget.manager import Base, Session
from flexget.utils.tools import console

log = logging.getLogger('history')


class History(Base):

    __tablename__ = 'history'

    id = Column(Integer, primary_key=True)
    task = Column('feed', String)
    filename = Column(String)
    url = Column(String)
    title = Column(Unicode)
    time = Column(DateTime)
    details = Column(String)

    def __init__(self):
        self.time = datetime.now()

    def __str__(self):
        return '<History(filename=%s,task=%s)>' % (self.filename, self.task)


class PluginHistory(object):
    """Records all accepted entries for later lookup"""

    schema = {'type': 'boolean'}

    def on_task_learn(self, task, config):
        """Add accepted entries to history"""
        if config is False:
            return  # Explicitly disabled with configuration

        for entry in task.accepted:
            item = History()
            item.task = task.name
            item.filename = entry.get('output', None)
            item.title = entry['title']
            item.url = entry['url']
            reason = ''
            if 'reason' in entry:
                reason = ' (reason: %s)' % entry['reason']
            item.details = 'Accepted by %s%s' % (entry.get('accepted_by', '<unknown>'), reason)
            task.session.add(item)


def do_cli(manager, options):
    session = Session()
    try:
        console('-- History: ' + '-' * 67)
        query = session.query(History)
        if options.search:
            search_term = options.search.replace(' ', '%').replace('.', '%')
            query = query.filter(History.title.like('%' + search_term + '%'))
        query = query.order_by(desc(History.time)).limit(options.limit)
        for item in reversed(query.all()):
            console(' Task    : %s' % item.task)
            console(' Title   : %s' % item.title)
            console(' Url     : %s' % item.url)
            if item.filename:
                console(' Stored  : %s' % item.filename)
            console(' Time    : %s' % item.time.strftime("%c"))
            console(' Details : %s' % item.details)
            console('-' * 79)
    finally:
        session.close()


@event('options.register')
def register_parser_arguments():
    parser = options.register_command('history', do_cli, help='view the history of entries that FlexGet has accepted')
    parser.add_argument('--limit', action='store', type=int, metavar='NUM', default=50,
                        help='limit to %(metavar)s results')
    parser.add_argument('--search', action='store', metavar='TERM', help='limit to results that contain %(metavar)s')


@event('plugin.register')
def register_plugin():
    plugin.register(PluginHistory, 'history', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = html
from __future__ import unicode_literals, division, absolute_import
import os
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.template import render_from_task, get_template

PLUGIN_NAME = 'make_html'

log = logging.getLogger(PLUGIN_NAME)


class OutputHtml:

    schema = {
        'type': 'object',
        'properties': {
            'template': {'type': 'string'},
            'file': {'type': 'string'}
        },
        'required': ['file'],
        'additionalProperties': False
    }

    def on_task_output(self, task, config):
        # Use the default template if none is specified
        if not config.get('template'):
            config['template'] = 'default.template'

        filename = os.path.expanduser(config['template'])
        output = os.path.expanduser(config['file'])
        # Output to config directory if absolute path has not been specified
        if not os.path.isabs(output):
            output = os.path.join(task.manager.config_base, output)

        # create the template
        template = render_from_task(get_template(filename, PLUGIN_NAME), task)

        log.verbose('Writing output html to %s' % output)
        with open(output, 'w') as f:
            f.write(template.encode('utf-8'))

@event('plugin.register')
def register_plugin():
    plugin.register(OutputHtml, PLUGIN_NAME, api_ver=2)

########NEW FILE########
__FILENAME__ = memusage
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import options, plugin
from flexget.event import event
from flexget.utils.tools import console

try:
    from guppy import hpy
except ImportError:
    # this will leave the plugin unloaded
    raise plugin.DependencyError(issued_by='memusage', missing='ext lib `guppy`', silent=True)

log = logging.getLogger('mem_usage')


"""
http://blog.mfabrik.com/2008/03/07/debugging-django-memory-leak-with-trackrefs-and-guppy/

# Print memory statistics
def update():
    print heapy.heap()

# Print relative memory consumption since last sycle
def update():
    print heapy.heap()
    heapy.setref()

# Print relative memory consumption w/heap traversing
def update()
    print heapy.heap().get_rp(40)
    heapy.setref()
"""

heapy = None


@event('manager.execute.started')
def on_exec_started(manager):
    if not manager.options.execute.mem_usage:
        return
    global heapy
    heapy = hpy()


@event('manager.execute.completed')
def on_exec_stopped(manager):
    if not manager.options.execute.mem_usage:
        return
    console('Calculating memory usage:')
    console(heapy.heap())
    console('-' * 79)
    console(heapy.heap().get_rp(40))


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--mem-usage', action='store_true', dest='mem_usage', default=False,
                                               help='display memory usage debug information')

########NEW FILE########
__FILENAME__ = mock_output
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('mock_output')

class MockOutput(object):
    """
    Debugging plugin which records a copy of all accepted entries into a list stored in `mock_output` attribute
    of the task.
    """
    schema = {'type': 'boolean'}

    def on_task_start(self, task, config):
        task.mock_output = []

    def on_task_output(self, task, config):
        task.mock_output.extend(e.copy() for e in task.all_entries if e.accepted)

    def on_task_exit(self, task, config):
        log.verbose('The following titles were output during this task run: %s' %
                    ', '.join(e['title'] for e in task.mock_output))


@event('plugin.register')
def register_plugin():
    plugin.register(MockOutput, 'mock_output', debug=True, api_ver=2)

########NEW FILE########
__FILENAME__ = move
from __future__ import unicode_literals, division, absolute_import
import os
import shutil
import logging
import time

from flexget import plugin
from flexget.event import event
from flexget.utils.template import RenderError
from flexget.utils.pathscrub import pathscrub

log = logging.getLogger('move')


def get_directory_size(directory):
    """
    :param directory: Path
    :return: Size in bytes (recursively)
    """
    dir_size = 0
    for (path, dirs, files) in os.walk(directory):
        for file in files:
            filename = os.path.join(path, file)
            dir_size += os.path.getsize(filename)
    return dir_size


class MovePlugin(object):

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {
                'type': 'object',
                'properties': {
                    'to': {'type': 'string', 'format': 'path'},
                    'filename': {'type': 'string'},
                    'unpack_safety': {'type': 'boolean'},
                    'allow_dir': {'type': 'boolean'},
                    'clean_source': {'type': 'number'},
                    'along': {'type': 'array', 'items': {'type': 'string'}}
                },
                'additionalProperties': False
            }
        ]
    }

    def on_task_output(self, task, config):
        if config is True:
            config = {}
        elif config is False:
            return
        for entry in task.accepted:
            if not 'location' in entry:
                log.warning('Cannot move `%s` because entry does not have location field.' % entry['title'])
                continue

            # SRC
            src = entry['location']
            if not os.path.exists(src):
                log.warning('Cannot move `%s` because location `%s` does not exists (anymore)' % (entry['title'], src))
                continue
            if os.path.isdir(src):
                if not config.get('allow_dir'):
                    log.warning('Cannot move `%s` because location `%s` is a directory' % (entry['title'], src))
                    continue
            elif not os.path.isfile(src):
                log.warning('Cannot move `%s` because location `%s` is not a file ' % (entry['title'], src))
                continue

            src_filename, src_ext = os.path.splitext(src)

            # DST
            filepath, filename = os.path.split(src)
            # get proper value in order of: entry, config, above split
            dst_path = entry.get('path', config.get('to', filepath))
            dst_path = os.path.expanduser(dst_path)

            if entry.get('filename') and entry['filename'] != filename:
                # entry specifies different filename than what was split from the path
                # since some inputs fill in filename it must be different in order to be used
                dst_filename = entry['filename']
            elif 'filename' in config:
                # use from configuration if given
                dst_filename = config['filename']
            else:
                # just use original filename
                dst_filename = filename

            try:
                dst_path = entry.render(dst_path)
            except RenderError:
                log.error('Path value replacement `%s` failed for `%s`' % (dst_path, entry['title']))
                continue
            try:
                dst_filename = entry.render(dst_filename)
            except RenderError:
                log.error('Filename value replacement `%s` failed for `%s`' % (dst_filename, entry['title']))
                continue
            # Clean invalid characters with pathscrub plugin
            dst_path, dst_filename = pathscrub(dst_path), pathscrub(dst_filename, filename=True)

            # Join path and filename
            dst = os.path.join(dst_path, dst_filename)
            if dst == entry['location']:
                log.info('Not moving %s because source and destination are the same.' % dst)
                continue

            if not os.path.exists(dst_path):
                if task.options.test:
                    log.info('Would create `%s`' % dst_path)
                else:
                    log.info('Creating destination directory `%s`' % dst_path)
                    os.makedirs(dst_path)
            if not os.path.isdir(dst_path) and not task.options.test:
                log.warning('Cannot move `%s` because destination `%s` is not a directory' % (entry['title'], dst_path))
                continue

            if src == dst:
                log.verbose('Source and destination are same, skipping `%s`' % entry['title'])
                continue

            # unpack_safety
            if config.get('unpack_safety', entry.get('unpack_safety', True)):
                count = 0
                while True:
                    if count > 60 * 30:
                        entry.fail('Move has been waiting unpacking for 30 minutes')
                        continue
                    size = os.path.getsize(src)
                    time.sleep(1)
                    new_size = os.path.getsize(src)
                    if size != new_size:
                        if not count % 10:
                            log.verbose('File `%s` is possibly being unpacked, waiting ...' % filename)
                    else:
                        break
                    count += 1

            # Check dst contains src_ext
            dst_filename, dst_ext = os.path.splitext(dst)
            if dst_ext != src_ext:
                log.verbose('Adding extension `%s` to dst `%s`' % (src_ext, dst))
                dst += src_ext
            
            # Collect wanted namesakes
            ns_src = []
            ns_dst = []
            if 'along' in config and os.path.isfile(src):
                for ext in config['along']:
                    if not ext.startswith('.'):
                        ext = '.' + ext
                    if os.path.exists(src_filename + ext):
                        ns_src.append(src_filename + ext)
                        ns_dst.append(dst_filename + ext)
            
            # Move stuff
            if task.options.test:
                log.info('Would move `%s` to `%s`' % (src, dst))
                # Collected namesakes
                for nss, nsd in zip(ns_src, ns_dst):
                    log.info('Would also move `%s` to `%s`' % (nss, nsd))
            else:
                try:
                    shutil.move(src, dst)
                except IOError as e:
                    entry.fail('IOError: %s' % (e))
                    log.debug('Unable to move %s to %s' % (src, dst))
                    continue
                # Collected namesakes
                for nss, nsd in zip(ns_src, ns_dst):
                    try:
                        log.info('Moving `%s` to `%s`' % (nss, nsd))
                        shutil.move(nss, nsd)
                    except Exception as err:
                        log.error(err.message)

            entry['output'] = dst
            if 'clean_source' in config:
                if not os.path.isdir(src):
                    base_path = os.path.split(src)[0]
                    size = get_directory_size(base_path) / 1024 / 1024
                    log.debug('base_path: %s size: %s' % (base_path, size))
                    if size <= config['clean_source']:
                        if task.options.test:
                            log.info('Would delete %s and everything under it' % base_path)
                        else:
                            log.info('Deleting `%s`' % base_path)
                            shutil.rmtree(base_path, ignore_errors=True)
                    else:
                        log.info(
                            'Path `%s` left because it exceeds safety value set in clean_source option' % base_path)
                else:
                    log.verbose('Cannot clean_source `%s` because source is a directory' % src)


@event('plugin.register')
def register_plugin():
    plugin.register(MovePlugin, 'move', api_ver=2)

########NEW FILE########
__FILENAME__ = notifymyandroid
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.template import RenderError

log = logging.getLogger('notifymyandroid')

__version__ = 0.1
headers = {'User-Agent': "FlexGet NMA plugin/%s" % str(__version__)}
url = 'https://nma.usk.bz/publicapi/notify'


class OutputNotifyMyAndroid(object):
    """
    Example::

      notifymyandroid:
        apikey: xxxxxxx
        [application: application name, default FlexGet]
        [event: event title, default New Release]
        [priority: -2 - 2 (2 = highest), default 0]

    Configuration parameters are also supported from entries (eg. through set).
    """

    schema = {
        'type': 'object',
        'properties': {
            'apikey': {'type': 'string'},
            'application': {'type': 'string', 'default': 'FlexGet'},
            'event': {'type': 'string', 'default': 'New release'},
            'description': {'type': 'string', 'default': '{{title}}'},
            'priority': {'type': 'integer', 'default': 0}
        },
        'required': ['apikey'],
        'additionalProperties': False
    }

    # Run last to make sure other outputs are successful before sending notification
    @plugin.priority(0)
    def on_task_output(self, task, config):
        for entry in task.accepted:

            if task.options.test:
                log.info("Would send notifymyandroid message about: %s", entry['title'])
                continue

            apikey = entry.get('apikey', config['apikey'])
            priority = entry.get('priority', config['priority'])
            application = entry.get('application', config['application'])
            try:
                application = entry.render(application)
            except RenderError as e:
                log.error('Error setting nma application: %s' % e)
            event = entry.get('event', config['event'])
            try:
                event = entry.render(event)
            except RenderError as e:
                log.error('Error setting nma event: %s' % e)
            description = config['description']
            try:
                description = entry.render(description)
            except RenderError as e:
                log.error('Error setting nma description: %s' % e)

            # Send the request
            data = {'priority': priority, 'application': application, 'apikey': apikey,
                    'event': event, 'description': description}
            response = task.requests.post(url, headers=headers, data=data, raise_status=False)

            # Check if it succeeded
            request_status = response.status_code

            # error codes and messages from http://nma.usk.bz/api.php
            if request_status == 200:
                log.debug("NotifyMyAndroid message sent")
            elif request_status == 400:
                log.error("Bad request, the parameters you provided did not validate")
            elif request_status == 401:
                log.error("Not authorized, the API key given is not valid, and does not correspond to a user.")
            elif request_status == 402:
                log.error("Not acceptable, your IP address has exceeded the API limit.")
            elif request_status == 500:
                log.error("Internal server error, something failed to execute properly on the NotifyMyAndroid side.")
            else:
                log.error("Unknown error when sending NotifyMyAndroid message")


@event('plugin.register')
def register_plugin():
    plugin.register(OutputNotifyMyAndroid, 'notifymyandroid', api_ver=2)

########NEW FILE########
__FILENAME__ = notify_osd
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.template import RenderError, render_from_task

log = logging.getLogger('notify_osd')


class OutputNotifyOsd(object):

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {
                'type': 'object',
                'properties': {
                    'title_template': {'type': 'string'},
                    'item_template': {'type': 'string'},
                    'timeout': {'type': 'integer'}
                },
                'additionalProperties': False
            }
        ]
    }

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {}
        config.setdefault('title_template', '{{task.name}}')
        config.setdefault('item_template', '{{title}}')
        config.setdefault('timeout', 4)
        return config

    def on_task_start(self, task, config):
        try:
            from gi.repository import Notify
        except ImportError as e:
            log.debug('Error importing Notify: %s' % e)
            raise plugin.DependencyError('notify_osd', 'gi.repository', 'Notify module required. ImportError: %s' % e)

    @plugin.priority(0)
    def on_task_output(self, task, config):
        """
        Configuration::
            notify_osd:
                title_template: Notification title, supports jinja templating, default {{task.name}}
                item_template: Notification body, suports jinja templating, default {{title}}
                timeout: Set how long the Notification is displayed, this is an integer default = 4 seconds, Default: 4
        """
        from gi.repository import Notify

        if not config or not task.accepted:
            return

        config = self.prepare_config(config)
        body_items = []
        for entry in task.accepted:
            try:
                body_items.append(entry.render(config['item_template']))
            except RenderError as e:
                log.error('Error setting body message: %s' % e)
        log.verbose("Send Notify-OSD notification about: %s", " - ".join(body_items))

        title = config['title_template']
        try:
            title = render_from_task(title, task)
            log.debug('Setting bubble title to :%s', title)
        except RenderError as e:
            log.error('Error setting title Notify-osd message: %s' % e)

        if not Notify.init("Flexget"):
            log.error('Unable to init libnotify.')
            return

        n = Notify.Notification.new(title, '\n'.join(body_items), None)
        timeout = (config['timeout'] * 1000)
        n.set_timeout(timeout)

        if not n.show():
            log.error('Unable to send notification for %s', title)
            return


@event('plugin.register')
def register_plugin():
    plugin.register(OutputNotifyOsd, 'notify_osd', api_ver=2)

########NEW FILE########
__FILENAME__ = notify_xmpp
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.template import RenderError, render_from_task

log = logging.getLogger('notify_xmpp')


try:
    import sleekxmpp
    
    class SendMsgBot(sleekxmpp.ClientXMPP):
    
        def __init__(self, jid, password, recipient, message):
            sleekxmpp.ClientXMPP.__init__(self, jid, password)
            self.recipient = recipient
            self.msg = message
            self.add_event_handler("session_start", self.start, threaded=True)
            self.register_plugin('xep_0030') # Service Discovery
            self.register_plugin('xep_0199') # XMPP Ping
    
        def start(self, event):
            self.send_presence(pto=self.recipient)
            self.send_message(mto=self.recipient, mbody=self.msg, mtype='chat')
            self.disconnect(wait=True)

except ImportError:
    # If sleekxmpp is not found, errors will be shown later
    pass
        
class OutputNotifyXmpp(object):
    
    schema = {
        'type': 'object',
        'properties': {
            'sender': {'type': 'string', 'format': 'email'},
            'password': {'type': 'string'},
            'recipient': {'type': 'string', 'format': 'email'},
            'title': {'type': 'string', 'default': '{{task.name}}'},
            'text': {'type': 'string', 'default': '{{title}}'}
        },
        'required': ['sender', 'password', 'recipient'],
        'additionalProperties': False
    }
    
    __version__ = '0.1'

    def on_task_start(self, task, config):
        try:
            import sleekxmpp
        except ImportError as e:
            log.debug('Error importing SleekXMPP: %s' % e)
            raise plugin.DependencyError('notify_xmpp', 'sleekxmpp', 'SleekXMPP module required. ImportError: %s' % e)
        try:
            import dnspython
        except ImportError as e:
            log.debug('Error importing dnspython: %s' % e)
            raise plugin.DependencyError('notify_xmpp', 'dnspython', 'dnspython module required. ImportError: %s' % e)
    
    def on_task_output(self, task, config):
        """
        Configuration::
            notify_xmpp:
                title: Message title, supports jinja templating, default {{task.name}}
                text: Message text, suports jinja templating, default {{title}}
        """
        if not config or not task.accepted:
            return
        
        title = config['title']
        try:
            title = render_from_task(title, task)
            log.debug('Setting message title to :%s', title)
        except RenderError as e:
            log.error('Error setting title message: %s' % e)
        items = []
        for entry in task.accepted:
            try:
                items.append(entry.render(config['text']))
            except RenderError as e:
                log.error('Error setting text message: %s' % e)
        text = '%s\n%s' % (title, '\n'.join(items))
        
        log.verbose('Send XMPP notification about: %s', ' - '.join(items))
        logging.getLogger('sleekxmpp').setLevel(logging.CRITICAL)
        
        xmpp = SendMsgBot(config['sender'], config['password'], config['recipient'], text)
        if xmpp.connect():
            xmpp.process(block=True)


@event('plugin.register')
def register_plugin():
    plugin.register(OutputNotifyXmpp, 'notify_xmpp', api_ver=2)

########NEW FILE########
__FILENAME__ = nzbget
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('nzbget')


class OutputNzbget(object):
    """
    Example::

      nzbget:
        url: http://nzbget:12345@localhost:6789/xmlrpc
        category: movies
        priority: 0
        top: False
    """

    schema = {
        'type': 'object',
        'properties': {
            'url': {'type': 'string'},
            'category': {'type': 'string', 'default': ''},
            'priority': {'type': 'integer', 'default': 0},
            'top': {'type': 'boolean', 'default': False}
        },
        'required': ['url'],
        'additionalProperties': False
    }

    def on_task_output(self, task, config):
        from xmlrpclib import ServerProxy

        params = dict(config)

        server = ServerProxy(params["url"])

        for entry in task.accepted:
            if task.options.test:
                log.info('Would add into nzbget: %s' % entry['title'])
                continue

            # allow overriding the category
            if 'category' in entry:
                params['category'] = entry['category']

            try:
                server.appendurl(entry["title"] + '.nzb', params["category"], params["priority"], params["top"], entry["url"])
                log.info("Added `%s` to nzbget" % entry["title"])
            except:
                log.critical("rpc call to nzbget failed")
                entry.fail("could not call appendurl via RPC")


@event('plugin.register')
def register_plugin():
    plugin.register(OutputNzbget, 'nzbget', api_ver=2)

########NEW FILE########
__FILENAME__ = prowl
from __future__ import unicode_literals, division, absolute_import
import logging

from requests import RequestException

from flexget import plugin
from flexget.event import event
from flexget.utils.template import RenderError

__version__ = 0.1

log = logging.getLogger('prowl')

headers = {'User-Agent': 'FlexGet Prowl plugin/%s' % str(__version__)}


class OutputProwl(object):
    """
    Send prowl notifications

    Example::

      prowl:
        apikey: xxxxxxx
        [application: application name, default FlexGet]
        [event: event title, default New Release]
        [priority: -2 - 2 (2 = highest), default 0]
        [description: notification to send]

    Configuration parameters are also supported from entries (eg. through set).
    """

    def validator(self):
        from flexget import validator
        config = validator.factory('dict')
        config.accept('text', key='apikey', required=True)
        config.accept('text', key='application')
        config.accept('text', key='event')
        config.accept('integer', key='priority')
        config.accept('text', key='description')
        return config

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {'enabled': config}
        config.setdefault('apikey', '')
        config.setdefault('application', 'FlexGet')
        config.setdefault('event', 'New release')
        config.setdefault('priority', 0)
        return config

    # Run last to make sure other outputs are successful before sending notification
    @plugin.priority(0)
    def on_task_output(self, task, config):
        config = self.prepare_config(config)
        for entry in task.accepted:

            # get the parameters
            apikey = entry.get('apikey', config['apikey'])
            application = entry.get('application', config['application'])
            event = entry.get('event', config['event'])
            priority = entry.get('priority', config['priority'])
            description = config.get('description', entry['title'])

            # If event has jinja template, render it
            try:
                event = entry.render(event)
            except RenderError as e:
                log.error('Error rendering jinja event: %s' % e)

            # If description has jinja template, render it
            try:
                description = entry.render(description)
            except RenderError as e:
                description = entry['title']
                log.error('Error rendering jinja description: %s' % e)

            url = 'https://api.prowlapp.com/publicapi/add'
            data = {'priority': priority, 'application': application, 'apikey': apikey,
                    'event': event, 'description': description}

            if task.options.test:
                log.info('Would send prowl message about: %s', entry['title'])
                log.debug('options: %s' % data)
                continue

            try:
                response = task.requests.post(url, headers=headers, data=data, raise_status=False)
            except RequestException as e:
                log.error('Error with request: %s' % e)
                continue

            # Check if it succeeded
            request_status = response.status_code

            # error codes and messages from http://prowl.weks.net/api.php
            if request_status == 200:
                log.debug("Prowl message sent")
            elif request_status == 400:
                log.error("Bad request, the parameters you provided did not validate")
            elif request_status == 401:
                log.error("Not authorized, the API key given is not valid, and does not correspond to a user.")
            elif request_status == 406:
                log.error("Not acceptable, your IP address has exceeded the API limit.")
            elif request_status == 409:
                log.error("Not approved, the user has yet to approve your retrieve request.")
            elif request_status == 500:
                log.error("Internal server error, something failed to execute properly on the Prowl side.")
            else:
                log.error("Unknown error when sending Prowl message")


@event('plugin.register')
def register_plugin():
    plugin.register(OutputProwl, 'prowl', api_ver=2)

########NEW FILE########
__FILENAME__ = pushbullet
from __future__ import unicode_literals, division, absolute_import
import logging
import base64

from flexget import plugin
from flexget.event import event
from flexget.utils import json
from flexget.utils.template import RenderError

log = logging.getLogger("pushbullet")

__version__ = 0.1
client_headers = {"User-Agent": "FlexGet Pushbullet plugin/%s" % str(__version__)}
pushbullet_url = "https://api.pushbullet.com/api/pushes"


class OutputPushbullet(object):
    """
    Example::

      pushbullet:
        apikey: <API_KEY>
        device: <DEVICE_IDEN> (can also be a list of device idens, or don't specify any idens to send to all devices)
        [title: <MESSAGE_TITLE>] (default: "{{task}} - Download started" -- accepts Jinja2)
        [body: <MESSAGE_BODY>] (default: "{{series_name}} {{series_id}}" -- accepts Jinja2)

    Configuration parameters are also supported from entries (eg. through set).
    """

    def validator(self):
        from flexget import validator
        config = validator.factory("dict")
        config.accept("text", key="apikey", required=True)
        config.accept("text", key="device", required=False)
        config.accept("list", key="device").accept("text")
        config.accept("text", key="title", required=False)
        config.accept("text", key="body", required=False)
        return config

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {"enabled": config}

        # TODO: don't assume it's a download
        config.setdefault("title", "{{task}} - Download started")
        # TODO: use template file
        config.setdefault("body", "{% if series_name is defined %}{{tvdb_series_name|d(series_name)}} "
                                     "{{series_id}} {{tvdb_ep_name|d('')}}{% elif imdb_name is defined %}{{imdb_name}} "
                                     "{{imdb_year}}{% else %}{{title}}{% endif %}")
        config.setdefault("device", None)
        return config

    # Run last to make sure other outputs are successful before sending notification
    @plugin.priority(0)
    def on_task_output(self, task, config):
        # get the parameters
        config = self.prepare_config(config)

        # Support for multiple devices 
        devices = config["device"]
        if not isinstance(devices, list):
            devices = [devices]
 
        # Set a bunch of local variables from the config
        apikey = config["apikey"]
        device = config["device"]
            
        client_headers["Authorization"] = "Basic %s" % base64.b64encode(apikey)
        
        if task.options.test:
            log.info("Test mode. Pushbullet configuration:")
            log.info("    API_KEY: %s" % apikey)
            log.info("    Type: Note")
            log.info("    Device: %s" % device)

        # Loop through the provided entries
        for entry in task.accepted:

            title = config["title"]
            body = config["body"]

            # Attempt to render the title field
            try:
                title = entry.render(title)
            except RenderError as e:
                log.warning("Problem rendering 'title': %s" % e)
                title = "Download started"

            # Attempt to render the body field
            try:
                body = entry.render(body)
            except RenderError as e:
                log.warning("Problem rendering 'body': %s" % e)
                body = entry["title"]

            for device in devices:
                # Build the request
                if not device:
                    data = {"type": "note", "title": title, "body": body}
                else:
                    data = {"device_iden": device, "type": "note", "title": title, "body": body}

                # Check for test mode
                if task.options.test:
                    log.info("Test mode. Pushbullet notification would be:")
                    log.info("    Title: %s" % title)
                    log.info("    Body: %s" % body)
                    
                    # Test mode.  Skip remainder.
                    continue

                # Make the request
                response = task.requests.post(pushbullet_url, headers=client_headers, data=data, raise_status=False)

                # Check if it succeeded
                request_status = response.status_code

                # error codes and messages from Pushbullet API
                if request_status == 200:
                    log.debug("Pushbullet notification sent")
                elif request_status == 500:
                    log.warning("Pushbullet notification failed, Pushbullet API having issues")
                    #TODO: Implement retrying. API requests 5 seconds between retries.
                elif request_status >= 400:
                    error = json.loads(response.content)['error']
                    log.error("Pushbullet API error: %s" % error['message'])
                else:
                    log.error("Unknown error when sending Pushbullet notification")

@event('plugin.register')
def register_plugin():
    plugin.register(OutputPushbullet, "pushbullet", api_ver=2)
########NEW FILE########
__FILENAME__ = pushover
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils import json
from flexget.utils.template import RenderError

log = logging.getLogger("pushover")

__version__ = 0.1
client_headers = {"User-Agent": "FlexGet Pushover plugin/%s" % str(__version__)}
pushover_url = "https://api.pushover.net/1/messages.json"


class OutputPushover(object):
    """
    Example::

      pushover:
        userkey: <USER_KEY> (can also be a list of userkeys)
        apikey: <API_KEY>
        [device: <DEVICE_STRING>] (default: (none))
        [title: <MESSAGE_TITLE>] (default: "Download started" -- accepts Jinja2)
        [message: <MESSAGE_BODY>] (default: "{{series_name}} {{series_id}}" -- accepts Jinja2)
        [priority: <PRIORITY>] (default = 0 -- normal = 0, high = 1, silent = -1)
        [url: <URL>] (default: "{{imdb_url}}" -- accepts Jinja2)
        [sound: <SOUND>] (default: pushover)

    Configuration parameters are also supported from entries (eg. through set).
    """

    def validator(self):
        from flexget import validator
        config = validator.factory("dict")
        config.accept("text", key="userkey", required=True)
        config.accept("list", key="userkey").accept("text")
        config.accept("text", key="apikey", required=True)
        config.accept("text", key="device", required=False)
        config.accept("text", key="title", required=False)
        config.accept("text", key="message", required=False)
        config.accept("integer", key="priority", required=False)
        config.accept("url", key="url", required=False)
        config.accept("text", key="sound", required=False)
        return config

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {"enabled": config}

        # Set the defaults
        config.setdefault("device", None)
        # TODO: don't assume it's a download
        config.setdefault("title", "{{task}}")
        # TODO: use template file
        config.setdefault("message", "{% if series_name is defined %}{{tvdb_series_name|d(series_name)}} "
                                     "{{series_id}} {{tvdb_ep_name|d('')}}{% elif imdb_name is defined %}{{imdb_name}} "
                                     "{{imdb_year}}{% else %}{{title}}{% endif %}")
        config.setdefault("priority", 0)
        config.setdefault("url", "{% if imdb_url is defined %}{{imdb_url}}{% endif %}")
        config.setdefault("sound", "pushover")

        return config

    # Run last to make sure other outputs are successful before sending notification
    @plugin.priority(0)
    def on_task_output(self, task, config):
        # get the parameters
        config = self.prepare_config(config)

        # Support for multiple userkeys 
        userkeys = config["userkey"]
        if not isinstance(userkeys, list):
            userkeys = [userkeys]
            
        # Set a bunch of local variables from the config
        apikey = config["apikey"]
        device = config["device"]
        priority = config["priority"]
        sound = config["sound"]

        # Loop through the provided entries
        for entry in task.accepted:

            title = config["title"]
            message = config["message"]
            url = config["url"]

            # Attempt to render the title field
            try:
                title = entry.render(title)
            except RenderError as e:
                log.warning("Problem rendering 'title': %s" % e)
                title = "Download started"

            # Attempt to render the message field
            try:
                message = entry.render(message)
            except RenderError as e:
                log.warning("Problem rendering 'message': %s" % e)
                message = entry["title"]

            # Attempt to render the url field
            try:
                url = entry.render(url)
            except RenderError as e:
                log.warning("Problem rendering 'url': %s" % e)
                url = entry.get("imdb_url", "")

            for userkey in userkeys:
                # Build the request
                data = {"user": userkey, "token": apikey, "title": title, "message": message, "url": url}
                if device:
                    data["device"] = device
                if priority:
                    data["priority"] = priority
                if sound:
                    data["sound"] = sound
    
                # Check for test mode
                if task.options.test:
                    log.info("Test mode.  Pushover notification would be:")
                    if device:
                        log.info("    Device: %s" % device)
                    else:
                        log.info("    Device: [broadcast]")
                    log.info("    Title: %s" % title)
                    log.info("    Message: %s" % message)
                    log.info("    URL: %s" % url)
                    log.info("    Priority: %d" % priority)
                    log.info("    userkey: %s" % userkey)
                    log.info("    apikey: %s" % apikey)
                    log.info("    sound: %s" % sound)

                    # Test mode.  Skip remainder.
                    continue

                # Make the request
                response = task.requests.post(pushover_url, headers=client_headers, data=data, raise_status=False)
    
                # Check if it succeeded
                request_status = response.status_code
    
                # error codes and messages from Pushover API
                if request_status == 200:
                    log.debug("Pushover notification sent")
                elif request_status == 500:
                    log.debug("Pushover notification failed, Pushover API having issues")
                    #TODO: Implement retrying. API requests 5 seconds between retries.
                elif request_status >= 400:
                    errors = json.loads(response.content)['errors']
                    log.error("Pushover API error: %s" % errors[0])
                else:
                    log.error("Unknown error when sending Pushover notification")


@event('plugin.register')
def register_plugin():
    plugin.register(OutputPushover, "pushover", api_ver=2)

########NEW FILE########
__FILENAME__ = pyload
# -*- coding: utf-8 -*-

from __future__ import unicode_literals, division, absolute_import
from logging import getLogger
from urllib import quote

from requests.exceptions import RequestException

from flexget import plugin, validator
from flexget.event import event
from flexget.utils import json, requests

log = getLogger('pyload')


class PluginPyLoad(object):
    """
    Parse task content or url for hoster links and adds them to pyLoad.

    Example::

      pyload:
        api: http://localhost:8000/api
        queue: yes
        username: my_username
        password: my_password
        folder: desired_folder
        package: desired_package_name (jinja2 supported)
        hoster:
          - YoutubeCom
        parse_url: no
        multiple_hoster: yes
        enabled: yes

    Default values for the config elements::

      pyload:
          api: http://localhost:8000/api
          queue: no
          hoster: ALL
          parse_url: no
          multiple_hoster: yes
          enabled: yes
    """

    __author__ = 'http://pyload.org'
    __version__ = '0.4'

    DEFAULT_API = 'http://localhost:8000/api'
    DEFAULT_QUEUE = False
    DEFAULT_FOLDER = ''
    DEFAULT_HOSTER = []
    DEFAULT_PARSE_URL = False
    DEFAULT_MULTIPLE_HOSTER = True
    DEFAULT_PREFERRED_HOSTER_ONLY = False
    DEFAULT_HANDLE_NO_URL_AS_FAILURE = False

    def validator(self):
        """Return config validator"""
        root = validator.factory()
        root.accept('boolean')
        advanced = root.accept('dict')
        advanced.accept('text', key='api')
        advanced.accept('text', key='username')
        advanced.accept('text', key='password')
        advanced.accept('text', key='folder')
        advanced.accept('text', key='package')
        advanced.accept('boolean', key='queue')
        advanced.accept('boolean', key='parse_url')
        advanced.accept('boolean', key='multiple_hoster')
        advanced.accept('list', key='hoster').accept('text')
        advanced.accept('boolean', key='preferred_hoster_only')
        advanced.accept('boolean', key='handle_no_url_as_failure')
        return root

    def on_task_output(self, task, config):
        if not config.get('enabled', True):
            return
        if not task.accepted:
            return

        self.add_entries(task, config)

    def add_entries(self, task, config):
        """Adds accepted entries"""

        try:
            session = self.get_session(config)
        except IOError:
            raise plugin.PluginError('pyLoad not reachable', log)
        except plugin.PluginError:
            raise
        except Exception as e:
            raise plugin.PluginError('Unknown error: %s' % str(e), log)

        api = config.get('api', self.DEFAULT_API)
        hoster = config.get('hoster', self.DEFAULT_HOSTER)
        folder = config.get('folder', self.DEFAULT_FOLDER)

        for entry in task.accepted:
            # bunch of urls now going to check
            content = entry.get('description', '') + ' ' + quote(entry['url'])
            content = json.dumps(content.encode("utf8"))

            url = json.dumps(entry['url']) if config.get('parse_url', self.DEFAULT_PARSE_URL) else "''"

            log.debug("Parsing url %s" % url)

            result = query_api(api, "parseURLs", {"html": content, "url": url, "session": session})

            # parsed { plugins: [urls] }
            parsed = result.json()

            urls = []

            # check for preferred hoster
            for name in hoster:
                if name in parsed:
                    urls.extend(parsed[name])
                    if not config.get('multiple_hoster', self.DEFAULT_MULTIPLE_HOSTER):
                        break

            # no preferred hoster and not preferred hoster only - add all recognized plugins
            if not urls and not config.get('preferred_hoster_only', self.DEFAULT_PREFERRED_HOSTER_ONLY):
                for name, purls in parsed.iteritems():
                    if name != "BasePlugin":
                        urls.extend(purls)

            if task.options.test:
                log.info('Would add `%s` to pyload' % urls)
                continue

            # no urls found
            if not urls:
                if config.get('handle_no_url_as_failure', self.DEFAULT_HANDLE_NO_URL_AS_FAILURE):
                    entry.fail("No suited urls in entry %s" % entry['title'])
                else:
                    log.info("No suited urls in entry %s" % entry['title'])
                continue

            log.debug("Add %d urls to pyLoad" % len(urls))

            try:
                dest = 1 if config.get('queue', self.DEFAULT_QUEUE) else 0  # Destination.Queue = 1

                # Use the title of the enty, if no naming schema for the package is defined.
                name = config.get('package', entry['title'])

                # If name has jinja template, render it
                try:
                    name = entry.render(name)
                except RenderError as e:
                    name = entry['title']
                    log.error('Error rendering jinja event: %s' % e)

                post = {'name': "'%s'" % name.encode("ascii", "ignore"),
                        'links': str(urls),
                        'dest': dest,
                        'session': session}

                pid = query_api(api, "addPackage", post).text
                log.debug('added package pid: %s' % pid)

                if folder:
                    # set folder with api
                    data = {'folder': folder}
                    query_api(api, "setPackageData", {'pid': pid, 'data': data, 'session': session})

            except Exception as e:
                entry.fail(str(e))

    def get_session(self, config):
        url = config.get('api', self.DEFAULT_API)

        # Login
        post = {'username': config['username'], 'password': config['password']}
        result = query_api(url, "login", post)
        response = result.json()
        if not response:
            raise plugin.PluginError('Login failed', log)
        return response.replace('"', '')


def query_api(url, method, post=None):
    try:
        response = requests.request(
            'post' if post is not None else 'get',
            url.rstrip("/") + "/" + method.strip("/"),
            data=post)
        response.raise_for_status()
        return response
    except RequestException as e:
        if e.response.status_code == 500:
            raise plugin.PluginError('Internal API Error: <%s> <%s> <%s>' % (method, url, post), log)
        raise


@event('plugin.register')
def register_plugin():
    plugin.register(PluginPyLoad, 'pyload', api_ver=2)

########NEW FILE########
__FILENAME__ = queue_movies
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils import qualities

try:
    from flexget.plugins.filter.movie_queue import queue_add, QueueError
except ImportError:
    raise plugin.DependencyError(issued_by='queue_movies', missing='movie_queue')

log = logging.getLogger('queue_movies')


class QueueMovies(object):
    """Adds all accepted entries to your movie queue."""

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {
                'type': 'object',
                'properties': {'quality': {'type': 'string', 'format': 'quality_requirements'}},
                'additionalProperties': False
            }
        ]
    }

    def on_task_output(self, task, config):
        if not config:
            return
        if not isinstance(config, dict):
            config = {}
        for entry in task.accepted:
            # Tell tmdb_lookup to add lazy lookup fields if not already present
            try:
                plugin.get_plugin_by_name('tmdb_lookup').instance.lookup(entry)
            except plugin.DependencyError:
                log.debug('tmdb_lookup is not available, queue will not work if movie ids are not populated')
            # Find one or both movie id's for this entry. See if an id is already populated before incurring lazy lookup
            kwargs = {}
            for lazy in [False, True]:
                if entry.get('imdb_id', eval_lazy=lazy):
                    kwargs['imdb_id'] = entry['imdb_id']
                if entry.get('tmdb_id', eval_lazy=lazy):
                    kwargs['tmdb_id'] = entry['tmdb_id']
                if kwargs:
                    break
            if not kwargs:
                log.warning('Could not determine a movie id for %s, it will not be added to queue.' % entry['title'])
                continue

            # since entries usually have unknown quality we need to ignore that ..
            if entry.get('quality'):
                quality = qualities.Requirements(entry['quality'].name)
            else:
                quality = qualities.Requirements(config.get('quality', 'any'))

            kwargs['quality'] = quality
            # Provide movie title if it is already available, to avoid movie_queue doing a lookup
            kwargs['title'] = (entry.get('imdb_name', eval_lazy=False) or
                               entry.get('tmdb_name', eval_lazy=False) or
                               entry.get('movie_name', eval_lazy=False))
            log.debug('queueing kwargs: %s' % kwargs)
            try:
                queue_add(**kwargs)
            except QueueError as e:
                # Ignore already in queue errors
                if e.errno != 1:
                    entry.fail('Error adding movie to queue: %s' % e.message)


@event('plugin.register')
def register_plugin():
    plugin.register(QueueMovies, 'queue_movies', api_ver=2)

########NEW FILE########
__FILENAME__ = rapidpush
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils import json
from flexget.utils.template import RenderError

log = logging.getLogger('rapidpush')

__version__ = 0.4
headers = {'User-Agent': "FlexGet RapidPush plugin/%s" % str(__version__)}
url = 'https://rapidpush.net/api'


class OutputRapidPush(object):
    """
    Example::

      rapidpush:
        apikey: xxxxxxx (can also be a list of api keys)
        [category: category, default FlexGet]
        [title: title, default New release]
        [group: device group, default no group]
        [message: the message, default {{title}}]
        [channel: the broadcast notification channel, if provided it will be send to the channel subscribers instead of
            your devices, default no channel]
        [priority: 0 - 6 (6 = highest), default 2 (normal)]
        [notify_accepted: boolean true or false, default true]
        [notify_rejected: boolean true or false, default false]
        [notify_failed: boolean true or false, default false]
        [notify_undecided: boolean true or false, default false]

    Configuration parameters are also supported from entries (eg. through set).
    """

    def validator(self):
        from flexget import validator
        config = validator.factory('dict')
        config.accept('text', key='apikey', required=True)
        config.accept('list', key='apikey').accept('text')
        config.accept('text', key='category')
        config.accept('text', key='title')
        config.accept('text', key='group')
        config.accept('text', key='channel')
        config.accept('integer', key='priority')
        config.accept('text', key='message')
        config.accept('boolean', key='notify_accepted')
        config.accept('boolean', key='notify_rejected')
        config.accept('boolean', key='notify_failed')
        config.accept('boolean', key='notify_undecided')
        return config

    def prepare_config(self, config):
        config.setdefault('title', 'New release')
        config.setdefault('category', 'FlexGet')
        config.setdefault('priority', 2)
        config.setdefault('group', '')
        config.setdefault('channel', '')
        config.setdefault('message', '{{title}}')
        config.setdefault('notify_accepted', True)
        config.setdefault('notify_rejected', False)
        config.setdefault('notify_failed', False)
        config.setdefault('notify_undecided', False)
        return config

    # Run last to make sure other outputs are successful before sending notification
    @plugin.priority(0)
    def on_task_output(self, task, config):
        # get the parameters
        config = self.prepare_config(config)

        if config['notify_accepted']:
            log.info("Notify accepted entries")
            self.process_notifications(task, task.accepted, config)
        if config['notify_rejected']:
            log.info("Notify rejected entries")
            self.process_notifications(task, task.rejected, config)
        if config['notify_failed']:
            log.info("Notify failed entries")
            self.process_notifications(task, task.failed, config)
        if config['notify_undecided']:
            log.info("Notify undecided entries")
            self.process_notifications(task, task.undecided, config)

    # Process the given events.
    def process_notifications(self, task, entries, config):
        for entry in entries:
            if task.options.test:
                log.info("Would send RapidPush notification about: %s", entry['title'])
                continue

            log.info("Send RapidPush notification about: %s", entry['title'])
            apikey = entry.get('apikey', config['apikey'])
            if isinstance(apikey, list):
                apikey = ','.join(apikey)

            title = config['title']
            try:
                title = entry.render(title)
            except RenderError as e:
                log.error('Error setting RapidPush title: %s' % e)

            message = config['message']
            try:
                message = entry.render(message)
            except RenderError as e:
                log.error('Error setting RapidPush message: %s' % e)

            # Check if we have to send a normal or a broadcast notification.
            if not config['channel']:
                priority = entry.get('priority', config['priority'])

                category = entry.get('category', config['category'])
                try:
                    category = entry.render(category)
                except RenderError as e:
                    log.error('Error setting RapidPush category: %s' % e)

                group = entry.get('group', config['group'])
                try:
                    group = entry.render(group)
                except RenderError as e:
                    log.error('Error setting RapidPush group: %s' % e)

                # Send the request
                data_string = json.dumps({
                    'title': title,
                    'message': message,
                    'priority': priority,
                    'category': category,
                    'group': group})
                data = {'apikey': apikey, 'command': 'notify', 'data': data_string}
            else:
                channel = config['channel']
                try:
                    channel = entry.render(channel)
                except RenderError as e:
                    log.error('Error setting RapidPush channel: %s' % e)

                # Send the broadcast request
                data_string = json.dumps({
                    'title': title,
                    'message': message,
                    'channel': channel})
                data = {'apikey': apikey, 'command': 'broadcast', 'data': data_string}

            response = task.requests.post(url, headers=headers, data=data, raise_status=False)

            json_data = response.json()
            if 'code' in json_data:
                if json_data['code'] == 200:
                    log.debug("RapidPush message sent")
                else:
                    log.error(json_data['desc'] + " (" + str(json_data['code']) + ")")
            else:
                for item in json_data:
                    if json_data[item]['code'] == 200:
                        log.debug(item + ": RapidPush message sent")
                    else:
                        log.error(item + ": " + json_data[item]['desc'] + " (" + str(json_data[item]['code']) + ")")


@event('plugin.register')
def register_plugin():
    plugin.register(OutputRapidPush, 'rapidpush', api_ver=2)

########NEW FILE########
__FILENAME__ = rss
from __future__ import unicode_literals, division, absolute_import
import base64
import hashlib
import logging
import datetime
import os

from sqlalchemy import Column, Integer, String, DateTime, Unicode

from flexget import db_schema, plugin
from flexget.event import event
from flexget.utils.sqlalchemy_utils import table_columns, table_add_column
from flexget.utils.template import render_from_entry, get_template, RenderError

log = logging.getLogger('make_rss')
Base = db_schema.versioned_base('make_rss', 0)

rss2gen = True
try:
    import PyRSS2Gen
except:
    rss2gen = False


@db_schema.upgrade('make_rss')
def upgrade(ver, session):
    if ver is None:
        columns = table_columns('make_rss', session)
        if not 'rsslink' in columns:
            log.info('Adding rsslink column to table make_rss.')
            table_add_column('make_rss', 'rsslink', String, session)
        ver = 0
    return ver


class RSSEntry(Base):

    __tablename__ = 'make_rss'

    id = Column(Integer, primary_key=True)
    title = Column(Unicode)
    description = Column(Unicode)
    link = Column(String)
    rsslink = Column(String)
    file = Column(Unicode)
    published = Column(DateTime, default=datetime.datetime.utcnow())


class OutputRSS(object):
    """
    Write RSS containing succeeded (downloaded) entries.

    Example::

      make_rss: ~/public_html/flexget.rss

    You may write into same file in multiple tasks.

    Example::

      my-task-A:
        make_rss: ~/public_html/series.rss
        .
        .
      my-task-B:
        make_rss: ~/public_html/series.rss
        .
        .

    With this example file series.rss would contain succeeded
    entries from both tasks.

    **Number of days / items**

    By default output contains items from last 7 days. You can specify
    different perioid, number of items or both. Value -1 means unlimited.

    Example::

      make_rss:
        file: ~/public_html/series.rss
        days: 2
        items: 10

    Generate RSS that will containing last two days and no more than 10 items.

    Example 2::

      make_rss:
        file: ~/public_html/series.rss
        days: -1
        items: 50

    Generate RSS that will contain last 50 items, regardless of dates.

    RSS location link:

    You can specify the url location of the rss file.

    Example::

      make_rss:
        file: ~/public_html/series.rss
        rsslink: http://my.server.net/series.rss

    **RSS link**

    You can specify what field from entry is used as a link in generated rss feed.

    Example::

      make_rss:
        file: ~/public_html/series.rss
        link:
          - imdb_url

    List should contain a list of fields in order of preference.
    Note that the url field is always used as last possible fallback
    even without explicitly adding it into the list.

    Default list: imdb_url, input_url, url
    """

    schema = {
        'oneOf': [
            {'type': 'string'},  # TODO: path / file
            {
                'type': 'object',
                'properties': {
                    'file': {'type': 'string'},
                    'days': {'type': 'integer'},
                    'items': {'type': 'integer'},
                    'history': {'type': 'boolean'},
                    'rsslink': {'type': 'string'},
                    'encoding': {'type': 'string'},  # TODO: only valid choices
                    'title': {'type': 'string'},
                    'template': {'type': 'string'},
                    'link': {'type': 'array', 'items': {'type': 'string'}}
                },
                'required': ['file'],
                'additionalProperties': False
            }
        ]
    }

    def on_task_output(self, task, config):
        # makes this plugin count as output (stops warnings about missing outputs)
        pass

    def prepare_config(self, config):
        if not isinstance(config, dict):
            config = {'file': config}
        config.setdefault('days', 7)
        config.setdefault('items', -1)
        config.setdefault('history', True)
        config.setdefault('encoding', 'iso-8859-1')
        config.setdefault('link', ['imdb_url', 'input_url'])
        config.setdefault('title', '{{title}} (from {{task}})')
        config.setdefault('template', 'default')
        # add url as last resort
        config['link'].append('url')
        return config

    def on_task_exit(self, task, config):
        """Store finished / downloaded entries at exit"""
        if not rss2gen:
            raise plugin.PluginWarning('plugin make_rss requires PyRSS2Gen library.')
        config = self.prepare_config(config)

        # when history is disabled, remove everything from backlog on every run (a bit hackish, rarely useful)
        if not config['history']:
            log.debug('disabling history')
            for item in task.session.query(RSSEntry).filter(RSSEntry.file == config['file']).all():
                task.session.delete(item)

        # save entries into db for RSS generation
        for entry in task.accepted:
            rss = RSSEntry()
            rss.title = entry.render(config['title'])
            for field in config['link']:
                if field in entry:
                    rss.link = entry[field]
                    break

            try:
                template = get_template(config['template'], 'rss')
            except ValueError as e:
                raise plugin.PluginError('Invalid template specified: %s' % e)
            try:
                rss.description = render_from_entry(template, entry)
            except RenderError as e:
                log.error('Error while rendering entry %s, falling back to plain title: %s' % (entry, e))
                rss.description = entry['title'] + ' - (Render Error)'
            rss.file = config['file']

            # TODO: check if this exists and suggest disabling history if it does since it shouldn't happen normally ...
            log.debug('Saving %s into rss database' % entry['title'])
            task.session.add(rss)

        if not rss2gen:
            return
        # don't generate rss when learning
        if task.options.learn:
            return

        db_items = task.session.query(RSSEntry).filter(RSSEntry.file == config['file']).\
            order_by(RSSEntry.published.desc()).all()

        # make items
        rss_items = []
        for db_item in db_items:
            add = True
            if config['items'] != -1:
                if len(rss_items) > config['items']:
                    add = False
            if config['days'] != -1:
                if datetime.datetime.today() - datetime.timedelta(days=config['days']) > db_item.published:
                    add = False
            if add:
                # add into generated feed
                hasher = hashlib.sha1()
                hasher.update(db_item.title.encode('utf8'))
                hasher.update(db_item.description.encode('utf8'))
                hasher.update(db_item.link.encode('utf8'))
                guid = base64.urlsafe_b64encode(hasher.digest())
                guid = PyRSS2Gen.Guid(guid, isPermaLink = False)

                gen = {'title': db_item.title,
                       'description': db_item.description,
                       'link': db_item.link,
                       'pubDate': db_item.published,
                       'guid': guid}
                log.trace('Adding %s into rss %s' % (gen['title'], config['file']))
                rss_items.append(PyRSS2Gen.RSSItem(**gen))
            else:
                # no longer needed
                task.session.delete(db_item)

        # make rss
        rss = PyRSS2Gen.RSS2(title='FlexGet',
                             link=config.get('rsslink', 'http://flexget.com'),
                             description='FlexGet generated RSS feed',
                             lastBuildDate=datetime.datetime.utcnow(),
                             items=rss_items)

        # don't run with --test
        if task.options.test:
            log.info('Would write rss file with %d entries.', len(rss_items))
            return

        # write rss
        fn = os.path.expanduser(config['file'])
        with open(fn, 'w') as file:
            try:
                log.verbose('Writing output rss to %s' % fn)
                rss.write_xml(file, encoding=config['encoding'])
            except LookupError:
                log.critical('Unknown encoding %s' % config['encoding'])
                return
            except IOError:
                # TODO: plugins cannot raise PluginWarnings in terminate event ..
                log.critical('Unable to write %s' % fn)
                return


@event('plugin.register')
def register_plugin():
    plugin.register(OutputRSS, 'make_rss', api_ver=2)

########NEW FILE########
__FILENAME__ = rtorrent_magnet
from __future__ import unicode_literals, division, absolute_import
import logging
import re
import os

from flexget import plugin
from flexget.event import event

log = logging.getLogger('rtorrent_magnet')
pat = re.compile('xt=urn:btih:([^&/]+)')


class PluginRtorrentMagnet(object):
    """
    Process Magnet URI's into rtorrent compatible torrent files

    Magnet URI's will look something like this:

    magnet:?xt=urn:btih:190F1ABAED7AE7252735A811149753AA83E34309&dn=URL+Escaped+Torrent+Name

    rTorrent would expect to see something like meta-URL_Escaped_Torrent_Name.torrent

    The torrent file must also contain the text:

    d10:magnet-uri88:xt=urn:btih:190F1ABAED7AE7252735A811149753AA83E34309&dn=URL+Escaped+Torrent+Namee

    This plugin will check if a download URL is a magnet link, and then create the appropriate torrent file.

    Example:
      rtorrent_magnet: ~/torrents/
    """

    schema = {'type': 'string', 'format': 'path'}

    def write_torrent_file(self, task, entry, path):
        path = os.path.join(path, 'meta-%s.torrent' % entry['title'])
        path = os.path.expanduser(path)

        if task.options.test:
            log.info('Would write: %s' % path)
        else:
            log.info('Writing rTorrent Magnet File: %s', path)
            with open(path, 'w') as f:
                f.write('d10:magnet-uri%d:%se' % (len(entry['url']), entry['url']))
        entry['output'] = path

    # Run after download plugin to only pick up entries it did not already handle
    @plugin.priority(0)
    def on_task_output(self, task, config):

        for entry in task.accepted:
            if 'output' in entry:
                log.debug('Ignoring, %s already has an output file: %s' % (entry['title'], entry['output']))
                continue

            for url in entry.get('urls', [entry['url']]):
                if url.startswith('magnet:'):
                    log.debug('Magnet URI detected for url %s (%s)' % (url, entry['title']))
                    if pat.search(url):
                        self.write_torrent_file(task, entry, entry.get('path', config))
                        break
                    else:
                        log.warning('Unrecognized Magnet URI Format: %s', url)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginRtorrentMagnet, 'rtorrent_magnet', api_ver=2)

########NEW FILE########
__FILENAME__ = sabnzbd
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib

from flexget import plugin
from flexget.event import event
from flexget.utils.tools import urlopener

log = logging.getLogger('sabnzbd')


class OutputSabnzbd(object):
    """
    Example::

      sabnzbd:
        apikey: 123456
        url: http://localhost/sabnzbd/api?
        category: movies

    All parameters::

      sabnzbd:
        apikey: ...
        url: ...
        category: ...
        script: ...
        pp: ...
        priority: ...
    """

    def validator(self):
        from flexget import validator
        config = validator.factory('dict')
        config.accept('text', key='key', required=True)
        config.accept('url', key='url', required=True)
        config.accept('text', key='category')
        config.accept('text', key='script')
        config.accept('text', key='pp')
        config.accept('integer', key='priority')
        config.accept('text', key='password')
        config.accept('text', key='username')
        return config

    def get_params(self, config):
        params = {}
        if 'key' in config:
            params['apikey'] = config['key']
        if 'category' in config:
            params['cat'] = '%s' % config['category']
        if 'script' in config:
            params['script'] = config['script']
        if 'pp' in config:
            params['pp'] = config['pp']
        if 'priority' in config:
            params['priority'] = config['priority']
        if 'username' in config:
            params['ma_username'] = config['username']
        if 'password' in config:
            params['ma_password'] = config['password']
        params['mode'] = 'addurl'
        return params

    def on_task_output(self, task, config):
        for entry in task.accepted:
            if task.options.test:
                log.info('Would add into sabnzbd: %s' % entry['title'])
                continue

            params = self.get_params(config)
            # allow overriding the category
            if 'category' in entry:
                # Dirty hack over the next few lines to strip out non-ascii
                # chars. We're going to urlencode this, which causes
                # serious issues in python2.x if it's not ascii input.
                params['cat'] = ''.join([x for x in entry['category'] if ord(x) < 128])
            params['name'] = ''.join([x for x in entry['url'] if ord(x) < 128])
            # add cleaner nzb name (undocumented api feature)
            params['nzbname'] = ''.join([x for x in entry['title'] if ord(x) < 128])

            request_url = config['url'] + urllib.urlencode(params)
            log.debug('request_url: %s' % request_url)
            try:
                response = urlopener(request_url, log).read()
            except Exception as e:
                log.critical('Failed to use sabnzbd. Requested %s' % request_url)
                log.critical('Result was: %s' % e)
                entry.fail('sabnzbd unreachable')
                if task.options.debug:
                    log.exception(e)
                continue

            if 'error' in response.lower():
                entry.fail(response.replace('\n', ''))
            else:
                log.info('Added `%s` to SABnzbd' % (entry['title']))


@event('plugin.register')
def register_plugin():
    plugin.register(OutputSabnzbd, 'sabnzbd', api_ver=2)

########NEW FILE########
__FILENAME__ = send_email
from __future__ import unicode_literals, division, absolute_import
import logging
import smtplib
import socket
import sys
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from smtplib import SMTPException
from email.utils import formatdate

from flexget import config_schema, manager, plugin
from flexget.event import event
from flexget.utils.template import render_from_task, get_template, RenderError
from flexget.utils.tools import merge_dict_from_to, MergeException
from flexget import validator

log = logging.getLogger('email')

# A dict which stores the email content from each task when plugin is configured globally
task_content = {}


def options_validator():
    email = validator.factory('dict')
    email.accept('boolean', key='active')
    email.accept('text', key='to', required=True)
    email.accept('list', key='to', required=True).accept('text')
    email.accept('text', key='from', required=True)
    email.accept('text', key='smtp_host')
    email.accept('integer', key='smtp_port')
    email.accept('boolean', key='smtp_login')
    email.accept('text', key='smtp_username')
    email.accept('text', key='smtp_password')
    email.accept('boolean', key='smtp_tls')
    email.accept('boolean', key='smtp_ssl')
    email.accept('text', key='template')
    email.accept('text', key='subject')
    return email


def prepare_config(config):
    config.setdefault('active', True)
    config.setdefault('smtp_host', 'localhost')
    config.setdefault('smtp_port', 25)
    config.setdefault('smtp_login', False)
    config.setdefault('smtp_username', '')
    config.setdefault('smtp_password', '')
    config.setdefault('smtp_tls', False)
    config.setdefault('smtp_ssl', False)
    config.setdefault('template', 'default.template')
    if not isinstance(config['to'], list):
        config['to'] = [config['to']]
    return config


@event('manager.execute.started')
def setup(manager):
    if not 'email' in manager.config:
        return
    config = prepare_config(manager.config['email'])
    config['global'] = True
    global task_content
    task_content = {}
    for task_name, task_config in manager.config['tasks'].iteritems():
        task_config.setdefault('email', {})
        try:
            merge_dict_from_to(config, task_config['email'])
        except MergeException as exc:
            raise plugin.PluginError('Failed to merge email config to task %s due to %s' % (task_name, exc))
        task_config.setdefault('email', config)


@event('manager.execute.completed')
def global_send(manager):
    if not 'email' in manager.config:
        return
    config = prepare_config(manager.config['email'])
    content = ''
    for task, text in task_content.iteritems():
        content += '_' * 30 + ' Task: %s ' % task + '_' * 30 + '\n'

        content += text + '\n'
    if not content:
        log.verbose('No tasks generated any email notifications. Not sending.')
        return
    if config.get('subject'):
        # If subject is specified, use it from the config
        subject = config['subject']
    elif config['template'].startswith('failed'):
        subject = '[FlexGet] Failures on task(s): %s' % ', '.join(task_content)
    else:
        subject = '[FlexGet] Notifications for task(s): %s' % ', '.join(task_content)
    send_email(subject, content, config)


def send_email(subject, content, config):
    """Send email at exit."""

    # prepare email message
    message = MIMEMultipart('alternative')
    message['To'] = ','.join(config['to'])
    message['From'] = config['from']
    message['Subject'] = subject
    message['Date'] = formatdate(localtime=True)
    content_type = 'html' if '<html>' in content else 'plain'
    message.attach(MIMEText(content.encode('utf-8'), content_type, _charset='utf-8'))

    # send email message
    if manager.manager.options.test:
        log.info('Would send email : %s' % message.as_string())
        log.info(content)
    else:
        log.verbose('Sending email.')
        try:
            if config['smtp_ssl']:
                if sys.version_info < (2, 6, 3):
                    raise plugin.PluginError('SSL email support requires python >= 2.6.3 due to python bug #4066, '
                                             'upgrade python or use TLS', log)
                    # Create a SSL connection to smtp server
                mailServer = smtplib.SMTP_SSL(config['smtp_host'], config['smtp_port'])
            else:
                mailServer = smtplib.SMTP(config['smtp_host'], config['smtp_port'])
                if config['smtp_tls']:
                    mailServer.ehlo()
                    mailServer.starttls()
                    mailServer.ehlo()
        except socket.error as e:
            log.warning('Socket error: %s' % e)
            return
        except SMTPException as e:
            # Ticket #1133
            log.warning('Unable to send email: %s' % e)
            return

        try:

            if config.get('smtp_username') and config.get('smtp_password'):
                mailServer.login(config['smtp_username'], config['smtp_password'])
            mailServer.sendmail(message['From'], config['to'], message.as_string())
        except IOError as e:
            # Ticket #686
            log.warning('Unable to send email! IOError: %s' % e)
            return
        except SMTPException as e:
            log.warning('Unable to send email! SMTPException: %s' % e)
            return

        mailServer.quit()


class OutputEmail(object):

    """
    Send an e-mail with the list of all succeeded (downloaded) entries.

    Configuration options

    ===============  ===================================================================
    Option           Description
    ===============  ===================================================================
    from             The email address from which the email will be sent (required)
    to               The email address of the recipient (required)
    smtp_host        The host of the smtp server
    smtp_port        The port of the smtp server
    smtp_username    The username to use to connect to the smtp server
    smtp_password    The password to use to connect to the smtp server
    smtp_tls         Should we use TLS to connect to the smtp server
    smtp_ssl         Should we use SSL to connect to the smtp server
                     Due to a bug in python, this only works in python 2.6.3 and up
    active           Is this plugin active or not
    ===============  ===================================================================

    Config basic example::

      email:
        from: xxx@xxx.xxx
        to: xxx@xxx.xxx
        smtp_host: smtp.host.com

    Config example with smtp login::

      email:
        from: xxx@xxx.xxx
        to: xxx@xxx.xxx
        smtp_host: smtp.host.com
        smtp_port: 25
        smtp_login: true
        smtp_username: my_smtp_login
        smtp_password: my_smtp_password
        smtp_tls: true

    Config multi-task example::

      global:
        email:
          from: xxx@xxx.xxx
          to: xxx@xxx.xxx
          smtp_host: smtp.host.com

      tasks:
        task1:
          rss: http://xxx
        task2:
          rss: http://yyy
          email:
            active: False
        task3:
          rss: http://zzz
          email:
            to: zzz@zzz.zzz

    GMAIL example::

      from: from@gmail.com
      to: to@gmail.com
      smtp_host: smtp.gmail.com
      smtp_port: 587
      smtp_login: true
      smtp_username: gmailUser
      smtp_password: gmailPassword
      smtp_tls: true

    Default values for the config elements::

      email:
        active: True
        smtp_host: localhost
        smtp_port: 25
        smtp_login: False
        smtp_username:
        smtp_password:
        smtp_tls: False
        smtp_ssl: False
    """

    def validator(self):
        v = options_validator()
        v.accept('boolean', key='global')
        return v

    @plugin.priority(0)
    def on_task_output(self, task, config):
        config = prepare_config(config)

        if not config['active']:
            return

        # don't send mail when learning
        if task.options.learn:
            return

        # generate email content
        if config.get('subject'):
            subject = config['subject']
        else:
            subject = '[FlexGet] {{task.name}}: '
            if task.aborted:
                subject += 'Aborted'
            elif task.failed:
                subject += '{{task.failed|length}} failed entries'
            else:
                subject += '{{task.accepted|length}} new entries downloaded'
        try:
            subject = render_from_task(subject, task)
        except RenderError as e:
            log.error('Error rendering email subject: %s' % e)
            return
        try:
            content = render_from_task(get_template(config['template'], 'email'), task)
        except RenderError as e:
            log.error('Error rendering email body: %s' % e)
            return

        if not content.strip():
            log.verbose('No content generated from template, not sending email.')
            return

        if config.get('global'):
            # Email plugin was configured at root, save the email output
            log.debug('Saving email content for task %s' % task.name)
            task_content[task.name] = content
        else:
            send_email(subject, content, config)

    def on_task_abort(self, task, config):
        if not task.silent_abort:
            self.on_task_output(task, config)


@event('plugin.register')
def register_plugin():
    plugin.register(OutputEmail, 'email', api_ver=2)


@event('config.register')
def register_config_key():
    config_schema.register_config_key('email', options_validator().schema())

########NEW FILE########
__FILENAME__ = series_begin
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event
from flexget.plugins.filter.series import Series, set_series_begin

log = logging.getLogger('set_series_begin')


class SetSeriesBegin(object):
    """
    Set the first episode for series. Uses series_name and series_id.
    
    Example::

      set_series_begin: yes
    
    """
    
    schema = {'type': 'boolean'}
    
    def on_task_output(self, task, config):
        if not (config and task.accepted):
            return
        for entry in task.accepted:
            if entry.get('series_name') and entry.get('series_id'):
                fshow = task.session.query(Series).filter(Series.name == entry['series_name']).first()
                if not fshow:
                    fshow = Series()
                    fshow.name = entry['series_name']
                    task.session.add(fshow)
                try:
                    set_series_begin(fshow, entry['series_id'])
                except ValueError as e:
                    self.log.error('An error occurred trying to set begin for %s: %s' % (entry['series_name'], e))
                self.log.info('First episode for "%s" set to %s' % (entry['series_name'], entry['series_id']))


@event('plugin.register')
def register_plugin():
    plugin.register(SetSeriesBegin, 'set_series_begin', api_ver=2)

########NEW FILE########
__FILENAME__ = sms_ru
from __future__ import unicode_literals, division, absolute_import
import logging
import hashlib

from flexget import plugin
from flexget.event import event
from flexget.utils.template import RenderError

__version__ = 0.1

log = logging.getLogger("sms_ru")

client_headers = {"User-Agent": "FlexGet sms_ru plugin/%s" % str(__version__)}
sms_send_url = "http://sms.ru/sms/send"
sms_token_url = "http://sms.ru/auth/get_token"


class OutputSMSru(object):
    """
    Sends SMS notification through sms.ru http api sms/send.
    Phone number is a login assigned to sms.ru account.

    Example:

      sms_ru:
        phonenumber: <PHONE_NUMBER> (accepted format example: "79997776655")
        password: <PASSWORD>
        [message: <MESSAGE_TEXT>] (default: "accepted {{title}}" -- accepts Jinja)

    Configuration parameters are also supported from entries (eg. through set).

    """

    def validator(self):
        from flexget import validator
        config = validator.factory("dict")
        config.accept("text", key="phonenumber", required=True)
        config.accept("text", key="password", required=True)
        config.accept("text", key="message", required=False)
        return config

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {"enabled": config}

        # Set the defaults
        config.setdefault("message", "accepted {{title}}")
        return config

    # Run last to make sure other outputs are successful before sending notification
    @plugin.priority(0)
    def on_task_output(self, task, config):
        # Get the parameters
        config = self.prepare_config(config)

        phonenumber = config["phonenumber"]
        password = config["password"]

        # Backend provides temporary token
        token_response = task.requests.get(sms_token_url, headers=client_headers, raise_status=False)

        if token_response.status_code == 200:
            log.debug("Got auth token")
            # Auth method without api_id based on hash of password combined with token
            sha512 = hashlib.sha512(password + token_response.text).hexdigest()
        else:
            log.error("Error getting auth token")

        # Loop through the accepted entries
        for entry in task.accepted:
            # Set message from entry
            message = config["message"]

            # Attempt to render the message field
            try:
                message = entry.render(message)
            except RenderError as e:
                log.debug("Problem rendering 'message': %s" % e)
                message = "accepted %s" % entry["title"]

            # Check for test mode
            if task.options.test:
                log.info("Test mode. Processing for %s" % phonenumber)
                log.info("Message: %s" % message)

            # Build request params
            send_params = {'login': phonenumber,
                           'sha512': sha512,
                           'token': token_response.text,
                           'to': phonenumber,
                           'text': message}
            if task.options.test:
                send_params.update({'test': 1})

            # Make the request
            response = task.requests.get(sms_send_url, params=send_params, headers=client_headers, raise_status=False)

            # Get resul code from sms.ru backend returned in body
            result_text = response.text

            # Check if it succeeded
            if response.text.find("100") == 0:
                log.debug("SMS notification for %s sent" % phonenumber)
            else:
                log.error("SMS was not sent. Server response was %s" % response.text)


@event('plugin.register')
def register_plugin():
    plugin.register(OutputSMSru, "sms_ru", api_ver=2)

########NEW FILE########
__FILENAME__ = subtitles
from __future__ import unicode_literals, division, absolute_import
from xmlrpclib import ServerProxy
import re
import difflib
import os.path
import logging

from flexget import plugin
from flexget.event import event
from flexget.utils.tools import urlopener

"""

DRAFT

class SubtitleQueue(Base):

    __tablename__ = 'subtitle_queue'

    id = Column(Integer, primary_key=True)
    task = Column(String)
    imdb_id = Column(String)
    added = Column(DateTime)

    def __init__(self, task, imdb_id):
        self.task = task
        self.imdb_id = imdb_id
        self.added = datetime.now()

    def __str__(self):
        return '<SubtitleQueue(%s=%s)>' % (self.task, self.imdb_id)

TODO:

 * add new option, retry: [n] days
 * add everything into queue using above class
 * consume queue (look up by task name), configuration is available from task
 * remove successful downloads
 * remove queue items that are part retry: n days

"""

log = logging.getLogger('subtitles')

# movie hash, won't work here though
# http://trac.opensubtitles.org/projects/opensubtitles/wiki/HashSourceCodes#Python

# xmlrpc spec
# http://trac.opensubtitles.org/projects/opensubtitles/wiki/XMLRPC


class Subtitles(object):
    """
    Fetch subtitles from opensubtitles.org
    """

    schema = {
        'type': 'object',
        'properties': {
            'languages': {'type': 'array', 'items': {'type': 'string'}},
            'min_sub_rating': {'type': 'number'},
            'match_limit': {'type': 'number'},
            'output': {'type': 'string', 'format': 'path'}
        },
        'additionalProperties': False
    }

    def prepare_config(self, config, task):
        if not isinstance(config, dict):
            config = {}
        config.setdefault('output', task.manager.config_base)
        config.setdefault('languages', ['eng'])
        config.setdefault('min_sub_rating', 0.0)
        config.setdefault('match_limit', 0.8)
        config['output'] = os.path.expanduser(config['output'])
        return config

    def on_task_download(self, task, config):

        # filter all entries that have IMDB ID set
        try:
            entries = [e for e in task.accepted if e['imdb_id'] is not None]
        except KeyError:
            # No imdb urls on this task, skip it
            # TODO: should do lookup via imdb_lookup plugin?
            return

        try:
            s = ServerProxy("http://api.opensubtitles.org/xml-rpc")
            res = s.LogIn("", "", "en", "FlexGet")
        except:
            log.warning('Error connecting to opensubtitles.org')
            return

        if res['status'] != '200 OK':
            raise Exception("Login to opensubtitles.org XML-RPC interface failed")

        config = self.prepare_config(config, task)

        token = res['token']

        # configuration
        languages = config['languages']
        min_sub_rating = config['min_sub_rating']
        match_limit = config['match_limit'] # no need to change this, but it should be configurable

        # loop through the entries
        for entry in entries:
            imdbid = entry.get('imdb_id')
            if not imdbid:
                log.debug('no match for %s' % entry['title'])
                continue

            query = []
            for language in languages:
                query.append({'sublanguageid': language, 'imdbid': imdbid})

            subtitles = s.SearchSubtitles(token, query)
            subtitles = subtitles['data']

            # nothing found -> continue
            if not subtitles:
                continue

            # filter bad subs
            subtitles = [x for x in subtitles if x['SubBad'] == '0']
            # some quality required (0.0 == not reviewed)
            subtitles = [x for x in subtitles if
                         float(x['SubRating']) >= min_sub_rating or float(x['SubRating']) == 0.0]

            filtered_subs = []

            # find the best rated subs for each language
            for language in languages:
                langsubs = [x for x in subtitles if x['SubLanguageID'] == language]

                # did we find any subs for this language?
                if langsubs:

                    def seqmatch(subfile):
                        s = difflib.SequenceMatcher(lambda x: x in " ._", entry['title'], subfile)
                        #print "matching: ", entry['title'], subfile, s.ratio()
                        return s.ratio() > match_limit

                    # filter only those that have matching release names
                    langsubs = [x for x in subtitles if seqmatch(x['MovieReleaseName'])]

                    if langsubs:
                        # find the best one by SubRating
                        langsubs.sort(key=lambda x: float(x['SubRating']))
                        langsubs.reverse()
                        filtered_subs.append(langsubs[0])

            # download
            for sub in filtered_subs:
                log.debug('SUBS FOUND: ', sub['MovieReleaseName'], sub['SubRating'], sub['SubLanguageID'])

                f = urlopener(sub['ZipDownloadLink'], log)
                subfilename = re.match('^attachment; filename="(.*)"$', f.info()['content-disposition']).group(1)
                outfile = os.path.join(config['output'], subfilename)
                fp = file(outfile, 'w')
                fp.write(f.read())
                fp.close()
                f.close()

        s.LogOut(token)


@event('plugin.register')
def register_plugin():
    plugin.register(Subtitles, 'subtitles', api_ver=2)

########NEW FILE########
__FILENAME__ = subtitles_periscope
import logging
import os
import tempfile

from flexget import plugin
from flexget.event import event

log = logging.getLogger('subtitles')


class PluginPeriscope(object):
    """
    Search and download subtitles using Periscope by Patrick Dessalle 
    (http://code.google.com/p/periscope/).
    
    Example (complete task)::

      subs:
        find:
          path: 
            - d:\media\incoming
          regexp: '.*\.(avi|mkv|mp4)$'
          recursive: yes
        accept_all: yes
        periscope:
          languages:
            - it
          alternatives:
            - en
          overwrite: yes
    """
    
    schema = {
        'type': 'object',
        'properties': {
            'languages': {'type': 'array', 'items': {'type': 'string'}, 'minItems': 1},
            'alternatives': {'type': 'array', 'items': {'type': 'string'}},
            'overwrite': {'type': 'boolean', 'default': False},
            'subexts': {'type': 'array', 'items': {'type': 'string'}, 'default': ['srt', 'stp', 'sub', 'stl', 'ssa']},
        },
        'additionalProperties': False
    }

    def on_task_start(self, task, config):
        try:
            import periscope
        except ImportError as e:
            log.debug('Error importing Periscope: %s' % e)
            raise plugin.DependencyError('periscope', 'periscope', 
                                  'Periscope module required. ImportError: %s' % e)
    
    def subbed(self, filename):
        for ext in self.exts:
            if os.path.exists(os.path.splitext(filename)[0] + ext):
                return True
        return False
    
    def on_task_output(self, task, config):
        """
        Configuration::
            periscope:
                languages: List of languages in order of preference (at least one is required).
                alternatives: List of second-choice languages; subs will be downloaded but entries rejected.
                overwrite: If yes it will try to download even for videos that are already subbed. Default: no.
                subexts: List of subtitles file extensions to check (only useful with overwrite=no). Default: srt, stp, sub, stl, ssa.
        """
        if not task.accepted:
            log.debug('nothing accepted, aborting')
            return
        import periscope
        psc = periscope.Periscope(tempfile.gettempdir())
        logging.getLogger('periscope').setLevel(logging.CRITICAL)  # LOT of messages otherwise
        langs = [s.encode('utf8') for s in config['languages']]  # avoid unicode warnings
        alts = [s.encode('utf8') for s in config.get('alternatives', [])]
        if not config['overwrite']:
            self.exts = ['.'+s for s in config['subexts']]
        for entry in task.accepted:
            if not 'location' in entry:
                log.warning('Cannot act on entries that do not represent a local file.')
            elif not os.path.exists(entry['location']):
                entry.fail('file not found: %s' % entry['location'])
            elif '$RECYCLE.BIN' in entry['location']:
                continue  # ignore deleted files in Windows shares
            elif not config['overwrite'] and self.subbed(entry['location']):
                log.warning('cannot overwrite existing subs for %s' % entry['location'])
            else:
                try:
                    if psc.downloadSubtitle(entry['location'].encode("utf8"), langs):
                        log.info('Subtitles found for %s' % entry['location'])
                    elif alts and psc.downloadSubtitle(entry['location'].encode('utf8'), alts):
                        entry.fail('subtitles found for a second-choice language.')
                    else:
                        entry.fail('cannot find any subtitles for now.')
                except Exception as err:
                    # don't want to abort the entire task for errors in a  
                    # single video file or for occasional network timeouts
                    entry.fail(err.message)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginPeriscope, 'periscope', api_ver=2)

########NEW FILE########
__FILENAME__ = subtitles_subliminal
import logging
import os
import sys
import tempfile

from flexget import plugin
from flexget.event import event

log = logging.getLogger('subtitles')


class PluginSubliminal(object):
    """
    Search and download subtitles using Subliminal by Antoine Bertin
    (https://pypi.python.org/pypi/subliminal).
    
    Example (complete task)::

      subs:
        find:
          path: 
            - d:\media\incoming
          regexp: '.*\.(avi|mkv|mp4)$'
          recursive: yes
        accept_all: yes
        subliminal:
          languages:
            - ita
          alternatives:
            - eng
          exact_match: no
    """
    
    schema = {
        'type': 'object',
        'properties': {
            'languages': {'type': 'array', 'items': {'type': 'string'}, 'minItems': 1},
            'alternatives': {'type': 'array', 'items': {'type': 'string'}},
            'exact_match': {'type': 'boolean', 'default': True},
        },
        'additionalProperties': False
    }

    def on_task_start(self, task, config):
        if list(sys.version_info) < [2, 7]:
            raise plugin.DependencyError('subliminal', 'Python 2.7', 'Subliminal plugin requires python 2.7.')
        try:
            import babelfish
        except ImportError as e:
            log.debug('Error importing Babelfish: %s' % e)
            raise plugin.DependencyError('subliminal', 'babelfish', 'Babelfish module required. ImportError: %s' % e)
        try:
            import subliminal
        except ImportError as e:
            log.debug('Error importing Subliminal: %s' % e)
            raise plugin.DependencyError('subliminal', 'subliminal', 'Subliminal module required. ImportError: %s' % e)
    
    def on_task_output(self, task, config):
        """
        Configuration::
            subliminal:
                languages: List of languages (3-letter ISO-639-3 code) in order of preference. At least one is required.
                alternatives: List of second-choice languages; subs will be downloaded but entries rejected.
                exact_match: Use file hash only to search for subs, otherwise Subliminal will try to guess by filename.
        """
        if not task.accepted:
            log.debug('nothing accepted, aborting')
            return
        from babelfish import Language
        from dogpile.cache.exception import RegionAlreadyConfigured
        import subliminal
        try:
            subliminal.cache_region.configure('dogpile.cache.dbm', 
                arguments={'filename': os.path.join(tempfile.gettempdir(), 'cachefile.dbm'), 
                           'lock_factory': subliminal.MutexLock})
        except RegionAlreadyConfigured:
            pass
        logging.getLogger("subliminal").setLevel(logging.CRITICAL)
        logging.getLogger("enzyme").setLevel(logging.WARNING)
        langs = set([Language(s) for s in config['languages']])
        alts = set([Language(s) for s in config.get('alternatives', [])])
        for entry in task.accepted:
            if not 'location' in entry:
                log.warning('Cannot act on entries that do not represent a local file.')
            elif not os.path.exists(entry['location']):
                entry.fail('file not found: %s' % entry['location'])
            elif not '$RECYCLE.BIN' in entry['location']:  # ignore deleted files in Windows shares
                try:
                    video = subliminal.scan_video(entry['location'])
                    msc = video.scores['hash'] if config['exact_match'] else 0
                    if langs & video.subtitle_languages:
                        continue  # subs for preferred lang(s) already exists
                    elif subliminal.download_best_subtitles([video], langs, min_score=msc):
                        log.info('Subtitles found for %s' % entry['location'])
                    elif alts and (alts - video.subtitle_languages) and \
                        subliminal.download_best_subtitles([video], alts, min_score=msc):
                        entry.fail('subtitles found for a second-choice language.')
                    else:
                        entry.fail('cannot find any subtitles for now.')
                except Exception as err:
                    # don't want to abort the entire task for errors in a  
                    # single video file or for occasional network timeouts
                    if err.args:
                        msg = err.args[0]
                    else:
                        # Subliminal errors don't always have a message, just use the name
                        msg = 'subliminal error: %s' % err.__class__.__name__
                    log.debug(msg)
                    entry.fail(msg)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginSubliminal, 'subliminal', api_ver=2)

########NEW FILE########
__FILENAME__ = utorrent
# -*- coding: utf-8 -*-


from __future__ import unicode_literals, division, absolute_import
import os
from logging import getLogger

from flexget import plugin
from flexget.event import event
from flexget.utils import requests
from flexget.utils.soup import get_soup
from flexget.utils.template import RenderError

log = getLogger('utorrent')


class PluginUtorrent(object):
    """
    Parse task content or url for hoster links and adds them to utorrent.

    Example::

      utorrent:
        url: http://localhost:8080/gui/
        username: my_username
        password: my_password
        path: Series

    """

    __author__ = 'Nil'
    __version__ = '0.1'

    schema = {
        'type': 'object',
        'properties': {
            'url': {'type': 'string', 'format': 'url'},
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'path': {'type': 'string'}
        },
        'required': ['username', 'password', 'url'],
        'additionalProperties': False
    }

    @plugin.internet(log)
    def on_task_output(self, task, config):
        if not config.get('enabled', True):
            return
        if not task.accepted:
            return

        session = requests.Session()
        url = config['url']
        if not url.endswith('/'):
            url += '/'
        auth = (config['username'], config['password'])
        # Login
        try:
            response = session.get(url + 'token.html', auth=auth)
        except requests.RequestException as e:
            if hasattr(e, 'response') and e.response.status_code == '401':
                raise plugin.PluginError('Invalid credentials, check your utorrent webui username and password.', log)
            raise plugin.PluginError('%s' % e, log)
        token = get_soup(response.text).find('div', id='token').text
        result = session.get(url, auth=auth, params={'action': 'list-dirs', 'token': token}).json()
        download_dirs = dict((os.path.normcase(dir['path']), i) for i, dir in enumerate(result['download-dirs']))

        for entry in task.accepted:
            # http://[IP]:[PORT]/gui/?action=add-url&s=[TORRENT URL]
            # bunch of urls now going to check
            folder = 0
            path = entry.get('path', config.get('path', ''))
            try:
                path = os.path.normcase(os.path.expanduser(entry.render(path)))
            except RenderError as e:
                log.error('Could not render path for `%s` downloading to default directory.' % entry['title'])
                # Add to default folder
                path = ''
            if path:
                for dir in download_dirs:
                    if path.startswith(dir):
                        folder = download_dirs[dir]
                        path = path[len(dir):].lstrip('\\')
                        break
                else:
                    log.error('path `%s` (or one of its parents)is not added to utorrent webui allowed download '
                              'directories. You must add it there before you can use it from flexget. '
                              'Adding to default download directory instead.' % path)
                    path = ''

            if task.options.test:
                log.info('Would add `%s` to utorrent' % entry['title'])
                continue

            # Add torrent
            data = {'action': 'add-url', 's': entry['url'], 'token': token, 'download_dir': folder, 'path': path}
            result = session.get(url, params=data, auth=auth)
            if 'build' in result.json():
                log.info('Added `%s` to utorrent' % entry['url'])
                log.info('in folder %s ' % folder + path)
            else:
                entry.fail('Fail to add `%s` to utorrent' % entry['url'])


@event('plugin.register')
def register_plugin():
    plugin.register(PluginUtorrent, 'utorrent', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_aria2
from __future__ import unicode_literals, division, absolute_import
import os
import logging
import re
import urlparse
import xmlrpclib

from flexget import plugin
from flexget.event import event
from flexget.entry import Entry
from flexget.utils.template import RenderError

from socket import error as socket_error

log = logging.getLogger('aria2')

# TODO: stop using torrent_info_hash[0:16] as the GID

# for RENAME_CONTENT_FILES:
# to rename TV episodes, content_is_episodes must be set to yes


class OutputAria2(object):

    """
    aria2 output plugin
    Version 1.0.0
    
    Configuration:
    server:     Where aria2 daemon is running. default 'localhost'
    port:       Port of that server. default '6800'
    username:   XML-RPC username set in aria2. default ''
    password:   XML-RPC password set in aria2. default ''
    do:         [add-new|remove-completed] What action to take with incoming
                entries.
    uri:        URI of file to download. Can include inline Basic Auth para-
                meters and use jinja2 templating with any fields available
                in the entry. If you are using any of the dynamic renaming
                options below, the filename can be included in this setting
                using {{filename}}.
    exclude_samples:
                [yes|no] Exclude any files that include the word 'sample' in
                their name. default 'no'
    exclude_non_content:
                [yes|no] Exclude any non-content files, as defined by filename
                extensions not listed in file_exts. (See below.) default 'no'
    rename_content_files:
                [yes|no] If set, rename all content files (as defined by
                extensions listed in file_exts). default 'no'
    rename_template:
                If set, and rename_content_files is yes, all content files
                will be renamed using the value of this field as a template.
                Will be parsed with jinja2 and can include any fields
                available in the entry. default ''
    parse_filename:
                [yes|no] If yes, filenames will be parsed with either the
                series parser (if content_is_episodes is set to yes) or the
                movie parser. default: 'no'
    content_is_episodes:
                [yes|no] If yes, files will be parsed by the series plugin
                parser to attempt to determine series name and series_id. If
                no, files will be treated as movies. Note this has no effect
                unless parse_filename is set to yes. default 'no'
    keep_parent_folders:
                [yes|no] If yes, any parent folders within the torrent itself
                will be kept and created within the download directory.
                For example, if a torrent has this structure:
                MyTorrent/
                  MyFile.mkv
                If this is set to yes, the MyTorrent folder will be created in
                the download directory. If set to no, the folder will be
                ignored and the file will be downloaded directly into the
                download directory. default: 'no'
    fix_year:   [yes|no] If yes, and the last four characters of the series
                name are numbers, enclose them in parantheses as they are
                likely a year. Example: Show Name 1995 S01E01.mkv would become
                Show Name (1995) S01E01.mkv. default 'yes'
    file_exts:  [list] File extensions of all files considered to be content
                files. Used to determine which files to rename or which files
                to exclude from download, with appropriate options set. (See
                above.)
                default: ['.mkv', '.avi', '.mp4', '.wmv', '.asf', '.divx',
                '.mov', '.mpg', '.rm']
    aria_config:
                "Parent folder" for any options to be passed directly to aria.
                Any command line option listed at
                http://aria2.sourceforge.net/manual/en/html/aria2c.html#options
                can be used by removing the two dashes (--) in front of the 
                command name, and changing key=value to key: value. All
                options will be treated as jinja2 templates and rendered prior
                to passing to aria2. default ''

    Sample configuration:
    aria2:
      server: myserver
      port: 6802
      do: add-new
      exclude_samples: yes
      exclude_non_content: yes
      parse_filename: yes
      content_is_episodes: yes
      rename_content_files: yes
      rename_template: '{{series_name}} - {{series_id||lower}}'
      aria_config:
        max-connection-per-server: 4
        max-concurrent-downloads: 4
        split: 4
        file-allocation: none
        dir: "/Volumes/all_my_tv/{{series_name}}"
    """

    schema = {
        'type': 'object',
        'properties': {
            'server': {'type': 'string', 'default': 'localhost'},
            'port': {'type': 'integer', 'default': 6800},
            'username': {'type': 'string', 'default': ''},
            'password': {'type': 'string', 'default': ''},
            'do': {'type': 'string', 'enum': ['add-new', 'remove-completed']},
            'uri': {'type': 'string'},
            'exclude_samples': {'type': 'boolean', 'default': False},
            'exclude_non_content': {'type': 'boolean', 'default': True},
            'rename_content_files': {'type': 'boolean', 'default': False},
            'content_is_episodes': {'type': 'boolean', 'default': False},
            'keep_parent_folders': {'type': 'boolean', 'default': False},
            'parse_filename': {'type': 'boolean', 'default': False},
            'fix_year': {'type': 'boolean', 'default': True},
            'rename_template': {'type': 'string', 'default': ''},
            'file_exts': {
                'type': 'array',
                'items': {'type': 'string'},
                'default': ['.mkv', '.avi', '.mp4', '.wmv', '.asf', '.divx', '.mov', '.mpg', '.rm']
            },
            'aria_config': {
                'type': 'object',
                'additionalProperties': {'oneOf': [{'type': 'string'}, {'type': 'integer'}]}
            }

        },
        'required': ['do'],
        'additionalProperties': False
    }

    def on_task_output(self, task, config):
        if 'aria_config' not in config:
            config['aria_config'] = {}
        if 'uri' not in config and config['do'] == 'add-new':
            raise plugin.PluginError('uri (path to folder containing file(s) on server) is required when adding new '
                                     'downloads.', log)
        if 'dir' not in config['aria_config']:
            if config['do'] == 'add-new':
                raise plugin.PluginError('dir (destination directory) is required.', log)
            else:
                config['aria_config']['dir'] = ''
        if config['keep_parent_folders'] and config['aria_config']['dir'].find('{{parent_folders}}') == -1:
            raise plugin.PluginError('When using keep_parent_folders, you must specify {{parent_folders}} in the dir '
                                     'option to show where it goes.', log)
        if config['rename_content_files'] and not config['rename_template']:
            raise plugin.PluginError('When using rename_content_files, you must specify a rename_template.', log)
        if config['username'] and not config['password']:
            raise plugin.PluginError('If you specify an aria2 username, you must specify a password.')

        try:
            userpass = ''
            if config['username']:
                userpass = '%s:%s@' % (config['username'], config['password'])
            baseurl = 'http://%s%s:%s/rpc' % (userpass, config['server'], config['port'])
            log.debug('base url: %s' % baseurl)
            s = xmlrpclib.ServerProxy(baseurl)
            log.info('Connected to daemon at ' + baseurl + '.')
        except xmlrpclib.ProtocolError as err:
            raise plugin.PluginError('Could not connect to aria2 at %s. Protocol error %s: %s'
                                     % (baseurl, err.errcode, err.errmsg), log)
        except xmlrpclib.Fault as err:
            raise plugin.PluginError('XML-RPC fault: Unable to connect to aria2 daemon at %s: %s'
                                     % (baseurl, err.faultString), log)
        except socket_error as (error, msg):
            raise plugin.PluginError('Socket connection issue with aria2 daemon at %s: %s'
                                     % (baseurl, msg), log)
        except:
            raise plugin.PluginError('Unidentified error during connection to aria2 daemon at %s' % baseurl, log)

        # loop entries
        for entry in task.accepted:
            config['aria_dir'] = config['aria_config']['dir']
            if 'aria_gid' in entry:
                config['aria_config']['gid'] = entry['aria_gid']
            elif 'torrent_info_hash' in entry:
                config['aria_config']['gid'] = entry['torrent_info_hash'][0:16]
            elif 'gid' in config['aria_config']:
                del(config['aria_config']['gid'])

            if 'content_files' not in entry:
                if entry['url']:
                    entry['content_files'] = [entry['url']]
                else:
                    entry['content_files'] = [entry['title']]
            else:
                if not isinstance(entry['content_files'], list):
                    entry['content_files'] = [entry['content_files']]

            counter = 0
            for cur_file in entry['content_files']:
                entry['parent_folders'] = ''
                # reset the 'dir' or it will only be rendered on the first loop
                config['aria_config']['dir'] = config['aria_dir']

                cur_filename = cur_file.split('/')[-1]
                if cur_file.split('/')[0] != cur_filename and config['keep_parent_folders']:
                    lastSlash = cur_file.rfind('/')
                    cur_path = cur_file[:lastSlash]
                    if cur_path[0:1] == '/':
                        cur_path = cur_path[1:]
                    entry['parent_folders'] = cur_path
                    log.debug('parent folders: %s' % entry['parent_folders'])

                file_dot = cur_filename.rfind(".")
                file_ext = cur_filename[file_dot:]

                if len(entry['content_files']) > 1 and 'gid' in config['aria_config']:
                    # if there is more than 1 file, need to give unique gids, this will work up to 999 files
                    counter += 1
                    strCounter = str(counter)
                    if len(entry['content_files']) > 99:
                        # sorry not sorry if you have more than 999 files
                        config['aria_config']['gid'] = ''.join([config['aria_config']['gid'][0:-3],
                                                               strCounter.rjust(3, str('0'))])
                    else:
                        config['aria_config']['gid'] = ''.join([config['aria_config']['gid'][0:-2],
                                                               strCounter.rjust(2, str('0'))])

                if config['exclude_samples'] == True:
                    # remove sample files from download list
                    if cur_filename.lower().find('sample') > -1:
                        continue

                if file_ext not in config['file_exts']:
                    if config['exclude_non_content'] == True:
                        # don't download non-content files, like nfos - definable in file_exts
                        continue

                if config['parse_filename']:
                    if config['content_is_episodes']:
                        metainfo_series = plugin.get_plugin_by_name('metainfo_series')
                        guess_series = metainfo_series.instance.guess_series
                        if guess_series(cur_filename):
                            parser = guess_series(cur_filename)
                            entry['series_name'] = parser.name
                            # if the last four chars are numbers, REALLY good chance it's actually a year...
                            # fix it if so desired
                            log.verbose(entry['series_name'])
                            if re.search(r'\d{4}', entry['series_name'][-4:]) is not None and config['fix_year']:
                                entry['series_name'] = ''.join([entry['series_name'][0:-4], '(',
                                                               entry['series_name'][-4:], ')'])
                                log.verbose(entry['series_name'])
                            parser.data = cur_filename
                            parser.parse
                            log.debug(parser.id_type)
                            if parser.id_type == 'ep':
                                entry['series_id'] = ''.join(['S', str(parser.season).rjust(2, str('0')), 'E',
                                                             str(parser.episode).rjust(2, str('0'))])
                            elif parser.id_type == 'sequence':
                                entry['series_id'] = parser.episode
                            elif parser.id_type and parser.id:
                                entry['series_id'] = parser.id
                    else:
                        from flexget.utils.titles.movie import MovieParser
                        parser = MovieParser()
                        parser.data = cur_filename
                        parser.parse()
                        log.info(parser)
                        testname = parser.name
                        testyear = parser.year
                        parser.data = entry['title']
                        parser.parse()
                        log.info(parser)
                        if len(parser.name) > len(testname):
                            entry['name'] = parser.name
                            entry['movie_name'] = parser.name
                        else:
                            entry['name'] = testname
                            entry['movie_name'] = testname
                        if parser.year:
                            entry['year'] = parser.year
                            entry['movie_year'] = parser.year
                        else:
                            entry['year'] = testyear
                            entry['movie_year'] = testyear

                if config['rename_content_files']:
                    if config['content_is_episodes']:
                        try:
                            config['aria_config']['out'] = entry.render(config['rename_template']) + file_ext
                            log.verbose(config['aria_config']['out'])
                        except RenderError as e:
                            log.error('Could not rename file %s: %s.' % (cur_filename, e))
                            continue
                    else:
                        try:
                            config['aria_config']['out'] = entry.render(config['rename_template']) + file_ext
                            log.verbose(config['aria_config']['out'])
                        except RenderError as e:
                            log.error('Could not rename file %s: %s. Try enabling imdb_lookup in this task'
                                      ' to assist.' % (cur_filename, e))
                            continue
                else:
                    config['aria_config']['out'] = cur_filename

                if config['do'] == 'add-new':
                    log.debug('Adding new file')
                    new_download = 0
                    if 'gid' in config['aria_config']:
                        try:
                            r = s.aria2.tellStatus(config['aria_config']['gid'], ['gid', 'status'])
                            log.info('Download status for %s (gid %s): %s' % (config['aria_config']['out'], r['gid'],
                                     r['status']))
                            if r['status'] == 'paused':
                                try:
                                    if not task.manager.options.test:
                                        s.aria2.unpause(r['gid'])
                                    log.info('  Unpaused download.')
                                except xmlrpclib.Fault as err:
                                    raise plugin.PluginError('aria2 response to unpause request: %s' % err.faultString, log)
                            else:
                                log.info('  Therefore, not re-adding.')
                        except xmlrpclib.Fault as err:
                            if err.faultString[-12:] == 'is not found':
                                new_download = 1
                            else:
                                raise plugin.PluginError('aria2 response to download status request: %s'
                                                         % err.faultString, log)
                        except xmlrpclib.ProtocolError as err:
                            raise plugin.PluginError('Could not connect to aria2 at %s. Protocol error %s: %s'
                                                     % (baseurl, err.errcode, err.errmsg), log)
                        except socket_error as (error, msg):
                            raise plugin.PluginError('Socket connection issue with aria2 daemon at %s: %s'
                                                     % (baseurl, msg), log)
                    else:
                        new_download = 1

                    if new_download == 1:
                        try:
                            entry['filename'] = cur_file
                            cur_uri = entry.render(config['uri'])
                            log.verbose('uri: %s' % cur_uri)
                        except RenderError as e:
                            raise plugin.PluginError('Unable to render uri: %s' % e)
                        try:
                            for key, value in config['aria_config'].iteritems():
                                log.trace('rendering %s: %s' % (key, value))
                                config['aria_config'][key] = entry.render(unicode(value))
                            log.debug('dir: %s' % config['aria_config']['dir'])
                            if not task.manager.options.test:
                                r = s.aria2.addUri([cur_uri], config['aria_config'])
                            else:
                                if 'gid' not in config['aria_config']:
                                    r = '1234567890123456'
                                else:
                                    r = config['aria_config']['gid']
                            log.info('%s successfully added to aria2 with gid %s.' % (config['aria_config']['out'], r))
                        except xmlrpclib.Fault as err:
                            raise plugin.PluginError('aria2 response to add URI request: %s' % err.faultString, log)
                        except socket_error as (error, msg):
                            raise plugin.PluginError('Socket connection issue with aria2 daemon at %s: %s'
                                                     % (baseurl, msg), log)
                        except RenderError as e:
                            raise plugin.PluginError('Unable to render one of the fields being passed to aria2:'
                                                     '%s' % e)

                elif config['do'] == 'remove-completed':
                    try:
                        r = s.aria2.tellStatus(config['aria_config']['gid'], ['gid', 'status'])
                        log.info('Status of download with gid %s: %s' % (r['gid'], r['status']))
                        if r['status'] in ['complete', 'removed']:
                            if not task.manager.options.test:
                                try:
                                    a = s.aria2.removeDownloadResult(r['gid'])
                                    if a == 'OK':
                                        log.info('Download with gid %s removed from memory' % r['gid'])
                                except xmlrpclib.Fault as err:
                                    raise plugin.PluginError('aria2 response to remove request: %s'
                                                             % err.faultString, log)
                                except socket_error as (error, msg):
                                    raise plugin.PluginError('Socket connection issue with aria2 daemon at %s: %s'
                                                             % (baseurl, msg), log)
                        else:
                            log.info('Download with gid %s could not be removed because of its status: %s'
                                     % (r['gid'], r['status']))
                    except xmlrpclib.Fault as err:
                        if err.faultString[-12:] == 'is not found':
                            log.warning('Download with gid %s could not be removed because it was not found. It was '
                                        'possibly previously removed or never added.' % config['aria_config']['gid'])
                        else:
                            raise plugin.PluginError('aria2 response to status request: %s' % err.faultString, log)
                    except socket_error as (error, msg):
                        raise plugin.PluginError('Socket connection issue with aria2 daemon at %s: %s'
                                                 % (baseurl, msg), log)


@event('plugin.register')
def register_plugin():
    plugin.register(OutputAria2, 'aria2', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_change_warn
from __future__ import unicode_literals, division, absolute_import
import logging
import sys
import os

from flexget import plugin
from flexget.event import event

log = logging.getLogger('change')
found_deprecated = False


class ChangeWarn(object):
    """
        Gives warning if user has deprecated / changed configuration in the root level.

        Will be replaced by root level validation in the future!

        Contains ugly hacks, better to include all deprecation warnings here during 1.0 BETA phase
    """

    def on_task_start(self, task, config):
        global found_deprecated

        if 'torrent_size' in task.config:
            log.critical('Plugin torrent_size is deprecated, use content_size instead')
            found_deprecated = True

        if 'nzb_size' in task.config:
            log.critical('Plugin nzb_size is deprecated, use content_size instead')
            found_deprecated = True

        if found_deprecated:
            task.manager.scheduler.shutdown(finish_queue=False)
            task.abort('Deprecated config.')


@event('plugin.register')
def register_plugin():
    plugin.register(ChangeWarn, 'change_warn', builtin=True, api_ver=2)


# check that no old plugins are in pre-compiled form (pyc)
try:
    import os.path
    plugin_dirs = (os.path.normpath(sys.path[0] + '/../flexget/plugins/'),
                   os.path.normpath(sys.path[0] + '/../flexget/plugins/input/'))
    for plugin_dir in plugin_dirs:
        for name in os.listdir(plugin_dir):
            require_clean = False

            if name.startswith('module'):
                require_clean = True

            if name == 'csv.pyc':
                require_clean = True

            if 'resolver' in name:
                require_clean = True

            if 'filter_torrent_size' in name:
                require_clean = True

            if 'filter_nzb_size' in name:
                require_clean = True

            if 'module_priority' in name:
                require_clean = True

            if 'ignore_feed' in name:
                require_clean = True

            if 'module_manual' in name:
                require_clean = True

            if 'output_exec' in name:
                require_clean = True

            if 'plugin_adv_exec' in name:
                require_clean = True

            if 'output_transmissionrpc' in name:
                require_clean = True

            if require_clean:
                log.critical('-' * 79)
                log.critical('IMPORTANT: Your installation has some files from older FlexGet!')
                log.critical('')
                log.critical('           Please remove all pre-compiled .pyc and .pyo files from %s' % plugin_dir)
                log.critical('           Offending file: %s' % name)
                log.critical('')
                log.critical('           After getting rid of these FlexGet should run again normally')

                from flexget import __version__ as version
                if version == '{git}':
                    log.critical('')
                    log.critical('           If you are using bootstrapped git checkout you can run:')
                    log.critical('           bin/paver clean_compiled')

                log.critical('')
                log.critical('-' * 79)
                found_deprecated = True
                break

except:
    pass

########NEW FILE########
__FILENAME__ = plugin_configure_series
from __future__ import unicode_literals, division, absolute_import
import hashlib
import logging

from sqlalchemy import Column, Integer, Unicode

from flexget import db_schema, plugin
from flexget.event import event
from flexget.config_schema import process_config
from flexget.plugins.filter.series import FilterSeriesBase

log = logging.getLogger('configure_series')
Base = db_schema.versioned_base('import_series', 0)


class LastHash(Base):
    __tablename__ = 'import_series_last_hash'

    id = Column(Integer, primary_key=True)
    task = Column(Unicode)
    hash = Column(Unicode)


class ConfigureSeries(FilterSeriesBase):

    """Generates series configuration from any input (supporting API version 2, soon all)

    Configuration::

      configure_series:
        [settings]:
           # same configuration as series plugin
        from:
          [input plugin]: <configuration>

    Example::

      configure_series:
        settings:
          quality: 720p
        from:
          listdir:
            - /media/series
    """

    @property
    def schema(self):
        return {
            'type': 'object',
            'properties': {
                'settings': self.settings_schema,
                'from': {'$ref': '/schema/plugins?phase=input'}
            },
            'additionalProperties': False
        }

    def on_task_start(self, task, config):

        series = {}
        for input_name, input_config in config.get('from', {}).iteritems():
            input = plugin.get_plugin_by_name(input_name)
            if input.api_ver == 1:
                raise plugin.PluginError('Plugin %s does not support API v2' % input_name)

            method = input.phase_handlers['input']
            result = method(task, input_config)
            if not result:
                log.warning('Input %s did not return anything' % input_name)
                continue

            for entry in result:
                s = series.setdefault(entry['title'], {})
                if entry.get('tvdb_id'):
                    s['set'] = {'tvdb_id': entry['tvdb_id']}

                # Allow configure_series to set anything available to series
                for key, schema in self.settings_schema['properties'].iteritems():
                    if 'configure_series_' + key in entry:
                        errors = process_config(entry['configure_series_' + key], schema, set_defaults=False)
                        if errors:
                            log.debug('not setting series option %s for %s. errors: %s' % (key, entry['title'], errors))
                        else:
                            s[key] = entry['configure_series_' + key]

        # Set the config_modified flag if the list of shows changed since last time
        new_hash = hashlib.md5(unicode(sorted(series))).hexdigest().decode('ascii')
        last_hash = task.session.query(LastHash).filter(LastHash.task == task.name).first()
        if not last_hash:
            last_hash = LastHash(task=task.name)
            task.session.add(last_hash)
        if last_hash.hash != new_hash:
            task.config_changed()
        last_hash.hash = new_hash

        if not series:
            log.info('Did not get any series to generate series configuration')
            return

        # Make a series config with the found series
        # Turn our dict of series with settings into a list of one item dicts
        series_config = {'generated_series': [dict([s]) for s in series.iteritems()]}
        # If options were specified, add them to the series config
        if 'settings' in config:
            series_config['settings'] = {'generated_series': config['settings']}
        # Merge our series config in with the base series config
        self.merge_config(task, series_config)


@event('plugin.register')
def register_plugin():
    plugin.register(ConfigureSeries, 'configure_series', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_cookies
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib2
import cookielib

from flexget import plugin
from flexget.event import event
from flexget.utils.tools import TimedDict

log = logging.getLogger('cookies')


class PluginCookies:
    """
    Adds cookie to all requests (rss, resolvers, download). Anything
    that uses urllib2 to be exact.

    Currently supports Firefox 3 cookies only.

    Example::

      cookies: /path/firefox/profile/something/cookies.sqlite
    """

    # TODO: 1.2 Is this a good way to handle this? How long should the time be?
    # Keeps loaded cookiejars cached for some time
    cookiejars = TimedDict(cache_time='5 minutes')

    schema = {
        'oneOf': [
            {'type': 'string', 'format': 'file'},
            {
                'type': 'object',
                'properties': {
                    'file': {'type': 'string', 'format': 'file'},
                    'type': {'type': 'string', 'enum': ['firefox3', 'mozilla', 'lwp']}
                },
                'additionalProperties': False
            }
        ]
    }

    def prepare_config(self, config):
        if isinstance(config, basestring):
            config = {'file': config}
        if config['file'].endswith('.txt'):
            config.setdefault('type', 'mozilla')
        elif config['file'].endswith('.lwp'):
            config.setdefault('type', 'lwp')
        else:
            config.setdefault('type', 'firefox3')
        return config

    def sqlite2cookie(self, filename):
        from cStringIO import StringIO
        try:
            from pysqlite2 import dbapi2 as sqlite
        except ImportError:
            try:
                from sqlite3 import dbapi2 as sqlite # try the 2.5+ stdlib
            except ImportError:
                raise plugin.PluginWarning('Unable to use sqlite3 or pysqlite2', log)

        log.debug('connecting: %s' % filename)
        try:
            con = sqlite.connect(filename)
        except:
            raise plugin.PluginError('Unable to open cookies sqlite database')

        cur = con.cursor()
        try:
            cur.execute('select host, path, isSecure, expiry, name, value from moz_cookies')
        except:
            raise plugin.PluginError('%s does not appear to be a valid Firefox 3 cookies file' % filename, log)

        ftstr = ['FALSE', 'TRUE']

        s = StringIO()
        s.write("""\
# Netscape HTTP Cookie File
# http://www.netscape.com/newsref/std/cookie_spec.html
# This is a generated file!  Do not edit.
""")
        count = 0
        failed = 0

        log.debug('fetching all cookies')

        def notabs(val):
            if isinstance(val, basestring):
                return val.replace('\t', '')
            return val

        while True:
            try:
                item = cur.next()
                # remove \t from item (#582)
                item = [notabs(field) for field in item]
                try:
                    s.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (item[0], ftstr[item[0].startswith('.')], item[1],
                                                              ftstr[item[2]], item[3], item[4], item[5]))

                    log.trace('Adding cookie for %s. key: %s value: %s' % (item[0], item[4], item[5]))
                    count += 1
                except:
                    to_hex = lambda x: ''.join([hex(ord(c))[2:].zfill(2) for c in x])
                    i = 0
                    for val in item:
                        if isinstance(val, basestring):
                            log.debug('item[%s]: %s' % (i, to_hex(val)))
                        else:
                            log.debug('item[%s]: %s' % (i, val))
                        i += 1
                    failed += 1

            except UnicodeDecodeError:
                # for some god awful reason the sqlite module can throw UnicodeDecodeError ...
                log.debug('got UnicodeDecodeError from sqlite, ignored')
                failed += 1
            except StopIteration:
                break

        log.debug('Added %s cookies to jar. %s failed (non-ascii)' % (count, failed))

        s.seek(0)
        con.close()

        cookie_jar = cookielib.MozillaCookieJar()
        cookie_jar._really_load(s, '', True, True)
        return cookie_jar

    def on_task_start(self, task, config):
        """Task starting, install cookiejar"""
        import os
        config = self.prepare_config(config)
        cookie_type = config.get('type')
        cookie_file = os.path.expanduser(config.get('file'))
        if cookie_file in self.cookiejars:
            log.debug('Loading cookiejar from cache.')
            cj = self.cookiejars[cookie_file]
        elif cookie_type == 'firefox3':
            log.debug('Loading %s cookies' % cookie_type)
            cj = self.sqlite2cookie(cookie_file)
        else:
            if cookie_type == 'mozilla':
                log.debug('Loading %s cookies' % cookie_type)
                cj = cookielib.MozillaCookieJar()
            elif cookie_type == 'lwp':
                log.debug('Loading %s cookies' % cookie_type)
                cj = cookielib.LWPCookieJar()
            else:
                raise plugin.PluginError('Unknown cookie type %s' % cookie_type, log)

            try:
                cj.load(filename=cookie_file, ignore_expires=True)
                log.debug('%s cookies loaded' % cookie_type)
            except (cookielib.LoadError, IOError):
                import sys
                raise plugin.PluginError('Cookies could not be loaded: %s' % sys.exc_info()[1], log)

        if cookie_file not in self.cookiejars:
            self.cookiejars[cookie_file] = cj

        # Add cookiejar to our requests session
        task.requests.add_cookiejar(cj)
        # Add handler to urllib2 default opener for backwards compatibility
        handler = urllib2.HTTPCookieProcessor(cj)
        if urllib2._opener:
            log.debug('Adding HTTPCookieProcessor to default opener')
            urllib2._opener.add_handler(handler)
        else:
            log.debug('Creating new opener and installing it')
            urllib2.install_opener(urllib2.build_opener(handler))

    def on_task_exit(self, task, config):
        """Task exiting, remove cookiejar"""
        log.debug('Removing urllib2 opener')
        urllib2.install_opener(None)

    # Task aborted, unhook the cookiejar
    on_task_abort = on_task_exit


@event('plugin.register')
def register_plugin():
    plugin.register(PluginCookies, 'cookies', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_deluge
from __future__ import unicode_literals, division, absolute_import
import base64
import glob
import logging
import pkg_resources
import os
import re
import sys
import time
import warnings

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.template import RenderError
from flexget.utils.pathscrub import pathscrub

log = logging.getLogger('deluge')


def add_deluge_windows_install_dir_to_sys_path():
# Deluge does not install to python system on Windows, add the install directory to sys.path if it is found
    if not (sys.platform.startswith('win') or os.environ.get('ProgramFiles')):
        return
    deluge_dir = os.path.join(os.environ['ProgramFiles'], 'Deluge')
    log.debug('Looking for deluge install in %s' % deluge_dir)
    if not os.path.isdir(deluge_dir):
        return
    deluge_egg = glob.glob(os.path.join(deluge_dir, 'deluge-*-py2.?.egg'))
    if not deluge_egg:
        return
    minor_version = int(re.search(r'py2\.(\d).egg', deluge_egg[0]).group(1))
    if minor_version != sys.version_info[1]:
        log.verbose('Cannot use deluge from install directory because its python version doesn\'t match.')
        return
    log.debug('Found deluge install in %s adding to sys.path' % deluge_dir)
    sys.path.append(deluge_dir)
    for item in os.listdir(deluge_dir):
        if item.endswith(('.egg', '.zip')):
            sys.path.append(os.path.join(deluge_dir, item))

add_deluge_windows_install_dir_to_sys_path()

# Some twisted import is throwing a warning see #2434
warnings.filterwarnings('ignore', message='Not importing directory .*')

try:
    from twisted.python import log as twisted_log
    from twisted.internet.main import installReactor
    from twisted.internet.selectreactor import SelectReactor

    class PausingReactor(SelectReactor):
        """A SelectReactor that can be paused and resumed."""

        def __init__(self):
            SelectReactor.__init__(self)
            self.paused = False
            self._return_value = None
            self._release_requested = False
            self._mainLoopGen = None

            # Older versions of twisted do not have the _started attribute, make it a synonym for running in that case
            if not hasattr(self, '_started'):
                PausingReactor._started = property(lambda self: self.running)

        def _mainLoopGenerator(self):
            """Generator that acts as mainLoop, but yields when requested."""
            while self._started:
                try:
                    while self._started:
                        if self._release_requested:
                            self._release_requested = False
                            self.paused = True
                            yield self._return_value
                            self.paused = False
                        self.iterate()
                except KeyboardInterrupt:
                    # Keyboard interrupt pauses the reactor
                    self.pause()
                except GeneratorExit:
                    # GeneratorExit means stop the generator; Do it cleanly by stopping the whole reactor.
                    log.debug('Got GeneratorExit, stopping reactor.', exc_info=True)
                    self.paused = False
                    self.stop()
                except:
                    twisted_log.msg("Unexpected error in main loop.")
                    twisted_log.err()
                else:
                    twisted_log.msg('Main loop terminated.')

        def run(self, installSignalHandlers=False):
            """Starts or resumes the reactor."""
            if not self._started:
                self.startRunning(installSignalHandlers)
                self._mainLoopGen = self._mainLoopGenerator()
            try:
                return self._mainLoopGen.next()
            except StopIteration:
                pass

        def pause(self, return_value=None):
            """Causes reactor to pause after this iteration.
            If :return_value: is specified, it will be returned by the reactor.run call."""
            self._return_value = return_value
            self._release_requested = True

        def stop(self):
            """Stops the reactor."""
            SelectReactor.stop(self)
            # If this was called while the reactor was paused we have to resume in order for it to complete
            if self.paused:
                self.run()

            # These need to be re-registered so that the PausingReactor can be safely restarted after a stop
            self.addSystemEventTrigger('during', 'shutdown', self.crash)
            self.addSystemEventTrigger('during', 'shutdown', self.disconnectAll)

    # Configure twisted to use the PausingReactor.
    installReactor(PausingReactor())

except ImportError:
    # If twisted is not found, errors will be shown later
    pass


# Define a base class with some methods that are used for all deluge versions
class DelugePlugin(object):
    """Base class for deluge plugins, contains settings and methods for connecting to a deluge daemon."""

    def prepare_connection_info(self, config):
        config.setdefault('host', 'localhost')
        config.setdefault('port', 58846)
        if 'user' in config or 'pass' in config:
            warnings.warn('deluge `user` and `pass` options have been renamed `username` and `password`',
                          DeprecationWarning)
            config.setdefault('username', config.get('user', ''))
            config.setdefault('password', config.get('pass', ''))
        config.setdefault('username', '')
        config.setdefault('password', '')

    def on_task_start(self, task, config):
        """Raise a DependencyError if our dependencies aren't available"""
        # This is overridden by OutputDeluge to add deluge 1.1 support
        try:
            from deluge.ui.client import client
        except ImportError as e:
            log.debug('Error importing deluge: %s' % e)
            raise plugin.DependencyError('output_deluge', 'deluge',
                                  'Deluge module and it\'s dependencies required. ImportError: %s' % e, log)
        try:
            from twisted.internet import reactor
        except:
            raise plugin.DependencyError('output_deluge', 'twisted.internet', 'Twisted.internet package required', log)
        log.debug('Using deluge 1.2 api')

    def on_task_abort(self, task, config):
        pass

# Add some more methods to the base class if we are using deluge 1.2+
try:
    from twisted.internet import reactor
    from deluge.ui.client import client
    from deluge.ui.common import get_localhost_auth

    class DelugePlugin(DelugePlugin):

        def on_disconnect(self):
            """Pauses the reactor. Gets called when we disconnect from the daemon."""
            # pause the reactor, so flexget can continue
            reactor.callLater(0, reactor.pause)

        def on_connect_fail(self, result):
            """Pauses the reactor, returns PluginError. Gets called when connection to deluge daemon fails."""
            log.debug('Connect to deluge daemon failed, result: %s' % result)
            reactor.callLater(0, reactor.pause, plugin.PluginError('Could not connect to deluge daemon', log))

        def on_connect_success(self, result, task, config):
            """Gets called when successfully connected to the daemon. Should do the work then call client.disconnect"""
            raise NotImplementedError

        def connect(self, task, config):
            """Connects to the deluge daemon and runs on_connect_success """

            if config['host'] in ['localhost', '127.0.0.1'] and not config.get('username'):
                # If an username is not specified, we have to do a lookup for the localclient username/password
                auth = get_localhost_auth()
                if auth[0]:
                    config['username'], config['password'] = auth
                else:
                    raise plugin.PluginError('Unable to get local authentication info for Deluge. You may need to '
                                             'specify an username and password from your Deluge auth file.')

            client.set_disconnect_callback(self.on_disconnect)

            d = client.connect(
                host=config['host'],
                port=config['port'],
                username=config['username'],
                password=config['password'])

            d.addCallback(self.on_connect_success, task, config).addErrback(self.on_connect_fail)
            result = reactor.run()
            if isinstance(result, Exception):
                raise result
            return result

    @event('manager.shutdown')
    def stop_reactor(manager):
        """Shut down the twisted reactor after all tasks have run."""
        if not reactor._stopped:
            log.debug('Stopping twisted reactor.')
            reactor.stop()

except (ImportError, pkg_resources.DistributionNotFound):
    pass


class InputDeluge(DelugePlugin):
    """Create entries for torrents in the deluge session."""
    #
    settings_map = {
        'name': 'title',
        'hash': 'torrent_info_hash',
        'num_peers': 'torrent_peers',
        'num_seeds': 'torrent_seeds',
        'progress': 'deluge_progress',
        'seeding_time': ('deluge_seed_time', lambda time: time / 3600),
        'private': 'deluge_private',
        'state': 'deluge_state',
        'eta': 'deluge_eta',
        'ratio': 'deluge_ratio',
        'move_on_completed_path': 'deluge_movedone',
        'save_path': 'deluge_path',
        'label': 'deluge_label',
        'total_size': ('content_size', lambda size: size / 1024 / 1024),
        'files': ('content_files', lambda file_dicts: [f['path'] for f in file_dicts])}

    def __init__(self):
        self.entries = []

    schema = {
        'anyOf': [
            {'type': 'boolean'},
            {
                'type': 'object',
                'properties': {
                    'host': {'type': 'string'},
                    'port': {'type': 'integer'},
                    'username': {'type': 'string'},
                    'password': {'type': 'string'},
                    'config_path': {'type': 'string', 'format': 'path'},
                    'filter': {
                        'type': 'object',
                        'properties': {
                            'label': {'type': 'string'},
                            'state': {
                                'type': 'string',
                                'enum': ['active', 'downloading', 'seeding', 'queued', 'paused']
                            }
                        },
                        'additionalProperties': False
                    }
                },
                'additionalProperties': False
            }
        ]
    }

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {}
        if 'filter' in config:
            filter = config['filter']
            if 'label' in filter:
                filter['label'] = filter['label'].lower()
            if 'state' in filter:
                filter['state'] = filter['state'].capitalize()
        self.prepare_connection_info(config)
        return config

    def on_task_input(self, task, config):
        """Generates and returns a list of entries from the deluge daemon."""
        # Reset the entries list
        self.entries = []
        # Call connect, entries get generated if everything is successful
        self.connect(task, self.prepare_config(config))
        return self.entries

    def on_connect_success(self, result, task, config):
        """Creates a list of FlexGet entries from items loaded in deluge and stores them to self.entries"""
        from deluge.ui.client import client

        def on_get_torrents_status(torrents):
            config_path = os.path.expanduser(config.get('config_path', ''))
            for hash, torrent_dict in torrents.iteritems():
                # Make sure it has a url so no plugins crash
                entry = Entry(deluge_id=hash, url='')
                if config_path:
                    torrent_path = os.path.join(config_path, 'state', hash + '.torrent')
                    if os.path.isfile(torrent_path):
                        entry['location'] = torrent_path
                        if not torrent_path.startswith('/'):
                            torrent_path = '/' + torrent_path
                        entry['url'] = 'file://' + torrent_path
                    else:
                        log.warning('Did not find torrent file at %s' % torrent_path)
                for key, value in torrent_dict.iteritems():
                    flexget_key = self.settings_map[key]
                    if isinstance(flexget_key, tuple):
                        flexget_key, format_func = flexget_key
                        value = format_func(value)
                    entry[flexget_key] = value
                self.entries.append(entry)
            client.disconnect()
        filter = config.get('filter', {})
        client.core.get_torrents_status(filter, self.settings_map.keys()).addCallback(on_get_torrents_status)


class OutputDeluge(DelugePlugin):
    """Add the torrents directly to deluge, supporting custom save paths."""
    schema = {
        'anyOf': [
            {'type': 'boolean'},
            {
                'type': 'object',
                'properties': {
                    'host': {'type': 'string'},
                    'port': {'type': 'integer'},
                    'username': {'type': 'string'},
                    'password': {'type': 'string'},
                    'path': {'type': 'string'},
                    'movedone': {'type': 'string'},
                    'label': {'type': 'string'},
                    'queuetotop': {'type': 'boolean'},
                    'automanaged': {'type': 'boolean'},
                    'maxupspeed': {'type': 'number'},
                    'maxdownspeed': {'type': 'number'},
                    'maxconnections': {'type': 'integer'},
                    'maxupslots': {'type': 'integer'},
                    'ratio': {'type': 'number'},
                    'removeatratio': {'type': 'boolean'},
                    'addpaused': {'type': 'boolean'},
                    'compact': {'type': 'boolean'},
                    'content_filename': {'type': 'string'},
                    'main_file_only': {'type': 'boolean'},
                    'enabled': {'type': 'boolean'},
                },
                'additionalProperties': False
            }
        ]
    }

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {'enabled': config}
        self.prepare_connection_info(config)
        config.setdefault('enabled', True)
        config.setdefault('path', '')
        config.setdefault('movedone', '')
        config.setdefault('label', '')
        return config

    def __init__(self):
        self.deluge12 = None
        self.deluge_version = None
        self.options = {'maxupspeed': 'max_upload_speed', 'maxdownspeed': 'max_download_speed',
                        'maxconnections': 'max_connections', 'maxupslots': 'max_upload_slots',
                        'automanaged': 'auto_managed', 'ratio': 'stop_ratio', 'removeatratio': 'remove_at_ratio',
                        'addpaused': 'add_paused', 'compact': 'compact_allocation'}

    @plugin.priority(120)
    def on_task_start(self, task, config):
        """
        Detect what version of deluge is loaded.
        """

        if self.deluge12 is None:
            logger = log.info if task.options.test else log.debug
            try:
                log.debug('Looking for deluge 1.1 API')
                from deluge.ui.client import sclient
                log.debug('1.1 API found')
            except ImportError:
                log.debug('Looking for deluge 1.2 API')
                DelugePlugin.on_task_start(self, task, config)
                logger('Using deluge 1.2 api')
                self.deluge12 = True
            else:
                logger('Using deluge 1.1 api')
                self.deluge12 = False

    @plugin.priority(120)
    def on_task_download(self, task, config):
        """
        Call download plugin to generate the temp files we will load into deluge
        then verify they are valid torrents
        """
        import deluge.ui.common
        config = self.prepare_config(config)
        if not config['enabled']:
            return
        # If the download plugin is not enabled, we need to call it to get our temp .torrent files
        if not 'download' in task.config:
            download = plugin.get_plugin_by_name('download')
            for entry in task.accepted:
                if not entry.get('deluge_id'):
                    download.instance.get_temp_file(task, entry, handle_magnets=True)

        # Check torrent files are valid
        for entry in task.accepted:
            if os.path.exists(entry.get('file', '')):
                # Check if downloaded file is a valid torrent file
                try:
                    deluge.ui.common.TorrentInfo(entry['file'])
                except Exception:
                    entry.fail('Invalid torrent file')
                    log.error('Torrent file appears invalid for: %s', entry['title'])

    @plugin.priority(135)
    def on_task_output(self, task, config):
        """Add torrents to deluge at exit."""
        config = self.prepare_config(config)
        # don't add when learning
        if task.options.learn:
            return
        if not config['enabled'] or not (task.accepted or task.options.test):
            return

        add_to_deluge = self.connect if self.deluge12 else self.add_to_deluge11
        add_to_deluge(task, config)
        # Clean up temp file if download plugin is not configured for this task
        if not 'download' in task.config:
            for entry in task.accepted + task.failed:
                if os.path.exists(entry.get('file', '')):
                    os.remove(entry['file'])
                    del(entry['file'])

    def add_to_deluge11(self, task, config):
        """Add torrents to deluge using deluge 1.1.x api."""
        try:
            from deluge.ui.client import sclient
        except:
            raise plugin.PluginError('Deluge module required', log)

        sclient.set_core_uri()
        for entry in task.accepted:
            try:
                before = sclient.get_session_state()
            except Exception as e:
                (errno, msg) = e.args
                raise plugin.PluginError('Could not communicate with deluge core. %s' % msg, log)
            if task.options.test:
                return
            opts = {}
            path = entry.get('path', config['path'])
            if path:
                try:
                    opts['download_location'] = os.path.expanduser(entry.render(path))
                except RenderError as e:
                    log.error('Could not set path for %s: %s' % (entry['title'], e))
            for fopt, dopt in self.options.iteritems():
                value = entry.get(fopt, config.get(fopt))
                if value is not None:
                    opts[dopt] = value
                    if fopt == 'ratio':
                        opts['stop_at_ratio'] = True

            # check that file is downloaded
            if not 'file' in entry:
                entry.fail('file missing?')
                continue

            # see that temp file is present
            if not os.path.exists(entry['file']):
                tmp_path = os.path.join(task.manager.config_base, 'temp')
                log.debug('entry: %s' % entry)
                log.debug('temp: %s' % ', '.join(os.listdir(tmp_path)))
                entry.fail('Downloaded temp file \'%s\' doesn\'t exist!?' % entry['file'])
                continue

            sclient.add_torrent_file([entry['file']], [opts])
            log.info('%s torrent added to deluge with options %s' % (entry['title'], opts))

            movedone = entry.get('movedone', config['movedone'])
            label = entry.get('label', config['label']).lower()
            queuetotop = entry.get('queuetotop', config.get('queuetotop'))

            # Sometimes deluge takes a moment to add the torrent, wait a second.
            time.sleep(2)
            after = sclient.get_session_state()
            for item in after:
                # find torrentid of just added torrent
                if not item in before:
                    try:
                        movedone = entry.render(movedone)
                    except RenderError as e:
                        log.error('Could not set movedone for %s: %s' % (entry['title'], e))
                        movedone = ''
                    if movedone:
                        movedone = os.path.expanduser(movedone)
                        if not os.path.isdir(movedone):
                            log.debug('movedone path %s doesn\'t exist, creating' % movedone)
                            os.makedirs(movedone)
                        log.debug('%s move on complete set to %s' % (entry['title'], movedone))
                        sclient.set_torrent_move_on_completed(item, True)
                        sclient.set_torrent_move_on_completed_path(item, movedone)
                    if label:
                        if not 'label' in sclient.get_enabled_plugins():
                            sclient.enable_plugin('label')
                        if not label in sclient.label_get_labels():
                            sclient.label_add(label)
                        log.debug('%s label set to \'%s\'' % (entry['title'], label))
                        sclient.label_set_torrent(item, label)
                    if queuetotop:
                        log.debug('%s moved to top of queue' % entry['title'])
                        sclient.queue_top([item])
                    break
            else:
                log.info('%s is already loaded in deluge. Cannot change label, movedone, or queuetotop' %
                         entry['title'])

    def on_connect_success(self, result, task, config):
        """Gets called when successfully connected to a daemon."""
        from deluge.ui.client import client
        from twisted.internet import reactor, defer

        if not result:
            log.debug('on_connect_success returned a failed result. BUG?')

        if task.options.test:
            log.debug('Test connection to deluge daemon successful.')
            client.disconnect()
            return

        def format_label(label):
            """Makes a string compliant with deluge label naming rules"""
            return re.sub('[^\w-]+', '_', label.lower())

        def set_torrent_options(torrent_id, entry, opts):
            """Gets called when a torrent was added to the daemon."""
            dlist = []
            if not torrent_id:
                log.error('There was an error adding %s to deluge.' % entry['title'])
                # TODO: Fail entry? How can this happen still now?
                return
            log.info('%s successfully added to deluge.' % entry['title'])
            entry['deluge_id'] = torrent_id

            def create_path(result, path):
                """Creates the specified path if deluge is older than 1.3"""
                from deluge.common import VersionSplit
                # Before 1.3, deluge would not create a non-existent move directory, so we need to.
                if VersionSplit('1.3.0') > VersionSplit(self.deluge_version):
                    if client.is_localhost():
                        if not os.path.isdir(path):
                            log.debug('path %s doesn\'t exist, creating' % path)
                            os.makedirs(path)
                    else:
                        log.warning('If path does not exist on the machine running the daemon, move will fail.')

            if opts.get('movedone'):
                dlist.append(version_deferred.addCallback(create_path, opts['movedone']))
                dlist.append(client.core.set_torrent_move_completed(torrent_id, True))
                dlist.append(client.core.set_torrent_move_completed_path(torrent_id, opts['movedone']))
                log.debug('%s move on complete set to %s' % (entry['title'], opts['movedone']))
            if opts.get('label'):

                def apply_label(result, torrent_id, label):
                    """Gets called after labels and torrent were added to deluge."""
                    return client.label.set_torrent(torrent_id, label)

                dlist.append(label_deferred.addCallback(apply_label, torrent_id, opts['label']))
            if opts.get('queuetotop') is not None:
                if opts['queuetotop']:
                    dlist.append(client.core.queue_top([torrent_id]))
                    log.debug('%s moved to top of queue' % entry['title'])
                else:
                    dlist.append(client.core.queue_bottom([torrent_id]))
                    log.debug('%s moved to bottom of queue' % entry['title'])

            def on_get_torrent_status(status):
                """Gets called with torrent status, including file info.
                Sets the torrent options which require knowledge of the current status of the torrent."""

                main_file_dlist = []

                # Determine where the file should be
                move_now_path = None
                if opts.get('movedone'):
                    if status['progress'] == 100:
                        move_now_path = opts['movedone']
                    else:
                        # Deluge will unset the move completed option if we move the storage, forgo setting proper
                        # path, in favor of leaving proper final location.
                        log.debug('Not moving storage for %s, as this will prevent movedone.' % entry['title'])
                elif opts.get('path'):
                    move_now_path = opts['path']

                if move_now_path and os.path.normpath(move_now_path) != os.path.normpath(status['save_path']):
                    main_file_dlist.append(version_deferred.addCallback(create_path, move_now_path))
                    log.debug('Moving storage for %s to %s' % (entry['title'], move_now_path))
                    main_file_dlist.append(client.core.move_storage([torrent_id], move_now_path))

                if opts.get('content_filename') or opts.get('main_file_only'):

                    def file_exists():
                        # Checks the download path as well as the move completed path for existence of the file
                        if os.path.exists(os.path.join(status['save_path'], filename)):
                            return True
                        elif status.get('move_on_completed') and status.get('move_on_completed_path'):
                            if os.path.exists(os.path.join(status['move_on_completed_path'], filename)):
                                return True
                        else:
                            return False

                    for file in status['files']:
                        # Only rename file if it is > 90% of the content
                        if file['size'] > (status['total_size'] * 0.9):
                            if opts.get('content_filename'):
                                filename = opts['content_filename'] + os.path.splitext(file['path'])[1]
                                counter = 1
                                if client.is_localhost():
                                    while file_exists():
                                        # Try appending a (#) suffix till a unique filename is found
                                        filename = ''.join([opts['content_filename'], '(', str(counter), ')',
                                                            os.path.splitext(file['path'])[1]])
                                        counter += 1
                                else:
                                    log.debug('Cannot ensure content_filename is unique '
                                              'when adding to a remote deluge daemon.')
                                log.debug('File %s in %s renamed to %s' % (file['path'], entry['title'], filename))
                                main_file_dlist.append(
                                    client.core.rename_files(torrent_id, [(file['index'], filename)]))
                            if opts.get('main_file_only'):
                                file_priorities = [1 if f['index'] == file['index'] else 0 for f in status['files']]
                                main_file_dlist.append(
                                    client.core.set_torrent_file_priorities(torrent_id, file_priorities))
                            break
                    else:
                        log.warning('No files in %s are > 90%% of content size, no files renamed.' % entry['title'])

                return defer.DeferredList(main_file_dlist)

            status_keys = ['files', 'total_size', 'save_path', 'move_on_completed_path',
                           'move_on_completed', 'progress']
            dlist.append(client.core.get_torrent_status(torrent_id, status_keys).addCallback(on_get_torrent_status))

            return defer.DeferredList(dlist)

        def on_fail(result, task, entry):
            """Gets called when daemon reports a failure adding the torrent."""
            log.info('%s was not added to deluge! %s' % (entry['title'], result))
            entry.fail('Could not be added to deluge')

        # dlist is a list of deferreds that must complete before we exit
        dlist = []
        # loop through entries to get a list of labels to add
        labels = set([format_label(entry['label']) for entry in task.accepted if entry.get('label')])
        if config.get('label'):
            labels.add(format_label(config['label']))
        label_deferred = defer.succeed(True)
        if labels:
            # Make sure the label plugin is available and enabled, then add appropriate labels

            def on_get_enabled_plugins(plugins):
                """Gets called with the list of enabled deluge plugins."""

                def on_label_enabled(result):
                    """ This runs when we verify the label plugin is enabled. """

                    def on_get_labels(d_labels):
                        """Gets available labels from deluge, and adds any new labels we need."""
                        dlist = []
                        for label in labels:
                            if not label in d_labels:
                                log.debug('Adding the label %s to deluge' % label)
                                dlist.append(client.label.add(label))
                        return defer.DeferredList(dlist)

                    return client.label.get_labels().addCallback(on_get_labels)

                if 'Label' in plugins:
                    return on_label_enabled(True)
                else:
                    # Label plugin isn't enabled, so we check if it's available and enable it.

                    def on_get_available_plugins(plugins):
                        """Gets plugins available to deluge, enables Label plugin if available."""
                        if 'Label' in plugins:
                            log.debug('Enabling label plugin in deluge')
                            return client.core.enable_plugin('Label').addCallback(on_label_enabled)
                        else:
                            log.error('Label plugin is not installed in deluge')

                    return client.core.get_available_plugins().addCallback(on_get_available_plugins)

            label_deferred = client.core.get_enabled_plugins().addCallback(on_get_enabled_plugins)
            dlist.append(label_deferred)

        def on_get_daemon_info(ver):
            """Gets called with the daemon version info, stores it in self."""
            log.debug('deluge version %s' % ver)
            self.deluge_version = ver

        version_deferred = client.daemon.info().addCallback(on_get_daemon_info)
        dlist.append(version_deferred)

        def on_get_session_state(torrent_ids):
            """Gets called with a list of torrent_ids loaded in the deluge session.
            Adds new torrents and modifies the settings for ones already in the session."""
            dlist = []
            # add the torrents
            for entry in task.accepted:

                def add_entry(entry, opts):
                    """Adds an entry to the deluge session"""
                    magnet, filedump = None, None
                    if entry.get('url', '').startswith('magnet:'):
                        magnet = entry['url']
                    else:
                        if not os.path.exists(entry['file']):
                            entry.fail('Downloaded temp file \'%s\' doesn\'t exist!' % entry['file'])
                            del(entry['file'])
                            return
                        with open(entry['file'], 'rb') as f:
                            filedump = base64.encodestring(f.read())

                    log.verbose('Adding %s to deluge.' % entry['title'])
                    if magnet:
                        return client.core.add_torrent_magnet(magnet, opts)
                    else:
                        return client.core.add_torrent_file(entry['title'], filedump, opts)

                # Generate deluge options dict for torrent add
                add_opts = {}
                try:
                    path = entry.render(entry.get('path', config['path']))
                    if path:
                        add_opts['download_location'] = pathscrub(os.path.expanduser(path))
                except RenderError as e:
                    log.error('Could not set path for %s: %s' % (entry['title'], e))
                for fopt, dopt in self.options.iteritems():
                    value = entry.get(fopt, config.get(fopt))
                    if value is not None:
                        add_opts[dopt] = value
                        if fopt == 'ratio':
                            add_opts['stop_at_ratio'] = True
                # Make another set of options, that get set after the torrent has been added
                modify_opts = {'label': format_label(entry.get('label', config['label'])),
                               'queuetotop': entry.get('queuetotop', config.get('queuetotop')),
                               'main_file_only': entry.get('main_file_only', config.get('main_file_only', False))}
                try:
                    movedone = entry.render(entry.get('movedone', config['movedone']))
                    modify_opts['movedone'] = pathscrub(os.path.expanduser(movedone))
                except RenderError as e:
                    log.error('Error setting movedone for %s: %s' % (entry['title'], e))
                try:
                    content_filename = entry.get('content_filename', config.get('content_filename', ''))
                    modify_opts['content_filename'] = pathscrub(entry.render(content_filename))
                except RenderError as e:
                    log.error('Error setting content_filename for %s: %s' % (entry['title'], e))

                torrent_id = entry.get('deluge_id') or entry.get('torrent_info_hash')
                torrent_id = torrent_id and torrent_id.lower()
                if torrent_id in torrent_ids:
                    log.info('%s is already loaded in deluge, setting options' % entry['title'])
                    # Entry has a deluge id, verify the torrent is still in the deluge session and apply options
                    # Since this is already loaded in deluge, we may also need to change the path
                    modify_opts['path'] = add_opts.pop('download_location', None)
                    dlist.extend([set_torrent_options(torrent_id, entry, modify_opts),
                                  client.core.set_torrent_options([torrent_id], add_opts)])
                else:
                    dlist.append(add_entry(entry, add_opts).addCallbacks(
                        set_torrent_options, on_fail, callbackArgs=(entry, modify_opts), errbackArgs=(task, entry)))
            return defer.DeferredList(dlist)
        dlist.append(client.core.get_session_state().addCallback(on_get_session_state))

        def on_complete(result):
            """Gets called when all of our tasks for deluge daemon are complete."""
            client.disconnect()
        tasks = defer.DeferredList(dlist).addBoth(on_complete)

        def on_timeout(result):
            """Gets called if tasks have not completed in 30 seconds.
            Should only happen when something goes wrong."""
            log.error('Timed out while adding torrents to deluge.')
            log.debug('dlist: %s' % result.resultList)
            client.disconnect()

        # Schedule a disconnect to happen if FlexGet hangs while connected to Deluge
        # Leave the timeout long, to give time for possible lookups to occur
        reactor.callLater(600, lambda: tasks.called or on_timeout(tasks))

    def on_task_exit(self, task, config):
        """Make sure all temp files are cleaned up when task exits"""
        # If download plugin is enabled, it will handle cleanup.
        if not 'download' in task.config:
            download = plugin.get_plugin_by_name('download')
            download.instance.cleanup_temp_files(task)

    def on_task_abort(self, task, config):
        """Make sure normal cleanup tasks still happen on abort."""
        DelugePlugin.on_task_abort(self, task, config)
        self.on_task_exit(task, config)


@event('plugin.register')
def register_plugin():
    plugin.register(InputDeluge, 'from_deluge', api_ver=2)
    plugin.register(OutputDeluge, 'deluge', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_entry_trace
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('entry_trace')


def on_entry_action(entry, act=None, task=None, reason=None, **kwargs):
    entry[act.lower() + '_by'] = task.current_plugin
    entry.pop('reason', None)
    if reason:
        entry['reason'] = reason


class EntryOperations(object):
    """
    Records accept, reject and fail metainfo into entries.

    Creates fields::

      accepted_by: <plugin name>
      rejected_by: <plugin name>
      failed_by: <plugin name>

      reason: <given message by plugin>
    """

    @plugin.priority(-255)
    def on_task_input(self, task, config):
        for entry in task.all_entries:
            entry.on_accept(on_entry_action, act='accepted', task=task)
            entry.on_reject(on_entry_action, act='rejected', task=task)
            entry.on_fail(on_entry_action, act='failed', task=task)


@event('plugin.register')
def register_plugin():
    plugin.register(EntryOperations, 'entry_operations', builtin=True, api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_formlogin
from __future__ import unicode_literals, division, absolute_import
import logging
import os
import urllib2

from flexget import plugin
from flexget.event import event

log = logging.getLogger('formlogin')


class FormLogin(object):
    """
    Login on form
    """

    schema = {
        'type': 'object',
        'properties': {
            'url': {'type': 'string', 'format': 'url'},
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'userfield': {'type': 'string'},
            'passfield': {'type': 'string'}
        },
        'required': ['url', 'username', 'password'],
        'additionalProperties': False
    }

    def on_task_start(self, task, config):
        try:
            from mechanize import Browser
        except ImportError:
            raise plugin.PluginError('mechanize required (python module), please install it.', log)

        userfield = config.get('userfield', 'username')
        passfield = config.get('passfield', 'password')

        url = config['url']
        username = config['username']
        password = config['password']

        br = Browser()
        br.set_handle_robots(False)
        try:
            br.open(url)
        except Exception as e:
            # TODO: improve error handling
            raise plugin.PluginError('Unable to post login form', log)

        #br.set_debug_redirects(True)
        #br.set_debug_responses(True)
        #br.set_debug_http(True)

        for form in br.forms():
            loginform = form

            try:
                loginform[userfield] = username
                loginform[passfield] = password
                break
            except Exception as e:
                pass
        else:
            received = os.path.join(task.manager.config_base, 'received')
            if not os.path.isdir(received):
                os.mkdir(received)
            filename = os.path.join(received, '%s.formlogin.html' % task.name)
            with open(filename, 'w') as f:
                f.write(br.response().get_data())
            log.critical('I have saved the login page content to %s for you to view' % filename)
            raise plugin.PluginError('Unable to find login fields', log)

        br.form = loginform

        br.submit()

        cookiejar = br._ua_handlers["_cookies"].cookiejar

        # Add cookiejar to our requests session
        task.requests.add_cookiejar(cookiejar)
        # Add handler to urllib2 default opener for backwards compatibility
        handler = urllib2.HTTPCookieProcessor(cookiejar)
        if urllib2._opener:
            log.debug('Adding HTTPCookieProcessor to default opener')
            urllib2._opener.add_handler(handler)
        else:
            log.debug('Creating new opener and installing it')
            urllib2.install_opener(urllib2.build_opener(handler))

    def on_task_exit(self, task, config):
        """Task exiting, remove cookiejar"""
        log.debug('Removing urllib2 opener')
        urllib2.install_opener(None)

    # Task aborted, unhook the cookiejar
    on_task_abort = on_task_exit

@event('plugin.register')
def register_plugin():
    plugin.register(FormLogin, 'form', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_include
from __future__ import unicode_literals, division, absolute_import
import logging
import os
import yaml

from flexget import plugin
from flexget.config_schema import one_or_more, process_config
from flexget.event import event
from flexget.utils.tools import MergeException, merge_dict_from_to

log = logging.getLogger('include')


class PluginInclude(object):
    """
    Include configuration from another yaml file.

    Example::

      include: series.yml

    File content must be valid for a task configuration
    """

    schema = one_or_more({'type': 'string'})

    @plugin.priority(254)
    def on_task_start(self, task, config):
        if not config:
            return

        files = config
        if isinstance(config, basestring):
            files = [config]

        for name in files:
            name = os.path.expanduser(name)
            if not os.path.isabs(name):
                name = os.path.join(task.manager.config_base, name)
            include = yaml.load(file(name))
            errors = process_config(include, plugin.plugin_schemas(context='task'))
            if errors:
                log.error('Included file %s has invalid config:' % name)
                for error in errors:
                    log.error('[%s] %s', error.json_pointer, error.message)
                task.abort('Invalid config in included file %s' % name)
            log.debug('Merging %s into task %s' % (name, task.name))
            # merge
            try:
                merge_dict_from_to(include, task.config)
            except MergeException:
                raise plugin.PluginError('Failed to merge include file to task %s, incompatible datatypes' % task.name)

@event('plugin.register')
def register_plugin():
    plugin.register(PluginInclude, 'include', api_ver=2, builtin=True)

########NEW FILE########
__FILENAME__ = plugin_sort_by
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('sort_by')


class PluginSortBy(object):

    """
    Sort task entries based on a field

    Example::

      sort_by: title

    More complex::

      sort_by:
        field: imdb_score
        reverse: yes

    Reverse the order of the entries, without sorting on a field::

      sort_by:
        reverse: yes
    """

    schema = {
        'oneOf': [
            {'type': 'string'},
            {
                'type': 'object',
                'properties': {
                    'field': {'type': 'string'},
                    'reverse': {'type': 'boolean'}},
                'additionalProperties': False
            }
        ]
    }

    def on_task_filter(self, task, config):
        if isinstance(config, basestring):
            field = config
            reverse = False
        else:
            field = config.get('field', None)
            reverse = config.get('reverse', False)

        log.debug('sorting entries by: %s' % config)

        if not field:
            task.entries.reverse()
            task.accepted.reverse()
            return

        def cmp_helper(a, b):
            va = a.get(field, 0)
            vb = b.get(field, 0)
            return cmp(va, vb)

        task.entries.sort(cmp_helper, reverse=reverse)
        task.accepted.sort(cmp_helper, reverse=reverse)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginSortBy, 'sort_by', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_spy_headers
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib2
import httplib
import socket

from flexget import plugin
from flexget.event import event

log = logging.getLogger('spy_headers')


class CustomHTTPConnection(httplib.HTTPConnection):

    def __init__(self, *args, **kwargs):
        httplib.HTTPConnection.__init__(self, *args, **kwargs)
        self.stored_headers = []

    def putheader(self, header, value):
        self.stored_headers.append((header, value))
        httplib.HTTPConnection.putheader(self, header, value)


class HTTPCaptureHeaderHandler(urllib2.AbstractHTTPHandler):

    handler_order = 400

    def http_open(self, req):
        return self.do_open(CustomHTTPConnection, req)

    http_request = urllib2.AbstractHTTPHandler.do_request_
    https_request = urllib2.AbstractHTTPHandler.do_request_
    https_open = http_open

    def do_open(self, http_class, req):
        # All code here lifted directly from the python library
        host = req.get_host()
        if not host:
            from urllib2 import URLError
            raise URLError('no host given')

        h = http_class(host) # will parse host:port
        h.set_debuglevel(self._debuglevel)

        headers = dict(req.headers)
        headers.update(req.unredirected_hdrs)
        headers["Connection"] = "close"
        headers = dict(
            (name.title(), val) for name, val in headers.items())
        try:
            h.request(req.get_method(), req.get_selector(), req.data, headers)
            r = h.getresponse()
        except socket.error as err: # XXX what error?
            raise urllib2.URLError(err)
        r.recv = r.read
        fp = socket._fileobject(r, close=True)

        resp = urllib2.addinfourl(fp, r.msg, req.get_full_url())
        resp.code = r.status
        resp.msg = r.reason

        # After this our custom code!
        req.all_sent_headers = h.stored_headers
        log.info('Request  : %s' % req.get_full_url())
        log.info('Response : %s (%s)' % (resp.code, resp.msg))

        # log headers
        log.info('-- Headers: --------------------------')
        for sh in h.stored_headers:
            log.info('%s: %s' % (sh[0], sh[1]))
        log.info('--------------------------------------')

        return resp


class PluginSpyHeaders(object):
    """
        Logs all headers sent in http requests. Useful for resolving issues.

        WARNING: At the moment this modifies requests somehow!
    """

    schema = {'type': 'boolean'}

    @staticmethod
    def log_requests_headers(response, **kwargs):
        log.info('Request  : %s' % response.request.url)
        log.info('Response : %s (%s)' % (response.status_code, response.reason))
        log.info('-- Headers: --------------------------')
        for header, value in response.request.headers.iteritems():
            log.info('%s: %s' % (header, value))
        log.info('--------------------------------------')
        return response

    def on_task_start(self, task, config):
        if not config:
            return
        # Add our hook to the requests session
        task.requests.hooks['response'].append(self.log_requests_headers)
        # Backwards compatibility for plugins still using urllib
        if urllib2._opener:
            log.debug('Adding HTTPCaptureHeaderHandler to default opener')
            urllib2._opener.add_handler(HTTPCaptureHeaderHandler())
        else:
            log.debug('Creating new opener and installing it')
            opener = urllib2.build_opener(HTTPCaptureHeaderHandler())
            urllib2.install_opener(opener)

    def on_task_exit(self, task, config):
        """Task exiting, remove additions"""
        if not config:
            return
        task.requests.hooks['response'].remove(self.log_requests_headers)
        if urllib2._opener:
            log.debug('Removing urllib2 default opener')
            # TODO: this uninstalls all other handlers as well, but does it matter?
            urllib2.install_opener(None)

    # remove also on abort
    on_task_abort = on_task_exit


@event('plugin.register')
def register_plugin():
    plugin.register(PluginSpyHeaders, 'spy_headers', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_template
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import options, plugin
from flexget.event import event
from flexget.config_schema import register_config_key, one_or_more
from flexget.utils.tools import MergeException, merge_dict_from_to

log = logging.getLogger('template')


class PluginTemplate(object):
    """
    Appyly templates with preconfigured plugins to a task config.

    Example::

      template: movies

    Example, list of templates::

      template:
        - movies
        - imdb
    """

    schema = {
        'oneOf': [
            {
                'description': 'Apply multiple templates to this task.',
                'type': 'array',
                'items': {'$ref': '#/definitions/template'}},
            {
                'description': 'Apply a single template to this task.',
                'allOf': [{'$ref': '#/definitions/template'}]
            },
            {
                'description': 'Disable all templates on this task.',
                'type': 'boolean',
                'enum': [False]
            }
        ],
        'definitions': {
            'template': {
                'type': 'string',
                'description': 'Name of a template which will be applied to this task.',
                'links': [{'rel': 'settings', 'href': '/api/config/templates/{$}'}]
            }
        }
    }

    def prepare_config(self, config):
        if config is None or isinstance(config, bool):
            config = []
        elif isinstance(config, basestring):
            config = [config]
        return config

    @plugin.priority(256)
    def on_task_start(self, task, config):
        if config is False:  # handles 'template: no' form to turn off template on this task
            return
        # implements --template NAME
        if task.options.template:
            if not config or task.options.template not in config:
                task.abort('does not use `%s` template' % task.options.template, silent=True)

        config = self.prepare_config(config)

        # add global in except when disabled with no_global
        if 'no_global' in config:
            config.remove('no_global')
            if 'global' in config:
                config.remove('global')
        elif not 'global' in config:
            config.append('global')

        toplevel_templates = task.manager.config.get('templates', {})

        # apply templates
        for template in config:
            if template not in toplevel_templates:
                if template == 'global':
                    continue
                raise plugin.PluginError('Unable to find template %s for task %s' % (template, task.name), log)
            if toplevel_templates[template] is None:
                log.warning('Template `%s` is empty. Nothing to merge.' % template)
                continue
            log.debug('Merging template %s into task %s' % (template, task.name))

            # We make a copy here because we need to remove
            template_config = toplevel_templates[template]
            # When there are templates within templates we remove the template
            # key from the config and append it's items to our own
            if 'template' in template_config:
                nested_templates = self.prepare_config(template_config['template'])
                for nested_template in nested_templates:
                    if nested_template not in config:
                        config.append(nested_template)
                    else:
                        log.warning('Templates contain each other in a loop.')
                # Replace template_config with a copy without the template key, to avoid merging errors
                template_config = dict(template_config)
                del template_config['template']

            # Merge
            try:
                merge_dict_from_to(template_config, task.config)
            except MergeException as exc:
                raise plugin.PluginError('Failed to merge template %s to task %s. Error: %s' %
                                  (template, task.name, exc.value))

        log.trace('templates: %s' % config)


class DisablePlugin(object):
    """
    Allows disabling plugins when using templates.

    Example::

      templates:
        movies:
          download: ~/torrents/movies/
          .
          .

      tasks:
        nzbs:
          template: movies
          disable_plugin:
            - download
          sabnzbd:
            .
            .

      # Task nzbs uses all other configuration from template movies but removes the download plugin
    """

    schema = one_or_more({'type': 'string'})

    @plugin.priority(250)
    def on_task_start(self, task, config):
        if isinstance(config, basestring):
            config = [config]
        # let's disable them
        for disable in config:
            if disable in task.config:
                log.debug('disabling %s' % disable)
                del(task.config[disable])




@event('plugin.register')
def register_plugin():
    plugin.register(PluginTemplate, 'template', builtin=True, api_ver=2)
    plugin.register(DisablePlugin, 'disable_plugin', api_ver=2)


@event('config.register')
def register_config():
    root_config_schema = {
        'type': 'object',
        'additionalProperties': plugin.plugin_schemas(context='task')
    }
    register_config_key('templates', root_config_schema)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--template', metavar='NAME', help='execute tasks using given template')

########NEW FILE########
__FILENAME__ = plugin_transmission
from __future__ import unicode_literals, division, absolute_import
import os
from datetime import datetime
from netrc import netrc, NetrcParseError
import logging
import base64

from flexget import plugin, validator
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.template import RenderError
from flexget.utils.pathscrub import pathscrub
from flexget.utils.tools import parse_timedelta

log = logging.getLogger('transmission')


def save_opener(f):
    """
        Transmissionrpc sets a new default opener for urllib2
        We use this as a decorator to capture and restore it when needed
    """

    def new_f(self, *args, **kwargs):
        import urllib2
        prev_opener = urllib2._opener
        urllib2.install_opener(self.opener)
        try:
            f(self, *args, **kwargs)
            self.opener = urllib2._opener
        finally:
            urllib2.install_opener(prev_opener)
    return new_f


class TransmissionBase(object):

    def __init__(self):
        self.client = None
        self.opener = None

    def _validator(self, advanced):
        """Return config validator"""
        advanced.accept('text', key='host')
        advanced.accept('integer', key='port')
        # note that password is optional in transmission
        advanced.accept('file', key='netrc')
        advanced.accept('text', key='username')
        advanced.accept('text', key='password')
        advanced.accept('boolean', key='enabled')
        return advanced

    def prepare_config(self, config):
        if isinstance(config, bool):
            config = {'enabled': config}
        config.setdefault('enabled', True)
        config.setdefault('host', 'localhost')
        config.setdefault('port', 9091)
        if 'netrc' in config:
            netrc_path = os.path.expanduser(config['netrc'])
            try:
                config['username'], _, config['password'] = netrc(netrc_path).authenticators(config['host'])
            except IOError as e:
                log.error('netrc: unable to open: %s' % e.filename)
            except NetrcParseError as e:
                log.error('netrc: %s, file: %s, line: %s' % (e.msg, e.filename, e.lineno))
        return config

    def create_rpc_client(self, config):
        import transmissionrpc
        from transmissionrpc import TransmissionError
        from transmissionrpc import HTTPHandlerError

        user, password = config.get('username'), config.get('password')

        try:
            cli = transmissionrpc.Client(config['host'], config['port'], user, password)
        except TransmissionError as e:
            if isinstance(e.original, HTTPHandlerError):
                if e.original.code == 111:
                    raise plugin.PluginError("Cannot connect to transmission. Is it running?")
                elif e.original.code == 401:
                    raise plugin.PluginError("Username/password for transmission is incorrect. Cannot connect.")
                elif e.original.code == 110:
                    raise plugin.PluginError("Cannot connect to transmission: Connection timed out.")
                else:
                    raise plugin.PluginError("Error connecting to transmission: %s" % e.original.message)
            else:
                raise plugin.PluginError("Error connecting to transmission: %s" % e.message)
        return cli

    def torrent_info(self, torrent):
        done = torrent.totalSize > 0
        vloc = None
        best = None
        for t in torrent.files().iteritems():
            tf = t[1]
            if tf['selected']:
                if tf['size'] <= 0 or tf['completed'] < tf['size']:
                    done = False
                    break
                if not best or tf['size'] > best[1]:
                    best = (tf['name'], tf['size'])
        if done and best and (100*float(best[1])/float(torrent.totalSize)) >= 90:
            vloc = ('%s/%s' % (torrent.downloadDir, best[0])).replace('/', os.sep)
        return done, vloc

    @save_opener
    def on_task_start(self, task, config):
        try:
            import transmissionrpc
            from transmissionrpc import TransmissionError
            from transmissionrpc import HTTPHandlerError
        except:
            raise plugin.PluginError('Transmissionrpc module version 0.11 or higher required.', log)
        if [int(part) for part in transmissionrpc.__version__.split('.')] < [0, 11]:
            raise plugin.PluginError('Transmissionrpc module version 0.11 or higher required, please upgrade', log)
        config = self.prepare_config(config)
        if config['enabled']:
            if task.options.test:
                log.info('Trying to connect to transmission...')
                self.client = self.create_rpc_client(config)
                if self.client:
                    log.info('Successfully connected to transmission.')
                else:
                    log.error('It looks like there was a problem connecting to transmission.')


class PluginTransmissionInput(TransmissionBase):

    def validator(self):
        """Return config validator"""
        root = validator.factory()
        root.accept('boolean')
        advanced = root.accept('dict')
        self._validator(advanced)
        advanced.accept('boolean', key='onlycomplete')
        return root

    def prepare_config(self, config):
        config = TransmissionBase.prepare_config(self, config)
        config.setdefault('onlycomplete', True)
        return config

    def on_task_input(self, task, config):
        config = self.prepare_config(config)
        if not config['enabled']:
            return

        if not self.client:
            self.client = self.create_rpc_client(config)
        entries = []

        # Hack/Workaround for http://flexget.com/ticket/2002
        # TODO: Proper fix
        if 'username' in config and 'password' in config:
            self.client.http_handler.set_authentication(self.client.url, config['username'], config['password'])

        for torrent in self.client.get_torrents():
            downloaded, bigfella = self.torrent_info(torrent)
            if not config['onlycomplete'] or (downloaded and torrent.status == 'stopped'):
                entry = Entry(title=torrent.name,
                              url='file://%s' % torrent.torrentFile,
                              torrent_info_hash=torrent.hashString,
                              content_size=torrent.totalSize/(1024*1024))
                for attr in ['comment', 'downloadDir', 'isFinished', 'isPrivate']:
                    entry['transmission_' + attr] = getattr(torrent, attr)
                entry['transmission_trackers'] = [t['announce'] for t in torrent.trackers]
                entry['location'] = bigfella
                entries.append(entry)
        return entries


class PluginTransmission(TransmissionBase):
    """
    Add url from entry url to transmission

    Example::

      transmission:
        host: localhost
        port: 9091
        netrc: /home/flexget/.tmnetrc
        username: myusername
        password: mypassword
        path: the download location

    Default values for the config elements::

      transmission:
        host: localhost
        port: 9091
        enabled: yes
    """

    def validator(self):
        """Return config validator"""
        root = validator.factory()
        root.accept('boolean')
        advanced = root.accept('dict')
        self._validator(advanced)
        advanced.accept('text', key='path')
        advanced.accept('boolean', key='addpaused')
        advanced.accept('boolean', key='honourlimits')
        advanced.accept('integer', key='bandwidthpriority')
        advanced.accept('integer', key='maxconnections')
        advanced.accept('number', key='maxupspeed')
        advanced.accept('number', key='maxdownspeed')
        advanced.accept('number', key='ratio')
        return root

    @plugin.priority(120)
    def on_task_download(self, task, config):
        """
            Call download plugin to generate the temp files we will load
            into deluge then verify they are valid torrents
        """
        config = self.prepare_config(config)
        if not config['enabled']:
            return
        # If the download plugin is not enabled, we need to call it to get
        # our temp .torrent files
        if not 'download' in task.config:
            download = plugin.get_plugin_by_name('download')
            download.instance.get_temp_files(task, handle_magnets=True, fail_html=True)

    @plugin.priority(135)
    @save_opener
    def on_task_output(self, task, config):
        from transmissionrpc import TransmissionError
        config = self.prepare_config(config)
        # don't add when learning
        if task.options.learn:
            return
        if not config['enabled']:
            return
        # Do not run if there is nothing to do
        if not task.accepted:
            return
        if self.client is None:
            self.client = self.create_rpc_client(config)
            if self.client:
                log.debug('Successfully connected to transmission.')
            else:
                raise plugin.PluginError("Couldn't connect to transmission.")
        if task.accepted:
            self.add_to_transmission(self.client, task, config)

    def _make_torrent_options_dict(self, config, entry):

        opt_dic = {}

        for opt_key in ('path', 'addpaused', 'honourlimits', 'bandwidthpriority',
                        'maxconnections', 'maxupspeed', 'maxdownspeed', 'ratio'):
            if opt_key in entry:
                opt_dic[opt_key] = entry[opt_key]
            elif opt_key in config:
                opt_dic[opt_key] = config[opt_key]

        options = {'add': {}, 'change': {}}

        add = options['add']
        if opt_dic.get('path'):
            try:
                path = os.path.expanduser(entry.render(opt_dic['path']))
                add['download_dir'] = pathscrub(path).encode('utf-8')
            except RenderError as e:
                log.error('Error setting path for %s: %s' % (entry['title'], e))
        if 'addpaused' in opt_dic:
            add['paused'] = opt_dic['addpaused']
        if 'bandwidthpriority' in opt_dic:
            add['bandwidthPriority'] = opt_dic['bandwidthpriority']
        if 'maxconnections' in opt_dic:
            add['peer_limit'] = opt_dic['maxconnections']

        change = options['change']
        if 'honourlimits' in opt_dic and not opt_dic['honourlimits']:
            change['honorsSessionLimits'] = False
        if 'maxupspeed' in opt_dic:
            change['uploadLimit'] = opt_dic['maxupspeed']
            change['uploadLimited'] = True
        if 'maxdownspeed' in opt_dic:
            change['downloadLimit'] = opt_dic['maxdownspeed']
            change['downloadLimited'] = True

        if 'ratio' in opt_dic:
            change['seedRatioLimit'] = opt_dic['ratio']
            if opt_dic['ratio'] == -1:
                # seedRatioMode:
                # 0 follow the global settings
                # 1 override the global settings, seeding until a certain ratio
                # 2 override the global settings, seeding regardless of ratio
                change['seedRatioMode'] = 2
            else:
                change['seedRatioMode'] = 1

        return options

    def add_to_transmission(self, cli, task, config):
        """Adds accepted entries to transmission """
        from transmissionrpc import TransmissionError
        for entry in task.accepted:
            if task.options.test:
                log.info('Would add %s to transmission' % entry['url'])
                continue
            options = self._make_torrent_options_dict(config, entry)

            downloaded = not entry['url'].startswith('magnet:')

            # Check that file is downloaded
            if downloaded and not 'file' in entry:
                entry.fail('file missing?')
                continue

            # Verify the temp file exists
            if downloaded and not os.path.exists(entry['file']):
                tmp_path = os.path.join(task.manager.config_base, 'temp')
                log.debug('entry: %s' % entry)
                log.debug('temp: %s' % ', '.join(os.listdir(tmp_path)))
                entry.fail("Downloaded temp file '%s' doesn't exist!?" % entry['file'])
                continue

            try:
                if downloaded:
                    with open(entry['file'], 'rb') as f:
                        filedump = base64.b64encode(f.read()).encode('utf-8')
                    r = cli.add_torrent(filedump, 30, **options['add'])
                else:
                    r = cli.add_torrent(entry['url'], timeout=30, **options['add'])
                if r:
                    torrent = r
                log.info('"%s" torrent added to transmission' % (entry['title']))
                if options['change'].keys():
                    cli.change_torrent(r.id, 30, **options['change'])
            except TransmissionError as e:
                log.debug('TransmissionError', exc_info=True)
                log.debug('Failed options dict: %s' % options)
                msg = 'TransmissionError: %s' % e.message or 'N/A'
                log.error(msg)
                entry.fail(msg)

    def on_task_exit(self, task, config):
        """Make sure all temp files are cleaned up when task exits"""
        # If download plugin is enabled, it will handle cleanup.
        if not 'download' in task.config:
            download = plugin.get_plugin_by_name('download')
            download.instance.cleanup_temp_files(task)

    on_task_abort = on_task_exit


class PluginTransmissionClean(TransmissionBase):
    """
    Remove completed torrents from Transmission.
    
    Examples::
      
      clean_transmission: yes  # ignore both time and ratio
      
      clean_transmission:      # matches time only
        finished_for: 2 hours
      
      clean_transmission:      # matches ratio only
        min_ratio: 0.5
      
      clean_transmission:      # matches time OR ratio
        finished_for: 2 hours
        min_ratio: 0.5
    
    Default values for the config elements::
    
      clean_transmission:
        host: localhost
        port: 9091
        enabled: yes
    """

    def validator(self):
        """Return config validator"""
        root = validator.factory()
        root.accept('boolean')
        advanced = root.accept('dict')
        self._validator(advanced)
        advanced.accept('number', key='min_ratio')
        advanced.accept('interval', key='finished_for')
        advanced.accept('boolean', key='delete_files')
        return root

    def on_task_exit(self, task, config):
        config = self.prepare_config(config)
        if not config['enabled'] or task.options.learn:
            return
        if not self.client:
            self.client = self.create_rpc_client(config)
        nrat = float(config['min_ratio']) if 'min_ratio' in config else None
        nfor = parse_timedelta(config['finished_for']) if 'finished_for' in config else None
        delete_files = bool(config['delete_files']) if 'delete_files' in config else False
        
        remove_ids = []
        for torrent in self.client.get_torrents():
            log.verbose('Torrent "%s": status: "%s" - ratio: %s - date done: %s' % 
                        (torrent.name, torrent.status, torrent.ratio, torrent.date_done))
            downloaded, dummy = self.torrent_info(torrent)
            if (downloaded and ((nrat is None and nfor is None) or
                                (nrat and (nrat <= torrent.ratio)) or
                                (nfor and ((torrent.date_done + nfor) <= datetime.now())))):
                if task.options.test:
                    log.info('Would remove finished torrent `%s` from transmission' % torrent.name)
                    continue
                log.info('Removing finished torrent `%s` from transmission' % torrent.name)
                remove_ids.append(torrent.id)
        if remove_ids:
            self.client.remove_torrent(remove_ids, delete_files)


@event('plugin.register')
def register_plugin():
    plugin.register(PluginTransmission, 'transmission', api_ver=2)
    plugin.register(PluginTransmissionInput, 'from_transmission', api_ver=2)
    plugin.register(PluginTransmissionClean, 'clean_transmission', api_ver=2)

########NEW FILE########
__FILENAME__ = plugin_try_regexp
from __future__ import unicode_literals, division, absolute_import, print_function
import logging

from flexget import options, plugin
from flexget.event import event

log = logging.getLogger('try_regexp')


class PluginTryRegexp(object):
    """
        This plugin allows user to test regexps for a task.
    """

    def __init__(self):
        self.abort = False

    def matches(self, entry, regexp):
        """Return True if any of the entry string fields match given regexp"""
        import re
        for field, value in entry.iteritems():
            if not isinstance(value, basestring):
                continue
            if re.search(regexp, value, re.IGNORECASE | re.UNICODE):
                return (True, field)
        return (False, None)

    def on_task_filter(self, task, config):
        if not task.options.try_regexp:
            return
        if self.abort:
            return

        print('-' * 79)
        print('Hi there, welcome to try regexps in realtime!')
        print('Press ^D or type \'exit\' to continue. Type \'continue\' to continue non-interactive execution.')
        print('Task \'%s\' has %s entries, enter regexp to see what matches it.' % (task.name, len(task.entries)))
        while (True):
            try:
                s = raw_input('--> ')
                if s == 'exit':
                    break
                if s == 'abort' or s == 'continue':
                    self.abort = True
                    break
            except EOFError:
                break

            count = 0
            for entry in task.entries:
                try:
                    match, field = self.matches(entry, s)
                    if match:
                        print('Title: %-40s URL: %-30s From: %s' % (entry['title'], entry['url'], field))
                        count += 1
                except:
                    print('Invalid regular expression')
                    break
            print('%s of %s entries matched' % (count, len(task.entries)))
        print('Bye!')


@event('plugin.register')
def register_plugin():
    plugin.register(PluginTryRegexp, '--try-regexp', builtin=True, api_ver=2)


@event('options.register')
def register_parser_arguments():
    options.get_parser('execute').add_argument('--try-regexp', action='store_true', dest='try_regexp', default=False,
                                               help='try regular expressions interactively')

########NEW FILE########
__FILENAME__ = plugin_urlrewriting
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('urlrewriter')


class UrlRewritingError(Exception):

    def __init__(self, value):
        self.value = value

    def __str__(self):
        return repr(self.value)


class PluginUrlRewriting(object):
    """
    Provides URL rewriting framework
    """

    def __init__(self):
        self.disabled_rewriters = []

    def on_task_urlrewrite(self, task, config):
        log.debug('Checking %s entries' % len(task.accepted))
        # try to urlrewrite all accepted
        for entry in task.accepted:
            try:
                self.url_rewrite(task, entry)
            except UrlRewritingError as e:
                log.warn(e.value)
                entry.fail()

    # API method
    def url_rewritable(self, task, entry):
        """Return True if entry is urlrewritable by registered rewriter."""
        for urlrewriter in plugin.get_plugins(group='urlrewriter'):
            if urlrewriter.name in self.disabled_rewriters:
                log.trace('Skipping rewriter %s since it\'s disabled' % urlrewriter.name)
                continue
            log.trace('checking urlrewriter %s' % urlrewriter.name)
            if urlrewriter.instance.url_rewritable(self, entry):
                return True
        return False

    # API method - why priority though?
    @plugin.priority(255)
    def url_rewrite(self, task, entry):
        """Rewrites given entry url. Raises UrlRewritingError if failed."""
        tries = 0
        while self.url_rewritable(task, entry) and entry.accepted:
            tries += 1
            if tries > 20:
                raise UrlRewritingError('URL rewriting was left in infinite loop while rewriting url for %s, '
                                        'some rewriter is returning always True' % entry)
            for urlrewriter in plugin.get_plugins(group='urlrewriter'):
                name = urlrewriter.name
                if name in self.disabled_rewriters:
                    log.trace('Skipping rewriter %s since it\'s disabled' % name)
                    continue
                try:
                    if urlrewriter.instance.url_rewritable(task, entry):
                        log.debug('Url rewriting %s' % entry['url'])
                        urlrewriter.instance.url_rewrite(task, entry)
                        log.info('Entry \'%s\' URL rewritten to %s (with %s)' % (entry['title'], entry['url'], name))
                except UrlRewritingError as r:
                    # increase failcount
                    #count = self.shared_cache.storedefault(entry['url'], 1)
                    #count += 1
                    raise UrlRewritingError('URL rewriting %s failed: %s' % (name, r.value))
                except plugin.PluginError as e:
                    raise UrlRewritingError('URL rewriting %s failed: %s' % (name, e.value))
                except Exception as e:
                    log.exception(e)
                    raise UrlRewritingError('%s: Internal error with url %s' % (name, entry['url']))


class DisableUrlRewriter(object):
    """Disable certain urlrewriters."""

    schema = {'type': 'array', 'items': {'type': 'string'}}

    def on_task_start(self, task, config):
        urlrewrite = plugin.get_plugin_by_name('urlrewriting')['instance']
        for disable in config:
            try:
                plugin.get_plugin_by_name(disable)
            except plugin.DependencyError:
                log.critical('Unknown url-rewriter %s' % disable)
                continue
            log.debug('Disabling url rewriter %s' % disable)
            urlrewrite.disabled_rewriters.append(disable)

    def on_task_exit(self, task, config):
        urlrewrite = plugin.get_plugin_by_name('urlrewriting')['instance']
        for disable in config:
            log.debug('Enabling url rewriter %s' % disable)
            try:
                urlrewrite.disabled_rewriters.remove(disable)
            except ValueError:
                log.debug('%s does not exists' % disable)

    on_task_abort = on_task_exit


@event('plugin.register')
def register_plugin():
    plugin.register(PluginUrlRewriting, 'urlrewriting', builtin=True, api_ver=2)
    plugin.register(DisableUrlRewriter, 'disable_urlrewriters', api_ver=2)

    plugin.register_task_phase('urlrewrite', before='download')

########NEW FILE########
__FILENAME__ = plugin_verbose
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import options, plugin
from flexget.event import event
from flexget.task import log as task_log
from flexget.utils.log import log_once

log = logging.getLogger('verbose')


class Verbose(object):

    """
    Verbose entry accept, reject and failure
    """

    # Run first thing after input phase
    @plugin.priority(255)
    def on_task_metainfo(self, task, config):
        if task.options.silent:
            return
        for entry in task.all_entries:
            entry.on_accept(self.verbose_details, task=task, act='accepted', reason='')
            entry.on_reject(self.verbose_details, task=task, act='rejected', reason='')
            entry.on_fail(self.verbose_details, task=task, act='failed', reason='')

    def verbose_details(self, entry, task=None, act=None, reason=None, **kwargs):
        msg = "%s: `%s` by %s plugin" % (act.upper(), entry['title'], task.current_plugin)
        if reason:
            msg += ' because %s' % reason[0].lower() + reason[1:]

        task_log.verbose(msg)

    def on_task_exit(self, task, config):
        if task.options.silent:
            return
        # verbose undecided entries
        if task.options.verbose:
            undecided = False
            for entry in task.entries:
                if entry in task.accepted:
                    continue
                undecided = True
                log.verbose('UNDECIDED: `%s`' % entry['title'])
            if undecided:
                log_once('Undecided entries have not been accepted or rejected. If you expected these to reach output,'
                         ' you must set up filter plugin(s) to accept them.', logger=log)


@event('plugin.register')
def register_plugin():
    plugin.register(Verbose, 'verbose', builtin=True, api_ver=2)


@event('options.register')
def register_parser_arguments():
    exec_parser = options.get_parser('execute')
    exec_parser.add_argument('-v', '--verbose', action='store_true', dest='verbose', default=False,
                             help='verbose undecided entries')
    exec_parser.add_argument('-s', '--silent', action='store_true', dest='silent', default=False,
                             help='don\'t verbose any actions (accept, reject, fail)')

########NEW FILE########
__FILENAME__ = plugin_verbose_details
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('details')


class PluginDetails(object):

    def on_task_start(self, task, config):
        # Make a flag for tasks to declare if it is ok not to produce entries
        task.no_entries_ok = False

    @plugin.priority(-512)
    def on_task_input(self, task, config):
        if not task.entries:
            if task.no_entries_ok:
                log.verbose('Task didn\'t produce any entries.')
            else:
                log.warning('Task didn\'t produce any entries. This is likely due to a mis-configured or non-functional input.')
        else:
            log.verbose('Produced %s entries.' % (len(task.entries)))

    @plugin.priority(-512)
    def on_task_download(self, task, config):
        # Needs to happen as the first in download, so it runs after urlrewrites
        # and IMDB queue acceptance.
        log.verbose('Summary - Accepted: %s (Rejected: %s Undecided: %s Failed: %s)' %
            (len(task.accepted), len(task.rejected),
            len(task.entries) - len(task.accepted), len(task.failed)))


class NoEntriesOk(object):
    """Allows manually silencing the warning message for tasks that regularly produce no entries."""
    schema = {'type': 'boolean'}

    # Run after details plugin task_start
    @plugin.priority(127)
    def on_task_start(self, task, config):
        task.no_entries_ok = config

@event('plugin.register')
def register_plugin():
    plugin.register(PluginDetails, 'details', builtin=True, api_ver=2)
    plugin.register(NoEntriesOk, 'no_entries_ok', api_ver=2)

########NEW FILE########
__FILENAME__ = search_btn
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import requests, json
from flexget.utils.search import torrent_availability

session = requests.Session()
log = logging.getLogger('search_btn')

# TODO: btn has a limit of 150 searches per hour


class SearchBTN(object):
    schema = {'type': 'string'}

    def search(self, entry, config):
        api_key = config

        searches = entry.get('search_strings', [entry['title']])

        if 'series_name' in entry:
            search = {'series': entry['series_name']}
            if 'series_id' in entry:
                # BTN wants an ep style identifier even for sequence shows
                if entry.get('series_id_type') == 'sequence':
                    search['name'] = 'S01E%02d' % entry['series_id']
                else:
                    search['name'] = entry['series_id']
            searches = [search]

        results = set()
        for search in searches:
            data = json.dumps({'method': 'getTorrents', 'params': [api_key, search], 'id': 1})
            try:
                r = session.post('http://api.btnapps.net/', data=data, headers={'Content-type': 'application/json'})
            except requests.RequestException as e:
                log.error('Error searching btn: %s' % e)
                continue
            content = r.json()
            if not content or not content['result']:
                log.debug('No results from btn')
                continue
            if 'torrents' in content['result']:
                for item in content['result']['torrents'].itervalues():
                    if item['Category'] != 'Episode':
                        continue
                    entry = Entry()
                    entry['title'] = item['ReleaseName']
                    entry['title'] += ' '.join(['', item['Resolution'], item['Source'], item['Codec']])
                    entry['url'] = item['DownloadURL']
                    entry['torrent_seeds'] = int(item['Seeders'])
                    entry['torrent_leeches'] = int(item['Leechers'])
                    entry['torrent_info_hash'] = item['InfoHash']
                    entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])
                    if item['TvdbID']:
                        entry['tvdb_id'] = int(item['TvdbID'])
                    results.add(entry)
        return results


@event('plugin.register')
def register_plugin():
    plugin.register(SearchBTN, 'btn', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = search_kat
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib

import feedparser

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.search import torrent_availability, normalize_unicode

log = logging.getLogger('kat')


class SearchKAT(object):
    """KAT search plugin.

    should accept:
    kat:
      category: <category>
      verified: yes/no

    categories:
      all
      movies
      tv
      music
      books
      xxx
      other
    """

    schema = {
        'type': 'object',
        'properties': {
            'category': {'type': 'string', 'enum': ['all', 'movies', 'tv', 'music', 'books', 'xxx', 'other']},
            'verified': {'type': 'boolean'}
        },
        'additionalProperties': False
    }

    def search(self, entry, config):
        search_strings = [normalize_unicode(s).lower() for s in entry.get('search_strings', [entry['title']])]
        entries = set()
        for search_string in search_strings:
            search_string_url_fragment = search_string

            if config.get('verified'):
                search_string_url_fragment += ' verified:1'
            url = 'http://kickass.to/search/%s/?rss=1' % urllib.quote(search_string_url_fragment.encode('utf-8'))
            if config.get('category', 'all') != 'all':
                url += '&category=%s' % config['category']

            sorters = [{'field': 'time_add', 'sorder': 'desc'},
                       {'field': 'seeders', 'sorder': 'desc'}]
            for sort in sorters:
                url += '&field=%(field)s&sorder=%(sorder)s' % sort

                log.debug('requesting: %s' % url)
                rss = feedparser.parse(url)

                status = rss.get('status', False)
                if status == 404:
                    # Kat returns status code 404 when no results found for some reason...
                    log.debug('No results found for search query: %s' % search_string)
                    continue
                elif status != 200:
                    raise plugin.PluginWarning('Search result not 200 (OK), received %s' % status)

                ex = rss.get('bozo_exception', False)
                if ex:
                    raise plugin.PluginWarning('Got bozo_exception (bad feed)')

                for item in rss.entries:
                    entry = Entry()
                    entry['title'] = item.title

                    if not item.get('enclosures'):
                        log.warning('Could not get url for entry from KAT. Maybe plugin needs updated?')
                        continue
                    entry['url'] = item.enclosures[0]['url']
                    entry['torrent_seeds'] = int(item.torrent_seeds)
                    entry['torrent_leeches'] = int(item.torrent_peers)
                    entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])
                    entry['content_size'] = int(item.torrent_contentlength) / 1024 / 1024
                    entry['torrent_info_hash'] = item.torrent_infohash

                    entries.add(entry)

                if len(rss.entries) < 25:
                    break

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(SearchKAT, 'kat', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = search_newznab
__author__ = 'deksan'

import logging
import urllib

import feedparser

from flexget import plugin, validator
from flexget.entry import Entry
from flexget.event import event
from flexget.plugins.api_tvrage import lookup_series

log = logging.getLogger('newznab')


class Newznab(object):
    """
    Newznab search plugin
    Provide a url or your website + apikey and a category

    Config example::

        newznab:
          url: "http://website/api?apikey=xxxxxxxxxxxxxxxxxxxxxxxxxx&t=movie&extended=1"
          website: https://website
          apikey: xxxxxxxxxxxxxxxxxxxxxxxxxx
          category: movie

    Category is any of: movie, tvsearch, music, book
    """

    schema = {
        'type': 'object',
        'properties': {
            'category': {'type': 'string', 'enum': ['movie', 'tvsearch', 'tv', 'music', 'book']},
            'url': {'type': 'string', 'format': 'url'},
            'website': {'type': 'string', 'format': 'url'},
            'apikey': {'type': 'string'}
        },
        'required': ['category'],
        'additionalProperties': False
    }

    def build_config(self, config):
        if config['category'] == 'tv':
            config['category'] = 'tvsearch'
        log.debug(config['category'])
        if 'url' not in config:
            if 'apikey' in config and 'website' in config:
                params = {
                    't': config['category'],
                    'apikey': config['apikey'],
                    'extended': 1
                }
                config['url'] = config['website']+'/api?'+urllib.urlencode(params)
        return config

    def fill_entries_for_url(self, url, config):
        entries = []
        rss = feedparser.parse(url)
        status = rss.get('status', False)
        if status != 200 and status != 301:     # in cae of redirection...
            log.error('Search result not 200 (OK), received %s' % status)
            raise

        if not len(rss.entries):
            log.info('No results returned')

        for rss_entry in rss.entries:
            new_entry = Entry()
            for key in rss_entry.keys():
                new_entry[key] = rss_entry[key]
            new_entry['url'] = new_entry['link']
            entries.append(new_entry)
        return entries

    def search(self, entry, config=None):
        config = self.build_config(config)
        if config['category'] == 'movie':
            return self.do_search_movie(entry, config)
        elif config['category'] == 'tvsearch':
            return self.do_search_tvsearch(entry, config)
        else:
            entries = []
            log.warning("Not done yet...")
            return entries

    def do_search_tvsearch(self, arg_entry, config=None):
        log.info('Searching for %s' % (arg_entry['title']))
        # normally this should be used with emit_series who has provided season and episodenumber
        if 'series_name' not in arg_entry or 'series_season' not in arg_entry or 'series_episode' not in arg_entry:
            return []
        serie_info = lookup_series(arg_entry['series_name'])
        if not serie_info:
            return []

        url = (config['url'] + '&rid=%s&season=%s&ep=%s' %
               (serie_info.showid, arg_entry['series_season'], arg_entry['series_episode']))
        return self.fill_entries_for_url(url, config)

    def do_search_movie(self, arg_entry, config=None):
        entries = []
        log.info('Searching for %s (imdbid:%s)' % (arg_entry['title'], arg_entry['imdb_id']))
        # normally this should be used with emit_movie_queue who has imdbid (i guess)
        if 'imdb_id' not in arg_entry:
            return entries

        imdb_id = arg_entry['imdb_id'].replace('tt', '')
        url = config['url'] + '&imdbid=' + imdb_id
        return self.fill_entries_for_url(url, config)


@event('plugin.register')
def register_plugin():
    plugin.register(Newznab, 'newznab', api_ver=2, groups=['search'])

########NEW FILE########
__FILENAME__ = search_ptn
from __future__ import unicode_literals, division, absolute_import
import logging

from requests.auth import AuthBase

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import requests
from flexget.utils.imdb import extract_id
from flexget.utils.soup import get_soup
from flexget.utils.search import torrent_availability

log = logging.getLogger('search_ptn')


class CookieAuth(AuthBase):
    def __init__(self, cookies):
        self.cookies = cookies

    def __call__(self, r):
        r.prepare_cookies(self.cookies)
        return r


categories = {
    '1080p': 'c5',
    '720p': 'c6',
    'bdrip': 'c10',
    'bluray': 'c1',
    'brrip': 'c11',
    'dvdr': 'c4',
    'dvdrip': 'c12',
    'mp4': 'c16',
    'ost/flac': 'c17',
    'ost/mp3': 'c18',
    'packs': 'c20',
    'r5/scr': 'c13',
    'remux': 'c2',
    'tvrip': 'c15',
    'webrip': 'c14'
}


class SearchPTN(object):
    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'login_key': {'type': 'string'},
            'password': {'type': 'string'},
            'categories': {
                'type': 'array',
                'items': {'type': 'string', 'enum': list(categories)}
            }
        },
        'required': ['username', 'login_key', 'password'],
        'additionalProperties': False
    }

    def search(self, entry, config):
        login_sess = requests.Session()
        login_params = {'username': config['username'],
                        'password': config['password'],
                        'loginkey': config['login_key']}
        try:
            login_sess.post('https://piratethenet.org/takelogin.php', data=login_params, verify=False)
        except requests.RequestException as e:
            log.error('Error while logging in to PtN: %s', e)

        download_auth = CookieAuth(login_sess.cookies)
        # Default to searching by title (0=title 3=imdb_id)
        search_by = 0
        if 'imdb_id' in entry:
            searches = [entry['imdb_id']]
            search_by = 3
        elif 'movie_name' in entry:
            search = entry['movie_name']
            if 'movie_year' in entry:
                search += ' %s' % entry['movie_year']
            searches = [search]
        else:
            searches = entry.get('search_strings', [entry['title']])

        params = {'_by': search_by}
        if config.get('categories'):
            for cat in config['categories']:
                params[categories[cat]] = 1
        results = set()
        for search in searches:
            params['search'] = search
            try:
                r = login_sess.get('http://piratethenet.org/browse.php', params=params)
            except requests.RequestException as e:
                log.error('Error searching ptn: %s' % e)
                continue
            soup = get_soup(r.text)
            if 'login' in soup.head.title.text.lower():
                log.error('PtN cookie info invalid')
                raise plugin.PluginError('PTN cookie info invalid')
            try:
                results_table = soup.find_all('table', attrs={'class': 'main'}, limit=2)[1]
            except IndexError:
                log.debug('no results found for `%s`' % search)
                continue
            for row in results_table.find_all('tr')[1:]:
                columns = row.find_all('td')
                entry = Entry()
                links = columns[1].find_all('a', recursive=False, limit=2)
                entry['title'] = links[0].text
                if len(links) > 1:
                    entry['imdb_id'] = extract_id(links[1].get('href'))
                entry['url'] = 'http://piratethenet.org/' + columns[2].a.get('href')
                entry['download_auth'] = download_auth
                entry['torrent_seeds'] = int(columns[8].text)
                entry['torrent_leeches'] = int(columns[9].text)
                entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])
                size = columns[6].find('br').previous_sibling
                unit = columns[6].find('br').next_sibling
                if unit == 'GB':
                    entry['content_size'] = int(float(size) * 1024)
                elif unit == 'MB':
                    entry['content_size'] = int(float(size))
                elif unit == 'KB':
                    entry['content_size'] = int(float(size) / 1024)
                results.add(entry)
        return results


@event('plugin.register')
def register_plugin():
    plugin.register(SearchPTN, 'ptn', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = search_publichd
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib
import re

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.config_schema import one_or_more
from flexget.utils import requests
from flexget.utils.soup import get_soup
from flexget.utils.search import torrent_availability, normalize_unicode

log = logging.getLogger('publichd')

CATEGORIES = {
    'all': 0,

    # Movies
    'BluRay 720p': 2,
    'BluRay 1080p': 5,
    'XviD': 15,
    'BRRip': 16,

    #TV
    'HDTV': 7,
    'SDTV': 24,
    'TV WEB-DL': 14
}

class SearchPublicHD(object):
    """
        PublicHD search plugin.

        To perform search against single category:

        publichd:
            category: BluRay 720p

        To perform search against multiple categories:

        publichd:
            category:
                - BluRay 720p
                - BluRay 1080p

        Movie categories accepted: BluRay 720p, BluRay 1080p, XviD, BRRip
        TV categories accepted: HDTV, SDTV, TV WEB-DL

        You can use also use category ID manually if you so desire (eg. BluRay 720p is actually category id '2')
    """

    schema = {
        'type': 'object',
        'properties': {
            'category': one_or_more({
                'oneOf': [
                    {'type': 'integer'},
                    {'type': 'string', 'enum': list(CATEGORIES)},
                ]})
        },
        "additionalProperties": False
    }

    @plugin.internet(log)
    def search(self, entry, config=None):
        """
            Search for entries on PublicHD
        """

        categories = config.get('category', 'all')
        # Ensure categories a list
        if not isinstance(categories, list):
            categories = [categories]
        # Convert named category to its respective category id number
        categories = [c if isinstance(c, int) else CATEGORIES[c] for c in categories]
        category_url_fragment = '&category=%s' % urllib.quote(';'.join(str(c) for c in categories))

        base_url = 'http://publichd.se/index.php?page=torrents&active=0'

        entries = set()
        for search_string in entry.get('search_strings', [entry['title']]):
            query = normalize_unicode(search_string)
            query_url_fragment = '&search=' + urllib.quote(query.encode('utf8'))

            # http://publichd.se/index.php?page=torrents&active=0&category=5;15&search=QUERY
            url = (base_url + category_url_fragment + query_url_fragment)
            log.debug('PublicHD search url: %s' % url)

            page = requests.get(url).content
            soup = get_soup(page)

            for result in soup.findAll('a', href=re.compile('page=torrent-details')):
                entry = Entry()
                entry['title'] = result.text
                # Expand the selection to whole row
                result = result.findPrevious('tr')
                download_url = result.find('a', href=re.compile('\.torrent$'))['href']
                torrent_hash = re.search(r'/([0-9a-fA-F]{5,40})/', download_url).group(1)

                entry['url'] = 'http://publichd.se/download.php?id=%s' % torrent_hash

                seeds, leeches = result.findAll('td', text=re.compile('^\d+$'))
                entry['torrent_seeds'] = int(seeds.text)
                entry['torrent_leeches'] = int(leeches.text)
                entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])
                size = result.find("td", text=re.compile('(\d+(?:[.,]\d+)*)\s?([KMG]B)')).text
                size = re.search('(\d+(?:[.,]\d+)*)\s?([KMG]B)', size)

                if size:
                    if size.group(2) == 'GB':
                        entry['content_size'] = int(float(size.group(1).replace(',', '')) * 1000 ** 3 / 1024 ** 2)
                    elif size.group(2) == 'MB':
                        entry['content_size'] = int(float(size.group(1).replace(',', '')) * 1000 ** 2 / 1024 ** 2)
                    elif size.group(2) == 'KB':
                        entry['content_size'] = int(float(size.group(1).replace(',', '')) * 1000 / 1024 ** 2)
                    else:
                        entry['content_size'] = int(float(size.group(1).replace(',', '')) / 1024 ** 2)

                entries.add(entry)

        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(SearchPublicHD, 'publichd', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = search_rss
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib

from flexget import plugin
from flexget.event import event
from flexget.task import Task
from flexget.utils.search import normalize_unicode

log = logging.getLogger('search_rss')


class SearchRSS(object):
    """A generic search plugin that can use rss based search feeds. Configure it like rss
    plugin, but include {{{search_term}}} in the url where the search term should go."""

    schema = {'$ref': '/schema/plugin/rss'}

    def search(self, entry, config=None):
        from flexget.utils.template import environment
        from flexget.manager import manager
        search_strings = [urllib.quote(normalize_unicode(s).encode('utf-8'))
                          for s in entry.get('search_strings', [entry['title']])]
        rss_plugin = plugin.get_plugin_by_name('rss')
        entries = set()
        for search_string in search_strings:
            # Create a fake task to pass to the rss plugin input handler
            task = Task(manager, 'search_rss_task', config={})
            # Use a copy of the config, so we don't overwrite jinja url when filling in search term
            config = rss_plugin.instance.build_config(config).copy()
            template = environment.from_string(config['url'])
            config['url'] = template.render({'search_term': search_string})
            config['all_entries'] = True
            # TODO: capture some other_fields to try to find seed/peer/content_size numbers?
            entries.update(rss_plugin.phase_handlers['input'](task, config))
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(SearchRSS, 'search_rss', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = search_sceneaccess
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from urllib import quote

from flexget import plugin
from flexget import validator
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.soup import get_soup
from flexget.utils.search import torrent_availability, normalize_unicode, clean_title
from flexget.utils.requests import Session

log = logging.getLogger('search_sceneaccess')

CATEGORIES = {
    'browse':
        {
            'Movies/DVD-R': 8,
            'Movies/x264': 22,
            'Movies/XviD': 7,

            'TV/HD-x264': 27,
            'TV/SD-x264': 17,
            'TV/XviD': 11,

            'Games/PC': 3,
            'Games/PS3': 5,
            'Games/PSP': 20,
            'Games/WII': 28,
            'Games/XBOX360': 23,

            'APPS/ISO': 1,
            'DOX': 14,
            'MISC': 21
        },
    'nonscene':
        {
            'Movies/HD-x264': 41,
            'Movies/SD-x264': 42,
            'Movies/XviD': 43,
            'TV/HD': 44,
            'TV/SD': 45
        },
    'mp3/0day':
        {
            '0DAY/APPS': 2,
            'FLAC': 40,
            'MP3': 13,
            'MVID': 15,
        },
    'archive':
        {
            'Movies/Packs': 4,
            'TV/Packs': 26,
            'Games/Packs': 29,
            'XXX/Packs': 37,
            'Music/Packs': 38
        },
    'foreign':
        {
            'Movies/DVD-R': 31,
            'Movies/x264': 32,
            'Movies/XviD': 30,
            'TV/x264': 34,
            'TV/XviD': 33,
        },
    'xxx':
        {
            'XXX/XviD': 12,
            'XXX/x264': 35,
            'XXX/0DAY': 36
        }
}

URL = 'https://sceneaccess.eu/'

class SceneAccessSearch(object):
    """ Scene Access Search plugin

    == Basic usage:

    sceneaccess:
        username: XXXX              (required)
        password: XXXX              (required)
        category: Movies/x264       (optional)
        gravity_multiplier: 200     (optional)

    == Categories:
    +---------------+----------------+-----------+--------------+--------------+----------+
    |    browse     |    nonscene    | mp3/0day  |   archive    |   foreign    |   xxx    |
    +---------------+----------------+-----------+--------------+--------------+----------+
    | APPS/ISO      | Movies/HD-x264 | 0DAY/APPS | Games/Packs  | Movies/DVD-R | XXX/0DAY |
    | DOX           | Movies/SD-x264 | FLAC      | Movies/Packs | Movies/x264  | XXX/x264 |
    | Games/PC      | Movies/XviD    | MP3       | Music/Packs  | Movies/XviD  | XXX/XviD |
    | Games/PS3     | TV/HD          | MVID      | TV/Packs     | TV/x264      |          |
    | Games/PSP     | TV/SD          |           | XXX/Packs    | TV/XviD      |          |
    | Games/WII     |                |           |              |              |          |
    | Games/XBOX360 |                |           |              |              |          |
    | MISC          |                |           |              |              |          |
    | Movies/DVD-R  |                |           |              |              |          |
    | Movies/x264   |                |           |              |              |          |
    | Movies/XviD   |                |           |              |              |          |
    | TV/HD-x264    |                |           |              |              |          |
    | TV/SD-x264    |                |           |              |              |          |
    | TV/XviD       |                |           |              |              |          |
    +---------------+----------------+-----------+--------------+--------------+----------+

    You can combine the categories almost any way you want, here are some examples:

    category:
      archive: yes          => Will search all categories within archive section

    category: Movies/x264   => Search Movies/x264 within 'browse' section (browse is always default if unspecified)

    category:
      browse:
        - 22  => This is custom category ID
        - Movies/XviD
      foreign:
        - Movies/x264
        - Movies/XviD

    Specifying specific category ID is also possible, you can extract ID from URL, for example
    if you hover or click on category on the site you'll see similar address:

    http://sceneaccess.URL/browse?cat=22

    In this example, according to this bit ?cat=22 , category id is 22.

    == Priority

    gravity_multiplier is optional parameter that increases odds of downloading found matches from sceneaccess
    instead of other search providers, that may have higer odds due to their higher number of peers.
    Although sceneaccess does not have many peers as some public trackers, the torrents are usually faster.
    By default, Flexget give higher priority to found matches according to following formula:

    gravity = number of seeds * 2 + number of leechers

    gravity_multiplier will multiply the above number by specified amount.
    If you use public trackers for searches, you may want to use this feature.
    """

    def validator(self):
        """Return config validator."""
        root = validator.factory('dict')
        root.accept('text', key='username', required=True)
        root.accept('text', key='password', required=True)
        root.accept('number', key='gravity_multiplier')

        # Scope as in pages like `browse`, `mp3/0day`, `foreign`, etc.
        # Will only accept categories from `browse` which will it default to, unless user specifies other scopes
        # via dict
        root.accept('choice', key='category').accept_choices(CATEGORIES['browse'])
        root.accept('number', key='category')
        categories = root.accept('dict', key='category')

        category_list = root.accept('list', key='category')
        category_list.accept('choice').accept_choices(CATEGORIES['browse'])

        for category in CATEGORIES:
            categories.accept('choice', key=category).accept_choices(CATEGORIES[category])
            categories.accept('boolean', key=category)
            categories.accept('number', key=category)
            category_list = categories.accept('list', key=category)
            category_list.accept('choice', key=category).accept_choices(CATEGORIES[category])
            category_list.accept('number', key=category)
        return root

    def processCategories(self, config):
        toProcess = dict()

        # Build request urls from config
        try:
            scope = 'browse' # Default scope to search in
            category = config['category']
            if isinstance(category, dict):                          # Categories have search scope specified.
                for scope in category:
                    if isinstance(category[scope], bool):           # If provided boolean, search all categories
                        category[scope] = []
                    elif not isinstance(category[scope], list):     # Convert single category into list
                        category[scope] = [category[scope]]
                    toProcess[scope] = category[scope]
            else:                       # Single category specified, will default to `browse` scope.
                category = [category]
                toProcess[scope] = category

        except KeyError:    # Category was not set, will default to `browse` scope and all categories.
            toProcess[scope] = []

        finally:    # Process the categories to be actually in usable format for search() method
            ret = list()

            for scope, categories in toProcess.iteritems():
                cat_id = list()

                for category in categories:
                    try:
                        id = CATEGORIES[scope][category]
                    except KeyError:            # User provided category id directly
                        id = category
                    finally:
                        if isinstance(id, list):      #
                            [cat_id.append(l) for l in id]
                        else:
                            cat_id.append(id)

                if scope == 'mp3/0day':     # mp3/0day is actually /spam?search= in URL, can safely change it now
                    scope = 'spam'

                category_url_string = ''.join(['&c' + str(id) + '=' + str(id) for id in cat_id])  # &c<id>=<id>&...
                ret.append({'url_path': scope, 'category_url_string': category_url_string})
            return ret

    @plugin.internet(log)
    def search(self, entry, config=None):
        """
            Search for entries on SceneAccess
        """

        try:
            multip = int(config['gravity_multiplier'])
        except KeyError:
            multip = 1

        # Login...
        params = {'username': config['username'],
                  'password': config['password'],
                  'submit': 'come on in'}

        session = Session()
        session.headers = {'User agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:27.0) Gecko/20100101 Firefox/27.0'}
        log.debug('Logging in to %s...' % URL)
        session.post(URL + 'login', data=params)

        # Prepare queries...
        BASE_URLS = list()
        entries = set()
        for category in self.processCategories(config):
            BASE_URLS.append(URL + '%(url_path)s?method=2%(category_url_string)s' % category)

        # Search...
        for search_string in entry.get('search_strings', [entry['title']]):
            search_string_normalized = normalize_unicode(clean_title(search_string))
            search_string_url_fragment = '&search=' + quote(search_string_normalized.encode('utf8'))

            for url in BASE_URLS:
                url += search_string_url_fragment
                log.debug('Search URL for `%s`: %s' % (search_string, url))

                page = session.get(url).content
                soup = get_soup(page)

                for result in soup.findAll('tr', attrs={'class': 'tt_row'}):
                    entry = Entry()
                    entry['title'] = result.find('a', href=re.compile(r'details\?id=\d+'))['title']
                    entry['url'] = URL + result.find('a', href=re.compile(r'.torrent$'))['href']

                    entry['torrent_seeds'] = result.find('td', attrs={'class': 'ttr_seeders'}).string
                    entry['torrent_leeches'] = result.find('td', attrs={'class': 'ttr_leechers'}).string
                    entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])*multip

                    size = result.find('td', attrs={'class': 'ttr_size'}).next
                    size = re.search('(\d+(?:[.,]\d+)*)\s?([KMG]B)', size)

                    if size:
                        if size.group(2) == 'GB':
                            entry['content_size'] = int(float(size.group(1)) * 1000 ** 3 / 1024 ** 2)
                        elif size.group(2) == 'MB':
                            entry['content_size'] = int(float(size.group(1)) * 1000 ** 2 / 1024 ** 2)
                        elif size.group(2) == 'KB':
                            entry['content_size'] = int(float(size.group(1)) * 1000 / 1024 ** 2)
                        else:
                            entry['content_size'] = int(float(size.group(1)) / 1024 ** 2)

                    entries.add(entry)

        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(SceneAccessSearch, 'sceneaccess', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = search_torrentshack
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from urllib import quote

from flexget import plugin
from flexget import validator
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.soup import get_soup
from flexget.utils.search import torrent_availability, normalize_unicode, clean_title
from flexget.utils.requests import Session

log = logging.getLogger('search_torrentshack')

CATEGORIES = {
    'Apps/PC': 100,
    'Apps/misc': 150,
    'eBooks': 180,
    'Games/PC': 200,
    'Games/PS3': 240,
    'Games/Xbox360': 260 ,
    'HandHeld': 280,
    'Movies/x264': 300,
    'REMUX': 320,
    'Movies/DVD-R': 350,
    'Movies/XviD': 400,
    'Music/MP3': 450,
    'Music/FLAC': 480,
    'Music/Videos': 500,
    'TV/x264-HD': 600,
    'TV/x264-SD': 620,
    'TV/DVDrip': 700,
    'Misc': 800,
    'Anime': 850,
    'Foreign': 960,
    'Full Blu-ray': 970,
    'TV-SD Pack': 980,
    'TV-HD Pack': 981,
    'Movies-HD Pack': 982,
    'Movies-SD Pack': 983,
    'MP3 Pack': 984,
    'FLAC Pack': 985,
    'Games Pack': 986
}

URL = 'http://torrentshack.net/'

class TorrentShackSearch(object):
    """ TorrentShack Search plugin

    == Basic usage:

    torrentshack:
        username: XXXX              (required)
        password: XXXX              (required)
        category: Movies/x264       (optional)
        gravity_multiplier: 200     (optional)

    == Categories
    +---------------+--------------+--------------+----------------+
    | Apps/PC       | Movies/x264  | TV/x264-HD   | TV-SD Pack     |
    | Apps/misc     | REMUX        | TV/x264-SD   | TV-HD Pack     |
    | eBooks        | Movies/DVD-R | TV/DVDrip    | Movies-HD Pack |
    | Games/PC      | Movies/XviD  | Misc         | Movies-SD Pack |
    | Games/PS3     | Music/MP3    | Anime        | MP3 Pack       |
    | Games/Xbox360 | Music/FLAC   | Foreign      | FLAC Pack      |
    | HandHeld      | Music/Videos | Full Blu-ray | Games Pack     |
    +---------------+--------------+--------------+----------------+

    You can specify either a single category or list of categories, example:

    category: Movies/x264

    or

    category:
        - Movies/XviD
        - Movies/x264

    Specifying specific category ID is also possible. You can extract ID from URL - for example
    if you hover or click on category on the site you'll see similar address:

    http://torrentshack.URL/torrents.php?filter_cat[300]=1

    In this particular example, category id is 300.

    == Priority

    gravity_multiplier is optional parameter that increases odds of downloading found matches from torrentshack
    instead of other search providers, that may have higer odds due to their higher number of peers.
    Although torrentshack  does not have many peers as some public trackers, the torrents are usually faster.
    By default, Flexget give higher priority to found matches according to following formula:

    gravity = number of seeds * 2 + number of leechers

    gravity_multiplier will multiply the above number by specified amount.
    If you use public trackers for searches, you may want to use this feature.
    """

    def validator(self):
        """Return config validator."""
        root = validator.factory('dict')
        root.accept('text', key='username', required=True)
        root.accept('text', key='password', required=True)
        root.accept('number', key='gravity_multiplier')

        root.accept('choice', key='category').accept_choices(CATEGORIES)
        root.accept('number', key='category')
        categories = root.accept('list', key='category')
        categories.accept('choice', key='category').accept_choices(CATEGORIES)
        categories.accept('number', key='category')

        return root

    @plugin.internet(log)
    def search(self, entry, config=None):

        try:
            multip = int(config['gravity_multiplier'])
        except KeyError:
            multip = 1

        if not isinstance(config['category'], list):
            config['category'] = [config['category']]

        categories_id = list()
        for category in config['category']:
            if not isinstance(category, int):
                categories_id.append(CATEGORIES.get(category))
            else:
                categories_id.append(category)
        category_url_fragment = ''.join(
            ['&' + quote('filter_cat[%s]' % id) + '=1' for id in categories_id])

        params = {
            'username': config['username'],
            'password': config['password'],
            'keeplogged': '1',
            'login': 'Login'
        }

        session = Session()
        log.debug('Logging in to %s...' % URL)
        session.post(URL + 'login.php', data=params)

        entries = set()
        for search_string in entry.get('search_strings', [entry['title']]):
            search_string_normalized = normalize_unicode(clean_title(search_string))
            search_string_url_fragment = 'searchstr=' + quote(search_string_normalized.encode('utf8'))

            url = URL + 'torrents.php?' + search_string_url_fragment + category_url_fragment
            log.debug('Fetching URL for `%s`: %s' % (search_string, url))

            page = session.get(url).content
            soup = get_soup(page)

            for result in soup.findAll('tr', attrs={'class': 'torrent'}):
                entry = Entry()
                entry['title'] = result.find('span', attrs={'class': 'torrent_name_link'}).string
                entry['url'] = URL + result.find('a',
                                                 href=re.compile(r'torrents.php\?action=download'),
                                                 attrs={'title': 'Download'})['href']
                entry['torrent_seeds'] = result.findAll('td')[-3].string
                entry['torrent_leeches'] = result.findAll('td')[-2].string
                entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches']) * multip

                size = result.findAll('td')[-5].string
                size = re.search('(\d+(?:[.,]\d+)*)\s?([KMG]B)', size)

                if size:
                        if size.group(2) == 'GB':
                            entry['content_size'] = int(float(size.group(1).replace(',', '')) * 1000 ** 3 / 1024 ** 2)
                        elif size.group(2) == 'MB':
                            entry['content_size'] = int(float(size.group(1).replace(',', '')) * 1000 ** 2 / 1024 ** 2)
                        elif size.group(2) == 'KB':
                            entry['content_size'] = int(float(size.group(1).replace(',', '')) * 1000 / 1024 ** 2)
                        else:
                            entry['content_size'] = int(float(size.group(1).replace(',', '')) / 1024 ** 2)

                entries.add(entry)
        return entries

@event('plugin.register')
def register_plugin():
    plugin.register(TorrentShackSearch, 'torrentshack', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = myepisodes
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib
import urllib2
import re
import cookielib
from datetime import datetime

from sqlalchemy import Column, Integer, String, DateTime

from flexget import db_schema, plugin
from flexget.event import event

try:
    from flexget.plugins.api_tvdb import lookup_series
except ImportError:
    raise plugin.DependencyError(issued_by='myepisodes', missing='api_tvdb',
                                 message='myepisodes requires the `api_tvdb` plugin')


log = logging.getLogger('myepisodes')
Base = db_schema.versioned_base('myepisodes', 0)


class MyEpisodesInfo(Base):
    __tablename__ = 'myepisodes'

    id = Column(Integer, primary_key=True)
    series_name = Column(String, unique=True)
    myepisodes_id = Column(Integer, unique=True)
    updated = Column(DateTime)

    def __init__(self, series_name, myepisodes_id):
        self.series_name = series_name
        self.myepisodes_id = myepisodes_id
        self.updated = datetime.now()

    def __repr__(self):
        return '<MyEpisodesInfo(series_name=%s, myepisodes_id=%s)>' % (self.series_name, self.myepisodes_id)


class MyEpisodes(object):
    """
    Marks a series episode as acquired in your myepisodes.com account.

    Simple Example:

    Most shows are recognized automatically from their TVDBname.
    And of course the plugin needs to know your MyEpisodes.com account details.

    tasks:
      tvshows:
        myepisodes:
          username: <username>
          password: <password>
        series:
         - human target
         - chuck

    Advanced Example:

    In some cases, the TVDB name is either not unique or won't even be discovered.
    In that case you need to specify the MyEpisodes id manually using the set plugin.

    tasks:
      tvshows:
        myepisodes:
          username: <username>
          password: <password>
        series:
         - human target:
             set:
               myepisodes_id: 5111
         - chuck

    How to find the MyEpisodes id: http://matrixagents.org/screencasts/myep_example-20110507-131555.png
    """

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'}
        },
        'required': ['username', 'password'],
        'additionalProperties': False
    }

    @plugin.priority(-255)
    def on_task_output(self, task, config):
        """Mark all accepted episodes as acquired on MyEpisodes"""
        if not task.accepted:
            # Nothing accepted, don't do anything
            return

        username = config['username']
        password = config['password']

        cookiejar = cookielib.CookieJar()
        opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookiejar))
        baseurl = urllib2.Request('http://www.myepisodes.com/login.php?')
        loginparams = urllib.urlencode({'username': username,
                                        'password': password,
                                        'action': 'Login'})
        try:
            logincon = opener.open(baseurl, loginparams)
            loginsrc = logincon.read()
        except urllib2.URLError as e:
            log.error('Error logging in to myepisodes: %s' % e)
            return

        if str(username) not in loginsrc:
            raise plugin.PluginWarning(('Login to myepisodes.com failed, please check '
                                 'your account data or see if the site is down.'), log)

        for entry in task.accepted:
            try:
                self.mark_episode(task, entry, opener)
            except plugin.PluginWarning as w:
                log.warning(str(w))

    def lookup_myepisodes_id(self, entry, opener, session):
        """Populates myepisodes_id field for an entry, and returns the id.

        Call will also set entry field `myepisode_id` if successful.

        Return:
            myepisode id

        Raises:
            LookupError if entry does not have field series_name
        """

        # Don't need to look it up if we already have it.
        if entry.get('myepisodes_id'):
            return entry['myepisodes_id']

        if not entry.get('series_name'):
            raise LookupError('Cannot lookup myepisodes id for entries without series_name')
        series_name = entry['series_name']

        # First check if we already have a myepisodes id stored for this series
        myepisodes_info = session.query(MyEpisodesInfo).\
            filter(MyEpisodesInfo.series_name == series_name.lower()).first()
        if myepisodes_info:
            entry['myepisodes_id'] = myepisodes_info.myepisodes_id
            return myepisodes_info.myepisodes_id

        # Get the series name from thetvdb to increase match chance on myepisodes
        if entry.get('tvdb_series_name'):
            query_name = entry['tvdb_series_name']
        else:
            try:
                series = lookup_series(name=series_name, tvdb_id=entry.get('tvdb_id'))
                query_name = series.seriesname
            except LookupError as e:
                log.warning('Unable to lookup series `%s` from tvdb, using raw name.' % series_name)
                query_name = series_name

        baseurl = urllib2.Request('http://www.myepisodes.com/search.php?')
        params = urllib.urlencode({'tvshow': query_name, 'action': 'Search myepisodes.com'})
        try:
            con = opener.open(baseurl, params)
            txt = con.read()
        except urllib2.URLError as e:
            log.error('Error searching for myepisodes id: %s' % e)

        matchObj = re.search(r'&showid=([0-9]*)">' + query_name + '</a>', txt, re.MULTILINE | re.IGNORECASE)
        if matchObj:
            myepisodes_id = matchObj.group(1)
            db_item = session.query(MyEpisodesInfo).filter(MyEpisodesInfo.myepisodes_id == myepisodes_id).first()
            if db_item:
                log.info('Changing name to `%s` for series with myepisodes_id %s' %
                    (series_name.lower(), myepisodes_id))
                db_item.series_name = series_name.lower()
            else:
                session.add(MyEpisodesInfo(series_name.lower(), myepisodes_id))
            entry['myepisodes_id'] = myepisodes_id
            return myepisodes_id

    def mark_episode(self, task, entry, opener):
        """Mark episode as acquired.

        Required entry fields:
            - series_name
            - series_season
            - series_episode

        Raises:
            PluginWarning if operation fails
        """

        if 'series_season' not in entry or 'series_episode' not in entry or 'series_name' not in entry:
            raise plugin.PluginWarning(
                'Can\'t mark entry `%s` in myepisodes without series_season, series_episode and series_name fields' %
                entry['title'], log)

        if not self.lookup_myepisodes_id(entry, opener, session=task.session):
            raise plugin.PluginWarning('Couldn\'t get myepisodes id for `%s`' % entry['title'], log)

        myepisodes_id = entry['myepisodes_id']
        season = entry['series_season']
        episode = entry['series_episode']

        if task.options.test:
            log.info('Would mark %s of `%s` as acquired.' % (entry['series_id'], entry['series_name']))
        else:
            baseurl2 = urllib2.Request(
                'http://www.myepisodes.com/myshows.php?action=Update&showid=%s&season=%s&episode=%s&seen=0' %
                (myepisodes_id, season, episode))
            opener.open(baseurl2)
            log.info('Marked %s of `%s` as acquired.' % (entry['series_id'], entry['series_name']))


@event('plugin.register')
def register_plugin():
    plugin.register(MyEpisodes, 'myepisodes', api_ver=2)

########NEW FILE########
__FILENAME__ = pogcal_acquired
from __future__ import unicode_literals, division, absolute_import
import logging
import re
from datetime import datetime

from sqlalchemy import Column, Unicode, Integer

from flexget import plugin
from flexget.event import event
from flexget.utils import requests
from flexget.utils.soup import get_soup
from flexget.utils.titles.series import SeriesParser
from flexget.db_schema import versioned_base

log = logging.getLogger('pogcal_acquired')
Base = versioned_base('pogcal_acquired', 0)
session = requests.Session(max_retries=3)
series_parser = SeriesParser()


class PogcalShow(Base):
    __tablename__ = 'pogcal_shows'
    id = Column(Integer, primary_key=True, autoincrement=False, nullable=False)
    name = Column(Unicode)


class PogcalAcquired(object):

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'}
        },
        'required': ['username', 'password'],
        'additionalProperties': False
    }

    @plugin.priority(-255)
    def on_task_output(self, task, config):
        if not task.accepted and not task.options.test:
            return
        try:
            result = session.post('http://www.pogdesign.co.uk/cat/',
                                  data={'username': config['username'],
                                        'password': config['password'],
                                        'sub_login': 'Account Login'})
        except requests.RequestException as e:
            log.error('Error logging in to pog calendar: %s' % e)
            return
        if 'logout' not in result.text:
            log.error('Username/password for pogdesign calendar appear to be incorrect.')
            return
        elif task.options.test:
            log.verbose('Successfully logged in to pogdesign calendar.')
        for entry in task.accepted:
            if not entry.get('series_name') or not entry.get('series_id_type') == 'ep':
                continue
            show_id = self.find_show_id(entry['series_name'], task.session)
            if not show_id:
                log.debug('Could not find pogdesign calendar id for `%s`' % entry['series_name'])
                continue
            if task.options.test:
                log.verbose('Would mark %s %s in pogdesign calenadar.' % (entry['series_name'], entry['series_id']))
                continue
            else:
                log.verbose('Marking %s %s in pogdesign calenadar.' % (entry['series_name'], entry['series_id']))
            shid = '%s-%s-%s/%s-%s' % (show_id, entry['series_season'], entry['series_episode'],
                                       datetime.now().month, datetime.now().year)
            try:
                session.post('http://www.pogdesign.co.uk/cat/watchhandle',
                             data={'watched': 'adding', 'shid': shid})
            except requests.RequestException as e:
                log.error('Error marking %s %s in pogdesign calendar: %s' %
                          (entry['series_name'], entry['series_id'], e))

    def find_show_id(self, show_name, db_sess):
        # Check if we have this show id cached
        show_name = show_name.lower()
        db_show = db_sess.query(PogcalShow).filter(PogcalShow.name == show_name).first()
        if db_show:
            return db_show.id
        try:
            page = session.get('http://www.pogdesign.co.uk/cat/showselect.php')
        except requests.RequestException as e:
            log.error('Error looking up show show list from pogdesign calendar: %s' % e)
            return
        # Try to find the show id from pogdesign show list
        show_re = series_parser.name_to_re(show_name)
        soup = get_soup(page.content)
        search = re.compile(show_re, flags=re.I)
        show = soup.find(text=search)
        if show:
            id = int(show.previous['value'])
            db_sess.add(PogcalShow(id=id, name=show_name))
            return id
        else:
            log.verbose('Could not find pogdesign calendar id for show `%s`' % show_re)

@event('plugin.register')
def register_plugin():
    plugin.register(PogcalAcquired, 'pogcal_acquired', api_ver=2)

########NEW FILE########
__FILENAME__ = thetvdb_submit
from __future__ import unicode_literals, division, absolute_import
import logging
import xml.etree.ElementTree as ElementTree

from requests import RequestException

from flexget import plugin
from flexget.event import event

try:
    from flexget.plugins.api_tvdb import get_mirror, api_key
except ImportError:
    raise plugin.DependencyError(issued_by='thetvdb_submit', missing='api_tvdb',
                                 message='thetvdb_add/remove requires the `api_tvdb` plugin')


class TVDBSubmit(object):
    
    schema = {
        'type': 'object',
        'properties': {
            'account_id': {'type': 'string'}
        },
        'required': ['account_id'],
        'additionalProperties': False
    }
    
    # Defined by subclasses
    remove = None
    log = None
    
    def exists(self, favs, tvdb_id):
        if favs is not None:
            for series in favs.findall('Series'):
                if series.text == tvdb_id:
                    return True
        return False
    
    @plugin.priority(-255)
    def on_task_output(self, task, config):
        mirror = None
        favs = None
        for entry in task.accepted:
            if entry.get('tvdb_id'):
                tvdb_id = str(entry['tvdb_id'])
                ser_info = entry.get('series_name', tvdb_id)
                if favs is not None:
                    isin = self.exists(favs, tvdb_id)
                    if (self.remove and not isin) or (isin and not self.remove):
                        self.log.verbose('Nothing to do for series %s, skipping...' % ser_info)
                        continue
                if not mirror:
                    mirror = get_mirror()
                url = mirror + 'User_Favorites.php?accountid=%s&type=%s&seriesid=%s' % \
                    (config['account_id'], 'remove' if self.remove else 'add', tvdb_id)
                try:
                    page = task.requests.get(url).content
                except RequestException as e:
                    self.log.error('Error submitting series %s to tvdb: %s' % (tvdb_id, e))
                    continue
                if not page:
                    self.log.error('Null response from tvdb, aborting task.')
                    return
                favs = ElementTree.fromstring(page)
                isin = self.exists(favs, tvdb_id)
                if (isin and not self.remove):
                    self.log.verbose('Series %s added to tvdb favorites.' % ser_info)
                elif (self.remove and not isin):
                    self.log.verbose('Series %s removed from tvdb favorites.' % ser_info)
                else:
                    self.log.info("Operation failed for series %s (don't know why)." % ser_info)


class TVDBAdd(TVDBSubmit):
    """Add all accepted shows to your tvdb favorites."""
    remove = False
    log = logging.getLogger('thetvdb_add')


class TVDBRemove(TVDBSubmit):
    """Remove all accepted shows from your tvdb favorites."""
    remove = True
    log = logging.getLogger('thetvdb_remove')


@event('plugin.register')
def register_plugin():
    plugin.register(TVDBAdd, 'thetvdb_add', api_ver=2)
    plugin.register(TVDBRemove, 'thetvdb_remove', api_ver=2)

########NEW FILE########
__FILENAME__ = torrent_cache
from __future__ import unicode_literals, division, absolute_import
import logging
import re
import random

from flexget import plugin
from flexget.event import event

log = logging.getLogger('torrent_cache')

MIRRORS = ['http://torrage.com/torrent/',
           'https://torcache.net/torrent/']


class TorrentCache(object):
    """Adds urls to torrent cache sites to the urls list."""

    @plugin.priority(120)
    def on_task_urlrewrite(self, task, config):
        for entry in task.accepted:
            info_hash = None
            if entry['url'].startswith('magnet:'):
                info_hash_search = re.search('btih:([0-9a-f]+)', entry['url'], re.IGNORECASE)
                if info_hash_search:
                    info_hash = info_hash_search.group(1)
            elif entry.get('torrent_info_hash'):
                info_hash = entry['torrent_info_hash']
            if info_hash:
                entry.setdefault('urls', [entry['url']])
                urls = set(host + info_hash.upper() + '.torrent' for host in MIRRORS)
                # Don't add any duplicate addresses
                urls = list(urls - set(entry['urls']))
                # Add the cache mirrors in a random order
                random.shuffle(urls)
                entry['urls'].extend(urls)


@event('plugin.register')
def register_plugin():
    plugin.register(TorrentCache, 'torrent_cache', api_ver=2, builtin=True)

########NEW FILE########
__FILENAME__ = trakt_submit
from __future__ import unicode_literals, division, absolute_import
import logging
import hashlib

from requests import RequestException

from flexget import plugin
from flexget.event import event
from flexget.utils import json


class TraktSubmit(object):

    schema = {
        'type': 'object',
        'properties': {
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'api_key': {'type': 'string'},
            'list': {'type': 'string'}
        },
        'required': ['username', 'password', 'api_key', 'list'],
        'additionalProperties': False
    }

    # Defined by subclasses
    remove = None
    log = None

    def submit_data(self, task, url, params):
        if task.manager.options.test:
            self.log.info('Not submitting to trakt.tv because of test mode.')
            return
        prm = json.dumps(params)
        self.log.debug('Submitting data to trakt.tv (%s): %s' % (url, prm))
        try:
            result = task.requests.post(url, data=prm, raise_status=False)
        except RequestException as e:
            self.log.error('Error submitting data to trakt.tv: %s' % e)
            return
        if result.status_code == 404:
            # Remove some info from posted json and print the rest to aid debugging
            for key in ['username', 'password', 'episodes']:
                params.pop(key, None)
            self.log.warning('Some movie/show is unknown to trakt.tv: %s' % params)
        elif result.status_code == 401:
            self.log.error('Authentication error: check your trakt.tv username/password/api_key')
            self.log.debug(result.text)
        elif result.status_code != 200:
            self.log.error('Error submitting data to trakt.tv: %s' % result.text)
        else:
            self.log.info('Data successfully sent to trakt.tv: ' + result.text)
    
    @plugin.priority(-255)
    def on_task_output(self, task, config):
        """Finds accepted movies and submits them to the user trakt watchlist."""
        config['password'] = hashlib.sha1(config['password']).hexdigest()
        # Don't edit the config, or it won't pass validation on rerun
        url_params = config.copy()
        url_params['data_type'] = 'list'
        # Do some translation from visible list name to prepare for use in url
        list_name = config['list'].lower()
        # These characters are just stripped in the url
        for char in '!@#$%^*()[]{}/=?+\\|_':
            list_name = list_name.replace(char, '')
        # These characters get replaced
        list_name = list_name.replace('&', 'and')
        list_name = list_name.replace(' ', '-')
        url_params['list_type'] = list_name
        # Sort out the data
        found = {'shows': {}, 'movies': {}}
        for entry in task.accepted:
            serie = None
            if entry.get('tvdb_id'):
                serie = found['shows'].setdefault(entry['tvdb_id'], 
                                                  {'tvdb_id': entry['tvdb_id']})
            elif entry.get('series_name'):
                serie = found['shows'].setdefault(entry['series_name'].lower(), 
                                                  {'title': entry['series_name'].lower()})
            elif entry.get('imdb_id'):
                found['movies'].setdefault(entry['imdb_id'], 
                                           {'imdb_id': entry['imdb_id']})
            elif entry.get('tmdb_id'):
                found['movies'].setdefault(entry['tmdb_id'], 
                                           {'tmdb_id': entry['tmdb_id']})
            elif entry.get('movie_name') and entry.get('movie_year'):
                found['movies'].setdefault(entry['movie_name'].lower(), 
                                           {'title': entry['movie_name'], 
                                            'year': entry['movie_year']})
            if serie:
                if entry.get('series_season') and entry.get('series_episode'):
                    serie.setdefault('episodes', []).append({'season': entry['series_season'], 
                                                             'episode': entry['series_episode']})
                else:
                    serie['whole'] = True
        if not (found['shows'] or found['movies']):
            self.log.debug('Nothing to submit to trakt.')
            return
        # Make the calls
        if not list_name in ['watchlist', 'seen', 'library']:
            post_params = {'username': config['username'], 
                           'password': config['password'], 
                           'slug': list_name, 'items': []}
            for item in found['movies'].itervalues():    
                data = {'type': 'movie'}
                data.update(item)
                post_params['items'].append(data)
            for item in found['shows'].itervalues():
                if 'whole' in item:
                    data = {'type': 'show'}
                    data.update(item)
                    del data['whole']
                    if 'episodes' in data:
                        del data['episodes']
                    post_params['items'].append(data)
                else:
                    for epi in item['episodes']:
                        data = {'type': 'episode'}
                        data.update(item)
                        data.update(epi)
                        del data['episodes']
                        post_params['items'].append(data)
            post_url = 'http://api.trakt.tv/lists/items/%s/%s' % \
                ('delete' if self.remove else 'add', config['api_key'])
            self.submit_data(task, post_url, post_params)
        else:
            base_params = {'username': config['username'], 'password': config['password']}
            if self.remove:
                list_name = 'un' + list_name
            post_params = {'movies': []}
            post_params.update(base_params)
            for item in found['movies'].itervalues():
                post_params['movies'].append(item)
            if post_params['movies']:
                post_url = 'http://api.trakt.tv/movie/%s/%s' % (list_name, config['api_key'])
                self.submit_data(task, post_url, post_params)
            for item in found['shows'].itervalues():
                if item.get('whole'):
                    post_params = item.copy()
                    post_params.update(base_params)
                    del post_params['whole']
                    if 'episodes' in post_params:
                        del post_params['episodes']
                    post_url = 'http://api.trakt.tv/show/%s/%s' % (list_name, config['api_key'])
                    self.submit_data(task, post_url, post_params)
                else:
                    post_params = {'episodes': item['episodes']}
                    post_params.update(base_params)
                    post_params.update(item)
                    post_url = 'http://api.trakt.tv/show/episode/%s/%s' % (list_name, config['api_key'])
                    self.submit_data(task, post_url, post_params)


class TraktAdd(TraktSubmit):
    """Add all accepted elements in your trakt.tv watchlist/library/seen or custom list."""
    remove = False
    log = logging.getLogger('trakt_add')


class TraktRemove(TraktSubmit):
    """Remove all accepted elements from your trakt.tv watchlist/library/seen or custom list."""
    remove = True
    log = logging.getLogger('trakt_remove')


@event('plugin.register')
def register_plugin():
    plugin.register(TraktAdd, 'trakt_add', api_ver=2)
    plugin.register(TraktRemove, 'trakt_remove', api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_anirena
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('anirena')


class UrlRewriteAniRena(object):
    """AniRena urlrewriter."""

    def url_rewritable(self, task, entry):
        return entry['url'].startswith('http://www.anirena.com/viewtracker.php?action=details&id=')

    def url_rewrite(self, task, entry):
        entry['url'] = entry['url'].replace('details', 'download')

@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteAniRena, 'anirena', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_bakabt
from __future__ import unicode_literals, division, absolute_import
import urllib2
import logging

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils.tools import urlopener
from flexget.utils.soup import get_soup

log = logging.getLogger('bakabt')


class UrlRewriteBakaBT(object):
    """BakaBT urlrewriter."""

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        if url.startswith('http://www.bakabt.com/download/'):
            return False
        if url.startswith('http://www.bakabt.com/') or url.startswith('http://bakabt.com/'):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        entry['url'] = self.parse_download_page(entry['url'])

    @plugin.internet(log)
    def parse_download_page(self, url):
        txheaders = {'User-agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}
        req = urllib2.Request(url, None, txheaders)
        page = urlopener(req, log)
        try:
            soup = get_soup(page)
        except Exception as e:
            raise UrlRewritingError(e)
        tag_a = soup.find('a', attrs={'class': 'download_link'})
        if not tag_a:
            raise UrlRewritingError('Unable to locate download link from url %s' % url)
        torrent_url = 'http://www.bakabt.com' + tag_a.get('href')
        return torrent_url

@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteBakaBT, 'bakabt', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_btchat
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger("btchat")


class UrlRewriteBtChat(object):
    """BtChat urlrewriter."""

    def url_rewritable(self, task, entry):
        return entry['url'].startswith('http://www.bt-chat.com/download.php')

    def url_rewrite(self, task, entry):
        entry['url'] = entry['url'].replace('download.php', 'download1.php')


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteBtChat, 'btchat', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_btjunkie
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger("btjunkie")


class UrlRewriteBtJunkie(object):
    """BtJunkie urlrewriter."""

    def url_rewritable(self, task, entry):
        return entry['url'].startswith('http://btjunkie.org')

    def url_rewrite(self, task, entry):
        entry['url'] = entry['url'].replace('btjunkie.org', 'dl.btjunkie.org')
        entry['url'] += "/download.torrent"


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteBtJunkie, 'btjunkie', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_deadfrog
from __future__ import unicode_literals, division, absolute_import
import logging
import re
import urllib2

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils.tools import urlopener
from flexget.utils.soup import get_soup

log = logging.getLogger('deadfrog')


class UrlRewriteDeadFrog(object):
    """DeadFrog urlrewriter."""

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        if url.startswith('http://www.deadfrog.us/download/'):
            return False
        if url.startswith('http://www.deadfrog.us/') or url.startswith('http://deadfrog.us/'):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        entry['url'] = self.parse_download_page(entry['url'])

    @plugin.internet(log)
    def parse_download_page(self, url):
        txheaders = {'User-agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}
        req = urllib2.Request(url, None, txheaders)
        page = urlopener(req, log)
        try:
            soup = get_soup(page)
        except Exception as e:
            raise UrlRewritingError(e)
        down_link = soup.find('a', attrs={'href': re.compile("download/\d+/.*\.torrent")})
        if not down_link:
            raise UrlRewritingError('Unable to locate download link from url %s' % url)
        return 'http://www.deadfrog.us/' + down_link.get('href')


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteDeadFrog, 'deadfrog', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_divxatope
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils import requests
from flexget.utils.soup import get_soup

log = logging.getLogger('divxatope')


class UrlRewriteDivxATope(object):
    """divxatope urlrewriter."""

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        return (
            url.startswith('http://www.divxatope.com/descargar_torrent')
            or url.startswith('http://divxatope.com/descargar_torrent')
        )

    # urlrewriter API
    def url_rewrite(self, task, entry):
        entry['url'] = self.parse_download_page(entry['url'])

    @plugin.internet(log)
    def parse_download_page(self, url):
        try:
            page = requests.get(url).content
            soup = get_soup(page, 'html.parser')
            download_link = soup.findAll(href=re.compile('redirect.php'))
            download_href = download_link[0]['href']
            return download_href[download_href.index('url=') + 4:]
        except Exception:
            raise UrlRewritingError(
                'Unable to locate torrent from url %s' % url
            )


@event('plugin.register')
def register_plugin():
    plugin.register(
        UrlRewriteDivxATope,
        'divxatope',
        groups=['urlrewriter'],
        api_ver=2
    )

########NEW FILE########
__FILENAME__ = urlrewrite_eztv
from __future__ import unicode_literals, division, absolute_import
import re
import logging
from urlparse import urlparse, urlunparse
from requests import RequestException

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils import requests
from flexget.utils.soup import get_soup

log = logging.getLogger('eztv')

EZTV_MIRRORS = [
    ('http', 'eztv.it'),
    ('https', 'eztv-proxy.net'),
    ('http', 'eztv.come.in')]


class UrlRewriteEztv(object):
    """Eztv url rewriter."""

    def url_rewritable(self, task, entry):
        return urlparse(entry['url']).netloc == 'eztv.it'

    def url_rewrite(self, task, entry):
        url = entry['url']
        page = None
        for (scheme, netloc) in EZTV_MIRRORS:
            try:
                _, _, path, params, query, fragment = urlparse(url)
                url = urlunparse((scheme, netloc, path, params, query, fragment))
                page = task.requests.get(url).content
            except RequestException as e:
                log.debug('Eztv mirror `%s` seems to be down', url)
                continue
            break

        if not page:
            raise UrlRewritingError('No mirrors found for url %s' % entry['url'])

        log.debug('Eztv mirror `%s` chosen', url)
        try:
            soup = get_soup(page)
            mirrors = soup.find_all('a', attrs={'class': re.compile(r'download_\d')})
        except Exception as e:
            raise UrlRewritingError(e)

        log.debug('%d torrent mirrors found', len(mirrors))

        if not mirrors:
            raise UrlRewritingError('Unable to locate download link from url %s' % url)

        entry['urls'] = [m.get('href') for m in mirrors]
        entry['url'] = mirrors[0].get('href')



@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteEztv, 'eztv', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_frenchtorrentdb
from __future__ import unicode_literals, division, absolute_import
import re
import logging

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils.tools import urlopener
from flexget.utils.soup import get_soup

log = logging.getLogger('FTDB')


class UrlRewriteFTDB(object):
    """FTDB RSS url_rewrite"""

    def url_rewritable(self, task, entry):
        #url = entry['url']
        if re.match(r'^http://www\.frenchtorrentdb\.com/[^/]+(?!/)[^/]+&rss=1', (entry['url'])):
            return True
        return False

    def url_rewrite(self, task, entry):
        old_url = entry['url']
        page_url = old_url.replace('DOWNLOAD', 'INFOS')
        page_url = page_url.replace('&rss=1', '')

        new_url = self.parse_download_page(page_url)
        log.debug('PAGE URL NEEDED : %s' % page_url)
        log.debug('%s OLD is rewrited to NEW %s' % (old_url, new_url))
        entry['url'] = new_url

    def parse_download_page(self, page_url):
        page = urlopener(page_url, log)
        try:
            soup = get_soup(page)
        except Exception as e:
            raise UrlRewritingError(e)
        tag_a = soup.find("a", {"class": "dl_link"})
        if not tag_a:
            raise UrlRewritingError(
                'FTDB Unable to locate download link from url %s and tag_a is : %s' % (page_url, tag_a)
            )
        torrent_url = "http://www3.frenchtorrentdb.com" + tag_a.get('href') + "&js=1"
        log.debug('TORRENT URL is : %s' % torrent_url)
        return torrent_url


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteFTDB, 'frenchtorrentdb', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_google_cse
from __future__ import unicode_literals, division, absolute_import
import re
import urllib2
import logging
import urlparse

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils.requests import Session
from flexget.utils.soup import get_soup
from flexget.utils.tools import urlopener

log = logging.getLogger('google')

requests = Session()
requests.headers.update({'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'})
requests.set_domain_delay('imdb.com', '2 seconds')


class UrlRewriteGoogleCse(object):
    """Google custom query urlrewriter."""

    # urlrewriter API
    def url_rewritable(self, task, entry):
        if entry['url'].startswith('http://www.google.com/cse?'):
            return True
        if entry['url'].startswith('http://www.google.com/custom?'):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        try:
            # need to fake user agent
            txheaders = {'User-agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}
            req = urllib2.Request(entry['url'], None, txheaders)
            page = urlopener(req, log)
            soup = get_soup(page)
            results = soup.find_all('a', attrs={'class': 'l'})
            if not results:
                raise UrlRewritingError('No results')
            for res in results:
                url = res.get('href')
                url = url.replace('/interstitial?url=', '')
                # generate match regexp from google search result title
                regexp = '.*'.join([x.contents[0] for x in res.find_all('em')])
                if re.match(regexp, entry['title']):
                    log.debug('resolved, found with %s' % regexp)
                    entry['url'] = url
                    return
            raise UrlRewritingError('Unable to resolve')
        except Exception as e:
            raise UrlRewritingError(e)


class UrlRewriteGoogle(object):

    # urlrewriter API
    def url_rewritable(self, task, entry):
        if entry['url'].startswith('https://www.google.com/search?q='):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        log.debug('Requesting %s' % entry['url'])
        page = requests.get(entry['url'])
        soup = get_soup(page.text)

        for link in soup.findAll('a', attrs={'href': re.compile(r'^/url')}):
            # Extract correct url from google internal link
            href = 'http://google.com' + link['href']
            args = urlparse.parse_qs(urlparse.urlparse(href).query)
            href = args['q'][0]

            # import IPython; IPython.embed()
            # import sys
            # sys.exit(1)
            #href = link['href'].lstrip('/url?q=').split('&')[0]

            # Test if entry with this url would be recognized by some urlrewriter
            log.trace('Checking if %s is known by some rewriter' % href)
            fake_entry = {'title': entry['title'], 'url': href}
            urlrewriting = plugin.get_plugin_by_name('urlrewriting')
            if urlrewriting['instance'].url_rewritable(task, fake_entry):
                log.debug('--> rewriting %s (known url pattern)' % href)
                entry['url'] = href
                return
            else:
                log.debug('<-- ignoring %s (unknown url pattern)' % href)
        raise UrlRewritingError('Unable to resolve')


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteGoogleCse, 'google_cse', groups=['urlrewriter'], api_ver=2)
    plugin.register(UrlRewriteGoogle, 'google', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_iptorrents
from __future__ import unicode_literals, division, absolute_import
import re
import urllib
import logging


from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.entry import Entry
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils import requests
from flexget.utils.soup import get_soup
from flexget.utils.search import torrent_availability, normalize_unicode

log = logging.getLogger('iptorrents')

CATEGORIES = {

    'Movie-all': 72,

    # Movies
    'Movie-3D': 87,
    'Movie-480p': 77,
    'Movie-BD-R': 89,
    'Movie-BD-Rip': 90,
    'Movie-DVD-R': 6,
    'Movie-HD-Bluray': 48,
    'Movie-Kids': 54,
    'Movie-MP4': 62,
    'Movie-Non-English': 38,
    'Movie-Packs': 68,
    'Movie-XviD': 17,

    #TV
    'TV-all': 73,

    'TV-Sports': 55,
    'TV-480p': 78,
    'TV-MP4': 66,
    'TV-Non-English': 82,
    'TV-Packs': 65,
    'TV-Packs-Non-English': 83,
    'TV-SD-x264': 79,
    'TV-x264': 5,
    'TV-XVID': 4
}
import sys


class UrlRewriteIPTorrents(object):
    """
        IpTorrents urlrewriter and search plugin.

        iptorrents:
          rss_key: xxxxxxxxx  (required)
          uid: xxxxxxxx  (required)
          password: xxxxxxxx  (required)
          category: HD

          Category is any combination of: all, Movie-3D, Movie-480p, Movie-3D,
                Movie-480p, Movie-BD-R, Movie-BD-Rip, Movie-DVD-R,
                Movie-HD-Bluray, Movie-Kids, Movie-MP4,
                Movie-Non-English, Movie-Packs, Movie-XviD,

                TV-all, TV-Sports, TV-480p, TV-MP4, TV-Non-English, TV-Packs,
                TV-Packs-Non-English, TV-SD-x264, TV-x264,	TV-XVID
    """

    schema = {
        'type': 'object',
        'properties': {
            'rss_key': {'type': 'string'},
            'uid': {'type': 'integer'},
            'password': {'type': 'string'},
            'category': one_or_more({
                'oneOf': [
                    {'type': 'integer'},
                    {'type': 'string', 'enum': list(CATEGORIES)},
                ]}),
        },
        'required': ['rss_key', 'uid', 'password'],
        'additionalProperties': False
    }

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        if url.startswith('http://iptorrents.com/download.php/'):
            return False
        if url.startswith('http://iptorrents.com/'):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        if not 'url' in entry:
            log.error("Didn't actually get a URL...")
        else:
            log.debug("Got the URL: %s" % entry['url'])
        if entry['url'].startswith('http://iptorrents.com/t?'):
            # use search
            results = self.search(entry)
            if not results:
                raise UrlRewritingError("No search results found")
            # TODO: Search doesn't enforce close match to title, be more picky
            entry['url'] = results[0]['url']

    @plugin.internet(log)
    def search(self, entry, config=None):
        """
        Search for name from torrentleech.
        """
        rss_key = config['rss_key']

        if not isinstance(config, dict):
            config = {}
        # sort = SORT.get(config.get('sort_by', 'seeds'))
        # if config.get('sort_reverse'):
            # sort += 1
        categories = config.get('category', 'all')
        # Make sure categories is a list
        if not isinstance(categories, list):
            categories = [categories]

        # If there are any text categories, turn them into their id number
        categories = [c if isinstance(c, int) else CATEGORIES[c]
                      for c in categories]
        filter_url = '&'.join(('l' + str(c) + '=') for c in categories)

        entries = set()

        for search_string in entry.get('search_strings', [entry['title']]):
            query = normalize_unicode(search_string)

            # urllib.quote will crash if the unicode string has non ascii
            # characters, so encode in utf-8 beforehand
            url = ('http://iptorrents.com/t?' + filter_url + '&q=' +
                   urllib.quote_plus(query.encode('utf-8')) + '&qf=')

            page = requests.get(url, cookies={'uid': str(config['uid']),
                                'pass': config['password']}).content
            soup = get_soup(page)

            if soup.find("title").contents[0] == "IPT":
                raise plugin.PluginError("Page title unexpected: Could it be the login page?...")

            log.debug('searching with url: %s' % url)

            tb = soup.find('table', {'class': 'torrents'})
            if not tb:
                continue

            # list all row of torrents table except first because it is titles
            for tr in tb.findAll('tr')[1:]:

                h1 = tr.find('h1')
                if h1 is not None:
                    if h1.contents[0] == 'No Torrents Found!':
                        break

                link = tr.find("a", attrs={'href':
                                           re.compile('/details\.php\?id=\d+')
                                           })
                log.debug('link phase: %s' % link.contents[0])

                entry = Entry()
                entry['title'] = link.contents[0]

                torrent_url = tr.find("a", attrs={'href': re.compile('/download.php/\d+/.*')}).get('href')
                torrent_url = normalize_unicode(torrent_url)
                torrent_url = urllib.quote(torrent_url.encode('utf-8'))
                torrent_url = 'http://iptorrents.com' + torrent_url + '?torrent_pass=' + rss_key

                log.debug('RSS-ified download link: %s' % torrent_url)

                entry['url'] = torrent_url

                seeders = tr.find_all('td', {'class': 'ac t_seeders'})
                leechers = tr.find_all('td', {'class': 'ac t_leechers'})
                entry['torrent_seeds'] = int(seeders[0].contents[0])
                entry['torrent_leeches'] = int(leechers[0].contents[0])
                entry['search_sort'] = torrent_availability(entry['torrent_seeds'],
                                                            entry['torrent_leeches'])
                size = tr.find("td", text=re.compile('([\.\d]+) ([GMK]?)B')).contents[0]

                size = re.search('([\.\d]+) ([GMK]?)B', size)
                if size:
                    if size.group(2) == 'G':
                        entry['content_size'] = int(float(size.group(1)) * 1000 ** 3 / 1024 ** 2)
                    elif size.group(2) == 'M':
                        entry['content_size'] = int(float(size.group(1)) * 1000 ** 2 / 1024 ** 2)
                    elif size.group(2) == 'K':
                        entry['content_size'] = int(float(size.group(1)) * 1000 / 1024 ** 2)
                    else:
                        entry['content_size'] = int(float(size.group(1)) / 1024 ** 2)
                entries.add(entry)

        return sorted(entries, reverse=True, key=lambda x: x.get('search_sort'))


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteIPTorrents, 'iptorrents',
                    groups=['urlrewriter', 'search'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_isohunt
from __future__ import unicode_literals, division, absolute_import
import logging
import re
import urllib

import feedparser

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.search import torrent_availability, normalize_unicode

log = logging.getLogger('isohunt')


class UrlRewriteIsoHunt(object):
    """IsoHunt urlrewriter and search plugin.

    should accept:
    isohunt: <category>

      categories:
      empty or -1: All
      0 : Misc.
      1 : Video/Movies
      2 : Audio
      3 : TV
      4 : Games
      5 : Apps
      6 : Pics
      7 : Anime
      8 : Comics
      9 : Books
      10: Music Video
      11: Unclassified
      12: ALL
    """

    schema = {
        'type': 'string',
        'enum': ['misc', 'movies', 'audio', 'tv', 'games', 'apps', 'pics', 'anime', 'comics', 'books', 'music video',
                 'unclassified', 'all']
    }

    def url_rewritable(self, task, entry):
        url = entry['url']
        # search is not supported
        if url.startswith('http://isohunt.com/torrents/?ihq='):
            return False
        # not replaceable
        if not 'torrent_details' in url:
            return False
        return url.startswith('http://isohunt.com') and url.find('download') == -1

    def url_rewrite(self, task, entry):
        entry['url'] = entry['url'].replace('torrent_details', 'download')

    def search(self, entry, config):
        # urllib.quote will crash if the unicode string has non ascii characters, so encode in utf-8 beforehand
        optionlist = ['misc', 'movies', 'audio', 'tv', 'games', 'apps', 'pics', 'anime', 'comics', 'books',
                      'music video', 'unclassified', 'all']
        entries = set()
        search_strings = [normalize_unicode(s) for s in entry.get('search_strings', [entry['title']])]
        for search_string in search_strings:
            url = 'http://isohunt.com/js/rss/%s?iht=%s&noSL' % (
                urllib.quote(search_string.encode('utf-8')), optionlist.index(config))

            log.debug('requesting: %s' % url)
            rss = feedparser.parse(url)

            status = rss.get('status', False)
            if status != 200:
                raise plugin.PluginWarning('Search result not 200 (OK), received %s' % status)

            ex = rss.get('bozo_exception', False)
            if ex:
                raise plugin.PluginWarning('Got bozo_exception (bad feed)')

            for item in rss.entries:
                entry = Entry()
                entry['title'] = item.title
                entry['url'] = item.link

                m = re.search(r'Size: ([\d]+).*Seeds: (\d+).*Leechers: (\d+)', item.description, re.IGNORECASE)
                if not m:
                    log.debug('regexp did not find seeds / peer data')
                    continue
                else:
                    log.debug('regexp found size(%s), Seeds(%s) and Leeches(%s)' % (m.group(1), m.group(2), m.group(3)))

                    entry['content_size'] = int(m.group(1))
                    entry['torrent_seeds'] = int(m.group(2))
                    entry['torrent_leeches'] = int(m.group(3))
                    entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])

                entries.add(entry)

        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteIsoHunt, 'isohunt', groups=['urlrewriter', 'search'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_newpct
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils.requests import Session
from flexget.utils.soup import get_soup

log = logging.getLogger('newpct')

requests = Session()
requests.headers.update({'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'})
requests.set_domain_delay('imdb.com', '2 seconds')


class UrlRewriteNewPCT(object):
    """NewPCT urlrewriter."""

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        if url.startswith('http://www.newpct.com/download/'):
            return False
        if url.startswith('http://www.newpct.com/') or url.startswith('http://newpct.com/'):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        entry['url'] = self.parse_download_page(entry['url'])

    @plugin.internet(log)
    def parse_download_page(self, url):
        page = requests.get(url)
        try:
            soup = get_soup(page.text)
        except Exception as e:
            raise UrlRewritingError(e)
        torrent_id_prog = re.compile("'torrentID': '(\d+)'")
        torrent_ids = soup.findAll(text=torrent_id_prog)
        if len(torrent_ids) == 0:
            raise UrlRewritingError('Unable to locate torrent ID from url %s' % url)
        torrent_id = torrent_id_prog.search(torrent_ids[0]).group(1)
        return 'http://www.pctorrent.com/descargar/index.php?link=descargar/torrent/%s/dummy.html' % torrent_id

@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteNewPCT, 'newpct', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_newtorrents
from __future__ import unicode_literals, division, absolute_import
import urllib
import urllib2
import logging
import re

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils.soup import get_soup
from flexget.utils.tools import urlopener
from flexget.utils.search import torrent_availability, normalize_unicode

timeout = 10
import socket
socket.setdefaulttimeout(timeout)

log = logging.getLogger('newtorrents')


class NewTorrents:
    """NewTorrents urlrewriter and search plugin."""

    def __init__(self):
        self.resolved = []

    # UrlRewriter plugin API
    def url_rewritable(self, task, entry):
        # Return true only for urls that can and should be resolved
        if entry['url'].startswith('http://www.newtorrents.info/down.php?'):
            return False
        return entry['url'].startswith('http://www.newtorrents.info') and not entry['url'] in self.resolved

    # UrlRewriter plugin API
    def url_rewrite(self, task, entry):
        url = entry['url']
        if (url.startswith('http://www.newtorrents.info/?q=') or
           url.startswith('http://www.newtorrents.info/search')):
            results = self.entries_from_search(entry['title'], url=url)
            if not results:
                raise UrlRewritingError("No matches for %s" % entry['title'])
            url = results[0]['url']
        else:
            url = self.url_from_page(url)

        if url:
            entry['url'] = url
            self.resolved.append(url)
        else:
            raise UrlRewritingError('Bug in newtorrents urlrewriter')

    # Search plugin API
    def search(self, entry, config=None):
        entries = set()
        for search_string in entry.get('search_string', [entry['title']]):
            entries.update(self.entries_from_search(search_string))
        return entries

    @plugin.internet(log)
    def url_from_page(self, url):
        """Parses torrent url from newtorrents download page"""
        try:
            page = urlopener(url, log)
            data = page.read()
        except urllib2.URLError:
            raise UrlRewritingError('URLerror when retrieving page')
        p = re.compile("copy\(\'(.*)\'\)", re.IGNORECASE)
        f = p.search(data)
        if not f:
            # the link in which plugin relies is missing!
            raise UrlRewritingError('Failed to get url from download page. Plugin may need a update.')
        else:
            return f.group(1)

    @plugin.internet(log)
    def entries_from_search(self, name, url=None):
        """Parses torrent download url from search results"""
        name = normalize_unicode(name)
        if not url:
            url = 'http://www.newtorrents.info/search/%s' % urllib.quote(name.encode('utf-8'), safe=b':/~?=&%')

        log.debug('search url: %s' % url)

        html = urlopener(url, log).read()
        # fix </SCR'+'IPT> so that BS does not crash
        # TODO: should use beautifulsoup massage
        html = re.sub(r'(</SCR.*?)...(.*?IPT>)', r'\1\2', html)

        soup = get_soup(html)
        # saving torrents in dict
        torrents = []
        for link in soup.find_all('a', attrs={'href': re.compile('down.php')}):
            torrent_url = 'http://www.newtorrents.info%s' % link.get('href')
            release_name = link.parent.next.get('title')
            # quick dirty hack
            seed = link.find_next('td', attrs={'class': re.compile('s')}).renderContents()
            if seed == 'n/a':
                seed = 0
            else:
                try:
                    seed = int(seed)
                except ValueError:
                    log.warning('Error converting seed value (%s) from newtorrents to integer.' % seed)
                    seed = 0

            #TODO: also parse content_size and peers from results
            torrents.append(Entry(title=release_name, url=torrent_url, torrent_seeds=seed,
                                  search_sort=torrent_availability(seed, 0)))
        # sort with seed number Reverse order
        torrents.sort(reverse=True, key=lambda x: x.get('search_sort', 0))
        # choose the torrent
        if not torrents:
            dashindex = name.rfind('-')
            if dashindex != -1:
                return self.entries_from_search(name[:dashindex])
            else:
                return torrents
        else:
            if len(torrents) == 1:
                log.debug('found only one matching search result.')
            else:
                log.debug('search result contains multiple matches, sorted %s by most seeders' % torrents)
            return torrents


@event('plugin.register')
def register_plugin():
    plugin.register(NewTorrents, 'newtorrents', groups=['urlrewriter', 'search'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_newzleech
from __future__ import unicode_literals, division, absolute_import
import urllib
import urllib2
import logging
import re

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.soup import get_soup
from flexget.utils.tools import urlopener

log = logging.getLogger("newzleech")


class UrlRewriteNewzleech(object):
    """
        UrlRewriter or search by using newzleech.com
        TODO: implement basic url rewriting
    """

    # Search API
    @plugin.internet(log)
    def search(self, entry, config=None):

        txheaders = {
            'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-us,en;q=0.5',
            'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7',
            'Keep-Alive': '300',
            'Connection': 'keep-alive',
        }
        nzbs = set()
        for search_string in entry.get('search_strings', [entry['title']]):
            query = entry['title']
            url = u'http://newzleech.com/?%s' % str(urllib.urlencode({'q': query.encode('latin1'),
                                                                      'm': 'search', 'group': '', 'min': 'min',
                                                                      'max': 'max', 'age': '', 'minage': '', 'adv': ''}))
            #log.debug('Search url: %s' % url)

            req = urllib2.Request(url, headers=txheaders)
            page = urlopener(req, log)
            soup = get_soup(page)

            for item in soup.find_all('table', attrs={'class': 'contentt'}):
                subject_tag = item.find('td', attrs={'class': 'subject'}).next
                subject = ''.join(subject_tag.find_all(text=True))
                complete = item.find('td', attrs={'class': 'complete'}).contents[0]
                size = item.find('td', attrs={'class': 'size'}).contents[0]
                nzb_url = 'http://newzleech.com/' + item.find('td', attrs={'class': 'get'}).next.get('href')

                # generate regexp from entry title and see if it matches subject
                regexp = query
                wildcardize = [' ', '-']
                for wild in wildcardize:
                    regexp = regexp.replace(wild, '.')
                regexp = '.*' + regexp + '.*'
                #log.debug('Title regexp: %s' % regexp)

                if re.match(regexp, subject):
                    log.debug('%s matches to regexp' % subject)
                    if complete != u'100':
                        log.debug('Match is incomplete %s from newzleech, skipping ..' % query)
                        continue
                    log.info('Found \'%s\'' % query)

                    try:
                        size_num = float(size[:-3])
                    except (ValueError, TypeError):
                        log.error('Failed to parse_size %s' % size)
                        size_num = 0
                    # convert into megabytes
                    if 'GB' in size:
                        size_num *= 1024
                    if 'KB' in size:
                        size_num /= 1024

                    # choose largest file
                    nzbs.add(Entry(title=subject, url=nzb_url, content_size=size_num, search_sort=size_num))

        return nzbs


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteNewzleech, 'newzleech', groups=['search'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_nyaa
from __future__ import unicode_literals, division, absolute_import
import logging
import urllib

import feedparser

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils.search import normalize_unicode

log = logging.getLogger('nyaa')

# TODO: Other categories
CATEGORIES = {'all': '0_0',
              'anime': '1_0'}
FILTERS = ['all', 'filter remakes', 'trusted only', 'a+ only']


class UrlRewriteNyaa(object):
    """Nyaa urlrewriter and search plugin."""

    def validator(self):
        from flexget import validator

        root = validator.factory()
        root.accept('choice').accept_choices(CATEGORIES)
        advanced = root.accept('dict')
        advanced.accept('choice', key='category').accept_choices(CATEGORIES)
        advanced.accept('choice', key='filter').accept_choices(FILTERS)
        return root

    def search(self, entry, config):
        if not isinstance(config, dict):
            config = {'category': config}
        config.setdefault('category', 'anime')
        config.setdefault('filter', 'all')
        entries = set()
        for search_string in entry.get('search_strings', [entry['title']]):
            name = normalize_unicode(search_string)
            url = 'http://www.nyaa.eu/?page=rss&cats=%s&filter=%s&term=%s' % (
                  CATEGORIES[config['category']], FILTERS.index(config['filter']), urllib.quote(name.encode('utf-8')))

            log.debug('requesting: %s' % url)
            rss = feedparser.parse(url)

            status = rss.get('status', False)
            if status != 200:
                raise plugin.PluginWarning('Search result not 200 (OK), received %s' % status)

            ex = rss.get('bozo_exception', False)
            if ex:
                raise plugin.PluginWarning('Got bozo_exception (bad feed)')

            for item in rss.entries:

                entry = Entry()
                entry['title'] = item.title
                entry['url'] = item.link
                # TODO: parse some shit
                #entry['torrent_seeds'] = int(item.seeds)
                #entry['torrent_leeches'] = int(item.leechs)
                #entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])
                #entry['content_size'] = int(item.size) / 1024 / 1024

                entries.add(entry)

        return entries

    def url_rewritable(self, task, entry):
        return entry['url'].startswith('http://www.nyaa.eu/?page=torrentinfo&tid=')

    def url_rewrite(self, task, entry):
        entry['url'] = entry['url'].replace('torrentinfo', 'download')


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteNyaa, 'nyaa', groups=['search', 'urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_piratebay
from __future__ import unicode_literals, division, absolute_import
import re
import urllib
import logging

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils import requests
from flexget.utils.soup import get_soup
from flexget.utils.search import torrent_availability, normalize_unicode

log = logging.getLogger('piratebay')

CUR_TLD = "se"
TLDS = "com|org|sx|ac|pe|gy|%s" % CUR_TLD

URL_MATCH = re.compile("^http://(?:torrents\.)?thepiratebay\.(?:%s)/.*$" % TLDS)
URL_SEARCH = re.compile("^http://thepiratebay\.(?:%s)/search/.*$" % TLDS)

CATEGORIES = {
    'all': 0,
    'audio': 100,
    'music': 101,
    'video': 200,
    'movies': 201,
    'tv': 205,
    'highres movies': 207,
    'comics': 602
}

SORT = {
    'default': 99, # This is piratebay default, not flexget default.
    'date': 3,
    'size': 5,
    'seeds': 7,
    'leechers': 9
}


class UrlRewritePirateBay(object):
    """PirateBay urlrewriter."""

    schema = {
        'oneOf': [
            {'type': 'boolean'},
            {
                'type': 'object',
                'properties': {
                    'category': {
                        'oneOf': [
                            {'type': 'string', 'enum': list(CATEGORIES)},
                            {'type': 'integer'}
                        ]
                    },
                    'sort_by': {'type': 'string', 'enum': list(SORT)},
                    'sort_reverse': {'type': 'boolean'}
                },
                'additionalProperties': False
            }
        ]
    }

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        if url.endswith('.torrent'):
            return False
        return bool(URL_MATCH.match(url))

    # urlrewriter API
    def url_rewrite(self, task, entry):
        if not 'url' in entry:
            log.error("Didn't actually get a URL...")
        else:
            log.debug("Got the URL: %s" % entry['url'])
        if URL_SEARCH.match(entry['url']):
            # use search
            results = self.search(entry)
            if not results:
                raise UrlRewritingError("No search results found")
            # TODO: Close matching was taken out of search methods, this may need to be fixed to be more picky
            entry['url'] = results[0]['url']
        else:
            # parse download page
            entry['url'] = self.parse_download_page(entry['url'])

    @plugin.internet(log)
    def parse_download_page(self, url):
        page = requests.get(url).content
        try:
            soup = get_soup(page)
            tag_div = soup.find('div', attrs={'class': 'download'})
            if not tag_div:
                raise UrlRewritingError('Unable to locate download link from url %s' % url)
            tag_a = tag_div.find('a')
            torrent_url = tag_a.get('href')
            # URL is sometimes missing the schema
            if torrent_url.startswith('//'):
                torrent_url = 'http:' + torrent_url
            return torrent_url
        except Exception as e:
            raise UrlRewritingError(e)

    @plugin.internet(log)
    def search(self, arg_entry, config=None):
        """
        Search for name from piratebay.
        """
        if not isinstance(config, dict):
            config = {}
        sort = SORT.get(config.get('sort_by', 'seeds'))
        if config.get('sort_reverse'):
            sort += 1
        if isinstance(config.get('category'), int):
            category = config['category']
        else:
            category = CATEGORIES.get(config.get('category', 'all'))
        filter_url = '/0/%d/%d' % (sort, category)

        entries = set()
        for search_string in arg_entry.get('search_string', [arg_entry['title']]):
            query = normalize_unicode(search_string)
            # TPB search doesn't like dashes
            query = query.replace('-', ' ')
            # urllib.quote will crash if the unicode string has non ascii characters, so encode in utf-8 beforehand
            url = 'http://thepiratebay.%s/search/%s%s' % (CUR_TLD, urllib.quote(query.encode('utf-8')), filter_url)
            log.debug('Using %s as piratebay search url' % url)
            page = requests.get(url).content
            soup = get_soup(page)
            for link in soup.find_all('a', attrs={'class': 'detLink'}):
                entry = Entry()
                entry['title'] = link.contents[0]
                entry['url'] = 'http://thepiratebay.%s%s' % (CUR_TLD, link.get('href'))
                tds = link.parent.parent.parent.find_all('td')
                entry['torrent_seeds'] = int(tds[-2].contents[0])
                entry['torrent_leeches'] = int(tds[-1].contents[0])
                entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])
                # Parse content_size
                size = link.find_next(attrs={'class': 'detDesc'}).contents[0]
                size = re.search('Size ([\.\d]+)\xa0([GMK])iB', size)
                if size:
                    if size.group(2) == 'G':
                        entry['content_size'] = int(float(size.group(1)) * 1000 ** 3 / 1024 ** 2)
                    elif size.group(2) == 'M':
                        entry['content_size'] = int(float(size.group(1)) * 1000 ** 2 / 1024 ** 2)
                    else:
                        entry['content_size'] = int(float(size.group(1)) * 1000 / 1024 ** 2)
                entries.add(entry)

        return sorted(entries, reverse=True, key=lambda x: x.get('search_sort'))


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewritePirateBay, 'piratebay', groups=['urlrewriter', 'search'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_redskunk
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger("redskunk")


class UrlRewriteRedskunk(object):
    """Redskunk urlrewriter."""

    def url_rewritable(self, task, entry):
        url = entry['url']
        return url.startswith('http://redskunk.org') and url.find('download') == -1

    def url_rewrite(self, task, entry):
        entry['url'] = entry['url'].replace('torrents-details', 'download')
        entry['url'] = entry['url'].replace('&hit=1', '')


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteRedskunk, 'redskunk', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_search
from __future__ import unicode_literals, division, absolute_import
from difflib import SequenceMatcher
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('urlrewrite_search')


class PluginSearch(object):
    """
    Search entry from sites. Accepts list of known search plugins, list is in priority order.
    Once hit has been found no more searches are performed. Should be used only when
    there is no other way to get working download url, ie. when input plugin does not provide
    any downloadable urls.

    Example::

      urlrewrite_search:
        - newtorrents
        - piratebay

    .. note:: Some url rewriters will use search plugins automatically if entry url
              points into a search page.
    """

    schema = {
        'type': 'array',
        'items': {
            'allOf': [
                {'$ref': '/schema/plugins?group=search'},
                {'maxProperties': 1, 'minProperties': 1}
            ]
        }
    }

    # Run before main urlrewriting
    @plugin.priority(130)
    def on_task_urlrewrite(self, task, config):
        # no searches in unit test mode
        if task.manager.unit_test:
            return

        plugins = {}
        for p in plugin.get_plugins(group='search'):
            plugins[p.name] = p.instance

        # search accepted
        for entry in task.accepted:
            # loop through configured searches
            for name in config:
                search_config = None
                if isinstance(name, dict):
                    # the name is the first/only key in the dict.
                    name, search_config = name.items()[0]
                log.verbose('Searching `%s` from %s' % (entry['title'], name))
                try:
                    results = plugins[name].search(entry, search_config)
                    matcher = SequenceMatcher(a=entry['title'])
                    for result in sorted(results, key=lambda e: e.get('search_sort'), reverse=True):
                        matcher.set_seq2(result['title'])
                        if matcher.ratio() > 0.9:
                            log.debug('Found url: %s', result['url'])
                            entry['url'] = result['url']
                            break
                    else:
                        continue
                    break
                except (plugin.PluginError, plugin.PluginWarning) as pw:
                    log.verbose('Failed: %s' % pw.value)
                    continue

            # Search failed
            else:
                # If I don't have a URL, doesn't matter if I'm immortal...
                entry['immortal'] = False
                entry.reject('search failed')


@event('plugin.register')
def register_plugin():
    plugin.register(PluginSearch, 'urlrewrite_search', api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_serienjunkies
from __future__ import unicode_literals, division, absolute_import
import re
import logging

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils import requests
from flexget.utils.soup import get_soup

log = logging.getLogger('serienjunkies')

LANGUAGE = ['de', 'en', 'both']
HOSTER = ['ul', 'cz', 'so']


class UrlRewriteSerienjunkies(object):

    """
    Serienjunkies urlrewriter
    Version 1.0.0

    Language setting works like a whitelist, the selected is needed,
    but others are still possible.

    Configuration
    language: [de|en|both] default "en"
    hoster: [ul|cz|so] default "ul"
    """

    schema = {
        'type': 'object',
        'properties': {
            'language': {'type': 'string', 'enum': LANGUAGE, 'default': 'en'},
            'hoster': {'type': 'string', 'enum': HOSTER, 'default': 'ul'}
        },
        'additionalProperties': False
    }

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        if url.startswith('http://download.serienjunkies.org/'):
            return False
        if url.startswith('http://www.serienjunkies.org/') or url.startswith('http://serienjunkies.org/'):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        series_url = entry['url']
        download_title = entry['title']
        search_title = re.sub('\[.*\] ', '', download_title)
        self.config = task.config.get('serienjunkies')
        download_url = self.parse_download(series_url, search_title, self.config, entry)
        log.debug('TV Show URL: %s' % series_url)
        log.debug('Episode: %s' % search_title)
        log.debug('Download URL: %s' % download_url)
        entry['url'] = download_url

    @plugin.internet(log)
    def parse_download(self, series_url, search_title, config, entry):
        page = requests.get(series_url).content
        try:
            soup = get_soup(page)
        except Exception as e:
            raise UrlRewritingError(e)

        config = config or {}
        config.setdefault('hoster', 'ul')
        config.setdefault('language', 'en')

        # find matching download
        episode_title = soup.find('strong', text=search_title)
        if not episode_title:
            raise UrlRewritingError('Unable to find episode')

        # find download container
        episode = episode_title.parent
        if not episode:
            raise UrlRewritingError('Unable to find episode container')

        # find episode language
        episode_lang = episode.find_previous('strong', text=re.compile('Sprache')).next_sibling
        if not episode_lang:
            raise UrlRewritingError('Unable to find episode language')

        # filter language
        if config['language'] in ['de', 'both']:
            if not re.search('german|deutsch', episode_lang, flags=re.IGNORECASE):
                entry.reject('Language does not match')
        if config['language'] in ['en', 'both']:
            if not re.search('englisc?h', episode_lang, flags=re.IGNORECASE):
                entry.reject('Language does not match')

        # find download links
        links = episode.find_all('a')
        if not links:
            raise UrlRewritingError('Unable to find download links')

        for link in links:
            if not link.has_attr('href'):
                continue

            url = link['href']
            pattern = 'http:\/\/download\.serienjunkies\.org.*%s_.*\.html' % config['hoster']

            if re.match(pattern, url):
                return url
            else:
                log.debug('Hoster does not match')
                continue


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteSerienjunkies, 'serienjunkies', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_shortened
from __future__ import unicode_literals, division, absolute_import
import logging
from urlparse import urlparse

from flexget import plugin
from flexget.utils import requests
from flexget.event import event

log = logging.getLogger('shortened')


class UrlRewriteShortened(object):
    """Shortened url rewriter."""

    def url_rewritable(self, task, entry):
        return urlparse(entry['url']).netloc in ['bit.ly', 't.co']

    def url_rewrite(self, task, entry):
        request = task.requests.head(entry['url'], allow_redirects=True)
        entry['url'] = request.url

@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteShortened, 'shortened', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_stmusic
from __future__ import unicode_literals, division, absolute_import
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger("stmusic")


class UrlRewriteSTMusic(object):
    """STMusic urlrewriter."""

    def url_rewritable(self, task, entry):
        return entry['url'].startswith('http://www.stmusic.org/details.php?id=')

    def url_rewrite(self, task, entry):
        import urllib
        entry['url'] = entry['url'].replace('details.php?id=', 'download.php/')
        entry['url'] += '/%s.torrent' % (urllib.quote(entry['title'], safe=''))


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteSTMusic, 'stmusic', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_torrent411
# coding: utf-8
from __future__ import unicode_literals, division, absolute_import
import re
import logging

from flexget import plugin
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils.tools import urlopener
from flexget.utils.soup import get_soup

log = logging.getLogger('torrent411')


class UrlRewriteTorrent411(object):
    """torrent411 RSS url_rewrite"""

    def url_rewritable(self, feed, entry):
        url = entry['url']
        # match si ce qui suit 'http://www.t411.me/torrents/' ne contient pas
        # '/' comme 'http://www.t411.me/torrents/browse/...' ou
        # 'http://www.t411.me/torrents/download/...'
        if re.match(r'^http://www\.t411\.me/torrents/[^/]+(?!/)[^/]+$', url):
            return True
        return False

    def url_rewrite(self, feed, entry):
        old_url = entry['url']
        entry['url'] = self.parse_download_page(entry['url'])
        log.debug('%s rewritten to %s' % (old_url, entry['url']))

    @plugin.internet(log)
    def parse_download_page(self, url):
        page = urlopener(url, log)
        log.debug('%s opened', url)
        try:
            soup = get_soup(page)
            torrent_url = 'http://www.t411.me' + soup.find(text='Télécharger').findParent().get('href')
        except Exception as e:
            raise UrlRewritingError(e)

        if not torrent_url:
            raise UrlRewritingError('Unable to locate download link from url %s' % url)

        return torrent_url


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteTorrent411, 'torrent411', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_torrentleech
from __future__ import unicode_literals, division, absolute_import
import re
import urllib
import logging

from flexget import plugin
from flexget.config_schema import one_or_more
from flexget.entry import Entry
from flexget.event import event
from flexget.plugins.plugin_urlrewriting import UrlRewritingError
from flexget.utils import requests
from flexget.utils.soup import get_soup
from flexget.utils.search import torrent_availability, normalize_unicode

log = logging.getLogger('torrentleech')

CATEGORIES = {
    'all': 0,

    # Movies
    'Cam': 8,
    'TS': 9,
    'R5': 10,
    'DVDRip': 11,
    'DVDR': 12,
    'HD': 13,
    'BDRip': 14,
    'Movie Boxsets': 15,
    'Documentaries': 29,

    #TV
    'Episodes': 26,
    'TV Boxsets': 27,
    'Episodes HD': 32
}


class UrlRewriteTorrentleech(object):
    """
        Torrentleech urlrewriter and search plugin.

        torrentleech:
          rss_key: xxxxxxxxx  (required)
          username: xxxxxxxx  (required)
          password: xxxxxxxx  (required)
          category: HD

          Category is any combination of: all, Cam, TS, R5,
          DVDRip, DVDR, HD, BDRip, Movie Boxsets, Documentaries,
          Episodes, TV BoxSets, Episodes HD
    """

    schema = {
        'type': 'object',
        'properties': {
            'rss_key': {'type': 'string'},
            'username': {'type': 'string'},
            'password': {'type': 'string'},
            'category': one_or_more({
                'oneOf': [
                    {'type': 'integer'},
                    {'type': 'string', 'enum': list(CATEGORIES)},
            ]}),
        },
        'required': ['rss_key', 'username', 'password'],
        'additionalProperties': False
    }

    # urlrewriter API
    def url_rewritable(self, task, entry):
        url = entry['url']
        if url.endswith('.torrent'):
            return False
        if url.startswith('http://torrentleech.org/'):
            return True
        return False

    # urlrewriter API
    def url_rewrite(self, task, entry):
        if not 'url' in entry:
            log.error("Didn't actually get a URL...")
        else:
            log.debug("Got the URL: %s" % entry['url'])
        if entry['url'].startswith('http://torrentleech.org/torrents/browse/index/query/'):
            # use search
            results = self.search(entry)
            if not results:
                raise UrlRewritingError("No search results found")
            # TODO: Search doesn't enforce close match to title, be more picky
            entry['url'] = results[0]['url']

    @plugin.internet(log)
    def search(self, entry, config=None):
        """
        Search for name from torrentleech.
        """
        rss_key = config['rss_key']

        # build the form request:
        data = {'username': config['username'], 'password': config['password'], 'remember_me': 'on', 'submit': 'submit'}
        # POST the login form:
        login = requests.post('http://torrentleech.org/', data=data)

        if not isinstance(config, dict):
            config = {}
        # sort = SORT.get(config.get('sort_by', 'seeds'))
        # if config.get('sort_reverse'):
            # sort += 1
        categories = config.get('category', 'all')
        # Make sure categories is a list
        if not isinstance(categories, list):
            categories = [categories]
        # If there are any text categories, turn them into their id number
        categories = [c if isinstance(c, int) else CATEGORIES[c] for c in categories]
        filter_url = '/categories/%s' % ','.join(str(c) for c in categories)
        entries = set()
        for search_string in entry.get('search_strings', [entry['title']]):
            query = normalize_unicode(search_string)
            # urllib.quote will crash if the unicode string has non ascii characters, so encode in utf-8 beforehand
            url = ('http://torrentleech.org/torrents/browse/index/query/' +
                   urllib.quote(query.encode('utf-8')) + filter_url)
            log.debug('Using %s as torrentleech search url' % url)

            page = requests.get(url, cookies=login.cookies).content
            soup = get_soup(page)

            for tr in soup.find_all("tr", ["even", "odd"]):
                # within each even or odd row, find the torrent names
                link = tr.find("a", attrs={'href': re.compile('/torrent/\d+')})
                log.debug('link phase: %s' % link.contents[0])
                entry = Entry()
                # extracts the contents of the <a>titlename/<a> tag
                entry['title'] = link.contents[0]

                # find download link
                torrent_url = tr.find("a", attrs={'href': re.compile('/download/\d+/.*')}).get('href')
                # parse link and split along /download/12345 and /name.torrent
                download_url = re.search('(/download/\d+)/(.+\.torrent)', torrent_url)
                # change link to rss and splice in rss_key
                torrent_url = 'http://torrentleech.org/rss' + download_url.group(1) + '/' + rss_key + '/' + download_url.group(2)
                log.debug('RSS-ified download link: %s' % torrent_url)
                entry['url'] = torrent_url

                # us tr object for seeders/leechers
                seeders, leechers = tr.find_all('td', ["seeders", "leechers"])
                entry['torrent_seeds'] = int(seeders.contents[0])
                entry['torrent_leeches'] = int(leechers.contents[0])
                entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])

                # use tr object for size
                size = tr.find("td", text=re.compile('([\.\d]+) ([GMK]?)B')).contents[0]
                size = re.search('([\.\d]+) ([GMK]?)B', size)
                if size:
                    if size.group(2) == 'G':
                        entry['content_size'] = int(float(size.group(1)) * 1000 ** 3 / 1024 ** 2)
                    elif size.group(2) == 'M':
                        entry['content_size'] = int(float(size.group(1)) * 1000 ** 2 / 1024 ** 2)
                    elif size.group(2) == 'K':
                        entry['content_size'] = int(float(size.group(1)) * 1000 / 1024 ** 2)
                    else:
                        entry['content_size'] = int(float(size.group(1)) / 1024 ** 2)
                entries.add(entry)

        return sorted(entries, reverse=True, key=lambda x: x.get('search_sort'))

@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteTorrentleech, 'torrentleech', groups=['urlrewriter', 'search'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_torrentz
from __future__ import unicode_literals, division, absolute_import
import logging
import re
import urllib
import feedparser

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event
from flexget.utils import requests
from flexget.utils.search import torrent_availability, normalize_unicode

log = logging.getLogger('torrentz')

REGEXP = re.compile(r'http://torrentz\.(eu|me)/(?P<hash>[a-f0-9]{40})')
REPUTATIONS = {  # Maps reputation name to feed address
    'any': 'feed_any',
    'low': 'feed_low',
    'good': 'feed',
    'verified': 'feed_verified'
}


class UrlRewriteTorrentz(object):
    """Torrentz urlrewriter."""

    schema = {
        'oneOf' : [
            {
                'title': 'specify options',
                'type': 'object',
                'properties': {
                    'reputation': {'enum': list(REPUTATIONS), 'default': 'good'},
                    'extra_terms': {'type': 'string'}
                },
                'additionalProperties': False
            },
            {'title': 'specify reputation', 'enum': list(REPUTATIONS), 'default': 'good'}
        ]
    }

    def process_config(self, config):
        """Return plugin configuration in advanced form"""
        if isinstance(config, basestring):
            config = {'reputation': config}
        if config.get('extra_terms'):
            config['extra_terms'] = ' '+config['extra_terms']
        return config

    def url_rewritable(self, task, entry):
        return REGEXP.match(entry['url'])

    def url_rewrite(self, task, entry):
        thash = REGEXP.match(entry['url']).group(2)
        entry['url'] = 'https://torcache.net/torrent/%s.torrent' % thash.upper()
        entry['torrent_info_hash'] = thash

    def search(self, entry, config=None):
        config = self.process_config(config)
        feed = REPUTATIONS[config['reputation']]
        entries = set()
        for search_string in entry.get('search_string', [entry['title']]):
            query = normalize_unicode(search_string+config.get('extra_terms', ''))
            for domain in ['eu', 'me']:
                # urllib.quote will crash if the unicode string has non ascii characters, so encode in utf-8 beforehand
                url = 'http://torrentz.%s/%s?q=%s' % (domain, feed, urllib.quote(query.encode('utf-8')))
                log.debug('requesting: %s' % url)
                try:
                    r = requests.get(url)
                    break
                except requests.RequestException as err:
                    log.warning('torrentz.%s failed. Error: %s' % (domain, err))
            else:
                raise plugin.PluginWarning('Error getting torrentz search results')

            rss = feedparser.parse(r.content)

            ex = rss.get('bozo_exception', False)
            if ex:
                raise plugin.PluginWarning('Got bozo_exception (bad feed)')

            for item in rss.entries:
                m = re.search(r'Size: ([\d]+) Mb Seeds: ([,\d]+) Peers: ([,\d]+) Hash: ([a-f0-9]+)',
                              item.description, re.IGNORECASE)
                if not m:
                    log.debug('regexp did not find seeds / peer data')
                    continue

                entry = Entry()
                entry['title'] = item.title
                entry['url'] = item.link
                entry['content_size'] = int(m.group(1))
                entry['torrent_seeds'] = int(m.group(2).replace(',', ''))
                entry['torrent_leeches'] = int(m.group(3).replace(',', ''))
                entry['torrent_info_hash'] = m.group(4).upper()
                entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])
                entries.add(entry)

        log.debug('Search got %d results' % len(entries))
        return entries


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewriteTorrentz, 'torrentz', groups=['urlrewriter', 'search'], api_ver=2)

########NEW FILE########
__FILENAME__ = urlrewrite_urlrewrite
from __future__ import unicode_literals, division, absolute_import
import re
import logging

from flexget import plugin
from flexget.event import event

log = logging.getLogger('urlrewrite')


class UrlRewrite(object):
    """
    Generic configurable urlrewriter.

    Example::

      urlrewrite:
        demonoid:
          regexp: http://www\.demonoid\.com/files/details/
          format: http://www.demonoid.com/files/download/HTTP/
    """

    resolves = {}

    # built-in resolves

#    resolves = yaml.safe_load("""
#    tvsubtitles:
#      match: http://www.tvsubtitles.net/subtitle-
#      replace: http://www.tvsubtitles.net/download-
#    """
#    )

    schema = {
        'type': 'object',
        'additionalProperties': {
            'type': 'object',
            'properties': {
                'regexp': {'type': 'string', 'format': 'regex'},
                'format': {'type': 'string'}
            },
            'required': ['regexp', 'format'],
            'additionalProperties': False
        }
    }

    def on_task_start(self, task, config):
        for name, rewrite_config in config.iteritems():
            match = re.compile(rewrite_config['regexp'])
            format = rewrite_config['format']
            self.resolves[name] = {'regexp_compiled': match, 'format': format, 'regexp': rewrite_config['regexp']}
            log.debug('Added rewrite %s' % name)

    def url_rewritable(self, task, entry):
        log.trace('running url_rewritable')
        log.trace(self.resolves)
        for name, config in self.resolves.iteritems():
            regexp = config['regexp_compiled']
            log.trace('testing %s' % config['regexp'])
            if regexp.search(entry['url']):
                return True
        return False

    def url_rewrite(self, task, entry):
        for name, config in self.resolves.iteritems():
            regexp = config['regexp_compiled']
            format = config['format']
            if regexp.search(entry['url']):
                log.debug('Regexp resolving %s with %s' % (entry['url'], name))

                # run the regexp
                entry['url'] = regexp.sub(format, entry['url'])

                if regexp.match(entry['url']):
                    entry.fail('urlrewriting')
                    task.purge()
                    from flexget.plugins.plugin_urlrewriting import UrlRewritingError
                    raise UrlRewritingError('Regexp %s result should NOT continue to match!' % name)
                return


@event('plugin.register')
def register_plugin():
    plugin.register(UrlRewrite, 'urlrewrite', groups=['urlrewriter'], api_ver=2)

########NEW FILE########
__FILENAME__ = scheduler
from __future__ import unicode_literals, division, absolute_import
import copy
from datetime import datetime, timedelta, time as dt_time
import fnmatch
from hashlib import md5
import itertools
import logging
import Queue
import threading
import time
import sys

from sqlalchemy import Column, String, DateTime

from flexget.config_schema import register_config_key, parse_time
from flexget.db_schema import versioned_base
from flexget.event import event
from flexget.logger import FlexGetFormatter
from flexget.manager import Session

log = logging.getLogger('scheduler')
Base = versioned_base('scheduler', 0)

UNITS = ['seconds', 'minutes', 'hours', 'days', 'weeks']
WEEKDAYS = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']


yaml_schedule = {
    'type': 'object',
    'properties': {
        'seconds': {'type': 'number'},
        'minutes': {'type': 'number'},
        'hours': {'type': 'number'},
        'days': {'type': 'number'},
        'weeks': {'type': 'number'},
        'at_time': {'type': 'string', 'format': 'time'},
        'on_day': {'type': 'string', 'enum': WEEKDAYS}
    },
    # Only allow one unit to be specified
    'oneOf': [{'required': [unit]} for unit in UNITS],
    'error_oneOf': 'Interval must be specified as one of %s' % ', '.join(UNITS),
    'dependencies': {
        'at_time': {
            'properties': {'days': {'type': 'integer'}, 'weeks': {'type': 'integer'}},
            'oneOf': [{'required': ['days']}, {'required': ['weeks']}],
            'error': 'Interval must be an integer number of days or weeks when `at_time` is specified.',
        },
        'on_day': {
            'properties': {'weeks': {'type': 'integer'}},
            'required': ['weeks'],
            'error': 'Unit must be an integer number of weeks when `on_day` is specified.'
        }
    },
    'additionalProperties': False
}


main_schema = {
    'type': 'array',
    'items': {
        'properties': {
            'tasks': {'type': ['array', 'string'], 'items': {'type': 'string'}},
            'interval': yaml_schedule
        },
        'required': ['tasks', 'interval'],
        'additionalProperties': False
    }
}


class DBTrigger(Base):
    __tablename__ = 'scheduler_triggers'

    uid = Column(String, primary_key=True)  # Hash of all trigger properties, to uniquely identify the trigger
    last_run = Column(DateTime)

    def __init__(self, uid, last_run=None):
        self.uid = uid
        self.last_run = last_run


@event('manager.config_updated')
def create_triggers(manager):
    manager.scheduler.load_schedules()


class Scheduler(threading.Thread):
    # We use a regular list for periodic jobs, so you must hold this lock while using it
    triggers_lock = threading.Lock()

    def __init__(self, manager):
        super(Scheduler, self).__init__(name='scheduler')
        self.daemon = True
        self.run_queue = Queue.PriorityQueue()
        self.manager = manager
        self.triggers = []
        self.run_schedules = True
        self._shutdown_now = False
        self._shutdown_when_finished = False

    def load_schedules(self):
        """Clears current schedules and loads them from the config."""
        with self.triggers_lock:
            self.triggers = []
            if 'schedules' not in self.manager.config:
                log.info('No schedules defined in config. Defaulting to run all tasks on a 1 hour interval.')
            for item in self.manager.config.get('schedules', [{'tasks': ['*'], 'interval': {'hours': 1}}]):
                tasks = item['tasks']
                if not isinstance(tasks, list):
                    tasks = [tasks]
                self.triggers.append(Trigger(item['interval'], tasks, options={'cron': True}))

    def execute(self, options=None, output=None, priority=1, trigger_id=None):
        """
        Add a task to the scheduler to be run immediately.

        :param options: Either an :class:`argparse.Namespace` instance, or a dict, containing options for execution
        :param output: If a file-like object is specified here, log messages and stdout from the execution will be
            written to it.
        :param priority: If there are other executions waiting to be run, they will be run in priority order,
            lowest first.
        :param trigger_id: If a trigger_id is specified, it will be attached to the :class:`Job` instance added to the
            run queue. Used to check that triggers are not fired faster than they can be executed.
        :returns: a list of :class:`threading.Event` instances which will be
            set when each respective task has finished running
        """
        if options is None:
            options = copy.copy(self.manager.options.execute)
        elif isinstance(options, dict):
            options_namespace = copy.copy(self.manager.options.execute)
            options_namespace.__dict__.update(options)
            options = options_namespace
        tasks = self.manager.tasks
        # Handle --tasks
        if options.tasks:
            # Create list of tasks to run, preserving order
            tasks = []
            for arg in options.tasks:
                matches = [t for t in self.manager.tasks if fnmatch.fnmatchcase(unicode(t).lower(), arg.lower())]
                if not matches:
                    log.error('`%s` does not match any tasks' % arg)
                    continue
                tasks.extend(m for m in matches if m not in tasks)
            # Set the option as a list of matching task names so plugins can use it easily
            options.tasks = tasks
        # TODO: 1.2 This is a hack to make task priorities work still, not sure if it's the best one
        tasks = sorted(tasks, key=lambda t: self.manager.config['tasks'][t].get('priority', 65535))

        finished_events = []
        for task in tasks:
            job = Job(task, options=options, output=output, priority=priority, trigger_id=trigger_id)
            self.run_queue.put(job)
            finished_events.append(job.finished_event)
        return finished_events

    def queue_pending_jobs(self):
        # Add pending jobs to the run queue
        with self.triggers_lock:
            for trigger in self.triggers:
                if trigger.should_run:
                    with self.run_queue.mutex:
                        if any(j.trigger_id == trigger.uid for j in self.run_queue.queue):
                            log.error('Not firing schedule %r. Tasks from last run have still not finished.' % trigger)
                            log.error('You may need to increase the interval for this schedule.')
                            continue
                    options = dict(trigger.options)
                    # If the user has specified all tasks with '*', don't add tasks option at all, so that manual
                    # tasks are not executed
                    if trigger.tasks != ['*']:
                        options['tasks'] = trigger.tasks
                    self.execute(options=options, priority=5, trigger_id=trigger.uid)
                    trigger.trigger()

    def start(self, run_schedules=None):
        if run_schedules is not None:
            self.run_schedules = run_schedules
        super(Scheduler, self).start()

    def run(self):
        from flexget.task import Task, TaskAbort
        while not self._shutdown_now:
            if self.run_schedules:
                self.queue_pending_jobs()
            # Grab the first job from the run queue and do it
            try:
                job = self.run_queue.get(timeout=0.5)
            except Queue.Empty:
                if self._shutdown_when_finished:
                    self._shutdown_now = True
                continue
            if job.output:
                # Hook up our log and stdout to give back to the requester
                old_stdout, old_stderr = sys.stdout, sys.stderr
                sys.stdout, sys.stderr = Tee(job.output, sys.stdout), Tee(job.output, sys.stderr)
                # TODO: Use a filter to capture only the logging for this execution?
                streamhandler = logging.StreamHandler(job.output)
                streamhandler.setFormatter(FlexGetFormatter())
                logging.getLogger().addHandler(streamhandler)
            try:
                Task(self.manager, job.task, options=job.options).execute()
            except TaskAbort as e:
                log.debug('task %s aborted: %r' % (job.task, e))
            finally:
                self.run_queue.task_done()
                job.finished_event.set()
                if job.output:
                    sys.stdout, sys.stderr = old_stdout, old_stderr
                    logging.getLogger().removeHandler(streamhandler)
        remaining_jobs = self.run_queue.qsize()
        if remaining_jobs:
            log.warning('Scheduler shut down with %s jobs remaining in the queue to run.' % remaining_jobs)
        log.debug('scheduler shut down')

    def wait(self):
        """
        Waits for the thread to exit.
        Similar to :method:`Thread.join`, except it allows ctrl-c to be caught still.
        """
        while self.is_alive():
            time.sleep(0.1)

    def shutdown(self, finish_queue=True):
        """
        Ends the thread. If a job is running, waits for it to finish first.

        :param bool finish_queue: If this is True, shutdown will wait until all queued tasks have finished.
        """
        if finish_queue:
            self._shutdown_when_finished = True
        else:
            self._shutdown_now = True


class Job(object):
    """A job for the scheduler to execute."""
    #: Used to determine which job to run first when multiple jobs are waiting.
    priority = 1
    #: The name of the task to execute
    task = None
    #: Options to run the task with
    options = None
    #: :class:`BufferQueue` to write the task execution output to. '[[END]]' will be sent to the queue when complete
    output = None
    # Used to keep jobs in order, when priority is the same
    _counter = itertools.count()

    def __init__(self, task, options=None, output=None, priority=1, trigger_id=None):
        self.task = task
        self.options = options
        self.output = output
        self.priority = priority
        self.count = next(self._counter)
        self.finished_event = threading.Event()
        # Used to make sure a certain trigger doesn't add jobs faster than they can run
        self.trigger_id = trigger_id
        # Lower priority if cron flag is present in either dict or Namespace form
        try:
            cron = self.options.cron
        except AttributeError:
            try:
                cron = self.options.get('cron')
            except AttributeError:
                cron = False
        if cron:
            self.priority = 5

    def __lt__(self, other):
        return (self.priority, self.count) < (other.priority, other.count)


class Trigger(object):
    def __init__(self, interval, tasks, options=None):
        """
        :param dict interval: An interval dictionary from the config.
        :param list tasks: List of task names specified to run. Wildcards are allowed.
        :param dict options: Dictionary of options that should be applied to this run.
        """
        self.tasks = tasks
        self.options = options
        self.unit = None
        self.amount = None
        self.on_day = None
        self.at_time = None
        self.last_run = None
        self.run_at = None
        self.interval = interval
        self._get_db_last_run()
        self.schedule_next_run()

    @property
    def uid(self):
        """A unique id which describes this trigger."""
        # Determine uniqueness based on interval,
        hashval = md5(str(sorted(self.interval)))
        # and tasks run on that interval.
        hashval.update(','.join(self.tasks).encode('utf-8'))
        return hashval.hexdigest()

    # Handles getting and setting interval in form validated by config
    @property
    def interval(self):
        interval = {self.unit: self.amount}
        if self.at_time:
            interval['at_time'] = self.at_time
        if self.on_day:
            interval['on_day'] = self.on_day
        return interval

    @interval.setter
    def interval(self, interval):
        if not interval:
            for attr in ['unit', 'amount', 'on_day', 'at_time']:
                setattr(self, attr, None)
            return
        for unit in UNITS:
            self.amount = interval.pop(unit, None)
            if self.amount:
                self.unit = unit
                break
        else:
            raise ValueError('Schedule interval must provide a unit and amount')
        self.at_time = interval.pop('at_time', None)
        if self.at_time and not isinstance(self.at_time, dt_time):
            self.at_time = parse_time(self.at_time)
        self.on_day = interval.pop('on_day', None)
        if interval:
            raise ValueError('the following are not valid keys in a schedule interval dictionary: %s' %
                             ', '.join(interval))
        self.schedule_next_run()

    def trigger(self):
        """Call when trigger is activated. Records current run time and schedules next run."""
        self.last_run = datetime.now()
        self._set_db_last_run()
        self.schedule_next_run()

    @property
    def should_run(self):
        return self.run_at and datetime.now() >= self.run_at

    @property
    def period(self):
        return timedelta(**{self.unit: self.amount})

    def schedule_next_run(self):
        last_run = self.last_run
        if not last_run:
            # Pretend we ran one period ago
            last_run = datetime.now() - self.period
        if self.on_day:
            days_ahead = WEEKDAYS.index(self.on_day) - last_run.weekday()
            if days_ahead <= 0:  # Target day already happened this week
                days_ahead += 7
            self.run_at = last_run + timedelta(days=days_ahead, weeks=self.amount-1)
        else:
            self.run_at = last_run + self.period
        if self.at_time:
            self.run_at = self.run_at.replace(hour=self.at_time.hour, minute=self.at_time.minute,
                                              second=self.at_time.second)

    def _get_db_last_run(self):
        session = Session()
        try:
            db_trigger = session.query(DBTrigger).get(self.uid)
            if db_trigger:
                self.last_run = db_trigger.last_run
                log.debug('loaded last_run from the database')
        finally:
            session.close()

    def _set_db_last_run(self):
        session = Session()
        try:
            db_trigger = session.query(DBTrigger).get(self.uid)
            if not db_trigger:
                db_trigger = DBTrigger(self.uid)
                session.add(db_trigger)
            db_trigger.last_run = self.last_run
            session.commit()
        finally:
            session.close()
        log.debug('recorded last_run to the database')

    def __repr__(self):
        return 'Trigger(tasks=%r, amount=%r, unit=%r)' % (self.tasks, self.amount, self.unit)


class Tee(object):
    """Used so that output to sys.stdout can be grabbed and still displayed."""
    def __init__(self, *files):
        self.files = files

    def __getattr__(self, meth):
        def method_runner(*args, **kwargs):
            for f in self.files:
                try:
                    getattr(f, meth)(*args, **kwargs)
                except AttributeError:
                    # We don't really care if all of our 'files' fully support the file api
                    pass
        return method_runner


class BufferQueue(Queue.Queue):
    """Used in place of a file-like object to capture text and access it safely from another thread."""
    # Allow access to the Empty error from here
    Empty = Queue.Empty

    def write(self, line):
        self.put(line)


@event('config.register')
def register_config():
    register_config_key('schedules', main_schema)

########NEW FILE########
__FILENAME__ = task
from __future__ import unicode_literals, division, absolute_import
import copy
from functools import wraps
import hashlib
import itertools
import logging

from sqlalchemy import Column, Unicode, String, Integer

from flexget import config_schema
from flexget import db_schema
from flexget.entry import EntryUnicodeError
from flexget.event import fire_event, event
from flexget.manager import Session
from flexget.plugin import (get_plugins, task_phases, phase_methods, PluginWarning, PluginError,
                            DependencyError, plugins as all_plugins, plugin_schemas)
from flexget.utils import requests
from flexget.utils.simple_persistence import SimpleTaskPersistence

log = logging.getLogger('task')
Base = db_schema.versioned_base('feed', 0)


class TaskConfigHash(Base):
    """Stores the config hash for tasks so that we can tell if the config has changed since last run."""

    __tablename__ = 'feed_config_hash'

    id = Column(Integer, primary_key=True)
    task = Column('name', Unicode, index=True, nullable=False)
    hash = Column('hash', String)

    def __repr__(self):
        return '<TaskConfigHash(task=%s,hash=%s)>' % (self.task, self.hash)


def config_changed(task):
    """Forces config_modified flag to come out true on next run. Used when the db changes, and all
    entries need to be reprocessed."""
    log.debug('Marking config as changed.')
    session = Session()
    try:
        task_hash = session.query(TaskConfigHash).filter(TaskConfigHash.task == task).first()
        if task_hash:
            task_hash.hash = ''
        session.commit()
    finally:
        session.close()


def useTaskLogging(func):

    @wraps(func)
    def wrapper(self, *args, **kw):
        # Set the task name in the logger
        from flexget import logger
        logger.set_task(self.name)
        try:
            return func(self, *args, **kw)
        finally:
            logger.set_task('')

    return wrapper


class EntryIterator(object):
    """An iterator over a subset of entries to emulate old task.accepted/rejected/failed/entries properties."""

    def __init__(self, entries, states):
        self.all_entries = entries
        if isinstance(states, basestring):
            states = [states]
        self.filter = lambda e: e._state in states

    def __iter__(self):
        return itertools.ifilter(self.filter, self.all_entries)

    def __bool__(self):
        return any(e for e in self)

    def __len__(self):
        return sum(1 for e in self)

    def __add__(self, other):
        return itertools.chain(self, other)

    def __radd__(self, other):
        return itertools.chain(other, self)

    def __getitem__(self, item):
        if not isinstance(item, int):
            raise ValueError('Index must be integer.')
        for index, entry in enumerate(self):
            if index == item:
                return entry
        else:
            raise IndexError('%d is out of bounds' % item)

    def __getslice__(self, a, b):
        return list(itertools.islice(self, a, b))

    def reverse(self):
        self.all_entries.sort(reverse=True)

    def sort(self, *args, **kwargs):
        self.all_entries.sort(*args, **kwargs)


class EntryContainer(list):
    """Container for a list of entries, also contains accepted, rejected failed iterators over them."""

    def __init__(self, iterable=None):
        list.__init__(self, iterable or [])

        self._entries = EntryIterator(self, ['undecided', 'accepted'])
        self._accepted = EntryIterator(self, 'accepted')  # accepted entries, can still be rejected
        self._rejected = EntryIterator(self, 'rejected')  # rejected entries, can not be accepted
        self._failed = EntryIterator(self, 'failed')  # failed entries
        self._undecided = EntryIterator(self, 'undecided')  # undecided entries (default)

    # Make these read-only properties
    entries = property(lambda self: self._entries)
    accepted = property(lambda self: self._accepted)
    rejected = property(lambda self: self._rejected)
    failed = property(lambda self: self._failed)
    undecided = property(lambda self: self._undecided)

    def __repr__(self):
        return '<EntryContainer(%s)>' % list.__repr__(self)


class TaskAbort(Exception):
    def __init__(self, reason, silent=False):
        self.reason = reason
        self.silent = silent

    def __repr__(self):
        return 'TaskAbort(reason=%s, silent=%s)' % (self.reason, self.silent)


class Task(object):

    """
    Represents one task in the configuration.

    **Fires events:**

    * task.execute.before_plugin

      Before a plugin is about to be executed. Note that since this will also include all
      builtin plugins the amount of calls can be quite high

      ``parameters: task, keyword``

    * task.execute.after_plugin

      After a plugin has been executed.

      ``parameters: task, keyword``

    * task.execute.completed

      After task execution has been completed

      ``parameters: task``

    """

    max_reruns = 5

    def __init__(self, manager, name, config=None, options=None):
        """
        :param Manager manager: Manager instance.
        :param string name: Name of the task.
        :param dict config: Task configuration.
        """
        self.name = unicode(name)
        self.manager = manager
        # raw_config should remain the untouched input config
        if config is None:
            config = manager.config['tasks'].get(name, {})
        self.config = copy.deepcopy(config)
        self.prepared_config = None
        if options is None:
            options = copy.copy(self.manager.options.execute)
        elif isinstance(options, dict):
            options_namespace = copy.copy(self.manager.options.execute)
            options_namespace.__dict__.update(options)
            options = options_namespace
        self.options = options

        # simple persistence
        self.simple_persistence = SimpleTaskPersistence(self)

        # not to be reset
        self._rerun_count = 0

        self.config_modified = None

        # use reset to init variables when creating
        self._reset()

    @property
    def undecided(self):
        """
        .. deprecated:: Use API v3
        """
        return self.all_entries.undecided

    @property
    def failed(self):
        """
        .. deprecated:: Use API v3
        """
        return self.all_entries.failed

    @property
    def rejected(self):
        """
        .. deprecated:: Use API v3
        """
        return self.all_entries.rejected

    @property
    def accepted(self):
        """
        .. deprecated:: Use API v3
        """
        return self.all_entries.accepted

    @property
    def entries(self):
        """
        .. deprecated:: Use API v3
        """
        return self.all_entries.entries

    @property
    def all_entries(self):
        """
        .. deprecated:: Use API v3
        """
        return self._all_entries

    @property
    def is_rerun(self):
        return self._rerun_count

    # TODO: can we get rid of this now that Tasks are instantiated on demand?
    def _reset(self):
        """Reset task state"""
        log.debug('resetting %s' % self.name)
        self.enabled = not self.name.startswith('_')
        self.session = None
        self.priority = 65535

        self.requests = requests.Session()

        # List of all entries in the task
        self._all_entries = EntryContainer()

        self.disabled_phases = []

        # These are just to query what happened in task. Call task.abort to set.
        self.aborted = False
        self.abort_reason = None
        self.silent_abort = False

        self._rerun = False

        # current state
        self.current_phase = None
        self.current_plugin = None

    def __cmp__(self, other):
        return cmp(self.priority, other.priority)

    def __str__(self):
        return '<Task(name=%s,aborted=%s)>' % (self.name, self.aborted)

    def disable_phase(self, phase):
        """Disable ``phase`` from execution.

        All disabled phases are re-enabled by :meth:`Task._reset()` after task
        execution has been completed.

        :param string phase: Name of ``phase``
        :raises ValueError: *phase* could not be found.
        """
        if phase not in task_phases:
            raise ValueError('%s is not a valid phase' % phase)
        if phase not in self.disabled_phases:
            log.debug('Disabling %s phase' % phase)
            self.disabled_phases.append(phase)

    def abort(self, reason='Unknown', silent=False):
        """Abort this task execution, no more plugins will be executed except the abort handling ones."""
        self.aborted = True
        self.abort_reason = reason
        self.silent_abort = silent
        if not self.silent_abort:
            log.warning('Aborting task (plugin: %s)' % self.current_plugin)
        else:
            log.debug('Aborting task (plugin: %s)' % self.current_plugin)
        raise TaskAbort(reason, silent=silent)

    def find_entry(self, category='entries', **values):
        """
        Find and return :class:`~flexget.entry.Entry` with given attributes from task or None

        :param string category: entries, accepted, rejected or failed. Defaults to entries.
        :param values: Key values of entries to be searched
        :return: Entry or None
        """
        cat = getattr(self, category)
        if not isinstance(cat, EntryIterator):
            raise TypeError('category must be a EntryIterator')
        for entry in cat:
            for k, v in values.iteritems():
                if not (k in entry and entry[k] == v):
                    break
            else:
                return entry
        return None

    def plugins(self, phase=None):
        """Get currently enabled plugins.

        :param string phase:
          Optional, limits to plugins currently configured on given phase, sorted in phase order.
        :return:
          An iterator over configured :class:`flexget.plugin.PluginInfo` instances enabled on this task.
        """
        if phase:
            plugins = sorted(get_plugins(phase=phase), key=lambda p: p.phase_handlers[phase], reverse=True)
        else:
            plugins = all_plugins.itervalues()
        return (p for p in plugins if p.name in self.config or p.builtin)

    def __run_task_phase(self, phase):
        """Executes task phase, ie. call all enabled plugins on the task.

        Fires events:

        * task.execute.before_plugin
        * task.execute.after_plugin

        :param string phase: Name of the phase
        """
        if phase not in phase_methods:
            raise Exception('%s is not a valid task phase' % phase)
        # warn if no inputs, filters or outputs in the task
        if phase in ['input', 'filter', 'output']:
            if not self.manager.unit_test:
                # Check that there is at least one manually configured plugin for these phases
                for p in self.plugins(phase):
                    if not p.builtin:
                        break
                else:
                    log.warning('Task doesn\'t have any %s plugins, you should add (at least) one!' % phase)

        for plugin in self.plugins(phase):
            # Abort this phase if one of the plugins disables it
            if phase in self.disabled_phases:
                return
            # store execute info, except during entry events
            self.current_phase = phase
            self.current_plugin = plugin.name

            if plugin.api_ver == 1:
                # backwards compatibility
                # pass method only task (old behaviour)
                args = (self,)
            else:
                # pass method task, copy of config (so plugin cannot modify it)
                args = (self, copy.copy(self.config.get(plugin.name)))

            try:
                fire_event('task.execute.before_plugin', self, plugin.name)
                response = self.__run_plugin(plugin, phase, args)
                if phase == 'input' and response:
                    # add entries returned by input to self.all_entries
                    for e in response:
                        e.task = self
                    self.all_entries.extend(response)
            finally:
                fire_event('task.execute.after_plugin', self, plugin.name)

    def __run_plugin(self, plugin, phase, args=None, kwargs=None):
        """
        Execute given plugins phase method, with supplied args and kwargs.
        If plugin throws unexpected exceptions :meth:`abort` will be called.

        :param PluginInfo plugin: Plugin to be executed
        :param string phase: Name of the phase to be executed
        :param args: Passed to the plugin
        :param kwargs: Passed to the plugin
        """
        keyword = plugin.name
        method = plugin.phase_handlers[phase]
        if args is None:
            args = []
        if kwargs is None:
            kwargs = {}

        # log.trace('Running %s method %s' % (keyword, method))
        # call the plugin
        try:
            return method(*args, **kwargs)
        except TaskAbort:
            raise
        except PluginWarning as warn:
            # check if this warning should be logged only once (may keep repeating)
            if warn.kwargs.get('log_once', False):
                from flexget.utils.log import log_once
                log_once(warn.value, warn.log)
            else:
                warn.log.warning(warn)
        except EntryUnicodeError as eue:
            msg = ('Plugin %s tried to create non-unicode compatible entry (key: %s, value: %r)' %
                   (keyword, eue.key, eue.value))
            log.critical(msg)
            self.abort(msg)
        except PluginError as err:
            err.log.critical(err.value)
            self.abort(err.value)
        except DependencyError as e:
            msg = ('Plugin `%s` cannot be used because dependency `%s` is missing.' %
                   (keyword, e.missing))
            log.critical(msg)
            log.debug(e.message)
            self.abort(msg)
        except Warning as e:
            # If warnings have been elevated to errors
            msg = 'Warning during plugin %s: %s' % (keyword, e)
            log.exception(msg)
            self.abort(msg)
        except Exception as e:
            msg = 'BUG: Unhandled error in plugin %s: %s' % (keyword, e)
            log.exception(msg)
            self.abort(msg)

    def rerun(self):
        """Immediately re-run the task after execute has completed,
        task can be re-run up to :attr:`.max_reruns` times."""
        msg = 'Plugin %s has requested task to be ran again after execution has completed.' % self.current_plugin
        # Only print the first request for a rerun to the info log
        log.debug(msg) if self._rerun else log.info(msg)
        if self._rerun_count >= self.max_reruns:
            self._rerun = False
            log.info('Task has been re-run %s times already, stopping for now' % self._rerun_count)
            return
        self._rerun = True

    def config_changed(self):
        """
        Sets config_modified flag to True for the remainder of this run.
        Used when the db changes, and all entries need to be reprocessed.
        """
        self.config_modified = True

    @useTaskLogging
    def execute(self):
        """
        Executes the the task.

        If :attr:`.enabled` is False task is not executed. Certain :attr:`.options`
        affect how execution is handled.

        - :attr:`.options.disable_phases` is a list of phases that are not enabled
          for this execution.
        - :attr:`.options.inject` is a list of :class:`Entry` instances used instead
          of running input phase.
        """
        if not self.enabled:
            log.debug('Not running disabled task %s' % self.name)
        if self.options.cron:
            self.manager.db_cleanup()

        self._reset()
        log.debug('executing %s' % self.name)
        if not self.enabled:
            log.debug('task %s disabled during preparation, not running' % self.name)
            return

        # Handle keyword args
        if self.options.learn:
            log.info('Disabling download and output phases because of --learn')
            self.disable_phase('download')
            self.disable_phase('output')
        if self.options.disable_phases:
            map(self.disable_phase, self.options.disable_phases)
        if self.options.inject:
            # If entries are passed for this execution (eg. rerun), disable the input phase
            self.disable_phase('input')
            self.all_entries.extend(self.options.inject)

        log.debug('starting session')
        self.session = Session()

        # Save current config hash and set config_modidied flag
        config_hash = hashlib.md5(str(sorted(self.config.items()))).hexdigest()
        last_hash = self.session.query(TaskConfigHash).filter(TaskConfigHash.task == self.name).first()
        if self.is_rerun:
            # Restore the config to state right after start phase
            if self.prepared_config:
                self.config = copy.deepcopy(self.prepared_config)
            else:
                log.error('BUG: No prepared_config on rerun, please report.')
            self.config_modified = False
        elif not last_hash:
            self.config_modified = True
            last_hash = TaskConfigHash(task=self.name, hash=config_hash)
            self.session.add(last_hash)
        elif last_hash.hash != config_hash:
            self.config_modified = True
            last_hash.hash = config_hash
        else:
            self.config_modified = False

        # run phases
        try:
            for phase in task_phases:
                if phase in self.disabled_phases:
                    # log keywords not executed
                    for plugin in self.plugins(phase):
                        if plugin.name in self.config:
                            log.info('Plugin %s is not executed because %s phase is disabled (e.g. --test)' %
                                     (plugin.name, phase))
                    continue
                if phase == 'start' and self.is_rerun:
                    log.debug('skipping task_start during rerun')
                elif phase == 'exit' and self._rerun:
                    log.debug('not running task_exit yet because task will rerun')
                else:
                    # run all plugins with this phase
                    self.__run_task_phase(phase)
                    if phase == 'start':
                        # Store a copy of the config state after start phase to restore for reruns
                        self.prepared_config = copy.deepcopy(self.config)
        except TaskAbort:
            # Roll back the session before calling abort handlers
            self.session.rollback()
            try:
                self.__run_task_phase('abort')
                # Commit just the abort handler changes if no exceptions are raised there
                self.session.commit()
            except TaskAbort as e:
                log.exception('abort handlers aborted: %s' % e)
            raise
        else:
            for entry in self.all_entries:
                entry.complete()
            log.debug('committing session')
            self.session.commit()
            fire_event('task.execute.completed', self)
        finally:
            # this will cause database rollback on exception
            self.session.close()

        # rerun task
        if self._rerun:
            log.info('Rerunning the task in case better resolution can be achieved.')
            self._rerun_count += 1
            # TODO: Potential optimization is to take snapshots (maybe make the ones backlog uses built in instead of
            # taking another one) after input and just inject the same entries for the rerun
            self.execute()

    def __eq__(self, other):
        if hasattr(other, 'name'):
            return self.name == other.name
        return NotImplemented

    def __copy__(self):
        new = type(self)(self.manager, self.name, self.config, self.options)
        # Update all the variables of new instance to match our own
        new.__dict__.update(self.__dict__)
        # Some mutable objects need to be copies
        new.options = copy.copy(self.options)
        new.config = copy.deepcopy(self.config)
        return new

    copy = __copy__


@event('config.register')
def register_config_key():
    task_config_schema = {
        'type': 'object',
        'additionalProperties': plugin_schemas(context='task')
    }

    config_schema.register_config_key('tasks', task_config_schema, required=True)

########NEW FILE########
__FILENAME__ = api
from flask import request, jsonify, Blueprint, Response, flash

import flexget
from flexget.config_schema import resolve_ref, process_config, get_schema
from flexget.manager import manager
from flexget.options import get_parser
from flexget.plugin import plugin_schemas
from flexget.scheduler import BufferQueue

API_VERSION = 1

api = Blueprint('api', __name__, url_prefix='/api')
# Serves the appropriate schema for any /api method. Schema for /api/x/y can be found at /schema/api/x/y
api_schema = Blueprint('api_schema', __name__, url_prefix='/schema/api')


@api.after_request
def attach_schema(response):
    # TODO: Check if /schema/ourpath exists
    schema_path = '/schema' + request.path
    response.headers[b'Content-Type'] += '; profile=%s' % schema_path
    return response


@api_schema.route('/version')
def version_schema():
    return jsonify({
        'type': 'object',
        'properties': {
            'flexget_version': {'type': 'string', 'description': 'FlexGet version string'},
            'api_version': {'type': 'integer', 'description': 'Version of the json api'}
        }
    })


@api.route('/version')
def version():
    return jsonify(flexget_version=flexget.__version__, api_version=API_VERSION)


exec_parser = get_parser('execute')


@api.route('/execute', methods=['GET', 'POST'])
def execute():
    kwargs = request.json or {}
    options_string = kwargs.pop('options_string', '')
    if options_string:
        try:
            kwargs['options'] = exec_parser.parse_args(options_string, raise_errors=True).execute
        except ValueError as e:
            return jsonify(error='invalid options_string specified: %s' % e.message), 400

    # We'll stream the log results as they arrive in the bufferqueue
    kwargs['output'] = BufferQueue()
    manager.scheduler.execute(**kwargs)

    return Response(kwargs['output'], mimetype='text/plain'), 200


task_schema = {
    'type': 'object',
    'properties': {
        'name': {'type': 'string', 'description': 'The name of this task.'},
        'config': plugin_schemas(context='task')
    },
    'required': ['name'],
    'additionalProperties': False,
    'links': [
        {'rel': 'self', 'href': '/api/tasks/{name}/'},
        {'rel': 'edit', 'method': 'PUT', 'href': '', 'schema': {'$ref': '#'}},
        {'rel': 'delete', 'method': 'DELETE', 'href': ''}
    ]
}

tasks_schema = {
    'type': 'object',
    'properties': {
        'tasks': {
            'type': 'array',
            'items': {'$ref': '/schema/api/tasks/task'},
            'links': [
                {'rel': 'add', 'method': 'POST', 'href': '/api/tasks/', 'schema': {'$ref': '/schema/api/tasks/task'}}
            ]
        }
    }
}


# TODO: Maybe these should be in /config/tasks
@api_schema.route('/tasks/')
def schema_tasks():
    return jsonify(tasks_schema)


@api.route('/tasks/', methods=['GET', 'POST'])
def api_tasks():
    if request.method == 'GET':
        tasks = []
        for name in manager.tasks:
            tasks.append({'name': name, 'config': manager.config['tasks'][name]})
        return jsonify(tasks=tasks)
    elif request.method == 'POST':
        # TODO: Validate and add task
        pass


@api_schema.route('/tasks/<task>/')
def schema_task(task):
    return jsonify(task_schema)


@api.route('/tasks/<task>/', methods=['GET', 'PUT', 'DELETE'])
def api_task(task):
    if request.method == 'GET':
        if not task in manager.tasks:
            return jsonify(error='task {task} not found'.format(task=task)), 404
        return jsonify({'name': task, 'config': manager.config[task]})
    elif request.method == 'PUT':
        # TODO: Validate then set
        # TODO: Return 204 if name has been changed
        pass
    elif request.method == 'DELETE':
        manager.config['tasks'].pop(task)


@api_schema.route('/config/')
def cs_root():
    root_schema = get_schema()
    hyper_schema = root_schema.copy()
    hyper_schema['links'] = [{'rel': 'self', 'href': '/api/config/'}]
    hyper_schema['properties'] = root_schema.get('properties', {}).copy()
    hs_props = hyper_schema['properties']
    for key, key_schema in root_schema.get('properties', {}).iteritems():
        hs_props[key] = hs_props[key].copy()
        hs_props[key]['links'] = [{'rel': 'self', 'href': key}]
        if key not in root_schema.get('required', []):
            hs_props[key]['links'].append({'rel': 'delete', 'href': '', 'method': 'DELETE'})
    return jsonify(hyper_schema)


# TODO: none of these should allow setting invalid config
@api.route('/config/', methods=['GET', 'PUT'])
def config_root():
    return jsonify(manager.config)


@api_schema.route('/config/<section>')
def schema_config_section(section):
    return jsonify(resolve_ref('/schema/config/%s' % section))


@api.route('/config/<section>/', methods=['GET', 'PUT', 'DELETE'])
def config_section(section):
    if request.method == 'PUT':
        schema = resolve_ref('/schema/config/%s' % section)
        errors = process_config(request.json, schema, set_defaults=False)
        if errors:
            return jsonify({'$errors': errors}), 400
        manager.config[section] = request.json
    if section not in manager.config:
        return jsonify(error='Not found'), 404
    if request.method == 'DELETE':
        del manager.config[section]
        return Response(status=204)
    response = jsonify(manager.config[section])
    response.headers[b'Content-Type'] += '; profile=/schema/config/%s' % section
    return response


# TODO: Abandon this and move above task handlers into /config?
@api.route('/config/tasks/<taskname>/', methods=['GET', 'PUT', 'DELETE'])
def config_tasks(taskname):
    if request.method != 'PUT':
        if taskname not in manager.config['tasks']:
            return jsonify(error='Requested task does not exist'), 404
    status_code = 200
    if request.method == 'PUT':
        if 'rename' in request.args:
            pass  # TODO: Rename the task, return 204 with new location header
        if taskname not in manager.config['tasks']:
            status_code = 201
        manager.config['tasks'][taskname] = request.json
    elif request.method == 'DELETE':
        del manager.config['tasks'][taskname]
        return Response(status=204)
    return jsonify(manager.config['tasks'][taskname]), status_code


# TODO: Move this route to template plugin
@api_schema.route('/config/templates/', defaults={'section': 'templates'})
@api_schema.route('/config/tasks/', defaults={'section': 'tasks'})
def cs_task_container(section):
    hyper_schema = {'links': [{'rel': 'create',
                               'href': '',
                               'method': 'POST',
                               'schema': {
                                   'type': 'object',
                                   'properties': {'name': {'type': 'string'}},
                                   'required': ['name']}}]}


# TODO: Move this route to template plugin
@api_schema.route('/config/templates/<name>', defaults={'section': 'templates'})
@api_schema.route('/config/tasks/<name>', defaults={'section': 'tasks'})
def cs_plugin_container(section, name):
    return plugin_schemas(context='task')

########NEW FILE########
__FILENAME__ = archive
from __future__ import unicode_literals, division, absolute_import
import logging
from flexget.ui.webui import db_session, app
from flask import request, render_template, flash, Blueprint
from flexget.plugin import DependencyError

try:
    from flexget.plugins.generic.archive import ArchiveEntry, search
except ImportError:
    raise DependencyError(issued_by='ui.archive', missing='archive')

log = logging.getLogger('ui.archive')
archive = Blueprint('archive', __name__)


# TODO: refactor this filter to some globally usable place (webui.py?)
#       also flexget/plugins/ui/utils.py needs to be removed
#       ... mainly because we have flexget/utils for that :)


@app.template_filter('pretty_age')
def pretty_age_filter(value):
    import time
    from flexget.ui.utils import pretty_date
    return pretty_date(time.mktime(value.timetuple()))


@archive.route('/', methods=['POST', 'GET'])
def index():
    context = {}
    if request.method == 'POST':
        text = request.form.get('keyword', None)
        if text == '':
            flash('Empty search?', 'error')
        elif len(text) < 5:
            flash('Search text is too short, use at least 5 characters', 'error')
        else:
            results = search(db_session, text)
            if not results:
                flash('No results', 'info')
            else:
                # not sure if this len check is a good idea, I think it forces to load all items from db ?
                if len(results) > 500:
                    flash('Too many results, displaying first 500', 'error')
                    results = results[0:500]
                context['results'] = results
    return render_template('archive/archive.html', **context)


@archive.route('/count')
def count():
    log.debug('getting count for archive')
    return str(db_session.query(ArchiveEntry).count())


# TODO: Fix this
@archive.route('/inject/<id>')
def inject(id):
    # TODO: Do it like the cli command does
    options = {'archive_inject_id': id, 'archive_inject_immortal': True}
    executor.execute(options=options)
    flash('Queued execution, see log for results', 'info')
    return render_template('archive/archive.html')


# TODO: Requires refactoring once archive plugin is refactored
#register_plugin(archive, menu='Archive')

########NEW FILE########
__FILENAME__ = authentication
from __future__ import unicode_literals, division, absolute_import
import logging

from sqlalchemy import Column, Unicode
from flask import Blueprint, request, Response

from flexget.event import event
from flexget.ui.webui import register_plugin, app, manager, db_session
from flexget.manager import Base

log = logging.getLogger('ui.authentication')
auth = Blueprint('authentication', __name__)
credentials = None


class AuthCredentials(Base):
    __tablename__ = 'authentication'

    username = Column(Unicode, primary_key=True)
    password = Column(Unicode)

    def __init__(self, username, password):
        self.username = username
        self.password = password


def check_auth(username, password):
    """This function is called to check if a username /
    password combination is valid.
    """
    return username == credentials.username and password == credentials.password


def authenticate():
    """Sends a 401 response that enables basic auth"""
    return Response(
        'Could not verify your access level for that URL.\n'
        'You have to login with proper credentials', 401,
        {b'WWW-Authenticate': b'Basic realm="Login Required"'})


@event('webui.start')
def enable_authentication():
    if manager.options.webui.no_auth:
        return
    global credentials
    credentials = db_session.query(AuthCredentials).first()
    if not credentials:
        credentials = AuthCredentials('flexget', 'flexget')
        db_session.add(credentials)

    if manager.options.webui.username:
        credentials.username = manager.options.username
    if manager.options.webui.password:
        credentials.password = manager.options.webui.password
    db_session.commit()

    app.before_request(check_authenticated)


def check_authenticated():
    # TODO: Is this a big security hole? Maybe figure out a better way to authenticate for local IPC
    if manager.options.webui.no_local_auth and request.remote_addr == '127.0.0.1':
        return
    auth = request.authorization
    if not auth or not check_auth(auth.username, auth.password):
        return authenticate()


register_plugin(auth)

########NEW FILE########
__FILENAME__ = config
from __future__ import unicode_literals, division, absolute_import
import logging

from flask import request, Blueprint, jsonify, json

from flexget.ui.webui import manager, register_plugin


config = Blueprint('config', __name__)

log = logging.getLogger('ui.config')


@config.route('/', methods=['GET', 'POST'])
def root():
    if request.method == 'POST':
        manager.config = json.loads(request.data)
    result = jsonify(manager.config)
    # TODO: This should not be hard coded
    result.headers[b'Content-Type'] += '; profile=/schema/root'
    return result


def getset_item(obj, item, set=None):
    """
    Get `item` in either a dict or a list `obj`
    If `set` is specified, `item` in `obj` will be set to `set`s value
    """
    # We don't know whether path elements are strings or ints, so try both
    for func in [lambda x: x, int]:
        try:
            item = func(item)
            if set is not None:
                obj[item] = set
            return obj[item]
        except (TypeError, LookupError, ValueError):
            pass
    raise LookupError('%s not in config' % item)


@config.route('/<path:path>', methods=['GET', 'POST'])
def with_path(path):
    path = path.split('/')
    result = manager.config
    try:
        for elem in path[:-1]:
            result = getset_item(result, elem)
        elem = path[-1]
        if request.method == 'POST':
            getset_item(result, elem, set=json.loads(request.data))
        result = jsonify(getset_item(result, elem))
    except LookupError as e:
        return unicode(e), 404
    # TODO: This should not be hard coded
    if len(path) == 2 and path[0] in ['tasks', 'presets']:
        result.headers[b'Content-Type'] += '; profile=/schema/plugins?context=task'
    return result


register_plugin(config)

########NEW FILE########
__FILENAME__ = configure
from __future__ import unicode_literals, division, absolute_import
from flexget.ui.webui import manager, register_plugin, app
from flexget.task import Task
from flask import render_template, request, flash, redirect, Blueprint
import yaml
import logging
from flask.helpers import url_for

configure = Blueprint('configure', __name__)

log = logging.getLogger('ui.configure')


@configure.route('/')
def index():
    return render_template('configure/configure.html')


@configure.route('/new/<root>', methods=['POST', 'GET'])
def new_text(root):
    if root not in ['tasks', 'presets']:
        flash('Invalid root.', 'error')
        return redirect(url_for('.index'))
    config_type = root.rstrip('s')
    context = {'root': root,
               'config_type': config_type}
    if request.method == 'POST':
        context['config'] = request.form.get('config')
        name = request.form.get('name')
        if not name:
            flash('You must enter a name for this %s' % config_type, 'error')
        elif name in manager.config.get(root, []):
            flash('%s with name %s already exists' % (config_type.capitalize(), name), 'error')
        else:
            manager.config.setdefault(root, {})[name] = {}
            return redirect(url_for('.edit_text', root=root, name=name))

    return render_template('configure/new.html', **context)


@configure.route('/delete/<root>/<name>')
def delete(root, name):
    if root in manager.config and name in manager.config[root]:
        log.info('Deleting %s %s' % (root, name))
        del manager.config[root][name]
        manager.save_config()
        flash('Deleted %s.' % name, 'delete')
    return redirect(url_for('.index'))


@configure.route('/edit/text/<root>/<name>', methods=['POST', 'GET'])
def edit_text(root, name):
    config_type = root.rstrip('s')
    context = {
        'name': name,
        'root': root,
        'config_type': config_type}

    if request.method == 'POST':
        context['config'] = request.form['config']
        try:
            config = yaml.load(request.form['config'])
        except yaml.scanner.ScannerError as e:
            flash('Invalid YAML document: %s' % e, 'error')
            log.exception(e)
        else:
            # valid yaml, now run validator
            errors = Task.validate_config(config)
            if errors:
                for error in errors:
                    flash(error, 'error')
                context['config'] = request.form['config']
            else:
                manager.config[root][name] = config
                manager.save_config()
                context['config'] = yaml.dump(config, default_flow_style=False)
                if request.form.get('name') != name:
                    # Renaming
                    new_name = request.form.get('name')
                    if new_name in manager.config[root]:
                        flash('%s with name %s already exists' % (config_type.capitalize(), new_name), 'error')
                    else:
                        # Do the rename
                        manager.config[root][new_name] = manager.config[root][name]
                        del manager.config[root][name]
                        manager.save_config()
                        flash('%s %s renamed to %s.' % (config_type.capitalize(), name, new_name), 'success')
                        return redirect(url_for('.edit_text', root=root, name=new_name))
                else:
                    flash('Configuration saved', 'success')
    else:
        config = manager.config[root][name]
        if config:
            context['config'] = yaml.dump(config, default_flow_style=False)
        else:
            context['config'] = ''
    context['related'] = get_related(root, name)
    return render_template('configure/edit_text.html', **context)


@configure.route('/edit/<root>/<name>', methods=['POST', 'GET'])
def edit(root, name):
    context = {'name': name}
    context['config'] = manager.config[root][name]
    return render_template('configure/edit.html', **context)

@configure.route('/jsonary')
def jsonary():
    return render_template('configure/edit_jsonary.html')


@app.template_filter('other_type')
def other_type(root):
    if root == 'tasks':
        return 'presets'
    return 'tasks'


def get_related(root, name):
    """Returns a list of related tasks/presets for a given preset/task"""
    if root == 'tasks':
        presets = manager.config[root][name].get('preset', [])
        if isinstance(presets, basestring):
            presets = [presets]
        return presets
    elif root == 'presets':
        tasks = []
        for task, config in manager.config['tasks'].iteritems():
            if config.get('preset'):
                if name == config['preset'] or name in config['preset']:
                    tasks.append(task)
        return tasks


register_plugin(configure, menu='Configure', order=10)

########NEW FILE########
__FILENAME__ = execute
from __future__ import unicode_literals, division, absolute_import
import logging
from Queue import Empty

from flask import render_template, request, flash
from flask import Blueprint, escape, jsonify

from flexget.options import get_parser
from flexget.ui.webui import register_plugin, manager
from flexget.scheduler import BufferQueue

execute = Blueprint('execute', __name__)

log = logging.getLogger('ui.execute')

bufferqueue = BufferQueue()
exec_parser = get_parser('execute')

@execute.route('/', methods=['POST', 'GET'])
def index():
    context = {'progress': exec_parser.format_help().split('\n')}
    if request.method == 'POST':
        try:
            options = exec_parser.parse_args(request.form.get('options', ''), raise_errors=True)
        except ValueError as e:
            flash(escape(e.message), 'error')
            context['options'] = request.form['options']
        else:
            manager.scheduler.execute(options=options.execute, output=bufferqueue)
            context['execute_progress'] = True
            context['progress'] = progress(as_list=True)

    return render_template('execute/execute.html', **context)


@execute.route('/progress.json')
def progress(as_list=False):
    """
    Gets messages from the queue and exports them to JSON.
    """
    result = {'items': []}
    try:
        while True:
            item = bufferqueue.get_nowait()
            result['items'].append(item)
            bufferqueue.task_done()
    except Empty:
        pass

    if as_list:
        return result['items']

    return jsonify(result)


register_plugin(execute, menu='Execute')

########NEW FILE########
__FILENAME__ = history
from __future__ import unicode_literals, division, absolute_import
import logging
from sqlalchemy import desc
from flexget.ui.webui import register_plugin, db_session
from flask import render_template, Blueprint
from flexget.plugin import DependencyError

try:
    from flexget.plugins.output.history import History
except ImportError:
    raise DependencyError(issued_by='ui.history', missing='history')

log = logging.getLogger('ui.history')
history = Blueprint('history', __name__)


@history.route('/')
def index():
    context = {'items': db_session.query(History).order_by(desc(History.time)).limit(50).all()}
    return render_template('history/history.html', **context)

register_plugin(history, menu='History')

########NEW FILE########
__FILENAME__ = home
from __future__ import unicode_literals, division, absolute_import
from flask import render_template, Blueprint
from flexget.ui.webui import register_plugin

home = Blueprint('home', __name__)


@home.route('/')
def index():
    return render_template('home/home.html')

register_plugin(home, menu='Home', order=0, home=True)

########NEW FILE########
__FILENAME__ = inject
from __future__ import unicode_literals, division, absolute_import
import logging
import posixpath
import urlparse
from flask import render_template, request, flash, redirect, Blueprint
from flask.helpers import url_for
from flexget.ui.webui import register_plugin, manager
from flexget.entry import Entry

inject = Blueprint('inject', __name__)

log = logging.getLogger('ui.inject')


@inject.route('/')
def index():
    return render_template('inject/inject.html')


@inject.route('/do', methods=['POST', 'GET'])
def do_inject():
    fields = {}
    # Requests is a special dict, and cannot be passed as keyword arguments, make it into a normal dict.
    for key, value in request.values.iteritems():
        # Translate on and off to True and False
        if value == 'on':
            fields[key] = True
        elif value == 'off':
            fields[key] = False
        else:
            fields[key] = value
    # If we only got a url, make a title from the url filename
    fields['title'] = fields.get('title') or posixpath.basename(urlparse.urlsplit(fields.get('url', '')).path)

    if fields.get('title') and fields.get('url'):
        # Create the entry for injection
        entry = Entry(**fields)
        # TODO: Fix
        for task in manager.tasks:
            manager.scheduler.execute(task, options={'dump_entries': True, 'inject': [entry]})
        flash('Scheduled execution for entry `%s`' % entry['title'], 'success')
    else:
        flash('Title and URL required for inject.', 'error')

    return redirect(url_for('.index'))


register_plugin(inject)

########NEW FILE########
__FILENAME__ = log_viewer
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime
from flask import render_template, Blueprint, jsonify, request
from sqlalchemy import Column, DateTime, Integer, Unicode, String, asc, desc, or_, and_
from flexget.ui.webui import register_plugin, db_session
from flexget.manager import Base, Session
from flexget.event import event

log_viewer = Blueprint('log_viewier', __name__, url_prefix='/log')


class LogEntry(Base):
    __tablename__ = 'log'

    id = Column(Integer, primary_key=True)
    created = Column(DateTime)
    logger = Column(String)
    levelno = Column(Integer)
    message = Column(Unicode)
    task = Column('feed', Unicode)
    execution = Column(String)

    def __init__(self, record):
        self.created = datetime.fromtimestamp(record.created)
        self.logger = record.name
        self.levelno = record.levelno
        self.message = unicode(record.getMessage())
        self.task = getattr(record, 'task', u'')
        self.execution = getattr(record, 'execution', '')


class DBLogHandler(logging.Handler):

    def emit(self, record):
        session = Session()
        try:
            session.add(LogEntry(record))
            session.commit()
        finally:
            session.close()


@log_viewer.context_processor
def update_menus():
    import time

    strftime = lambda secs: time.strftime('%Y-%m-%d %H:%M', time.localtime(float(secs)))
    menu_tasks = [i[0] for i in db_session.query(LogEntry.task).filter(LogEntry.task != u'')
                                          .distinct().order_by(asc(LogEntry.task))[:]]
    menu_execs = [(i[0], strftime(i[0])) for i in db_session.query(LogEntry.execution)
                                                            .filter(LogEntry.execution != '')
                                                            .distinct().order_by(desc('execution'))[:10]]
    return {'menu_tasks': menu_tasks, 'menu_execs': menu_execs}


@log_viewer.route('/')
def index():
    return render_template('log_viewer/log.html')


@log_viewer.route('/_get_logdata.json')
def get_logdata():
    log_type = request.args.get('log_type')
    task = request.args.get('task')
    execution = request.args.get('exec')
    page = int(request.args.get('page'))
    limit = int(request.args.get('rows', 0))
    sidx = request.args.get('sidx')
    sord = request.args.get('sord')
    sord = desc if sord == 'desc' else asc
    # Generate the filtered query
    query = db_session.query(LogEntry)
    if log_type == 'webui':
        query = query.filter(or_(LogEntry.logger.in_(['webui', 'werkzeug', 'event']), LogEntry.logger.like('%ui.%')))
    elif log_type == 'core':
        query = query.filter(and_(~LogEntry.logger.in_(['webui', 'werkzeug', 'event']), ~LogEntry.logger.like('%ui.%')))
    if task:
        query = query.filter(LogEntry.task == task)
    if execution:
        query = query.filter(LogEntry.execution == execution)
    count = query.count()
    # Use a trick to do ceiling division
    total_pages = 0 - ((0 - count) / limit)
    if page > total_pages:
        page = total_pages
    start = limit * page - limit
    json = {'total': total_pages,
            'page': page,
            'records': count,
            'rows': []}
    result = query.order_by(sord(sidx))[start:start + limit]
    for entry in result:
        json['rows'].append({'id': entry.id,
                             'created': entry.created.strftime('%Y-%m-%d %H:%M'),
                             'levelno': logging.getLevelName(entry.levelno),
                             'logger': entry.logger,
                             'task': entry.task,
                             'message': entry.message})
    return jsonify(json)


@event('webui.start')
def initialize():
    # Register db handler with base logger
    logger = logging.getLogger()
    handler = DBLogHandler()
    logger.addHandler(handler)

register_plugin(log_viewer, menu='Log', order=256)

########NEW FILE########
__FILENAME__ = movies
from __future__ import unicode_literals, division, absolute_import
import time
import logging
import posixpath
from flask import render_template, Blueprint, request, redirect, flash, send_file
from flask.helpers import url_for
from flexget.plugin import DependencyError, get_plugin_by_name
from flexget.ui.webui import register_plugin, app, manager
from flexget.utils import qualities

try:
    from flexget.plugins.filter.movie_queue import QueueError, queue_get, queue_add, queue_del, queue_edit
except ImportError:
    raise DependencyError(issued_by='ui.movies', missing='movie_queue')


movies_module = Blueprint('movies', __name__)
log = logging.getLogger('ui.movies')


# TODO: refactor this filter to some globally usable place (webui.py?)
#       also flexget/plugins/ui/utils.py needs to be removed
#       ... mainly because we have flexget/utils for that :)


@app.template_filter('pretty_age')
def pretty_age_filter(value):
    from flexget.ui.utils import pretty_date
    return pretty_date(time.mktime(value.timetuple()))


@movies_module.route('/')
def index():
    movie_queue = queue_get()
    tmdb_lookup = get_plugin_by_name('api_tmdb').instance.lookup
    for item in movie_queue:
        try:
            movie = tmdb_lookup(tmdb_id=item.tmdb_id, only_cached=True)
        except LookupError:
            item.overview = ('TMDb lookup was not successful, no overview available.'
                             'Lookup is being retried in the background.')
            log.debug('No themoviedb result for tmdb id %s' % item.tmdb_id)

            # this is probably not needed since non cached movies are retried also
            # in the cover function
            #
            #import thread
            #thread.start_new_thread(tmdb_lookup, (), {'imdb_id': item.imdb_id})
            continue

        # set thumb, but only if already in cache because retrieving is too slow here
        # movies without cached thumb use img tag reading /cover/<imdb_id> which will
        # retrieve the image and thus allows rendering the page immediattely
        for poster in movie.posters:
            if poster.size == 'thumb':
                thumb = poster.get_file(only_cached=True)
                if thumb:
                    item.thumb = url_for('userstatic', filename=posixpath.join(*thumb))
                break

        item.title = movie.name
        item.year = movie.released and movie.released.year
        item.overview = movie.overview

    context = {'movies': movie_queue}
    return render_template('movies/movies.html', **context)


@movies_module.route('/add', methods=['GET', 'POST'])
def add_to_queue():
    what = request.values.get('what')
    imdb_id = request.values.get('imdb_id')
    # TODO: This is a rather limited selection of quality considering the new quality system. Improve it.
    quality = qualities.Requirements(request.values.get('quality', 'ANY'))
    force = request.values.get('force') == 'on'
    try:
        title = queue_add(title=what, imdb_id=imdb_id, quality=quality, force=force)['title']
    except QueueError as e:
        flash(e.message, 'error')
    else:
        flash('%s successfully added to queue.' % title, 'success')
    return redirect(url_for('.index'))


@movies_module.route('/del')
def del_from_queue():
    imdb_id = request.values.get('imdb_id')
    try:
        title = queue_del(imdb_id=imdb_id)
    except QueueError as e:
        flash(e.message, 'error')
    else:
        flash('%s removed from queue.' % title, 'delete')
    return redirect(url_for('.index'))


@movies_module.route('/edit')
def edit_movie_quality():
    imdb_id = request.values.get('imdb_id')
    quality = request.values.get('quality')
    try:
        queue_edit(quality, imdb_id=imdb_id)
    except QueueError as e:
        flash(e.message, 'error')
    else:
        # TODO: Display movie name instead of id
        flash('%s quality changed to %s' % (imdb_id, quality), 'success')
    return redirect(url_for('.index'))


@movies_module.route('/cover/<imdb_id>')
def cover(imdb_id):
    import os

    # TODO: return '' should be replaced with something sane, http error 404 ?

    tmdb_lookup = get_plugin_by_name('api_tmdb').instance.lookup
    try:
        movie = tmdb_lookup(imdb_id=imdb_id)
    except LookupError:
        log.error('No cached data for %s' % imdb_id)
        return ''

    filepath = None
    for poster in movie.posters:
        if poster.size == 'thumb':
            filepath = os.path.join(manager.config_base, 'userstatic', *poster.get_file())
            break

    if filepath is None:
        log.error('No cover for %s' % imdb_id)
        return ''
    elif not os.path.exists(filepath):
        log.error('File %s does not exist' % filepath)
        return ''

    log.debug('sending thumb file %s' % filepath)
    return send_file(filepath, mimetype='image/png')


register_plugin(movies_module, menu='Movies')

########NEW FILE########
__FILENAME__ = plugins
from __future__ import unicode_literals, division, absolute_import
from flask import Blueprint, jsonify
from flexget.ui.webui import register_plugin
from flexget.plugin import plugins, get_plugins, task_phases, plugin_contexts

plugins_module = Blueprint('plugins', __name__)


def plugin_infos(plugins):
    def plugin_info(plugin):
        return {'contexts': plugin.contexts,
                'groups': plugin.groups,
                'category': plugin.category}

    return dict((p.name, plugin_info(p)) for p in plugins)


# JSON API
@plugins_module.route('/all')
def all_plugins():
    return jsonify(plugins=plugin_infos(plugins.itervalues()))


@plugins_module.route('/phases')
def phases():
    return jsonify(phases=task_phases)


@plugins_module.route('/phase/<phase>')
def plugins_by_phase(phase):
    try:
        return jsonify(plugins=plugin_infos(get_plugins(phase=phase)))
    except Exception as e:
        return e.message, 404


@plugins_module.route('/groups')
def groups():
    # TODO: There should probably be a function in plugin.py to get this
    groups = set()
    for plugin in plugins.itervalues():
        groups.update(plugin.get('groups'))
    return jsonify(groups=list(groups))


@plugins_module.route('/group/<group>')
def plugins_by_group(group):
    return jsonify(plugins=plugin_infos(get_plugins(group=group)))


@plugins_module.route('/contexts')
def contexts():
    return jsonify(contexts=plugin_contexts)


@plugins_module.route('/context/<context>')
def plugins_by_context(context):
    return jsonify(plugins=plugin_infos(get_plugins(context=context)))


@plugins_module.route('/categories')
def categories():
    # TODO: Should this only return the list of categories that actually have plugins?
    return jsonify(categories=task_phases)


@plugins_module.route('/category/<category>')
def plugins_by_category(category):
    return jsonify(plugins=plugin_infos(get_plugins(category=category)))


register_plugin(plugins_module)

########NEW FILE########
__FILENAME__ = schedule
from __future__ import unicode_literals, division, absolute_import
import logging
from datetime import datetime, timedelta
import threading

from sqlalchemy import Column, Integer, Unicode
from flask import request, render_template, flash, Blueprint, redirect, url_for

from flexget.ui.webui import register_plugin, db_session, manager
from flexget.manager import Base
from flexget.event import event, fire_event

log = logging.getLogger('ui.schedule')
schedule = Blueprint('schedule', __name__)


def get_task_interval(task):
    task_interval = db_session.query(Schedule).filter(Schedule.task == task).first()
    if task_interval:
        return task_interval.interval


def set_task_interval(task, interval):
    task_interval = db_session.query(Schedule).filter(Schedule.task == task).first()
    if task_interval:
        log.debug('Updating %s interval' % task)
        task_interval.interval = interval
    else:
        log.debug('Creating new %s interval' % task)
        db_session.add(Schedule(task, interval))
    db_session.commit()
    stop_empty_timers()


@schedule.context_processor
def get_intervals():
    config = manager.config.setdefault('schedules', {})
    config_tasks = config.setdefault('tasks', {})
    task_schedules = []
    for task in set(config_tasks) | set(manager.tasks):
        task_schedules.append(
            {'name': task,
             'enabled': task in config_tasks,
             'schedule': config_tasks.get(task, ''),
             'valid': task in manager.tasks})
    default_schedule = {'enabled': 'default' in config, 'schedule': config.get('default', '')}
    return {'default_schedule': default_schedule, 'task_schedules': task_schedules}


def update_interval(form, task):
    try:
        interval = float(form[task + '_interval'])
    except ValueError:
        flash('%s interval must be a number!' % task.capitalize(), 'error')
    else:
        if interval <= 0:
            flash('%s interval must be greater than zero!' % task.capitalize(), 'error')
        else:
            unit = form[task + '_unit']
            delta = timedelta(**{unit: interval})
            # Convert the timedelta to integer minutes
            interval = int((delta.seconds + delta.days * 24 * 3600) / 60.0)
            if interval < 1:
                interval = 1
            log.info('new interval for %s: %d minutes' % (task, interval))
            set_task_interval(task, interval)
            start_timer(interval)
            flash('%s scheduling updated successfully.' % task.capitalize(), 'success')


@schedule.route('/', methods=['POST', 'GET'])
def index():
    global timer
    if request.method == 'POST':
        if request.form.get('default_interval'):
            pass  # TODO: something
        for task in manager.tasks:
            if request.form.get('task_%s_interval' % task):
                update_interval(request.form, task)

    return render_template('schedule/schedule.html')


@schedule.route('/delete/<task>')
def delete_schedule(task):
    db_session.query(Schedule).filter(Schedule.task == task).delete()
    db_session.commit()
    stop_empty_timers()
    return redirect(url_for('.index'))


@schedule.route('/add/<task>')
def add_schedule(task):
    schedule = db_session.query(Schedule).filter(Schedule.task == task).first()
    if not schedule:
        schedule = Schedule(task, DEFAULT_INTERVAL)
        db_session.add(schedule)
        db_session.commit()
    start_timer(DEFAULT_INTERVAL)
    return redirect(url_for('.index'))


def get_all_tasks():
    return [task for task in manager.config.get('tasks', {}).keys() if not task.startswith('_')]


def get_scheduled_tasks():
    return [item.task for item in db_session.query(Schedule).all()]


register_plugin(schedule, menu='Schedule')

########NEW FILE########
__FILENAME__ = schema
from __future__ import unicode_literals, division, absolute_import

from flask import Blueprint, jsonify, request
from jsonschema import RefResolutionError

from flexget.config_schema import resolve_ref
from flexget.ui.webui import register_plugin

schema = Blueprint('schema', __name__)


@schema.route('/', defaults={'path': ''})
@schema.route('/<path:path>')
def get_schema(path):
    refpath = '/schema/' + path
    if request.query_string:
        refpath += '?' + request.query_string
    try:
        return jsonify(resolve_ref(refpath))
    except RefResolutionError:
        return 'Schema not found', 404


register_plugin(schema)

########NEW FILE########
__FILENAME__ = series
from __future__ import unicode_literals, division, absolute_import
import time
import logging

from flask import redirect, render_template, Blueprint, request, flash, url_for
from sqlalchemy.sql.expression import desc, asc

from flexget.plugin import DependencyError
from flexget.ui.webui import register_plugin, db_session, app
from flexget.ui.utils import pretty_date

try:
    from flexget.plugins.filter.series import Series, Episode, Release, forget_series, forget_series_episode
except ImportError:
    raise DependencyError(issued_by='ui.series', missing='series')


series_module = Blueprint('series', __name__)
log = logging.getLogger('ui.series')


# TODO: refactor this filter to some globally usable place (webui.py?)
#       also flexget/plugins/ui/utils.py needs to be removed
#       ... mainly because we have flexget/utils for that :)
#
# Josh  Changing the package layout to use 'flexget.ui.utils' instead
# says  seems to illeviated the need to do this no? I don't think this
#       will be of use for anything but UI related functions.


@app.template_filter('pretty_age')
def pretty_age_filter(value):
    return pretty_date(time.mktime(value.timetuple()))


@series_module.route('/')
def index():
    releases = db_session.query(Release).order_by(desc(Release.id)).limit(10).all()
    for release in releases:
        if release.downloaded == False and len(release.episode.releases) > 1:
            for prev_rel in release.episode.releases:
                if prev_rel.downloaded:
                    release.previous = prev_rel

    context = {'releases': releases}
    return render_template('series/series.html', **context)


@series_module.context_processor
def series_list():
    """Add series list to all pages under series"""
    return {'report': db_session.query(Series).order_by(asc(Series.name)).all()}


@series_module.route('/<name>')
def episodes(name):
    query = db_session.query(Episode).join(Episode.series)
    episodes = query.filter(Series.name == name).order_by(desc(Episode.identifier)).all()
    context = {'episodes': episodes, 'name': name}
    return render_template('series/series.html', **context)


@series_module.route('/mark/downloaded/<int:rel_id>')
def mark_downloaded(rel_id):
    db_session.query(Release).get(rel_id).downloaded = True
    db_session.commit()
    return redirect('/series')


@series_module.route('/mark/not_downloaded/<int:rel_id>')
def mark_not_downloaded(rel_id):
    db_session.query(Release).get(rel_id).downloaded = False
    db_session.commit()
    return redirect('/series')


@series_module.route('/forget/<int:rel_id>', methods=['POST', 'GET'])
def forget_episode(rel_id):
    """
    Executes a --series-forget statement for an episode.
    Redirects back to the series index.
    """
    release = db_session.query(Release).get(rel_id)

    context = {'release': release, 'command': 'series forget "%s" %s' % (
        release.episode.series.name, release.episode.identifier)}

    if request.method == 'POST':
        if request.form.get('really', False):
            try:
                forget_series_episode(release.episode.series.name, release.episode.identifier)
                flash('Forgot %s %s.' % (
                    release.episode.series.name, release.episode.identifier), 'delete')
            except ValueError as e:
                flash(e.message, 'error')

        return redirect(url_for('.index'))

    return render_template('series/forget.html', **context)


register_plugin(series_module, menu='Series')

########NEW FILE########
__FILENAME__ = shutdown
from __future__ import unicode_literals, division, absolute_import
from flexget.ui.webui import register_plugin, stop_server
from flask import render_template, Blueprint

import logging

shutdown = Blueprint('shutdown', __name__)

log = logging.getLogger('shutdown')


@shutdown.route('/')
def index():
    return render_template('shutdown/shutdown.html')


@shutdown.route('/now')
def now():
    stop_server()
    return 'Shutdown Complete'


register_plugin(shutdown, menu='Shutdown', order=512)

########NEW FILE########
__FILENAME__ = icon
# Synopsis   : Windows System tray icon.
# Programmer : Simon Brunning - simon@brunningonline.net
# Date       : 11 April 2005
# Notes      : Based on (i.e. ripped off from) Mark Hammond's
#              win32gui_taskbar.py and win32gui_menu.py demos from PyWin32

from __future__ import unicode_literals, division, absolute_import, print_function
import os
from flexget.event import event
from flexget.plugin import DependencyError

if os.name != 'nt':
    raise EnvironmentError('win32 only')

try:
    import win32api
    import win32con
    import win32gui_struct

    try:
        import winxpgui as win32gui
    except ImportError:
        import win32gui
except ImportError:
    raise DependencyError(issued_by='ui.win32tray', missing='win32 extensions',
                          message='Task tray icon requires win32 extensions')


class SysTrayIcon(object):
    '''TODO'''
    QUIT = 'QUIT'
    SPECIAL_ACTIONS = [QUIT]

    FIRST_ID = 1023

    def __init__(self,
                 icon,
                 hover_text,
                 menu_options,
                 on_quit=None,
                 default_menu_index=None,
                 window_class_name=None, ):
        self.icon = icon
        self.hover_text = hover_text
        self.on_quit = on_quit

        menu_options = menu_options + (('Quit', None, self.QUIT),)
        self._next_action_id = self.FIRST_ID
        self.menu_actions_by_id = set()
        self.menu_options = self._add_ids_to_menu_options(list(menu_options))
        self.menu_actions_by_id = dict(self.menu_actions_by_id)
        del self._next_action_id

        self.default_menu_index = (default_menu_index or 0)
        self.window_class_name = window_class_name or b"SysTrayIconPy"

        message_map = {win32gui.RegisterWindowMessage("TaskbarCreated"): self.restart,
                       win32con.WM_DESTROY: self.destroy,
                       win32con.WM_COMMAND: self.command,
                       win32con.WM_USER + 20: self.notify, }
        # Register the Window class.
        window_class = win32gui.WNDCLASS()
        hinst = window_class.hInstance = win32gui.GetModuleHandle(None)
        window_class.lpszClassName = self.window_class_name
        window_class.style = win32con.CS_VREDRAW | win32con.CS_HREDRAW
        window_class.hCursor = win32gui.LoadCursor(0, win32con.IDC_ARROW)
        window_class.hbrBackground = win32con.COLOR_WINDOW
        window_class.lpfnWndProc = message_map # could also specify a wndproc.
        classAtom = win32gui.RegisterClass(window_class)
        # Create the Window.
        style = win32con.WS_OVERLAPPED | win32con.WS_SYSMENU
        self.hwnd = win32gui.CreateWindow(classAtom,
                                          self.window_class_name,
                                          style,
                                          0,
                                          0,
                                          win32con.CW_USEDEFAULT,
                                          win32con.CW_USEDEFAULT,
                                          0,
                                          0,
                                          hinst,
                                          None)
        win32gui.UpdateWindow(self.hwnd)
        self.notify_id = None
        self.refresh_icon()

        win32gui.PumpMessages()

    def _add_ids_to_menu_options(self, menu_options):
        result = []
        for menu_option in menu_options:
            option_text, option_icon, option_action = menu_option
            if callable(option_action) or option_action in self.SPECIAL_ACTIONS:
                self.menu_actions_by_id.add((self._next_action_id, option_action))
                result.append(menu_option + (self._next_action_id,))
            elif non_string_iterable(option_action):
                result.append((option_text,
                               option_icon,
                               self._add_ids_to_menu_options(option_action),
                               self._next_action_id))
            else:
                print('Unknown item', option_text, option_icon, option_action)
            self._next_action_id += 1
        return result

    def refresh_icon(self):
        # Try and find a custom icon
        hinst = win32gui.GetModuleHandle(None)
        if os.path.isfile(self.icon):
            icon_flags = win32con.LR_LOADFROMFILE | win32con.LR_DEFAULTSIZE
            hicon = win32gui.LoadImage(hinst,
                                       self.icon,
                                       win32con.IMAGE_ICON,
                                       0,
                                       0,
                                       icon_flags)
        else:
            print("Can't find icon file - using default.")
            hicon = win32gui.LoadIcon(0, win32con.IDI_APPLICATION)

        if self.notify_id:
            message = win32gui.NIM_MODIFY
        else:
            message = win32gui.NIM_ADD
        self.notify_id = (self.hwnd,
                          0,
                          win32gui.NIF_ICON | win32gui.NIF_MESSAGE | win32gui.NIF_TIP,
                          win32con.WM_USER + 20,
                          hicon,
                          self.hover_text)
        win32gui.Shell_NotifyIcon(message, self.notify_id)

    def restart(self, hwnd, msg, wparam, lparam):
        self.refresh_icon()

    def destroy(self, hwnd, msg, wparam, lparam):
        if self.on_quit:
            self.on_quit(self)
        nid = (self.hwnd, 0)
        win32gui.Shell_NotifyIcon(win32gui.NIM_DELETE, nid)
        win32gui.PostQuitMessage(0) # Terminate the app.

    def notify(self, hwnd, msg, wparam, lparam):
        if lparam == win32con.WM_LBUTTONDBLCLK:
            self.execute_menu_option(self.default_menu_index + self.FIRST_ID)
        elif lparam == win32con.WM_RBUTTONUP:
            self.show_menu()
        elif lparam == win32con.WM_LBUTTONUP:
            pass
        return True

    def show_menu(self):
        menu = win32gui.CreatePopupMenu()
        self.create_menu(menu, self.menu_options)
        #win32gui.SetMenuDefaultItem(menu, 1000, 0)

        pos = win32gui.GetCursorPos()
        # See http://msdn.microsoft.com/library/default.asp?url=/library/en-us/winui/menus_0hdi.asp
        win32gui.SetForegroundWindow(self.hwnd)
        win32gui.TrackPopupMenu(menu,
                                win32con.TPM_LEFTALIGN,
                                pos[0],
                                pos[1],
                                0,
                                self.hwnd,
                                None)
        win32gui.PostMessage(self.hwnd, win32con.WM_NULL, 0, 0)

    def create_menu(self, menu, menu_options):
        for option_text, option_icon, option_action, option_id in menu_options[::-1]:
            if option_icon:
                option_icon = self.prep_menu_icon(option_icon)

            if option_id in self.menu_actions_by_id:
                item, extras = win32gui_struct.PackMENUITEMINFO(text=option_text,
                                                                hbmpItem=option_icon,
                                                                wID=option_id)
                win32gui.InsertMenuItem(menu, 0, 1, item)
            else:
                submenu = win32gui.CreatePopupMenu()
                self.create_menu(submenu, option_action)
                item, extras = win32gui_struct.PackMENUITEMINFO(text=option_text,
                                                                hbmpItem=option_icon,
                                                                hSubMenu=submenu)
                win32gui.InsertMenuItem(menu, 0, 1, item)

    def prep_menu_icon(self, icon):
        # First load the icon.
        ico_x = win32api.GetSystemMetrics(win32con.SM_CXSMICON)
        ico_y = win32api.GetSystemMetrics(win32con.SM_CYSMICON)
        hicon = win32gui.LoadImage(0, icon, win32con.IMAGE_ICON, ico_x, ico_y, win32con.LR_LOADFROMFILE)

        hdcBitmap = win32gui.CreateCompatibleDC(0)
        hdcScreen = win32gui.GetDC(0)
        hbm = win32gui.CreateCompatibleBitmap(hdcScreen, ico_x, ico_y)
        hbmOld = win32gui.SelectObject(hdcBitmap, hbm)
        # Fill the background.
        brush = win32gui.GetSysColorBrush(win32con.COLOR_MENU)
        win32gui.FillRect(hdcBitmap, (0, 0, 16, 16), brush)
        # unclear if brush needs to be feed.  Best clue I can find is:
        # "GetSysColorBrush returns a cached brush instead of allocating a new
        # one." - implies no DeleteObject
        # draw the icon
        win32gui.DrawIconEx(hdcBitmap, 0, 0, hicon, ico_x, ico_y, 0, 0, win32con.DI_NORMAL)
        win32gui.SelectObject(hdcBitmap, hbmOld)
        win32gui.DeleteDC(hdcBitmap)

        return hbm

    def command(self, hwnd, msg, wparam, lparam):
        id = win32gui.LOWORD(wparam)
        self.execute_menu_option(id)

    def execute_menu_option(self, id):
        menu_action = self.menu_actions_by_id[id]
        if menu_action == self.QUIT:
            win32gui.DestroyWindow(self.hwnd)
        else:
            menu_action(self)


def non_string_iterable(obj):
    try:
        iter(obj)
    except TypeError:
        return False
    else:
        return not isinstance(obj, basestring)


def create_icon():
    """Creates FlexGet tasktray icon"""

    hover_text = "FlexGet"

    icon_file = os.path.join(os.path.dirname(__file__), 'flexget.ico')

    def view(sysTrayIcon):
        # View FlexGet in default web browser
        from flexget.ui.webui import manager
        import webbrowser
        webbrowser.open('http://127.0.0.1:%d' % manager.options.webui.port)

    menu_options = (('View FlexGet', None, view), )

    def bye(sysTrayIcon):
        # Shutdown FlexGet
        from flexget.ui.webui import stop_server
        stop_server()

    SysTrayIcon(icon_file, hover_text, menu_options, on_quit=bye, default_menu_index=1)


@event('webui.start')
def iconize():
    import thread

    thread.start_new_thread(create_icon, ())

########NEW FILE########
__FILENAME__ = utils
from __future__ import division


def pretty_date(time=False):
    """
    Get a datetime object or a int() Epoch timestamp and return a
    pretty string like 'an hour ago', 'Yesterday', '3 months ago',
    'just now', etc
    """
    from datetime import datetime
    now = datetime.now()
    diff = now - datetime.fromtimestamp(time)
    second_diff = diff.seconds
    day_diff = diff.days

    if day_diff < 0:
        return ''

    if day_diff == 0:
        if second_diff < 10:
            return "just now"
        if second_diff < 60:
            return str(second_diff) + " seconds ago"
        if second_diff < 120:
            return "a minute ago"
        if second_diff < 3600:
            return str(second_diff / 60) + " minutes ago"
        if second_diff < 7200:
            return "an hour ago"
        if second_diff < 86400:
            return str(second_diff / 3600) + " hours ago"
    if day_diff == 1:
        return "Yesterday"
    if day_diff < 7:
        return str(day_diff) + " days ago"
    if day_diff < 31:
        return str(day_diff / 7) + " weeks ago"
    if day_diff < 365:
        return str(day_diff / 30) + " months ago"
    return str(day_diff / 365) + " years ago"

########NEW FILE########
__FILENAME__ = webui
"""
Fires events:

webui.start
  When webui is being started, this is just before WSGI server is started. Everything else is already initialized.

webui.stop
  When webui is being shut down, the WSGI server has exited the "serve forever" loop.
"""

from __future__ import unicode_literals, division, absolute_import
import logging
import os
import urllib
import socket
import sys

from flask import Flask, redirect, url_for, abort, request, send_from_directory
from sqlalchemy.orm import scoped_session
from sqlalchemy.orm.session import sessionmaker

from flexget.event import fire_event
from flexget.plugin import DependencyError
from flexget.ui.api import api, api_schema

log = logging.getLogger('webui')

app = Flask(__name__)
manager = None
db_session = None
server = None

_home = None
_menu = []


def _update_menu(root):
    """Iterates trough menu navigation and sets the item selected based on the :root:"""
    for item in _menu:
        if item['href'].startswith(root):
            item['current'] = True
            log.debug('current menu item %s' % root)
        else:
            if 'current' in item:
                item.pop('current')


@app.route('/')
def start_page():
    """Redirect user to registered home plugin"""
    if not _home:
        abort(404)
    return redirect(url_for(_home))


@app.route('/userstatic/<path:filename>')
def userstatic(filename):
    return send_from_directory(os.path.join(manager.config_base, 'userstatic'), filename)


@app.context_processor
def flexget_variables():
    path = urllib.splitquery(request.path)[0]
    root = '/' + path.split('/', 2)[1]
    # log.debug('root is: %s' % root)
    _update_menu(root)
    return {'menu': _menu, 'manager': manager}


def load_ui_plugins():

    # TODO: load from ~/.flexget/ui/plugins too (or something like that)

    import flexget.ui.plugins
    d = flexget.ui.plugins.__path__[0]

    plugin_names = set()
    for f in os.listdir(d):
        path = os.path.join(d, f, '__init__.py')
        if os.path.isfile(path):
            plugin_names.add(f)

    for name in plugin_names:
        try:
            log.info('Loading UI plugin %s' % name)
            exec "import flexget.ui.plugins.%s" % name
        except DependencyError as e:
            # plugin depends on another plugin that was not imported successfully
            log.error(e.message)
        except EnvironmentError as e:
            log.info('Plugin %s: %s' % (name, e.message))
        except Exception as e:
            log.critical('Exception while loading plugin %s' % name)
            log.exception(e)
            raise


def register_plugin(blueprint, menu=None, order=128, home=False):
    """
    Registers UI plugin.

    :plugin: :class:`flask.Blueprint` object for this plugin.
    """
    # Set up some defaults if the plugin did not already specify them
    if blueprint.url_prefix is None:
        blueprint.url_prefix = '/' + blueprint.name
    if not blueprint.template_folder and os.path.isdir(os.path.join(blueprint.root_path, 'templates')):
        blueprint.template_folder = 'templates'
    if not blueprint.static_folder and os.path.isdir(os.path.join(blueprint.root_path, 'static')):
        blueprint.static_folder = 'static'
    log.info('Registering UI plugin %s' % blueprint.name)
    app.register_blueprint(blueprint)
    if menu:
        register_menu(blueprint.url_prefix, menu, order=order)
    if home:
        register_home(blueprint.name + '.index')


def register_menu(href, caption, order=128):
    global _menu
    _menu.append({'href': href, 'caption': caption, 'order': order})
    _menu = sorted(_menu, key=lambda item: item['order'])


def register_home(route, order=128):
    """Registers homepage elements"""
    global _home
    # TODO: currently supports only one plugin
    if _home is not None:
        raise Exception('Home is already registered')
    _home = route


@app.teardown_appcontext
def shutdown_session(exception=None):
    """Remove db_session after request"""
    db_session.remove()
    log.debug('db_session removed')


def start(mg):
    """Start WEB UI"""

    global manager
    manager = mg

    # Create sqlalchemy session for Flask usage
    global db_session
    db_session = scoped_session(sessionmaker(autocommit=False,
                                             autoflush=False,
                                             bind=manager.engine))
    if db_session is None:
        raise Exception('db_session is None')

    load_ui_plugins()

    # quick hack: since ui plugins may add tables to SQLAlchemy too and they're not initialized because create
    # was called when instantiating manager .. so we need to call it again
    from flexget.manager import Base
    Base.metadata.create_all(bind=manager.engine)

    app.register_blueprint(api)
    app.register_blueprint(api_schema)
    fire_event('webui.start')

    # Start Flask
    app.secret_key = os.urandom(24)

    set_exit_handler(stop_server)

    log.info('Starting server on port %s' % manager.options.webui.port)

    if manager.options.webui.autoreload:
        # Create and destroy a socket so that any exceptions are raised before
        # we spawn a separate Python interpreter and lose this ability.
        from werkzeug.serving import run_with_reloader
        reloader_interval = 1
        extra_files = None
        test_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        test_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        test_socket.bind((manager.options.webui.bind, manager.options.webui.port))
        test_socket.close()
        log.warning('Not starting scheduler, since autoreload is enabled.')
        run_with_reloader(start_server, extra_files, reloader_interval)
    else:
        # Start the scheduler
        manager.scheduler.start()
        start_server()

    log.debug('server exited')
    fire_event('webui.stop')
    manager.shutdown(finish_queue=False)


def start_server():
    global server
    from cherrypy import wsgiserver
    d = wsgiserver.WSGIPathInfoDispatcher({'/': app})
    server = wsgiserver.CherryPyWSGIServer((manager.options.webui.bind, manager.options.webui.port), d)

    log.debug('server %s' % server)
    try:
        server.start()
    except KeyboardInterrupt:
        stop_server()


def stop_server(*args):
    log.debug('Shutting down server')
    manager.scheduler.shutdown(finish_queue=False)
    if server:
        server.stop()


def set_exit_handler(func):
    """Sets a callback function for term signal on windows or linux"""
    if os.name == 'nt':
        try:
            import win32api
            win32api.SetConsoleCtrlHandler(func, True)
        except ImportError:
            version = '.'.join(map(str, sys.version_info[:2]))
            raise Exception('pywin32 not installed for Python ' + version)
    else:
        import signal
        signal.signal(signal.SIGTERM, func)




########NEW FILE########
__FILENAME__ = bittorrent
"""Torrenting utils, mostly for handling bencoding and torrent files."""
# Torrent decoding is a short fragment from effbot.org. Site copyright says:
# Test scripts and other short code fragments can be considered as being in the public domain.
from __future__ import unicode_literals, division, absolute_import
import re
import logging

log = logging.getLogger('torrent')

# Magic indicator used to quickly recognize torrent files
TORRENT_RE = re.compile(r'^d\d{1,3}:')

# List of all standard keys in a metafile
# See http://packages.python.org/pyrocore/apidocs/pyrocore.util.metafile-module.html#METAFILE_STD_KEYS
METAFILE_STD_KEYS = [i.split('.') for i in (
    "announce",
    "announce-list", # BEP-0012
    "comment",
    "created by",
    "creation date",
    "encoding",
    "info",
    "info.length",
    "info.name",
    "info.piece length",
    "info.pieces",
    "info.private",
    "info.files",
    "info.files.length",
    "info.files.path",
)]


def clean_meta(meta, including_info=False, logger=None):
    """ Clean meta dict. Optionally log changes using the given logger.

        See also http://packages.python.org/pyrocore/apidocs/pyrocore.util.metafile-pysrc.html#clean_meta

        @param logger: If given, a callable accepting a string message.
        @return: Set of keys removed from C{meta}.
    """
    modified = set()

    for key in meta.keys():
        if [key] not in METAFILE_STD_KEYS:
            if logger:
                logger("Removing key %r..." % (key,))
            del meta[key]
            modified.add(key)

    if including_info:
        for key in meta["info"].keys():
            if ["info", key] not in METAFILE_STD_KEYS:
                if logger:
                    logger("Removing key %r..." % ("info." + key,))
                del meta["info"][key]
                modified.add("info." + key)

        for idx, entry in enumerate(meta["info"].get("files", [])):
            for key in entry.keys():
                if ["info", "files", key] not in METAFILE_STD_KEYS:
                    if logger:
                        logger("Removing key %r from file #%d..." % (key, idx + 1))
                    del entry[key]
                    modified.add("info.files." + key)

    return modified


def is_torrent_file(metafilepath):
    """ Check whether a file looks like a metafile by peeking into its content.

        Note that this doesn't ensure that the file is a complete and valid torrent,
        it just allows fast filtering of candidate files.

        @param metafilepath: Path to the file to check, must have read permissions for it.
        @return: True if there is a high probability this is a metafile.
    """
    with open(metafilepath, 'rb') as f:
        data = f.read(200)

    magic_marker = bool(TORRENT_RE.match(data))
    if not magic_marker:
        log.trace('%s doesn\'t seem to be a torrent, got `%s` (hex)' % (metafilepath, data.encode('hex')))

    return bool(magic_marker)


def tokenize(text, match=re.compile("([idel])|(\d+):|(-?\d+)").match):
    i = 0
    while i < len(text):
        m = match(text, i)
        s = m.group(m.lastindex)
        i = m.end()
        if m.lastindex == 2:
            yield b"s"
            yield text[i:i + int(s)]
            i += int(s)
        else:
            yield s


def decode_item(next, token):
    if token == b"i":
        # integer: "i" value "e"
        data = int(next())
        if next() != b"e":
            raise ValueError
    elif token == b"s":
        # string: "s" value (virtual tokens)
        data = next()
        # Strings in torrent file are defined as utf-8 encoded
        try:
            data = data.decode('utf-8')
        except UnicodeDecodeError as e:
            # The pieces field is a byte string, and should be left as such.
            pass
    elif token == b"l" or token == b"d":
        # container: "l" (or "d") values "e"
        data = []
        tok = next()
        while tok != b"e":
            data.append(decode_item(next, tok))
            tok = next()
        if token == b"d":
            data = dict(zip(data[0::2], data[1::2]))
    else:
        raise ValueError
    return data


def bdecode(text):
    try:
        src = tokenize(text)
        data = decode_item(src.next, src.next()) # pylint:disable=E1101
        for token in src: # look for more tokens
            raise SyntaxError("trailing junk")
    except (AttributeError, ValueError, StopIteration) as e:
        raise SyntaxError("syntax error: %s" % e)
    return data


# encoding implementation by d0b
def encode_string(data):
    return b"%d:%s" % (len(data), data)


def encode_unicode(data):
    return encode_string(data.encode('utf8'))


def encode_integer(data):
    return b"i%de" % data


def encode_list(data):
    encoded = b"l"
    for item in data:
        encoded += bencode(item)
    encoded += b"e"
    return encoded


def encode_dictionary(data):
    encoded = b"d"
    items = data.items()
    items.sort()
    for (key, value) in items:
        encoded += bencode(key)
        encoded += bencode(value)
    encoded += b"e"
    return encoded


def bencode(data):
    encode_func = {
        str: encode_string,
        unicode: encode_unicode,
        int: encode_integer,
        long: encode_integer,
        list: encode_list,
        dict: encode_dictionary}
    return encode_func[type(data)](data)


class Torrent(object):
    """Represents a torrent"""
    # string type used for keys, if this ever changes, stuff like "x in y"
    # gets broken unless you coerce to this type
    KEY_TYPE = str

    @classmethod
    def from_file(cls, filename):
        """Create torrent from file on disk."""
        with open(filename, 'rb') as handle:
            return cls(handle.read())

    def __init__(self, content):
        """Accepts torrent file as string"""
        # Make sure there is no trailing whitespace. see #1592
        content = content.strip()
        # decoded torrent structure
        self.content = bdecode(content)
        self.modified = False

    def __repr__(self):
        return "%s(%s, %s)" % (self.__class__.__name__,
            ", ".join("%s=%r" % (key, self.content["info"].get(key))
               for key in ("name", "length", "private",)),
            ", ".join("%s=%r" % (key, self.content.get(key))
               for key in ("announce", "comment",)))

    def get_filelist(self):
        """Return array containing fileinfo dictionaries (name, length, path)"""
        files = []
        if 'length' in self.content['info']:
            # single file torrent
            t = {'name': self.content['info']['name'],
                 'size': self.content['info']['length'],
                 'path': ''}
            files.append(t)
        else:
            # multifile torrent
            for item in self.content['info']['files']:
                t = {'path': b'/'.join(item['path'][:-1]),
                     'name': item['path'][-1],
                     'size': item['length']}
                files.append(t)

        # Decode strings
        for item in files:
            for field in ('name', 'path'):
                # These should already be decoded if they were utf-8, if not we can try some other stuff
                if not isinstance(item[field], unicode):
                    try:
                        item[field] = item[field].decode(self.content.get('encoding', 'cp1252'))
                    except UnicodeError:
                        # Broken beyond anything reasonable
                        fallback = item[field].decode('utf-8', 'replace').replace(u'\ufffd', '_')
                        log.warning('%s=%r field in torrent %r is wrongly encoded, falling back to `%s`' %
                                    (field, item[field], self.content['info']['name'], fallback))
                        item[field] = fallback

        return files

    @property
    def size(self):
        """Return total size of the torrent"""
        size = 0
        # single file torrent
        if 'length' in self.content['info']:
            size = int(self.content['info']['length'])
        else:
            # multifile torrent
            for item in self.content['info']['files']:
                size += int(item['length'])
        return size

    @property
    def private(self):
        return self.content['info'].get('private', False)

    @property
    def trackers(self):
        """
        :returns: List of trackers, supports single-tracker and multi-tracker implementations
        """
        trackers = []
        # the spec says, if announce-list present use ONLY that
        # funny iteration because of nesting, ie:
        # [ [ tracker1, tracker2 ], [backup1] ]
        for tl in self.content.get('announce-list', []):
            for t in tl:
                trackers.append(t)
        if not self.content.get('announce') in trackers:
            trackers.append(self.content.get('announce'))
        return trackers

    @property
    def info_hash(self):
        """Return Torrent info hash"""
        import hashlib
        hash = hashlib.sha1()
        info_data = encode_dictionary(self.content['info'])
        hash.update(info_data)
        return hash.hexdigest().upper()

    @property
    def comment(self):
        return self.content['comment']

    @comment.setter
    def comment(self, comment):
        self.content['comment'] = comment
        self.modified = True

    def remove_multitracker(self, tracker):
        """Removes passed multi-tracker from this torrent"""
        for tl in self.content.get('announce-list', [])[:]:
            try:
                tl.remove(tracker)
                self.modified = True
                # if no trackers left in list, remove whole list
                if not tl:
                    self.content['announce-list'].remove(tl)
            except:
                pass

    def add_multitracker(self, tracker):
        """Appends multi-tracker to this torrent"""
        self.content.setdefault('announce-list', [])
        self.content['announce-list'].append([tracker])
        self.modified = True

    def __str__(self):
        return '<Torrent instance. Files: %s>' % self.get_filelist()

    def encode(self):
        return bencode(self.content)

########NEW FILE########
__FILENAME__ = cached_input
from __future__ import unicode_literals, division, absolute_import
import copy
import logging
import hashlib
from datetime import datetime, timedelta
from sqlalchemy import Column, Integer, String, DateTime, PickleType, Unicode, ForeignKey
from sqlalchemy.orm import relation
from flexget import db_schema
from flexget.utils.database import safe_pickle_synonym
from flexget.utils.tools import parse_timedelta, TimedDict
from flexget.entry import Entry
from flexget.event import event
from flexget.plugin import PluginError

log = logging.getLogger('input_cache')
Base = db_schema.versioned_base('input_cache', 0)


class InputCache(Base):

    __tablename__ = 'input_cache'

    id = Column(Integer, primary_key=True)
    name = Column(Unicode)
    hash = Column(String)
    added = Column(DateTime, default=datetime.now)

    entries = relation('InputCacheEntry', backref='cache', cascade='all, delete, delete-orphan')


class InputCacheEntry(Base):

    __tablename__ = 'input_cache_entry'

    id = Column(Integer, primary_key=True)
    _entry = Column('entry', PickleType)
    entry = safe_pickle_synonym('_entry')

    cache_id = Column(Integer, ForeignKey('input_cache.id'), nullable=False)


@event('manager.db_cleanup')
def db_cleanup(session):
    """Removes old input caches from plugins that are no longer configured."""
    result = session.query(InputCache).filter(InputCache.added < datetime.now() - timedelta(days=7)).delete()
    if result:
        log.verbose('Removed %s old input caches.' % result)


def config_hash(config):
    """
    :param dict config: Configuration
    :return: MD5 hash for *config*
    """
    if isinstance(config, dict):
        # this does in fact support nested dicts, they're sorted too!
        return hashlib.md5(str(sorted(config.items()))).hexdigest()
    else:
        return hashlib.md5(str(config)).hexdigest()


class cached(object):
    """
    Implements transparent caching decorator @cached for inputs.

    Decorator has two parameters:

    * **name** in which the configuration is present in tasks configuration.
    * **key** in which the configuration has the cached resource identifier (ie. url).
      If the key is not given or present in the configuration :name: is expected to be a cache name (ie. url)

    .. note:: Configuration assumptions may make this unusable in some (future) inputs
    """

    cache = TimedDict(cache_time='5 minutes')

    def __init__(self, name, persist=None):
        # Cast name to unicode to prevent sqlalchemy warnings when filtering
        self.name = unicode(name)
        # Parse persist time
        self.persist = persist and parse_timedelta(persist)

    def __call__(self, func):

        def wrapped_func(*args, **kwargs):
            # get task from method parameters
            task = args[1]

            # detect api version
            api_ver = 1
            if len(args) == 3:
                api_ver = 2

            if api_ver == 1:
                # get name for a cache from tasks configuration
                if not self.name in task.config:
                    raise Exception('@cache config name %s is not configured in task %s' % (self.name, task.name))
                hash = config_hash(task.config[self.name])
            else:
                hash = config_hash(args[2])

            log.trace('self.name: %s' % self.name)
            log.trace('hash: %s' % hash)

            cache_name = self.name + '_' + hash
            log.debug('cache name: %s (has: %s)' % (cache_name, ', '.join(self.cache.keys())))

            if not task.options.nocache and cache_name in self.cache:
                # return from the cache
                log.trace('cache hit')
                entries = []
                for entry in self.cache[cache_name]:
                    fresh = copy.deepcopy(entry)
                    entries.append(fresh)
                if entries:
                    log.verbose('Restored %s entries from cache' % len(entries))
                return entries
            else:
                if self.persist and not task.options.nocache:
                    # Check database cache
                    db_cache = task.session.query(InputCache).filter(InputCache.name == self.name).\
                        filter(InputCache.hash == hash).\
                        filter(InputCache.added > datetime.now() - self.persist).\
                        first()
                    if db_cache:
                        entries = [Entry(e.entry) for e in db_cache.entries]
                        log.verbose('Restored %s entries from db cache' % len(entries))
                        # Store to in memory cache
                        self.cache[cache_name] = copy.deepcopy(entries)
                        return entries

                # Nothing was restored from db or memory cache, run the function
                log.trace('cache miss')
                # call input event
                try:
                    response = func(*args, **kwargs)
                except PluginError as e:
                    # If there was an error producing entries, but we have valid entries in the db cache, return those.
                    if self.persist and not task.options.nocache:
                        db_cache = task.session.query(InputCache).filter(InputCache.name == self.name).\
                            filter(InputCache.hash == hash).first()
                        if db_cache and db_cache.entries:
                            log.error('There was an error during %s input (%s), using cache instead.' %
                                    (self.name, e))
                            entries = [Entry(e.entry) for e in db_cache.entries]
                            log.verbose('Restored %s entries from db cache' % len(entries))
                            # Store to in memory cache
                            self.cache[cache_name] = copy.deepcopy(entries)
                            return entries
                    # If there was nothing in the db cache, re-raise the error.
                    raise
                if api_ver == 1:
                    response = task.entries
                if not isinstance(response, list):
                    log.warning('Input %s did not return a list, cannot cache.' % self.name)
                    return response
                # store results to cache
                log.debug('storing to cache %s %s entries' % (cache_name, len(response)))
                try:
                    self.cache[cache_name] = copy.deepcopy(response)
                except TypeError:
                    # might be caused because of backlog restoring some idiotic stuff, so not neccessarily a bug
                    log.critical('Unable to save task content into cache, if problem persists longer than a day please report this as a bug')
                if self.persist:
                    # Store to database
                    log.debug('Storing cache %s to database.' % cache_name)
                    db_cache = task.session.query(InputCache).filter(InputCache.name == self.name).\
                        filter(InputCache.hash == hash).first()
                    if not db_cache:
                        db_cache = InputCache(name=self.name, hash=hash)
                    db_cache.entries = [InputCacheEntry(entry=e) for e in response]
                    db_cache.added = datetime.now()
                    task.session.merge(db_cache)
                return response

        return wrapped_func

########NEW FILE########
__FILENAME__ = database
from __future__ import unicode_literals, division, absolute_import
from datetime import datetime

from sqlalchemy import extract, func
from sqlalchemy.orm import synonym
from sqlalchemy.ext.hybrid import Comparator, hybrid_property

from flexget.manager import Session
from flexget.utils import qualities


def with_session(func):
    """"
    A decorator which creates a new session if one was not passed
    via keyword argument to the function.

    Automatically commits and closes the session if one was created,
    caller is responsible for commit if passed in.
    """

    def wrapper(*args, **kwargs):
        if not kwargs.get('session'):
            kwargs['session'] = Session(autoflush=True, expire_on_commit=False)
            try:
                result = func(*args, **kwargs)
                kwargs['session'].commit()
                return result
            finally:
                kwargs['session'].close()
        else:
            return func(*args, **kwargs)
    return wrapper


def pipe_list_synonym(name):
    """Converts pipe separated text into a list"""

    def getter(self):
        attr = getattr(self, name)
        if attr:
            return attr.strip('|').split('|')

    def setter(self, value):
        if isinstance(value, basestring):
            setattr(self, name, value)
        else:
            setattr(self, name, '|'.join(value))

    return synonym(name, descriptor=property(getter, setter))


def text_date_synonym(name):
    """Converts Y-M-D date strings into datetime objects"""

    def getter(self):
        return getattr(self, name)

    def setter(self, value):
        if isinstance(value, basestring):
            try:
                setattr(self, name, datetime.strptime(value, '%Y-%m-%d'))
            except ValueError:
                # Invalid date string given, set to None
                setattr(self, name, None)
        else:
            setattr(self, name, value)

    return synonym(name, descriptor=property(getter, setter))


def safe_pickle_synonym(name):
    """Used to store Entry instances into a PickleType column in the database.

    In order to ensure everything can be loaded after code changes, makes sure no custom python classes are pickled.
    """

    def only_builtins(item):
        """Casts all subclasses of builtin types to their builtin python type. Works recursively on iterables.

        Raises ValueError if passed an object that doesn't subclass a builtin type.
        """

        supported_types = [str, unicode, int, float, long, bool, datetime]
        # dict, list, tuple and set are also supported, but handled separately

        if type(item) in supported_types:
            return item
        elif isinstance(item, dict):
            result = {}
            for key, value in item.iteritems():
                try:
                    result[key] = only_builtins(value)
                except TypeError:
                    continue
            return result
        elif isinstance(item, (list, tuple, set)):
            result = []
            for value in item:
                try:
                    result.append(only_builtins(value))
                except ValueError:
                    continue
            if isinstance(item, list):
                return result
            elif isinstance(item, tuple):
                return tuple(result)
            else:
                return set(result)
        else:
            for s_type in supported_types:
                if isinstance(item, s_type):
                    return s_type(item)

        # If item isn't a subclass of a builtin python type, raise ValueError.
        raise TypeError('%r is not a subclass of a builtin python type.' % type(item))

    def getter(self):
        return getattr(self, name)

    def setter(self, entry):
        setattr(self, name, only_builtins(entry))

    return synonym(name, descriptor=property(getter, setter))


class CaseInsensitiveWord(Comparator):
    """Hybrid value representing a string that compares case insensitively."""

    def __init__(self, word):
        if isinstance(word, CaseInsensitiveWord):
            self.word = word.word
        else:
            self.word = word

    def lower(self):
        if isinstance(self.word, basestring):
            return self.word.lower()
        else:
            return func.lower(self.word)

    def operate(self, op, other):
        if not isinstance(other, CaseInsensitiveWord):
            other = CaseInsensitiveWord(other)
        return op(self.lower(), other.lower())

    def __clause_element__(self):
        return self.lower()

    def __str__(self):
        return self.word

    def __getattr__(self, item):
        """Expose string methods to be called directly on this object."""
        return getattr(self.word, item)


def quality_property(text_attr):

    def getter(self):
        return qualities.Quality(getattr(self, text_attr))

    def setter(self, value):
        if isinstance(value, basestring):
            setattr(self, text_attr, value)
        else:
            setattr(self, text_attr, value.name)

    class QualComparator(Comparator):
        def operate(self, op, other):
            if isinstance(other, qualities.Quality):
                other = other.name
            return op(self.__clause_element__(), other)

    def comparator(self):
        return QualComparator(getattr(self, text_attr))

    prop = hybrid_property(getter, setter)
    prop.comparator(comparator)
    return prop


def quality_requirement_property(text_attr):

    def getter(self):
        return qualities.Requirements(getattr(self, text_attr))

    def setter(self, value):
        if isinstance(value, basestring):
            setattr(self, text_attr, value)
        else:
            setattr(self, text_attr, value.text)

    prop = hybrid_property(getter, setter)
    return prop


def ignore_case_property(text_attr):

    def getter(self):
        return CaseInsensitiveWord(getattr(self, text_attr))

    def setter(self, value):
        setattr(self, text_attr, value)

    return hybrid_property(getter, setter)


def year_property(date_attr):

    def getter(self):
        date = getattr(self, date_attr)
        return date and date.year

    def expr(cls):
        return extract('year', getattr(cls, date_attr))

    return hybrid_property(getter, expr=expr)

########NEW FILE########
__FILENAME__ = imdb
from __future__ import unicode_literals, division, absolute_import
import difflib
import logging
import re

from bs4.element import Tag

from flexget.utils.soup import get_soup
from flexget.utils.requests import Session
from flexget.utils.tools import str_to_int


log = logging.getLogger('utils.imdb')
# IMDb delivers a version of the page which is unparsable to unknown (and some known) user agents, such as requests'
# Spoof the old urllib user agent to keep results consistent
requests = Session()
requests.headers.update({'User-Agent': 'Python-urllib/2.6'})
#requests.headers.update({'User-Agent': random.choice(USERAGENTS)})

# this makes most of the titles to be returned in english translation, but not all of them
requests.headers.update({'Accept-Language': 'en-US,en;q=0.8'})

# give imdb a little break between requests (see: http://flexget.com/ticket/129#comment:1)
requests.set_domain_delay('imdb.com', '3 seconds')


def is_imdb_url(url):
    """Tests the url to see if it's for imdb.com."""
    if not isinstance(url, basestring):
        return
    # Probably should use urlparse.
    return re.match(r'https?://[^/]*imdb\.com/', url)


def extract_id(url):
    """Return IMDb ID of the given URL. Return None if not valid or if URL is not a string."""
    if not isinstance(url, basestring):
        return
    m = re.search(r'((?:nm|tt)[\d]{7})', url)
    if m:
        return m.group(1)


def make_url(imdb_id):
    """Return IMDb URL of the given ID"""
    return u'http://www.imdb.com/title/%s/' % imdb_id


class ImdbSearch(object):

    def __init__(self):
        # de-prioritize aka matches a bit
        self.aka_weight = 0.95
        # prioritize first
        self.first_weight = 1.1
        self.min_match = 0.5
        self.min_diff = 0.01
        self.debug = False

        self.max_results = 10

    def ireplace(self, text, old, new, count=0):
        """Case insensitive string replace"""
        pattern = re.compile(re.escape(old), re.I)
        return re.sub(pattern, new, text, count)

    def smart_match(self, raw_name):
        """Accepts messy name, cleans it and uses information available to make smartest and best match"""
        from flexget.utils.titles.movie import MovieParser
        parser = MovieParser()
        parser.data = raw_name
        parser.parse()
        name = parser.name
        year = parser.year
        if name == '':
            log.critical('Failed to parse name from %s' % raw_name)
            return None
        log.debug('smart_match name=%s year=%s' % (name, str(year)))
        return self.best_match(name, year)

    def best_match(self, name, year=None):
        """Return single movie that best matches name criteria or None"""
        movies = self.search(name)

        if not movies:
            log.debug('search did not return any movies')
            return None

        # remove all movies below min_match, and different year
        for movie in movies[:]:
            if year and movie.get('year'):
                if movie['year'] != str(year):
                    log.debug('best_match removing %s - %s (wrong year: %s)' % (movie['name'], movie['url'], str(movie['year'])))
                    movies.remove(movie)
                    continue
            if movie['match'] < self.min_match:
                log.debug('best_match removing %s (min_match)' % movie['name'])
                movies.remove(movie)
                continue

        if not movies:
            log.debug('FAILURE: no movies remain')
            return None

        # if only one remains ..
        if len(movies) == 1:
            log.debug('SUCCESS: only one movie remains')
            return movies[0]

        # check min difference between best two hits
        diff = movies[0]['match'] - movies[1]['match']
        if diff < self.min_diff:
            log.debug('unable to determine correct movie, min_diff too small (`%s` <-?-> `%s`)' %
                      (movies[0], movies[1]))
            for m in movies:
                log.debug('remain: %s (match: %s) %s' % (m['name'], m['match'], m['url']))
            return None
        else:
            return movies[0]

    def search(self, name):
        """Return array of movie details (dict)"""
        log.debug('Searching: %s' % name)
        url = u'http://www.imdb.com/find'
        # This will only include movies searched by title in the results
        params = {'q': name, 's': 'tt', 'ttype': 'ft'}

        log.debug('Serch query: %s' % repr(url))
        page = requests.get(url, params=params)
        actual_url = page.url

        movies = []
        # in case we got redirected to movie page (perfect match)
        re_m = re.match(r'.*\.imdb\.com/title/tt\d+/', actual_url)
        if re_m:
            actual_url = re_m.group(0)
            log.debug('Perfect hit. Search got redirected to %s' % actual_url)
            movie = {}
            movie['match'] = 1.0
            movie['name'] = name
            movie['url'] = actual_url
            movie['imdb_id'] = extract_id(actual_url)
            movie['year'] = None  # skips year check
            movies.append(movie)
            return movies

        # the god damn page has declared a wrong encoding
        soup = get_soup(page.text)

        section_table = soup.find('table', 'findList')
        if not section_table:
            log.debug('results table not found')
            return

        rows = section_table.find_all('td', 'result_text')
        if not rows:
            log.debug('Titles section does not have links')
        for count, row in enumerate(rows):
            # Title search gives a lot of results, only check the first ones
            if count > self.max_results:
                break

            movie = {}
            additional = re.findall(r'\((.*?)\)', row.text)
            if len(additional) > 0:
                movie['year'] = additional[-1]

            link = row.find_next('a')
            movie['name'] = link.text
            movie['url'] = 'http://www.imdb.com' + link.get('href')
            movie['imdb_id'] = extract_id(movie['url'])
            log.debug('processing name: %s url: %s' % (movie['name'], movie['url']))

            # calc & set best matching ratio
            seq = difflib.SequenceMatcher(lambda x: x == ' ', movie['name'].title(), name.title())
            ratio = seq.ratio()

            # check if some of the akas have better ratio
            for aka in link.parent.find_all('i'):
                aka = aka.next.string
                match = re.search(r'".*"', aka)
                if not match:
                    log.debug('aka `%s` is invalid' % aka)
                    continue
                aka = match.group(0).replace('"', '')
                log.trace('processing aka %s' % aka)
                seq = difflib.SequenceMatcher(lambda x: x == ' ', aka.title(), name.title())
                aka_ratio = seq.ratio()
                if aka_ratio > ratio:
                    ratio = aka_ratio * self.aka_weight
                    log.debug('- aka `%s` matches better to `%s` ratio %s (weighted to %s)' %
                              (aka, name, aka_ratio, ratio))

            # prioritize items by position
            position_ratio = (self.first_weight - 1) / (count + 1) + 1
            log.debug('- prioritizing based on position %s `%s`: %s' % (count, movie['url'], position_ratio))
            ratio *= position_ratio

            # store ratio
            movie['match'] = ratio
            movies.append(movie)

        movies.sort(key=lambda x: x['match'], reverse=True)
        return movies


class ImdbParser(object):
    """Quick-hack to parse relevant imdb details"""

    def __init__(self):
        self.genres = []
        self.languages = []
        self.actors = {}
        self.directors = {}
        self.score = 0.0
        self.votes = 0
        self.year = 0
        self.plot_outline = None
        self.name = None
        self.original_name = None
        self.url = None
        self.imdb_id = None
        self.photo = None
        self.mpaa_rating = ''

    def __str__(self):
        return '<ImdbParser(name=%s,imdb_id=%s)>' % (self.name, self.imdb_id)

    def parse(self, imdb_id):
        self.imdb_id = extract_id(imdb_id)
        url = make_url(self.imdb_id)
        self.url = url
        page = requests.get(url)
        soup = get_soup(page.text)

        # get photo
        tag_photo = soup.find('td', attrs={'id': 'img_primary'})
        if tag_photo:
            tag_img = tag_photo.find('img')
            if tag_img:
                self.photo = tag_img.get('src')
                log.debug('Detected photo: %s' % self.photo)

        # get rating. contentRating <span> in infobar.
        tag_infobar_div = soup.find('div', attrs={'class': 'infobar'})
        if tag_infobar_div:
            tag_mpaa_rating = tag_infobar_div.find('span', attrs={'itemprop': 'contentRating'})
            if tag_mpaa_rating:
                if not tag_mpaa_rating.get('class') or not tag_mpaa_rating['class'][0].startswith('us_'):
                    log.warning('Could not determine mpaa rating for %s' % url)
                else:
                    rating_class = tag_mpaa_rating['class'][0]
                    if rating_class == 'us_not_rated':
                        self.mpaa_rating = 'NR'
                    else:
                        self.mpaa_rating = rating_class.lstrip('us_').replace('_', '-').upper()
                log.debug('Detected mpaa rating: %s' % self.mpaa_rating)
            else:
                log.debug('Unable to match signature of mpaa rating for %s - '
                          'could be a TV episode, or plugin needs update?' % url)
        else:
            # We should match the infobar, it's an integral part of the IMDB page.
            log.warning('Unable to get infodiv class for %s - plugin needs update?' % url)

        # get name
        tag_name = soup.find('h1')
        if tag_name:
            tag_name = tag_name.find('span', attrs={'itemprop': 'name'})
        if tag_name:
            self.name = tag_name.text
            log.debug('Detected name: %s' % self.name)
        else:
            log.warning('Unable to get name for %s - plugin needs update?' % url)

        tag_original_title_i = soup.find('i', text=re.compile(r'original title'))
        if tag_original_title_i:
            span = tag_original_title_i.parent
            tag_original_title_i.decompose()
            self.original_name = span.text.strip().strip('"')
            log.debug('Detected original name: %s' % self.original_name)
        else:
            # if title is already in original language, it doesn't have the tag
            log.debug('Unable to get original title for %s - it probably does not exists' % url)

        star_box = soup.find('div', attrs={'class': 'star-box giga-star'})
        if star_box:
            # detect if movie is eligible for ratings
            rating_ineligible = star_box.find('div', attrs={'class': 'rating-ineligible'})
            if rating_ineligible:
                log.debug('movie is not eligible for ratings')
            else:
                # get votes
                tag_votes = star_box.find(itemprop='ratingCount')
                if tag_votes:
                    self.votes = str_to_int(tag_votes.string) or 0
                    log.debug('Detected votes: %s' % self.votes)
                else:
                    log.warning('Unable to get votes for %s - plugin needs update?' % url)

                # get score - find the ratingValue item that contains a numerical value
                span_score = star_box.find(itemprop='ratingValue', text=re.compile('[\d\.]+'))
                if span_score:
                    try:
                        self.score = float(span_score.string)
                    except (ValueError, TypeError):
                        log.debug('tag_score %r is not valid float' % span_score.string)
                    log.debug('Detected score: %s' % self.score)
                else:
                    log.warning('Unable to get score for %s - plugin needs update?' % url)
        else:
            log.warning('Unable to find score/vote section for %s - plugin needs update?' % url)

        # get genres
        genres = soup.find('div', itemprop='genre')
        if genres:
            for link in genres.find_all('a'):
                self.genres.append(link.text.strip().lower())
        else:
            log.warning('Unable to find genres section for %s - plugin needs update?' % url)

        # get languages
        for link in soup.find_all('a', href=re.compile('/language/.*')):
            # skip non-primary languages "(a few words)", etc.
            m = re.search('(?x) \( [^()]* \\b few \\b', link.next_sibling)
            if not m:
                lang = link.text.lower()
                if not lang in self.languages:
                    self.languages.append(lang.strip())

        # get year
        tag_year = soup.find('a', attrs={'href': re.compile('^/year/\d+')})
        if tag_year:
            self.year = int(tag_year.text)
            log.debug('Detected year: %s' % self.year)
        elif soup.head.title:
            m = re.search(r'(\d{4})\)', soup.head.title.string)
            if m:
                self.year = int(m.group(1))
                log.debug('Detected year: %s' % self.year)
            else:
                log.warning('Unable to get year for %s (regexp mismatch) - plugin needs update?' % url)
        else:
            log.warning('Unable to get year for %s (missing title) - plugin needs update?' % url)

        # get main cast
        tag_cast = soup.find('table', 'cast_list')
        if tag_cast:
            for actor in tag_cast.find_all('a', href=re.compile('/name/nm')):
                actor_id = extract_id(actor['href'])
                actor_name = actor.text.strip()
                # tag instead of name
                if isinstance(actor_name, Tag):
                    actor_name = None
                self.actors[actor_id] = actor_name

        # get director(s)
        h4_director = soup.find('h4', text=re.compile('Director'))
        if h4_director:
            for director in h4_director.parent.find_all('a', href=re.compile('/name/nm')):
                director_id = extract_id(director['href'])
                director_name = director.text
                # tag instead of name
                if isinstance(director_name, Tag):
                    director_name = None
                self.directors[director_id] = director_name

        log.debug('Detected genres: %s' % self.genres)
        log.debug('Detected languages: %s' % self.languages)
        log.debug('Detected director(s): %s' % ', '.join(self.directors))
        log.debug('Detected actors: %s' % ', '.join(self.actors))

        # get plot
        h2_plot = soup.find('h2', text='Storyline')
        if h2_plot:
            p_plot = h2_plot.find_next('p')
            if p_plot and p_plot.next.string:
                self.plot_outline = p_plot.next.string.strip()
                log.debug('Detected plot outline: %s' % self.plot_outline)
            else:
                log.debug('Plot does not have p-tag')
        else:
            log.debug('Failed to find plot')

########NEW FILE########
__FILENAME__ = json
"""
Helper module that can load whatever version of the json module is available.
Plugins can just import the methods from this module.
"""
from __future__ import unicode_literals, division, absolute_import
from flexget.plugin import DependencyError

try:
    import simplejson as json
except ImportError:
    try:
        import json
    except ImportError:
        try:
            # Google Appengine offers simplejson via django
            from django.utils import simplejson as json
        except ImportError:
            raise DependencyError(missing='simplejson')

load = json.load
loads = json.loads
dump = json.dump
dumps = json.dumps

########NEW FILE########
__FILENAME__ = log
"""Logging utilities"""

from __future__ import unicode_literals, division, absolute_import
import logging
import hashlib
from datetime import datetime, timedelta
from sqlalchemy import Column, Integer, String, DateTime, Index
from flexget import db_schema
from flexget.utils.sqlalchemy_utils import table_schema
from flexget.manager import Session
from flexget.event import event
from flexget import logger as f_logger

log = logging.getLogger('util.log')
Base = db_schema.versioned_base('log_once', 0)


@db_schema.upgrade('log_once')
def upgrade(ver, session):
    if ver is None:
        log.info('Adding index to md5sum column of log_once table.')
        table = table_schema('log_once', session)
        Index('log_once_md5sum', table.c.md5sum, unique=True).create()
        ver = 0
    return ver


class LogMessage(Base):
    """Declarative"""

    __tablename__ = 'log_once'

    id = Column(Integer, primary_key=True)
    md5sum = Column(String, unique=True)
    added = Column(DateTime, default=datetime.now())

    def __init__(self, md5sum):
        self.md5sum = md5sum

    def __repr__(self):
        return "<LogMessage('%s')>" % self.md5sum


@event('manager.db_cleanup')
def purge(session):
    """Purge old messages from database"""
    old = datetime.now() - timedelta(days=365)

    result = session.query(LogMessage).filter(LogMessage.added < old).delete()
    if result:
        log.verbose('Purged %s entries from log_once table.' % result)


def log_once(message, logger=logging.getLogger('log_once'), once_level=logging.INFO, suppressed_level=f_logger.VERBOSE):
    """
    Log message only once using given logger`. Returns False if suppressed logging.
    When suppressed, `suppressed_level` level is still logged.
    """

    digest = hashlib.md5()
    digest.update(message.encode('latin1', 'replace')) # ticket:250
    md5sum = digest.hexdigest()

    session = Session()
    try:
        # abort if this has already been logged
        if session.query(LogMessage).filter_by(md5sum=md5sum).first():
            logger.log(suppressed_level, message)
            return False

        row = LogMessage(md5sum)
        session.add(row)
        session.commit()
    finally:
        session.close()

    logger.log(once_level, message)
    return True

########NEW FILE########
__FILENAME__ = pathscrub
from __future__ import unicode_literals, division, absolute_import
import ntpath
import sys
import re

os_mode = None  # Can be 'windows', 'mac', 'linux' or None. None will auto-detect os.
# Replacement order is important, don't use dicts to store
platform_replaces = {
    'windows': [
        ['[:*?"<>| ]+', ' '],  # Turn illegal characters into a space
        [r'[\.\s]+([/\\]|$)', r'\1']],  # Dots cannot end file or directory names
    'mac': [
        ['[: ]+', ' ']],  # Only colon is illegal here
    'linux': []}  # No illegal chars


def pathscrub(dirty_path, os=None, filename=False):
    """
    Strips illegal characters for a given os from a path.

    :param dirty_path: Path to be scrubbed.
    :param os: Defines which os mode should be used, can be 'windows', 'mac', 'linux', or None to auto-detect
    :param filename: If this is True, path separators will be replaced with '-'
    :return: A valid path.
    """

    # See if global os_mode has been defined by pathscrub plugin
    if os_mode and not os:
        os = os_mode

    if not os:
        # If os is not defined, try to detect appropriate
        drive, path = ntpath.splitdrive(dirty_path)
        if sys.platform.startswith('win') or drive:
            os = 'windows'
        elif sys.platform.startswith('darwin'):
            os = 'mac'
        else:
            os = 'linux'
    replaces = platform_replaces[os]

    # Make sure not to mess with windows drive specifications
    drive, path = ntpath.splitdrive(dirty_path)

    if filename:
        path = path.replace('/', ' ').replace('\\', ' ')
    # Remove spaces surrounding path components
    path = '/'.join(comp.strip() for comp in path.split('/'))
    if os == 'windows':
        path = '\\'.join(comp.strip() for comp in path.split('\\'))
    for search, replace in replaces:
        path = re.sub(search, replace, path)
    path = path.strip()
    # If we stripped everything from a filename, complain
    if filename and dirty_path and not path:
        raise ValueError('Nothing was left after stripping invalid characters from path `%s`!' % dirty_path)
    return drive + path

########NEW FILE########
__FILENAME__ = qualities
from __future__ import unicode_literals, division, absolute_import
import re
import copy
import logging

log = logging.getLogger('utils.qualities')


class QualityComponent(object):
    """"""
    def __init__(self, type, value, name, regexp=None, modifier=None, defaults=None):
        """
        :param type: Type of quality component. (resolution, source, codec, or audio)
        :param value: Value used to sort this component with others of like type.
        :param name: Canonical name for this quality component.
        :param regexp: Regexps used to match this component.
        :param modifier: An integer that affects sorting above all other components.
        :param defaults: An iterable defining defaults for other quality components if this component matches.
        """

        if type not in ['resolution', 'source', 'codec', 'audio']:
            raise ValueError('%s is not a valid quality component type.' % type)
        self.type = type
        self.value = value
        self.name = name
        self.modifier = modifier
        self.defaults = defaults or []

        # compile regexp
        if regexp is None:
            regexp = re.escape(name)
        self.regexp = re.compile('(?<![^\W_])(' + regexp + ')(?![^\W_])', re.IGNORECASE)

    def matches(self, text):
        """Test if quality matches to text.

        :param string text: data te be tested against
        :returns: tuple (matches, remaining text without quality data)
        """

        match = self.regexp.search(text)
        if not match:
            return False, ""
        else:
            # remove matching part from the text
            text = text[:match.start()] + text[match.end():]
        return True, text

    def __hash__(self):
        return hash(self.type + str(self.value))

    def __nonzero__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, basestring):
            other = _registry.get(other)
        if not isinstance(other, QualityComponent):
            raise TypeError('Cannot compare %r and %r' % (self, other))
        if other.type == self.type:
            return self.value == other.value
        else:
            raise TypeError('Cannot compare %s and %s' % (self.type, other.type))

    def __ne__(self, other):
        return not self.__eq__(other)

    def __lt__(self, other):
        if isinstance(other, basestring):
            other = _registry.get(other)
        if not isinstance(other, QualityComponent):
            raise TypeError('Cannot compare %r and %r' % (self, other))
        if other.type == self.type:
            return self.value < other.value
        else:
            raise TypeError('Cannot compare %s and %s' % (self.type, other.type))

    def __ge__(self, other):
        return not self.__lt__(other)

    def __le__(self, other):
        return self.__lt__(other) or self.__eq__(other)

    def __gt__(self, other):
        return not self.__le__(other)

    def __add__(self, other):
        if not isinstance(other, int):
            raise TypeError()
        l = globals().get('_' + self.type + 's')
        index = l.index(self) + other
        if index >= len(l):
            index = -1
        return l[index]

    def __sub__(self, other):
        if not isinstance(other, int):
            raise TypeError()
        l = globals().get('_' + self.type + 's')
        index = l.index(self) - other
        if index < 0:
            index = 0
        return l[index]

    def __repr__(self):
        return '<%s(name=%s,value=%s)>' % (self.type.title(), self.name, self.value)

    def __str__(self):
        return self.name

    def __deepcopy__(self, memo=None):
        # No mutable attributes, return a regular copy
        return copy.copy(self)

_resolutions = [
    QualityComponent('resolution', 10, '360p'),
    QualityComponent('resolution', 20, '368p', '368p?'),
    QualityComponent('resolution', 30, '480p', '480p?'),
    QualityComponent('resolution', 40, '576p', '576p?'),
    QualityComponent('resolution', 45, 'hr'),
    QualityComponent('resolution', 50, '720i'),
    QualityComponent('resolution', 60, '720p', '(1280x)?720p?x?'),
    QualityComponent('resolution', 70, '1080i'),
    QualityComponent('resolution', 80, '1080p', '(1920x)?1080p?')
]
_sources = [
    QualityComponent('source', 10, 'workprint', modifier=-8),
    QualityComponent('source', 20, 'cam', '(?:hd)?cam', modifier=-7),
    QualityComponent('source', 30, 'ts', '(?:hd)?ts|telesync', modifier=-6),
    QualityComponent('source', 40, 'tc', 'tc|telecine', modifier=-5),
    QualityComponent('source', 50, 'r5', 'r[2-8c]', modifier=-4),
    QualityComponent('source', 60, 'hdrip', 'hd[\W_]?rip', modifier=-3),
    QualityComponent('source', 70, 'ppvrip', 'ppv[\W_]?rip', modifier=-2),
    QualityComponent('source', 80, 'preair', modifier=-1),
    QualityComponent('source', 90, 'tvrip', 'tv[\W_]?rip'),
    QualityComponent('source', 100, 'dsr', 'dsr|ds[\W_]?rip'),
    QualityComponent('source', 110, 'sdtv', '(?:[sp]dtv|dvb)(?:[\W_]?rip)?'),
    QualityComponent('source', 120, 'webrip', 'web[\W_]?rip'),
    QualityComponent('source', 130, 'dvdscr', '(?:(?:dvd|web)[\W_]?)?scr(?:eener)?', modifier=0),
    QualityComponent('source', 140, 'bdscr', 'bdscr(?:eener)?'),
    QualityComponent('source', 150, 'hdtv', 'a?hdtv(?:[\W_]?rip)?'),
    QualityComponent('source', 160, 'webdl', 'web(?:[\W_]?(dl|hd))?'),
    QualityComponent('source', 170, 'dvdrip', 'dvd(?:[\W_]?rip)?'),
    QualityComponent('source', 175, 'remux'),
    QualityComponent('source', 180, 'bluray', '(?:b[dr][\W_]?rip|blu[\W_]?ray(?:[\W_]?rip)?)')
]
_codecs = [
    QualityComponent('codec', 10, 'divx'),
    QualityComponent('codec', 20, 'xvid'),
    QualityComponent('codec', 30, 'h264', '[hx].?264'),
    QualityComponent('codec', 40, '10bit', '10.?bit|hi10p')
]
channels = '(?:(?:[\W_]?5[\W_]?1)|(?:[\W_]?2[\W_]?(?:0|ch)))'
_audios = [
    QualityComponent('audio', 10, 'mp3'),
    # TODO: No idea what order these should go in or if we need different regexps
    QualityComponent('audio', 20, 'dd5.1', 'dd%s' % channels),
    QualityComponent('audio', 30, 'aac', 'aac%s?' % channels),
    QualityComponent('audio', 40, 'ac3', 'ac3%s?' % channels),
    QualityComponent('audio', 50, 'flac', 'flac%s?' % channels),
    # The DTSs are a bit backwards, but the more specific one needs to be parsed first
    QualityComponent('audio', 60, 'dtshd', 'dts[\W_]?hd(?:[\W_]?ma)?'),
    QualityComponent('audio', 70, 'dts'),
    QualityComponent('audio', 80, 'truehd')
]

_UNKNOWNS = {
    'resolution': QualityComponent('resolution', 0, 'unknown'),
    'source': QualityComponent('source', 0, 'unknown'),
    'codec': QualityComponent('codec', 0, 'unknown'),
    'audio': QualityComponent('audio', 0, 'unknown')
}

# For wiki generating help
'''for type in (_resolutions, _sources, _codecs, _audios):
    print '{{{#!td style="vertical-align: top"'
    for item in reversed(type):
        print '- ' + item.name
    print '}}}'
'''


_registry = {}
for items in (_resolutions, _sources, _codecs, _audios):
    for item in items:
        _registry[item.name] = item


def all_components():
    return _registry.itervalues()


class Quality(object):
    """Parses and stores the quality of an entry in the four component categories."""

    def __init__(self, text=''):
        """
        :param text: A string to parse quality from
        """
        self.text = text
        self.clean_text = text
        if text:
            self.parse(text)
        else:
            self.resolution = _UNKNOWNS['resolution']
            self.source = _UNKNOWNS['source']
            self.codec = _UNKNOWNS['codec']
            self.audio = _UNKNOWNS['audio']

    def parse(self, text):
        """Parses a string to determine the quality in the four component categories.

        :param text: The string to parse
        """
        self.text = text
        self.clean_text = text
        self.resolution = self._find_best(_resolutions, _UNKNOWNS['resolution'])
        self.source = self._find_best(_sources, _UNKNOWNS['source'])
        self.codec = self._find_best(_codecs, _UNKNOWNS['codec'])
        self.audio = self._find_best(_audios, _UNKNOWNS['audio'])
        # If any of the matched components have defaults, set them now.
        for component in self.components:
            for default in component.defaults:
                default = _registry[default]
                if not getattr(self, default.type):
                    setattr(self, default.type, default)

    def _find_best(self, qlist, default=None):
        """Finds the highest matching quality component from `qlist`"""
        result = None
        for item in qlist:
            match = item.matches(self.clean_text)
            if match[0]:
                result = item
                self.clean_text = match[1]
                if item.modifier is not None:
                    # If this item has a modifier, do not proceed to check higher qualities in the list
                    break
        return result or default

    @property
    def name(self):
        name = ' '.join(str(p) for p in (self.resolution, self.source, self.codec, self.audio) if p.value != 0)
        return name or 'unknown'

    @property
    def components(self):
        return [self.resolution, self.source, self.codec, self.audio]

    @property
    def _comparator(self):
        modifier = sum(c.modifier for c in self.components if c.modifier)
        return [modifier] + self.components

    def __contains__(self, other):
        if isinstance(other, basestring):
            other = Quality(other)
        if not other or not self:
            return False
        for cat in ('resolution', 'source', 'audio', 'codec'):
            othercat = getattr(other, cat)
            if othercat and othercat != getattr(self, cat):
                return False
        return True

    def __nonzero__(self):
        return any(self._comparator)

    def __eq__(self, other):
        if isinstance(other, basestring):
            other = Quality(other)
            if not other:
                raise TypeError('`%s` does not appear to be a valid quality string.' % other.text)
        if not isinstance(other, Quality):
            if other is None:
                return False
            raise TypeError('Cannot compare %r and %r' % (self, other))
        return self._comparator == other._comparator

    def __ne__(self, other):
        return not self.__eq__(other)

    def __lt__(self, other):
        if isinstance(other, basestring):
            other = Quality(other)
            if not other:
                raise TypeError('`%s` does not appear to be a valid quality string.' % other.text)
        if not isinstance(other, Quality):
            raise TypeError('Cannot compare %r and %r' % (self, other))
        return self._comparator < other._comparator

    def __ge__(self, other):
        return not self.__lt__(other)

    def __le__(self, other):
        return self.__lt__(other) or self.__eq__(other)

    def __gt__(self, other):
        return not self.__le__(other)

    def __repr__(self):
        return '<Quality(resolution=%s,source=%s,codec=%s,audio=%s)>' % (self.resolution, self.source,
                                                                         self.codec, self.audio)

    def __str__(self):
        return self.name

    def __hash__(self):
        # Make these usable as dict keys
        return hash(self.name)


def get(quality_name):
    """Returns a quality object based on canonical quality name."""

    found_components = {}
    for part in quality_name.lower().split():
        component = _registry.get(part)
        if not component:
            raise ValueError('`%s` is not a valid quality string' % part)
        if component.type in found_components:
            raise ValueError('`%s` cannot be defined twice in a quality' % component.type)
        found_components[component.type] = component
    if not found_components:
        raise ValueError('No quality specified')
    result = Quality()
    for type, component in found_components.iteritems():
        setattr(result, type, component)
    return result


class RequirementComponent(object):
    """Represents requirements for a given component type. Can evaluate whether a given QualityComponent
    meets those requirements."""

    def __init__(self, type):
        self.type = type
        self.reset()

    def reset(self):
        self.min = None
        self.max = None
        self.acceptable = []
        self.none_of = []

    def allows(self, comp, loose=False):
        if comp.type != self.type:
            raise TypeError('Cannot compare %r against %s' % (comp, self.type))
        if comp in self.none_of:
            return False
        if loose:
            return True
        if comp in self.acceptable:
            return True
        if self.min or self.max:
            if self.min and comp < self.min:
                return False
            if self.max and comp > self.max:
                return False
            return True
        if not self.acceptable:
            return True
        return False

    def add_requirement(self, text):
        if '-' in text:
            min, max = text.split('-')
            min, max = _registry[min], _registry[max]
            if min.type != max.type != self.type:
                raise ValueError('Component type mismatch: %s' % text)
            self.min, self.max = min, max
        elif '|' in text:
            quals = text.split('|')
            quals = [_registry[qual] for qual in quals]
            if any(qual.type != self.type for qual in quals):
                raise ValueError('Component type mismatch: %s' % text)
            self.acceptable.extend(quals)
        else:
            qual = _registry[text.strip('!<>=+')]
            if qual.type != self.type:
                raise ValueError('Component type mismatch!')
            if text in _registry:
                self.acceptable.append(qual)
            else:
                if text[0] == '<':
                    if text[1] != '=':
                        qual -= 1
                    self.max = qual
                elif text[0] == '>' or text.endswith('+'):
                    if text[1] != '=' and not text.endswith('+'):
                        qual += 1
                    self.min = qual
                elif text[0] == '!':
                    self.none_of.append(qual)


class Requirements(object):
    """Represents requirements for allowable qualities. Can determine whether a given Quality passes requirements."""
    def __init__(self, req=''):
        self.text = ''
        self.resolution = RequirementComponent('resolution')
        self.source = RequirementComponent('source')
        self.codec = RequirementComponent('codec')
        self.audio = RequirementComponent('audio')
        if req:
            self.parse_requirements(req)

    @property
    def components(self):
        return [self.resolution, self.source, self.codec, self.audio]

    def parse_requirements(self, text):
        """
        Parses a requirements string.

        :param text: The string containing quality requirements.
        """
        text = text.lower()
        if self.text:
            self.text += ' '
        self.text += text
        if self.text == 'any':
            for component in self.components:
                component.reset()
                return

        text = text.replace(',', ' ')
        parts = text.split()
        try:
            for part in parts:
                if '-' in part:
                    found = _registry[part.split('-')[0]]
                elif '|' in part:
                    found = _registry[part.split('|')[0]]
                else:
                    found = _registry[part.strip('!<>=+')]
                for component in self.components:
                    if found.type == component.type:
                        component.add_requirement(part)
        except KeyError as e:
            raise ValueError('%s is not a valid quality component.' % e.args[0])

    def allows(self, qual, loose=False):
        """Determine whether this set of requirements allows a given quality.

        :param Quality qual: The quality to evaluate.
        :param bool loose: If True, only ! (not) requirements will be enforced.
        :rtype: bool
        :returns: True if given quality passes all component requirements.
        """
        if isinstance(qual, basestring):
            qual = Quality(qual)
            if not qual:
                raise TypeError('`%s` does not appear to be a valid quality string.' % qual.text)
        for r_component, q_component in zip(self.components, qual.components):
            if not r_component.allows(q_component, loose=loose):
                return False
        return True

    def __str__(self):
        return self.text or 'any'

    def __repr__(self):
        return '<Requirements(%s)>' % self

########NEW FILE########
__FILENAME__ = requests
from __future__ import unicode_literals, division, absolute_import
import urllib2
import time
import logging
from datetime import timedelta, datetime
from urlparse import urlparse
import requests
# Allow some request objects to be imported from here instead of requests
from requests import RequestException, HTTPError
from flexget.utils.tools import parse_timedelta, TimedDict

log = logging.getLogger('requests')

# Don't emit info level urllib3 log messages or below
logging.getLogger('requests.packages.urllib3').setLevel(logging.WARNING)

# Time to wait before trying an unresponsive site again
WAIT_TIME = timedelta(seconds=60)
# Remembers sites that have timed out
unresponsive_hosts = TimedDict(WAIT_TIME)


def is_unresponsive(url):
    """
    Checks if host of given url has timed out within WAIT_TIME

    :param url: The url to check
    :return: True if the host has timed out within WAIT_TIME
    :rtype: bool
    """
    host = urlparse(url).hostname
    return host in unresponsive_hosts


def set_unresponsive(url):
    """
    Marks the host of a given url as unresponsive

    :param url: The url that timed out
    """
    host = urlparse(url).hostname
    if host in unresponsive_hosts:
        # If somehow this is called again before previous timer clears, don't refresh
        return
    unresponsive_hosts[host] = True


def _wrap_urlopen(url, timeout=None):
    """
    Handles alternate schemes using urllib, wraps the response in a requests.Response

    This is not installed as an adapter in requests, since urls without network locations
    (e.g. file:///somewhere) will cause errors

    """
    try:
        raw = urllib2.urlopen(url, timeout=timeout)
    except IOError as e:
        msg = 'Error getting %s: %s' % (url, e)
        log.error(msg)
        raise RequestException(msg)
    resp = requests.Response()
    resp.raw = raw
    # requests passes the `decode_content` kwarg to read
    orig_read = raw.read
    resp.raw.read = lambda size, **kwargs: orig_read(size)
    resp.status_code = raw.code or 200
    resp.headers = requests.structures.CaseInsensitiveDict(raw.headers)
    return resp


class Session(requests.Session):
    """
    Subclass of requests Session class which defines some of our own defaults, records unresponsive sites,
    and raises errors by default.

    """

    def __init__(self, timeout=30, max_retries=1):
        """Set some defaults for our session if not explicitly defined."""
        requests.Session.__init__(self)
        self.timeout = timeout
        self.stream = True
        self.adapters['http://'].max_retries = max_retries
        # Stores min intervals between requests for certain sites
        self.domain_delay = {}

    def add_cookiejar(self, cookiejar):
        """
        Merges cookies from `cookiejar` into cookiejar for this session.

        :param cookiejar: CookieJar instance to add to the session.
        """
        for cookie in cookiejar:
            self.cookies.set_cookie(cookie)

    def set_domain_delay(self, domain, delay):
        """
        Registers a minimum interval between requests to `domain`

        :param domain: The domain to set the interval on
        :param delay: The amount of time between requests, can be a timedelta or string like '3 seconds'
        """
        self.domain_delay[domain] = {'delay': parse_timedelta(delay)}

    def request(self, method, url, *args, **kwargs):
        """
        Does a request, but raises Timeout immediately if site is known to timeout, and records sites that timeout.
        Also raises errors getting the content by default.
        """

        # Raise Timeout right away if site is known to timeout
        if is_unresponsive(url):
            raise requests.Timeout('Requests to this site have timed out recently. Waiting before trying again.')

        # Check if we need to add a delay before request to this site
        for domain, domain_dict in self.domain_delay.iteritems():
            if domain in url:
                next_req = domain_dict.get('next_req')
                if next_req and datetime.now() < next_req:
                    wait_time = next_req - datetime.now()
                    seconds = wait_time.seconds + (wait_time.microseconds / 1000000.0)
                    log.debug('Waiting %.2f seconds until next request to %s' % (seconds, domain))
                    # Sleep until it is time for the next request
                    time.sleep(seconds)
                # Record the next allowable request time for this domain
                domain_dict['next_req'] = datetime.now() + domain_dict['delay']
                break

        kwargs.setdefault('timeout', self.timeout)
        raise_status = kwargs.pop('raise_status', True)

        # If we do not have an adapter for this url, pass it off to urllib
        if not any(url.startswith(adapter) for adapter in self.adapters):
            return _wrap_urlopen(url, timeout=kwargs['timeout'])

        try:
            result = requests.Session.request(self, method, url, *args, **kwargs)
        except (requests.Timeout, requests.ConnectionError):
            # Mark this site in known unresponsive list
            set_unresponsive(url)
            raise

        if raise_status:
            result.raise_for_status()

        return result


# Define some module level functions that use our Session, so this module can be used like main requests module
def request(method, url, **kwargs):
    s = kwargs.pop('session', Session())
    return s.request(method=method, url=url, **kwargs)


def get(url, **kwargs):
    """Sends a GET request. Returns :class:`Response` object.

    :param url: URL for the new :class:`Request` object.
    :param kwargs: Optional arguments that ``request`` takes.
    """
    kwargs.setdefault('allow_redirects', True)
    return request('get', url, **kwargs)


def post(url, data=None, **kwargs):
    """Sends a POST request. Returns :class:`Response` object.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary or bytes to send in the body of the :class:`Request`.
    :param kwargs: Optional arguments that ``request`` takes.
    """
    return request('post', url, data=data, **kwargs)

########NEW FILE########
__FILENAME__ = search
""" Common tools used by plugins implementing search plugin api """
from __future__ import unicode_literals, division, absolute_import
import re
from unicodedata import normalize

from flexget.utils.titles.parser import TitleParser


def clean_symbols(text):
    """Replaces common symbols with spaces. Also normalize unicode strings in decomposed form."""
    result = text
    if isinstance(result, unicode):
        result = normalize('NFKD', result)
    return re.sub('[ \(\)\-_\[\]\.]+', ' ', result).lower()


def clean_title(title):
    """Removes common codec, sound keywords, and special characters info from titles to facilitate
    loose title comparison.
    """
    result = TitleParser.remove_words(title, TitleParser.sounds + TitleParser.codecs)
    result = clean_symbols(result)
    return result


def normalize_unicode(text):
    if isinstance(text, unicode):
        # Convert to combined form for better search results
        return normalize('NFC', text)
    return text


def torrent_availability(seeds, leeches):
    """Returns a rating based on seeds and leeches for a given torrent.

    :param seeds: Number of seeds on the torrent
    :param leeches: Number of leeches on the torrent
    :return: A numeric rating
    """

    return seeds * 2 + leeches

########NEW FILE########
__FILENAME__ = simple_persistence
"""
NOTE:

Avoid using this module on your own or in plugins, this was originally made for 0.9 -> 1.0 transition.

You can safely use task.simple_persistence and manager.persist, if we implement something better we
can replace underlying mechanism in single point (and provide transparent switch).
"""

from __future__ import unicode_literals, division, absolute_import
from collections import MutableMapping
from contextlib import contextmanager
from datetime import datetime
import logging
import pickle

from sqlalchemy import Column, Integer, String, DateTime, PickleType, select, Index

from flexget import db_schema
from flexget.manager import Session
from flexget.utils.database import safe_pickle_synonym
from flexget.utils.sqlalchemy_utils import table_schema, create_index

log = logging.getLogger('util.simple_persistence')
Base = db_schema.versioned_base('simple_persistence', 2)


@db_schema.upgrade('simple_persistence')
def upgrade(ver, session):
    if ver is None:
        # Upgrade to version 0 was a failed attempt at cleaning bad entries from our table, better attempt in ver 1
        ver = 0
    if ver == 0:
        # Remove any values that are not loadable.
        table = table_schema('simple_persistence', session)
        for row in session.execute(select([table.c.id, table.c.plugin, table.c.key, table.c.value])):
            try:
                p = pickle.loads(row['value'])
            except Exception as e:
                log.warning('Couldn\'t load %s:%s removing from db: %s' % (row['plugin'], row['key'], e))
                session.execute(table.delete().where(table.c.id == row['id']))
        ver = 1
    if ver == 1:
        log.info('Creating index on simple_persistence table.')
        create_index('simple_persistence', session, 'feed', 'plugin', 'key')
        ver = 2
    return ver


class SimpleKeyValue(Base):
    """Declarative"""

    __tablename__ = 'simple_persistence'

    id = Column(Integer, primary_key=True)
    task = Column('feed', String)
    plugin = Column(String)
    key = Column(String)
    _value = Column('value', PickleType)
    value = safe_pickle_synonym('_value')
    added = Column(DateTime, default=datetime.now())

    def __init__(self, task, plugin, key, value):
        self.task = task
        self.plugin = plugin
        self.key = key
        self.value = value

    def __repr__(self):
        return "<SimpleKeyValue('%s','%s','%s')>" % (self.task, self.key, self.value)

Index('ix_simple_persistence_feed_plugin_key', SimpleKeyValue.task, SimpleKeyValue.plugin, SimpleKeyValue.key)


class SimplePersistence(MutableMapping):

    def __init__(self, plugin, session=None):
        self.taskname = None
        self.plugin = plugin
        self._session = session

    @contextmanager
    def session_manager(self):
        """Context manager which creates commits and closes a Session if this instance does not have its own."""
        session = self._session or Session()
        try:
            yield session
            if not self._session:
                session.commit()
        except Exception:
            raise
        finally:
            if not self._session:
                session.close()

    def __setitem__(self, key, value):
        with self.session_manager() as session:
            skv = session.query(SimpleKeyValue).filter(SimpleKeyValue.task == self.taskname).\
                filter(SimpleKeyValue.plugin == self.plugin).filter(SimpleKeyValue.key == key).first()
            if skv:
                # update existing
                log.debug('updating key %s value %s' % (key, repr(value)))
                skv.value = value
            else:
                # add new key
                skv = SimpleKeyValue(self.taskname, self.plugin, key, value)
                log.debug('adding key %s value %s' % (key, repr(value)))
                session.add(skv)

    def __getitem__(self, key):
        with self.session_manager() as session:
            skv = session.query(SimpleKeyValue).filter(SimpleKeyValue.task == self.taskname).\
                filter(SimpleKeyValue.plugin == self.plugin).filter(SimpleKeyValue.key == key).first()
            if not skv:
                raise KeyError('%s is not contained in the simple_persistence table.' % key)
            else:
                return skv.value

    def __delitem__(self, key):
        with self.session_manager() as session:
            session.query(SimpleKeyValue).filter(SimpleKeyValue.task == self.taskname).\
                filter(SimpleKeyValue.plugin == self.plugin).filter(SimpleKeyValue.key == key).delete()

    def __iter__(self):
        with self.session_manager() as session:
            query = session.query(SimpleKeyValue.key).filter(SimpleKeyValue.task == self.taskname).\
                filter(SimpleKeyValue.plugin == self.plugin).all()
            if query:
                return [item.key for item in query]
            else:
                return []

    def __len__(self):
        with self.session_manager() as session:
            return session.query(SimpleKeyValue.key).filter(SimpleKeyValue.task == self.taskname).\
                filter(SimpleKeyValue.plugin == self.plugin).count()


class SimpleTaskPersistence(SimplePersistence):

    def __init__(self, task):
        self.task = task

    @property
    def plugin(self):
        return self.task.current_plugin

    @property
    def taskname(self):
        return self.task.name

    @property
    def _session(self):
        return self.task.session

########NEW FILE########
__FILENAME__ = soup
from __future__ import unicode_literals, division, absolute_import
from bs4 import BeautifulSoup

# Hack, hide DataLossWarnings
# Based on html5lib code namespaceHTMLElements=False should do it, but nope ...
# Also it doesn't seem to be available in older version from html5lib, removing it
import warnings
from html5lib.constants import DataLossWarning
warnings.simplefilter('ignore', DataLossWarning)


def get_soup(obj, parser='html5lib'):
    return BeautifulSoup(obj, parser)

########NEW FILE########
__FILENAME__ = sqlalchemy_utils
"""
Miscellaneous SQLAlchemy helpers.
"""
from __future__ import unicode_literals, division, absolute_import
import logging
from sqlalchemy import ColumnDefault, Sequence, Index
from sqlalchemy.types import TypeEngine
from sqlalchemy.schema import Table, MetaData
from sqlalchemy.exc import NoSuchTableError, OperationalError

log = logging.getLogger('sql_utils')


def table_exists(name, session):
    """
    Use SQLAlchemy reflect to check table existences.

    :param string name: Table name to check
    :param Session session: Session to use
    :return: True if table exists, False otherwise
    :rtype: bool
    """
    try:
        table_schema(name, session)
    except NoSuchTableError:
        return False
    return True


def table_schema(name, session):
    """
    :returns: Table schema using SQLAlchemy reflect as it currently exists in the db
    :rtype: Table
    """
    return Table(name, MetaData(bind=session.bind), autoload=True)


def table_columns(table, session):
    """
    :param string table: Name of table or table schema
    :param Session session: SQLAlchemy Session
    :returns: List of column names in the table or empty list
    """

    res = []
    if isinstance(table, basestring):
        table = table_schema(table, session)
    for column in table.columns:
        res.append(column.name)
    return res


def table_add_column(table, name, col_type, session, default=None):
    """Adds a column to a table

    .. warning:: Uses raw statements, probably needs to be changed in
                 order to work on other databases besides SQLite

    :param string table: Table to add column to (can be name or schema)
    :param string name: Name of new column to add
    :param col_type: The sqlalchemy column type to add
    :param Session session: SQLAlchemy Session to do the alteration
    :param default: Default value for the created column (optional)
    """
    if isinstance(table, basestring):
        table = table_schema(table, session)
    if name in table_columns(table, session):
        # If the column already exists, we don't have to do anything.
        return
    # Add the column to the table
    if not isinstance(col_type, TypeEngine):
        # If we got a type class instead of an instance of one, instantiate it
        col_type = col_type()
    type_string = session.bind.engine.dialect.type_compiler.process(col_type)
    statement = 'ALTER TABLE %s ADD %s %s' % (table.name, name, type_string)
    session.execute(statement)
    # Update the table with the default value if given
    if default is not None:
        # Get the new schema with added column
        table = table_schema(table.name, session)
        if not isinstance(default, (ColumnDefault, Sequence)):
            default = ColumnDefault(default)
        default._set_parent(getattr(table.c, name))
        statement = table.update().values({name: default.execute(bind=session.bind)})
        session.execute(statement)


def drop_tables(names, session):
    """Takes a list of table names and drops them from the database if they exist."""
    metadata = MetaData()
    metadata.reflect(bind=session.bind)
    for table in metadata.sorted_tables:
        if table.name in names:
            table.drop()


def get_index_by_name(table, name):
    """
    Find declaratively defined index from table by name

    :param table: Table object
    :param string name: Name of the index to get
    :return: Index object
    """
    for index in table.indexes:
        if index.name == name:
            return index


def create_index(table_name, session, *column_names):
    """
    Creates an index on specified `columns` in `table_name`

    :param table_name: Name of table to create the index on.
    :param session: Session object which should be used
    :param column_names: The names of the columns that should belong to this index.
    """
    index_name = '_'.join(['ix', table_name] + list(column_names))
    table = table_schema(table_name, session)
    columns = [getattr(table.c, column) for column in column_names]
    try:
        Index(index_name, *columns).create(bind=session.bind)
    except OperationalError:
        log.debug('Error creating index.', exc_info=True)

########NEW FILE########
__FILENAME__ = template
from __future__ import unicode_literals, division, absolute_import
import logging
import os
import re
import sys
from copy import copy
from datetime import datetime, date, time
import locale
from email.utils import parsedate
from time import mktime

from jinja2 import (Environment, StrictUndefined, ChoiceLoader,
                    FileSystemLoader, PackageLoader, TemplateNotFound,
                    TemplateSyntaxError, Undefined)

from flexget.event import event
from flexget.utils.pathscrub import pathscrub

log = logging.getLogger('utils.template')

# The environment will be created after the manager has started
environment = None


class RenderError(Exception):
    """Error raised when there is a problem with jinja rendering."""
    pass


def filter_pathbase(val):
    """Base name of a path."""
    return os.path.basename(val or '')


def filter_pathname(val):
    """Base name of a path, without its extension."""
    return os.path.splitext(os.path.basename(val or ''))[0]


def filter_pathext(val):
    """Extension of a path (including the '.')."""
    return os.path.splitext(val or '')[1]


def filter_pathdir(val):
    """Directory containing the given path."""
    return os.path.dirname(val or '')


def filter_pathscrub(val, os_mode=None):
    """Replace problematic characters in a path."""
    return pathscrub(val, os_mode)


def filter_re_replace(val, pattern, repl):
    """Perform a regexp replacement on the given string."""
    return re.sub(pattern, repl, unicode(val))


def filter_re_search(val, pattern):
    """Perform a search for given regexp pattern, return the matching portion of the text."""
    if not isinstance(val, basestring):
        return val
    result = re.search(pattern, val)
    if result:
        return result.group(0)
    return ''


def filter_formatdate(val, format):
    """Returns a string representation of a datetime object according to format string."""
    encoding = locale.getpreferredencoding()
    if not isinstance(val, (datetime, date, time)):
        return val
    return val.strftime(format.encode(encoding)).decode(encoding)


def filter_parsedate(val):
    """Attempts to parse a date according to the rules in RFC 2822"""
    return datetime.fromtimestamp(mktime(parsedate(val)))

def filter_date_suffix(date):
    day = int(date[-2:])
    if 4 <= day <= 20 or 24 <= day <= 30:
        suffix = "th"
    else:
        suffix = ["st", "nd", "rd"][day % 10 - 1]
    return date + suffix

def filter_format_number(val, places=None, grouping=True):
    """Formats a number according to the user's locale."""
    if not isinstance(val, (int, float, long)):
        return val
    if places is not None:
        format = '%.' + str(places) + 'f'
    elif isinstance(val, (int, long)):
        format = '%d'
    else:
        format = '%.02f'

    locale.setlocale(locale.LC_ALL, '')
    return locale.format(format, val, grouping)


def filter_pad(val, width, fillchar='0'):
    """Pads a number or string with fillchar to the specified width."""
    return unicode(val).rjust(width, fillchar)

def filter_to_date(date_time_val):
    if not isinstance(date_time_val, (datetime, date, time)):
        return date_time_val
    return date_time_val.date()

def now():
    return datetime.now()

# Override the built-in Jinja default filter due to Jinja bug
# https://github.com/mitsuhiko/jinja2/pull/138
def filter_default(value, default_value=u'', boolean=False):
    if isinstance(value, Undefined) or (boolean and not value):
        return default_value
    return value


filter_d = filter_default


@event('manager.initialize')
def make_environment(manager):
    """Create our environment and add our custom filters"""
    global environment
    environment = Environment(undefined=StrictUndefined,
        loader=ChoiceLoader([PackageLoader('flexget'),
                             FileSystemLoader(os.path.join(manager.config_base, 'templates'))]),
        extensions=['jinja2.ext.loopcontrols'])
    for name, filt in globals().items():
        if name.startswith('filter_'):
            environment.filters[name.split('_', 1)[1]] = filt
        elif name == 'now':
            environment.globals['now'] = now


# TODO: list_templates function


def get_template(templatename, pluginname=None):
    """Loads a template from disk. Looks in both included plugins and users custom plugin dir."""

    if not templatename.endswith('.template'):
        templatename += '.template'
    locations = []
    if pluginname:
        locations.append(pluginname + '/' + templatename)
    locations.append(templatename)
    for location in locations:
        try:
            return environment.get_template(location)
        except TemplateNotFound:
            pass
    else:
        # TODO: Plugins need to catch and reraise this as PluginError, or perhaps we should have
        # a validator for template files
        raise ValueError('Template not found: %s (%s)' % (templatename, pluginname))


def render(template, context):
    """
    Renders a Template with `context` as its context.

    :param template: Template or template string to render.
    :param context: Context to render the template from.
    :return: The rendered template text.
    """
    if isinstance(template, basestring):
        template = environment.from_string(template)
    try:
        result = template.render(context)
    except Exception as e:
        raise RenderError('(%s) %s' % (type(e).__name__, e))

    return result


def render_from_entry(template_string, entry):
    """Renders a Template or template string with an Entry as its context."""

    # If a plain string was passed, turn it into a Template
    if isinstance(template_string, basestring):
        try:
            template = environment.from_string(template_string)
        except TemplateSyntaxError as e:
            raise RenderError('Error in template syntax: ' + e.message)
    else:
        # We can also support an actual Template being passed in
        template = template_string
    # Make a copy of the Entry so we can add some more fields
    variables = copy(entry)
    variables['now'] = datetime.now()
    # Add task name to variables, usually it's there because metainfo_task plugin, but not always
    if 'task' not in variables and hasattr(entry, 'task'):
        variables['task'] = entry.task.name
    # We use the lower level render function, so that our Entry is not cast into a dict (and lazy loading lost)
    try:
        result = u''.join(template.root_render_func(template.new_context(variables, shared=True)))
    except:
        exc_info = sys.exc_info()
        try:
            return environment.handle_exception(exc_info, True)
        except Exception as e:
            error = RenderError('(%s) %s' % (type(e).__name__, e))
            log.debug('Error during rendering: %s' % error)
            raise error

    # Only try string replacement if jinja didn't do anything
    if result == template_string:
        try:
            result = template_string % entry
        except KeyError as e:
            raise RenderError('Does not contain the field `%s` for string replacement.' % e)
        except ValueError as e:
            raise RenderError('Invalid string replacement template: %s (%s)' % (template_string, e))
        except TypeError as e:
            raise RenderError('Error during string replacement: %s' % e.message)

    return result


def render_from_task(template, task):
    """
    Renders a Template with a task as its context.

    :param template: Template or template string to render.
    :param task: Task to render the template from.
    :return: The rendered template text.
    """
    if isinstance(template, basestring):
        template = environment.from_string(template)
    try:
        result = template.render({'task': task})
    except Exception as e:
        raise RenderError('(%s) %s' % (type(e).__name__, e))

    return result

########NEW FILE########
__FILENAME__ = movie
from __future__ import unicode_literals, division, absolute_import
import logging
import re

from flexget.utils.titles.parser import TitleParser
from flexget.utils import qualities
from flexget.utils.tools import str_to_int

log = logging.getLogger('movieparser')


def diff_pos(string1, string2):
    """Returns first position where string1 and string2 differ."""
    for (count, c) in enumerate(string1):
        if len(string2) <= count:
            return count
        if string2[count] != c:
            return count


class MovieParser(TitleParser):

    def __init__(self):
        self.data = None
        self.reset()
        TitleParser.__init__(self)

    def reset(self):
        # parsing results
        self.name = None
        self.year = None
        self.quality = qualities.Quality()
        self.proper_count = 0

    def __str__(self):
        return "<MovieParser(name=%s,year=%s,quality=%s)>" % (self.name, self.year, self.quality)

    def parse(self, data=None):
        """Parse movie name. Populates name, year, quality and proper_count attributes"""

        # Reset before parsing, so the parser can be reused.
        self.reset()

        if data is None:
            data = self.data

        # Move anything in leading brackets to the end
        data = re.sub(r'^\[(.*?)\](.*)', r'\2 \1', data)

        for char in '[]()_,.':
            data = data.replace(char, ' ')

        # if there are no spaces
        if data.find(' ') == -1:
            data = data.replace('-', ' ')

        # remove unwanted words (imax, ..)
        self.remove_words(data, self.remove)

        data = self.strip_spaces(data)

        # split to parts
        parts = data.split(' ')
        cut_part = 256
        all_caps = True
        for part_pos, part in enumerate(parts):
            cut = False
            # Don't let the first word be cutoff word
            if part_pos < 1:
                continue
            # check for year
            num = str_to_int(part)
            if num is not None:
                if 1930 < num < 2050:
                    self.year = num
                    cut = True
            # Don't consider all caps words cut words if the whole title has been all caps
            if not part.isupper():
                all_caps = False
            # if length > 3 and whole word in uppers, consider as cut word (most likely a group name)
            if len(part) > 3 and part.isupper() and part.isalpha() and not all_caps:
                cut = True
            # check for cutoff words
            if part.lower() in self.cutoffs:
                cut = True
            # check for propers
            if part.lower() in self.propers:
                self.proper_count += 1
                cut = True
            # update cut position
            if cut and parts.index(part) < cut_part:
                cut_part = part_pos

        if cut_part != 256:
            log.debug('parts: %s, cut is: %s', parts, parts[cut_part])

        # calculate cut positon from cut_part
        abs_cut = len(' '.join(parts[:cut_part]))

        log.debug('after parts check, cut data would be: `%s` abs_cut: %i', data[:abs_cut], abs_cut)

        # parse quality
        quality = qualities.Quality(data)
        if quality:
            self.quality = quality
            # remaining string is same as data but quality information removed
            # find out position where there is first difference, this is earliest
            # quality bit, anything after that has no relevance to the movie name
            dp = diff_pos(data, quality.clean_text)
            if dp is not None:
                log.debug('quality start: %s', dp)
                if dp < abs_cut:
                    log.debug('quality cut is even shorter')
                    abs_cut = dp

        # make cut
        data = data[:abs_cut].strip()
        log.debug('data cut to `%s` - this will be the name', data)

        # save results
        self.name = data

########NEW FILE########
__FILENAME__ = parser
from __future__ import unicode_literals, division, absolute_import
import re


class ParseWarning(Warning):

    def __init__(self, value, **kwargs):
        self.value = value
        self.kwargs = kwargs

    def __unicode__(self):
        return self.value

    def __str__(self):
        return self.__unicode__().encode('utf-8')

    def __repr__(self):
        return str('ParseWarning({}, **{})').format(self, repr(self.kwargs))


class TitleParser(object):

    propers = ['proper', 'repack', 'rerip', 'real', 'final']

    specials = ['special', 'bonus', 'extra', 'omake', 'ova']

    editions = ['dc', 'extended', 'uncut', 'remastered', 'unrated', 'theatrical', 'chrono', 'se']

    # TODO: All of the quality related keywords can probably be removed from here, as the quality module handles them
    codecs = ['x264', 'x.264', 'h264', 'h.264', 'XViD']

    # lowercase required
    cutoffs = ['limited', 'xvid', 'h264', 'x264', 'h.264', 'x.264', 'screener', 'unrated', '3d', 'extended',
               'directors', 'director\'s', 'multisubs', 'dubbed', 'subbed', 'multi'] + propers + specials + editions

    remove = ['imax']

    sounds = ['AC3', 'DD5.1', 'DTS']

    @staticmethod
    def re_not_in_word(regexp):
        return r'(?<![^\W_])' + regexp + r'(?![^\W_])'

    @staticmethod
    def strip_spaces(text):
        """Removes all unnecessary duplicate spaces from a text"""
        return ' '.join(text.split())

    @staticmethod
    def remove_words(text, words, not_in_word=False):
        """Clean all given :words: from :text: case insensitively"""
        for word in words:
            text = TitleParser.ireplace(text, word, '', not_in_word=not_in_word)
        # remove duplicate spaces
        text = ' '.join(text.split())
        return text

    @staticmethod
    def ireplace(data, old, new, count=0, not_in_word=False):
        """Case insensitive string replace"""
        old = re.escape(old)
        if not_in_word:
            old = TitleParser.re_not_in_word(old)
        pattern = re.compile(old, re.I)
        return re.sub(pattern, new, data, count)

########NEW FILE########
__FILENAME__ = series
from __future__ import unicode_literals, division, absolute_import
import logging
import re
from datetime import datetime, timedelta

from dateutil.parser import parse as parsedate

from flexget.utils.titles.parser import TitleParser, ParseWarning
from flexget.utils import qualities
from flexget.utils.tools import ReList

log = logging.getLogger('seriesparser')

# Forced to INFO !
# switch to logging.DEBUG if you want to debug this class (produces quite a bit info ..)
log.setLevel(logging.INFO)

ID_TYPES = ['ep', 'date', 'sequence', 'id']


class SeriesParser(TitleParser):

    """
    Parse series.

    :name: series name
    :data: data to parse
    :expect_ep: expect series to be in season, ep format (ep_regexps)
    :expect_id: expect series to be in id format (id_regexps)
    """

    separators = '[/ -]'
    roman_numeral_re = 'X{0,3}(?:IX|XI{0,4}|VI{0,4}|IV|V|I{1,4})'
    english_numbers = ['one', 'two', 'three', 'four', 'five', 'six', 'seven',
                       'eight', 'nine', 'ten']

    # Make sure none of these are found embedded within a word or other numbers
    ep_regexps = ReList([TitleParser.re_not_in_word(regexp) for regexp in [
        '(?:series|season|s)\s?(\d{1,4})(?:\s(?:.*\s)?)?(?:episode|ep|e|part|pt)\s?(\d{1,3}|%s)(?:\s?e?(\d{1,2}))?' %
        roman_numeral_re,
        '(?:series|season)\s?(\d{1,4})\s(\d{1,3})\s?of\s?(?:\d{1,3})',
        '(\d{1,2})\s?x\s?(\d+)(?:\s(\d{1,2}))?',
        '(\d{1,3})\s?of\s?(?:\d{1,3})',
        '(?:episode|ep|part|pt)\s?(\d{1,3}|%s)' % roman_numeral_re,
        'part\s(%s)' % '|'.join(map(str, english_numbers))]])
    unwanted_regexps = ReList([
        '(\d{1,3})\s?x\s?(0+)[^1-9]',  # 5x0
        'S(\d{1,3})D(\d{1,3})',  # S3D1
        '(\d{1,3})\s?x\s?(all)',  # 1xAll
        r'(?:season(?:s)|s|series|\b)\s?\d\s?(?:&\s?\d)?[\s-]*(?:complete|full)',
        'seasons\s(\d\s){2,}',
        'disc\s\d'])
    # Make sure none of these are found embedded within a word or other numbers
    date_regexps = ReList([TitleParser.re_not_in_word(regexp) for regexp in [
        '(\d{2,4})%s(\d{1,2})%s(\d{1,2})' % (separators, separators),
        '(\d{1,2})%s(\d{1,2})%s(\d{2,4})' % (separators, separators),
        '(\d{4})x(\d{1,2})%s(\d{1,2})' % separators]])
    sequence_regexps = ReList([TitleParser.re_not_in_word(regexp) for regexp in [
        '(\d{1,3})(?:v(?P<version>\d))?',
        '(?:pt|part)\s?(\d+|%s)' % roman_numeral_re]])
    unwanted_sequence_regexps = ReList(['seasons?\s?\d{1,2}'])
    id_regexps = ReList([])
    clean_regexps = ReList(['\[.*?\]', '\(.*?\)'])
    # ignore prefix regexps must be passive groups with 0 or 1 occurrences  eg. (?:prefix)?
    ignore_prefixes = [
        '(?:\[[^\[\]]*\])',  # ignores group names before the name, eg [foobar] name
        '(?:HD.720p?:)',
        '(?:HD.1080p?:)']

    def __init__(self, name='', alternate_names=None, identified_by='auto', name_regexps=None, ep_regexps=None,
                 date_regexps=None, sequence_regexps=None, id_regexps=None, strict_name=False, allow_groups=None,
                 allow_seasonless=True, date_dayfirst=None, date_yearfirst=None, special_ids=None,
                 prefer_specials=False, assume_special=False):
        """
        Init SeriesParser.

        :param string name: Name of the series parser is going to try to parse.

        :param list alternate_names: Other names for this series that should be allowed.
        :param string identified_by: What kind of episode numbering scheme is expected,
            valid values are ep, date, sequence, id and auto (default).
        :param list name_regexps: Regexps for name matching or None (default),
            by default regexp is generated from name.
        :param list ep_regexps: Regexps detecting episode,season format.
            Given list is prioritized over built-in regexps.
        :param list date_regexps: Regexps detecting date format.
            Given list is prioritized over built-in regexps.
        :param list sequence_regexps: Regexps detecting sequence format.
            Given list is prioritized over built-in regexps.
        :param list id_regexps: Custom regexps detecting id format.
            Given list is prioritized over built in regexps.
        :param boolean strict_name: If True name must be immediately be followed by episode identifier.
        :param list allow_groups: Optionally specify list of release group names that are allowed.
        :param date_dayfirst: Prefer day first notation of dates when there are multiple possible interpretations.
        :param date_yearfirst: Prefer year first notation of dates when there are multiple possible interpretations.
            This will also populate attribute `group`.
        :param special_ids: Identifiers which will cause entry to be flagged as a special.
        :param boolean prefer_specials: If True, label entry which matches both a series identifier and a special
            identifier as a special.
        """

        self.name = name
        self.alternate_names = alternate_names or []
        self.data = ''
        self.identified_by = identified_by
        # Stores the type of identifier found, 'ep', 'date', 'sequence' or 'special'
        self.id_type = None
        self.name_regexps = ReList(name_regexps or [])
        self.re_from_name = False
        # If custom identifier regexps were provided, prepend them to the appropriate type of built in regexps
        for mode in ID_TYPES:
            listname = mode + '_regexps'
            if locals()[listname]:
                setattr(self, listname, ReList(locals()[listname] + getattr(SeriesParser, listname)))
        self.specials = self.specials + [i.lower() for i in (special_ids or [])]
        self.prefer_specials = prefer_specials
        self.assume_special = assume_special
        self.strict_name = strict_name
        self.allow_groups = allow_groups or []
        self.allow_seasonless = allow_seasonless
        self.date_dayfirst = date_dayfirst
        self.date_yearfirst = date_yearfirst

        self.field = None
        self._reset()

    def _reset(self):
        # parse produces these
        self.season = None
        self.episode = None
        self.episodes = 1
        self.id = None
        self.id_type = None
        self.id_groups = None
        self.quality = None
        self.proper_count = 0
        self.special = False
        # TODO: group is only produced with allow_groups
        self.group = None

        # false if item does not match series
        self.valid = False

    def __setattr__(self, name, value):
        """
        Some conversions when setting attributes.
        `self.name` and `self.data` are converted to unicode.
        """
        if name == 'name' or name == 'data':
            if isinstance(value, str):
                value = unicode(value)
            elif not isinstance(value, unicode):
                raise Exception('%s cannot be %s' % (name, repr(value)))
        object.__setattr__(self, name, value)

    def remove_dirt(self, data):
        """Replaces some characters with spaces"""
        return re.sub(r'[_.,\[\]\(\): ]+', ' ', data).strip().lower()

    def name_to_re(self, name):
        """Convert 'foo bar' to '^[^...]*foo[^...]*bar[^...]+"""
        parenthetical = None
        if name.endswith(')'):
            p_start = name.rfind('(')
            if p_start != -1:
                parenthetical = re.escape(name[p_start + 1:-1])
                name = name[:p_start - 1]
        # Blanks are any non word characters except & and _
        blank = r'(?:[^\w&]|_)'
        ignore = '(?:' + '|'.join(self.ignore_prefixes) + ')?'
        res = re.sub(re.compile(blank + '+', re.UNICODE), ' ', name)
        res = res.strip()
        # accept either '&' or 'and'
        res = re.sub(' (&|and) ', ' (?:and|&) ', res, re.UNICODE)
        res = re.sub(' +', blank + '*', res, re.UNICODE)
        if parenthetical:
            res += '(?:' + blank + '+' + parenthetical + ')?'
            # Turn on exact mode for series ending with a parenthetical,
            # so that 'Show (US)' is not accepted as 'Show (UK)'
            self.strict_name = True
        res = '^' + ignore + blank + '*' + '(' + res + ')(?:\\b|_)' + blank + '*'
        return res

    def parse(self, data=None, field=None, quality=None):
        # Clear the output variables before parsing
        self._reset()
        self.field = field
        if quality:
            self.quality = quality
        if data:
            self.data = data
        if not self.name or not self.data:
            raise Exception('SeriesParser initialization error, name: %s data: %s' %
                            (repr(self.name), repr(self.data)))

        # check if data appears to be unwanted (abort)
        if self.parse_unwanted(self.remove_dirt(self.data)):
            raise ParseWarning('`{data}` appears to be an episode pack'.format(data=self.data))

        name = self.remove_dirt(self.name)

        log.debug('name: %s data: %s', name, self.data)

        # name end position
        name_start = 0
        name_end = 0

        # regexp name matching
        if not self.name_regexps:
            # if we don't have name_regexps, generate one from the name
            self.name_regexps = ReList(self.name_to_re(name) for name in [self.name] + self.alternate_names)
            # With auto regex generation, the first regex group captures the name
            self.re_from_name = True
        # try all specified regexps on this data
        for name_re in self.name_regexps:
            match = re.search(name_re, self.data)
            if match:
                match_start, match_end = match.span(1 if self.re_from_name else 0)
                # Always pick the longest matching regex
                if match_end > name_end:
                    name_start, name_end = match_start, match_end
                log.debug('NAME SUCCESS: %s matched to %s', name_re.pattern, self.data)
        if not name_end:
            # leave this invalid
            log.debug('FAIL: name regexps %s do not match %s',
                      [regexp.pattern for regexp in self.name_regexps], self.data)
            return

        # remove series name from raw data, move any prefix to end of string
        data_stripped = self.data[name_end:] + ' ' + self.data[:name_start]
        data_stripped = data_stripped.lower()
        log.debug('data stripped: %s', data_stripped)

        # allow group(s)
        if self.allow_groups:
            for group in self.allow_groups:
                group = group.lower()
                for fmt in ['[%s]', '-%s']:
                    if fmt % group in data_stripped:
                        log.debug('%s is from group %s', self.data, group)
                        self.group = group
                        data_stripped = data_stripped.replace(fmt % group, '')
                        break
                if self.group:
                    break
            else:
                log.debug('%s is not from groups %s', self.data, self.allow_groups)
                return  # leave invalid

        # Find quality and clean from data
        log.debug('parsing quality ->')
        quality = qualities.Quality(data_stripped)
        if quality:
            # Remove quality string from data
            log.debug('quality detected, using remaining data `%s`', quality.clean_text)
            data_stripped = quality.clean_text
        # Don't override passed in quality
        if not self.quality:
            self.quality = quality

        # Remove unwanted words from data for ep / id parsing
        data_stripped = self.remove_words(data_stripped, self.remove, not_in_word=True)

        data_parts = re.split('[\W_]+', data_stripped)

        for part in data_parts[:]:
            if part in self.propers:
                self.proper_count += 1
                data_parts.remove(part)
            elif part == 'fastsub':
                # Subtract 5 to leave room for fastsub propers before the normal release
                self.proper_count -= 5
                data_parts.remove(part)
            elif part in self.specials:
                self.special = True
                data_parts.remove(part)

        data_stripped = ' '.join(data_parts).strip()

        log.debug("data for date/ep/id parsing '%s'", data_stripped)

        # Try date mode before ep mode
        if self.identified_by in ['date', 'auto']:
            date_match = self.parse_date(data_stripped)
            if date_match:
                if self.strict_name:
                    if date_match['match'].start() > 1:
                        return
                self.id = date_match['date']
                self.id_groups = date_match['match'].groups()
                self.id_type = 'date'
                self.valid = True
                if not (self.special and self.prefer_specials):
                    return
            else:
                log.debug('-> no luck with date_regexps')

        if self.identified_by in ['ep', 'auto'] and not self.valid:
            ep_match = self.parse_episode(data_stripped)
            if ep_match:
                # strict_name
                if self.strict_name:
                    if ep_match['match'].start() > 1:
                        return

                if ep_match['end_episode'] > ep_match['episode'] + 2:
                    # This is a pack of too many episodes, ignore it.
                    log.debug('Series pack contains too many episodes (%d). Rejecting',
                              ep_match['end_episode'] - ep_match['episode'])
                    return

                self.season = ep_match['season']
                self.episode = ep_match['episode']
                if ep_match['end_episode']:
                    self.episodes = (ep_match['end_episode'] - ep_match['episode']) + 1
                else:
                    self.episodes = 1
                self.id_type = 'ep'
                self.valid = True
                if not (self.special and self.prefer_specials):
                    return
            else:
                log.debug('-> no luck with ep_regexps')

            if self.identified_by == 'ep':
                # we should be getting season, ep !
                # try to look up idiotic numbering scheme 101,102,103,201,202
                # ressu: Added matching for 0101, 0102... It will fail on
                #        season 11 though
                log.debug('expect_ep enabled')
                match = re.search(self.re_not_in_word(r'(0?\d)(\d\d)'), data_stripped, re.IGNORECASE | re.UNICODE)
                if match:
                    # strict_name
                    if self.strict_name:
                        if match.start() > 1:
                            return

                    self.season = int(match.group(1))
                    self.episode = int(match.group(2))
                    log.debug(self)
                    self.id_type = 'ep'
                    self.valid = True
                    return
                else:
                    log.debug('-> no luck with the expect_ep')

        # Check id regexps
        if self.identified_by in ['id', 'auto'] and not self.valid:
            for id_re in self.id_regexps:
                match = re.search(id_re, data_stripped)
                if match:
                    # strict_name
                    if self.strict_name:
                        if match.start() > 1:
                            return
                    found_id = '-'.join(g for g in match.groups() if g)
                    if not found_id:
                        # If match groups were all blank, don't accept this match
                        continue
                    self.id = found_id
                    self.id_type = 'id'
                    self.valid = True
                    log.debug('found id \'%s\' with regexp \'%s\'', self.id, id_re.pattern)
                    if not (self.special and self.prefer_specials):
                        return
                    else:
                        break
            else:
                log.debug('-> no luck with id_regexps')

        # Other modes are done, check for unwanted sequence ids
        if self.parse_unwanted_sequence(data_stripped):
            return

        # Check sequences last as they contain the broadest matches
        if self.identified_by in ['sequence', 'auto'] and not self.valid:
            for sequence_re in self.sequence_regexps:
                match = re.search(sequence_re, data_stripped)
                if match:
                    # strict_name
                    if self.strict_name:
                        if match.start() > 1:
                            return
                    # First matching group is the sequence number
                    try:
                        self.id = int(match.group(1))
                    except ValueError:
                        self.id = self.roman_to_int(match.group(1))
                    self.season = 0
                    self.episode = self.id
                    # If anime style version was found, overwrite the proper count with it
                    if 'version' in match.groupdict():
                        if match.group('version'):
                            self.proper_count = int(match.group('version')) - 1
                    self.id_type = 'sequence'
                    self.valid = True
                    log.debug('found id \'%s\' with regexp \'%s\'', self.id, sequence_re.pattern)
                    if not (self.special and self.prefer_specials):
                        return
                    else:
                        break
            else:
                log.debug('-> no luck with sequence_regexps')

        # No id found, check if this is a special
        if self.special or self.assume_special:
            # Attempt to set id as the title of the special
            self.id = data_stripped or 'special'
            self.id_type = 'special'
            self.valid = True
            log.debug('found special, setting id to \'%s\'', self.id)
            return
        if self.valid:
            return

        msg = 'Title `%s` looks like series `%s` but cannot find ' % (self.data, self.name)
        if self.identified_by == 'auto':
            msg += 'any series numbering.'
        else:
            msg += 'a(n) `%s` style identifier.' % self.identified_by
        raise ParseWarning(msg)

    def parse_unwanted(self, data):
        """Parses data for an unwanted hits. Return True if the data contains unwanted hits."""
        for unwanted_re in self.unwanted_regexps:
            match = re.search(unwanted_re, data)
            if match:
                log.debug('unwanted regexp %s matched %s', unwanted_re.pattern, match.groups())
                return True

    def parse_unwanted_sequence(self, data):
        """Parses data for an unwanted id hits. Return True if the data contains unwanted hits."""
        for seq_unwanted_re in self.unwanted_sequence_regexps:
            match = re.search(seq_unwanted_re, data)
            if match:
                log.debug('unwanted id regexp %s matched %s', seq_unwanted_re, match.groups())
                return True

    def parse_date(self, data):
        """
        Parses :data: for a date identifier.
        If found, returns the date and regexp match object
        If no date is found returns False
        """
        for date_re in self.date_regexps:
            match = re.search(date_re, data)
            if match:
                # Check if this is a valid date
                possdates = []

                try:
                    # By default dayfirst and yearfirst will be tried as both True and False
                    # if either have been defined manually, restrict that option
                    dayfirst_opts = [True, False]
                    if self.date_dayfirst is not None:
                        dayfirst_opts = [self.date_dayfirst]
                    yearfirst_opts = [True, False]
                    if self.date_yearfirst is not None:
                        yearfirst_opts = [self.date_yearfirst]
                    kwargs_list = ({'dayfirst': d, 'yearfirst': y} for d in dayfirst_opts for y in yearfirst_opts)
                    for kwargs in kwargs_list:
                        possdate = parsedate(' '.join(match.groups()), **kwargs)
                        # Don't accept dates farther than a day in the future
                        if possdate > datetime.now() + timedelta(days=1):
                            continue
                        # Don't accept dates that are too old
                        if possdate < datetime(1970, 1, 1):
                            continue
                        if possdate not in possdates:
                            possdates.append(possdate)
                except ValueError:
                    log.debug('%s is not a valid date, skipping', match.group(0))
                    continue
                if not possdates:
                    log.debug('All possible dates for %s were in the future', match.group(0))
                    continue
                possdates.sort()
                # Pick the most recent date if there are ambiguities
                bestdate = possdates[-1]
                return {'date': bestdate, 'match': match}

        return False

    def parse_episode(self, data):
        """
        Parses :data: for an episode identifier.
        If found, returns a dict with keys for season, episode, end_episode and the regexp match object
        If no episode id is found returns False
        """

        # search for season and episode number
        for ep_re in self.ep_regexps:
            match = re.search(ep_re, data)

            if match:
                log.debug('found episode number with regexp %s (%s)', ep_re.pattern, match.groups())
                matches = match.groups()
                if len(matches) >= 2:
                    season = matches[0]
                    episode = matches[1]
                elif self.allow_seasonless:
                    # assume season 1 if the season was not specified
                    season = 1
                    episode = matches[0]
                else:
                    # Return False if we are not allowing seasonless matches and one is found
                    return False
                # Convert season and episode to integers
                try:
                    season = int(season)
                    if not episode.isdigit():
                        try:
                            idx = self.english_numbers.index(str(episode).lower())
                            episode = 1 + idx
                        except ValueError:
                            episode = self.roman_to_int(episode)
                    else:
                        episode = int(episode)
                except ValueError:
                    log.critical('Invalid episode number match %s returned with regexp `%s` for %s',
                                 match.groups(), ep_re.pattern, self.data)
                    raise
                end_episode = None
                if len(matches) == 3 and matches[2]:
                    end_episode = int(matches[2])
                    if end_episode <= episode or end_episode > episode + 12:
                        # end episode cannot be before start episode
                        # Assume large ranges are not episode packs, ticket #1271 TODO: is this the best way?
                        end_episode = None
                # Successfully found an identifier, return the results
                return {'season': season,
                        'episode': episode,
                        'end_episode': end_episode,
                        'match': match}
        return False

    def roman_to_int(self, roman):
        """Converts roman numerals up to 39 to integers"""

        roman_map = [('X', 10), ('IX', 9), ('V', 5), ('IV', 4), ('I', 1)]
        roman = roman.upper()

        # Return False if this is not a roman numeral we can translate
        for char in roman:
            if char not in 'XVI':
                raise ValueError('`%s` is not a valid roman numeral' % roman)

        # Add up the parts of the numeral
        i = result = 0
        for numeral, integer in roman_map:
            while roman[i:i + len(numeral)] == numeral:
                result += integer
                i += len(numeral)
        return result

    @property
    def identifiers(self):
        """Return all identifiers this parser represents. (for packs)"""
        # Currently 'ep' is the only id type that supports packs
        if not self.valid:
            raise Exception('Series flagged invalid')
        if self.id_type == 'ep':
            return ['S%02dE%02d' % (self.season, self.episode + x) for x in xrange(self.episodes)]
        elif self.id_type == 'date':
            return [self.id.strftime('%Y-%m-%d')]
        if self.id is None:
            raise Exception('Series is missing identifier')
        else:
            return [self.id]

    @property
    def identifier(self):
        """Return String identifier for parsed episode, eg. S01E02
        (will be the first identifier if this is a pack)
        """
        return self.identifiers[0]

    @property
    def pack_identifier(self):
        """Return a combined identifier for the whole pack if this has more than one episode."""
        # Currently only supports ep mode
        if self.id_type == 'ep' and self.episodes > 1:
            return 'S%02dE%02d-E%02d' % (self.season, self.episode, self.episode + self.episodes - 1)
        else:
            return self.identifier

    @property
    def proper(self):
        return self.proper_count > 0

    def __str__(self):
        # for some fucking reason it's impossible to print self.field here, if someone figures out why please
        # tell me!
        valid = 'INVALID'
        if self.valid:
            valid = 'OK'
        return '<SeriesParser(data=%s,name=%s,id=%s,season=%s,episode=%s,quality=%s,proper=%s,status=%s)>' % \
            (self.data, self.name, str(self.id), self.season, self.episode,
             self.quality, self.proper_count, valid)

    def __cmp__(self, other):
        """Compares quality of parsers, if quality is equal, compares proper_count."""
        return cmp((self.quality, self.episodes, self.proper_count),
                   (other.quality, other.episodes, other.proper_count))

    def __eq__(self, other):
        return self is other

########NEW FILE########
__FILENAME__ = tools
"""Contains miscellaneous helpers"""

from __future__ import unicode_literals, division, absolute_import, print_function
import urllib2
import httplib
import os
import socket
import time
import re
import sys
import locale
from collections import MutableMapping
from urlparse import urlparse
from htmlentitydefs import name2codepoint
from datetime import timedelta, datetime


def str_to_boolean(string):
    if string.lower() in ['true', '1', 't', 'y', 'yes']:
        return True
    else:
        return False


def str_to_int(string):
    try:
        return int(string.replace(',', ''))
    except ValueError:
        return None


def convert_bytes(bytes):
    """Returns given bytes as prettified string."""

    bytes = float(bytes)
    if bytes >= 1099511627776:
        terabytes = bytes / 1099511627776
        size = '%.2fT' % terabytes
    elif bytes >= 1073741824:
        gigabytes = bytes / 1073741824
        size = '%.2fG' % gigabytes
    elif bytes >= 1048576:
        megabytes = bytes / 1048576
        size = '%.2fM' % megabytes
    elif bytes >= 1024:
        kilobytes = bytes / 1024
        size = '%.2fK' % kilobytes
    else:
        size = '%.2fb' % bytes
    return size


class MergeException(Exception):

    def __init__(self, value):
        self.value = value

    def __str__(self):
        return repr(self.value)


def strip_html(text):
    """Tries to strip all HTML tags from *text*. If unsuccessful returns original text."""
    from bs4 import BeautifulSoup
    try:
        text = ' '.join(BeautifulSoup(text).find_all(text=True))
        return ' '.join(text.split())
    except:
        return text


# This pattern matches a character entity reference (a decimal numeric
# references, a hexadecimal numeric reference, or a named reference).
charrefpat = re.compile(r'&(#(\d+|x[\da-fA-F]+)|[\w.:-]+);?')


def _htmldecode(text):
    """Decode HTML entities in the given text."""
    # From screpe.py - licensed under apache 2.0 .. should not be a problem for a MIT afaik
    if type(text) is unicode:
        uchr = unichr
    else:
        uchr = lambda value: value > 127 and unichr(value) or chr(value)

    def entitydecode(match, uchr=uchr):
        entity = match.group(1)
        if entity.startswith('#x'):
            return uchr(int(entity[2:], 16))
        elif entity.startswith('#'):
            return uchr(int(entity[1:]))
        elif entity in name2codepoint:
            return uchr(name2codepoint[entity])
        else:
            return match.group(0)
    return charrefpat.sub(entitydecode, text)


def decode_html(value):
    """
    :param string value: String to be html-decoded
    :returns: Html decoded string
    """
    return _htmldecode(value)


def encode_html(unicode_data, encoding='ascii'):
    """
    Encode unicode_data for use as XML or HTML, with characters outside
    of the encoding converted to XML numeric character references.
    """
    try:
        return unicode_data.encode(encoding, 'xmlcharrefreplace')
    except ValueError:
        # ValueError is raised if there are unencodable chars in the
        # data and the 'xmlcharrefreplace' error handler is not found.
        # Pre-2.3 Python doesn't support the 'xmlcharrefreplace' error
        # handler, so we'll emulate it.
        return _xmlcharref_encode(unicode_data, encoding)


def _xmlcharref_encode(unicode_data, encoding):
    """Emulate Python 2.3's 'xmlcharrefreplace' encoding error handler."""
    chars = []
    # Phase through the unicode_data string one character at a time in
    # order to catch unencodable characters:
    for char in unicode_data:
        try:
            chars.append(char.encode(encoding, 'strict'))
        except UnicodeError:
            chars.append('&#%i;' % ord(char))
    return ''.join(chars)


def merge_dict_from_to(d1, d2):
    """Merges dictionary d1 into dictionary d2. d1 will remain in original form."""
    import copy
    for k, v in d1.items():
        if k in d2:
            if type(v) == type(d2[k]):
                if isinstance(v, dict):
                    merge_dict_from_to(d1[k], d2[k])
                elif isinstance(v, list):
                    d2[k].extend(copy.deepcopy(v))
                elif isinstance(v, (basestring, bool, int, float, type(None))):
                    pass
                else:
                    raise Exception('Unknown type: %s value: %s in dictionary' % (type(v), repr(v)))
            elif isinstance(v, basestring) and isinstance(d2[k], basestring):
                # Strings are compatible by definition
                # (though we could get a decode error later, this is higly unlikely for config values)
                pass
            else:
                raise MergeException('Merging key %s failed, conflicting datatypes %r vs. %r.' % (
                    k, type(v).__name__, type(d2[k]).__name__))
        else:
            d2[k] = copy.deepcopy(v)


class SmartRedirectHandler(urllib2.HTTPRedirectHandler):

    def http_error_301(self, req, fp, code, msg, headers):
        result = urllib2.HTTPRedirectHandler.http_error_301(self, req, fp, code, msg, headers)
        result.status = code
        return result

    def http_error_302(self, req, fp, code, msg, headers):
        result = urllib2.HTTPRedirectHandler.http_error_302(self, req, fp, code, msg, headers)
        result.status = code
        return result


def urlopener(url_or_request, log, **kwargs):
    """
    Utility function for pulling back a url, with a retry of 3 times, increasing the timeout, etc.
    Re-raises any errors as URLError.

    .. warning:: This is being replaced by requests library.
                 flexget.utils.requests should be used going forward.

    :param str url_or_request: URL or Request object to get.
    :param log: Logger to log debug info and errors to
    :param kwargs: Keyword arguments to be passed to urlopen
    :return: The file-like object returned by urlopen
    """
    from flexget.utils.requests import is_unresponsive, set_unresponsive

    if isinstance(url_or_request, urllib2.Request):
        url = url_or_request.get_host()
    else:
        url = url_or_request
    if is_unresponsive(url):
        msg = '%s is known to be unresponsive, not trying again.' % urlparse(url).hostname
        log.warning(msg)
        raise urllib2.URLError(msg)

    retries = kwargs.get('retries', 3)
    timeout = kwargs.get('timeout', 15.0)

    # get the old timeout for sockets, so we can set it back to that when done. This is NOT threadsafe by the way.
    # In order to avoid requiring python 2.6, we're not using the urlopen timeout parameter. That really should be used
    # after checking for python 2.6.
    oldtimeout = socket.getdefaulttimeout()
    try:
        socket.setdefaulttimeout(timeout)

        handlers = [SmartRedirectHandler()]
        if urllib2._opener:
            handlers.extend(urllib2._opener.handlers)
        if kwargs.get('handlers'):
            handlers.extend(kwargs['handlers'])
        if len(handlers) > 1:
            handler_names = [h.__class__.__name__ for h in handlers]
            log.debug('Additional handlers have been specified for this urlopen: %s' % ', '.join(handler_names))
        opener = urllib2.build_opener(*handlers).open
        for i in range(retries):  # retry getting the url up to 3 times.
            if i > 0:
                time.sleep(3)
            try:
                retrieved = opener(url_or_request, kwargs.get('data'))
            except urllib2.HTTPError as e:
                if e.code < 500:
                    # If it was not a server error, don't keep retrying.
                    log.warning('Could not retrieve url (HTTP %s error): %s' % (e.code, e.url))
                    raise
                log.debug('HTTP error (try %i/%i): %s' % (i + 1, retries, e.code))
            except (urllib2.URLError, socket.timeout) as e:
                if hasattr(e, 'reason'):
                    reason = str(e.reason)
                else:
                    reason = 'N/A'
                if reason == 'timed out':
                    set_unresponsive(url)
                log.debug('Failed to retrieve url (try %i/%i): %s' % (i + 1, retries, reason))
            except httplib.IncompleteRead as e:
                log.critical('Incomplete read - see python bug 6312')
                break
            else:
                # make the returned instance usable in a with statement by adding __enter__ and __exit__ methods

                def enter(self):
                    return self

                def exit(self, exc_type, exc_val, exc_tb):
                    self.close()

                retrieved.__class__.__enter__ = enter
                retrieved.__class__.__exit__ = exit
                return retrieved

        log.warning('Could not retrieve url: %s' % url_or_request)
        raise urllib2.URLError('Could not retrieve url after %s tries.' % retries)
    finally:
        socket.setdefaulttimeout(oldtimeout)


class ReList(list):
    """
    A list that stores regexps.

    You can add compiled or uncompiled regexps to the list.
    It will always return the compiled version.
    It will compile the text regexps on demand when first accessed.
    """

    # Set the default flags
    flags = re.IGNORECASE | re.UNICODE

    def __init__(self, *args, **kwargs):
        """Optional :flags: keyword argument with regexp flags to compile with"""
        if 'flags' in kwargs:
            self.flags = kwargs['flags']
            del kwargs['flags']
        list.__init__(self, *args, **kwargs)

    def __getitem__(self, k):
        item = list.__getitem__(self, k)
        if isinstance(item, basestring):
            item = re.compile(item, re.IGNORECASE | re.UNICODE)
            self[k] = item
        return item

    def __iter__(self):
        for i in range(len(self)):
            yield self[i]


# Determine the encoding for io
io_encoding = None
if hasattr(sys.stdout, 'encoding'):
    io_encoding = sys.stdout.encoding
if not io_encoding:
    try:
        io_encoding = locale.getpreferredencoding()
    except Exception:
        pass
if not io_encoding:
    # Default to utf8 if nothing can be determined
    io_encoding = 'utf8'
else:
    # Normalize the encoding
    io_encoding = io_encoding.lower()
    if io_encoding == 'cp65001':
        io_encoding = 'utf8'
    elif io_encoding in ['us-ascii', '646', 'ansi_x3.4-1968']:
        io_encoding = 'ascii'


def console(text):
    """Print to console safely."""
    if isinstance(text, str):
        print(text)
        return
    print(unicode(text).encode(io_encoding, 'replace'))


def parse_timedelta(value):
    """Parse a string like '5 days' into a timedelta object. Also allows timedeltas to pass through."""
    if isinstance(value, timedelta):
        # Allow timedelta objects to pass through
        return value
    if not value:
        # If no time is given, default to 0
        return timedelta()
    amount, unit = value.lower().split(' ')
    # Make sure unit name is plural.
    if not unit.endswith('s'):
        unit += 's'
    params = {unit: float(amount)}
    try:
        return timedelta(**params)
    except TypeError:
        raise ValueError('Invalid time format \'%s\'' % value)

def multiply_timedelta(interval, number):
    """timedeltas can not normally be multiplied by floating points. This does that."""
    # Python 2.6 doesn't have total seconds
    total_seconds = interval.seconds + interval.days * 24 * 3600
    return timedelta(seconds=total_seconds*number)

if os.name == 'posix':
    def pid_exists(pid):
        """Check whether pid exists in the current process table."""
        import errno
        if pid < 0:
            return False
        try:
            os.kill(pid, 0)
        except OSError as e:
            return e.errno == errno.EPERM
        else:
            return True
else:
    def pid_exists(pid):
        import ctypes
        import ctypes.wintypes
        kernel32 = ctypes.windll.kernel32
        PROCESS_QUERY_INFORMATION = 0x0400
        STILL_ACTIVE = 259

        handle = kernel32.OpenProcess(PROCESS_QUERY_INFORMATION, 0, pid)
        if handle == 0:
            return False

        # If the process exited recently, a pid may still exist for the handle.
        # So, check if we can get the exit code.
        exit_code = ctypes.wintypes.DWORD()
        is_running = kernel32.GetExitCodeProcess(handle, ctypes.byref(exit_code)) == 0
        kernel32.CloseHandle(handle)

        # See if we couldn't get the exit code or the exit code indicates that the
        # process is still running.
        return is_running or exit_code.value == STILL_ACTIVE


class TimedDict(MutableMapping):
    """Acts like a normal dict, but keys will only remain in the dictionary for a specified time span."""
    def __init__(self, cache_time='5 minutes'):
        self.cache_time = parse_timedelta(cache_time)
        self._store = dict()

    def __getitem__(self, key):
        add_time, value = self._store[key]
        # Prune data and raise KeyError when expired
        if add_time < datetime.now() - self.cache_time:
            del self._store[key]
            raise KeyError(key, 'cache time expired')
        return value

    def __setitem__(self, key, value):
        self._store[key] = (datetime.now(), value)

    def __delitem__(self, key):
        del self._store[key]

    def __iter__(self):
        # Uses our getitem to skip expired items
        return (key for key in self._store.keys() if key in self)

    def __len__(self):
        return len(list(self.__iter__()))

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, dict(zip(self._store, (v[1] for v in self._store.values()))))

########NEW FILE########
__FILENAME__ = validator
from __future__ import unicode_literals, division, absolute_import, print_function
import re

from flexget.config_schema import process_config

# TODO: rename all validator.valid -> validator.accepts / accepted / accept ?


class Errors(object):
    """Create and hold validator error messages."""

    def __init__(self):
        self.messages = []
        self.path = []
        self.path_level = None

    def count(self):
        """Return number of errors."""
        return len(self.messages)

    def add(self, msg):
        """Add new error message to current path."""
        path = [unicode(p) for p in self.path]
        msg = '[/%s] %s' % ('/'.join(path), msg)
        self.messages.append(msg)

    def back_out_errors(self, num=1):
        """Remove last num errors from list"""
        if num > 0:
            del self.messages[0 - num:]

    def path_add_level(self, value='?'):
        """Adds level into error message path"""
        self.path_level = len(self.path)
        self.path.append(value)

    def path_remove_level(self):
        """Removes level from path by depth number"""
        if self.path_level is None:
            raise Exception('no path level')
        del(self.path[self.path_level])
        self.path_level -= 1

    def path_update_value(self, value):
        """Updates path level value"""
        if self.path_level is None:
            raise Exception('no path level')
        self.path[self.path_level] = value

# A registry mapping validator names to their class
registry = {}


def factory(name='root', **kwargs):
    """Factory method, returns validator instance."""
    if name not in registry:
        raise Exception('Asked unknown validator \'%s\'' % name)
    return registry[name](**kwargs)


def any_schema(schemas):
    """
    Creates a schema that will match any of the given schemas.
    Will not use anyOf if there is just one validator in the list, for simpler error messages.

    """
    schemas = list(schemas)
    if len(schemas) == 1:
        return schemas[0]
    else:
        return {'anyOf': schemas}


class Validator(object):
    name = 'validator'

    class __metaclass__(type):
        """Automatically adds subclasses to the registry."""

        def __init__(cls, name, bases, dict):
            type.__init__(cls, name, bases, dict)
            if not 'name' in dict:
                raise Exception('Validator %s is missing class-attribute name' % name)
            registry[dict['name']] = cls

    def __init__(self, parent=None, message=None, **kwargs):
        self.valid = []
        self.message = message
        self.parent = parent
        self._errors = None

    @property
    def errors(self):
        """Recursively return the Errors class from the root of the validator tree."""
        if self.parent:
            return self.parent.errors
        else:
            if not self._errors:
                self._errors = Errors()
            return self._errors

    def add_root_parent(self):
        if self.name == 'root':
            return self
        root = factory('root')
        root.accept(self)
        return root

    def add_parent(self, parent):
        self.parent = parent
        return parent

    def get_validator(self, value, **kwargs):
        """Returns a child validator of this one.

        :param value:
          Can be a validator type string, an already created Validator instance,
          or a function that returns a validator instance.
        :param kwargs:
          Keyword arguments are passed on to validator init if a new validator is created.
        """
        if isinstance(value, Validator):
            # If we are passed a Validator instance, make it a child of this validator and return it.
            value.add_parent(self)
            return value
        elif callable(value):
            raise ValueError('lazy validators are no longer supported. Upgrade plugin to use new schema validation.')
        # Otherwise create a new child Validator
        kwargs['parent'] = self
        return factory(value, **kwargs)

    def accept(self, value, **kwargs):
        raise NotImplementedError('Validator %s should override accept method' % self.__class__.__name__)

    def schema(self):
        schema = self._schema()
        if self.message:
            schema['error'] = self.message
        return schema

    def _schema(self):
        """Return schema for validator"""
        raise NotImplementedError(self.__name__)

    def validate(self, value):
        """This is just to unit test backwards compatibility of json schema with old validators"""
        errors = list(e.message for e in process_config(value, self.schema()))
        self.errors.messages = errors
        return not errors

    def __str__(self):
        return '<validator:name=%s>' % self.name

    __repr__ = __str__


class RootValidator(Validator):
    name = 'root'

    def accept(self, value, **kwargs):
        v = self.get_validator(value, **kwargs)
        self.valid.append(v)
        return v

    def _schema(self):
        return any_schema([v.schema() for v in self.valid])


class ChoiceValidator(Validator):
    name = 'choice'

    def __init__(self, parent=None, **kwargs):
        self.valid_ic = []
        Validator.__init__(self, parent, **kwargs)

    def accept(self, value, ignore_case=False):
        """
        :param value: accepted text, int or boolean
        :param bool ignore_case: Whether case matters for text values
        """
        if not isinstance(value, (basestring, int, float)):
            raise Exception('Choice validator only accepts strings and numbers')
        if isinstance(value, basestring) and ignore_case:
            self.valid_ic.append(value.lower())
        else:
            self.valid.append(value)

    def accept_choices(self, values, **kwargs):
        """Same as accept but with multiple values (list)"""
        for value in values:
            self.accept(value, **kwargs)

    def _schema(self):
        schemas = []
        if self.valid:
            schemas.append({'enum': self.valid + self.valid_ic})
        if self.valid_ic:
            schemas.append(any_schema({"type": "string", "pattern": "(?i)^%s$" % p} for p in self.valid_ic))
        s = any_schema(schemas)
        s['error'] = 'Must be one of the following: %s' % ', '.join(map(unicode, self.valid + self.valid_ic))
        return s


class AnyValidator(Validator):
    name = 'any'

    def accept(self, value, **kwargs):
        self.valid = value

    def _schema(self):
        return {}


class EqualsValidator(Validator):
    name = 'equals'

    def accept(self, value, **kwargs):
        self.valid = value

    def _schema(self):
        return {'enum': [self.valid]}


class NumberValidator(Validator):
    name = 'number'

    def accept(self, name, **kwargs):
        pass

    def _schema(self):
        return {'type': 'number'}


class IntegerValidator(Validator):
    name = 'integer'

    def accept(self, name, **kwargs):
        pass

    def _schema(self):
        return {'type': 'integer'}


# TODO: Why would we need this instead of NumberValidator?
class DecimalValidator(Validator):
    name = 'decimal'

    def accept(self, name, **kwargs):
        pass

    def _schema(self):
        return {'type': 'number'}


class BooleanValidator(Validator):
    name = 'boolean'

    def accept(self, name, **kwargs):
        pass

    def _schema(self):
        return {'type': 'boolean'}


class TextValidator(Validator):
    name = 'text'

    def accept(self, name, **kwargs):
        pass

    def _schema(self):
        return {'type': 'string'}


class RegexpValidator(Validator):
    name = 'regexp'

    def accept(self, name, **kwargs):
        pass

    def _schema(self):
        return {'type': 'string', 'format': 'regex'}


class RegexpMatchValidator(Validator):
    name = 'regexp_match'

    def __init__(self, parent=None, **kwargs):
        Validator.__init__(self, parent, **kwargs)
        self.regexps = []
        self.reject_regexps = []

    def add_regexp(self, regexp_list, regexp):
        try:
            regexp_list.append(re.compile(regexp))
        except:
            raise ValueError('Invalid regexp given to match_regexp')

    def accept(self, regexp, **kwargs):
        self.add_regexp(self.regexps, regexp)
        if kwargs.get('message'):
            self.message = kwargs['message']

    def reject(self, regexp):
        self.add_regexp(self.reject_regexps, regexp)

    def _schema(self):
        schema = any_schema([{'type': 'string', 'pattern': regexp.pattern} for regexp in self.regexps])
        if self.reject_regexps:
            schema['not'] = any_schema([{'pattern': rej_regexp.pattern} for rej_regexp in self.reject_regexps])
        return schema


class IntervalValidator(RegexpMatchValidator):
    name = 'interval'

    def __init__(self, parent=None, **kwargs):
        RegexpMatchValidator.__init__(self, parent, **kwargs)
        self.accept(r'^\d+ (second|minute|hour|day|week)s?$')
        self.message = "should be in format 'x (seconds|minutes|hours|days|weeks)'"


class FileValidator(TextValidator):
    name = 'file'

    def validate(self, data):
        import os

        if not os.path.isfile(os.path.expanduser(data)):
            self.errors.add('File %s does not exist' % data)
            return False
        return True

    def _schema(self):
        return {'type': 'string', 'format': 'file'}


class PathValidator(TextValidator):
    name = 'path'

    def __init__(self, parent=None, allow_replacement=False, allow_missing=False, **kwargs):
        self.allow_replacement = allow_replacement
        self.allow_missing = allow_missing
        Validator.__init__(self, parent, **kwargs)

    def _schema(self):
        if self.allow_missing:
            return {'type': 'string'}
        return {'type': 'string', 'format': 'path'}


class UrlValidator(TextValidator):
    name = 'url'

    def __init__(self, parent=None, protocols=None, **kwargs):
        if protocols:
            self.protocols = protocols
        else:
            self.protocols = ['ftp', 'http', 'https', 'file']
        Validator.__init__(self, parent, **kwargs)

    def _schema(self):
        return {'type': 'string', 'format': 'url'}


class ListValidator(Validator):
    name = 'list'

    def accept(self, value, **kwargs):
        v = self.get_validator(value, **kwargs)
        self.valid.append(v)
        return v

    def _schema(self):
        return {'type': 'array', 'items': any_schema([v.schema() for v in self.valid])}


class DictValidator(Validator):
    name = 'dict'

    def __init__(self, parent=None, **kwargs):
        self.reject = {}
        self.any_key = []
        self.required_keys = []
        self.key_validators = []
        Validator.__init__(self, parent, **kwargs)
        # TODO: not dictionary?
        self.valid = {}

    def accept(self, value, key=None, required=False, **kwargs):
        """
        :param value: validator name, instance or function that returns an instance, which validates the given `key`
        :param string key: The dictionary key to accept
        :param bool required: = Mark this `key` as required
        :raises ValueError: `key` was not specified
        """
        if not key:
            raise ValueError('%s.accept() must specify key' % self.name)

        if required:
            self.require_key(key)

        v = self.get_validator(value, **kwargs)
        self.valid.setdefault(key, []).append(v)
        return v

    def reject_key(self, key, message=None):
        """Rejects a key"""
        self.reject[key] = message

    def reject_keys(self, keys, message=None):
        """Reject list of keys"""
        for key in keys:
            self.reject[key] = message

    def require_key(self, key):
        """Flag key as mandatory"""
        if not key in self.required_keys:
            self.required_keys.append(key)

    def accept_any_key(self, value, **kwargs):
        """Accepts any leftover keys in dictionary, which will be validated with `value`"""
        v = self.get_validator(value, **kwargs)
        self.any_key.append(v)
        return v

    def accept_valid_keys(self, value, key_type=None, key_validator=None, **kwargs):
        """
        Accepts keys that pass a given validator, and validates them using validator specified in `value`

        :param value: Validator name, instance or function returning an instance
            that will be used to validate dict values.
        :param key_type: Name of validator or list of names that determine which keys in this dict `value` will govern
        :param Validator key_validator: A validator instance that will be used to determine which keys in the dict
            `value` will govern
        :raises ValueError: If both `key_type` and `key_validator` are specified.
        """
        if key_type and key_validator:
            raise ValueError('key_type and key_validator are mutually exclusive')
        if key_validator:
            # Make sure errors show up in our list
            key_validator.add_parent(self)
        elif key_type:
            if isinstance(key_type, basestring):
                key_type = [key_type]
            key_validator = self.get_validator('root')
            for key_type in key_type:
                key_validator.accept(key_type)
        else:
            raise ValueError('%s.accept_valid_keys() must specify key_type or key_validator' % self.name)
        v = self.get_validator(value, **kwargs)
        self.key_validators.append((key_validator, v))
        return v

    def _schema(self):
        schema = {'type': 'object'}
        properties = schema['properties'] = {}
        for key, validators in self.valid.iteritems():
            if not validators:
                continue
            properties[key] = any_schema(v.schema() for v in validators)
        if self.required_keys:
            schema['required'] = self.required_keys
        if self.any_key:
            schema['additionalProperties'] = any_schema([v.schema() for v in self.any_key])
        elif self.key_validators:
            # TODO: this doesn't actually validate keys
            schema['additionalProperties'] = any_schema(kv[1].schema() for kv in self.key_validators)
        else:
            schema['additionalProperties'] = False
        # TODO: implement this
        #if self.reject_keys:
        #    schema['reject_keys'] = self.reject

        return schema


class QualityValidator(TextValidator):
    name = 'quality'

    def _schema(self):
        return {'type': 'string', 'format': 'quality'}


class QualityRequirementsValidator(TextValidator):
    name = 'quality_requirements'

    def _schema(self):
        return {'type': 'string', 'format': 'qualityRequirements'}

# ---- TESTING ----


def build_options_validator(options):
    quals = ['720p', '1080p', '720p bluray', 'hdtv']
    options.accept('text', key='path')
    # set
    options.accept('dict', key='set').accept_any_key('any')
    # regexes can be given in as a single string ..
    options.accept('regexp', key='name_regexp')
    options.accept('regexp', key='ep_regexp')
    options.accept('regexp', key='id_regexp')
    # .. or as list containing strings
    options.accept('list', key='name_regexp').accept('regexp')
    options.accept('list', key='ep_regexp').accept('regexp')
    options.accept('list', key='id_regexp').accept('regexp')
    # quality
    options.accept('choice', key='quality').accept_choices(quals, ignore_case=True)
    options.accept('list', key='qualities').accept('choice').accept_choices(quals, ignore_case=True)
    options.accept('boolean', key='upgrade')
    options.accept('choice', key='min_quality').accept_choices(quals, ignore_case=True)
    options.accept('choice', key='max_quality').accept_choices(quals, ignore_case=True)
    # propers
    options.accept('boolean', key='propers')
    message = "should be in format 'x (minutes|hours|days|weeks)' e.g. '5 days'"
    time_regexp = r'\d+ (minutes|hours|days|weeks)'
    options.accept('regexp_match', key='propers', message=message + ' or yes/no').accept(time_regexp)
    # expect flags
    options.accept('choice', key='identified_by').accept_choices(['ep', 'id', 'auto'])
    # timeframe
    options.accept('regexp_match', key='timeframe', message=message).accept(time_regexp)
    # strict naming
    options.accept('boolean', key='exact')
    # watched in SXXEXX form
    watched = options.accept('regexp_match', key='watched')
    watched.accept('(?i)s\d\de\d\d$', message='Must be in SXXEXX format')
    # watched in dict form
    watched = options.accept('dict', key='watched')
    watched.accept('integer', key='season')
    watched.accept('integer', key='episode')
    # from group
    options.accept('text', key='from_group')
    options.accept('list', key='from_group').accept('text')
    # parse only
    options.accept('boolean', key='parse_only')


def complex_test():

    def build_list(series):
        """Build series list to series."""
        series.accept('text')
        series.accept('number')
        bundle = series.accept('dict')
        # prevent invalid indentation level
        """
        bundle.reject_keys(['set', 'path', 'timeframe', 'name_regexp',
            'ep_regexp', 'id_regexp', 'watched', 'quality', 'min_quality',
            'max_quality', 'qualities', 'exact', 'from_group'],
            'Option \'$key\' has invalid indentation level. It needs 2 more spaces.')
        """
        bundle.accept_any_key('path')
        options = bundle.accept_any_key('dict')
        build_options_validator(options)

    root = factory()

    # simple format:
    #   - series
    #   - another series

    simple = root.accept('list')
    build_list(simple)

    # advanced format:
    #   settings:
    #     group: {...}
    #   group:
    #     {...}

    """
    advanced = root.accept('dict')
    settings = advanced.accept('dict', key='settings')
    settings_group = settings.accept_any_key('dict')
    build_options_validator(settings_group)

    group = advanced.accept_any_key('list')
    build_list(group)
    """

    return root


if __name__ == '__main__':
    from flexget.plugins.input.rss import InputRSS
    #v = complex_test()
    v = InputRSS().validator()
    schema = v.schema()

    import json

    print(json.dumps(schema, sort_keys=True, indent=4))

    """
    root = factory()
    list = root.accept('list')
    list.accept('text')
    list.accept('regexp')
    list.accept('choice').accept_choices(['foo', 'bar'])

    print root.schema()
    """

########NEW FILE########
__FILENAME__ = flexget_vanilla
#!/usr/bin/env python 
"""
    Run FlexGet without bootstrap and virtualenv.
    
    You need to have all dependencies installed in site-packages when using this.
"""

import flexget

if __name__ == '__main__':
    flexget.main()

########NEW FILE########
__FILENAME__ = gen-changelog
# Writes a changelog in trac WikiFormatting based on a git log
from __future__ import unicode_literals, division, absolute_import

import codecs
from itertools import ifilter
import os
import re
import subprocess
import sys

from bs4 import BeautifulSoup
import dateutil.parser
import requests

out_path = 'ChangeLog'
if len(sys.argv) > 1:
    dir_name = os.path.dirname(sys.argv[1])
    if dir_name and not os.path.isdir(dir_name):
        print 'Output dir doesn\'t exist: %s' % sys.argv[1]
        sys.exit(1)
    out_path = sys.argv[1]

ua_response = requests.get('http://flexget.com/wiki/UpgradeActions')
ua_soup = BeautifulSoup(ua_response.text, 'html5lib')

# 1.0.3280 was last revision on svn
git_log_output = subprocess.check_output(['git', 'log', '--pretty=%n---%n.%d%n%ci%n%h%n%s%n%-b%n---%n',
                                          '--topo-order', '--decorate=full','refs/tags/1.0.3280..HEAD'])
git_log_iter = ifilter(None, git_log_output.decode('utf-8').splitlines())

with codecs.open(out_path, 'w', encoding='utf-8') as out_file:
    for line in git_log_iter:
        assert line == '---'
        tag = re.search('refs/tags/([\d.]+)', next(git_log_iter))
        date = dateutil.parser.parse(next(git_log_iter))
        commit_hash = next(git_log_iter)
        body = list(iter(git_log_iter.next, '---'))
        if tag:
            ver = tag.group(1)
            ua_link = ''
            result = ua_soup.find('h3', text=re.compile(' %s$' % re.escape(ver)))
            if result:
                ua_link = '^[wiki:UpgradeActions#%s upgrade actions]^ ' % result['id']
            out_file.write('\n=== %s (%s) %s===\n\n' % (ver, date.strftime('%Y.%m.%d'), ua_link))
        out_file.write(' * (%s) %s\n' % (commit_hash, '[[BR]]\n   '.join(body)))




########NEW FILE########
__FILENAME__ = pavement
"""
FlexGet build and development utilities - unfortunately this file is somewhat messy
"""

import os
import sys
from paver.easy import *
import paver.virtual
import paver.setuputils
from paver.setuputils import setup, find_package_data, find_packages

sphinxcontrib = False
try:
    from sphinxcontrib import paverutils
    sphinxcontrib = True
except ImportError:
    pass

sys.path.insert(0, '')

options = environment.options
# There is a bug in sqlalchemy 0.9.0, see gh#127
install_requires = ['FeedParser>=5.1.3', 'SQLAlchemy >=0.7.5, !=0.9.0, <0.9.99', 'PyYAML',
                    # There is a bug in beautifulsoup 4.2.0 that breaks imdb parsing, see http://flexget.com/ticket/2091
                    'beautifulsoup4>=4.1, !=4.2.0, <4.4', 'html5lib>=0.11', 'PyRSS2Gen', 'pynzb', 'progressbar', 'rpyc',
                    'jinja2', 'requests>=1.0, <2.99', 'python-dateutil!=2.0, !=2.2', 'jsonschema>=2.0', 'python-tvrage',
                    'tmdb3']
if sys.version_info < (2, 7):
    # argparse is part of the standard library in python 2.7+
    install_requires.append('argparse')

entry_points = {'console_scripts': ['flexget = flexget:main']}

# Provide an alternate exe on windows which does not cause a pop-up when scheduled
if sys.platform.startswith('win'):
    entry_points.setdefault('gui_scripts', []).append('flexget-headless = flexget:main')

with open("README.rst") as readme:
    long_description = readme.read()

setup(
    name='FlexGet',
    version='1.2',  # our tasks append the .1234 (current build number) to the version number
    description='FlexGet is a program aimed to automate downloading or processing content (torrents, podcasts, etc.) '
                'from different sources like RSS-feeds, html-pages, various sites and more.',
    long_description=long_description,
    author='Marko Koivusalo',
    author_email='marko.koivusalo@gmail.com',
    license='MIT',
    url='http://flexget.com',
    download_url='http://download.flexget.com',
    install_requires=install_requires,
    packages=find_packages(exclude=['tests']),
    package_data=find_package_data('flexget', package='flexget',
        exclude=['FlexGet.egg-info', '*.pyc'],
        only_in_packages=False),  # NOTE: the exclude does not seem to work
    zip_safe=False,
    test_suite='nose.collector',
    extras_require={
        'memusage': ['guppy'],
        'NZB': ['pynzb'],
        'TaskTray': ['pywin32'],
        'webui': ['flask>=0.7', 'cherrypy']
    },
    entry_points=entry_points,
    classifiers=[
        "Development Status :: 5 - Production/Stable",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python",
        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.6",
        "Programming Language :: Python :: 2.7",
        "Programming Language :: Python :: Implementation :: CPython",
        "Programming Language :: Python :: Implementation :: PyPy",
    ]

)

options(
    minilib=Bunch(
        # 'version' is included as workaround to https://github.com/paver/paver/issues/112, TODO: remove
        extra_files=['virtual', 'svn', 'version']
    ),
    virtualenv=Bunch(
        paver_command_line='develop'
    ),
    # sphinxcontrib.paverutils
    sphinx=Bunch(
        docroot='docs',
        builddir='build',
        builder='html',
        confdir='docs'
    ),
)


def set_init_version(ver):
    """Replaces the version with ``ver`` in __init__.py"""
    import fileinput
    for line in fileinput.FileInput('flexget/__init__.py', inplace=1):
        if line.startswith('__version__ = '):
            line = "__version__ = '%s'\n" % ver
        print line,


@task
@cmdopts([
    ('online', None, 'Run online tests')
])
def test(options):
    """Run FlexGet unit tests"""
    options.setdefault('test', Bunch())
    import nose
    from nose.plugins.manager import DefaultPluginManager

    cfg = nose.config.Config(plugins=DefaultPluginManager(), verbosity=2)

    args = []
    # Adding the -v flag makes the tests fail in python 2.7
    #args.append('-v')
    args.append('--processes=4')
    args.append('-x')
    if not options.test.get('online'):
        args.append('--attr=!online')
    args.append('--where=tests')

    # Store current path since --where changes it, restore when leaving
    cwd = os.getcwd()
    try:
        return nose.run(argv=args, config=cfg)
    finally:
        os.chdir(cwd)


@task
def clean():
    """Cleans up the virtualenv"""
    import os
    import glob

    for p in ('bin', 'Scripts', 'build', 'dist', 'include', 'lib', 'man',
              'share', 'FlexGet.egg-info', 'paver-minilib.zip', 'setup.py'):
        pth = path(p)
        if pth.isdir():
            pth.rmtree()
        elif pth.isfile():
            pth.remove()

    for pkg in set(options.setup.packages) | set(('tests',)):
        for filename in glob.glob(pkg.replace('.', os.sep) + "/*.py[oc~]"):
            path(filename).remove()


@task
@cmdopts([
    ('dist-dir=', 'd', 'directory to put final built distributions in'),
    ('revision=', 'r', 'minor revision number of this build')
], share_with=['make_egg'])
def sdist(options):
    """Build tar.gz distribution package"""

    if not options.sdist.get('revision'):
        print 'Revision number required.'
        sys.exit(1)
    revision = options.sdist.pop('revision')

    print 'Revision: %s' % revision

    # clean previous build
    print 'Cleaning build...'
    for p in ['build']:
        pth = path(p)
        if pth.isdir():
            pth.rmtree()
        elif pth.isfile():
            pth.remove()
        else:
            print 'Unable to remove %s' % pth

    # remove pre-compiled pycs from tests, I don't know why paver even tries to include them ...
    # seems to happen only with sdist though
    for pyc in path('tests/').files('*.pyc'):
        pyc.remove()

    ver = '%s.%s' % (options['version'], revision)

    print 'Building %s' % ver

    # replace version number
    set_init_version(ver)

    # hack version number into setup( ... options='1.0' ...)
    from paver import tasks
    setup_section = tasks.environment.options.setdefault("setup", Bunch())
    setup_section.update(version=ver)

    for t in ['minilib', 'generate_setup', 'setuptools.command.sdist']:
        call_task(t)

    # restore version ...
    set_init_version('{git}')
    return ver


@task
@cmdopts([
    ('dist-dir=', 'd', 'directory to put final built distributions in'),
    ('revision=', 'r', 'minor revision number of this build')
], share_with=['sdist'])
def make_egg(options):
    # naming this task to bdist_egg will make egg installation fail

    if not options.make_egg.get('revision'):
        print 'Revision number required.'
        sys.exit(1)
    revision = options.make_egg.revision
    ver = '%s.%s' % (options['version'], revision)

    # hack version number into setup( ... options='1.0-svn' ...)
    from paver import tasks
    setup_section = tasks.environment.options.setdefault("setup", Bunch())
    setup_section.update(version=ver)

    # replace version number
    set_init_version(ver)

    print 'Making egg release'
    import shutil
    shutil.copytree('FlexGet.egg-info', 'FlexGet.egg-info-backup')

    options.setdefault('bdist_egg', Bunch())['dist_dir'] = options.make_egg.get('dist_dir')

    for t in ["minilib", "generate_setup", "setuptools.command.bdist_egg"]:
        call_task(t)

    # restore version ...
    set_init_version('{git}')

    # restore egg info from backup
    print 'Removing FlexGet.egg-info ...'
    shutil.rmtree('FlexGet.egg-info')
    print 'Restoring FlexGet.egg-info'
    shutil.move('FlexGet.egg-info-backup', 'FlexGet.egg-info')
    return ver


@task
def coverage():
    """Make coverage.flexget.com"""
    # --with-coverage --cover-package=flexget --cover-html --cover-html-dir /var/www/flexget_coverage/
    import nose
    from nose.plugins.manager import DefaultPluginManager

    cfg = nose.config.Config(plugins=DefaultPluginManager(), verbosity=2)
    argv = ['bin/paver']
    argv.extend(['--attr=!online'])
    argv.append('--with-coverage')
    argv.append('--cover-html')
    argv.extend(['--cover-package', 'flexget'])
    argv.extend(['--cover-html-dir', '/var/www/flexget_coverage/'])
    nose.run(argv=argv, config=cfg)
    print 'Coverage generated'


@task
@cmdopts([
    ('docs-dir=', 'd', 'directory to put the documetation in')
])
def docs():
    if not sphinxcontrib:
        print 'ERROR: requires sphinxcontrib-paverutils'
        sys.exit(1)
    from paver import tasks
    if not os.path.exists('build'):
        os.mkdir('build')
    if not os.path.exists(os.path.join('build', 'sphinx')):
        os.mkdir(os.path.join('build', 'sphinx'))

    setup_section = tasks.environment.options.setdefault("sphinx", Bunch())
    setup_section.update(outdir=options.docs.get('docs_dir', 'build/sphinx'))
    call_task('html')


@task
@might_call('test', 'sdist', 'make_egg')
@cmdopts([
    ('no-tests', None, 'skips unit tests'),
    ('type=', None, 'type of release (src | egg)'),
    ('ver-file=', None, 'java properties file to create with version number FG_VERSION')
])
def release(options):
    """Make a FlexGet release. Same as bdist_egg but adds version information."""

    if options.release.get('type') not in ['src', 'egg']:
        print 'Invalid --type, must be src or egg'
        sys.exit(1)

    print 'Cleaning build...'
    for p in ['build']:
        pth = path(p)
        if pth.isdir():
            pth.rmtree()
        elif pth.isfile():
            pth.remove()
        else:
            print 'Unable to remove %s' % pth

    # run unit tests
    if not options.release.get('no_tests'):
        if not test():
            print 'Unit tests did not pass'
            sys.exit(1)

    if options.release.get('type') == 'egg':
        print 'Making egg release'
        ver = make_egg()
    else:
        print 'Making src release'
        ver = sdist()

    if getattr(options.release, 'ver_file', False):
        with open(options.release.ver_file, 'w') as ver_file:
            ver_file.write('FG_VERSION=%s' % ver)

@task
def install_tools():
    """Install development / jenkins tools and dependencies"""

    try:
        import pip
    except:
        print 'FATAL: Unable to import pip, please install it and run this again!'
        sys.exit(1)

    try:
        import sphinxcontrib
        print 'sphinxcontrib INSTALLED'
    except:
        pip.main(['install', 'sphinxcontrib-paverutils'])

    pip.main(['install', '-r', 'jenkins-requirements.txt'])


@task
def clean_compiled():
    for root, dirs, files in os.walk('flexget'):
        for name in files:
            fqn = os.path.join(root, name)
            if fqn[-3:] == 'pyc' or fqn[-3:] == 'pyo' or fqn[-5:] == 'cover':
                print 'Deleting %s' % fqn
                os.remove(fqn)


@task
@consume_args
def pep8(args):
    try:
        import pep8
    except:
        print 'Run bin/paver install_tools'
        sys.exit(1)

    # Ignoring certain errors
    ignore = [
        'E711', 'E712',  # These are comparisons to singletons i.e. == False, and == None. We need these for sqlalchemy.
        'W291', 'W293', 'E261',
        'E128'  # E128 continuation line under-indented for visual indent
    ]
    styleguide = pep8.StyleGuide(show_source=True, ignore=ignore, repeat=1, max_line_length=120,
                                 parse_argv=args)
    styleguide.input_dir('flexget')

########NEW FILE########
__FILENAME__ = exec
"""
This is a helper script to call from test_exec.py
It requires 2 arguments, the output directory and filename.
A file will be created in the output directory with the given filename.
If there are more arguments to the script, they will be written 1 per line to the file.
"""
from __future__ import unicode_literals, division, absolute_import
import sys
import os

if __name__ == "__main__":
    # Make sure we have an output folder argument
    if len(sys.argv) < 3:
        print "exec.py must have parameter for output directory and filename"
        sys.exit(1)
    out_dir = sys.argv[1]
    filename = sys.argv[2]
    # Make sure the output folder exists
    if not os.path.exists(out_dir):
        print "output dir %s does not exist" % sys.argv[1]
        sys.exit(1)

    with open(os.path.join(out_dir, filename), 'w') as outfile:
        for arg in sys.argv[3:]:
            outfile.write(arg + '\n')

########NEW FILE########
__FILENAME__ = external_plugin
from __future__ import unicode_literals, division, absolute_import

from flexget import plugin
from flexget.entry import Entry
from flexget.event import event


class ExternalPlugin(object):
    def on_task_input(self, task, config):
        return [Entry('test entry', 'fake url')]


@event('plugin.register')
def register_plugin():
    plugin.register(ExternalPlugin, 'external_plugin', api_ver=2)

########NEW FILE########
__FILENAME__ = test_abort
from __future__ import unicode_literals, division, absolute_import

from flexget import plugin
from flexget.event import event
from tests import FlexGetBase


class AbortPlugin(object):
    def on_task_output(self, task, config):
        task.abort('abort plugin')


@event('plugin.register')
def register():
    plugin.register(AbortPlugin, 'abort', debug=True, api_ver=2)


class TestAbort(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            # causes on_task_abort to be called
            disable_builtins: yes

            # causes abort
            abort: yes

            # another event hookup with this plugin
            headers:
              test: value
    """

    def test_abort(self):
        self.execute_task('test', abort_ok=True)
        assert self.task.aborted, 'Task not aborted'

########NEW FILE########
__FILENAME__ = test_assume_quality
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.tools import assert_raises
from flexget.task import TaskAbort
import flexget.utils.qualities as qualities

class TestAssumeQuality(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            mock:
              - {title: 'Testfile[h264-720p]'}
              - {title: 'Testfile.1280x720'}
              - {title: 'Testfile.HDTV'}
              - {title: 'Testfile.cam'}
              - {title: 'Testfile.noquality'}
              - {title: 'Testfile.xvid.mp3'}
            accept_all: yes
        tasks:
          test_default:
            assume_quality:
              720p: flac
              h264: 10bit
              HDTV: truehd
              any: 720p h264

          test_simple:
            assume_quality: 720p h264

          test_priority:
            assume_quality:
              720p: mp3
              720p h264: flac
              h264: mp3

          test_matching:
            assume_quality:
              hdtv: 720p

          test_negative_matching:
            assume_quality:
              '!xvid !divx !mp3': 1080p

          test_no_clobber:
            assume_quality:
              720p: xvid

          test_invalid_target:
            assume_quality:
              potato: 720p

          test_invalid_quality:
            assume_quality:
              hdtv: rhubarb

          test_with_series:
            template: no_global
            mock:
            - title: my show S01E01
            assume_quality: 720p
            series:
            - my show:
                quality: 720p
    """

    def test_matching(self):
        self.execute_task('test_matching')
        entry = self.task.find_entry('entries', title='Testfile.HDTV')
        assert entry.get('quality') == qualities.Quality('720p HDTV')

    def test_negative_matching(self):
        self.execute_task('test_negative_matching')
        entry = self.task.find_entry('entries', title='Testfile.HDTV')
        assert entry.get('quality') == qualities.Quality('1080p HDTV')

        entry = self.task.find_entry('entries', title='Testfile.xvid.mp3')
        assert entry.get('quality') == qualities.Quality('xvid mp3')

    def test_no_clobber(self):
        self.execute_task('test_no_clobber')
        entry = self.task.find_entry('entries', title='Testfile[h264-720p]')
        assert entry.get('quality') != qualities.Quality('720p xvid')
        assert entry.get('quality') == qualities.Quality('720p h264')

    def test_default(self):
        self.execute_task('test_default')
        entry = self.task.find_entry('entries', title='Testfile.noquality')
        assert entry.get('quality') == qualities.Quality('720p h264'), 'Testfile.noquality quality not \'720p h264\''

    def test_simple(self):
        self.execute_task('test_simple')
        entry = self.task.find_entry('entries', title='Testfile.noquality')
        assert entry.get('quality') == qualities.Quality('720p h264'), 'Testfile.noquality quality not \'720p h264\''

    def test_priority(self):
        self.execute_task('test_priority')
        entry = self.task.find_entry('entries', title='Testfile[h264-720p]')
        assert entry.get('quality') != qualities.Quality('720p h264 mp3')
        assert entry.get('quality') == qualities.Quality('720p h264 flac')

    def test_invalid_target(self):
        #with assert_raises(TaskAbort): self.execute_task('test_invalid_target')  #Requires Python 2.7
        assert_raises(TaskAbort, self.execute_task, 'test_invalid_target')

    def test_invalid_quality(self):
        #with assert_raises(TaskAbort): self.execute_task('test_invalid_quality')  #Requires Python 2.7
        assert_raises(TaskAbort, self.execute_task, 'test_invalid_quality')

    def test_with_series(self):
        self.execute_task('test_with_series')
        assert self.task.accepted, 'series plugin should have used assumed quality'

########NEW FILE########
__FILENAME__ = test_backlog
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestBacklog(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Test.S01E01.hdtv-FlexGet', description: ''}
            set:
              description: '{{description}}I'
              laterfield: 'something'
            # Change the priority of set plugin so it runs on all entries. TODO: Remove, this is an ugly hack.
            plugin_priority:
              set: -254
            backlog: 10 minutes
    """

    def test_backlog(self):
        """Tests backlog (and snapshot) functionality."""

        # Test entry comes out as expected on first run
        self.execute_task('test')
        entry = self.task.find_entry(title='Test.S01E01.hdtv-FlexGet')
        assert entry['description'] == 'I'
        assert entry['laterfield'] == 'something'
        # Simulate entry leaving the task, make sure backlog injects it
        del(self.manager.config['tasks']['test']['mock'])
        self.execute_task('test')
        entry = self.task.find_entry(title='Test.S01E01.hdtv-FlexGet')
        assert entry['description'] == 'I'
        assert entry['laterfield'] == 'something'
        # This time take away the set plugin too, to make sure data is being restored at it's state from input
        del(self.manager.config['tasks']['test']['set'])
        self.execute_task('test')
        entry = self.task.find_entry(title='Test.S01E01.hdtv-FlexGet')
        assert entry['description'] == ''
        assert 'laterfield' not in entry

########NEW FILE########
__FILENAME__ = test_cached_input
from __future__ import unicode_literals, division, absolute_import
from datetime import timedelta
import os

from tests import FlexGetBase, with_filecopy
from flexget.utils.cached_input import cached
from flexget import plugin
from flexget.entry import Entry


class InputPersist(object):
    """Fake input plugin to test db cache. Only emits an entry the first time it is run."""

    hasrun = False

    @cached('test_input', persist='5 minutes')
    def on_task_input(self, task, config):
        if self.hasrun:
            return []
        self.hasrun = True
        return [Entry(title='Test', url='http://test.com')]

plugin.register(InputPersist, 'test_input', api_ver=2)


class TestInputCache(FlexGetBase):

    __yaml__ = """
        tasks:
          test_memory:
            rss:
              url: cached.xml
          test_db:
            test_input: True
    """

    @with_filecopy('rss.xml', 'cached.xml')
    def test_memory_cache(self):
        """Test memory input caching"""
        self.execute_task('test_memory')
        assert self.task.entries, 'should have created entries at the start'
        os.remove('cached.xml')
        f = open('cached.xml', 'w')
        f.write('')
        f.close()
        self.execute_task('test_memory')
        assert self.task.entries, 'should have created entries from the cache'
        # Turn the cache time down and run again to make sure the entries are not created again
        from flexget.utils.cached_input import cached
        cached.cache.cache_time = timedelta(minutes=0)
        self.execute_task('test_memory')
        assert not self.task.entries, 'cache should have been expired'

    def test_db_cache(self):
        """Test db input caching"""

        self.execute_task('test_db')
        assert self.task.entries, 'should have created entries at the start'
        self.execute_task('test_db')
        assert self.task.entries, 'should have created entries from the cache'

########NEW FILE########
__FILENAME__ = test_condition
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestCondition(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            disable_builtins: [seen]
            mock:
              - {title: 'test', year: 2000}
              - {title: 'brilliant', rating: 9.9}
              - {title: 'fresh', year: 2011}

        tasks:
          test_condition_reject:
            if:
              - year < 2011: reject

          test_condition_accept:
            if:
              - year>=2010: accept
              - rating>9: accept

          test_condition_and1:
            if:
              - "'t' in title and rating>9": accept
          test_condition_and2:
            if:
              - "'t' in title": accept

          test_has_field:
            if:
              - has_field('year'): accept

          test_sub_plugin:
            if:
              - title.upper() == 'TEST':
                  set:
                    some_field: some value
                  accept_all: yes
    """

    def test_reject(self):
        self.execute_task('test_condition_reject')
        count = len(self.task.rejected)
        assert count == 1

    def test_accept(self):
        self.execute_task('test_condition_accept')
        count = len(self.task.accepted)
        assert count == 2

    def test_implicit_and(self):
        for i in "12":
            self.execute_task('test_condition_and' + i)
            count = len(self.task.accepted)
            assert count == int(i)

    def test_has_field(self):
        self.execute_task('test_has_field')
        assert len(self.task.accepted) == 2

    def test_sub_plugin(self):
        self.execute_task('test_sub_plugin')
        entry = self.task.find_entry('accepted', title='test', some_field='some value')
        assert entry
        assert len(self.task.accepted) == 1


class TestQualityCondition(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            disable_builtins: [seen]
            mock:
              - {title: 'Smoke.1280x720'}
              - {title: 'Smoke.720p'}
              - {title: 'Smoke.1080i'}
              - {title: 'Smoke.HDTV'}
              - {title: 'Smoke.cam'}
              - {title: 'Smoke.HR'}
            accept_all: yes

        tasks:
          test_condition_quality_name_2:
            if:
              - "quality in ['hdtv', '1080i']": reject

          test_condition_quality_value_3:
            if:
              - "quality < '720p'": reject
    """

    def test_quality(self):
        for taskname in self.manager.config['tasks']:
            self.execute_task(taskname)
            count = len(self.task.rejected)
            expected = int(taskname[-1])
            assert count == expected, "Expected %s rejects, got %d" % (expected, count)

########NEW FILE########
__FILENAME__ = test_config
# -*- coding: utf-8 -*-
from __future__ import unicode_literals, division, absolute_import
import os

from tests import FlexGetBase
from flexget.manager import Manager


class TestConfig(FlexGetBase):
    def setup(self):
        super(TestConfig, self).setup()
        # Replace config loading methods of MockManager with the real ones
        self.manager.find_config = Manager.find_config.__get__(self.manager, self.manager.__class__)
        self.manager.load_config = Manager.load_config.__get__(self.manager, self.manager.__class__)

    def test_config_find_load_and_check_utf8(self):
        config_utf8_filename = os.path.join(self.base_path, 'config_utf8.yml')
        self.manager.options.config = config_utf8_filename

        self.manager.config = {}
        self.manager.find_config()
        self.manager.load_config()
        assert self.manager.config, 'Config didn\'t load'

########NEW FILE########
__FILENAME__ = test_configure_series_betaseries_list
from __future__ import unicode_literals, division, absolute_import
from mock import patch, call
from tests import FlexGetBase
import flexget.plugins.input.betaseries_list


def assert_mock_calls(expected_calls, mock_object):
    assert expected_calls == mock_object.mock_calls, "expecting calls %r, got %r instead" % \
                                                     (expected_calls, mock_object.mock_calls)


def assert_series_count_in_db(expected_count):
    from flexget.plugins.filter.series import Series
    from flexget.manager import Session
    session = Session()
    actual_series_count = session.query(Series).count()
    assert expected_count == actual_series_count, "expecting %s series stored in db, got %s instead" % \
                                                  (expected_count, actual_series_count)


class Test_configure_series_betaseries_list(FlexGetBase):

    __yaml__ = """
        tasks:
          test_no_members:
            configure_series:
              from:
                betaseries_list:
                  username: user_foo
                  password: passwd_foo
                  api_key: api_key_foo

          test_with_one_members:
            configure_series:
              from:
                betaseries_list:
                  username: user_foo
                  password: passwd_foo
                  api_key: api_key_foo
                  members:
                    - other_member_1

          test_with_two_members:
            configure_series:
              from:
                betaseries_list:
                  username: user_foo
                  password: passwd_foo
                  api_key: api_key_foo
                  members:
                    - other_member_1
                    - other_member_2
    """

    def setup(self):
        super(Test_configure_series_betaseries_list, self).setup()
        ## mock create_token
        self.create_token_patcher = patch.object(flexget.plugins.input.betaseries_list, "create_token",
                                                 return_value='token_foo')
        self.create_token_mock = self.create_token_patcher.start()

        ## mock query_series
        self.query_series_patcher = patch.object(flexget.plugins.input.betaseries_list, "query_series",
                                                 return_value=[])
        self.query_series_mock = self.query_series_patcher.start()

    def teardown(self):
        super(Test_configure_series_betaseries_list, self).teardown()
        self.create_token_patcher.stop()
        self.query_series_patcher.stop()

    def test_no_members(self):
        # GIVEN
        self.query_series_mock.return_value = ["Breaking Bad", "Dexter"]
        # WHEN
        self.execute_task('test_no_members')
        # THEN
        assert_series_count_in_db(2)
        assert_mock_calls([call('api_key_foo', 'user_foo', 'passwd_foo')],  self.create_token_mock)
        assert_mock_calls([call('api_key_foo', 'token_foo', 'user_foo')], self.query_series_mock)

    def test_with_one_members(self):
        # GIVEN
        self.query_series_mock.return_value = ["Breaking Bad", "Dexter", "The Simpsons"]
        # WHEN
        self.execute_task('test_with_one_members')
        # THEN
        assert_series_count_in_db(3)
        assert_mock_calls([call('api_key_foo', 'user_foo', 'passwd_foo')],  self.create_token_mock)
        assert_mock_calls([call('api_key_foo', 'token_foo', 'other_member_1')], self.query_series_mock)

    def test_with_two_members(self):
        # GIVEN
        return_values_generator = (val for val in [
            ["Family guy", "The Simpsons"],
            ["Breaking Bad", "Dexter", "The Simpsons"],
        ])
        self.query_series_mock.side_effect = lambda *args: return_values_generator.next()
        # WHEN
        self.execute_task('test_with_two_members')
        # THEN
        assert_series_count_in_db(4)
        assert_mock_calls([call('api_key_foo', 'user_foo', 'passwd_foo')],  self.create_token_mock)
        assert_mock_calls(
            [
                call('api_key_foo', 'token_foo', 'other_member_1'),
                call('api_key_foo', 'token_foo', 'other_member_2')
            ], self.query_series_mock)

########NEW FILE########
__FILENAME__ = test_config_schema
from __future__ import unicode_literals, division, absolute_import

import jsonschema

from flexget import config_schema
from tests import FlexGetBase


def iter_registered_schemas():
    for path in config_schema.schema_paths:
        schema = config_schema.resolve_ref(path)
        yield path, schema


class TestSchemaValidator(FlexGetBase):
    def test_registered_schemas_are_valid(self):
        for path, schema in iter_registered_schemas():
            try:
                config_schema.SchemaValidator.check_schema(schema)
            except jsonschema.SchemaError as e:
                assert False, 'plugin `%s` has an invalid schema. %s %s %s' % (
                    path, '/'.join(str(p) for p in e.path), e.validator, e.message)
            except Exception as e:
                assert False, 'plugin `%s` has an invalid schema. %s' % (path, e)

    def test_refs_in_schemas_are_resolvable(self):
        def refs_in(item):
            if isinstance(item, dict):
                for key, value in item.iteritems():
                    if key == '$ref':
                        yield value
                    else:
                        for ref in refs_in(value):
                            yield ref
            elif isinstance(item, list):
                for i in item:
                    for ref in refs_in(i):
                        yield ref

        for path, schema in iter_registered_schemas():
            resolver = config_schema.RefResolver.from_schema(schema)
            for ref in refs_in(schema):
                try:
                    with resolver.resolving(ref):
                        pass
                except jsonschema.RefResolutionError:
                    assert False, '$ref %s in schema %s is invalid' % (ref, path)

    def test_resolves_local_refs(self):
        schema = {'$ref': '/schema/plugin/accept_all'}
        # accept_all schema should be for type boolean
        assert not config_schema.process_config(True, schema)
        assert config_schema.process_config(14, schema)

    def test_custom_format_checker(self):
        schema = {'type': 'string', 'format': 'quality'}
        assert not config_schema.process_config('720p', schema)
        assert config_schema.process_config('aoeu', schema)

    def test_custom_error(self):
        schema = {'type': 'string', 'error': 'This is not okay'}
        errors = config_schema.process_config(13, schema)
        assert errors[0].message == schema['error']

    def test_custom_error_template(self):
        schema = {'type': 'string', 'minLength': 10, 'error': '{{validator}} failed for {{instance}}'}
        errors = config_schema.process_config(13, schema)
        assert errors[0].message == "type failed for 13"
        errors = config_schema.process_config('aoeu', schema)
        assert errors[0].message == "minLength failed for aoeu"

    def test_custom_keyword_error(self):
        schema = {'type': 'string', 'error_type': 'This is not okay'}
        errors = config_schema.process_config(13, schema)
        assert errors[0].message == schema['error_type']

    def test_custom_keyword_error_overrides(self):
        schema = {'type': 'string', 'error_type': 'This is not okay', 'error': 'This is worse'}
        errors = config_schema.process_config(13, schema)
        assert errors[0].message == schema['error_type']

    def test_error_with_path(self):
        schema = {'properties': {'p': {'items': {'type': 'string', 'error': 'ERROR'}}}}
        errors = config_schema.process_config({'p': [13]}, schema)
        assert errors[0].json_pointer == '/p/0'
        assert errors[0].message == 'ERROR'

    def test_builtin_error_rewriting(self):
        schema = {'type': 'object'}
        errors = config_schema.process_config(42, schema)
        # We don't call them objects around here
        assert 'object' not in errors[0].message
        assert 'dict' in errors[0].message

    def test_anyOf_branch_is_chosen_based_on_type_errors(self):
        schema = {
            "anyOf": [
                {"type": ["string", "array"]},
                {
                    "anyOf": [
                        {"type": "integer"},
                        {"type": "number", "minimum": 5}
                    ]
                }
            ]
        }
        # If there are type errors on both sides, it should be a virtual type error with all types
        errors = config_schema.process_config(True, schema)
        assert len(errors) == 1
        assert tuple(errors[0].schema_path) == ('anyOf', 'type')
        # It should have all the types together
        assert set(errors[0].validator_value) == set(['string', 'array', 'number', 'integer'])
        # If there are no type errors going down one branch it should choose it
        errors = config_schema.process_config(1.5, schema)
        assert len(errors) == 1
        assert errors[0].validator == 'minimum'

    def test_oneOf_branch_is_chosen_based_on_type_errors(self):
        schema = {
            "oneOf": [
                {"type": ["string", "array"]},
                {
                    "oneOf": [
                        {"type": "integer"},
                        {"type": "number", "minimum": 5}
                    ]
                }
            ]
        }
        errors = config_schema.process_config(True, schema)
        # If there are type errors on both sides, it should be a virtual type error with all types
        assert len(errors) == 1
        assert tuple(errors[0].schema_path) == ('oneOf', 'type')
        # It should have all the types together
        assert set(errors[0].validator_value) == set(['string', 'array', 'number', 'integer'])
        # If there are no type errors going down one branch it should choose it
        errors = config_schema.process_config(1.5, schema)
        assert len(errors) == 1
        assert errors[0].validator == 'minimum'

    def test_defaults_are_filled(self):
        schema = {"properties": {"p": {"default": 5}}}
        config = {}
        config_schema.process_config(config, schema)
        assert config["p"] == 5

    def test_defaults_does_not_override_explicit_value(self):
        schema = {"properties": {"p": {"default": 5}}}
        config = {"p": "foo"}
        config_schema.process_config(config, schema)
        assert config["p"] == "foo"

########NEW FILE########
__FILENAME__ = test_content_filter
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase, with_filecopy


class TestContentFilter(FlexGetBase):

    __yaml__ = """
        tasks:
          test_reject1:
            mock:
              - {title: 'test', file: 'test_reject1.torrent'}
            accept_all: yes
            content_filter:
              reject: '*.iso'

          test_reject2:
            mock:
              - {title: 'test', file: 'test_reject2.torrent'}
            accept_all: yes
            content_filter:
              reject: '*.avi'

          test_require1:
            mock:
              - {title: 'test', file: 'test_require1.torrent'}
            accept_all: yes
            content_filter:
              require:
                - '*.bin'
                - '*.iso'

          test_require2:
            mock:
              - {title: 'test', file: 'test_require2.torrent'}
            accept_all: yes
            content_filter:
              require: '*.avi'

          test_require_all1:
            mock:
              - {title: 'test', file: 'test_require_all.torrent'}
            accept_all: yes
            content_filter:
              require_all:
                - 'ubu*'
                - '*.iso'

          test_require_all2:
            mock:
              - {title: 'test', file: 'test_require_all.torrent'}
            accept_all: yes
            content_filter:
              require_all:
                - '*.iso'
                - '*.avi'

          test_strict:
            mock:
              - {title: 'test'}
            accept_all: yes
            content_filter:
              require: '*.iso'
              strict: true

          test_cache:
            mock:
              - {title: 'test', url: 'http://localhost/', file: 'test.torrent'}
            accept_all: yes
            content_filter:
              reject: ['*.iso']
    """

    @with_filecopy('test.torrent', 'test_reject1.torrent')
    def test_reject1(self):
        self.execute_task('test_reject1')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected, contains *.iso'

    @with_filecopy('test.torrent', 'test_reject2.torrent')
    def test_reject2(self):
        self.execute_task('test_reject2')
        assert self.task.find_entry('accepted', title='test'), \
            'should have accepted, doesn\t contain *.avi'

    @with_filecopy('test.torrent', 'test_require1.torrent')
    def test_require1(self):
        self.execute_task('test_require1')
        assert self.task.find_entry('accepted', title='test'), \
            'should have accepted, contains *.iso'

    @with_filecopy('test.torrent', 'test_require2.torrent')
    def test_require2(self):
        self.execute_task('test_require2')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected, doesn\t contain *.avi'

    @with_filecopy('test.torrent', 'test_require_all.torrent')
    def test_require_all1(self):
        self.execute_task('test_require_all1')
        assert self.task.find_entry('accepted', title='test'), \
            'should have accepted, both masks are satisfied'

    @with_filecopy('test.torrent', 'test_require_all.torrent')
    def test_require_all2(self):
        self.execute_task('test_require_all2')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected, one mask isn\'t satisfied'

    @with_filecopy('test.torrent', 'test_strict.torrent')
    def test_strict(self):
        """Content Filter: strict enabled"""
        self.execute_task('test_strict')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected non torrent'

    def test_cache(self):
        """Content Filter: caching"""
        self.execute_task('test_cache')

        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected, contains *.iso'

        # Test that remember_rejected rejects the entry before us next time
        self.execute_task('test_cache')
        assert self.task.find_entry('rejected', title='test', rejected_by='remember_rejected'), \
            'should have rejected, content files present from the cache'

########NEW FILE########
__FILENAME__ = test_content_size
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase, with_filecopy


class TestTorrentSize(FlexGetBase):

    __yaml__ = """
        tasks:
          test_min:
            mock:
              - {title: 'test', file: 'test_min.torrent'}
            accept_all: yes
            content_size:
              min: 2000

          test_max:
            mock:
              - {title: 'test', file: 'test_max.torrent'}
            accept_all: yes
            content_size:
              max: 10

          test_strict:
            mock:
              - {title: 'test'}
            accept_all: yes
            content_size:
              min: 1
              strict: yes

          test_cache:
            mock:
              - {title: 'test', url: 'http://localhost/', file: 'test.torrent'}
            accept_all: yes
            content_size:
              min: 2000
    """

    @with_filecopy('test.torrent', 'test_min.torrent')
    def test_min(self):
        """Content Size: torrent with min size"""
        self.execute_task('test_min')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected, minimum size'

    @with_filecopy('test.torrent', 'test_max.torrent')
    def test_max(self):
        """Content Size: torrent with max size"""
        self.execute_task('test_max')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected, maximum size'

    @with_filecopy('test.torrent', 'test_strict.torrent')
    def test_strict(self):
        """Content Size: strict enabled"""
        self.execute_task('test_strict')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected non torrent'

    def test_cache(self):
        """Content Size: caching"""
        self.execute_task('test_cache')
        assert self.task.find_entry('rejected', title='test'), \
            'should have rejected, too small'

        # Make sure remember_rejected rejects on the second execution
        self.execute_task('test_cache')
        assert self.task.find_entry('rejected', title='test', rejected_by='remember_rejected'), \
            'should have rejected, size present from the cache'


class TestFileSize(FlexGetBase):
    """This is to test that content_size is picked up from the file itself when listdir is used as the input.
    This doesn't do a super job of testing, because we don't have any test files bigger than 1 MB."""

    __yaml__ = """
        tasks:
          test_min:
            mock:
              - {title: 'test', location: 'min.file'}
            accept_all: yes
            content_size:
              min: 2000

          test_max:
            mock:
              - {title: 'test', location: 'max.file'}
            accept_all: yes
            content_size:
              max: 2000

          test_torrent:
            mock:
              # content_size should not be read for this directly, as it is a torrent file
              - {title: 'test', location: 'test.torrent'}
    """

    @with_filecopy('test.torrent', 'min.file')
    def test_min(self):
        """Content Size: torrent with min size"""
        self.execute_task('test_min')
        entry = self.task.find_entry('rejected', title='test')
        assert entry, 'should have rejected, minimum size'
        assert entry['content_size'] == 0, \
            'content_size was not detected'

    @with_filecopy('test.torrent', 'max.file')
    def test_max(self):
        """Content Size: torrent with max size"""
        self.execute_task('test_max')
        entry = self.task.find_entry('accepted', title='test')
        assert entry, 'should have been accepted, it is below maximum size'
        assert entry['content_size'] == 0, \
            'content_size was not detected'

    def test_torrent(self):
        self.execute_task('test_torrent')
        entry = self.task.find_entry('entries', title='test')
        assert 'content_size' not in entry, \
            'size of .torrent file should not be read as content_size'

########NEW FILE########
__FILENAME__ = test_cookies
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.plugins.attrib import attr

class TestCookies(FlexGetBase):
    __yaml__ = """
        tasks:
          test_cookies:
            text:
              url: http://httpbin.org/cookies
              entry:
                title: '\"title\": \"(.*)\"'
                url: '\"url\": \"(.*)\"'
            cookies: cookies.txt
    """

    @attr(online=True)
    def test_cookies(self):
        self.execute_task('test_cookies', options={'nocache': True})
        assert self.task.find_entry(title='blah', url='aoeu'), 'Entry should have been created.'

########NEW FILE########
__FILENAME__ = test_crossmatch
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase

class TestCrossmatch(FlexGetBase):
    __yaml__ = """
        tasks:
          test_title:
            mock:
            - title: entry 1
            - title: entry 2
            crossmatch:
              from:
              - mock:
                - title: entry 2
              action: reject
              fields: [title]
    """

    def test_reject_title(self):
        self.execute_task('test_title')
        assert self.task.find_entry('rejected', title='entry 2')
        assert len(self.task.rejected) == 1

########NEW FILE########
__FILENAME__ = test_delay
from __future__ import unicode_literals, division, absolute_import
from datetime import timedelta
from tests import FlexGetBase
from flexget.manager import Session
from flexget.plugins.filter.delay import DelayedEntry


class TestDelay(FlexGetBase):
    __yaml__ = """
        tasks:
          test:
            mock:
              - title: entry 1
            delay: 1 hours
        """

    def test_delay(self):
        self.execute_task('test')
        assert not self.task.entries, 'No entries should have passed delay'
        # Age the entry in the db
        session = Session()
        delayed_entries = session.query(DelayedEntry).all()
        for entry in delayed_entries:
            entry.expire = entry.expire - timedelta(hours=1)
        session.commit()
        self.execute_task('test')
        assert self.task.entries, 'Entry should have passed delay and been inserted'
        # Make sure entry is only injected once
        self.execute_task('test')
        assert not self.task.entries, 'Entry should only be insert'

########NEW FILE########
__FILENAME__ = test_discover
from __future__ import unicode_literals, division, absolute_import
from datetime import datetime, timedelta

from flexget.entry import Entry
from flexget import plugin
import flexget.validator
from tests import FlexGetBase


class SearchPlugin(object):
    """Fake search plugin. Just returns the entry it was given."""

    def validator(self):
        return flexget.validator.factory('boolean')

    def search(self, entry, comparator=None, config=None):
        return [Entry(entry)]

plugin.register(SearchPlugin, 'test_search', groups=['search'], api_ver=2)


class EstRelease(object):
    """Fake release estimate plugin. Just returns 'est_release' entry field."""

    def estimate(self, entry):
        return entry.get('est_release')

plugin.register(EstRelease, 'test_release', groups=['estimate_release'], api_ver=2)


class TestDiscover(FlexGetBase):
    __yaml__ = """
        tasks:
          test_sort:
            discover:
              ignore_estimations: yes
              what:
              - mock:
                - title: Foo
                  search_sort: 1
                - title: Bar
                  search_sort: 3
                - title: Baz
                  search_sort: 2
              from:
              - test_search: yes
          test_interval:
            discover:
              ignore_estimations: yes
              what:
              - mock:
                - title: Foo
              from:
              - test_search: yes
          test_estimates:
            discover:
              interval: 0 seconds
              what:
              - mock:
                - title: Foo
              from:
              - test_search: yes
          test_emit_series:
            discover:
              ignore_estimations: yes
              what:
              - emit_series:
                  from_start: yes
              from:
              - test_search: yes
            series:
            - My Show:
                identified_by: ep
            rerun: 0

    """

    def test_sort(self):
        self.execute_task('test_sort')
        assert len(self.task.entries) == 3
        # Entries should be ordered by search_sort
        order = list(e.get('search_sort') for e in self.task.entries)
        assert order == sorted(order, reverse=True)

    def test_interval(self):
        self.execute_task('test_interval')
        assert len(self.task.entries) == 1

        # Insert a new entry into the search input
        self.manager.config['tasks']['test_interval']['discover']['what'][0]['mock'].append({'title': 'Bar'})
        self.execute_task('test_interval')
        # First entry should be waiting for interval
        assert len(self.task.entries) == 1
        assert self.task.entries[0]['title'] == 'Bar'

        # Now they should both be waiting
        self.execute_task('test_interval')
        assert len(self.task.entries) == 0

    def test_estimates(self):
        mock_config = self.manager.config['tasks']['test_estimates']['discover']['what'][0]['mock']
        # It should not be searched before the release date
        mock_config[0]['est_release'] = datetime.now() + timedelta(days=7)
        self.execute_task('test_estimates')
        assert len(self.task.entries) == 0
        # It should be searched after the release date
        mock_config[0]['est_release'] = datetime.now()
        self.execute_task('test_estimates')
        assert len(self.task.entries) == 1

    def test_emit_series(self):
        self.execute_task('test_emit_series')
        assert self.task.find_entry(title='My Show S01E01')

class TestEmitSeriesInDiscover(FlexGetBase):
    __yaml__ = """
        tasks:
          inject_series:
            series:
              - My Show 2
          test_emit_series_backfill:
            discover:
              ignore_estimations: yes
              what:
              - emit_series:
                  backfill: yes
              from:
              - test_search: yes
            series:
            - My Show 2:
                tracking: backfill
                identified_by: ep
            rerun: 0
    """

    def inject_series(self, release_name):
        self.execute_task('inject_series', options = {'inject': [Entry(title=release_name, url='')]})

    def test_emit_series_backfill(self):
        self.inject_series('My Show 2 S02E01')
        self.execute_task('test_emit_series_backfill')
        assert self.task.find_entry(title='My Show 2 S01E01')
        assert self.task.find_entry(title='My Show 2 S02E02')

########NEW FILE########
__FILENAME__ = test_download
from __future__ import unicode_literals, division, absolute_import
import sys
import tempfile

from nose.plugins.attrib import attr
from nose.plugins.skip import SkipTest
from nose.tools import assert_raises

from flexget.task import TaskAbort
from tests import FlexGetBase

# TODO more checks: fail_html, etc.
class TestDownload(FlexGetBase):
    __yaml__ = """
        tasks:
          path_and_temp:
            mock:
              - {title: 'entry 1', url: 'http://www.speedtest.qsc.de/1kB.qsc'}
            accept_all: yes
            download:
              path: ~/
              temp: """ + tempfile.gettempdir() + """
          just_path:
            mock:
              - {title: 'entry 2', url: 'http://www.speedtest.qsc.de/10kB.qsc'}
            accept_all: yes
            download:
              path: ~/
          just_string:
            mock:
              - {title: 'entry 3', url: 'http://www.speedtest.qsc.de/100kB.qsc'}
            accept_all: yes
            download: ~/
      """

    @attr(online=True)
    def test_path_and_temp(self):
        """Download plugin: Path and Temp directories set"""
        self.execute_task('path_and_temp')
        assert not self.task.aborted, 'Task should not have aborted'

    @attr(online=True)
    def test_just_path(self):
        """Download plugin: Path directory set as dict"""
        self.execute_task('just_path')
        assert not self.task.aborted, 'Task should not have aborted'

    @attr(online=True)
    def test_just_string(self):
        """Download plugin: Path directory set as string"""
        self.execute_task('just_string')
        assert not self.task.aborted, 'Task should not have aborted'


class TestDownloadTemp(FlexGetBase):
    __yaml__ = """
        tasks:
          temp_wrong_permission:
            mock:
              - {title: 'entry 1', url: 'http://www.speedtest.qsc.de/1kB.qsc'}
            accept_all: yes
            download:
              path: ~/
              temp: /root
          temp_non_existent:
            download:
              path: ~/
              temp: /a/b/c/non/existent/
          temp_wrong_config_1:
            download:
              path: ~/
              temp: no
          temp_wrong_config_2:
            download:
              path: ~/
              temp: 3
          temp_empty:
            download:
              path: ~/
              temp:
        """
# TODO: These are really just config validation tests, and I have config validation turned off at the moment for unit
# tests due to some problems
'''
    def test_wrong_permission(self):
        """Download plugin: Temp directory has wrong permissions"""
        if sys.platform.startswith('win'):
            raise SkipTest  # TODO: Windows doesn't have a guaranteed 'private' directory afaik
        self.execute_task('temp_wrong_permission', abort_ok=True)
        assert self.task.aborted

    def test_temp_non_existent(self):
        """Download plugin: Temp directory does not exist"""
        self.execute_task('temp_non_existent', abort_ok=True)
        assert self.task.aborted

    def test_wrong_config_1(self):
        """Download plugin: Temp directory config error [1of3]"""
        self.execute_task('temp_wrong_config_1', abort_ok=True)
        assert self.task.aborted

    def test_wrong_config_2(self):
        """Download plugin: Temp directory config error [2of3]"""
        self.execute_task('temp_wrong_config_2', abort_ok=True)
        assert self.task.aborted

    def test_wrong_config_3(self):
        """Download plugin: Temp directory config error [3of3]"""
        self.execute_task('temp_empty', abort_ok=True)
        assert self.task.aborted
'''

########NEW FILE########
__FILENAME__ = test_emit_movie_queue
from __future__ import unicode_literals, division, absolute_import
from datetime import timedelta, datetime

from nose.plugins.attrib import attr

from flexget.manager import Session
from flexget.plugins.filter.movie_queue import queue_add, QueuedMovie
from tests import FlexGetBase


def age_last_emit(**kwargs):
    session = Session()
    for item in session.query(QueuedMovie).all():
        item.last_emit = datetime.utcnow() - timedelta(**kwargs)
    session.commit()


class TestEmitMovieQueue(FlexGetBase):
    __yaml__ = """
        tasks:
          test_default:
            emit_movie_queue:
              # TODO: Currently plugin calls tmdb lookup to get year, movie queue should probably store
              year: no
        """

    def test_default(self):
        queue_add(title='The Matrix 1999', imdb_id='tt0133093', tmdb_id=603)
        self.execute_task('test_default')
        assert len(self.task.entries) == 1
        # Movie ids should be provided on the entry without needing lookups
        entry = self.task.entries[0]
        assert entry.get('imdb_id', eval_lazy=False) == 'tt0133093'
        assert entry.get('tmdb_id', eval_lazy=False) == 603
        self.execute_task('test_default')
        assert len(self.task.entries) == 1, 'Movie should be emitted every run'

########NEW FILE########
__FILENAME__ = test_emit_series
from __future__ import unicode_literals, division, absolute_import

from tests import FlexGetBase
from flexget.entry import Entry


class TestEmitSeries(FlexGetBase):
    __yaml__ = """
        tasks:
          inject_series:
            series:
              - Test Series 1
              - Test Series 2:
                  quality: 1080p
              - Test Series 3
              - Test Series 4
              - Test Series 5
              - Test Series 6
              - Test Series 7
              - Test Series 8
          test_emit_series_backfill:
            emit_series:
              backfill: yes
            series:
            - Test Series 1:
                tracking: backfill
                identified_by: ep
            rerun: 0
          test_emit_series_rejected:
            emit_series:
              backfill: yes
            series:
            - Test Series 2:
                tracking: backfill
                identified_by: ep
            rerun: 0
          test_emit_series_from_start:
            emit_series: yes
            series:
            - Test Series 3:
                from_start: yes
                identified_by: ep
            rerun: 0
          test_emit_series_begin:
            emit_series: yes
            series:
            - Test Series 4:
                begin: S03E03
                identified_by: ep
            rerun: 0
          test_emit_series_begin_and_backfill:
            emit_series:
              backfill: yes
            series:
            - Test Series 5:
                begin: S02E02
                tracking: backfill
            rerun: 0
          test_emit_series_begin_backfill_and_rerun:
            emit_series:
              backfill: yes
            series:
            - Test Series 6:
                begin: S02E02
                tracking: backfill
            mock_output: yes
            rerun: 1
          test_emit_series_backfill_advancement:
            emit_series:
              backfill: yes
            series:
            - Test Series 7:
                identified_by: ep
                tracking: backfill
            regexp:
              reject:
              - .
          test_emit_series_advancement:
            emit_series: yes
            series:
            - Test Series 8:
                identified_by: ep
            regexp:
              reject:
              - .
    """

    def inject_series(self, release_name):
        self.execute_task('inject_series', options = {'inject': [Entry(title=release_name, url='')]})

    def test_emit_series_backfill(self):
        self.inject_series('Test Series 1 S02E01')
        self.execute_task('test_emit_series_backfill')
        assert self.task.find_entry(title='Test Series 1 S01E01')
        assert self.task.find_entry(title='Test Series 1 S02E02')
        self.execute_task('test_emit_series_backfill')
        assert self.task.find_entry(title='Test Series 1 S01E02')
        assert self.task.find_entry(title='Test Series 1 S02E03')
        self.inject_series('Test Series 1 S02E08')
        self.execute_task('test_emit_series_backfill')
        assert self.task.find_entry(title='Test Series 1 S01E03')
        assert self.task.find_entry(title='Test Series 1 S02E04')
        assert self.task.find_entry(title='Test Series 1 S02E05')
        assert self.task.find_entry(title='Test Series 1 S02E06')
        assert self.task.find_entry(title='Test Series 1 S02E07')

    def test_emit_series_rejected(self):
        self.inject_series('Test Series 2 S01E03 720p')
        self.execute_task('test_emit_series_rejected')
        assert self.task.find_entry(title='Test Series 2 S01E01')
        assert self.task.find_entry(title='Test Series 2 S01E02')
        assert self.task.find_entry(title='Test Series 2 S01E03')

    def test_emit_series_from_start(self):
        self.inject_series('Test Series 3 S01E03')
        self.execute_task('test_emit_series_from_start')
        assert self.task.find_entry(title='Test Series 3 S01E01')
        assert self.task.find_entry(title='Test Series 3 S01E02')
        assert self.task.find_entry(title='Test Series 3 S01E04')
        self.execute_task('test_emit_series_from_start')
        assert self.task.find_entry(title='Test Series 3 S01E05')

    def test_emit_series_begin(self):
        self.execute_task('test_emit_series_begin')
        assert self.task.find_entry(title='Test Series 4 S03E03')

    def test_emit_series_begin_and_backfill(self):
        self.execute_task('test_emit_series_begin_and_backfill')
        # with backfill and begin, no backfilling should be done
        assert self.task.find_entry(title='Test Series 5 S02E02')

    def test_emit_series_begin_backfill_and_rerun(self):
        self.execute_task('test_emit_series_begin_backfill_and_rerun')
        # with backfill and begin, no backfilling should be done
        assert len(self.task.mock_output) == 2 # Should have S02E02 and S02E03

    def test_emit_series_backfill_advancement(self):
        self.inject_series('Test Series 7 S02E01')
        self.execute_task('test_emit_series_backfill_advancement')
        assert self.task._rerun_count == 1
        assert len(self.task.all_entries) == 1
        assert self.task.find_entry('rejected', title='Test Series 7 S03E01')

    def test_emit_series_advancement(self):
        self.inject_series('Test Series 8 S01E01')
        self.execute_task('test_emit_series_advancement')
        assert self.task._rerun_count == 1
        assert len(self.task.all_entries) == 1
        assert self.task.find_entry('rejected', title='Test Series 8 S02E01')

########NEW FILE########
__FILENAME__ = test_exec
from __future__ import unicode_literals, division, absolute_import
import os
import sys

from tests import FlexGetBase


class TestExec(FlexGetBase):

    __tmp__ = True
    __yaml__ = """
        templates:
          global:
            set:
              temp_dir: '__tmp__'
            accept_all: yes
        tasks:
          replace_from_entry:
            mock:
              - {title: 'replace'}
              - {title: 'replace with spaces'}
            exec: """ + sys.executable + """ exec.py "{{temp_dir}}" "{{title}}"
          test_adv_format:
            mock:
              - {title: entry1, location: '/path/with spaces', quotefield: "with'quote"}
            exec:
              on_output:
                for_entries: """ + sys.executable + """ exec.py "{{temp_dir}}" "{{title}}" "{{location}}" "/the/final destinaton/" "a {{quotefield}}" "/a hybrid{{location}}"
          test_auto_escape:
            mock:
              - {title: entry2, quotes: single ' double", otherchars: '% a $a! ` *'}
            exec:
              auto_escape: yes
              on_output:
                for_entries: """ + sys.executable + """ exec.py "{{temp_dir}}" "{{title}}" "{{quotes}}" "/start/{{quotes}}" "{{otherchars}}"
    """

    def test_replace_from_entry(self):
        self.execute_task('replace_from_entry')
        assert len(self.task.accepted) == 2, "not all entries were accepted"
        for entry in self.task.accepted:
            assert os.path.exists(os.path.join(self.__tmp__, entry['title'])), "exec.py did not create a file for %s" % entry['title']

    def test_adv_format(self):
        self.execute_task('test_adv_format')
        for entry in self.task.accepted:
            with open(os.path.join(self.__tmp__, entry['title']), 'r') as infile:
                line = infile.readline().rstrip('\n')
                assert line == '/path/with spaces', '%s != /path/with spaces' % line
                line = infile.readline().rstrip('\n')
                assert line == '/the/final destinaton/', '%s != /the/final destinaton/' % line
                line = infile.readline().rstrip('\n')
                assert line == 'a with\'quote', '%s != a with\'quote' % line
                line = infile.readline().rstrip('\n')
                assert line == '/a hybrid/path/with spaces', '%s != /a hybrid/path/with spaces' % line

    # TODO: This doesn't work on linux.
    """
    def test_auto_escape(self):
        self.execute_task('test_auto_escape')
        for entry in self.task.accepted:
            with open(os.path.join(self.__tmp__, entry['title']), 'r') as infile:
                line = infile.readline().rstrip('\n')
                assert line == 'single \' double\"', '%s != single \' double\"' % line
                line = infile.readline().rstrip('\n')
                assert line == '/start/single \' double\"', '%s != /start/single \' double\"' % line
                line = infile.readline().rstrip('\n')
                assert line == '% a $a! ` *', '%s != % a $a! ` *' % line
    """

########NEW FILE########
__FILENAME__ = test_exists_series
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
import os
from tests.util import maketemp


class TestExistsSeries(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Foo.Bar.S01E02.XViD'}
              - {title: 'Foo.Bar.S01E03.XViD'}
            series:
              - foo bar
            exists_series:
              path: autogenerated in setup()

          test_diff_qualities_allowed:
            mock:
              - {title: 'Asdf.S01E02.720p'}
            series:
              - asdf
            exists_series:
              path:  path autogenerated in setup()
              allow_different_qualities: yes

          test_diff_qualities_not_allowed:
            mock:
              - {title: 'Asdf.S01E02.720p'}
            series:
              - asdf
            exists_series: path autogenerated in setup()

          test_diff_qualities_downgrade:
            mock:
              - {title: 'Asdf.S01E02.sdtv'}
            series:
              - asdf
            exists_series:
              path:  path autogenerated in setup()
              allow_different_qualities: better

          test_diff_qualities_upgrade:
            mock:
              - {title: 'Asdf.S01E02.webdl'}
            series:
              - asdf
            exists_series:
              path:  path autogenerated in setup()
              allow_different_qualities: better

          test_propers:
            mock:
              - {title: 'Mock.S01E01.Proper'}
              - {title: 'Test.S01E01'}
            series:
              - mock
              - test
            exists_series: path autogenerated in setup()

          test_invalid:
            mock:
              - {title: 'Invalid.S01E01'}
            series:
              - invalid
            exists_series: path autogenerated in setup()

          test_with_metainfo_series:
            metainfo_series: yes
            mock:
              - {title: 'Foo.Bar.S01E02.XViD'}
              - {title: 'Foo.Bar.S01E03.XViD'}
            accept_all: yes
            exists_series: path autogenerated in setup()
          test_jinja_path:
            series:
            - jinja
            - jinja2
            mock:
            - title: jinja s01e01
            - title: jinja s01e02
            - title: jinja2 s01e01
            accept_all: yes
            exists_series: path autogenerated in setup()
    """

    test_dirs = ['Foo.Bar.S01E02.XViD-GrpA', 'Asdf.S01E02.HDTV', 'Mock.S01E01.XViD', 'Test.S01E01.Proper',
                 'jinja/jinja.s01e01', 'jinja.s01e02', 'jinja2/jinja2.s01e01', 'invalid']

    def __init__(self):
        self.test_home = None
        FlexGetBase.__init__(self)

    def setup(self):
        FlexGetBase.setup(self)
        # generate config
        self.test_home = maketemp()
        for task_name in self.manager.config['tasks'].iterkeys():
            if isinstance(self.manager.config['tasks'][task_name]['exists_series'], dict):
                self.manager.config['tasks'][task_name]['exists_series']['path'] = self.test_home
            else:
                self.manager.config['tasks'][task_name]['exists_series'] = self.test_home
        # create test dirs
        for test_dir in self.test_dirs:
            os.makedirs(os.path.join(self.test_home, test_dir))

    def teardown(self):
        curdir = os.getcwd()
        os.chdir(self.test_home)
        for test_dir in self.test_dirs:
            os.removedirs(test_dir)
        os.chdir(curdir)
        os.rmdir(self.test_home)
        FlexGetBase.teardown(self)

    def test_existing(self):
        """Exists_series plugin: existing"""
        self.execute_task('test')
        assert not self.task.find_entry('accepted', title='Foo.Bar.S01E02.XViD'), \
            'Foo.Bar.S01E02.XViD should not have been accepted (exists)'
        assert self.task.find_entry('accepted', title='Foo.Bar.S01E03.XViD'), \
            'Foo.Bar.S01E03.XViD should have been accepted'

    def test_diff_qualities_allowed(self):
        """Exists_series plugin: existsting but w. diff quality"""
        self.execute_task('test_diff_qualities_allowed')
        assert self.task.find_entry('accepted', title='Asdf.S01E02.720p'), \
            'Asdf.S01E02.720p should have been accepted'

    def test_diff_qualities_not_allowed(self):
        """Exists_series plugin: existsting but w. diff quality"""
        self.execute_task('test_diff_qualities_not_allowed')
        assert self.task.find_entry('rejected', title='Asdf.S01E02.720p'), \
            'Asdf.S01E02.720p should have been rejected'

    def test_diff_qualities_downgrade(self):
        """Test worse qualities than exist are rejected."""
        self.execute_task('test_diff_qualities_downgrade')
        assert self.task.find_entry('rejected', title='Asdf.S01E02.sdtv'), \
            'Asdf.S01E02.sdtv should have been rejected'

    def test_diff_qualities_upgrade(self):
        """Test better qualities than exist are accepted."""
        self.execute_task('test_diff_qualities_upgrade')
        assert self.task.find_entry('accepted', title='Asdf.S01E02.webdl'), \
            'Asdf.S01E02.webdl should have been rejected'

    def test_propers(self):
        """Exists_series plugin: new proper & proper already exists"""
        self.execute_task('test_propers')
        assert self.task.find_entry('accepted', title='Mock.S01E01.Proper'), \
            'new proper not accepted'
        assert self.task.find_entry('rejected', title='Test.S01E01'), \
            'pre-existin proper should have caused reject'

    def test_invalid(self):
        """Exists_series plugin: no episode numbering on the disk"""
        # shouldn't raise anything
        self.execute_task('test_invalid')

    def test_with_metainfo_series(self):
        """Tests that exists_series works with series data from metainfo_series"""
        self.execute_task('test_with_metainfo_series')
        assert self.task.find_entry('rejected', title='Foo.Bar.S01E02.XViD'), \
            'Foo.Bar.S01E02.XViD should have been rejected(exists)'
        assert not self.task.find_entry('rejected', title='Foo.Bar.S01E03.XViD'), \
            'Foo.Bar.S01E03.XViD should not have been rejected'

    def test_jinja_path(self):
        self.manager.config['tasks']['test_jinja_path']['exists_series'] += '/{{series_name}}'
        self.execute_task('test_jinja_path')
        assert self.task.find_entry('rejected', title='jinja s01e01'), \
            'jinja s01e01 should have been rejected (exists)'
        assert self.task.find_entry('rejected', title='jinja2 s01e01'), \
            'jinja2 s01e01 should have been rejected (exists)'
        assert self.task.find_entry('accepted', title='jinja s01e02'), \
            'jinja s01e02 should have been accepted'

########NEW FILE########
__FILENAME__ = test_feed_control
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestOnlytask(FlexGetBase):
    """
        Test --task option
    """

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'download', url: 'http://localhost/download'}
          test2:
            mock:
              - {title: 'nodownload', url: 'http://localhost/nodownload'}
    """

    def test_manual_with_onlytask(self):
        # TODO: 1.2 we need to test this with execute command
        return
        # Pretend we have been run with --task test
        # This task should run normally, as we specified it as onlytask
        self.execute_task('test', options=dict(tasks=['test']))
        assert self.task.find_entry(title='download'), 'task failed to download with --task'
        # This task should be disabled, as it wasn't specified with onlytask
        self.execute_task('test2', options=dict(tasks=['test']), abort_ok=True)
        assert self.task.aborted
        assert not self.task.find_entry(title='nodownload'), 'task should not have been executed'


class TestManualAutomatic(FlexGetBase):
    """
        Test manual download tasks
    """

    __yaml__ = """
        tasks:
          test:
            manual: true
            mock:
              - {title: 'nodownload', url: 'http://localhost/nodownload'}
    """

    def test_manual_without_onlytask(self):
        self.execute_task('test', abort_ok=True)
        assert self.task.aborted
        assert not self.task.find_entry(title='nodownload'), 'Manual tasks downloaded on automatic run'


class TestManualOnlytask(FlexGetBase):
    """
        Test manual download tasks
    """

    __yaml__ = """
        tasks:
          test2:
            manual: true
            mock:
              - {title: 'download', url: 'http://localhost/download'}
    """

    def test_manual_with_onlytask(self):
        # Pretend we have been run with --task test2
        self.execute_task('test2', options=dict(tasks=['test2']))
        assert self.task.find_entry(title='download'), 'Manual tasks failed to download on manual run'

########NEW FILE########
__FILENAME__ = test_headers
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.plugins.attrib import attr

class TestHeaders(FlexGetBase):
    __yaml__ = """
        tasks:
          test_headers:
            text:
              url: http://httpbin.org/cookies
              entry:
                title: '\"title\": \"(.*)\"'
                url: '\"url\": \"(.*)\"'
            headers:
              Cookie: "title=blah; url=other"
    """

    @attr(online=True)
    def test_headers(self):
        self.execute_task('test_headers', options={'nocache': True})
        assert self.task.find_entry(title='blah', url='other'), 'Entry should have been created.'

########NEW FILE########
__FILENAME__ = test_html5lib
from __future__ import unicode_literals, division, absolute_import
from flexget.utils.soup import get_soup


class TestHtml5Lib():

    def test_parse_broken(self):
        s = """<html>
<head><title>Foo</title>
<body>
<p class=foo><b>Some Text</b>
<p><em>Some Other Text</em>"""
        soup = get_soup(s)

        body = soup.find('body')
        ps = body.find_all('p')
        assert ps[0].parent.name == 'body'
        assert ps[1].parent.name == 'body'
        b = soup.find('b')
        assert b.parent.name == 'p'
        em = soup.find('em')
        assert em.parent.name == 'p'

        assert soup.find('p', attrs={'class': 'foo'})

########NEW FILE########
__FILENAME__ = test_imdb
# TODO: these tests don't work outside US due imdb implementing geoip based crappy name translation
# imdb_name needs to be replaced with our own title lookup

"""
.. NOTE::

   Added `imdb_original_name` recently, so in case the title lookup translations cause problems
   switch to find_entry to use that instead!
"""

from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.plugins.attrib import attr


class TestImdb(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              # tests search
              - {title: 'Spirited Away'}
              # tests direct url
              - {title: 'Princess Mononoke', imdb_url: 'http://www.imdb.com/title/tt0119698/'}
              # generic test material, some tricky ones here :)
              - {title: 'Taken[2008]DvDrip[Eng]-FOO'}
              # test short title, with repack and without year
              - {title: 'Up.REPACK.720p.Bluray.x264-FlexGet'}
            imdb:
              min_votes: 20

          year:
            mock:
              - {title: 'Princess Mononoke', imdb_url: 'http://www.imdb.com/title/tt0119698/'}
              - {title: 'Taken[2008]DvDrip[Eng]-FOO', imdb_url: 'http://www.imdb.com/title/tt0936501/'}
              - {title: 'Inglourious Basterds 2009', imdb_url: 'http://www.imdb.com/title/tt0361748/'}
            imdb:
              min_year: 2003
              max_year: 2008

          actor:
            mock:
              - {title: 'The Matrix', imdb_url: 'http://www.imdb.com/title/tt0133093/'}
              - {title: 'The Terminator', imdb_url: 'http://www.imdb.com/title/tt0088247/'}
            imdb:
              accept_actors:
                - nm0000206
              reject_actors:
                - nm0000216

          director:
            mock:
              - {title: 'The Matrix', imdb_url: 'http://www.imdb.com/title/tt0133093/'}
              - {title: 'The Terminator', imdb_url: 'http://www.imdb.com/title/tt0088247/'}
            imdb:
              accept_directors:
                - nm0905152
                - nm0905154
              reject_directors:
                - nm0000116

          score:
            mock:
              - {title: 'The Matrix', imdb_url: 'http://www.imdb.com/title/tt0133093/'}
              - {title: 'Battlefield Earth', imdb_url: 'http://www.imdb.com/title/tt0185183/'}
            imdb:
              min_score: 5.0

          genre:
            mock:
              - {title: 'The Matrix', imdb_url: 'http://www.imdb.com/title/tt0133093/'}
              - {title: 'Terms of Endearment', imdb_url: 'http://www.imdb.com/title/tt0086425/'}
            imdb:
              reject_genres:
                - drama
            # No accept_genres?!

          language:
            mock:
              - {title: 'The Matrix', imdb_url: 'http://www.imdb.com/title/tt0133093/'}
              - {title: '22 Bullets', imdb_url: 'http://www.imdb.com/title/tt1167638/'}
              - {title: 'Crank', imdb_url: 'http://www.imdb.com/title/tt0479884/'}
              - {title: 'The Damned United', imdb_url: 'http://www.imdb.com/title/tt1226271/'}
              - {title: 'Rockstar', imdb_url: 'http://www.imdb.com/title/tt1839596/'}
              - {title: 'Breakaway', imdb_url: 'http://www.imdb.com/title/tt1736552/'}
            imdb:
              accept_languages:
                - english
              reject_languages:
                - french
          mpaa:
            mock:
            - title: Saw 2004
              imdb_url: http://www.imdb.com/title/tt0387564/
            - title: Aladdin 1992
              imdb_url: http://www.imdb.com/title/tt0103639/
            imdb:
              reject_mpaa_ratings:
              - R
    """

    @attr(online=True)
    def test_lookup(self):
        """IMDB: Test Lookup (ONLINE)"""
        self.execute_task('test')
        assert self.task.find_entry(imdb_name='Spirited Away'), \
            'Failed IMDB lookup (search Spirited Away)'
        assert self.task.find_entry(imdb_name='Princess Mononoke'), \
            'Failed imdb lookup (direct)'
        assert self.task.find_entry(imdb_name='Taken', imdb_url='http://www.imdb.com/title/tt0936501/'), \
            'Failed to pick correct Taken from search results'
        assert self.task.find_entry(imdb_url='http://www.imdb.com/title/tt1049413/'), \
            'Failed to lookup Up.REPACK.720p.Bluray.x264-FlexGet'

    @attr(online=True)
    def test_year(self):
        self.execute_task('year')
        assert self.task.find_entry('accepted', imdb_name='Taken'), \
            'Taken should\'ve been accepted'
        # mononoke should not be accepted or rejected
        assert not self.task.find_entry('accepted', imdb_name='Mononoke-hime'), \
            'Mononoke-hime should not have been accepted'
        assert not self.task.find_entry('rejected', imdb_name='Mononoke-hime'), \
            'Mononoke-hime should not have been rejected'
        assert not self.task.find_entry('accepted', imdb_name='Inglourious Basterds 2009'), \
            'Inglourious Basterds should not have been accepted'

    @attr(online=True)
    def test_actors(self):
        self.execute_task('actor')

        # check that actors have been parsed properly
        matrix = self.task.find_entry(imdb_name='The Matrix')
        assert matrix, 'entry for matrix missing'

        assert 'nm0000206' in matrix['imdb_actors'], \
            'Keanu Reeves is missing'
        assert matrix['imdb_actors']['nm0000206'] == 'Keanu Reeves', \
            'Keanu Reeves name is missing'

        assert self.task.find_entry('accepted', imdb_name='The Matrix'), \
            'The Matrix should\'ve been accepted'
        assert not self.task.find_entry('rejected', imdb_name='The Terminator'), \
            'The The Terminator have been rejected'

    @attr(online=True)
    def test_directors(self):
        self.execute_task('director')
        # check that directors have been parsed properly
        matrix = self.task.find_entry(imdb_name='The Matrix')
        assert 'nm0905154' in matrix['imdb_directors'], \
            'Lana Wachowski is missing'
        assert matrix['imdb_directors']['nm0905154'] == 'Lana Wachowski', \
            'Lana Wachowski name is missing'

        assert self.task.find_entry('accepted', imdb_name='The Matrix'), \
            'The Matrix should\'ve been accepted'
        assert not self.task.find_entry('rejected', imdb_name='The Terminator'), \
            'The The Terminator have been rejected'

    @attr(online=True)
    def test_score(self):
        self.execute_task('score')
        assert self.task.find_entry(imdb_name='The Matrix'), 'The Matrix not found'
        matrix = float(self.task.find_entry(imdb_name='The Matrix')['imdb_score'])
        # Currently The Matrix has an 8.7, check a range in case it changes
        assert 8.6 < matrix < 8.8, \
            'The Matrix should have score 8.7 not %s. (Did the rating change?)' % matrix
        assert int(self.task.find_entry(imdb_name='The Matrix')['imdb_votes']) > 450000, \
            'The Matrix should have more than 450000 votes'
        bfe = float(self.task.find_entry(title='Battlefield Earth')['imdb_score'])
        # Currently Battlefield Earth has an 2.4, check a range in case it changes
        assert 2.3 <= bfe <= 2.5, \
            'Battlefield Earth should have score 2.3 not %s. (Did the rating change?)' % bfe
        assert self.task.find_entry('accepted', imdb_name='The Matrix'), \
            'The Matrix should\'ve been accepted'
        assert not self.task.find_entry('accepted', title='Battlefield Earth'), \
            'Battlefield Earth shouldn\'t have been accepted'

    @attr(online=True)
    def test_genre(self):
        self.execute_task('genre')
        matrix = (self.task.find_entry(imdb_name='The Matrix')['imdb_genres'])
        assert matrix == ['action', 'sci-fi'], \
            'Could not find genres for The Matrix'
        toe = (self.task.find_entry(imdb_name='Terms of Endearment')['imdb_genres'])
        assert toe == ['comedy', 'drama'], \
            'Could not find genres for Terms of Endearment'
        assert self.task.find_entry('accepted', imdb_name='The Matrix'), \
            'The Matrix should\'ve been accepted'
        assert not self.task.find_entry('rejected', title='Terms of Endearment'), \
            'Terms of Endearment should have been rejected'

    @attr(online=True)
    def test_language(self):
        self.execute_task('language')
        matrix = self.task.find_entry(imdb_name='The Matrix')['imdb_languages']
        assert matrix == ['english'], 'Could not find languages for The Matrix'
        # IMDB may return imdb_name of "L'immortel" for 22 Bullets
        bullets = self.task.find_entry(imdb_original_name='L\'immortel')['imdb_languages']
        assert bullets[0] == 'french', 'Could not find languages for 22 Bullets'
        for movie in ['The Matrix', 'Crank', 'The Damned United']:
            assert self.task.find_entry('accepted', imdb_name=movie), \
                '%s should\'ve been accepted' % movie
        assert not self.task.find_entry('rejected', title='22 Bullets'), \
            '22 Bullets should have been rejected'
        # This test no longer valid (01/31/13) with IMDB language change
        # rockstar = self.task.find_entry(imdb_name='Rockstar')['imdb_languages']
        # # http://flexget.com/ticket/1399
        # assert rockstar == ['hindi'], 'Did not find only primary language'
        breakaway = self.task.find_entry(imdb_name='Breakaway')['imdb_languages']
        # switched to panjabi since that's what I got ...
        assert breakaway == ['panjabi', 'english'], \
            'Languages were not returned in order of prominence, got %s' % (', '.join(breakaway))

    @attr(online=True)
    def test_mpaa(self):
        self.execute_task('mpaa')
        aladdin = self.task.find_entry(imdb_name='Aladdin')
        assert aladdin['imdb_mpaa_rating'] == 'G', ('Didn\'t get right rating for Aladdin. Should be G got %s' %
                                                    aladdin['imdb_mpaa_rating'])
        assert aladdin.accepted, 'Non R rated movie should have been accepted'
        saw = self.task.find_entry(imdb_name='Saw')
        assert saw['imdb_mpaa_rating'] == 'R', 'Didn\'t get right rating for Saw'
        assert not saw.accepted, 'R rated movie should not have been accepted'


class TestImdbRequired(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Taken[2008]DvDrip[Eng]-FOO', imdb_url: 'http://www.imdb.com/title/tt0936501/'}
              - {title: 'ASDFASDFASDF'}
            imdb_required: yes
    """

    @attr(online=True)
    def test_imdb_required(self):
        self.execute_task('test')
        assert not self.task.find_entry('rejected', title='Taken[2008]DvDrip[Eng]-FOO'), \
            'Taken should NOT have been rejected'
        assert self.task.find_entry('rejected', title='ASDFASDFASDF'), \
            'ASDFASDFASDF should have been rejected'


class TestImdbLookup(FlexGetBase):

    __yaml__ = """
        tasks:
          invalid url:
            mock:
              - {title: 'Taken', imdb_url: 'imdb.com/title/tt0936501/'}
            imdb_lookup: yes
    """

    @attr(online=True)
    def test_invalid_url(self):
        self.execute_task('invalid url')
        # check that these were created
        assert self.task.entries[0]['imdb_score'], 'didn\'t get score'
        assert self.task.entries[0]['imdb_year'], 'didn\'t get year'
        assert self.task.entries[0]['imdb_plot_outline'], 'didn\'t get plot'

########NEW FILE########
__FILENAME__ = test_imdb_parser
from __future__ import unicode_literals, division, absolute_import

from nose.plugins.attrib import attr

from flexget.utils.imdb import ImdbParser


class TestImdbParser(object):
    @attr(online=True)
    def test_parsed_data(self):
        parser = ImdbParser()
        parser.parse('tt0114814')
        assert parser.actors == {
            'nm0000592': 'Pete Postlethwaite',
            'nm0261452': 'Christine Estabrook',
            'nm0000751': 'Suzy Amis',
            'nm0000286': 'Stephen Baldwin',
            'nm0000445': 'Dan Hedaya',
            'nm0800339': 'Phillipe Simon',
            'nm0002064': 'Giancarlo Esposito',
            'nm0001590': 'Chazz Palminteri',
            'nm0000321': 'Gabriel Byrne',
            'nm0790436': 'Jack Shearer',
            'nm0000228': 'Kevin Spacey',
            'nm0001629': 'Kevin Pollak',
            'nm0107808': 'Carl Bressler',
            'nm0001125': 'Benicio Del Toro',
            'nm0000860': 'Paul Bartel'
        }, 'Actors not parsed correctly'
        assert parser.directors == {'nm0001741': 'Bryan Singer'}, 'Directors not parsed correctly'
        assert parser.genres == [u'crime', u'drama', u'thriller'], 'Genres not parsed correctly'
        assert parser.imdb_id == 'tt0114814', 'ID not parsed correctly'
        assert parser.languages == ['english', 'hungarian', 'spanish', 'french'], 'Languages not parsed correctly'
        assert parser.mpaa_rating == 'R', 'Rating not parsed correctly'
        assert parser.name == 'The Usual Suspects', 'Name not parsed correctly'
        assert (parser.photo ==
                'http://ia.media-imdb.com/images/M/MV5BMzI1MjI5MDQyOV5BMl5BanBnXkFtZTcwNzE4Mjg3NA@@._V1_SX214_AL_.jpg'
        ), 'Photo not parsed correctly'
        assert parser.plot_outline == (
            'Following a truck hijack in New York, five conmen are arrested and brought together for questioning. '
            'As none of them is guilty, they plan a revenge operation against the police. The operation goes well, '
            'but then the influence of a legendary mastermind criminal called Keyser S\xf6ze is felt. It becomes '
            'clear that each one of them has wronged S\xf6ze at some point and must pay back now. The payback job '
            'leaves 27 men dead in a boat explosion, but the real question arises now: Who actually is Keyser S\xf6ze?'
        ), 'Plot outline not parsed correctly'
        assert 8.0 < parser.score < 9.0, 'Score not parsed correctly'
        assert parser.url == 'http://www.imdb.com/title/tt0114814/', 'URL not parsed correctly'
        assert 400000 < parser.votes < 1000000, 'Votes not parsed correctly'
        assert parser.year == 1995, 'Year not parsed correctly'

    @attr(online=True)
    def test_no_plot(self):
        # Make sure parser doesn't crash for movies with no plot
        parser = ImdbParser()
        parser.parse('tt0245062')
        assert parser.name == 'The Magnet'
        # There is no plot
        assert not parser.plot_outline

########NEW FILE########
__FILENAME__ = test_inputs
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestInputs(FlexGetBase):

    __yaml__ = """
        tasks:
          test_inputs:
            inputs:
              - mock:
                  - {title: 'title1', url: 'http://url1'}
              - mock:
                  - {title: 'title2', url: 'http://url2'}
          test_no_dupes:
            inputs:
              - mock:
                  - {title: 'title1a', url: 'http://url1'}
                  - {title: 'title2', url: 'http://url2a'}
              - mock:
                  - {title: 'title1b', url: 'http://url1'}
                  - {title: 'title1c', url: 'http://other', urls: ['http://url1']}
                  - {title: 'title2', url: 'http://url2b'}
          test_no_url:
            inputs:
              - mock:
                  - title: title1
              - mock:
                  - title: title2
    """

    def test_inputs(self):
        self.execute_task('test_inputs')
        assert len(self.task.entries) == 2, 'Should have created 2 entries'

    def test_no_dupes(self):
        self.execute_task('test_no_dupes')
        assert len(self.task.entries) == 2, 'Should only have created 2 entries'
        assert self.task.find_entry(title='title1a'), 'title1a should be in entries'
        assert self.task.find_entry(title='title2'), 'title2 should be in entries'

    """def test_no_url(self):
        # Oops, this test doesn't do anything, as the mock plugin adds a fake url to entries
        # TODO: fix this
        self.execute_task('test_no_url')
        assert len(self.task.entries) == 2, 'Should have created 2 entries'"""

########NEW FILE########
__FILENAME__ = test_input_sites
from __future__ import unicode_literals, division, absolute_import

from nose.plugins.attrib import attr

from tests import FlexGetBase


class TestInputSites(FlexGetBase):

    __yaml__ = """
        tasks:
          test_rlslog:
            rlslog: http://www.rlslog.net/category/movies/dvdrip/
          test_sceper:
            sceper: http://sceper.ws/category/movies/movies-dvd-rip
          test_apple_trailers:
            apple_trailers:
              quality: 480p
              genres: ['Action and Adventure']
          test_apple_trailers_simple:
            apple_trailers: 720p

    """

    @attr(online=True)
    def test_rlslog(self):
        self.execute_task('test_rlslog')
        assert self.task.entries, 'no entries created / site may be down'

    @attr(online=True)
    def test_sceper(self):
        self.execute_task('test_sceper')
        assert self.task.entries, 'no entries created / site may be down'

    @attr(online=True)
    def test_apple_trailers(self):
        self.execute_task('test_apple_trailers')
        assert self.task.entries, 'no entries created / site may be down'

    @attr(online=True)
    def test_apple_trailers_simple(self):
        self.execute_task('test_apple_trailers_simple')
        assert self.task.entries, 'no entries created / site may be down'

########NEW FILE########
__FILENAME__ = test_lazy_fields
from __future__ import unicode_literals, division, absolute_import
from flexget.entry import Entry


class TestLazyFields(object):

    def test_lazy_queue(self):
        """Tests behavior when multiple plugins register lazy lookups for the same field"""

        def lazy_a(entry, field):
            if field == 'a_fail':
                entry.unregister_lazy_fields(['ab_field', 'a_field', 'a_fail'], lazy_a)
                return None
            for f in ['a_field', 'ab_field']:
                entry[f] = 'a'
            return entry[field]

        def lazy_b(entry, field):
            for f in ['b_field', 'ab_field', 'a_fail']:
                entry[f] = 'b'
            return entry[field]

        def setup_entry():
            entry = Entry()
            entry.register_lazy_fields(['ab_field', 'a_field', 'a_fail'], lazy_a)
            entry.register_lazy_fields(['ab_field', 'b_field', 'a_fail'], lazy_b)
            return entry

        entry = setup_entry()
        assert entry['b_field'] == 'b', 'Lazy lookup failed'
        assert entry['ab_field'] == 'b', 'ab_field should be `b` when lazy_b is run first'
        # Now cause 'a' lookup to occur
        assert entry['a_field'] == 'a'
        # TODO: What is the desired result when a lookup has information that is already populated?
        #assert entry['ab_field'] == 'b'

        # Test fallback when first lookup fails
        entry = setup_entry()
        assert entry['a_fail'] == 'b', 'Lookup should have fallen back to b'
        assert 'a_field' not in entry, 'a_field should no longer be in entry after failed lookup'
        assert entry['ab_field'] == 'b', 'ab_field should be `b`'

########NEW FILE########
__FILENAME__ = test_limit_new
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestLimitNew(FlexGetBase):
    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Item 1'}
              - {title: 'Item 2'}
              - {title: 'Item 3'}
              - {title: 'Item 4'}
            accept_all: yes
            limit_new: 1
    """

    def test_limit_new(self):
        self.execute_task('test')
        assert len(self.task.entries) == 1, 'accepted too many'
        assert self.task.find_entry('accepted', title='Item 1'), 'accepted wrong item'
        self.execute_task('test')
        assert len(self.task.entries) == 1, 'accepted too many on second run'
        assert self.task.find_entry('accepted', title='Item 2'), 'accepted wrong item on second run'

########NEW FILE########
__FILENAME__ = test_manipulate
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestManipulate(FlexGetBase):

    __yaml__ = """
        tasks:

          test_1:
            mock:
              - {title: 'abc FOO'}
            manipulate:
              - title:
                  replace:
                    regexp: FOO
                    format: BAR

          test_2:
            mock:
              - {title: '1234 abc'}
            manipulate:
              - title:
                  extract: \d+\s*(.*)

          test_multiple_edits:
            mock:
              - {title: 'abc def'}
            manipulate:
              - title:
                  replace:
                    regexp: abc
                    format: "123"
              - title:
                  extract: \d+\s+(.*)

          test_phase:
            mock:
              - {title: '1234 abc'}
            manipulate:
              - title:
                  phase: metainfo
                  extract: \d+\s*(.*)

          test_remove:
            mock:
              - {title: 'abc', description: 'def'}
            manipulate:
              - description: { remove: yes }
    """

    def test_replace(self):
        self.execute_task('test_1')
        assert self.task.find_entry('entries', title='abc BAR'), 'replace failed'

    def test_extract(self):
        self.execute_task('test_2')
        assert self.task.find_entry('entries', title='abc'), 'extract failed'

    def test_multiple_edits(self):
        self.execute_task('test_multiple_edits')
        assert self.task.find_entry('entries', title='def'), 'multiple edits on 1 field failed'

    def test_phase(self):
        self.execute_task('test_phase')
        assert self.task.find_entry('entries', title='abc'), 'extract failed at metainfo phase'

    def test_remove(self):
        self.execute_task('test_remove')
        assert 'description' not in self.task.find_entry('entries', title='abc'), 'remove failed'

########NEW FILE########
__FILENAME__ = test_metainfo
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestMetainfo(FlexGetBase):

    __yaml__ = """
        tasks:
          test_quality:
            mock:
              - {title: 'test quality', description: 'metainfo quality should parse quality 720p from this'}
          test_content_size:
            mock:
              - {title: 'size 10MB', description: 'metainfo content size should parse size 10.2MB from this'}
              - {title: 'size 200MB', description: 'metainfo content size should parse size 200MB from this'}
              - {title: 'size 1024MB', description: 'metainfo content size should parse size 1.0GB from this'}
    """

    def test_quality(self):
        """Metainfo: parse quality"""
        self.execute_task('test_quality')
        assert self.task.find_entry(quality='720p'), 'Quality not parsed'

    def test_content_size(self):
        """Metainfo: parse content size"""
        self.execute_task('test_content_size')
        assert self.task.find_entry(content_size=10), 'Content size 10 MB absent'
        assert self.task.find_entry(content_size=200), 'Content size 200 MB absent'
        assert self.task.find_entry(content_size=1024), 'Content size 1024 MB absent'


class TestMetainfoImdb(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Scan Test 1', description: 'title: Foo Bar Asdf\n imdb-url: http://www.imdb.com/title/tt0330793/ more text'}
              - {title: 'Scan Test 2', description: '<a href="http://imdb.com/title/tt0472198/">IMDb</a>'}
              - {title: 'Scan Test 3', description: 'nothing here'}
              - {title: 'Scan Test 4', description: 'imdb.com/title/tt66666 http://imdb.com/title/tt99999'}
    """

    def test_imdb(self):
        """Metainfo: imdb url"""
        self.execute_task('test')
        assert self.task.find_entry(imdb_url='http://www.imdb.com/title/tt0330793/'), \
            'Failed to pick url from test 1'
        assert self.task.find_entry(imdb_url='http://www.imdb.com/title/tt0472198/'), \
            'Failed to pick url from test 2'
        assert not self.task.find_entry(imdb_url='http://www.imdb.com/title/tt66666/'), \
            'Failed to ignore multiple imdb urls in test 4'
        assert not self.task.find_entry(imdb_url='http://www.imdb.com/title/tt99999/'), \
            'Failed to ignore multiple imdb urls in test 4'


class TestMetainfoQuality(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'FooBar.S01E02.720p.HDTV'}
              - {title: 'ShowB.S04E19.Name of Ep.720p.WEB-DL.DD5.1.H.264'}
              - {title: 'Good.Movie', description: '720p'}
              - {title: 'Good.Movie.hdtv', description: '720p'}
    """

    def test_quality(self):
        self.execute_task('test')
        entry = self.task.find_entry(title='FooBar.S01E02.720p.HDTV')
        assert entry, 'entry not found?'
        assert 'quality' in entry, 'failed to pick up quality'
        assert entry['quality'].name == '720p hdtv', 'picked up wrong quality %s' % entry.get('quality', None)
        entry = self.task.find_entry(title='ShowB.S04E19.Name of Ep.720p.WEB-DL.DD5.1.H.264')
        assert entry, 'entry not found?'
        assert 'quality' in entry, 'failed to pick up quality'
        assert entry['quality'].name == '720p webdl h264 dd5.1', 'picked up wrong quality %s' % entry.get('quality', None)
        # Check that quality gets picked up from description when not in title
        entry = self.task.find_entry(title='Good.Movie')
        assert 'quality' in entry, 'failed to pick up quality from description'
        assert entry['quality'].name == '720p', 'picked up wrong quality %s' % entry.get('quality', None)
        # quality in description should not override one found in title
        entry = self.task.find_entry(title='Good.Movie.hdtv')
        assert 'quality' in entry, 'failed to pick up quality'
        assert entry['quality'].name == 'hdtv', 'picked up wrong quality %s' % entry.get('quality', None)


class TestMetainfoSeries(FlexGetBase):
    __yaml__ = """
        templates:
          global:
            metainfo_series: yes
        tasks:
          test:
            mock:
              - {title: 'FlexGet.S01E02.TheName.HDTV.xvid'}
              - {title: 'some.series.S03E14.Title.Here.720p'}
              - {title: '[the.group] Some.Series.S03E15.Title.Two.720p'}
              - {title: 'HD 720p: Some series.S03E16.Title.Three'}
              - {title: 'Something.Season.2.1of4.Ep.Title.HDTV.torrent'}
              - {title: 'Show-A (US) - Episode Title S02E09 hdtv'}
              - {title: "Jack's.Show.S03E01.blah.1080p"}
          false_positives:
            mock:
              - {title: 'FlexGet.epic'}
              - {title: 'FlexGet.Apt.1'}
              - {title: 'FlexGet.aptitude'}
              - {title: 'FlexGet.Step1'}
              - {title: 'Something.1x0.Complete.Season-FlexGet'}
              - {title: 'Something.1xAll.Season.Complete-FlexGet'}
              - {title: 'Something Seasons 1 & 2 - Complete'}
              - {title: 'Something Seasons 4 Complete'}
              - {title: 'Something.S01D2.DVDR-FlexGet'}
    """

    def test_metainfo_series(self):
        """Metainfo series: name/episode"""
        # We search for series name in title case to make sure case is being normalized
        self.execute_task('test')
        assert self.task.find_entry(series_name='Flexget', series_season=1, series_episode=2, quality='hdtv xvid'), \
            'Failed to parse series info'
        assert self.task.find_entry(series_name='Some Series', series_season=3, series_episode=14, quality='720p'), \
            'Failed to parse series info'
        assert self.task.find_entry(series_name='Something', series_season=2, series_episode=1, quality='hdtv'), \
            'Failed to parse series info'
        # Test unwanted prefixes get stripped from series name
        assert self.task.find_entry(series_name='Some Series', series_season=3, series_episode=15, quality='720p'), \
            'Failed to parse series info'
        assert self.task.find_entry(series_name='Some Series', series_season=3, series_episode=16, quality='720p'), \
            'Failed to parse series info'
        # Test episode title and parentheses are stripped from series name
        assert self.task.find_entry(series_name='Show-a Us', series_season=2, series_episode=9, quality='hdtv'), \
            'Failed to parse series info'
        assert self.task.find_entry(series_name='Jack\'s Show', series_season=3, series_episode=1, quality='1080p'), \
            'Failed to parse series info'

    def test_false_positives(self):
        """Metainfo series: check for false positives"""
        self.execute_task('false_positives')
        for entry in self.task.entries:
            # None of these should be detected as series
            error = '%s sholud not be detected as a series' % entry['title']
            assert 'series_name' not in entry, error
            assert 'series_guessed' not in entry, error
            assert 'series_parser' not in entry, error

########NEW FILE########
__FILENAME__ = test_migrate
from __future__ import unicode_literals, division, absolute_import
import os
from tests import FlexGetBase


class TestMigrate(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'foobar'}
            accept_all: yes
    """

    def setup(self):
        import logging
        logging.critical('TestMigrate.setup()')
        db_filename = os.path.join(self.base_path, 'upgrade_test.sqlite')
        # in case running on windows, needs double \\
        filename = db_filename.replace('\\', '\\\\')
        self.database_uri = 'sqlite:///%s' % filename
        super(TestMigrate, self).setup()

    # This fails on windows when it tries to delete upgrade_test.sqlite
    # WindowsError: [Error 32] The process cannot access the file because it is being used by another process: 'upgrade_test.sqlite'
    #@with_filecopy('db-r1042.sqlite', 'upgrade_test.sqlite')
    def test_upgrade(self):
        # TODO: for some reason this will fail
        return

        self.execute_task('test')
        assert self.task.accepted

########NEW FILE########
__FILENAME__ = test_misc
from __future__ import unicode_literals, division, absolute_import
import os
import stat
from tests import FlexGetBase
from nose.plugins.attrib import attr
from nose.tools import raises
from flexget.entry import EntryUnicodeError, Entry


class TestDisableBuiltins(FlexGetBase):
    """
        Quick a hack, test disable functionality by checking if seen filtering (builtin) is working
    """

    __yaml__ = """
        tasks:
            test:
                mock:
                    - {title: 'dupe1', url: 'http://localhost/dupe', 'imdb_score': 5}
                    - {title: 'dupe2', url: 'http://localhost/dupe', 'imdb_score': 5}
                disable_builtins: true

            test2:
                mock:
                    - {title: 'dupe1', url: 'http://localhost/dupe', 'imdb_score': 5, description: 'http://www.imdb.com/title/tt0409459/'}
                    - {title: 'dupe2', url: 'http://localhost/dupe', 'imdb_score': 5}
                disable_builtins:
                    - seen
                    - cli_config
    """

    def test_disable_builtins(self):
        self.execute_task('test')
        assert self.task.find_entry(title='dupe1') and self.task.find_entry(title='dupe2'), 'disable_builtins is not working?'


class TestInputHtml(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            html: http://download.flexget.com/
    """

    @attr(online=True)
    def test_parsing(self):
        self.execute_task('test')
        assert self.task.entries, 'did not produce entries'


class TestPriority(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Smoke hdtv'}
            accept_all: yes
            set:
              quality: 720p
            quality: 720p
            plugin_priority:
              set: 3
              quality: 2
              accept_all: 1

          test2:
            mock:
              - {title: 'Smoke hdtv'}
            accept_all: yes
            set:
              quality: 720p
            quality: 720p
            plugin_priority:
              set: 3
              quality: 2
              accept_all: 1
    """

    def test_smoke(self):
        self.execute_task('test')
        assert self.task.accepted, 'set plugin should have changed quality before quality plugin was run'
        self.execute_task('test2')
        assert self.task.rejected, 'quality plugin should have rejected Smoke as hdtv'


class TestImmortal(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'title1', immortal: yes}
              - {title: 'title2'}
            regexp:
              reject:
                - .*
    """

    def test_immortal(self):
        self.execute_task('test')
        assert self.task.find_entry(title='title1'), 'rejected immortal entry'
        assert not self.task.find_entry(title='title2'), 'did not reject mortal'


class TestDownload(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - title: README
                url: https://github.com/Flexget/Flexget/raw/master/README.rst
                filename: flexget_test_data
            accept_all: true
            download:
              path: ~/
              fail_html: no
    """

    def __init__(self):
        self.testfile = None
        FlexGetBase.__init__(self)

    def teardown(self):
        FlexGetBase.teardown(self)
        if hasattr(self, 'testfile') and os.path.exists(self.testfile):
            os.remove(self.testfile)
        temp_dir = os.path.join(self.manager.config_base, 'temp')
        if os.path.exists(temp_dir) and os.path.isdir(temp_dir):
            os.rmdir(temp_dir)

    @attr(online=True)
    def test_download(self):
        # NOTE: what the hell is .obj and where it comes from?
        # Re: seems to come from python mimetype detection in download plugin ...
        # Re Re: Implemented in such way that extension does not matter?
        self.testfile = os.path.expanduser('~/flexget_test_data.obj')
        # A little convoluted, but you have to set the umask in order to have
        # the current value returned to you
        curr_umask = os.umask(0)
        tmp_umask = os.umask(curr_umask)
        if os.path.exists(self.testfile):
            os.remove(self.testfile)
        # executes task and downloads the file
        self.execute_task('test')
        assert self.task.entries[0]['output'], 'output missing?'
        self.testfile = self.task.entries[0]['output']
        assert os.path.exists(self.testfile), 'download file does not exists'
        testfile_stat = os.stat(self.testfile)
        modes_equal = 666 - int(oct(curr_umask)) == \
                      int(oct(stat.S_IMODE(testfile_stat.st_mode)))
        assert modes_equal, 'download file mode not honoring umask'


class TestEntryUnicodeError(object):

    @raises(EntryUnicodeError)
    def test_encoding(self):
        e = Entry('title', 'url')
        e['invalid'] = b'\x8e'


class TestFilterRequireField(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Taken[2008]DvDrip[Eng]-FOO', imdb_url: 'http://www.imdb.com/title/tt0936501/'}
              - {title: 'ASDFASDFASDF'}
            require_field: imdb_url
          test2:
            mock:
              - {title: 'Entry.S01E05.720p', series_name: 'Entry'}
              - {title: 'Entry2.is.a.Movie'}
            require_field: series_name
    """

    def test_field_required(self):
        self.execute_task('test')
        assert not self.task.find_entry('rejected', title='Taken[2008]DvDrip[Eng]-FOO'), \
            'Taken should NOT have been rejected'
        assert self.task.find_entry('rejected', title='ASDFASDFASDF'), \
            'ASDFASDFASDF should have been rejected'

        self.execute_task('test2')
        assert not self.task.find_entry('rejected', title='Entry.S01E05.720p'), \
            'Entry should NOT have been rejected'
        assert self.task.find_entry('rejected', title='Entry2.is.a.Movie'), \
            'Entry2 should have been rejected'


class TestHtmlUtils(object):

    def test_decode_html(self):
        """utils decode_html"""
        from flexget.utils.tools import decode_html
        assert decode_html('&lt;&#51;') == u'<3'
        assert decode_html('&#x2500;') == u'\u2500'

    def test_encode_html(self):
        """utils encode_html (FAILS - DISABLED)"""
        return

        # why this does not encode < ?
        from flexget.utils.tools import encode_html
        print encode_html('<3')
        assert encode_html('<3') == '&lt;3'


class TestSetPlugin(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            accept_all: yes
        tasks:
          test:
            mock:
              - {title: 'Entry 1'}
            set:
              thefield: TheValue
              otherfield: 3.0
          test_jinja:
            mock:
              - {title: 'Entry 1', series_name: 'Value'}
              - {title: 'Entry 2'}
            set:
              field: 'The {{ series_name|upper }}'
              otherfield: '{% if series_name is not defined %}no series{% endif %}'
              alu: '{{ series_name|re_search(".l.") }}'
    """

    def test_set(self):
        self.execute_task('test')
        entry = self.task.find_entry('entries', title='Entry 1')
        assert entry['thefield'] == 'TheValue'
        assert entry['otherfield'] == 3.0

    def test_jinja(self):
        self.execute_task('test_jinja')
        entry = self.task.find_entry('entries', title='Entry 1')
        assert entry['field'] == 'The VALUE'
        assert entry['otherfield'] == ''
        assert entry['alu'] == 'alu'
        entry = self.task.find_entry('entries', title='Entry 2')
        assert 'field' not in entry,\
                '`field` should not have been created when jinja rendering fails'
        assert entry['otherfield'] == 'no series'

########NEW FILE########
__FILENAME__ = test_movieparser
from __future__ import unicode_literals, division, absolute_import
from flexget.utils.titles import MovieParser


class TestMovieParser:

    def parse(self, data):
        movieparser = MovieParser()
        movieparser.data = data
        movieparser.parse()
        return movieparser

    def test_parsing(self):
        movie = self.parse('The.Matrix.1999.1080p.HDDVD.x264-FlexGet')
        assert movie.name == 'The Matrix', 'failed to parse %s (got %s)' % (movie.data, movie.name)
        assert movie.year == 1999, 'failed to parse year from %s' % movie.data

        movie = self.parse('WALL-E 720p BluRay x264-FlexGet')
        assert movie.name == 'WALL-E', 'failed to parse %s' % movie.data
        assert movie.quality.name == '720p bluray h264', 'failed to parse quality from %s' % movie.data

        movie = self.parse('The.Pianist.2002.HDDVD.1080p.DTS.x264-FlexGet')
        assert movie.name == 'The Pianist', 'failed to parse %s' % movie.data
        assert movie.year == 2002, 'failed to parse year from %s' % movie.data
        assert movie.quality.name == '1080p h264 dts', 'failed to parse quality from %s' % movie.data

        movie = self.parse("Howl's_Moving_Castle_(2004)_[720p,HDTV,x264,DTS]-FlexGet")
        assert movie.name == "Howl's Moving Castle", 'failed to parse %s' % movie.data
        assert movie.year == 2004, 'failed to parse year from %s' % movie.data
        assert movie.quality.name == '720p hdtv h264 dts', 'failed to parse quality from %s' % movie.data

        movie = self.parse('Coraline.3D.1080p.BluRay.x264-FlexGet')
        assert movie.name == 'Coraline', 'failed to parse %s' % movie.data
        assert movie.quality.name == '1080p bluray h264', 'failed to parse quality from %s' % movie.data

        movie = self.parse('Slumdog.Millionaire.DVDRip.XviD-FlexGet')
        assert movie.name == 'Slumdog Millionaire', 'failed to parse %s' % movie.data
        assert movie.quality.name == 'dvdrip xvid', 'failed to parse quality from %s' % movie.data

        movie = self.parse('TRON.Legacy.3D.2010.1080p.BluRay.Half.Over-Under.DTS.x264-FlexGet')
        assert movie.name == 'TRON Legacy', 'failed to parse %s' % movie.data

        movie = self.parse('[SomeThing]Up.2009.720p.x264-FlexGet')
        assert movie.name == 'Up', 'failed to parse %s (got %s)' % (movie.data, movie.name)
        assert movie.year == 2009, 'failed to parse year from %s' % movie.data

        movie = self.parse('[720p] A.Movie.Title.2013.otherstuff.x264')
        assert movie.name == 'A Movie Title', 'failed to parse %s (got %s)' % (movie.data, movie.name)
        assert movie.year == 2013, 'failed to parse year from %s' % movie.data
        assert movie.quality.name == '720p h264'

########NEW FILE########
__FILENAME__ = test_myepisodes
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.plugins.attrib import attr


class TestMyEpisodes(FlexGetBase):
    """Uses test account at MyEpisodes, username and password are 'flexget'"""

    __yaml__ = """
        tasks:
          test:
            mock:
              - title: the.simpsons.S10E10.hdtv
            all_series: yes
            myepisodes:
              username: flexget
              password: flexget
    """

    @attr(online=True)
    def test_myepisodes_id(self):
        """Test myepisodes (DISABLED) -- account locked?"""
        return

        self.execute_task('test')
        entry = self.task.find_entry('accepted', title='the.simpsons.S10E10.hdtv')
        assert entry, 'entry not present'
        # It's tough to verify the marking worked properly, at least check that myepisodes_id is populated
        assert entry['myepisodes_id'] == '10', 'myepisodes_id should be 10 for The Simpsons'

########NEW FILE########
__FILENAME__ = test_only_new
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestOnlyNew(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'title 1', url: 'http://localhost/title1'}
            only_new: yes
            disable_builtins: [seen] # Disable the seen plugin to make sure only_new does the filtering.
            accept_all: yes
    """

    def test_only_new(self):
        self.execute_task('test')
        # only_new will reject the entry on task_exit, make sure accept_all accepted it during filter event though
        assert self.task.find_entry('rejected', title='title 1', accepted_by='accept_all'), 'Test entry missing'
        # run again, should filter
        self.execute_task('test')
        assert self.task.find_entry('rejected', title='title 1', rejected_by='remember_rejected'), 'Seen test entry remains'

        # add another entry to the task
        self.manager.config['tasks']['test']['mock'].append({'title': 'title 2', 'url': 'http://localhost/title2'})
        # execute again
        self.execute_task('test')
        # both entries should be present as config has changed
        assert self.task.find_entry('rejected', title='title 1', accepted_by='accept_all'), 'title 1 was not found'
        assert self.task.find_entry('rejected', title='title 2', accepted_by='accept_all'), 'title 2 was not found'

        # TODO: Test that new entries are accepted. Tough to do since we can't change the task name or config..

########NEW FILE########
__FILENAME__ = test_pathscrub
from __future__ import unicode_literals, division, absolute_import

from nose.tools import assert_raises

from flexget.utils.pathscrub import pathscrub


class TestPathscrub(object):
    def test_windows_filenames(self):
        # Windows filename tests
        # 'None' indicates there should be no changes after path scrub
        win_fn = {
            'afilename': 'afilename',
            'filename/with/slash': 'filename with slash',
            'filename\\with\\backslash': 'filename with backslash',
            'afilename.': 'afilename',  # filenames can't end in dot
            'a<b>c:d"e/f\\g|h?i*j': 'a b c d e f g h i j',  # Can't contain invalid characters
            'a<<b?*?c: d': 'a b c d',  # try with some repeated bad characters
            'something.>': 'something',  # Don't leave dots at the end
            'something *': 'something',  # Don't leave spaces at the end
            'aoeu. > * . * <': 'aoeu'  # Really don't leave spaces or dots at the end
        }
        for test in win_fn:
            result = pathscrub(test, os='windows', filename=True)
            assert result == win_fn[test], '%s != %s' % (result, win_fn[test])

    def test_windows_paths(self):
        win_path = {
            'aoeu/aoeu': 'aoeu/aoeu',  # Don't strip slashes in path mode
            'aoeu\\aoeu': 'aoeu\\aoeu',  # Or backslashes
            'aoeu / aoeu ': 'aoeu/aoeu',  # Don't leave spaces at the begin or end of folder names
            'aoeu \\aoeu ': 'aoeu\\aoeu',
            'aoeu./aoeu.\\aoeu.': 'aoeu/aoeu\\aoeu'  # Or dots
        }
        for test in win_path:
            result = pathscrub(test, os='windows', filename=False)
            assert result == win_path[test], '%s != %s' % (result, win_path[test])

    def test_degenerate(self):
        # If path is reduced to nothing, make sure it complains
        assert_raises(ValueError, pathscrub, '<<<<:>>>>', os='windows', filename=True)

    def test_space_around(self):
        # We don't want folder or file names to end or start with spaces on any platform
        space_paths = {
            ' / aoeu /aoeu ': '/aoeu/aoeu',
            '/   a/a   ': '/a/a',
            '/a  /': '/a/'
        }
        for platform in ['windows', 'linux', 'mac']:
            for test in space_paths:
                result = pathscrub(test, filename=False)
                assert result == space_paths[test], '%s != %s (%s)' % (result, space_paths[test], platform)

        # Windows only should also use backslashes as dir separators
        test = ['c:\\ aoeu \\aoeu /aoeu ', 'c:\\aoeu\\aoeu/aoeu']
        result = pathscrub(test[0], os='windows', filename=False)
        assert result == test[1], '%s != %s' % (result, test[1])


########NEW FILE########
__FILENAME__ = test_pluginapi
from __future__ import unicode_literals, division, absolute_import
import os
import glob

from nose.tools import raises

from tests import FlexGetBase
from flexget import plugin, plugins
from flexget.event import event


class TestPluginApi(object):
    """
    Contains plugin api related tests
    """

    @raises(plugin.DependencyError)
    def test_unknown_plugin(self):
        plugin.get_plugin_by_name('nonexisting_plugin')

    def test_no_dupes(self):
        plugin.load_plugins()

        assert plugin.PluginInfo.dupe_counter == 0, "Duplicate plugin names, see log"

    def test_load(self):

        plugin.load_plugins()
        plugin_path = os.path.dirname(plugins.__file__)
        plugin_modules = set(os.path.basename(i)
            for k in ("/*.py", "/*/*.py")
            for i in glob.glob(plugin_path + k))
        assert len(plugin_modules) >= 10, "Less than 10 plugin modules looks fishy"
        # Hmm, this test isn't good, because we have plugin modules that don't register a class (like cli ones)
        # and one module can load multiple plugins TODO: Maybe consider some replacement
        # assert len(plugin.plugins) >= len(plugin_modules) - 1, "Less plugins than plugin modules"

    def test_register_by_class(self):

        class TestPlugin(object):
            pass

        class Oneword(object):
            pass

        class TestHTML(object):
            pass

        assert 'test_plugin' not in plugin.plugins

        @event('plugin.register')
        def rp():
            plugin.register(TestPlugin, api_ver=2)
            plugin.register(Oneword, api_ver=2)
            plugin.register(TestHTML, api_ver=2)

        # Call load_plugins again to register our new plugins
        plugin.load_plugins()
        assert 'test_plugin' in plugin.plugins
        assert 'oneword' in plugin.plugins
        assert 'test_html' in plugin.plugins


class TestExternalPluginLoading(FlexGetBase):
    __yaml__ = """
        tasks:
          ext_plugin:
            external_plugin: yes
    """

    def setup(self):
        os.environ['FLEXGET_PLUGIN_PATH'] = os.path.join(self.base_path, 'external_plugins')
        plugin.load_plugins()
        super(TestExternalPluginLoading, self).setup()

    def teardown(self):
        del os.environ['FLEXGET_PLUGIN_PATH']
        super(TestExternalPluginLoading, self).teardown()

    def test_external_plugin_loading(self):
        self.execute_task('ext_plugin')
        assert self.task.find_entry(title='test entry'), 'External plugin did not create entry'

########NEW FILE########
__FILENAME__ = test_proper_movies
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestProperMovies(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            seen_movies: strict
            accept_all: yes
            proper_movies: yes

        tasks:
          test1:
            mock:
              - {title: 'Movie.Name.2011.720p-FlexGet', imdb_id: 'tt12345678'}

          test2:
            mock:
              - {title: 'Movie.Name.2011.720p-FooBar', imdb_id: 'tt12345678'}

          test3:
            mock:
              - {title: 'Movie.Name.2011.PROPER.DVDRip-AsdfAsdf', imdb_id: 'tt12345678'}


          test4:
            mock:
              - {title: 'Movie.Name.2011.PROPER.720p-FlexGet', imdb_id: 'tt12345678'}
    """

    def test_proper_movies(self):
        # first occurence
        self.execute_task('test1')
        assert self.task.find_entry('accepted', title='Movie.Name.2011.720p-FlexGet')

        # duplicate movie
        self.execute_task('test2')
        assert self.task.find_entry('rejected', title='Movie.Name.2011.720p-FooBar')

        # proper with wrong quality
        self.execute_task('test3')
        assert self.task.find_entry('rejected', title='Movie.Name.2011.PROPER.DVDRip-AsdfAsdf')

        # proper version of same quality
        self.execute_task('test4')
        assert self.task.find_entry('accepted', title='Movie.Name.2011.PROPER.720p-FlexGet')

########NEW FILE########
__FILENAME__ = test_qualities
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from flexget.utils.qualities import Quality


class TestQualityModule(object):

    def test_get(self):
        assert not Quality(), 'unknown quality is not false'
        assert Quality('foobar') == Quality(), 'unknown not returned'

    def test_common_name(self):
        for test_val in ('720p', '1280x720'):
            got_val = Quality(test_val).name
            assert got_val == '720p', got_val


class TestQualityParser(object):

    def test_qualities(self):
        items = [('Test.File 1080p.web-dl', '1080p webdl'),
                 ('Test.File.web-dl.1080p', '1080p webdl'),
                 ('Test.File.WebHD.720p', '720p webdl'),
                 ('Test.File.720p.bluray', '720p bluray'),
                 ('Test.File.1080p.bluray', '1080p bluray'),
                 ('Test.File.1080p.cam', '1080p cam'),
                 ('A Movie 2011 TS 576P XviD-DTRG', '576p ts xvid'),

                 ('Test.File.720p.bluray.r5', '720p r5'),
                 ('Test.File.1080p.bluray.rc', '1080p r5'),

                 # 10bit
                 ('Test.File.480p.10bit', '480p 10bit'),
                 ('Test.File.720p.10bit', '720p 10bit'),
                 ('Test.File.720p.bluray.10bit', '720p bluray 10bit'),
                 ('Test.File.1080p.10bit', '1080p 10bit'),
                 ('Test.File.1080p.bluray.10bit', '1080p bluray 10bit'),

                 ('Test.File.720p.webdl', '720p webdl'),
                 ('Test.File.1280x720_web dl', '720p webdl'),
                 ('Test.File.720p.h264.web.dl', '720p webdl h264'),
                 ('Test.File.1080p.web.x264', '1080p webdl h264'),
                 ('Test.File.web-dl', 'webdl'),
                 ('Test.File.720P', '720p'),
                 ('Test.File.1920x1080', '1080p'),
                 ('Test.File.1080i', '1080i'),
                 ('Test File blurayrip', 'bluray'),
                 ('Test.File.br-rip', 'bluray'),
                 ('Test.File.720px', '720p'),

                 ('Test.File.dvd.rip', 'dvdrip'),
                 ('Test.File.dvd.rip.r5', 'r5'),

                 ('Test.File.[576p][00112233].mkv', '576p'),

                 ('Test.TS.FooBar', 'ts'),

                 ('Test.File.360p.avi', '360p'),
                 ('Test.File.[360p].mkv', '360p'),
                 ('Test.File.368.avi', '368p'),
                 ('Test.File.720p.hdtv.avi', '720p hdtv'),
                 ('Test.File.1080p.hdtv.avi', '1080p hdtv'),
                 ('Test.File.720p.preair.avi', '720p preair'),
                 ('Test.File.ts.dvdrip.avi', 'ts'),
                 ('Test.File.HDTS.blah', 'ts'),
                 ('Test.File.HDCAM.bluray.lie', 'cam'),

                 # Test qualities as part of words. #1593
                 ('Tsar.File.720p', '720p'),
                 ('Camera.1080p', '1080p'),

                 # Some audio formats
                 ('Test.File.DTSHDMA', 'dtshd'),
                 ('Test.File.DTSHD.MA', 'dtshd'),
                 ('Test.File.DTS.HDMA', 'dtshd'),
                 ('Test.File.dts.hd.ma', 'dtshd'),
                 ('Test.File.DTS.HD', 'dtshd'),
                 ('Test.File.DTSHD', 'dtshd'),
                 ('Test.File.DTS', 'dts'),
                 ('Test.File.truehd', 'truehd')]

        for item in items:
            quality = Quality(item[0]).name
            assert quality == item[1], '`%s` quality should be `%s` not `%s`' % (item[0], item[1], quality)


class TestFilterQuality(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            mock:
              - {title: 'Smoke.1280x720'}
              - {title: 'Smoke.HDTV'}
              - {title: 'Smoke.cam'}
              - {title: 'Smoke.HR'}
            accept_all: yes
        tasks:
          qual:
            quality:
              - hdtv
              - 720p
          min:
            quality: HR+
          max:
            quality: "<=cam <HR"
          min_max:
            quality: HR-720i
    """

    def test_quality(self):
        self.execute_task('qual')
        entry = self.task.find_entry('rejected', title='Smoke.cam')
        assert entry, 'Smoke.cam should have been rejected'

        entry = self.task.find_entry(title='Smoke.1280x720')
        assert entry, 'entry not found?'
        assert entry in self.task.accepted, '720p should be accepted'
        assert len(self.task.rejected) == 2, 'wrong number of entries rejected'
        assert len(self.task.accepted) == 2, 'wrong number of entries accepted'

    def test_min(self):
        self.execute_task('min')
        entry = self.task.find_entry('rejected', title='Smoke.HDTV')
        assert entry, 'Smoke.HDTV should have been rejected'

        entry = self.task.find_entry(title='Smoke.1280x720')
        assert entry, 'entry not found?'
        assert entry in self.task.accepted, '720p should be accepted'
        assert len(self.task.rejected) == 2, 'wrong number of entries rejected'
        assert len(self.task.accepted) == 2, 'wrong number of entries accepted'

    def test_max(self):
        self.execute_task('max')
        entry = self.task.find_entry('rejected', title='Smoke.1280x720')
        assert entry, 'Smoke.1280x720 should have been rejected'

        entry = self.task.find_entry(title='Smoke.cam')
        assert entry, 'entry not found?'
        assert entry in self.task.accepted, 'cam should be accepted'
        assert len(self.task.rejected) == 3, 'wrong number of entries rejected'
        assert len(self.task.accepted) == 1, 'wrong number of entries accepted'

    def test_min_max(self):
        self.execute_task('min_max')
        entry = self.task.find_entry('rejected', title='Smoke.1280x720')
        assert entry, 'Smoke.1280x720 should have been rejected'

        entry = self.task.find_entry(title='Smoke.HR')
        assert entry, 'entry not found?'
        assert entry in self.task.accepted, 'HR should be accepted'
        assert len(self.task.rejected) == 3, 'wrong number of entries rejected'
        assert len(self.task.accepted) == 1, 'wrong number of entries accepted'

########NEW FILE########
__FILENAME__ = test_regexp
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestRegexp(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            mock:
              - {title: 'regexp1', 'imdb_score': 5}
              - {title: 'regexp2', 'bool_attr': true}
              - {title: 'regexp3', 'imdb_score': 5}
              - {title: 'regexp4', 'imdb_score': 5}
              - {title: 'regexp5', 'imdb_score': 5}
              - {title: 'regexp6', 'imdb_score': 5}
              - {title: 'regexp7', 'imdb_score': 5}
              - {title: 'regexp8', 'imdb_score': 5}
              - {title: 'regexp9', 'imdb_score': 5}
              - {title: 'regular', otherfield: 'genre1', genre: ['genre2', 'genre3']}
              - {title: 'expression', genre: ['genre1', 'genre2']}
            seen: false

        tasks:
          # test accepting, setting custom path (both ways), test not (secondary regexp)
          test_accept:
            regexp:
              accept:
                - regexp1
                - regexp2: '~'
                - regexp3:
                    path: '~'
                - regexp4:
                    not:
                      - exp4
                - regexp5:
                    not: exp7

          # test rejecting
          test_reject:
            regexp:
              reject:
                - regexp1

          # test rest
          test_rest:
            regexp:
              accept:
                - regexp1
              rest: reject


          # test excluding
          test_excluding:
            regexp:
              accept_excluding:
                - regexp1

          # test from
          test_from:
            regexp:
              accept:
                - localhost:
                    from:
                      - title

          test_multiple_excluding:
            regexp:
              reject_excluding:
                - reg
                - exp
                - 5
              rest: accept

          # test complicated
          test_complicated:
            regexp:
              accept:
                - regular
                - regexp9
              accept_excluding:
                - reg
                - exp5
              rest: reject

          test_match_in_list:
            regexp:
              # Also tests global from option
              from: genre
              accept:
                - genre1
                - genre2:
                    not: genre3
    """

    def test_accept(self):
        self.execute_task('test_accept')
        assert self.task.find_entry('accepted', title='regexp1'), 'regexp1 should have been accepted'
        assert self.task.find_entry('accepted', title='regexp2'), 'regexp2 should have been accepted'
        assert self.task.find_entry('accepted', title='regexp3'), 'regexp3 should have been accepted'
        assert self.task.find_entry('entries', title='regexp4') not in self.task.accepted, 'regexp4 should have been left'
        assert self.task.find_entry('accepted', title='regexp2', path='~'), 'regexp2 should have been accepter with custom path'
        assert self.task.find_entry('accepted', title='regexp3', path='~'), 'regexp3 should have been accepter with custom path'
        assert self.task.find_entry('accepted', title='regexp5'), 'regexp5 should have been accepted'

    def test_reject(self):
        self.execute_task('test_reject')
        assert self.task.find_entry('rejected', title='regexp1'), 'regexp1 should have been rejected'

    def test_rest(self):
        self.execute_task('test_rest')
        assert self.task.find_entry('accepted', title='regexp1'), 'regexp1 should have been accepted'
        assert self.task.find_entry('rejected', title='regexp3'), 'regexp3 should have been rejected'

    def test_excluding(self):
        self.execute_task('test_excluding')
        assert not self.task.find_entry('accepted', title='regexp1'), 'regexp1 should not have been accepted'
        assert self.task.find_entry('accepted', title='regexp2'), 'regexp2 should have been accepted'
        assert self.task.find_entry('accepted', title='regexp3'), 'regexp3 should have been accepted'

    def test_from(self):
        self.execute_task('test_from')
        assert not self.task.accepted, 'should not have accepted anything'

    def test_multiple_excluding(self):
        self.execute_task('test_multiple_excluding')
        assert self.task.find_entry('rejected', title='regexp2'), '\'regexp2\' should have been rejected'
        assert self.task.find_entry('rejected', title='regexp7'), '\'regexp7\' should have been rejected'
        assert self.task.find_entry('accepted', title='regexp5'), '\'regexp5\' should have been accepted'

    def test_multiple_excluding(self):
        self.execute_task('test_complicated')
        assert self.task.find_entry('accepted', title='regular'), '\'regular\' should have been accepted'
        assert self.task.find_entry('accepted', title='expression'), '\'expression\' should have been accepted'
        assert self.task.find_entry('accepted', title='regexp9'), '\'regexp9\' should have been accepted'
        assert self.task.find_entry('rejected', title='regexp5'), '\'regexp5\' should have been rejected'

    def test_match_in_list(self):
        self.execute_task('test_match_in_list')
        assert self.task.find_entry('accepted', title='expression'), '\'expression\' should have been accepted'
        assert self.task.find_entry('entries', title='regular') not in self.task.accepted, '\'regular\' should not have been accepted'

########NEW FILE########
__FILENAME__ = test_remember_rejected
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestRememberRejected(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'title 1', url: 'http://localhost/title1'}
    """

    def test_remember_rejected(self):
        self.execute_task('test')
        entry = self.task.find_entry(title='title 1')
        entry.reject(remember=True)
        self.execute_task('test')
        assert self.task.find_entry('rejected', title='title 1', rejected_by='remember_rejected'),\
            'remember_rejected should have rejected'

########NEW FILE########
__FILENAME__ = test_rottentomatoes
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.plugins.attrib import attr


class TestRottenTomatoesLookup(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              # tests search
              - {title: 'Toy Story'}
              - {title: 'The Matrix'}
              - {title: 'Star Wars: Episode I - The Phantom Menace (3D)'}
              # tests direct id
              - {title: '[Group] Taken 720p', rt_id: 770680780}
              # tests title + year
              - {title: 'Rush.Hour[1998]1080p[Eng]-FOO'}
              # test short title, with repack and without year
              - {title: 'Up.REPACK.720p.Bluray.x264-FlexGet'}
            rottentomatoes_lookup: yes
    """

    @attr(online=True)
    def test_rottentomatoes_lookup(self):
        self.execute_task('test')
        # check that these were created
        assert self.task.find_entry(rt_name='Toy Story', rt_year=1995, rt_id=9559, imdb_id='tt0114709'), \
            'Didn\'t populate RT info for Toy Story'
        assert self.task.find_entry(imdb_id='tt0114709'), \
            'Didn\'t populate imdb_id info for Toy Story'
        assert self.task.find_entry(rt_name='The Matrix', rt_year=1999, rt_id=12897, imdb_id='tt0133093'), \
            'Didn\'t populate RT info for The Matrix'
        assert self.task.find_entry(rt_name='Star Wars: Episode I - The Phantom Menace',
                                    rt_year=1999, rt_id=10008), \
            'Didn\'t populate RT info for Star Wars: Episode I - The Phantom Menace (in 3D)'
        assert self.task.find_entry(rt_name='Taken', rt_year=2008, rt_id=770680780), \
            'Didn\'t populate RT info for Taken'
        assert self.task.find_entry(rt_name='Rush Hour', rt_year=1998, rt_id=10201), \
            'Didn\'t populate RT info for Rush Hour'
        assert self.task.find_entry(rt_name='Up', rt_year=2009, rt_id=770671912), \
            'Didn\'t populate RT info for Up'

########NEW FILE########
__FILENAME__ = test_rss
from __future__ import unicode_literals, division, absolute_import
import yaml
from tests import FlexGetBase
from nose.plugins.attrib import attr


class TestInputRSS(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            rss:
              url: rss.xml
              silent: yes
        tasks:
          test: {}
          test2:
            rss:
              link: otherlink
          test3:
            rss:
              other_fields: ['Otherfield']
          test_group_links:
            rss:
              group_links: yes
          test_multiple_links:
            rss:
              link:
                - guid
                - otherlink
          test_all_entries_no:
            rss:
              all_entries: no
          test_all_entries_yes:
            rss:
              all_entries: yes
    """

    def test_rss(self):
        self.execute_task('test')

        # normal entry
        assert self.task.find_entry(title='Normal', url='http://localhost/normal',
                                    description='Description, normal'), \
            'RSS entry missing: normal'

        # multiple enclosures
        assert self.task.find_entry(title='Multiple enclosures', url='http://localhost/enclosure1',
                                    filename='enclosure1', description='Description, multiple'), \
            'RSS entry missing: enclosure1'
        assert self.task.find_entry(title='Multiple enclosures', url='http://localhost/enclosure2',
                                    filename='enclosure2', description='Description, multiple'), \
            'RSS entry missing: enclosure2'
        assert self.task.find_entry(title='Multiple enclosures', url='http://localhost/enclosure3',
                                    filename='enclosure3', description='Description, multiple'), \
            'RSS entry missing: enclosure3'

        # zero sized enclosure should not pick up filename (some idiotic sites)
        e = self.task.find_entry(title='Zero sized enclosure')
        assert e, 'RSS entry missing: zero sized'
        assert not e.has_key('filename'), \
            'RSS entry with 0-sized enclosure should not have explicit filename'

        # messy enclosure
        e = self.task.find_entry(title='Messy enclosure')
        assert e, 'RSS entry missing: messy'
        assert e.has_key('filename'), 'Messy RSS enclosure: missing filename'
        assert e['filename'] == 'enclosure.mp3', 'Messy RSS enclosure: wrong filename'

        # pick link from guid
        assert self.task.find_entry(title='Guid link', url='http://localhost/guid',
                                    description='Description, guid'), \
                                    'RSS entry missing: guid'

        # empty title, should be skipped
        assert not self.task.find_entry(description='Description, empty title'), \
            'RSS entry without title should be skipped'

    def test_rss2(self):
        # custom link field
        self.execute_task('test2')
        assert self.task.find_entry(title='Guid link', url='http://localhost/otherlink'), \
            'Custom field link not found'

    def test_rss3(self):
        # grab other_fields and attach to entry
        self.execute_task('test3')
        for entry in self.task.rejected:
            print entry['title']
        assert self.task.find_entry(title='Other fields', otherfield='otherfield'), \
            'Specified other_field not attached to entry'

    def test_group_links(self):
        self.execute_task('test_group_links')
        # Test the composite entry was made
        entry = self.task.find_entry(title='Multiple enclosures', url='http://localhost/multiple_enclosures')
        assert entry, 'Entry not created for item with multiple enclosures'
        urls = ['http://localhost/enclosure%d' % num for num in range(1, 3)]
        urls_not_present = [url for url in urls if url not in entry.get('urls')]
        assert not urls_not_present, '%s should be present in urls list' % urls_not_present
        # Test no entries were made for the enclosures
        for url in urls:
            assert not self.task.find_entry(title='Multiple enclosures', url=url), \
                'Should not have created an entry for each enclosure'

    def test_multiple_links(self):
        self.execute_task('test_multiple_links')
        entry = self.task.find_entry(title='Guid link', url='http://localhost/guid',
                                    description='Description, guid')
        assert entry['urls'] == ['http://localhost/guid', 'http://localhost/otherlink'], \
            'Failed to set urls with both links'

    def test_all_entries_no(self):
        self.execute_task('test_all_entries_no')
        assert self.task.entries, 'Entries should have been produced on first run.'
        # reset input cache so that the cache is not used for second execution
        from flexget.utils.cached_input import cached
        cached.cache.clear()
        self.execute_task('test_all_entries_no')
        assert not self.task.entries, 'No entries should have been produced the second run.'

    def test_all_entries_yes(self):
        self.execute_task('test_all_entries_yes')
        assert self.task.entries, 'Entries should have been produced on first run.'
        self.execute_task('test_all_entries_yes')
        assert self.task.entries, 'Entries should have been produced on second run.'


class TestRssOnline(FlexGetBase):

    __yaml__ = """
        tasks:
          normal:
            rss: http://labs.silverorange.com/local/solabs/rsstest/rss_plain.xml

          ssl_no_http_auth:
            rss: https://secure3.silverorange.com/rsstest/rss_with_ssl.xml

          auth_no_ssl:
            rss:
              url: http://labs.silverorange.com/local/solabs/rsstest/httpauth/rss_with_auth.xml
              username: testuser
              password: testpass

          ssl_auth:
            rss:
              url: https://secure3.silverorange.com/rsstest/httpauth/rss_with_ssl_and_auth.xml
              username: testuser
              password: testpass

    """

    @attr(online=True)
    def test_rss_online(self):
        # Make sure entries are created for all test tasks
        tasks = yaml.load(self.__yaml__)['tasks']
        for task in tasks:
            self.execute_task(task)
            assert self.task.entries, 'No results for task `%s`' % task

########NEW FILE########
__FILENAME__ = test_seen
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestFilterSeen(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            accept_all: true

        tasks:
          test:
            mock:
              - {title: 'Seen title 1', url: 'http://localhost/seen1'}

          test2:
            mock:
              - {title: 'Seen title 2', url: 'http://localhost/seen1'} # duplicate by url
              - {title: 'Seen title 1', url: 'http://localhost/seen2'} # duplicate by title
              - {title: 'Seen title 3', url: 'http://localhost/seen3'} # new

          test_number:
            mock:
              - {title: 'New title 1', url: 'http://localhost/new1', imdb_score: 5}
              - {title: 'New title 2', url: 'http://localhost/new2', imdb_score: 5}

          test_learn:
            mock:
            - title: learned entry
            accept_all: yes
            mock_output: yes
    """

    def test_seen(self):
        self.execute_task('test')
        assert self.task.find_entry(title='Seen title 1'), 'Test entry missing'
        # run again, should filter
        self.task.execute()
        assert not self.task.find_entry(title='Seen title 1'), 'Seen test entry remains'

        # execute another task
        self.execute_task('test2')
        # should not contain since fields seen in previous task
        assert not self.task.find_entry(title='Seen title 1'), 'Seen test entry 1 remains in second task'
        assert not self.task.find_entry(title='Seen title 2'), 'Seen test entry 2 remains in second task'
        # new item in task should exists
        assert self.task.find_entry(title='Seen title 3'), 'Unseen test entry 3 not in second task'

        # test that we don't filter reject on non-string fields (ie, seen same imdb_score)

        self.execute_task('test_number')
        assert self.task.find_entry(title='New title 1') and self.task.find_entry(title='New title 2'), \
            'Item should not have been rejected because of number field'

    def test_learn(self):
        self.execute_task('test_learn', options={'learn': True})
        assert len(self.task.accepted) == 1, 'entry should have been accepted'
        assert not self.task.mock_output, 'Entry should not have been output with --learn'
        self.execute_task('test_learn')
        assert len(self.task.rejected) == 1, 'Seen plugin should have rejected on second run'

class TestSeenLocal(FlexGetBase):

    __yaml__ = """
      templates:
        global:
          accept_all: yes
      tasks:
        global seen 1:
          mock:
          - title: item 1
        local seen:
          seen: local
          mock:
          - title: item 1
          - title: item 2
        global seen 2:
          mock:
          - title: item 1
          - title: item 2
    """

    def test_local(self):
        self.execute_task('global seen 1')
        # global seen 1 task should not affect seen in the local seen task
        self.execute_task('local seen')
        assert self.task.find_entry('accepted', title='item 1'), 'item 1 should be accepted first run'
        # seen should still work normally within the local seen task
        self.execute_task('local seen')
        assert self.task.find_entry('rejected', title='item 1'), 'item 1 should be seen on second run'
        # local seen task should not affect global seen 2 task, but global seen 1 should
        self.execute_task('global seen 2')
        assert self.task.find_entry('rejected', title='item 1'), 'item 1 should be seen'
        assert self.task.find_entry('accepted', title='item 2'), 'item 2 should be accepted'


class TestFilterSeenMovies(FlexGetBase):

    __yaml__ = """
        tasks:
          test_1:
            mock:
               - {title: 'Seen movie title 1', url: 'http://localhost/seen_movie1', imdb_id: 'tt0103064', tmdb_id: 123}
               - {title: 'Seen movie title 2', url: 'http://localhost/seen_movie2', imdb_id: 'tt0103064'}
            accept_all: yes
            seen_movies: loose

          test_2:
            mock:
              - {title: 'Seen movie title 3', url: 'http://localhost/seen_movie3', imdb_id: 'tt0103064'}
              - {title: 'Seen movie title 4', url: 'http://localhost/seen_movie4', imdb_id: 'tt0103064'}
              - {title: 'Seen movie title 5', url: 'http://localhost/seen_movie5', imdb_id: 'tt0231264'}
              - {title: 'Seen movie title 6', url: 'http://localhost/seen_movie6', tmdb_id: 123}
            seen_movies: loose

          strict:
            mock:
              - {title: 'Seen movie title 7', url: 'http://localhost/seen_movie7', imdb_id: 'tt0134532'}
              - {title: 'Seen movie title 8', url: 'http://localhost/seen_movie8', imdb_id: 'tt0103066'}
              - {title: 'Seen movie title 9', url: 'http://localhost/seen_movie9', tmdb_id: 456}
              - {title: 'Seen movie title 10', url: 'http://localhost/seen_movie10'}
            seen_movies: strict
    """

    def test_seen_movies(self):
        self.execute_task('test_1')
        assert not (self.task.find_entry(title='Seen movie title 1') and self.task.find_entry(title='Seen movie title 2')), 'Movie accepted twice in one run'

        # execute again
        self.task.execute()
        assert not self.task.find_entry(title='Seen movie title 1'), 'Test movie entry 1 should be rejected in second execution'
        assert not self.task.find_entry(title='Seen movie title 2'), 'Test movie entry 2 should be rejected in second execution'

        # execute another task
        self.execute_task('test_2')

        # should not contain since fields seen in previous task
        assert not self.task.find_entry(title='Seen movie title 3'), 'seen movie 3 exists'
        assert not self.task.find_entry(title='Seen movie title 4'), 'seen movie 4 exists'
        assert not self.task.find_entry(title='Seen movie title 6'), 'seen movie 6 exists (tmdb_id)'
        assert self.task.find_entry(title='Seen movie title 5'), 'unseen movie 5 doesn\'t exist'

    def test_seen_movies_strict(self):
        self.execute_task('strict')
        assert len(self.task.rejected) == 1, 'Too many movies were rejected'
        assert not self.task.find_entry(title='Seen movie title 10'), 'strict should not have passed movie 10'

########NEW FILE########
__FILENAME__ = test_series
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


def age_series(**kwargs):
    from flexget.plugins.filter.series import Release
    from flexget.manager import Session
    import datetime
    session = Session()
    session.query(Release).update({'first_seen': datetime.datetime.now() - datetime.timedelta(**kwargs)})
    session.commit()


class TestQuality(FlexGetBase):

    __yaml__ = """
        tasks:
          exact_quality:
            mock:
              - {title: 'QTest.S01E01.HDTV.XViD-FlexGet'}
              - {title: 'QTest.S01E01.PDTV.XViD-FlexGet'}
              - {title: 'QTest.S01E01.DSR.XViD-FlexGet'}
              - {title: 'QTest.S01E01.1080p.XViD-FlexGet'}
              - {title: 'QTest.S01E01.720p.XViD-FlexGet'}
            series:
              - QTest:
                  quality: 720p

          quality_fail:
            mock:
              - {title: 'Q2Test.S01E01.HDTV.XViD-FlexGet'}
              - {title: 'Q2Test.S01E01.PDTV.XViD-FlexGet'}
              - {title: 'Q2Test.S01E01.DSR.XViD-FlexGet'}
            series:
              - Q2Test:
                 quality: 720p

          min_quality:
            mock:
              - {title: 'MinQTest.S01E01.HDTV.XViD-FlexGet'}
              - {title: 'MinQTest.S01E01.PDTV.XViD-FlexGet'}
              - {title: 'MinQTest.S01E01.DSR.XViD-FlexGet'}
              - {title: 'MinQTest.S01E01.1080p.XViD-FlexGet'}
              - {title: 'MinQTest.S01E01.720p.XViD-FlexGet'}
            series:
              - MinQTest:
                  quality: ">720p"

          max_quality:
            mock:
              - {title: 'MaxQTest.S01E01.HDTV.XViD-FlexGet'}
              - {title: 'MaxQTest.S01E01.PDTV.XViD-FlexGet'}
              - {title: 'MaxQTest.S01E01.DSR.XViD-FlexGet'}
              - {title: 'MaxQTest.S01E01.1080p.XViD-FlexGet'}
              - {title: 'MaxQTest.S01E01.720p.XViD-FlexGet'}
              - {title: 'MaxQTest.S01E01.720p.bluray-FlexGet'}
            series:
              - MaxQTest:
                  quality: "<720p <=HDTV"

          min_max_quality:
            mock:
              - {title: 'MinMaxQTest.S01E01.HDTV.XViD-FlexGet'}
              - {title: 'MinMaxQTest.S01E01.PDTV.XViD-FlexGet'}
              - {title: 'MinMaxQTest.S01E01.DSR.XViD-FlexGet'}
              - {title: 'MinMaxQTest.S01E01.720p.XViD-FlexGet'}
              - {title: 'MinMaxQTest.S01E01.HR.XViD-FlexGet'}
              - {title: 'MinMaxQTest.S01E01.1080p.XViD-FlexGet'}
            series:
              - MinMaxQTest:
                  quality: 480p-hr

          max_unknown_quality:
            mock:
              - {title: 'MaxUnknownQTest.S01E01.XViD-FlexGet'}
            series:
              - MaxUnknownQTest:
                  quality: "<=hdtv"

          description_quality:
            mock:
              - {'title': 'Description.S01E01', 'description': 'The quality should be 720p'}
            series:
              - description: {quality: 720p}

          quality_from_group:
            mock:
              - {title: 'GroupQual.S01E01.HDTV.XViD-FlexGet'}
              - {title: 'GroupQual.S01E01.PDTV.XViD-FlexGet'}
              - {title: 'GroupQual.S01E01.DSR.XViD-FlexGet'}
              - {title: 'GroupQual.S01E01.1080p.XViD-FlexGet'}
              - {title: 'GroupQual.S01E01.720p.XViD-FlexGet'}
              - {title: 'Other.S01E01.hdtv.dd5.1.XViD-FlexGet'}
              - {title: 'Other.S01E01.720p.hdtv.XViD-FlexGet'}
            series:
              720P:
                - GroupQual
              # Test that an integer group name doesn't cause an exception.
              1080:
                - Test
              hdtv <hr !dd5.1:
                - Other
    """

    def test_exact_quality(self):
        """Series plugin: choose by quality"""
        self.execute_task('exact_quality')
        assert self.task.find_entry('accepted', title='QTest.S01E01.720p.XViD-FlexGet'), \
            '720p should have been accepted'
        assert len(self.task.accepted) == 1, 'should have accepted only one'

    def test_quality_fail(self):
        self.execute_task('quality_fail')
        assert not self.task.accepted, 'No qualities should have matched'

    def test_min_quality(self):
        """Series plugin: min_quality"""
        self.execute_task('min_quality')
        assert self.task.find_entry('accepted', title='MinQTest.S01E01.1080p.XViD-FlexGet'), \
            'MinQTest.S01E01.1080p.XViD-FlexGet should have been accepted'
        assert len(self.task.accepted) == 1, 'should have accepted only one'

    def test_max_quality(self):
        """Series plugin: max_quality"""
        self.execute_task('max_quality')
        assert self.task.find_entry('accepted', title='MaxQTest.S01E01.HDTV.XViD-FlexGet'), \
            'MaxQTest.S01E01.HDTV.XViD-FlexGet should have been accepted'
        assert len(self.task.accepted) == 1, 'should have accepted only one'

    def test_min_max_quality(self):
        """Series plugin: min_quality with max_quality"""
        self.execute_task('min_max_quality')
        assert self.task.find_entry('accepted', title='MinMaxQTest.S01E01.HR.XViD-FlexGet'), \
            'MinMaxQTest.S01E01.HR.XViD-FlexGet should have been accepted'
        assert len(self.task.accepted) == 1, 'should have accepted only one'

    def test_max_unknown_quality(self):
        """Series plugin: max quality with unknown quality"""
        self.execute_task('max_unknown_quality')
        assert len(self.task.accepted) == 1, 'should have accepted'

    def test_quality_from_description(self):
        """Series plugin: quality from description"""
        self.execute_task('description_quality')
        assert len(self.task.accepted) == 1, 'should have accepted'

    def test_group_quality(self):
        """Series plugin: quality from group name"""
        self.execute_task('quality_from_group')
        assert self.task.find_entry('accepted', title='GroupQual.S01E01.720p.XViD-FlexGet'), \
            'GroupQual.S01E01.720p.XViD-FlexGet should have been accepted'
        assert len(self.task.accepted) == 1, 'should have accepted only one (no entries should pass for series `other`'


class TestDatabase(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            series:
              - some series
              - progress

        tasks:
          test_1:
            mock:
              - {title: 'Some.Series.S01E20.720p.XViD-FlexGet'}
          test_2:
            mock:
              - {title: 'Some.Series.S01E20.720p.XViD-DoppelGanger'}

          progress_1:
            mock:
              - {title: 'Progress.S01E20.720p-FlexGet'}
              - {title: 'Progress.S01E20.HDTV-FlexGet'}

          progress_2:
            mock:
              - {title: 'Progress.S01E20.720p.Another-FlexGet'}
              - {title: 'Progress.S01E20.HDTV-Another-FlexGet'}
    """

    def test_database(self):
        """Series plugin: simple database"""

        self.execute_task('test_1')
        self.execute_task('test_2')
        assert self.task.find_entry('rejected', title='Some.Series.S01E20.720p.XViD-DoppelGanger'), \
            'failed basic download remembering'

    def test_doppelgangers(self):
        """Series plugin: doppelganger releases (dupes)"""

        self.execute_task('progress_1')
        assert self.task.find_entry('accepted', title='Progress.S01E20.720p-FlexGet'), \
            'best quality not accepted'
        # should not accept anything
        self.execute_task('progress_1')
        assert not self.task.accepted, 'repeated execution accepted'
        # introduce new doppelgangers
        self.execute_task('progress_2')
        assert not self.task.accepted, 'doppelgangers accepted'


class TestFilterSeries(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Another.Series.S01E20.720p.XViD-FlexGet'}
              - {title: 'Another.Series.S01E21.1080p.H264-FlexGet'}
              - {title: 'Date.Series.10-11-2008.XViD'}
              - {title: 'Date.Series.10.12.2008.XViD'}
              - {title: 'Date.Series.2008-10-13.XViD'}
              - {title: 'Date.Series.10.14.09.XViD'}
              - {title: 'Date Series 2010 11 17 XViD'}
              - {title: 'Useless title', filename: 'Filename.Series.S01E26.XViD'}
              - {title: 'Empty.Description.S01E22.XViD', description: ''}

            # test chaining
            regexp:
              reject:
                - 1080p

            series:
              - another series
              - date series
              - filename series
              - empty description

          metainfo_series_override:
            metainfo_series: yes
            mock:
              - {title: 'Test.Series.with.extra.crap.S01E02.PDTV.XViD-FlexGet'}
              - {title: 'Other.Show.with.extra.crap.S02E01.PDTV.XViD-FlexGet'}
            series:
              - Test Series

          test_all_series_mode:
            mock:
              - {title: 'Test.Series.S01E02.PDTV.XViD-FlexGet'}
              - {title: 'Test Series - 1x03 - PDTV XViD-FlexGet'}
              - {title: 'Other.Show.S02E01.PDTV.XViD-FlexGet'}
              - {title: 'other show season 2 episode 2'}
              - {title: 'Date.Show.03-29-2012.HDTV.XViD-FlexGet'}
            all_series: yes

          test_alternate_name:
            mock:
            - title: The.Show.S01E01
            - title: Other.Name.S01E02
            - title: many.names.S01E01
            - title: name.1.S01E02
            - title: name.2.S01E03
            - title: paren.title.2013.S01E01
            series:
            - The Show:
                alternate_name: Other Name
            - many names:
                alternate_name:
                - name 1
                - name 2
            - paren title (US):
                alternate_name: paren title 2013
    """

    def test_smoke(self):
        """Series plugin: test several standard features"""
        self.execute_task('test')

        # normal passing
        assert self.task.find_entry(title='Another.Series.S01E20.720p.XViD-FlexGet'), \
            'Another.Series.S01E20.720p.XViD-FlexGet should have passed'

        # date formats
        df = ['Date.Series.10-11-2008.XViD', 'Date.Series.10.12.2008.XViD', 'Date Series 2010 11 17 XViD',
              'Date.Series.2008-10-13.XViD', 'Date.Series.10.14.09.XViD']
        for d in df:
            entry = self.task.find_entry(title=d)
            assert entry, 'Date format did not match %s' % d
            assert 'series_parser' in entry, 'series_parser missing from %s' % d
            assert entry['series_parser'].id_type == 'date', '%s did not return three groups for dates' % d

        # parse from filename
        assert self.task.find_entry(filename='Filename.Series.S01E26.XViD'), 'Filename parsing failed'

        # empty description
        assert self.task.find_entry(title='Empty.Description.S01E22.XViD'), 'Empty Description failed'

        # chaining with regexp plugin
        assert self.task.find_entry('rejected', title='Another.Series.S01E21.1080p.H264-FlexGet'), \
            'regexp chaining'

    def test_metainfo_series_override(self):
        """Series plugin: override metainfo_series"""
        self.execute_task('metainfo_series_override')
        # Make sure the metainfo_series plugin is working first
        entry = self.task.find_entry('entries', title='Other.Show.with.extra.crap.S02E01.PDTV.XViD-FlexGet')
        assert entry['series_guessed'], 'series should have been guessed'
        assert entry['series_name'] == entry['series_parser'].name == 'Other Show With Extra Crap', \
            'metainfo_series is not running'
        # Make sure the good series data overrode metainfo data for the listed series
        entry = self.task.find_entry('accepted', title='Test.Series.with.extra.crap.S01E02.PDTV.XViD-FlexGet')
        assert not entry.get('series_guessed'), 'series plugin should override series_guessed'
        assert entry['series_name'] == entry['series_parser'].name == 'Test Series', \
            'Series name should be \'Test Series\', was: entry: %s, parser: %s' % (entry['series_name'], entry['series_parser'].name)

    def test_all_series_mode(self):
        """Series plugin: test all option"""
        self.execute_task('test_all_series_mode')
        assert self.task.find_entry('accepted', title='Test.Series.S01E02.PDTV.XViD-FlexGet')
        entry = self.task.find_entry('accepted', title='Test Series - 1x03 - PDTV XViD-FlexGet')
        assert entry['series_name'] == 'Test Series'
        entry = self.task.find_entry('accepted', title='Other.Show.S02E01.PDTV.XViD-FlexGet')
        assert entry['series_guessed']
        entry2 = self.task.find_entry('accepted', title='other show season 2 episode 2')
        # Make sure case is normalized so series are marked with the same name no matter the case in the title
        assert entry['series_name'] == entry2['series_name'] == 'Other Show', 'Series names should be in title case'
        entry = self.task.find_entry('accepted', title='Date.Show.03-29-2012.HDTV.XViD-FlexGet')
        assert entry['series_guessed']
        assert entry['series_name'] == 'Date Show'

    def test_alternate_name(self):
        self.execute_task('test_alternate_name')
        assert all(e.accepted for e in self.task.all_entries), 'All releases should have matched a show'


class TestEpisodeAdvancement(FlexGetBase):

    __yaml__ = """
        tasks:

          test_backwards_1:
            mock:
              - {title: 'backwards s02e12'}
              - {title: 'backwards s02e10'}
            series:
              - backwards

          test_backwards_2:
            mock:
              - {title: 'backwards s02e01'}
            series:
              - backwards

          test_backwards_3:
            mock:
              - {title: 'backwards s01e01'}
            series:
              - backwards

          test_backwards_okay_1:
            mock:
              - {title: 'backwards s01e02'}
            series:
              - backwards:
                  tracking: backfill

          test_backwards_okay_2:
            mock:
              - {title: 'backwards s01e03'}
            series:
              - backwards:
                  tracking: no

          test_forwards_1:
            mock:
              - {title: 'forwards s01e01'}
            series:
              - forwards

          test_forwards_2:
            mock:
              - {title: 'forwards s02e01'}
            series:
              - forwards

          test_forwards_3:
            mock:
              - {title: 'forwards s03e01'}
            series:
              - forwards

          test_forwards_4:
            mock:
              - {title: 'forwards s04e02'}
            series:
              - forwards

          test_forwards_5:
            mock:
              - {title: 'forwards s05e01'}
            series:
              - forwards

          test_forwards_okay_1:
            mock:
              - {title: 'forwards s05e01'}
            series:
              - forwards:
                  tracking: no

          test_unordered:
            mock:
              - {title: 'zzz s01e05'}
              - {title: 'zzz s01e06'}
              - {title: 'zzz s01e07'}
              - {title: 'zzz s01e08'}
              - {title: 'zzz s01e09'}
              - {title: 'zzz s01e10'}
              - {title: 'zzz s01e15'}
              - {title: 'zzz s01e14'}
              - {title: 'zzz s01e13'}
              - {title: 'zzz s01e12'}
              - {title: 'zzz s01e11'}
              - {title: 'zzz s01e01'}
            series:
              - zzz

          test_seq1:
            mock:
              - title: seq 05
            series:
              - seq
          test_seq2:
            mock:
              - title: seq 06
            series:
              - seq
          test_seq3:
            mock:
              - title: seq 10
            series:
              - seq
          test_seq4:
            mock:
              - title: seq 01
            series:
              - seq

    """

    def test_backwards(self):
        """Series plugin: episode advancement (backwards)"""
        self.execute_task('test_backwards_1')
        assert self.task.find_entry('accepted', title='backwards s02e12'), \
            'backwards s02e12 should have been accepted'
        assert self.task.find_entry('accepted', title='backwards s02e10'), \
            'backwards s02e10 should have been accepted within grace margin'
        self.execute_task('test_backwards_2')
        assert self.task.find_entry('accepted', title='backwards s02e01'), \
            'backwards s02e01 should have been accepted, in current season'
        self.execute_task('test_backwards_3')
        assert self.task.find_entry('rejected', title='backwards s01e01'), \
            'backwards s01e01 should have been rejected, in previous season'
        self.execute_task('test_backwards_okay_1')
        assert self.task.find_entry('accepted', title='backwards s01e02'), \
            'backwards s01e01 should have been accepted, backfill enabled'
        self.execute_task('test_backwards_okay_2')
        assert self.task.find_entry('accepted', title='backwards s01e03'), \
            'backwards s01e01 should have been accepted, tracking off'

    def test_forwards(self):
        """Series plugin: episode advancement (future)"""
        self.execute_task('test_forwards_1')
        assert self.task.find_entry('accepted', title='forwards s01e01'), \
            'forwards s01e01 should have been accepted'
        self.execute_task('test_forwards_2')
        assert self.task.find_entry('accepted', title='forwards s02e01'), \
            'forwards s02e01 should have been accepted'
        self.execute_task('test_forwards_3')
        assert self.task.find_entry('accepted', title='forwards s03e01'), \
            'forwards s03e01 should have been accepted'
        self.execute_task('test_forwards_4')
        assert self.task.find_entry('rejected', title='forwards s04e02'),\
            'forwards s04e02 should have been rejected'
        self.execute_task('test_forwards_5')
        assert self.task.find_entry('rejected', title='forwards s05e01'), \
            'forwards s05e01 should have been rejected'
        self.execute_task('test_forwards_okay_1')
        assert self.task.find_entry('accepted', title='forwards s05e01'), \
            'forwards s05e01 should have been accepted with tracking turned off'

    def test_unordered(self):
        """Series plugin: unordered episode advancement"""
        self.execute_task('test_unordered')
        assert len(self.task.accepted) == 12, \
            'not everyone was accepted'

    def test_sequence(self):
        # First should be accepted
        self.execute_task('test_seq1')
        entry = self.task.find_entry('accepted', title='seq 05')
        assert entry['series_id'] == 5

        # Next in sequence should be accepted
        self.execute_task('test_seq2')
        entry = self.task.find_entry('accepted', title='seq 06')
        assert entry['series_id'] == 6

        # Should be too far in the future
        self.execute_task('test_seq3')
        entry = self.task.find_entry(title='seq 10')
        assert entry not in self.task.accepted, 'Should have been too far in future'

        # Should be too far in the past
        self.execute_task('test_seq4')
        entry = self.task.find_entry(title='seq 01')
        assert entry not in self.task.accepted, 'Should have been too far in the past'


class TestFilterSeriesPriority(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'foobar 720p s01e01'}
              - {title: 'foobar hdtv s01e01'}
            regexp:
              reject:
                - 720p
            series:
              - foobar
    """

    def test_priorities(self):
        """Series plugin: regexp plugin is able to reject before series plugin"""
        self.execute_task('test')
        assert self.task.find_entry('rejected', title='foobar 720p s01e01'), \
            'foobar 720p s01e01 should have been rejected'
        assert self.task.find_entry('accepted', title='foobar hdtv s01e01'), \
            'foobar hdtv s01e01 is not accepted'


class TestPropers(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            # prevents seen from rejecting on second execution,
            # we want to see that series is able to reject
            disable_builtins: yes
            series:
              - test
              - foobar
              - asfd:
                  quality: HR-1080p
              - V
              - tftest:
                  propers: 3 hours
              - notest:
                  propers: no

        tasks:
          propers_1:
            mock:
              - {title: 'Test.S01E01.720p-FlexGet'}

          # introduce proper, should be accepted
          propers_2:
            mock:
              - {title: 'Test.S01E01.720p.Proper-FlexGet'}

          # introduce non-proper, should not be downloaded
          propers_3:
            mock:
              - {title: 'Test.S01E01.FlexGet'}

          # introduce proper at the same time, should nuke non-proper and get proper
          proper_at_first:
            mock:
              - {title: 'Foobar.S01E01.720p.FlexGet'}
              - {title: 'Foobar.S01E01.720p.proper.FlexGet'}

          # test a lot of propers at once
          lot_propers:
            mock:
              - {title: 'V.2009.S01E01.PROPER.HDTV.A'}
              - {title: 'V.2009.S01E01.PROPER.HDTV.B'}
              - {title: 'V.2009.S01E01.PROPER.HDTV.C'}

          diff_quality_1:
            mock:
              - {title: 'Test.S01E02.720p-FlexGet'}

          # low quality proper, should not be accepted
          diff_quality_2:
            mock:
              - {title: 'Test.S01E02.HDTV.Proper-FlexGet'}

          # min + max quality with propers
          min_max_quality_1:
            mock:
              - {title: 'asfd.S01E01.720p-FlexGet'}
          min_max_quality_2:
            mock:
              - {title: 'asfd.S01E01.720p.Proper-FlexGet'}

          proper_timeframe_1:
            mock:
              - {title: 'TFTest.S01E01.720p-FlexGet'}

          proper_timeframe_2:
            mock:
              - {title: 'TFTest.S01E01.720p.proper-FlexGet'}

          no_propers_1:
            mock:
              - {title: 'NoTest.S01E01.720p-FlexGet'}

          no_propers_2:
            mock:
              - {title: 'NoTest.S01E01.720p.proper-FlexGet'}

          proper_upgrade_1:
            mock:
              - {title: 'Test.S02E01.hdtv.proper'}

          proper_upgrade_2:
            mock:
              - {title: 'Test.S02E01.hdtv.real.proper'}

          anime_proper_1:
            mock:
              - title: test 04v0 hdtv

          anime_proper_2:
            mock:
              - title: test 04 hdtv

          fastsub_proper_1:
            mock:
              - title: test s01e01 Fastsub hdtv

          fastsub_proper_2:
               mock:
                 - title: test s01e01 Fastsub repack hdtv

          fastsub_proper_3:
             mock:
              - title: test s01e01 hdtv

          fastsub_proper_4:
            mock:
               - title: test s01e01 proper hdtv
        """

    def test_propers_timeframe(self):
        """Series plugin: propers timeframe"""
        self.execute_task('proper_timeframe_1')
        assert self.task.find_entry('accepted', title='TFTest.S01E01.720p-FlexGet'), \
            'Did not accept before timeframe'

        # let 6 hours pass
        age_series(hours=6)

        self.execute_task('proper_timeframe_2')
        assert self.task.find_entry('rejected', title='TFTest.S01E01.720p.proper-FlexGet'), \
            'Did not reject after proper timeframe'

    def test_no_propers(self):
        """Series plugin: no propers at all"""
        self.execute_task('no_propers_1')
        assert len(self.task.accepted) == 1, 'broken badly'
        self.execute_task('no_propers_2')
        assert len(self.task.rejected) == 1, 'accepted proper'

    def test_min_max_propers(self):
        """Series plugin: min max propers"""
        self.execute_task('min_max_quality_1')
        assert len(self.task.accepted) == 1, 'uhh, broken badly'
        self.execute_task('min_max_quality_2')
        assert len(self.task.accepted) == 1, 'should have accepted proper'

    def test_lot_propers(self):
        """Series plugin: proper flood"""
        self.execute_task('lot_propers')
        assert len(self.task.accepted) == 1, 'should have accepted (only) one of the propers'

    def test_diff_quality_propers(self):
        """Series plugin: proper in different/wrong quality"""
        self.execute_task('diff_quality_1')
        assert len(self.task.accepted) == 1
        self.execute_task('diff_quality_2')
        assert len(self.task.accepted) == 0, 'should not have accepted lower quality proper'

    def test_propers(self):
        """Series plugin: proper accepted after episode is downloaded"""
        # start with normal download ...
        self.execute_task('propers_1')
        assert self.task.find_entry('accepted', title='Test.S01E01.720p-FlexGet'), \
            'Test.S01E01-FlexGet should have been accepted'

        # rejects downloaded
        self.execute_task('propers_1')
        assert self.task.find_entry('rejected', title='Test.S01E01.720p-FlexGet'), \
            'Test.S01E01-FlexGet should have been rejected'

        # accepts proper
        self.execute_task('propers_2')
        assert self.task.find_entry('accepted', title='Test.S01E01.720p.Proper-FlexGet'), \
            'new undownloaded proper should have been accepted'

        # reject downloaded proper
        self.execute_task('propers_2')
        assert self.task.find_entry('rejected', title='Test.S01E01.720p.Proper-FlexGet'), \
            'downloaded proper should have been rejected'

        # reject episode that has been downloaded normally and with proper
        self.execute_task('propers_3')
        assert self.task.find_entry('rejected', title='Test.S01E01.FlexGet'), \
            'Test.S01E01.FlexGet should have been rejected'

    def test_proper_available(self):
        """Series plugin: proper available immediately"""
        self.execute_task('proper_at_first')
        assert self.task.find_entry('accepted', title='Foobar.S01E01.720p.proper.FlexGet'), \
            'Foobar.S01E01.720p.proper.FlexGet should have been accepted'

    def test_proper_upgrade(self):
        """Series plugin: real proper after proper"""
        self.execute_task('proper_upgrade_1')
        assert self.task.find_entry('accepted', title='Test.S02E01.hdtv.proper')
        self.execute_task('proper_upgrade_2')
        assert self.task.find_entry('accepted', title='Test.S02E01.hdtv.real.proper')

    def test_anime_proper(self):
        self.execute_task('anime_proper_1')
        assert self.task.accepted, 'ep should have accepted'
        self.execute_task('anime_proper_2')
        assert self.task.accepted, 'proper ep should have been accepted'

    def test_fastsub_proper(self):
        self.execute_task('fastsub_proper_1')
        assert self.task.accepted, 'ep should have accepted'
        self.execute_task('fastsub_proper_2')
        assert self.task.accepted, 'proper ep should have been accepted'
        self.execute_task('fastsub_proper_3')
        assert self.task.accepted, 'proper ep should have been accepted'
        self.execute_task('fastsub_proper_4')
        assert self.task.accepted, 'proper ep should have been accepted'


class TestSimilarNames(FlexGetBase):

    # hmm, not very good way to test this .. seriesparser should be tested alone?

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'FooBar.S03E01.DSR-FlexGet'}
              - {title: 'FooBar: FirstAlt.S02E01.DSR-FlexGet'}
              - {title: 'FooBar: SecondAlt.S01E01.DSR-FlexGet'}
            series:
              - FooBar
              - 'FooBar: FirstAlt'
              - 'FooBar: SecondAlt'
          test_ambiguous:
            mock:
            - title: Foo.2.2
            series:
            - Foo:
                identified_by: sequence
            - Foo 2:
                identified_by: sequence
    """

    def test_names(self):
        """Series plugin: similar namings"""
        self.execute_task('test')
        assert self.task.find_entry('accepted', title='FooBar.S03E01.DSR-FlexGet'), 'Standard failed?'
        assert self.task.find_entry('accepted', title='FooBar: FirstAlt.S02E01.DSR-FlexGet'), 'FirstAlt failed'
        assert self.task.find_entry('accepted', title='FooBar: SecondAlt.S01E01.DSR-FlexGet'), 'SecondAlt failed'

    def test_ambiguous(self):
        self.execute_task('test_ambiguous')
        # In the event of ambiguous match, more specific one should be chosen
        assert self.task.find_entry('accepted', title='Foo.2.2')['series_name'] == 'Foo 2'


class TestDuplicates(FlexGetBase):

    __yaml__ = """

        templates:
          global: # just cleans log a bit ..
            disable_builtins:
              - seen

        tasks:
          test_dupes:
            mock:
              - {title: 'Foo.2009.S02E04.HDTV.XviD-2HD[FlexGet]'}
              - {title: 'Foo.2009.S02E04.HDTV.XviD-2HD[ASDF]'}
            series:
              - Foo 2009

          test_1:
            mock:
              - {title: 'Foo.Bar.S02E04.HDTV.XviD-2HD[FlexGet]'}
              - {title: 'Foo.Bar.S02E04.HDTV.XviD-2HD[ASDF]'}
            series:
              - foo bar

          test_2:
            mock:
              - {title: 'Foo.Bar.S02E04.XviD-2HD[ASDF]'}
              - {title: 'Foo.Bar.S02E04.HDTV.720p.XviD-2HD[FlexGet]'}
              - {title: 'Foo.Bar.S02E04.DSRIP.XviD-2HD[ASDF]'}
              - {title: 'Foo.Bar.S02E04.HDTV.1080p.XviD-2HD[ASDF]'}
              - {title: 'Foo.Bar.S02E03.HDTV.XviD-FlexGet'}
              - {title: 'Foo.Bar.S02E05.HDTV.XviD-ZZZ'}
              - {title: 'Foo.Bar.S02E05.720p.HDTV.XviD-YYY'}
            series:
              - foo bar

          test_true_dupes:
            mock:
              - {title: 'Dupe.S02E04.HDTV.XviD-FlexGet'}
              - {title: 'Dupe.S02E04.HDTV.XviD-FlexGet'}
              - {title: 'Dupe.S02E04.HDTV.XviD-FlexGet'}
            series:
              - dupe
    """

    def test_dupes(self):
        """Series plugin: dupes with same quality"""
        self.execute_task('test_dupes')
        assert len(self.task.accepted) == 1, 'accepted both'

    def test_true_dupes(self):
        """Series plugin: true duplicate items"""
        self.execute_task('test_true_dupes')
        assert len(self.task.accepted) == 1, 'should have accepted (only) one'

    def test_downloaded(self):
        """Series plugin: multiple downloaded and new episodes are handled correctly"""

        self.execute_task('test_1')
        self.execute_task('test_2')

        # these should be accepted
        accepted = ['Foo.Bar.S02E03.HDTV.XviD-FlexGet', 'Foo.Bar.S02E05.720p.HDTV.XviD-YYY']
        for item in accepted:
            assert self.task.find_entry('accepted', title=item), \
                '%s should have been accepted' % item

        # these should be rejected
        rejected = ['Foo.Bar.S02E04.XviD-2HD[ASDF]', 'Foo.Bar.S02E04.HDTV.720p.XviD-2HD[FlexGet]',
                    'Foo.Bar.S02E04.DSRIP.XviD-2HD[ASDF]', 'Foo.Bar.S02E04.HDTV.1080p.XviD-2HD[ASDF]']
        for item in rejected:
            assert self.task.find_entry('rejected', title=item), \
                '%s should have been rejected' % item


class TestQualities(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            disable_builtins: yes
            series:
              - FooBar:
                  qualities:
                    - SDTV
                    - 720p
                    - 1080p
              - FooBaz:
                  upgrade: yes
                  qualities:
                    - hdtv
                    - hr
                    - 720p
              - FooBum:
                  quality: 720p-1080i
                  upgrade: yes
              - FooD:
                  target: 720p
                  timeframe: 0 hours
                  upgrade: yes
        tasks:
          test_1:
            mock:
              - {title: 'FooBar.S01E01.PDTV-FlexGet'}
              - {title: 'FooBar.S01E01.1080p-FlexGet'}
              - {title: 'FooBar.S01E01.HR-FlexGet'}
          test_2:
            mock:
              - {title: 'FooBar.S01E01.720p-FlexGet'}

          propers_1:
            mock:
              - {title: 'FooBar.S01E02.720p-FlexGet'}
          propers_2:
            mock:
              - {title: 'FooBar.S01E02.720p.Proper-FlexGet'}

          upgrade_1:
            mock:
              - {title: 'FooBaz.S01E02.pdtv-FlexGet'}
              - {title: 'FooBaz.S01E02.HR-FlexGet'}

          upgrade_2:
            mock:
              - {title: 'FooBaz.S01E02.720p-FlexGet'}
              - {title: 'FooBaz.S01E02.1080p-FlexGet'}

          upgrade_3:
            mock:
              - {title: 'FooBaz.S01E02.hdtv-FlexGet'}
              - {title: 'FooBaz.S01E02.720p rc-FlexGet'}
          quality_upgrade_1:
            mock:
              - title: FooBum.S03E01.1080p # too high
              - title: FooBum.S03E01.hdtv # too low
              - title: FooBum.S03E01.720p # in range
          quality_upgrade_2:
            mock:
              - title: FooBum.S03E01.1080i # should be upgraded to
              - title: FooBum.S03E01.720p-ver2 # Duplicate ep
          target_1:
            mock:
              - title: Food.S06E11.sdtv
              - title: Food.S06E11.hdtv
          target_2:
            mock:
              - title: Food.S06E11.1080p
              - title: Food.S06E11.720p
    """

    def test_qualities(self):
        """Series plugin: qualities"""
        self.execute_task('test_1')

        assert self.task.find_entry('accepted', title='FooBar.S01E01.PDTV-FlexGet'), \
            'Didn''t accept FooBar.S01E01.PDTV-FlexGet'
        assert self.task.find_entry('accepted', title='FooBar.S01E01.1080p-FlexGet'), \
            'Didn''t accept FooBar.S01E01.1080p-FlexGet'

        assert not self.task.find_entry('accepted', title='FooBar.S01E01.HR-FlexGet'), \
            'Accepted FooBar.S01E01.HR-FlexGet'

        self.execute_task('test_2')

        assert self.task.find_entry('accepted', title='FooBar.S01E01.720p-FlexGet'), \
            'Didn''t accept FooBar.S01E01.720p-FlexGet'

        # test that it rejects them afterwards

        self.execute_task('test_1')

        assert self.task.find_entry('rejected', title='FooBar.S01E01.PDTV-FlexGet'), \
            'Didn\'t reject FooBar.S01E01.PDTV-FlexGet'
        assert self.task.find_entry('rejected', title='FooBar.S01E01.1080p-FlexGet'), \
            'Didn\'t reject FooBar.S01E01.1080p-FlexGet'

        assert not self.task.find_entry('accepted', title='FooBar.S01E01.HR-FlexGet'), \
            'Accepted FooBar.S01E01.HR-FlexGet'

    def test_propers(self):
        """Series plugin: qualities + propers"""
        self.execute_task('propers_1')
        assert self.task.accepted
        self.execute_task('propers_2')
        assert self.task.accepted, 'proper not accepted'
        self.execute_task('propers_2')
        assert not self.task.accepted, 'proper accepted again'

    def test_qualities_upgrade(self):
        self.execute_task('upgrade_1')
        assert self.task.find_entry('accepted', title='FooBaz.S01E02.HR-FlexGet'), 'HR quality should be accepted'
        assert len(self.task.accepted) == 1, 'Only best quality should be accepted'
        self.execute_task('upgrade_2')
        assert self.task.find_entry('accepted', title='FooBaz.S01E02.720p-FlexGet'), '720p quality should be accepted'
        assert len(self.task.accepted) == 1, 'Only best quality should be accepted'
        self.execute_task('upgrade_3')
        assert not self.task.accepted, 'Should not have accepted worse qualities'

    def test_quality_upgrade(self):
        self.execute_task('quality_upgrade_1')
        assert len(self.task.accepted) == 1, 'Only one ep should have passed quality filter'
        assert self.task.find_entry('accepted', title='FooBum.S03E01.720p')
        self.execute_task('quality_upgrade_2')
        assert len(self.task.accepted) == 1, 'one ep should be valid upgrade'
        assert self.task.find_entry('accepted', title='FooBum.S03E01.1080i')

    def test_target_upgrade(self):
        self.execute_task('target_1')
        assert len(self.task.accepted) == 1, 'Only one ep should have been grabbed'
        assert self.task.find_entry('accepted', title='Food.S06E11.hdtv')
        self.execute_task('target_2')
        assert len(self.task.accepted) == 1, 'one ep should be valid upgrade'
        assert self.task.find_entry('accepted', title='Food.S06E11.720p'), 'Should upgrade to `target`'


class TestIdioticNumbering(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            series:
              - FooBar:
                  identified_by: ep

        tasks:
          test_1:
            mock:
              - {title: 'FooBar.S01E01.PDTV-FlexGet'}
          test_2:
            mock:
              - {title: 'FooBar.102.PDTV-FlexGet'}
    """

    def test_idiotic(self):
        """Series plugin: idiotic numbering scheme"""

        self.execute_task('test_1')
        self.execute_task('test_2')
        entry = self.task.find_entry(title='FooBar.102.PDTV-FlexGet')
        assert entry, 'entry not found?'
        assert entry['series_season'] == 1, 'season not detected'
        assert entry['series_episode'] == 2, 'episode not detected'


class TestNormalization(FlexGetBase):

    __yaml__ = """
        tasks:
          global:
            disable_builtins: [seen]
          test_1:
            mock:
              - {title: 'FooBar.S01E01.PDTV-FlexGet'}
            series:
              - FOOBAR
          test_2:
            mock:
              - {title: 'FooBar.S01E01.PDTV-aoeu'}
            series:
              - foobar
          test_3:
            mock:
              - title: Foo bar & co 2012.s01e01.sdtv.a
            series:
              - foo bar & co 2012
          test_4:
            mock:
              - title: Foo bar & co 2012.s01e01.sdtv.b
            series:
              - Foo/Bar and Co. (2012)
    """

    def test_capitalization(self):
        """Series plugin: configuration capitalization"""
        self.execute_task('test_1')
        assert self.task.find_entry('accepted', title='FooBar.S01E01.PDTV-FlexGet')
        self.execute_task('test_2')
        assert self.task.find_entry('rejected', title='FooBar.S01E01.PDTV-aoeu')

    def test_normalization(self):
        self.execute_task('test_3')
        assert self.task.find_entry('accepted', title='Foo bar & co 2012.s01e01.sdtv.a')
        self.execute_task('test_4')
        assert self.task.find_entry('rejected', title='Foo bar & co 2012.s01e01.sdtv.b')


class TestMixedNumbering(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            series:
              - FooBar:
                  identified_by: ep

        tasks:
          test_1:
            mock:
              - {title: 'FooBar.S03E07.PDTV-FlexGet'}
          test_2:
            mock:
              - {title: 'FooBar.0307.PDTV-FlexGet'}
    """

    def test_mixednumbering(self):
        """Series plugin: Mixed series numbering"""

        self.execute_task('test_1')
        assert self.task.find_entry('accepted', title='FooBar.S03E07.PDTV-FlexGet')
        self.execute_task('test_2')
        assert self.task.find_entry('rejected', title='FooBar.0307.PDTV-FlexGet')


class TestExact(FlexGetBase):

    __yaml__ = """
        tasks:
          auto:
            mock:
              - {title: 'ABC.MIAMI.S01E01.PDTV-FlexGet'}
              - {title: 'ABC.S01E01.PDTV-FlexGet'}
              - {title: 'ABC.LA.S01E01.PDTV-FlexGet'}
            series:
              - ABC
              - ABC LA
              - ABC Miami
          name_regexp:
            mock:
              - title: show s09e05 hdtv
              - title: show a s09e06 hdtv
            series:
              - show:
                  name_regexp: ^show
                  exact: yes
          date:
            mock:
              - title: date show 04.01.2011 hdtv
              - title: date show b 04.02.2011 hdtv
            series:
              - date show:
                  exact: yes
    """

    def test_auto(self):
        """Series plugin: auto enable exact"""
        self.execute_task('auto')
        assert self.task.find_entry('accepted', title='ABC.S01E01.PDTV-FlexGet')
        assert self.task.find_entry('accepted', title='ABC.LA.S01E01.PDTV-FlexGet')
        assert self.task.find_entry('accepted', title='ABC.MIAMI.S01E01.PDTV-FlexGet')

    def test_with_name_regexp(self):
        self.execute_task('name_regexp')
        assert self.task.find_entry('accepted', title='show s09e05 hdtv')
        assert not self.task.find_entry('accepted', title='show a s09e06 hdtv')

    def test_dated_show(self):
        self.execute_task('date')
        assert self.task.find_entry('accepted', title='date show 04.01.2011 hdtv')
        assert not self.task.find_entry('accepted', title='date show b 04.02.2011 hdtv')

class TestTimeframe(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            series:
              - test:
                  timeframe: 5 hours
                  target: 720p
        tasks:
          test_no_waiting:
            mock:
              - {title: 'Test.S01E01.720p-FlexGet'}

          test_stop_waiting_1:
            mock:
              - {title: 'Test.S01E02.HDTV-FlexGet'}

          test_stop_waiting_2:
             mock:
               - {title: 'Test.S01E02.720p-FlexGet'}

          test_proper_afterwards:
             mock:
               - {title: 'Test.S01E02.720p.Proper-FlexGet'}

          test_expires:
            mock:
              - {title: 'Test.S01E03.pdtv-FlexGet'}

          test_min_max_fail:
            series:
              - mm test:
                  timeframe: 5 hours
                  target: 720p
                  quality: hdtv+ <=720p
            mock:
              - {title: 'MM Test.S01E02.pdtv-FlexGet'}
              - {title: 'MM Test.S01E02.1080p-FlexGet'}

          test_min_max_pass:
            series:
              - mm test:
                  timeframe: 5 hours
                  target: 720p
                  quality: hdtv+ <=720p
            mock:
              - {title: 'MM Test.S01E02.pdtv-FlexGet'}
              - {title: 'MM Test.S01E02.hdtv-FlexGet'}
              - {title: 'MM Test.S01E02.1080p-FlexGet'}

          test_qualities_fail:
            series:
              - q test:
                  timeframe: 5 hours
                  qualities:
                    - hdtv
                    - 1080p

            mock:
              - {title: 'Q Test.S01E02.pdtv-FlexGet'}
              - {title: 'Q Test.S01E02.1080p-FlexGet'}

          test_qualities_pass:
            series:
              - q test:
                  timeframe: 5 hours
                  qualities:
                    - sdtv
                    - 720p

            mock:
              - {title: 'Q Test.S01E02.hdtv-FlexGet'}
              - {title: 'Q Test.S01E02.1080p-FlexGet'}

          test_with_quality_1:
            series:
            - q test:
                timeframe: 5 hours
                quality: hdtv+
                target: 720p
            mock:
            - title: q test s01e01 pdtv 720p

          test_with_quality_2:
            series:
            - q test:
                timeframe: 5 hours
                quality: hdtv+
                target: 720p
            mock:
            - title: q test s01e01 hdtv

    """

    def test_no_waiting(self):
        """Series plugin: no timeframe waiting needed"""
        self.execute_task('test_no_waiting')
        assert self.task.find_entry('accepted', title='Test.S01E01.720p-FlexGet'), \
            '720p not accepted immediattely'

    def test_stop_waiting(self):
        """Series plugin: timeframe quality appears, stop waiting, proper appears"""
        self.execute_task('test_stop_waiting_1')
        assert self.task.entries and not self.task.accepted
        self.execute_task('test_stop_waiting_2')
        assert self.task.find_entry('accepted', title='Test.S01E02.720p-FlexGet'), \
            '720p should have caused stop waiting'
        self.execute_task('test_proper_afterwards')
        assert self.task.find_entry('accepted', title='Test.S01E02.720p.Proper-FlexGet'), \
            'proper should have been accepted'

    def test_expires(self):
        """Series plugin: timeframe expires"""
        # first execution should not accept anything
        self.execute_task('test_expires')
        assert not self.task.accepted

        # let 3 hours pass
        age_series(hours=3)
        self.execute_task('test_expires')
        assert not self.task.accepted, 'expired too soon'

        # let another 3 hours pass, should expire now!
        age_series(hours=6)
        self.execute_task('test_expires')
        assert self.task.accepted, 'timeframe didn\'t expire'

    def test_min_max_fail(self):
        self.execute_task('test_min_max_fail')
        assert not self.task.accepted

        # Let 6 hours pass, timeframe should not even been started, as pdtv doesn't meet min_quality
        age_series(hours=6)
        self.execute_task('test_min_max_fail')
        assert self.task.entries and not self.task.accepted

    def test_min_max_pass(self):
        self.execute_task('test_min_max_pass')
        assert not self.task.accepted

        # Let 6 hours pass, timeframe should expire and accept hdtv copy
        age_series(hours=6)
        self.execute_task('test_min_max_pass')
        assert self.task.find_entry('accepted', title='MM Test.S01E02.hdtv-FlexGet')
        assert len(self.task.accepted) == 1

    def test_qualities_fail(self):
        self.execute_task('test_qualities_fail')
        assert self.task.find_entry('accepted', title='Q Test.S01E02.1080p-FlexGet'),\
            'should have accepted wanted quality'
        assert len(self.task.accepted) == 1

        # Let 6 hours pass, timeframe should not even been started, as we already have one of our qualities
        age_series(hours=6)
        self.execute_task('test_qualities_fail')
        assert self.task.entries and not self.task.accepted

    def test_qualities_pass(self):
        self.execute_task('test_qualities_pass')
        assert not self.task.accepted, 'None of the qualities should have matched'

        # Let 6 hours pass, timeframe should expire and accept 1080p copy
        age_series(hours=6)
        self.execute_task('test_qualities_pass')
        assert self.task.find_entry('accepted', title='Q Test.S01E02.1080p-FlexGet')
        assert len(self.task.accepted) == 1

    def test_with_quality(self):
        self.execute_task('test_with_quality_1')
        assert not self.task.accepted, 'Entry does not pass quality'

        age_series(hours=6)
        # Entry from first test feed should not pass quality
        self.execute_task('test_with_quality_1')
        assert not self.task.accepted, 'Entry does not pass quality'
        # Timeframe should not yet have started
        self.execute_task('test_with_quality_2')
        assert not self.task.accepted, 'Timeframe should not yet have passed'

        age_series(hours=6)
        self.execute_task('test_with_quality_2')
        assert self.task.accepted, 'Timeframe should have passed'


class TestBacklog(FlexGetBase):

    __yaml__ = """
        tasks:
          backlog:
            mock:
              - {title: 'Test.S01E01.hdtv-FlexGet'}
            series:
              - test: {timeframe: 6 hours}
    """

    def testBacklog(self):
        """Series plugin: backlog"""
        self.execute_task('backlog')
        assert self.task.entries and not self.task.accepted, 'no entries at the start'
        # simulate test going away from the task
        del(self.manager.config['tasks']['backlog']['mock'])
        age_series(hours=12)
        self.execute_task('backlog')
        assert self.task.accepted, 'backlog is not injecting episodes'


class TestManipulate(FlexGetBase):

    """Tests that it's possible to manipulate entries before they're parsed by series plugin"""

    __yaml__ = """
        tasks:
          test_1:
            mock:
              - {title: 'PREFIX: Test.S01E01.hdtv-FlexGet'}
            series:
              - test
          test_2:
            mock:
              - {title: 'PREFIX: Test.S01E01.hdtv-FlexGet'}
            series:
              - test
            manipulate:
              - title:
                  extract: '^PREFIX: (.*)'
    """

    def testManipulate(self):
        """Series plugin: test manipulation priority"""
        # should not work with the prefix
        self.execute_task('test_1')
        assert not self.task.accepted, 'series accepted even with prefix?'
        assert not self.task.accepted, 'series rejecte even with prefix?'
        self.execute_task('test_2')
        assert self.task.accepted, 'manipulate failed to pre-clean title'


class TestFromGroup(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: '[Ignored] Test 12'}
              - {title: '[FlexGet] Test 12'}
              - {title: 'Test.13.HDTV-Ignored'}
              - {title: 'Test.13.HDTV-FlexGet'}
              - {title: 'Test.14.HDTV-Name'}
            series:
              - test: {from_group: [Name, FlexGet]}
    """

    def testFromGroup(self):
        """Series plugin: test from_group"""
        self.execute_task('test')
        assert self.task.find_entry('accepted', title='[FlexGet] Test 12')
        assert self.task.find_entry('accepted', title='Test.13.HDTV-FlexGet')
        assert self.task.find_entry('accepted', title='Test.14.HDTV-Name')


class TestBegin(FlexGetBase):

    __yaml__ = """
        templates:
          eps:
            mock:
              - {title: 'WTest.S02E03.HDTV.XViD-FlexGet'}
              - {title: 'W2Test.S02E03.HDTV.XViD-FlexGet'}
        tasks:
          before_ep_test:
            template: eps
            series:
              - WTest:
                  begin: S02E05
              - W2Test:
                  begin: S03E02
          after_ep_test:
            template: eps
            series:
              - WTest:
                  begin: S02E03
              - W2Test:
                  begin: S02E01
          before_seq_test:
            mock:
            - title: WTest.1.HDTV.XViD-FlexGet
            - title: W2Test.13.HDTV.XViD-FlexGet
            series:
              - WTest:
                  begin: 2
              - W2Test:
                  begin: 120
          after_seq_test:
            mock:
            - title: WTest.2.HDTV.XViD-FlexGet
            - title: W2Test.123.HDTV.XViD-FlexGet
            series:
              - WTest:
                  begin: 2
              - W2Test:
                  begin: 120
          before_date_test:
            mock:
            - title: WTest.2001.6.6.HDTV.XViD-FlexGet
            - title: W2Test.12.30.2012.HDTV.XViD-FlexGet
            series:
              - WTest:
                  begin: '2009-05-05'
              - W2Test:
                  begin: '2012-12-31'
          after_date_test:
            mock:
            - title: WTest.2009.5.5.HDTV.XViD-FlexGet
            - title: W2Test.1.1.2013.HDTV.XViD-FlexGet
            series:
              - WTest:
                  begin: '2009-05-05'
              - W2Test:
                  begin: '2012-12-31'
          test_advancement1:
            mock:
            - title: WTest.S01E01
            series:
            - WTest
          test_advancement2:
            mock:
            - title: WTest.S03E01
            series:
            - WTest
          test_advancement3:
            mock:
            - title: WTest.S03E01
            series:
            - WTest:
                begin: S03E01

    """

    def test_before_ep(self):
        self.execute_task('before_ep_test')
        assert not self.task.accepted, 'No entries should have been accepted, they are before the begin episode'

    def test_after_ep(self):
        self.execute_task('after_ep_test')
        assert len(self.task.accepted) == 2, 'Entries should have been accepted, they are not before the begin episode'

    def test_before_seq(self):
        self.execute_task('before_seq_test')
        assert not self.task.accepted, 'No entries should have been accepted, they are before the begin episode'

    def test_after_seq(self):
        self.execute_task('after_seq_test')
        assert len(self.task.accepted) == 2, 'Entries should have been accepted, they are not before the begin episode'

    def test_before_date(self):
        self.execute_task('before_date_test')
        assert not self.task.accepted, 'No entries should have been accepted, they are before the begin episode'

    def test_after_date(self):
        self.execute_task('after_date_test')
        assert len(self.task.accepted) == 2, 'Entries should have been accepted, they are not before the begin episode'

    def test_advancement(self):
        # Put S01E01 into the database as latest download
        self.execute_task('test_advancement1')
        assert self.task.accepted
        # Just verify regular ep advancement would block S03E01
        self.execute_task('test_advancement2')
        assert not self.task.accepted, 'Episode advancement should have blocked'
        # Make sure ep advancement doesn't block it when we've set begin to that ep
        self.execute_task('test_advancement3')
        assert self.task.accepted, 'Episode should have been accepted'


class TestSeriesPremiere(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            metainfo_series: yes
            series_premiere: yes
        tasks:
          test:
            mock:
              - {title: 'Foobar.S01E01.PDTV-FlexGet'}
              - {title: 'Foobar.S01E11.1080p-FlexGet'}
              - {title: 'Foobar.S02E02.HR-FlexGet'}
    """

    def testOnlyPremieres(self):
        """Test series premiere"""
        self.execute_task('test')
        assert self.task.find_entry('accepted', title='Foobar.S01E01.PDTV-FlexGet',
            series_name='Foobar', series_season=1, series_episode=1), 'Series premiere should have been accepted'
        assert len(self.task.accepted) == 1
    # TODO: Add more tests, test interaction with series plugin and series_exists


class TestImportSeries(FlexGetBase):

    __yaml__ = """
        tasks:
          timeframe_max:
            configure_series:
              settings:
                propers: 12 hours
                target: 720p
                timeframe: 5 minutes
                quality: "<=720p <=bluray"
              from:
                mock:
                  - title: the show
            mock:
              - title: the show s03e02 1080p bluray
              - title: the show s03e02 hdtv
          test_import_altnames:
            configure_series:
              from:
                mock:
                  - {title: 'the show', configure_series_alternate_name: 'le show'}
            mock:
              - title: le show s03e03
    """

    def test_timeframe_max(self):
        """Tests configure_series as well as timeframe with max_quality."""
        self.execute_task('timeframe_max')
        assert not self.task.accepted, 'Entry shouldnot have been accepted on first run.'
        age_series(minutes=6)
        self.execute_task('timeframe_max')
        assert self.task.find_entry('accepted', title='the show s03e02 hdtv'), \
                'hdtv should have been accepted after timeframe.'

    def test_import_altnames(self):
        """Tests configure_series with alternate_name."""
        self.execute_task('test_import_altnames')
        entry = self.task.find_entry(title='le show s03e03')
        assert entry.accepted, 'entry matching series alternate name should have been accepted.'
        assert entry['series_name'] == 'the show', 'entry series should be set to the main name'

class TestIDTypes(FlexGetBase):

    __yaml__ = """
        tasks:
          all_types:
            series:
              - episode
              - date
              - sequence
              - stupid id:
                  id_regexp: (\\dcat)
            mock:
              - title: episode S03E04
              - title: episode 3x05
              - title: date 2011.4.3 other crap hdtv
              - title: date 4.5.11
              - title: sequence 003
              - title: sequence 4
              - title: stupid id 3cat
    """

    def test_id_types(self):
        self.execute_task('all_types')
        for entry in self.task.entries:
            assert entry['series_name'], '%s not parsed by series plugin' % entry['title']
            assert entry['series_id_type'] in entry['series_name']


class TestCaseChange(FlexGetBase):

    __yaml__ = """
        tasks:
          first:
            mock:
              - title: theshow s02e04
            series:
              - TheShow
          second:
            mock:
              - title: thEshoW s02e04 other
            series:
              - THESHOW
    """

    def test_case_change(self):
        self.execute_task('first')
        # Make sure series_name uses case from config, make sure episode is accepted
        assert self.task.find_entry('accepted', title='theshow s02e04', series_name='TheShow')
        self.execute_task('second')
        # Make sure series_name uses new case from config, make sure ep is rejected because we have a copy
        assert self.task.find_entry('rejected', title='thEshoW s02e04 other', series_name='THESHOW')


class TestInvalidSeries(FlexGetBase):

    __yaml__ = """
        tasks:
          blank:
            mock:
              - title: whatever
            series:
              - '':
                  quality: 720p
    """

    def test_blank_series(self):
        """Make sure a blank series doesn't crash."""
        self.execute_task('blank')
        assert not self.task.aborted, 'Task should not have aborted'


class TestDoubleEps(FlexGetBase):

    __yaml__ = """
        tasks:
          test_double1:
            mock:
              - title: double S01E02-E03
            series:
              - double
          test_double2:
            mock:
              - title: double S01E03
            series:
              - double

          test_double_prefered:
            mock:
              - title: double S02E03
              - title: double S02E03-04
            series:
              - double
    """

    def test_double(self):
        # First should be accepted
        self.execute_task('test_double1')
        assert self.task.find_entry('accepted', title='double S01E02-E03')

        # We already got ep 3 as part of double, should not be accepted
        self.execute_task('test_double2')
        assert not self.task.find_entry('accepted', title='double S01E03')

    def test_double_prefered(self):
        # Given a choice of single or double ep at same quality, grab the double
        self.execute_task('test_double_prefered')
        assert self.task.find_entry('accepted', title='double S02E03-04')
        assert not self.task.find_entry('accepted', title='S02E03')


class TestAutoLockin(FlexGetBase):
    __yaml__ = """
        templates:
          global:
            series:
            - FooBar
            - BarFood
        tasks:
          try_date_1:
            mock:
            - title: FooBar 2012-10-10 HDTV
          lock_ep:
            mock:
            - title: FooBar S01E01 HDTV
            - title: FooBar S01E02 HDTV
            - title: FooBar S01E03 HDTV
          try_date_2:
            mock:
            - title: FooBar 2012-10-11 HDTV
          test_special_lock:
            mock:
            - title: BarFood christmas special HDTV
            - title: BarFood easter special HDTV
            - title: BarFood haloween special HDTV
            - title: BarFood bad special HDTV
          try_reg:
            mock:
            - title: BarFood S01E01 HDTV
            - title: BarFood 2012-9-9 HDTV

    """

    def test_ep_lockin(self):
        self.execute_task('try_date_1')
        assert self.task.find_entry('accepted', title='FooBar 2012-10-10 HDTV'), \
            'dates should be accepted before locked in on an identifier type'
        self.execute_task('lock_ep')
        assert len(self.task.accepted) == 3, 'All ep mode episodes should have been accepted'
        self.execute_task('try_date_2')
        assert not self.task.find_entry('accepted', title='FooBar 2012-10-11 HDTV'), \
            'dates should not be accepted after series has locked in to ep mode'

    def test_special_lock(self):
        """Make sure series plugin does not lock in to type 'special'"""
        self.execute_task('test_special_lock')
        assert len(self.task.accepted) == 4, 'All specials should have been accepted'
        self.execute_task('try_reg')
        assert len(self.task.accepted) == 2, 'Specials should not have caused episode type lock-in'


class TestReruns(FlexGetBase):
    __yaml__ = """
        tasks:
          one_accept:
            mock:
            - title: the show s01e01
            - title: the show s01e01 different
            series:
            - the show
            rerun: 2
            mock_output: yes
    """

    def test_one_accept(self):
        self.execute_task('one_accept')
        assert len(self.task.mock_output) == 1, \
            'should have accepted once!: %s' % ', '.join(e['title'] for e in self.task.mock_output)


class TestSpecials(FlexGetBase):
    __yaml__ = """
        tasks:
          preferspecials:
            mock:
            - title: the show s03e04 special
            series:
            - the show:
                prefer_specials: True

          nopreferspecials:
            mock:
            - title: the show s03e05 special
            series:
            - the show:
                prefer_specials: False

          assumespecial:
            mock:
            - title: the show SOMETHING
            series:
            - the show:
                assume_special: True

          noassumespecial:
            mock:
            - title: the show SOMETHING
            series:
            - the show:
                assume_special: False
    """

    def test_prefer_specials(self):
        #Test that an entry matching both ep and special is flagged as a special when prefer_specials is True
        self.execute_task('preferspecials')
        entry = self.task.find_entry('accepted', title='the show s03e04 special')
        assert entry.get('series_id_type') == 'special', 'Entry which should have been flagged a special was not.'

    def test_not_prefer_specials(self):
        #Test that an entry matching both ep and special is flagged as an ep when prefer_specials is False
        self.execute_task('nopreferspecials')
        entry = self.task.find_entry('accepted', title='the show s03e05 special')
        assert entry.get('series_id_type') != 'special', 'Entry which should not have been flagged a special was.'

    def test_assume_special(self):
        #Test that an entry with no ID found gets flagged as a special and accepted if assume_special is True
        self.execute_task('assumespecial')
        entry = self.task.find_entry(title='the show SOMETHING')
        assert entry.get('series_id_type') == 'special', 'Entry which should have been flagged as a special was not.'
        assert entry.accepted, 'Entry which should have been accepted was not.'

    def test_not_assume_special(self):
        #Test that an entry with no ID found does not get flagged as a special and accepted if assume_special is False
        self.execute_task('noassumespecial')
        entry = self.task.find_entry(title='the show SOMETHING')
        assert entry.get('series_id_type') != 'special', 'Entry which should not have been flagged as a special was.'
        assert not entry.accepted, 'Entry which should not have been accepted was.'

########NEW FILE########
__FILENAME__ = test_seriesparser
# -*- coding: utf-8 -*-

from __future__ import unicode_literals, division, absolute_import
from nose.tools import assert_raises, raises
from flexget.utils.titles import SeriesParser, ParseWarning

#
# NOTE:
#
# Logging doesn't properly work if you run this test only as it is initialized
# in FlexGetBase which this does NOT use at all. I spent hour debugging why
# logging doesn't work ...
#

# try to get logging running ...
# enable enable_logging and add --nologcapture to nosetest to see debug
# (should not be needed, logging is not initialized properly?)

enable_logging = True

if enable_logging:
    #level = 5
    #import logging
    import flexget.logger
    flexget.logger.initialize(True)
    ##log = logging.getLogger()
    ##log.setLevel(level)
    # switch seriesparser logging to debug
    import tests
    from flexget.utils.titles.series import log as parser_log
    parser_log.setLevel(tests.setup_logging_level())


class TestSeriesParser(object):

    def parse(self, name, data, **kwargs):
        s = SeriesParser(name, **kwargs)
        s.parse(data)
        return s

    def parse_invalid(self, name, data, **kwargs):
        """Makes sure either ParseWarning is raised, or return is invalid."""
        try:
            r = self.parse(name, data, **kwargs)
            assert not r.valid, '{data} should not be valid'.format(data=data)
        except ParseWarning:
            pass

    def test_proper(self):
        """SeriesParser: proper"""
        s = self.parse(name='Something Interesting', data='Something.Interesting.S01E02.Proper-FlexGet')
        assert s.season == 1
        assert s.episode == 2
        assert s.quality.name == 'unknown'
        assert s.proper, 'did not detect proper from %s' % s.data
        s = self.parse(name='foobar', data='foobar 720p proper s01e01')
        assert s.proper, 'did not detect proper from %s' % s.data

    def test_non_proper(self):
        """SeriesParser: non-proper"""
        s = self.parse(name='Something Interesting', data='Something.Interesting.S01E02-FlexGet')
        assert s.season == 1
        assert s.episode == 2
        assert s.quality.name == 'unknown'
        assert not s.proper, 'detected proper'

    def test_anime_proper(self):
        """SeriesParser: anime fansub style proper (13v2)"""
        s = self.parse(name='Anime', data='[aoeu] Anime 19v2 [23BA98]')
        assert s.identifier == 19
        assert s.proper_count == 1
        s = self.parse(name='Anime', data='Anime_-_19v3')
        assert s.identifier == 19
        assert s.proper_count == 2

    def test_basic(self):
        """SeriesParser: basic parsing"""
        s = self.parse(name='Something Interesting', data='The.Something.Interesting.S01E02-FlexGet')
        assert not s.valid, 'Should not be valid'

        s = self.parse(name='25', data='25.And.More.S01E02-FlexGet')
        assert s.valid, 'Fix the implementation, should be valid'
        assert s.identifier == 'S01E02', 'identifier broken'

    @raises(Exception)
    def test_invalid_name(self):
        """SeriesParser: invalid name"""
        s = SeriesParser()
        s.name = 1
        s.data = 'Something'

    @raises(Exception)
    def test_invalid_data(self):
        """SeriesParser: invalid data"""
        s = SeriesParser()
        s.name = 'Something Interesting'
        s.data = 1

    def test_confusing_date(self):
        """SeriesParser: confusing (invalid) numbering scheme"""
        s = self.parse(name='Something', data='Something.2008x12.13-FlexGet')
        assert not s.episode, 'Should not have episode'
        assert not s.season, 'Should not have season'
        assert s.id_type == 'date'
        assert s.identifier == '2008-12-13', 'invalid id'
        assert s.valid, 'should be valid'

    def test_unwanted_disc(self):
        """SeriesParser: unwanted disc releases"""
        self.parse_invalid(name='Something', data='Something.S01D2.DVDR-FlexGet')

    def test_season_x_ep(self):
        """SeriesParser: 01x02"""
        s = self.parse(name='Something', data='Something.01x02-FlexGet')
        assert (s.season == 1 and s.episode == 2), 'failed to parse 01x02'

        s = self.parse(name='Something', data='Something 1 x 2-FlexGet')
        assert (s.season == 1 and s.episode == 2), 'failed to parse 1 x 2'

        # Ticket #732
        s = self.parse(name='Something', data='Something - This is the Subtitle 14x9 [Group-Name]')
        assert (s.season == 14 and s.episode == 9), 'failed to parse %s' % s.data

    def test_ep_in_square_brackets(self):
        """SeriesParser: [S01] [E02] NOT IMPLEMENTED"""
        return

        # FIX: #402 .. a bit hard to do
        s = self.parse(name='Something', data='Something [S01] [E02]')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

    def test_ep_in_parenthesis(self):
        """SeriesParser: test ep in parenthesis"""
        s = self.parse(name='Something', data='Something (S01E02)')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

    def test_season_episode(self):
        """SeriesParser: season X, episode Y"""
        s = self.parse(name='Something', data='Something - Season 3, Episode 2')
        assert (s.season == 3 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Season2, Episode2')
        assert (s.season == 2 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Season2 Episode2')
        assert (s.season == 2 and s.episode == 2), 'failed to parse %s' % s

    def test_series_episode(self):
        """SeriesParser: series X, episode Y"""
        s = self.parse(name='Something', data='Something - Series 2, Episode 2')
        assert (s.season == 2 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Series3, Episode2')
        assert (s.season == 3 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Series4 Episode2')
        assert (s.season == 4 and s.episode == 2), 'failed to parse %s' % s

    def test_episode(self):
        """SeriesParser: episode X (assume season 1)"""
        s = self.parse(name='Something', data='Something - Episode2')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Episode 2')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Episode VIII')
        assert (s.season == 1 and s.episode == 8), 'failed to parse %s' % s

    def test_ep(self):
        """SeriesParser: ep X (assume season 1)"""
        s = self.parse(name='Something', data='Something - Ep2')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Ep 2')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something - Ep VIII')
        assert (s.season == 1 and s.episode == 8), 'failed to parse %s' % s

    def test_season_episode_of_total(self):
        """SeriesParser: season X YofZ"""
        s = self.parse(name='Something', data='Something Season 2 2of12')
        assert (s.season == 2 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something Season 2, 2 of 12')
        assert (s.season == 2 and s.episode == 2), 'failed to parse %s' % s

    def test_episode_of_total(self):
        """SeriesParser: YofZ (assume season 1)"""
        s = self.parse(name='Something', data='Something 2of12')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

        s = self.parse(name='Something', data='Something 2 of 12')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s

    def test_part(self):
        """SeriesParser: test parsing part numeral (assume season 1)"""
        s = self.parse(name='Test', data='Test.Pt.I.720p-FlexGet')
        assert (s.season == 1 and s.episode == 1), 'failed to parse %s' % s
        s = self.parse(name='Test', data='Test.Pt.VI.720p-FlexGet')
        assert (s.season == 1 and s.episode == 6), 'failed to parse %s' % s
        s = self.parse(name='Test', data='Test.Part.2.720p-FlexGet')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s
        s = self.parse(name='Test', data='Test.Part3.720p-FlexGet')
        assert (s.season == 1 and s.episode == 3), 'failed to parse %s' % s
        s = self.parse(name='Test', data='Test.Season.3.Part.IV')
        assert (s.season == 3 and s.episode == 4), 'failed to parse %s' % s
        s = self.parse(name='Test', data='Test.Part.One')
        assert (s.season == 1 and s.episode == 1), 'failed to parse %s' % s

    def test_digits(self):
        """SeriesParser: digits (UID)"""
        s = self.parse(name='Something', data='Something 01 FlexGet')
        assert (s.id == 1), 'failed to parse %s' % s.data
        assert s.id_type == 'sequence'

        s = self.parse(name='Something', data='Something-121.H264.FlexGet')
        assert (s.id == 121), 'failed to parse %s' % s.data
        assert s.id_type == 'sequence'

        s = self.parse(name='Something', data='Something 1 AC3')
        assert (s.id == 1), 'failed to parse %s' % s.data
        assert s.id_type == 'sequence'

        s = self.parse(name='Something', data='[TheGroup] Something - 12 1280x720 x264-Hi10P')
        assert (s.id == 12), 'failed to parse %s' % s.data
        assert s.id_type == 'sequence'

    def test_quality(self):
        """SeriesParser: quality"""
        s = self.parse(name='Foo Bar', data='Foo.Bar.S01E01.720p.HDTV.x264-FlexGet')
        assert (s.season == 1 and s.episode == 1), 'failed to parse episodes from %s' % s.data
        assert (s.quality.name == '720p hdtv h264'), 'failed to parse quality from %s' % s.data

        s = self.parse(name='Test', data='Test.S01E01.720p-FlexGet')
        assert s.quality.name == '720p', 'failed to parse quality from %s' % s.data

        s = self.parse(name='30 Suck', data='30 Suck 4x4 [HDTV - FlexGet]')
        assert s.quality.name == 'hdtv', 'failed to parse quality %s' % s.data

        s = self.parse(name='ShowB', data='ShowB.S04E19.Name of Ep.720p.WEB-DL.DD5.1.H.264')
        assert s.quality.name == '720p webdl h264 dd5.1', 'failed to parse quality %s' % s.data

    def test_quality_parenthesis(self):
        """SeriesParser: quality in parenthesis"""
        s = self.parse(name='Foo Bar', data='Foo.Bar.S01E01.[720p].HDTV.x264-FlexGet')
        assert (s.season == 1 and s.episode == 1), 'failed to parse episodes from %s' % s.data
        assert (s.quality.name == '720p hdtv h264'), 'failed to parse quality from %s' % s.data

        s = self.parse(name='Foo Bar', data='Foo.Bar.S01E01.(720p).HDTV.x264-FlexGet')
        assert (s.season == 1 and s.episode == 1), 'failed to parse episodes from %s' % s.data
        assert (s.quality.name == '720p hdtv h264'), 'failed to parse quality from %s' % s.data

        s = self.parse(name='Foo Bar', data='[720p]Foo.Bar.S01E01.HDTV.x264-FlexGet')
        assert (s.season == 1 and s.episode == 1), 'failed to parse episodes from %s' % s.data
        assert (s.quality.name == '720p hdtv h264'), 'failed to parse quality from %s' % s.data

    def test_numeric_names(self):
        """SeriesParser: numeric names (24)"""
        s = self.parse(name='24', data='24.1x2-FlexGet')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s.data

        s = self.parse(name='90120', data='90120.1x2-FlexGet')
        assert (s.season == 1 and s.episode == 2), 'failed to parse %s' % s.data

    def test_group_prefix(self):
        """SeriesParser: [group] before name"""
        s = self.parse(name='Foo Bar', data='[l.u.l.z] Foo Bar - 11 (H.264) [5235532D].mkv')
        assert (s.id == 11), 'failed to parse %s' % s.data

        s = self.parse(name='Foo Bar', data='[7.1.7.5] Foo Bar - 11 (H.264) [5235532D].mkv')
        assert (s.id == 11), 'failed to parse %s' % s.data

    def test_hd_prefix(self):
        """SeriesParser: HD 720p before name"""
        s = self.parse(name='Foo Bar', data='HD 720p: Foo Bar - 11 (H.264) [5235532D].mkv')
        assert (s.id == 11), 'failed to parse %s' % s.data
        assert (s.quality.name == '720p h264'), 'failed to pick up quality'

    def test_partially_numeric(self):
        """SeriesParser: partially numeric names"""
        s = self.parse(name='Foo 2009', data='Foo.2009.S02E04.HDTV.XviD-2HD[FlexGet]')
        assert (s.season == 2 and s.episode == 4), 'failed to parse %s' % s.data
        assert (s.quality.name == 'hdtv xvid'), 'failed to parse quality from %s' % s.data

    def test_ignore_seasonpacks(self):
        """SeriesParser: ignoring season packs"""
        #self.parse_invalid(name='The Foo', data='The.Foo.S04.1080p.FlexGet.5.1')
        self.parse_invalid(name='The Foo', data='The Foo S05 720p BluRay DTS x264-FlexGet')
        self.parse_invalid(name='The Foo', data='The Foo S05 720p BluRay DTS x264-FlexGet')
        self.parse_invalid(name='Something', data='Something S02 Pack 720p WEB-DL-FlexGet')
        self.parse_invalid(name='Something', data='Something S06 AC3-CRAPL3SS')
        self.parse_invalid(name='Something', data='Something SEASON 1 2010 540p BluRay QEBS AAC ANDROID IPAD MP4 FASM')
        self.parse_invalid(name='Something', data='Something.1x0.Complete.Season-FlexGet')
        self.parse_invalid(name='Something', data='Something.1xAll.Season.Complete-FlexGet')
        self.parse_invalid(name='Something', data='Something Seasons 1 & 2 - Complete')
        self.parse_invalid(name='Something', data='Something Seasons 4 Complete')
        self.parse_invalid(name='Something', data='Something Seasons 1 2 3 4')
        self.parse_invalid(name='Something', data='Something S6 E1-4')
        self.parse_invalid(name='Something', data='Something_Season_1_Full_Season_2_EP_1-7_HD')
        self.parse_invalid(name='Something', data='Something - Season 10 - FlexGet')
        self.parse_invalid(name='Something', data='Something_ DISC_1_OF_2 MANofKENT INVICTA RG')
        # Make sure no false positives
        assert self.parse(name='Something', data='Something S01E03 Full Throttle').valid


    def test_similar(self):
        s = self.parse(name='Foo Bar', data='Foo.Bar:Doppelganger.S02E04.HDTV.FlexGet', strict_name=True)
        assert not s.valid, 'should not have parser Foo.Bar:Doppelganger'
        s = self.parse(name='Foo Bar', data='Foo.Bar.Doppelganger.S02E04.HDTV.FlexGet', strict_name=True)
        assert not s.valid, 'should not have parser Foo.Bar.Doppelganger'

    def test_idiotic_numbering(self):
        """SeriesParser: idiotic 101, 102, 103, .. numbering"""
        s = SeriesParser(name='test', identified_by='ep')
        s.parse('Test.706.720p-FlexGet')
        assert s.season == 7, 'didn\'t pick up season'
        assert s.episode == 6, 'didn\'t pick up episode'

    def test_idiotic_numbering_with_zero(self):
        """SeriesParser: idiotic 0101, 0102, 0103, .. numbering"""
        s = SeriesParser(name='test', identified_by='ep')
        s.parse('Test.0706.720p-FlexGet')
        assert s.season == 7, 'season missing'
        assert s.episode == 6, 'episode missing'
        assert s.identifier == 'S07E06', 'identifier broken'

    def test_idiotic_invalid(self):
        """SeriesParser: idiotic confused by invalid"""
        s = SeriesParser(name='test', identified_by='ep')
        s.data = 'Test.Revealed.WS.PDTV.XviD-aAF.5190458.TPB.torrent'
        assert_raises(ParseWarning, s.parse)
        assert not s.season == 5, 'confused, got season'
        assert not s.season == 4, 'confused, got season'
        assert not s.episode == 19, 'confused, got episode'
        assert not s.episode == 58, 'confused, got episode'

    def test_zeroes(self):
        """SeriesParser: test zeroes as a season, episode"""

        for data in ['Test.S00E00-FlexGet', 'Test.S00E01-FlexGet', 'Test.S01E00-FlexGet']:
            s = self.parse(name='Test', data=data)
            id = s.identifier
            assert s.valid, 'parser not a valid for %s' % data
            assert isinstance(id, basestring), 'id is not a string for %s' % data
            assert isinstance(s.season, int), 'season is not a int for %s' % data
            assert isinstance(s.episode, int), 'season is not a int for %s' % data

    def test_exact_name(self):
        """SeriesParser: test exact/strict name parsing"""

        s = SeriesParser()
        s.name = 'test'
        s.data = 'Test.Foobar.S01E02.720p-FlexGet'
        s.parse()
        assert s.valid, 'normal failed'

        s = SeriesParser()
        s.strict_name = True
        s.name = 'test'
        s.data = 'Test.A.S01E02.720p-FlexGet'
        s.parse()
        assert not s.valid, 'strict A failed'

        s = SeriesParser()
        s.strict_name = True
        s.name = 'Test AB'
        s.data = 'Test.AB.S01E02.720p-FlexGet'
        s.parse()
        assert s.valid, 'strict AB failed'

        s = SeriesParser()
        s.strict_name = True
        s.name = 'Red Tomato'
        s.data = 'Red Tomato (US) S01E02 720p-FlexGet'
        s.parse()
        assert not s.valid, 'Red Tomato (US) should not match Red Tomato in exact mode'

    def test_name_word_boundries(self):
        s = SeriesParser(name='test')
        s.parse('Test.S01E02.720p-FlexGet')
        assert s.valid, 'normal failed'
        # In non-exact mode these should match
        s.parse('Test.crap.S01E02.720p-FlexGet')
        assert s.valid, 'normal failed'
        s.parse('Test_crap.S01E02.720p-FlexGet')
        assert s.valid, 'underscore failed'
        # However if the title ends mid-word, it should not match
        s.parse('Testing.S01E02.720p-FlexGet')
        assert not s.valid, 'word border failed'

    def test_quality_as_ep(self):
        """SeriesParser: test that qualities are not picked as ep"""
        from flexget.utils import qualities
        for quality in qualities.all_components():
            s = SeriesParser(name='FooBar')
            s.data = 'FooBar %s XviD-FlexGet' % quality.name
            assert_raises(ParseWarning, s.parse)

    def test_sound_as_ep(self):
        """SeriesParser: test that sound infos are not picked as ep"""
        for sound in SeriesParser.sounds:
            s = SeriesParser()
            s.name = 'FooBar'
            s.data = 'FooBar %s XViD-FlexGet' % sound
            assert_raises(ParseWarning, s.parse)

    def test_name_with_number(self):
        """SeriesParser: test number in a name"""
        s = SeriesParser()
        s.name = 'Storage 13'
        s.data = 'Storage 13 no ep number'
        assert_raises(ParseWarning, s.parse)

    def test_name_uncorrupted(self):
        """SeriesParser: test name doesn't get corrupted when cleaned"""
        s = self.parse(name='The New Adventures of Old Christine',
                       data='The.New.Adventures.of.Old.Christine.S05E16.HDTV.XviD-FlexGet')
        assert s.name == 'The New Adventures of Old Christine'
        assert s.season == 5
        assert s.episode == 16
        assert s.quality.name == 'hdtv xvid'

    def test_from_groups(self):
        """SeriesParser: test from groups"""
        s = SeriesParser()
        s.name = 'Test'
        s.data = 'Test.S01E01-Group'
        s.allow_groups = ['xxxx', 'group']
        s.parse()
        assert s.group == 'group', 'did not get group'

    def test_group_dashes(self):
        """SeriesParser: group name around extra dashes"""
        s = SeriesParser()
        s.name = 'Test'
        s.data = 'Test.S01E01-FooBar-Group'
        s.allow_groups = ['xxxx', 'group']
        s.parse()
        assert s.group == 'group', 'did not get group with extra dashes'

    def test_id_and_hash(self):
        """SeriesParser: Series with confusing hash"""
        s = self.parse(name='Something', data='Something 63 [560D3414]')
        assert (s.id == 63), 'failed to parse %s' % s.data

        s = self.parse(name='Something', data='Something 62 [293A8395]')
        assert (s.id == 62), 'failed to parse %s' % s.data

    def test_ticket_700(self):
        """SeriesParser: confusing name (#700)"""
        s = self.parse(name='Something', data='Something 9x02 - Episode 2')
        assert s.season == 9, 'failed to parse season'
        assert s.episode == 2, 'failed to parse episode'

    def test_date_id(self):
        """SeriesParser: Series with dates"""
        s = self.parse(name='Something', data='Something.2010.10.25')
        assert (s.identifier == '2010-10-25'), 'failed to parse %s' % s.data
        assert s.id_type == 'date'

        s = self.parse(name='Something', data='Something 2010-10-25')
        assert (s.identifier == '2010-10-25'), 'failed to parse %s' % s.data
        assert s.id_type == 'date'

        s = self.parse(name='Something', data='Something 10/25/2010')
        assert (s.identifier == '2010-10-25'), 'failed to parse %s' % s.data
        assert s.id_type == 'date'

        s = self.parse(name='Something', data='Something 25.10.2010')
        assert (s.identifier == '2010-10-25'), 'failed to parse %s' % s.data
        assert s.id_type == 'date'

        # February 1 is picked rather than January 2 because it is closer to now
        s = self.parse(name='Something', data='Something 1.2.11')
        assert s.identifier == '2011-02-01', 'failed to parse %s' % s.data
        assert s.id_type == 'date'

        # Future dates should not be considered dates
        s = self.parse(name='Something', data='Something 01.02.32')
        assert s.id_type != 'date'

        # Dates with parts used to be parsed as episodes.
        s = self.parse(name='Something', data='Something.2010.10.25, Part 2')
        assert (s.identifier == '2010-10-25'), 'failed to parse %s' % s.data
        assert s.id_type == 'date'

    def test_date_options(self):
        # By default we should pick the latest interpretation
        s = self.parse(name='Something', data='Something 01-02-03')
        assert (s.identifier == '2003-02-01'), 'failed to parse %s' % s.data

        # Test it still works with both options specified
        s = self.parse(name='Something', data='Something 01-02-03', date_yearfirst=False, date_dayfirst=True)
        assert (s.identifier == '2003-02-01'), 'failed to parse %s' % s.data

        # If we specify yearfirst yes it should force another interpretation
        s = self.parse(name='Something', data='Something 01-02-03', date_yearfirst=True)
        assert (s.identifier == '2001-02-03'), 'failed to parse %s' % s.data

        # If we specify dayfirst no it should force the third interpretation
        s = self.parse(name='Something', data='Something 01-02-03', date_dayfirst=False)
        assert (s.identifier == '2003-01-02'), 'failed to parse %s' % s.data

    def test_season_title_episode(self):
        """SeriesParser: Series with title between season and episode"""
        s = self.parse(name='Something', data='Something.S5.Drunk.Santa.Part1')
        assert s.season == 5, 'failed to parse season'
        assert s.episode == 1, 'failed to parse episode'

    def test_specials(self):
        """SeriesParser: Special episodes with no id"""
        s = self.parse(name='The Show', data='The Show 2005 A Christmas Carol 2010 Special 720p HDTV x264')
        assert s.valid, 'Special episode should be valid'

    def test_double_episodes(self):
        s = self.parse(name='Something', data='Something.S04E05-06')
        assert s.season == 4, 'failed to parse season'
        assert s.episode == 5, 'failed to parse episode'
        assert s.episodes == 2, 'failed to parse episode range'
        s = self.parse(name='Something', data='Something.S04E05-E06')
        assert s.season == 4, 'failed to parse season'
        assert s.episode == 5, 'failed to parse episode'
        assert s.episodes == 2, 'failed to parse episode range'
        s = self.parse(name='Something', data='Something.S04E05E06')
        assert s.season == 4, 'failed to parse season'
        assert s.episode == 5, 'failed to parse episode'
        assert s.episodes == 2, 'failed to parse episode range'
        s = self.parse(name='Something', data='Something.4x05-06')
        assert s.season == 4, 'failed to parse season'
        assert s.episode == 5, 'failed to parse episode'
        assert s.episodes == 2, 'failed to parse episode range'
        # Test that too large a range is not accepted
        s = self.parse(name='Something', data='Something.S04E05E09')
        assert s.valid == False, 'large episode range should not be valid'
        # Make sure regular identifier doesn't have end_episode
        s = self.parse(name='Something', data='Something.S04E05')
        assert s.episodes == 1, 'should not have detected end_episode'

    def test_and_replacement(self):
        titles = ['Alpha.&.Beta.S01E02.hdtv', 'alpha.and.beta.S01E02.hdtv', 'alpha&beta.S01E02.hdtv']
        for title in titles:
            s = self.parse(name='Alpha & Beta', data=title)
            assert s.valid
            s = self.parse(name='Alpha and Beta', data=title)
            assert s.valid
        # Test 'and' isn't replaced within a word
        s = self.parse(name='Sandy Dunes', data='S&y Dunes.S01E01.hdtv')
        assert not s.valid

    def test_unicode(self):
        s = self.parse(name=u'abc äää abc', data=u'abc.äää.abc.s01e02')
        assert s.season == 1
        assert s.episode == 2

    def test_parentheticals(self):
        s = SeriesParser('The Show (US)')
        # Make sure US is ok outside of parentheses
        s.parse('The.Show.US.S01E01')
        assert s.valid
        # Make sure US is ok inside parentheses
        s.parse('The Show (US) S01E01')
        assert s.valid
        # Make sure it works without US
        s.parse('The.Show.S01E01')
        assert s.valid
        # Make sure it doesn't work with a different country
        s.parse('The Show (UK) S01E01')
        assert not s.valid

    def test_id_regexps(self):
        s = SeriesParser('The Show', id_regexps=['(dog)?e(cat)?'])
        s.parse('The Show dogecat')
        assert s.valid
        assert s.id == 'dog-cat'
        s.parse('The Show doge')
        assert s.valid
        assert s.id == 'dog'
        s.parse('The Show ecat')
        assert s.valid
        assert s.id == 'cat'
        assert_raises(ParseWarning, s.parse, 'The Show e')

    def test_apostrophe(self):
        s = self.parse(name=u"FlexGet's show", data=u"FlexGet's show s01e01")
        assert s.valid
        s = self.parse(name=u"FlexGet's show", data=u"FlexGets show s01e01")
        assert s.valid
        s = self.parse(name=u"FlexGet's show", data=u"FlexGet s show s01e01")
        assert s.valid
        s = self.parse(name=u"FlexGet's show", data=u"FlexGet show s01e01")
        assert not s.valid
        # bad data with leftover escaping
        s = self.parse(name=u"FlexGet's show", data=u"FlexGet\\'s show s01e01")
        assert s.valid

    def test_alternate_names(self):
        s = SeriesParser('The Show', alternate_names=['Show', 'Completely Different'])
        s.parse('The Show S01E01')
        assert s.valid
        s.parse('Show S01E01')
        assert s.valid
        s.parse('Completely.Different.S01E01')
        assert s.valid
        s.parse('Not The Show S01E01')
        assert not s.valid

    def test_long_season(self):
        """SeriesParser: long season ID Ticket #2197"""
        s = self.parse(name='FlexGet', data='FlexGet.US.S2013E14.Title.Here.720p.HDTV.AAC5.1.x264-NOGRP')
        assert s.season == 2013
        assert s.episode == 14
        assert s.quality.name == '720p hdtv h264 aac'
        assert not s.proper, 'detected proper'

        s = self.parse(name='FlexGet', data='FlexGet.Series.2013.14.of.21.Title.Here.720p.HDTV.AAC5.1.x264-NOGRP')
        assert s.season == 2013
        assert s.episode == 14
        assert s.quality.name == '720p hdtv h264 aac'
        assert not s.proper, 'detected proper'

########NEW FILE########
__FILENAME__ = test_series_premiere
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestSeriesPremiere(FlexGetBase):

    __yaml__ = """

        templates:
          global: # just cleans log a bit ..
            disable_builtins:
              - seen

        tasks:
          test_only_one:
            mock:
              - title: Foo's.&.Bar's.2009.S01E01.HDTV.XviD-2HD[FlexGet]
              - title: Foos and Bars 2009 S01E01 HDTV XviD-2HD[ASDF]
              - title: Foo's &amp; Bars (2009) S01E01 720p XviD-2HD[AOEU]
              - title: Foos&bars-2009-S01E01 1080p x264

              - title: Foos and Bars 2009 S01E02 HDTV Xvid-2HD[AOEU]
            series_premiere: yes

          test_dupes_across_tasks_1:
            mock:
              - {title: 'Foo.Bar.2009.S01E01.HDTV.XviD-2HD[FlexGet]'}
            series_premiere: yes

          test_dupes_across_tasks_2:
            mock:
              - {title: 'foo bar (2009) s01e01 dsr xvid-2hd[dmg]'}
            series_premiere: yes
          test_path_set:
            mock:
              - {title: 'foo bar s01e01 hdtv'}
            series_premiere:
              path: .
          test_pilot_and_premiere:
            mock:
              - {title: 'foo bar s01e00 hdtv'}
              - {title: 'foo bar s01e01 hdtv'}
            series_premiere: yes
          test_no_teasers:
            mock:
              - {title: 'foo bar s01e00 hdtv'}
              - {title: 'foo bar s01e01 hdtv'}
            series_premiere:
              allow_teasers: no
          test_multi_episode:
            mock:
              - {title: 'foo bar s01e01e02 hdtv'}
            series_premiere: yes
          test_rerun:
            mock:
              - title: theshow s01e01
              - title: theshow s01e02
            series_premiere: yes
            rerun: yes
          test_no_configured_1:
            series:
            - explicit show
          test_no_configured_2:
            series_premiere: yes
            mock:
            - title: explicit show s01e01
            - title: other show s01e01
    """

    def test_only_one(self):
        self.execute_task('test_only_one')
        assert len(self.task.accepted) == 1, 'should only have accepted one'
        assert not self.task.find_entry('accepted', title='Foos and Bars 2009 S01E02 HDTV Xvid-2HD[AOEU]'), \
            'Non premiere accepted'

    def test_dupes_across_tasks(self):
        self.execute_task('test_dupes_across_tasks_1')
        assert len(self.task.accepted) == 1, 'didn\'t accept first premiere'
        self.execute_task('test_dupes_across_tasks_2')
        assert len(self.task.accepted) == 0, 'accepted duplicate premiere'

    def test_path_set(self):
        self.execute_task('test_path_set')
        assert self.task.find_entry(title='foo bar s01e01 hdtv', path='.')

    def test_pilot_and_premiere(self):
        self.execute_task('test_pilot_and_premiere')
        assert len(self.task.accepted) == 2, 'should have accepted pilot and premiere'

    def test_no_teasers(self):
        self.execute_task('test_no_teasers')
        assert len(self.task.accepted) == 1, 'should have accepted only premiere'
        assert not self.task.find_entry('accepted', title='foo bar s01e00 hdtv')

    def test_multi_episode(self):
        self.execute_task('test_multi_episode')
        assert len(self.task.accepted) == 1, 'should have accepted multi-episode premiere'

    def test_rerun(self):
        self.execute_task('test_rerun')
        assert not self.task.find_entry('accepted', title='theshow s01e02'), 'accepted non-premiere'

    def test_no_configured_shows(self):
        self.execute_task('test_no_configured_1')
        self.execute_task('test_no_configured_2')
        entry = self.task.find_entry(title='explicit show s01e01')
        assert not entry.accepted
        entry = self.task.find_entry(title='other show s01e01')
        assert entry.accepted

########NEW FILE########
__FILENAME__ = test_simple_persistence
from __future__ import unicode_literals, division, absolute_import

from flexget.manager import Session
from flexget.utils.simple_persistence import SimplePersistence
from tests import FlexGetBase


class TestSimplePersistence(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'irrelevant'}
    """

    def test_setdefault(self):
        self.execute_task('test')

        task = self.task

        value1 = task.simple_persistence.setdefault('test', 'abc')
        value2 = task.simple_persistence.setdefault('test', 'def')

        assert value1 == value2, 'set default broken'

    def test_nosession(self):
        persist = SimplePersistence('testplugin')
        persist['aoeu'] = 'test'
        assert persist['aoeu'] == 'test'
        # Make sure it commits and actually persists
        persist = SimplePersistence('testplugin')
        assert persist['aoeu'] == 'test'

    def test_withsession(self):
        session = Session()
        persist = SimplePersistence('testplugin', session=session)
        persist['aoeu'] = 'test'
        assert persist['aoeu'] == 'test'
        # Make sure it didn't commit or close our session
        session.rollback()
        assert 'aoeu' not in persist

########NEW FILE########
__FILENAME__ = test_sort_by
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestSortBy(FlexGetBase):

    __yaml__ = """
        tasks:
          test1:
            sort_by: title
            mock:
              - {title: 'B C D', url: 'http://localhost/1'}
              - {title: 'A B C', url: 'http://localhost/2'}
              - {title: 'A P E', url: 'http://localhost/3'}
          test2:
            sort_by:
              field: title
              reverse: true
            mock:
              - {title: 'B C D', url: 'http://localhost/1'}
              - {title: 'A B C', url: 'http://localhost/2'}
              - {title: 'A P E', url: 'http://localhost/3'}
          test3:
            sort_by:
                reverse: true
            mock:
              - {title: 'B C D', url: 'http://localhost/1'}
              - {title: 'A B C', url: 'http://localhost/2'}
              - {title: 'A P E', url: 'http://localhost/3'}
          test_quality:
            sort_by:
              field: quality
              reverse: true
            mock:
              - {title: 'Test.720p'}
              - {title: 'Test.hdtv'}
              - {title: 'Test.1080p'}

    """

    def test_sort_by_title(self):
        self.execute_task('test1')
        assert self.task.entries[0]['title'] == 'A B C', 'Entries sorted alphabetically by title'
        assert self.task.entries[1]['title'] == 'A P E', 'Entries sorted alphabetically by title'
        assert self.task.entries[2]['title'] == 'B C D', 'Entries sorted alphabetically by title'

    def test_sort_by_title_reverse(self):
        self.execute_task('test2')
        assert self.task.entries[0]['title'] == 'B C D', 'Entries sorted alphabetically by title'
        assert self.task.entries[1]['title'] == 'A P E', 'Entries sorted alphabetically by title'
        assert self.task.entries[2]['title'] == 'A B C', 'Entries sorted alphabetically by title'

    def test_sort_by_reverse(self):
        self.execute_task('test3')
        assert self.task.entries[0]['title'] == 'A P E', 'Entries sorted alphabetically by title'
        assert self.task.entries[1]['title'] == 'A B C', 'Entries sorted alphabetically by title'
        assert self.task.entries[2]['title'] == 'B C D', 'Entries sorted alphabetically by title'

    def test_quality_sort(self):
        self.execute_task('test_quality')
        assert self.task.entries[0]['title'] == 'Test.1080p', 'Entries should be sorted by descending quality'
        assert self.task.entries[1]['title'] == 'Test.720p', 'Entries should be sorted by descending quality'
        assert self.task.entries[2]['title'] == 'Test.hdtv', 'Entries should be sorted by descending quality'

########NEW FILE########
__FILENAME__ = test_template
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestTemplate(FlexGetBase):
    __yaml__ = """
        templates:
          global:
            mock:
              - {title: 'global'}
          movies:
            mock:
              - {title: 'movies'}
          a:
            mock:
              - {title: 'a'}
            template: b
          b:
            mock:
              - {title: 'b'}

        tasks:
          test1:
            template: movies

          test2:
            template: no

          test3:
            template:
              - movies
              - no_global

          test_nested:
            template:
              - a
              - no_global
    """

    def test_preset1(self):
        self.execute_task('test1')
        assert self.task.find_entry(title='global'), 'test1, preset global not applied'
        assert self.task.find_entry(title='movies'), 'test1, preset movies not applied'

    def test_preset2(self):
        self.execute_task('test2')
        assert not self.task.find_entry(title='global'), 'test2, preset global applied'
        assert not self.task.find_entry(title='movies'), 'test2, preset movies applied'

    def test_preset3(self):
        self.execute_task('test3')
        assert not self.task.find_entry(title='global'), 'test3, preset global applied'
        assert self.task.find_entry(title='movies'), 'test3, preset movies not applied'

    def test_nested(self):
        self.execute_task('test_nested')
        assert self.task.find_entry(title='a'), 'Entry from preset a was not created'
        assert self.task.find_entry(title='b'), 'Entry from preset b was not created'
        assert len(self.task.entries) == 2, 'Should only have been 2 entries created'


class TestTemplateMerge(FlexGetBase):

    __yaml__ = """
        templates:
          movies:
            seen_movies: strict
            imdb:
              min_score: 6.0
              min_votes: 500
              min_year: 2006
              reject_genres:
                - musical
                - music
                - biography
                - romance

        tasks:
          test:
            template: movies
            imdb:
              min_score: 6.5
              reject_genres:
                - comedy
    """

    def test_merge(self):
        self.execute_task('test')
        assert self.task.config['imdb']['min_score'] == 6.5, 'float merge failed'
        assert 'comedy' in self.task.config['imdb']['reject_genres'], 'list merge failed'


class TestTemplateRerun(FlexGetBase):
    __yaml__ = """
        templates:
          a:
            series:
            - someseries
        tasks:
          test_rerun:
            template: a
            rerun: 1
    """

    def test_rerun(self):
        self.execute_task('test_rerun')
        assert len(self.task.config['series']) == 1

########NEW FILE########
__FILENAME__ = test_thetvdb
from __future__ import unicode_literals, division, absolute_import
from nose.plugins.attrib import attr
from flexget.manager import Session
from flexget.plugins.api_tvdb import lookup_episode
from tests import FlexGetBase


class TestThetvdbLookup(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            thetvdb_lookup: yes
            # Access a tvdb field to cause lazy loading to occur
            set:
              afield: "{{ tvdb_id }}{{ tvdb_ep_name }}"
        tasks:
          test:
            mock:
              - {title: 'House.S01E02.HDTV.XViD-FlexGet'}
              - {title: 'Doctor.Who.2005.S02E03.PDTV.XViD-FlexGet'}
            series:
              - House
              - Doctor Who 2005
          test_unknown_series:
            mock:
              - {title: 'Aoeu.Htns.S01E01.htvd'}
            series:
              - Aoeu Htns
          test_mark_expired:
            mock:
              - {title: 'House.S02E02.hdtv'}
            metainfo_series: yes
            accept_all: yes
            disable_builtins: [seen]
          test_date:
            mock:
              - title: the daily show 2012-6-6
            series:
              - the daily show (with jon stewart)
          test_absolute:
            mock:
              - title: naruto 128
            series:
              - naruto

    """

    @attr(online=True)
    def test_lookup(self):
        """thetvdb: Test Lookup (ONLINE)"""
        self.execute_task('test')
        entry = self.task.find_entry(title='House.S01E02.HDTV.XViD-FlexGet')
        assert entry['tvdb_ep_name'] == 'Paternity', \
            '%s tvdb_ep_name should be Paternity' % entry['title']
        assert entry['tvdb_status'] == 'Ended', \
            'runtime for %s is %s, should be Ended' % (entry['title'], entry['tvdb_status'])
        assert entry['tvdb_absolute_number'] == 3
        assert entry['afield'] == '73255Paternity', 'afield was not set correctly'
        assert self.task.find_entry(tvdb_ep_name='School Reunion'), \
            'Failed imdb lookup Doctor Who 2005 S02E03'

    @attr(online=True)
    def test_unknown_series(self):
        # Test an unknown series does not cause any exceptions
        self.execute_task('test_unknown_series')
        # Make sure it didn't make a false match
        entry = self.task.find_entry('accepted', title='Aoeu.Htns.S01E01.htvd')
        assert entry.get('tvdb_id') is None, 'should not have populated tvdb data'

    @attr(online=True)
    def test_mark_expired(self):

        def test_run():
            # Run the task and check tvdb data was populated.
            self.execute_task('test_mark_expired')
            entry = self.task.find_entry(title='House.S02E02.hdtv')
            assert entry['tvdb_ep_name'] == 'Autopsy'

        # Run the task once, this populates data from tvdb
        test_run()
        # Run the task again, this should load the data from cache
        test_run()
        # Manually mark the data as expired, to test cache update
        session = Session()
        ep = lookup_episode(name='House', seasonnum=2, episodenum=2, session=session)
        ep.expired = True
        ep.series.expired = True
        session.commit()
        session.close()
        test_run()

    @attr(online=True)
    def test_date(self):
        self.execute_task('test_date')
        entry = self.task.find_entry(title='the daily show 2012-6-6')
        assert entry
        assert entry['tvdb_ep_name'] == 'Michael Fassbender'

    @attr(online=True)
    def test_absolute(self):
        self.execute_task('test_absolute')
        entry = self.task.find_entry(title='naruto 128')
        assert entry
        assert entry['tvdb_ep_name'] == 'A Cry on Deaf Ears'


class TestThetvdbFavorites(FlexGetBase):
    """
        Tests thetvdb favorites plugin with a test user at thetvdb.
        Test user info:
        username: flexget
        password: flexget
        Account ID: 80FB8BD0720CA5EC
        Favorites: House, Doctor Who 2005, Penn & Teller: Bullshit, Hawaii Five-0 (2010)
    """

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'House.S01E02.HDTV.XViD-FlexGet'}
              - {title: 'Doctor.Who.2005.S02E03.PDTV.XViD-FlexGet'}
              - {title: 'Lost.S03E02.720p-FlexGet'}
              - {title: 'Penn.and.Teller.Bullshit.S02E02.720p.x264'}
            configure_series:
              from:
                thetvdb_favorites:
                  account_id: 80FB8BD0720CA5EC
          test_strip_dates:
            thetvdb_favorites:
              account_id: 80FB8BD0720CA5EC
              strip_dates: yes
    """

    @attr(online=True)
    def test_favorites(self):
        """thetvdb: Test favorites (ONLINE)"""
        self.execute_task('test')
        assert self.task.find_entry('accepted', title='House.S01E02.HDTV.XViD-FlexGet'), \
            'series House should have been accepted'
        assert self.task.find_entry('accepted', title='Doctor.Who.2005.S02E03.PDTV.XViD-FlexGet'), \
            'series Doctor Who 2005 should have been accepted'
        assert self.task.find_entry('accepted', title='Penn.and.Teller.Bullshit.S02E02.720p.x264'), \
            'series Penn and Teller Bullshit should have been accepted'
        entry = self.task.find_entry(title='Lost.S03E02.720p-FlexGet')
        assert entry, 'Entry not found?'
        assert entry not in self.task.accepted, \
            'series Lost should not have been accepted'

    @attr(online=True)
    def test_strip_date(self):
        self.execute_task('test_strip_dates')
        assert self.task.find_entry(title='Hawaii Five-0'), \
            'series Hawaii Five-0 (2010) should have date stripped'

########NEW FILE########
__FILENAME__ = test_tmdb
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.plugins.attrib import attr


class TestTmdbLookup(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: '[Group] Taken 720p', imdb_url: 'http://www.imdb.com/title/tt0936501/'}
              - {title: 'The Matrix'}
            tmdb_lookup: yes
            # Access a field to cause lazy loading to occur
            set:
              afield: "{{ tmdb_id }}"
    """

    @attr(online=True)
    def test_tmdb_lookup(self):
        self.execute_task('test')
        # check that these were created
        assert self.task.find_entry(tmdb_name='Taken', tmdb_year=2008), 'Didn\'t populate tmdb info for Taken'
        assert self.task.find_entry(tmdb_name='The Matrix', tmdb_year=1999), \
                'Didn\'t populate tmdb info for The Matrix'

########NEW FILE########
__FILENAME__ = test_torrent
from __future__ import unicode_literals, division, absolute_import
import os

from nose.plugins.attrib import attr
from tests import FlexGetBase, with_filecopy
from flexget.utils.bittorrent import Torrent


class TestInfoHash(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'test', file: 'test.torrent'}
            accept_all: yes
          test_magnet:
            mock:
              - title: test magnet
                url: magnet:?xt=urn:btih:2a8959bed2be495bb0e3ea96f497d873d5faed05&dn=some.thing.720p
              - title: test magnet 2
                urls: ['magnet:?xt=urn:btih:2b3959bed2be445bb0e3ea96f497d873d5faed05&dn=some.thing.else.720p']
    """

    def test_infohash(self):
        """Torrent: infohash parsing"""
        self.execute_task('test')
        info_hash = self.task.entries[0].get('torrent_info_hash')
        assert info_hash == '14FFE5DD23188FD5CB53A1D47F1289DB70ABF31E', \
            'InfoHash does not match (got %s)' % info_hash

    def test_magnet_infohash(self):
        """Tests metainfo/magnet_btih plugin"""
        self.execute_task('test_magnet')
        assert self.task.all_entries[0]['torrent_info_hash'] == '2A8959BED2BE495BB0E3EA96F497D873D5FAED05'
        assert self.task.all_entries[1]['torrent_info_hash'] == '2B3959BED2BE445BB0E3EA96F497D873D5FAED05'


class TestSeenInfoHash(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: test, file: test.torrent}
            accept_all: yes
          test2:
            mock:
              - {title: test2, file: test2.torrent}
            accept_all: yes
          test_same_run:
            mock:
              - {title: test, torrent_info_hash: 20AE692114DC343C86DF5B07C276E5077E581766}
              - {title: test2, torrent_info_hash: 20ae692114dc343c86df5b07c276e5077e581766}
            accept_all: yes
    """

    @with_filecopy('test.torrent', 'test2.torrent')
    def test_seen_info_hash(self):
        self.execute_task('test')
        assert self.task.find_entry('accepted', title='test'), 'torrent should have been accepted on first run'
        self.execute_task('test2')
        assert self.task.find_entry('rejected', title='test2'), 'torrent should have been rejected on second run'

    def test_same_run(self):
        # Test that 2 entries with the same info hash don't get accepted on the same run.
        # Also tests that the plugin compares info hash case insensitively.
        self.execute_task('test_same_run')
        assert len(self.task.accepted) == 1, 'Should not have accepted both entries with the same info hash'


class TestModifyTrackers(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            accept_all: yes
        tasks:
          test_add_trackers:
            mock:
              - {title: 'test', file: 'test_add_trackers.torrent'}
              - {title: 'test_magnet'}
            set:
              url: 'magnet:?xt=urn:btih:HASH&dn=title'
            add_trackers:
              - udp://thetracker.com/announce

          test_remove_trackers:
            mock:
              - {title: 'test', file: 'test_remove_trackers.torrent'}
              - title: 'test_magnet'
            set:
              url: 'magnet:?xt=urn:btih:HASH&dn=title&tr=http://ipv6.torrent.ubuntu.com:6969/announce'
            remove_trackers:
              - ipv6

          test_modify_trackers:
            mock:
              - {title: 'test', file: 'test_modify_trackers.torrent'}
            modify_trackers:
              - test:
                  from: ubuntu
                  to: replaced
    """

    def load_torrent(self, filename):
        with open(filename, 'rb') as f:
            data = f.read()
        return Torrent(data)

    @with_filecopy('test.torrent', 'test_add_trackers.torrent')
    def test_add_trackers(self):
        self.execute_task('test_add_trackers')
        torrent = self.load_torrent('test_add_trackers.torrent')
        assert 'udp://thetracker.com/announce' in torrent.trackers, \
            'udp://thetracker.com/announce should have been added to trackers'
        # Check magnet url
        assert 'tr=udp://thetracker.com/announce' in self.task.find_entry(title='test_magnet')['url']

    @with_filecopy('test.torrent', 'test_remove_trackers.torrent')
    def test_remove_trackers(self):
        self.execute_task('test_remove_trackers')
        torrent = self.load_torrent('test_remove_trackers.torrent')
        assert 'http://ipv6.torrent.ubuntu.com:6969/announce' not in torrent.trackers, \
            'ipv6 tracker should have been removed'

        # Check magnet url
        assert 'tr=http://ipv6.torrent.ubuntu.com:6969/announce' not in self.task.find_entry(title='test_magnet')['url']

    @with_filecopy('test.torrent', 'test_modify_trackers.torrent')
    def test_modify_trackers(self):
        self.execute_task('test_modify_trackers')
        torrent = self.load_torrent('test_modify_trackers.torrent')
        assert 'http://torrent.replaced.com:6969/announce' in torrent.trackers, \
            'ubuntu tracker should have been added'

        # TODO: due implementation this bugs! Torrent class needs to be fixed ...
        return
        assert 'http://torrent.ubuntu.com:6969/announce' not in torrent.trackers, \
            'ubuntu tracker should have been removed'


class TestPrivateTorrents(FlexGetBase):

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'test_private', file: 'private.torrent'}
              - {title: 'test_public', file: 'test.torrent'}
            accept_all: yes
            private_torrents: no
    """

    def test_private_torrents(self):
        self.execute_task('test')
        assert self.task.find_entry('rejected', title='test_private'), 'did not reject private torrent'
        assert self.task.find_entry('accepted', title='test_public'), 'did not pass public torrent'


class TestTorrentScrub(FlexGetBase):

    __tmp__ = True
    __yaml__ = """
        tasks:
          test_all:
            mock:
              - {title: 'test', file: '__tmp__test.torrent'}
              - {title: 'LICENSE', file: '__tmp__LICENSE.torrent'}
              - {title: 'LICENSE-resume', file: '__tmp__LICENSE-resume.torrent'}
            accept_all: yes
            torrent_scrub: all
            disable_builtins: [seen_info_hash]

          test_fields:
            mock:
              - {title: 'fields.LICENSE', file: '__tmp__LICENSE.torrent'}
            accept_all: yes
            torrent_scrub:
              - comment
              - info.x_cross_seed
              - field.that.never.exists

          test_off:
            mock:
              - {title: 'off.LICENSE-resume', file: '__tmp__LICENSE-resume.torrent'}
            accept_all: yes
            torrent_scrub: off
    """

    test_cases = (
        (True, 'test.torrent'),
        (False, 'LICENSE.torrent'),
        (False, 'LICENSE-resume.torrent'),
    )
    test_files = [i[1] for i in test_cases]

    @with_filecopy(test_files, "__tmp__")
    def test_torrent_scrub(self):
        # Run task
        self.execute_task('test_all')

        for clean, filename in self.test_cases:
            original = Torrent.from_file(filename)
            title = os.path.splitext(filename)[0]

            modified = self.task.find_entry(title=title)
            assert modified, "%r cannot be found in %r" % (title, self.task)
            modified = modified.get('torrent')
            assert modified, "No 'torrent' key in %r" % (title,)

            osize = os.path.getsize(filename)
            msize = os.path.getsize(self.__tmp__ + filename)

            # Dump small torrents on demand
            if 0 and not clean:
                print "original=%r" % original.content
                print "modified=%r" % modified.content

            # Make sure essentials survived
            assert 'announce' in modified.content
            assert 'info' in modified.content
            assert 'name' in modified.content['info']
            assert 'piece length' in modified.content['info']
            assert 'pieces' in modified.content['info']

            # Check that hashes have changed accordingly
            if clean:
                assert osize == msize, "Filesizes aren't supposed to differ!"
                assert original.info_hash == modified.info_hash, 'info dict changed in ' + filename
            else:
                assert osize > msize, "Filesizes must be different!"
                assert original.info_hash != modified.info_hash, filename + " wasn't scrubbed!"

            # Check essential keys were scrubbed
            if filename == 'LICENSE.torrent':
                assert 'x_cross_seed' in original.content['info']
                assert 'x_cross_seed' not in modified.content['info']

            if filename == 'LICENSE-resume.torrent':
                assert 'libtorrent_resume' in original.content
                assert 'libtorrent_resume' not in modified.content

    @with_filecopy(test_files, "__tmp__")
    def test_torrent_scrub_fields(self):
        self.execute_task('test_fields')
        title = 'fields.LICENSE'
        torrent = self.task.find_entry(title=title)
        assert torrent, "%r cannot be found in %r" % (title, self.task)
        torrent = torrent.get('torrent')
        assert torrent, "No 'torrent' key in %r" % (title,)

        assert 'name' in torrent.content['info'], "'info.name' was lost"
        assert 'comment' not in torrent.content, "'comment' not scrubbed"
        assert 'x_cross_seed' not in torrent.content['info'], "'info.x_cross_seed' not scrubbed"

    @with_filecopy(test_files, "__tmp__")
    def test_torrent_scrub_off(self):
        self.execute_task('test_off')

        for filename in self.test_files:
            osize = os.path.getsize(filename)
            msize = os.path.getsize(self.__tmp__ + filename)
            assert osize == msize, "Filesizes aren't supposed to differ (%r %d, %r %d)!" % (
                filename, osize, self.__tmp__ + filename, msize)


class TestTorrentAlive(FlexGetBase):
    __yaml__ = """
        templates:
          global:
            accept_all: yes
        tasks:
          test_torrent_alive_fail:
            mock:
              - {title: 'test', file: 'test_torrent_alive.torrent', url: fake}
            torrent_alive: 100000
          test_torrent_alive_pass:
            mock:
              - {title: 'test', file: 'test_torrent_alive.torrent', url: fake}
            torrent_alive: 0
    """

    @attr(online=True)
    @with_filecopy('test.torrent', 'test_torrent_alive.torrent')
    def test_torrent_alive_fail(self):
        self.execute_task('test_torrent_alive_fail')
        assert not self.task.accepted, 'Torrent should not have met seed requirement.'
        assert self.task._rerun_count == 1, ('Task should have been rerun 1 time. Was rerun %s times.' %
                                             self.task._rerun_count)

        # Run it again to make sure remember_rejected prevents a rerun from occurring
        self.execute_task('test_torrent_alive_fail')
        assert not self.task.accepted, 'Torrent should have been rejected by remember_rejected.'
        assert self.task._rerun_count == 0, 'Task should not have been rerun.'

    @attr(online=True)
    @with_filecopy('test.torrent', 'test_torrent_alive.torrent')
    def test_torrent_alive_pass(self):
        self.execute_task('test_torrent_alive_pass')
        assert self.task.accepted
        assert self.task._rerun_count == 0, 'Torrent should have been accepted without rerun.'

    @attr(online=True)
    def test_torrent_alive_udp_invalid_port(self):
        from flexget.plugins.filter.torrent_alive import get_udp_seeds
        assert get_udp_seeds('udp://[2001::1]/announce','HASH') == 0
        assert get_udp_seeds('udp://[::1]/announce','HASH') == 0
        assert get_udp_seeds('udp://["2100::1"]:-1/announce', 'HASH') == 0
        assert get_udp_seeds('udp://127.0.0.1/announce','HASH') == 0
        assert get_udp_seeds('udp://127.0.0.1:-1/announce','HASH') == 0
        assert get_udp_seeds('udp://127.0.0.1:PORT/announce','HASH') == 0
        assert get_udp_seeds('udp://127.0.0.1:65536/announce','HASH') == 0

class TestRtorrentMagnet(FlexGetBase):
    __tmp__ = True
    __yaml__ = """
        tasks:
          test:
            mock:
              - title: 'test'
                url: 'magnet:?xt=urn:btih:HASH&dn=title&tr=http://torrent.ubuntu.com:6969/announce'
            rtorrent_magnet: __tmp__
            accept_all: yes
    """


    def test_rtorrent_magnet(self):
        self.execute_task('test')
        filename = 'meta-test.torrent'
        fullpath = os.path.join(self.__tmp__, filename)
        assert os.path.isfile(fullpath)
        with open(fullpath) as f:
            assert (f.read() ==
                    'd10:magnet-uri76:magnet:?xt=urn:btih:HASH&dn=title&tr=http://torrent.ubuntu.com:6969/announcee')

########NEW FILE########
__FILENAME__ = test_trakt
from __future__ import unicode_literals, division, absolute_import
from nose.plugins.attrib import attr
from flexget.manager import Session
from flexget.plugins.api_trakt import ApiTrakt
lookup_series = ApiTrakt.lookup_series
lookup_episode = ApiTrakt.lookup_episode
from tests import FlexGetBase


class TestTraktLookup(FlexGetBase):

    __yaml__ = """
        templates:
          global:
            trakt_lookup: yes
            # Access a tvdb field to cause lazy loading to occur
            set:
              afield: "{{trakt_series_tvdb_id}}{{trakt_ep_name}}"
        tasks:
          test:
            mock:
              - {title: 'House.S01E02.HDTV.XViD-FlexGet'}
              - {title: 'Doctor.Who.2005.S02E03.PDTV.XViD-FlexGet'}
            series:
              - House
              - Doctor Who 2005
          test_unknown_series:
            mock:
              - {title: 'Aoeu.Htns.S01E01.htvd'}
            series:
              - Aoeu Htns
          test_date:
            mock:
              - title: the daily show 2012-6-6
            series:
              - the daily show (with jon stewart)
          test_absolute:
            mock:
              - title: naruto 128
            series:
              - naruto

    """

    @attr(online=True)
    def test_lookup(self):
        """trakt: Test Lookup (ONLINE)"""
        self.execute_task('test')
        entry = self.task.find_entry(title='House.S01E02.HDTV.XViD-FlexGet')
        assert entry['trakt_ep_name'] == 'Paternity', \
            '%s trakt_ep_name should be Paternity' % entry['title']
        assert entry['trakt_series_status'] == 'Ended', \
            'runtime for %s is %s, should be Ended' % (entry['title'], entry['trakt_series_status'])
        assert entry['afield'] == '73255Paternity', 'afield was not set correctly'
        assert self.task.find_entry(trakt_ep_name='School Reunion'), \
            'Failed imdb lookup Doctor Who 2005 S02E03'

    @attr(online=True)
    def test_unknown_series(self):
        # Test an unknown series does not cause any exceptions
        self.execute_task('test_unknown_series')
        # Make sure it didn't make a false match
        entry = self.task.find_entry('accepted', title='Aoeu.Htns.S01E01.htvd')
        assert entry.get('tvdb_id') is None, 'should not have populated tvdb data'

    @attr(online=True)
    def test_date(self):
        self.execute_task('test_date')
        entry = self.task.find_entry(title='the daily show 2012-6-6')
        assert entry.get('tvdb_id') is None, 'should not have populated trakt data'

    @attr(online=True)
    def test_absolute(self):
        self.execute_task('test_absolute')
        entry = self.task.find_entry(title='naruto 128')
        assert entry.get('tvdb_id') is None, 'should not have populated trakt data'

########NEW FILE########
__FILENAME__ = test_tvrage
from __future__ import unicode_literals, division, absolute_import
import logging

from nose.plugins.attrib import attr

from tests import FlexGetBase
from flexget.plugins.api_tvrage import lookup_series

log = logging.getLogger('TestTvRage')


class TestTvRage(FlexGetBase):

    @attr(online=True)
    def test_tvrage(self):
        friends = lookup_series("Friends")
        assert friends.genres == ['Comedy', 'Romance/Dating']
        s1e22 = friends.find_episode(1, 22)
        log.info("s1e22 %s " % s1e22)

        # Testing next
        s1e23 = s1e22.next()
        log.info("s1e23 %s " % s1e23)
        assert s1e23.episode == 23 and s1e23.season == 1

        s1e24 = s1e23.next()
        assert s1e24.episode == 24 and s1e24.season == 1
        log.info("s1e24 %s " % s1e24)

        s2e1 = s1e24.next()
        assert s2e1.episode == 1 and s2e1.season == 2
        log.info("s2e1 %s " % s2e1)

        s31e1 = friends.find_episode(31, 1)
        assert not s31e1
        log.info("s31e1 %s " % s31e1)
        s1e1 = friends.find_episode(1, 1)
        assert s1e1
        log.info("s1e1 %s " % s1e1)
        s1e32 = friends.find_episode(1 ,32)
        log.info("s1e32 %s " % s1e32)
        assert not s1e32
        assert friends.finished()

########NEW FILE########
__FILENAME__ = test_urlfix
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase


class TestUrlfix(FlexGetBase):
    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'Test', url: 'http://localhost/foo?bar=asdf&amp;xxx=yyy'}
          test2:
            mock:
              - {title: 'Test', url: 'http://localhost/foo?bar=asdf&amp;xxx=yyy'}
            urlfix: no
    """

    def test_urlfix(self):
        self.execute_task('test')
        entry = self.task.find_entry('entries', title='Test')
        assert entry['url'] == 'http://localhost/foo?bar=asdf&xxx=yyy', \
            'failed to auto fix url, got %s' % entry['url']

    def test_urlfix_disabled(self):
        self.execute_task('test2')
        entry = self.task.find_entry('entries', title='Test')
        assert entry['url'] != 'http://localhost/foo?bar=asdf&xxx=yyy', \
            'fixed even when disabled, got %s' % entry['url']

########NEW FILE########
__FILENAME__ = test_urlrewriting
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.tools import assert_true
from flexget.plugin import get_plugin_by_name


class TestURLRewriters(FlexGetBase):
    """
        Bad example, does things manually, you should use task.find_entry to check existance
    """

    __yaml__ = """
        tasks:
          test:
            # make test data
            mock:
              - {title: 'tpb page', url: 'http://thepiratebay.org/tor/8492471/Test.avi'}
              - {title: 'tbp search', url: 'http://thepiratebay.com/search/something'}
              - {title: 'tbp torrent', url: 'http://torrents.thepiratebay.se/8492471/Test.torrent'}
              - {title: 'tbp torrent subdomain', url: 'http://torrents.thepiratebay.se/8492471/Test.avi'}
              - {title: 'tbp torrent bad subdomain', url: 'http://torrent.thepiratebay.se/8492471/Test.avi'}
              - {title: 'nyaa', url: 'http://www.nyaa.eu/?page=torrentinfo&tid=12345'}
              - {title: 'isohunt search', url: 'http://isohunt.com/torrents/?ihq=Query.Here'}
              - {title: 'isohunt direct', url: 'http://isohunt.com/torrent_details/123456789/Name.Here'}
    """

    def setup(self):
        FlexGetBase.setup(self)
        self.execute_task('test')

    def get_urlrewriter(self, name):
        info = get_plugin_by_name(name)
        return info.instance

    def test_piratebay(self):
        # test with piratebay entry
        urlrewriter = self.get_urlrewriter('piratebay')
        entry = self.task.find_entry(title='tpb page')
        assert_true(urlrewriter.url_rewritable(self.task, entry))
        entry = self.task.find_entry(title='tbp torrent')
        assert not urlrewriter.url_rewritable(self.task, entry), \
            'TPB direct torrent link should not be url_rewritable'
        entry = self.task.find_entry(title='tbp torrent subdomain')
        assert_true(urlrewriter.url_rewritable(self.task, entry))
        entry = self.task.find_entry(title='tbp torrent bad subdomain')
        assert not urlrewriter.url_rewritable(self.task, entry), \
            'TPB link with invalid subdomain should not be url_rewritable'

    def test_piratebay_search(self):
        # test with piratebay entry
        urlrewriter = self.get_urlrewriter('piratebay')
        entry = self.task.find_entry(title='tbp search')
        assert_true(urlrewriter.url_rewritable(self.task, entry))

    def test_nyaa_torrents(self):
        entry = self.task.find_entry(title='nyaa')
        urlrewriter = self.get_urlrewriter('nyaa')
        assert entry['url'] == 'http://www.nyaa.eu/?page=torrentinfo&tid=12345'
        assert_true(urlrewriter.url_rewritable(self.task, entry))
        urlrewriter.url_rewrite(self.task, entry)
        assert entry['url'] == 'http://www.nyaa.eu/?page=download&tid=12345'

    def test_isohunt(self):
        entry = self.task.find_entry(title='isohunt search')
        urlrewriter = self.get_urlrewriter('isohunt')
        assert not urlrewriter.url_rewritable(self.task, entry), \
            'search entry should not be url_rewritable'
        entry = self.task.find_entry(title='isohunt direct')
        assert urlrewriter.url_rewritable(self.task, entry), \
            'direct entry should be url_rewritable'


class TestRegexpurlrewriter(FlexGetBase):
    # TODO: this test is broken?

    __yaml__ = """
        tasks:
          test:
            mock:
              - {title: 'irrelevant', url: 'http://newzleech.com/?p=123'}
            accept_all: yes
            urlrewrite:
              newzleech:
                regexp: 'http://newzleech.com/\?p=(?P<id>\d+)'
                format: 'http://newzleech.com/?m=gen&dl=1&post=\g<id>'
    """

    def test_newzleech(self):
        self.execute_task('test')
        assert self.task.find_entry(url='http://newzleech.com/?m=gen&dl=1&post=123'), \
            'did not url_rewrite properly'

########NEW FILE########
__FILENAME__ = test_validator
"""
These validate methods are never run by FlexGet anymore, but these tests serve as a sanity check that the
old validators will get converted to new schemas properly for plugins still using the `validator` method.
"""
from __future__ import unicode_literals, division, absolute_import

from flexget import validator
from tests.util import maketemp

class TestValidator(object):

    def test_default(self):
        root = validator.factory()
        assert root.name == 'root', 'expected root'
        dv = root.accept('dict')
        assert dv.name == 'dict', 'expected dict'
        dv.accept('text', key='text')

    def test_dict(self):
        dv = validator.factory('dict')
        dv.accept('dict', key='foo')
        result = dv.validate({'foo': {}})
        assert not dv.errors.messages, 'should have passed foo'
        assert result, 'invalid result for foo'
        result = dv.validate({'bar': {}})
        assert dv.errors.messages, 'should not have passed bar'
        assert not result, 'should have an invalid result for bar'
        # Test validation of dictionary keys
        dv = validator.factory('dict')
        dv.accept_valid_keys('dict', key_type='number')
        result = dv.validate({3: {}})
        assert not dv.errors.messages, 'should have passed 3'
        assert result, 'invalid result for key 3'
        # Json schema cannot do key validation
        """result = dv.validate({'three': {}})
        assert dv.errors.messages, 'should not have passed three'
        assert not result, 'should have an invalid result for 3'"""

    def test_regexp_match(self):
        re_match = validator.factory('regexp_match')
        re_match.accept('abc.*')
        assert not re_match.validate('foobar'), 'foobar should not have passed'
        assert re_match.validate('abcdefg'), 'abcdefg should have passed'

    def test_interval(self):
        interval = validator.factory('interval')
        assert interval.validate('3 days')
        assert interval.validate('12 hours')
        assert interval.validate('1 minute')

        assert not interval.validate('aoeu')
        assert not interval.validate('14')
        assert not interval.validate('3 dayz')
        assert not interval.validate('about 5 minutes')

    def test_choice(self):
        choice = validator.factory('choice')
        choice.accept('foo')
        choice.accept('Bar', ignore_case=True)
        choice.accept(120)
        choice.validate('foo')
        print choice.errors.messages
        assert not choice.errors.messages, 'foo should be valid'
        choice.validate(120)
        print choice.errors.messages
        assert not choice.errors.messages, '120 should be valid'
        choice.validate('bAR')
        print choice.errors.messages
        assert not choice.errors.messages, 'bAR should be valid'
        choice.validate('xxx')
        print choice.errors.messages
        assert choice.errors.messages, 'xxx should be invalid'
        choice.errors.messages = []
        choice.validate(300)
        print choice.errors.messages
        assert choice.errors.messages, '300 should be invalid'
        choice.errors.messages = []
        choice.validate('fOO')
        print choice.errors.messages
        assert choice.errors.messages, 'fOO should be invalid'

    # This validator is not supported with json schema
    def _lazy(self):
        """Test lazy validators by making a recursive one."""

        def recursive_validator():
            root = validator.factory('dict')
            root.accept('integer', key='int')
            root.accept(recursive_validator, key='recurse')
            return root

        test_config = {'int': 1,
                       'recurse': {
                           'int': 2,
                           'recurse': {
                               'int': 3}}}

        assert recursive_validator().validate(test_config), 'Config should pass validation'
        test_config['recurse']['badkey'] = 4
        assert not recursive_validator().validate(test_config), 'Config should not be valid'

    def test_path(self):
        path = validator.factory('path')
        path_allow_missing = validator.factory('path', allow_missing=True)
        temp_path = maketemp()
        path.validate(temp_path)
        print path.errors.messages
        assert not path.errors.messages, '%s should be valid' % (temp_path)
        path_allow_missing.validate('missing_directory')
        print path_allow_missing.errors.messages
        assert not path_allow_missing.errors.messages, 'missing_directory should be valid with allow_missing'
        path.validate('missing_directory')
        print path.errors.messages
        assert path.errors.messages, 'missing_directory should be invalid'
        path_allow_missing.errors.messages = []

########NEW FILE########
__FILENAME__ = test_whatcd
from __future__ import unicode_literals, division, absolute_import
from tests import FlexGetBase
from nose.plugins.attrib import attr


class TestInputWhatCD(FlexGetBase):

    __yaml__ = """
        tasks:
          no_fields:
            whatcd:
          no_user:
            whatcd:
              password: test
          no_pass:
            whatcd:
              username: test
    """

    def test_missing_fields(self):
        self.execute_task('no_fields', abort_ok=True)
        assert self.task.aborted, 'Task not aborted with no fields present'
        self.execute_task('no_user', abort_ok=True)
        assert self.task.aborted, 'Task not aborted with no username'
        self.execute_task('no_pass', abort_ok=True)
        assert self.task.aborted, 'Task not aborted with no password'


class TestWhatCDOnline(FlexGetBase):

    __yaml__ = """
        tasks:
          badlogin:
            whatcd:
              username: invalid
              password: invalid
    """

    @attr(online=True)
    def test_invalid_login(self):
        self.execute_task("badlogin", abort_ok=True)
        assert self.task.aborted, 'Task not aborted with invalid login credentials'

########NEW FILE########
__FILENAME__ = util
"""
A method to create test-specific temporary directories
"""

from __future__ import unicode_literals, division, absolute_import
import sys
import os
import shutil
import errno
import logging
import time
from flexget.utils.tools import parse_timedelta

log = logging.getLogger('tests.util')


def mkdir(*a, **kw):
    try:
        os.mkdir(*a, **kw)
    except OSError as e:
        if e.errno == errno.EEXIST:
            pass
        else:
            raise


def find_test_name():
    try:
        from nose.case import Test
        from nose.suite import ContextSuite
        import types

        def get_nose_name(its_self):
            if isinstance(its_self, Test):
                file_, module, class_ = its_self.address()
                name = '%s:%s' % (module, class_)
                return name
            elif isinstance(its_self, ContextSuite):
                if isinstance(its_self.context, types.ModuleType):
                    return its_self.context.__name__
    except ImportError:
        # older nose
        from nose.case import FunctionTestCase, MethodTestCase
        from nose.suite import TestModule
        from nose.util import test_address

        def get_nose_name(its_self):
            if isinstance(its_self, (FunctionTestCase, MethodTestCase)):
                file_, module, class_ = test_address(its_self)
                name = '%s:%s' % (module, class_)
                return name
            elif isinstance(its_self, TestModule):
                return its_self.moduleName

    i = 0
    while True:
        i += 1
        frame = sys._getframe(i)
        # kludge, hunt callers upwards until we find our nose
        if (frame.f_code.co_varnames
            and frame.f_code.co_varnames[0] == 'self'):
            its_self = frame.f_locals['self']
            name = get_nose_name(its_self)
            if name is not None:
                return name


def maketemp(name=None):
    tmp = os.path.join(os.path.dirname(__file__), 'tmp')
    mkdir(tmp)

    if not name:
        # Colons are not valid characters in directories on Windows
        name = find_test_name().replace(':', '_')

    # Always use / instead of \ to avoid escaping issues
    tmp = os.path.join(tmp, name).replace('\\', '/')
    log.trace("Creating empty tmpdir %r" % tmp)
    try:
        shutil.rmtree(tmp)
    except OSError as e:
        if e.errno == errno.ENOENT:
            pass
        else:
            raise
    os.mkdir(tmp)
    return tmp

########NEW FILE########
