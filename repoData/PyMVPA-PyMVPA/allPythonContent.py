__FILENAME__ = cachedkernel
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Efficient cross-validation using a cached kernel
================================================

.. index:: Cross-validation

This is a simple example showing how to use cached kernel with a SVM
classifier from the Shogun library.  Pre-caching of the kernel for all
samples in dataset eliminates necessity of possibly lengthy
recomputation of the same kernel values on different splits of the
data.  Depending on the data it might provide considerable speed-ups.
"""

from mvpa2.suite import *
from time import time

"""The next few calls load an fMRI dataset and do basic preprocessing."""

# load PyMVPA example dataset
attr = SampleAttributes(os.path.join(pymvpa_dataroot,
                        'attributes_literal.txt'))
dataset = fmri_dataset(samples=os.path.join(pymvpa_dataroot, 'bold.nii.gz'),
                       targets=attr.targets, chunks=attr.chunks,
                       mask=os.path.join(pymvpa_dataroot, 'mask.nii.gz'))

# do chunkswise linear detrending on dataset
poly_detrend(dataset, polyord=1, chunks_attr='chunks')

# zscore dataset relative to baseline ('rest') mean
zscore(dataset, chunks_attr='chunks', param_est=('targets', ['rest']))

"""Cached kernel is just a proxy around an existing kernel."""

# setup a cached kernel
kernel_plain = LinearSGKernel(normalizer_cls=False)
kernel = CachedKernel(kernel_plain)

"""Lets setup two cross-validation, where first would use cached
kernel, whenever the later one plain kernel to demonstrate the
speed-up and achievement of exactly the same results"""

# setup a classifier and cross-validation procedure
clf = sg.SVM(svm_impl='libsvm', C=-1.0, kernel=kernel)
cv = CrossValidation(clf, NFoldPartitioner())

# setup exactly the same using a plain kernel for demonstration of
# speedup and equivalence of the results
clf_plain = sg.SVM(svm_impl='libsvm', C=-1.0, kernel=kernel_plain)
cv_plain = CrossValidation(clf_plain, NFoldPartitioner())


"""Although it would be done internally by cached kernel during
initial computation, it is advisable to make initialization of origids
for samples explicit. It would prepare dataset by cleaning up
attributes used by cached kernel possibly on another version of the
same dataset prior to this analysis in real use cases."""

dataset.init_origids(which='samples')

"""Cached kernel needs to be computed given the full dataset which
would later on be used during cross-validation.
"""

# compute kernel for the dataset
t0 = time()
kernel.compute(dataset)
t_caching = time() - t0

"""Lets run both cross-validation procedures using plain and cached
kernels and report the results."""

# run cached cross-validation
t0 = time()
error = np.mean(cv(dataset))
t_cached = time() - t0

# run plain SVM cross-validation for validation and benchmarking
t0 = time()
error_plain = np.mean(cv_plain(dataset))
t_plain = time() - t0

# UC: unique chunks, UT: unique targets
print "Results for %i-fold cross-validation on %i-class problem:" \
      % (len(dataset.UC), len(dataset.UT))
print " plain kernel:  error=%.3f computed in %.2f sec" \
      % (error_plain, t_plain)
print " cached kernel: error=%.3f computed in %.2f sec (cached in %.2f sec)" \
      % (error, t_cached, t_caching)

"""The following is output from running this example::

 Results for 12-fold cross-validation on 9-class problem:
  plain kernel:  error=0.273 computed in 35.82 sec
  cached kernel: error=0.273 computed in 6.50 sec (cached in 3.68 sec)
"""

########NEW FILE########
__FILENAME__ = clfs_examples
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Classifier Sweep
================

This examples shows a test of various classifiers on different datasets.
"""

from mvpa2.suite import *

# no MVPA warnings during this example
warning.handlers = []

def main():

    # fix seed or set to None for new each time
    np.random.seed(44)


    # Load Haxby dataset example
    attrs = SampleAttributes(os.path.join(pymvpa_dataroot,
                                          'attributes_literal.txt'))
    haxby8 = fmri_dataset(samples=os.path.join(pymvpa_dataroot,
                                               'bold.nii.gz'),
                          targets=attrs.targets,
                          chunks=attrs.chunks,
                          mask=os.path.join(pymvpa_dataroot, 'mask.nii.gz'))
    haxby8.samples = haxby8.samples.astype(np.float32)

    # preprocess slightly
    poly_detrend(haxby8, chunks_attr='chunks', polyord=1)
    zscore(haxby8, chunks_attr='chunks', param_est=('targets', 'rest'))

    haxby8_no0 = haxby8[haxby8.targets != 'rest']

    dummy2 = normal_feature_dataset(perlabel=30, nlabels=2,
                                  nfeatures=100,
                                  nchunks=6, nonbogus_features=[11, 10],
                                  snr=3.0)

    for (dataset, datasetdescr), clfs_ in \
        [
        ((dummy2,
          "Dummy 2-class univariate with 2 useful features out of 100"),
          clfswh[:]),
        ((pure_multivariate_signal(8, 3),
          "Dummy XOR-pattern"),
          clfswh['non-linear']),
        ((haxby8_no0,
          "Haxby 8-cat subject 1"),
          clfswh['multiclass']),
        ]:
        # XXX put back whenever there is summary() again
        #print "%s\n %s" % (datasetdescr, dataset.summary(idhash=False))
        print " Classifier on %s\n" \
                "                                          :   %%corr   " \
                "#features\t train  predict full" % datasetdescr
        for clf in clfs_:
            print "  %-40s: "  % clf.descr,
            # Let's prevent failures of the entire script if some particular
            # classifier is not appropriate for the data
            try:
                # Change to False if you want to use CrossValidation
                # helper, instead of going through splits manually to
                # track training/prediction time of the classifiers
                do_explicit_splitting = True
                if not do_explicit_splitting:
                    cv = CrossValidation(
                        clf, NFoldPartitioner(), enable_ca=['stats', 'calling_time'])
                    error = cv(dataset)
                    # print cv.ca.stats
                    print "%5.1f%%      -    \t   -       -    %.2fs" \
                          % (cv.ca.stats.percent_correct, cv.ca.calling_time)
                    continue

                # To report transfer error (and possibly some other metrics)
                confusion = ConfusionMatrix()
                times = []
                nf = []
                t0 = time.time()
                #TODO clf.ca.enable('nfeatures')
                partitioner = NFoldPartitioner()
                for nfold, ds in enumerate(partitioner.generate(dataset)):
                    (training_ds, validation_ds) = tuple(
                        Splitter(attr=partitioner.space).generate(ds))
                    clf.train(training_ds)
                    #TODO nf.append(clf.ca.nfeatures)
                    predictions = clf.predict(validation_ds.samples)
                    confusion.add(validation_ds.targets, predictions)
                    times.append([clf.ca.training_time, clf.ca.predicting_time])

                tfull = time.time() - t0
                times = np.mean(times, axis=0)
                #TODO nf = np.mean(nf)
                # print confusion
                #TODO print "%5.1f%%   %-4d\t %.2fs  %.2fs   %.2fs" % \
                print "%5.1f%%       -   \t %.2fs  %.2fs   %.2fs" % \
                      (confusion.percent_correct, times[0], times[1], tfull)
                #TODO      (confusion.percent_correct, nf, times[0], times[1], tfull)
            except LearnerError, e:
                print " skipped due to '%s'" % e

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = curvefitting
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Curve-Fitting
=============

Here we are going to take a look at a few examples of fitting a function to
data. The first example shows how to fit an HRF model to noisy peristimulus
time-series data.

First, importing the necessary pieces:
"""

import numpy as np
from scipy.stats import norm

from mvpa2.support.pylab import pl
from mvpa2.misc.plot import plot_err_line, plot_bars
from mvpa2.misc.fx import *
from mvpa2 import cfg

"""
BOLD-Response parameters
------------------------

Let's generate some noisy "trial time courses" from a simple gamma function
(40 samples, 6s time-to-peak, 7s FWHM and no additional scaling:
"""

a = np.asarray([single_gamma_hrf(np.arange(20), A=6, W=7, K=1)] * 40)
# get closer to reality with noise
a += np.random.normal(size=a.shape)

"""
Fitting a gamma function to this data is easy (using resonable seeds for the
parameter search (5s time-to-peak, 5s FWHM, and no scaling):
"""

fpar, succ = least_sq_fit(single_gamma_hrf, [5,5,1], a)

"""
With these parameters we can compute high-resultion curves for the estimated
time course, and plot it together with the "true" time course, and the data:
"""

x = np.linspace(0,20)
curves = [(x, single_gamma_hrf(x, 6, 7, 1)),
          (x, single_gamma_hrf(x, *fpar))]

# plot data (with error bars) and both curves
plot_err_line(a, curves=curves, linestyle='-')

# add legend to plot
pl.legend(('original', 'fit'))
pl.title('True and estimated BOLD response')

"""

.. image:: ../pics/ex_curvefitting_bold.*
   :align: center
   :alt: BOLD response fitting example


Searchlight accuracy distributions
----------------------------------

When doing a searchlight analysis one might have the idea that the
resulting accuracies are actually sampled from two distributions: one
causes by an actual signal source and the chance distribution.  Let's
assume the these two distributions can be approximated by a Gaussian,
and take a look at a toy example, how we could explore the data.

First, we generate us a few searchlight accuracy maps that might have
been computed in the folds of a cross-validation procedure. We
generate the data from two normal distributions. The majority of
datapoints comes from the chance distribution that is centered at 0.5.
A fraction of the data is samples from the "signal" distribution
located around 0.75.
"""

nfolds = 10
raw_data = np.vstack([np.concatenate((np.random.normal(0.5, 0.08, 10000),
                                      np.random.normal(0.75, 0.05, 500)))
                        for i in range(nfolds)])

"""
Now we bin the data into one histogram per fold and fit a dual Gaussian
(the sum of two Gaussians) to the total of 10 histograms.
"""

histfit = fit2histogram(raw_data,
                        dual_gaussian, (1000, 0.5, 0.1, 1000, 0.8, 0.05),
                        nbins=20)
H, bin_left, bin_width, fit = histfit

"""
All that is left to do is composing a figure -- showing the accuracy
histogram and its variation across folds, as well as the two estimated
Gaussians.
"""

# new figure
pl.figure()

# Gaussian parameters
params = fit[0]

# plot the histogram
plot_bars(H.T, xloc=bin_left, width=bin_width, yerr='std')

# show the Gaussians
x = np.linspace(0, 1, 100)
# first gaussian
pl.plot(x, params[0] * norm.pdf(x, params[1], params[2]), "r-", zorder=2)
pl.axvline(params[1], color='r', linestyle='--', alpha=0.6)
# second gaussian
pl.plot(x, params[3] * norm.pdf(x, params[4], params[5]), "b-", zorder=3)
pl.axvline(params[4], color='b', linestyle='--', alpha=0.6)
# dual gaussian
pl.plot(x, dual_gaussian(x, *params), "k--", alpha=0.5, zorder=1)
pl.xlim(0, 1)
pl.ylim(ymin=0)

pl.title('Dual Gaussian fit of searchlight accuracies')

if cfg.getboolean('examples', 'interactive', True):
    # show the cool figures
    pl.show()

"""
And this is how it looks like.

.. image:: ../pics/ex_curvefitting_searchlight.*
   :align: center
   :alt: Dual Gaussian fit of searchlight accuracies

"""

########NEW FILE########
__FILENAME__ = erp_plot
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
ERP/ERF-Plots
=============

Example demonstrating an ERP-style plots. Actually, this code can be used to
plot various time-locked data types. This example uses MEG data and therefore
generates an ERF-plot.
"""

from mvpa2.suite import *

# load data
meg = TuebingenMEG(os.path.join(pymvpa_dataroot, 'tueb_meg.dat.gz'))


# Define plots for easy feeding into plot_erp
plots = []
colors = ['r', 'b', 'g']

# figure out pre-stimulus onset interval
t0 = -meg.timepoints[0]

plots = [ {'label' : meg.channelids[i],
           'color' : colors[i],
           'data' : meg.data[:, i, :]}
          for i in xrange(len(meg.channelids)) ]

# Common arguments for all plots
cargs = {
    'SR' : meg.samplingrate,
    'pre_onset' : t0,
    # Plot only 50ms before and 100ms after the onset since we have
    # just few trials
    'pre' : 0.05, 'post' : 0.1,
    # Plot all 'errors' in different degrees of shadings
    'errtype' : ['ste', 'ci95', 'std'],
    # Set to None if legend manages to obscure the plot
    'legend' : 'best',
    'alinewidth' : 1  # assume that we like thin lines
    }

# Create a new figure
fig = pl.figure(figsize=(12, 8))

# Following plots are plotted inverted (negative up) for the
# demonstration of this capability and elderly convention for ERP
# plots. That is controlled with ymult (negative gives negative up)


# Plot MEG sensors

# frame_on=False guarantees abent outside rectangular axis with
# labels. plot_erp recreates its own axes centered at (0,0)
ax = fig.add_subplot(2, 1, 1, frame_on=False)
plot_erps(plots[:2], ylabel='$pT$', ymult=-1e12, ax=ax, **cargs)

# Plot EEG sensor
ax = fig.add_subplot(2, 1, 2, frame_on=False)
plot_erps(plots[2:3], ax=ax, ymult=-1e6, **cargs)

# Additional example: plotting a single ERP on an existing plot
# without drawing axis:
#
# plot_erp(data=meg.data[:, 0, :], SR=meg.samplingrate, pre=pre,
#         pre_mean=pre, errtype=errtype, ymult=-1.0)

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""
The ouput of the provided example is presented below. It is not a very
fascinating one due to the limited number of samples provided in the
dataset shipped within the toolbox.

.. image:: ../pics/ex_erp_plot.*
   :align: center
   :alt: ERP plot example

"""

########NEW FILE########
__FILENAME__ = eventrelated
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Spatio-temporal Analysis of event-related fMRI data
===================================================

.. index:: event-related fMRI

In this example we are going to take a look at an event-related analysis of
timeseries data. We will do this on fMRI data, implementing a spatio-temporal
analysis of multi-volume samples. It starts as usual by loading PyMVPA and
the fMRI dataset.
"""

from mvpa2.suite import *

ds = load_datadb_tutorial_data(roi=(36,38,39,40))

"""

The dataset we have just loaded is the full timeseries of voxels in the
ventral temporal cortex for 12 concatenated experiment runs. Although
originally a block-design experiment, we'll analyze it in an event-related
fashion, where each stimulation block will be considered as an individual
event.

For an event-related analysis most of the processing is done on data
samples that are somehow derived from a set of events. The rest of the data
could be considered irrelevant. However, some preprocessing is only
meaningful when performed on the full timeseries and not on the segmented
event samples. An example is the detrending that typically needs to be done
on the original, continuous timeseries.

In its current shape our dataset consists of 1452 samples that represent
contiguous fMRI volumes. At this stage we can easily perform linear
detrending. We are going to do it per each experiment run (the dataset has
to runs encoded in the ``chunk`` sample attribute), since we do not assume a
contiguous linear trend throughout the whole recording session.

"""

# detrend on full timeseries
poly_detrend(ds, polyord=1, chunks_attr='chunks')

"""

Let's make a copy of the detrended dataset that we can later on use for
some visualization.

"""

orig_ds = ds.copy()

"""

We still need to normalize each feature (i.e. a voxel at this point). In
this case we are going to Z-score them, using the mean and standard
deviation from the experiment's rest condition. The resulting values might
be interpreted as "activation scores". We are again doing it per each run.

"""

zscore(ds, chunks_attr='chunks', param_est=('targets', 'rest'))

"""

After detrending and normalization, we can now segment the timeseries into
a set of events. To achieve this we have to compile a list of event
definitions first. In this example we will simply convert the block-design
setup defined by the samples attributes into events, so that each
stimulation block becomes an event with an associated onset and duration.
The events are defined by a change in any of the provided attributes, hence
we get an event for starting stimulation block and any start of a run in
the experiment.

"""

events = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)

"""

``events`` is a simple list of event definitions (each one being a
dictionary) that can easily inspected for startpoints and duration of
events. Later on we want to look at the sensitivity profile ranging from
just before until a little after each block. Therefore we are slightly
moving the event onsets prior the block start and request to extract a
set of 13 consecutive volumes a as sample for each event.  Finally, in this
example we are only interested in `face` or `house` blocks.

"""

# filter out events
events = [ev for ev in events if ev['targets'] in ['house', 'face']]

# modify event start and set uniform duration
for ev in events:
    ev['onset'] -= 2
    ev['duration'] = 13

"""

Now we get to the core of an event-related analysis. We turn our existing
timeseries datasets into one with samples of timeseries segments.

PyMVPA offers :func:`~mvpa2.datasets.eventrelated.eventrelated_dataset()`
to perform this conversion -- given a list of events and a dataset with
samples that are sorted by time. If a dataset has information about
acquisition time :func:`~mvpa2.datasets.eventrelated.eventrelated_dataset()`
can also convert event-definition in real time.

"""

evds = eventrelated_dataset(ds, events=events)

"""

Now we have our final dataset with spatio-temporal fMRI samples. Look at
the attributes of the dataset to see what information is available about
each event. The rest is pretty much standard.

We want to perform a cross-validation analysis of a SVM classifier. We are
not primarily interested in its performance, but in the weights it assigns
to the features. Remember, each feature is now voxel-timepoint, so we get a
chance of looking at the spatio-temporal profile of classification relevant
information in the data. We will nevertheless enable computing a confusion
matrix, so we can assure ourselves that the classifier is performing
reasonably well, since only a generalizing classifier model is worth
inspecting, as otherwise the assigned weights are meaningless.

"""

clf = LinearCSVMC()
sclf = SplitClassifier(clf, enable_ca=['confusion'])

# Compute sensitivity, which internally trains the classifier
analyzer = sclf.get_sensitivity_analyzer()
sensitivities = analyzer(evds)

"""

Now let's look at the confusion matrix -- it turns out that the classifier
performs excellent.

"""

print sclf.ca.confusion

"""

We could now convert the computed sensitivities back into a 4D fMRI image
to look at the spatio-temporal sensitivity profile using the datasets
mapper. However, in this example we are going to plot it for two example
voxels and compare it to the actual signal timecourse prior and after
normalization.

"""

# example voxel coordinates
example_voxels = [(28,25,25), (28,23,25)]

"""

First we plot the orginal signal after initial detrending. To do this, we
apply the timeseries segmentation to the original detrended dataset and
plot to mean signal for all face and house events for both of our example
voxels.

"""

vx_lty = ['-', '--']
t_col = ['b', 'r']

pl.subplot(311)
for i, v in enumerate(example_voxels):
    slicer = np.array([tuple(idx) == v for idx in ds.fa.voxel_indices])
    evds_detrend = eventrelated_dataset(orig_ds[:, slicer], events=events)
    for j, t in enumerate(evds.uniquetargets):
        pl.plot(np.mean(evds_detrend[evds_detrend.sa.targets == t], axis=0),
               t_col[j] + vx_lty[i],
               label='Voxel %i: %s' % (i, t))
pl.ylabel('Detrended signal')
pl.axhline(linestyle='--', color='0.6')
pl.legend()

"""

In the next step we do exactly the same again, but this time for the
normalized data.

"""

pl.subplot(312)
for i, v in enumerate(example_voxels):
    slicer = np.array([tuple(idx) == v for idx in ds.fa.voxel_indices])
    evds_norm = eventrelated_dataset(ds[:, slicer], events=events)
    for j, t in enumerate(evds.uniquetargets):
        pl.plot(np.mean(evds_norm[evds_norm.sa.targets == t], axis=0),
               t_col[j] + vx_lty[i])
pl.ylabel('Normalized signal')
pl.axhline(linestyle='--', color='0.6')

"""

Finally, we plot the associated SVM weight profile for each peristimulus
timepoint of both voxels. For easier selection we do a little trick and
reverse-map the sensitivity profile through the last mapper in the
dataset's chain mapper (look at ``evds.a.mapper`` for the whole chain).
This will reshape the sensitivities into ``cross-validation fold x volume x
voxel features``.

"""

pl.subplot(313)
smaps = evds.a.mapper[-1].reverse(sensitivities)

for i, v in enumerate(example_voxels):
    slicer = np.array([tuple(idx) == v for idx in ds.fa.voxel_indices])
    smap = smaps.samples[:,:,slicer].squeeze()
    plot_err_line(smap, fmt='ko', linestyle=vx_lty[i])
pl.xlim((0,12))
pl.ylabel('Sensitivity')
pl.axhline(linestyle='--', color='0.6')
pl.xlabel('Peristimulus volumes')

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""

.. figure:: ../pics/ex_eventrelated.*
   :align: center

   Sensitivity profile for two example voxels for *face* vs. *house*
   classification on event-related fMRI data from ventral temporal cortex.

This demo showed an event-related data analysis. Although we have performed
it on fMRI data, an analogous analysis can be done for any timeseries-based
data in an almost identical fashion.
"""

########NEW FILE########
__FILENAME__ = eyemovements
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Analysis of eye movement patterns
=================================

In this example we are going to look at a classification analysis of eye
movement patterns. Although complex preprocessing steps can be performed to
extract higher-order features from the raw coordinate timeseries provided by an
eye-tracker, we are keeping it simple.

Right after importing the PyMVPA suite, we load the data from a textfile.
It contains coordinate timeseries of 144 trials (recorded with 350 Hz), where
subjects either looked at upright or inverted images of human faces. Each
timeseries snippet covers 3 seconds. This data has been pre-processed to remove
eyeblink artefacts.

In addition to the coordinates we also load trial attributes from a second
textfile. These attributes indicate which image was shown, whether it was
showing a male or female face, and wether it was upright or inverted.
"""

from mvpa2.suite import *

# where is the data
datapath = os.path.join(pymvpa_datadbroot,
                        'face_inversion_demo', 'face_inversion_demo')
# (X, Y, trial id) for all timepoints
data = np.loadtxt(os.path.join(datapath, 'gaze_coords.txt'))
# (orientation, gender, image id) for each trial
attribs = np.loadtxt(os.path.join(datapath, 'trial_attrs.txt'))

"""
As a first step we put the coordinate timeseries into a dataset, and labels each
timepoint with its associated trial ID. We also label the two features
accordingly.
"""

raw_ds = Dataset(data[:,:2],
                 sa = {'trial': data[:,2]},
                 fa = {'fid': ['rawX', 'rawY']})

"""
The second step is down-sampling the data to about 33 Hz, resampling each trial
timeseries individually (using the trial ID attribute to define dataset chunks).
"""

ds = fft_resample(raw_ds, 100, window='hann',
                  chunks_attr='trial', attr_strategy='sample')

"""
Now we can use a :class:`~mvpa2.mappers.boxcar.BoxcarMapper` to turn each
trial-timeseries into an individual sample. We know that each sample consists
of 100 timepoints. After the dataset is mapped we can add all per-trial
attributes into the sample attribute collection.
"""

bm = BoxcarMapper(np.arange(len(ds.sa['trial'].unique)) * 100,
                  boxlength=100)
bm.train(ds)
ds=ds.get_mapped(bm)

ds.sa.update({'orient': attribs[:,0].astype(int),
              'gender': attribs[:,1].astype(int),
              'img_id': attribs[:,1].astype(int)})

"""
In comparison with upright faces, inverted ones had prominent features at very
different locations on the screen. Most notably, the eyes were flipped to the
bottom half. To prevent the classifier from using such differences, we flip the
Y-coordinates for trials with inverted to align the with the upright condition.
"""

ds.samples[ds.sa.orient == 1, :, 1] = \
        -1 * (ds.samples[ds.sa.orient == 1, :, 1] - 512) + 512

"""
The current dataset has 100 two-dimensional features, the X and Y
coordinate for each of the hundred timepoints. We use a
:class:`~mvpa2.mappers.flatten.FlattenMapper` to convert each sample into a
one-dimensionl vector (of length 200). However, we also keep the original
dataset, because it will allow us to perform some plotting much easier.
"""

fm = FlattenMapper()
fm.train(ds)
# want to make a copy to keep the original pristine for later plotting
fds = ds.copy().get_mapped(fm)

# simplify the trial attribute
fds.sa['trial'] = [t[0] for t in ds.sa.trial]

"""
The last steps of preprocessing are Z-scoring all features
(coordinate-timepoints) and dividing the dataset into 8 chunks -- to simplify
a cross-validation analysis.
"""

zscore(fds, chunks_attr=None)

# for classification divide the data into chunks
nchunks =  8
chunks = np.zeros(len(fds), dtype='int')
for o in fds.sa['orient'].unique:
    chunks[fds.sa.orient == o] = np.arange(len(fds.sa.orient == o)) % nchunks
fds.sa['chunks'] = chunks

"""
Now everything is set and we can proceed to the classification analysis. We
are using a support vector machine that is going to be trained on the
``orient`` attribute, indicating trials with upright and inverted faces. We are
going to perform the analysis with a :class:`~mvpa2.clfs.meta.SplitClassifier`,
because we are also interested in the temporal sensitivity profile. That one is
easily accessible via the corresponding sensitivity analyzer.
"""


clf = SVM(space='orient')
mclf = SplitClassifier(clf, space='orient',
                       enable_ca=['confusion'])
sensana = mclf.get_sensitivity_analyzer()
sens = sensana(fds)
print mclf.ca.confusion

"""
The 8-fold cross-validation shows a trial-wise classification accuracy of
over 80%. Now we can take a look at the sensitivity. We use the
:class:`~mvpa2.mappers.flatten.FlattenMapper` that is stored in the dataset to
unmangle X and Y coordinate vectors in the sensitivity array.
"""

# split mean sensitivities into X and Y coordinate parts by reversing through
# the flatten mapper
xy_sens = fds.a.mapper[-2].reverse(sens).samples

"""
Plotting the results
--------------------

The analysis is done and we can compile a figure to visualize the results.
After some inital preparations, we plot an example image of a face that was
used in this experiment. We align the image coordinates with the original
on-screen coordinates to match them to the gaze track, and overlay the image
with the mean gaze track across all trials for each condition.
"""

# descriptive plots
pl.figure()
# original screen size was
axes = ('x', 'y')
screen_size = np.array((1280, 1024))
screen_center = screen_size / 2
colors = ('r','b')
fig = 1

pl.subplot(2, 2, fig)
pl.title('Mean Gaze Track')
face_img = pl.imread(os.path.join(datapath, 'demo_face.png'))
# determine the extend of the image in original screen coordinates
# to match with gaze position
orig_img_extent=(screen_center[0] - face_img.shape[1]/2,
                 screen_center[0] + face_img.shape[1]/2,
                 screen_center[1] + face_img.shape[0]/2,
                 screen_center[1] - face_img.shape[0]/2)
# show face image and put it with original pixel coordinates
pl.imshow(face_img,
          extent=orig_img_extent,
          cmap=pl.cm.gray)
pl.plot(np.mean(ds.samples[ds.sa.orient == 1,:,0], axis=0),
        np.mean(ds.samples[ds.sa.orient == 1,:,1], axis=0),
        colors[0], label='inverted')
pl.plot(np.mean(ds.samples[ds.sa.orient == 2,:,0], axis=0),
        np.mean(ds.samples[ds.sa.orient == 2,:,1], axis=0),
        colors[1], label='upright')
pl.axis(orig_img_extent)
pl.legend()
fig += 1

"""
The next two subplot contain the gaze coordinate over the peri-stimulus time
for both, X and Y axis respectively.
"""

pl.subplot(2, 2, fig)
pl.title('Gaze Position X-Coordinate')
plot_erp(ds.samples[ds.sa.orient == 1,:,1], pre=0, errtype = 'std',
         color=colors[0], SR=100./3.)
plot_erp(ds.samples[ds.sa.orient == 2,:,1], pre=0, errtype = 'std',
         color=colors[1], SR=100./3.)
pl.ylim(orig_img_extent[2:])
pl.xlabel('Peristimulus Time')
fig += 1

pl.subplot(2, 2, fig)
pl.title('Gaze Position Y-Coordinate')
plot_erp(ds.samples[ds.sa.orient == 1,:,0], pre=0, errtype = 'std',
         color=colors[0], SR=100./3.)
plot_erp(ds.samples[ds.sa.orient == 2,:,0], pre=0, errtype = 'std',
         color=colors[1], SR=100./3.)
pl.ylim(orig_img_extent[:2])
pl.xlabel('Peristimulus Time')
fig += 1

"""
The last panel has the associated sensitivity profile for both coordinate axes.
"""

pl.subplot(2, 2, fig)
pl.title('SVM-Sensitivity Profiles')
lines = plot_err_line(xy_sens[..., 0], linestyle='-', fmt='ko', errtype='std')
lines[0][0].set_label('X')
lines = plot_err_line(xy_sens[..., 1], linestyle='-', fmt='go', errtype='std')
lines[0][0].set_label('Y')
pl.legend()
pl.ylim((-0.1, 0.1))
pl.xlim(0,100)
pl.axhline(y=0, color='0.6', ls='--')
pl.xlabel('Timepoints')

from mvpa2.base import cfg
if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""
The following figure is not exactly identical to the product of this code, but
rather shows the result of a few minutes of beautifications in Inkscape_.

.. _Inkscape: http://www.inkscape.org/

.. figure:: ../pics/ex_eyemovements.*
   :align: center

   Gaze track for viewing upright vs. inverted faces. The figure shows the mean
   gaze path for both conditions overlayed on an example face. The panels to
   the left and below show the X and Y coordinates over the trial timecourse
   (shaded aread corresponds to one standard deviation across all trials above
   and below the mean). The black curve depicts the associated temporal SVM
   weight profile for the classification of both conditions.

"""

########NEW FILE########
__FILENAME__ = gpr
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com>
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
The effect of different hyperparameters in GPR
==============================================

.. index:: GPR

The following example runs Gaussian Process Regression (GPR) on a
simple 1D dataset using squared exponential (i.e., Gaussian or RBF)
kernel and different hyperparameters. The resulting classifier
solutions are finally visualized in a single figure.

As usual we start by importing all of PyMVPA:
"""

# Lets use LaTeX for proper rendering of greek
from matplotlib import rc
rc('text', usetex=True)

from mvpa2.suite import *

"""
The next lines build two datasets using one of PyMVPA's data
generators.
"""

# Generate dataset for training:
train_size = 40
F = 1
dataset = data_generators.sin_modulated(train_size, F)

# Generate dataset for testing:
test_size = 100
dataset_test = data_generators.sin_modulated(test_size, F, flat=True)

"""
The last configuration step is the definition of four sets of
hyperparameters to be used for GPR.
"""

# Hyperparameters. Each row is [sigma_f, length_scale, sigma_noise]
hyperparameters = np.array([[1.0, 0.2, 0.4],
                           [1.0, 0.1, 0.1],
                           [1.0, 1.0, 0.1],
                           [1.0, 0.1, 1.0]])

"""
The plotting of the final figure and the actually GPR runs are
performed in a single loop.
"""

rows = 2
columns = 2
pl.figure(figsize=(12, 12))
for i in range(rows*columns):
    pl.subplot(rows, columns, i+1)
    regression = True
    logml = True

    data_train = dataset.samples
    label_train = dataset.sa.targets
    data_test = dataset_test.samples
    label_test = dataset_test.sa.targets

    """
    The next lines configure a squared exponential kernel with the set of
    hyperparameters for the current subplot and assign the kernel to the GPR
    instance.
    """

    sigma_f, length_scale, sigma_noise = hyperparameters[i, :]
    kse = SquaredExponentialKernel(length_scale=length_scale,
                                   sigma_f=sigma_f)
    g = GPR(kse, sigma_noise=sigma_noise)
    if not regression:
        g = RegressionAsClassifier(g)
    print g

    if regression:
        g.ca.enable("predicted_variances")

    if logml:
        g.ca.enable("log_marginal_likelihood")

    """
    After training GPR the predictions are queried by passing the test
    dataset samples and accuracy measures are computed.
    """

    g.train(dataset)
    prediction = g.predict(data_test)

    # print label_test
    # print prediction
    accuracy = None
    if regression:
        accuracy = np.sqrt(((prediction-label_test)**2).sum()/prediction.size)
        print "RMSE:", accuracy
    else:
        accuracy = (prediction.astype('l')==label_test.astype('l')).sum() \
                   / float(prediction.size)
        print "accuracy:", accuracy

    """
    The remaining code simply plots both training and test datasets, as
    well as the GPR solutions.
    """

    if F == 1:
        pl.title(r"$\sigma_f=%0.2f$, $length_s=%0.2f$, $\sigma_n=%0.2f$" \
                % (sigma_f,length_scale,sigma_noise))
        pl.plot(data_train, label_train, "ro", label="train")
        pl.plot(data_test, prediction, "b-", label="prediction")
        pl.plot(data_test, label_test, "g+", label="test")
        if regression:
            pl.plot(data_test, prediction - np.sqrt(g.ca.predicted_variances),
                       "b--", label=None)
            pl.plot(data_test, prediction+np.sqrt(g.ca.predicted_variances),
                       "b--", label=None)
            pl.text(0.5, -0.8, "$RMSE=%.3f$" %(accuracy))
            pl.text(0.5, -0.95, "$LML=%.3f$" %(g.ca.log_marginal_likelihood))
        else:
            pl.text(0.5, -0.8, "$accuracy=%s" % accuracy)

        pl.legend(loc='lower right')

    print "LML:", g.ca.log_marginal_likelihood


if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = gpr_model_selection0
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com>
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Simple model selection: grid search for GPR
===========================================

.. index:: GPR, model selection

Run simple model selection (grid search over hyperparameters' space) of
Gaussian Process Regression (GPR) on a simple 1D example.
"""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.suite import *
import pylab as pl

# Generate train and test dataset:
train_size = 40
test_size = 100
F = 1
dataset = data_generators.sin_modulated(train_size, F)
dataset_test = data_generators.sin_modulated(test_size, F, flat=True)

print "Looking for better hyperparameters: grid search"

# definition of the search grid:
sigma_noise_steps = np.linspace(0.1, 0.5, num=20)
length_scale_steps = np.linspace(0.05, 0.6, num=20)

# Evaluation of log maringal likelohood spanning the hyperparameters' grid:
lml = np.zeros((len(sigma_noise_steps), len(length_scale_steps)))
lml_best = -np.inf
length_scale_best = 0.0
sigma_noise_best = 0.0
i = 0
for x in sigma_noise_steps:
    j = 0
    for y in length_scale_steps:
        kse = SquaredExponentialKernel(length_scale=y)
        g = GPR(kse, sigma_noise=x)
        g.ca.enable("log_marginal_likelihood")
        g.train(dataset)
        lml[i, j] = g.ca.log_marginal_likelihood
        if lml[i, j] > lml_best:
            lml_best = lml[i, j]
            length_scale_best = y
            sigma_noise_best = x
            # print x,y,lml_best
            pass
        j += 1
        pass
    i += 1
    pass

# Log marginal likelihood contour plot:
pl.figure()
X = np.repeat(sigma_noise_steps[:, np.newaxis], sigma_noise_steps.size,
             axis=1)
Y = np.repeat(length_scale_steps[np.newaxis, :], length_scale_steps.size,
             axis=0)
step = (lml.max()-lml.min())/30
pl.contour(X, Y, lml, np.arange(lml.min(), lml.max()+step, step),
              colors='k')
pl.plot([sigma_noise_best], [length_scale_best], "k+",
           markeredgewidth=2, markersize=8)
pl.xlabel("noise standard deviation")
pl.ylabel("characteristic length_scale")
pl.title("log marginal likelihood")
pl.axis("tight")
print "lml_best", lml_best
print "sigma_noise_best", sigma_noise_best
print "length_scale_best", length_scale_best
print "number of expected upcrossing on the unitary intervale:", \
      1.0/(2*np.pi*length_scale_best)


# TODO: reincarnate by providing a function within gpr.py
#
# Plot predicted values using best hyperparameters:
# pl.figure()
# compute_prediction(1.0, length_scale_best, sigma_noise_best, True, dataset,
#                    dataset_test.samples, dataset_test.targets, F, True)
if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = hyperalignment
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Hyperalignment for between-subject analysis
===========================================

.. index:: hyperalignment, between-subject classification

Multivariate pattern analysis (MVPA) reveals how the brain represents
fine-scale information. Its power lies in its sensitivity to subtle pattern
variations that encode this fine-scale information but that also presents a
hurdle for group analyses due to between-subject variability of both anatomical
& functional architectures. :ref:`Haxby et al. (2011) <HGC+11>` recently
proposed a method of aligning subjects' brain data in a high-dimensional
functional space and showed how to build a common model of ventral temporal
cortex that captures visual object category information. They tested their
model by successfully performing between-subject classification of category
information.  Moreover, when they built the model using a complex naturalistic
stimulation (a feature film), it even generalized to other independent
experiments even after removing any occurrences of the experimental stimuli
from the movie data.

In this example we show how to perform Hyperalignment within a single
experiment. We will compare between-subject classification after hyperalignment
to between-subject classification on anatomically aligned data (currently the
most typical approach), and within-subject classification performance.


Analysis setup
--------------

"""

from mvpa2.suite import *

verbose.level = 2

"""
We start by loading preprocessed datasets of 10 subjects with BOLD-responses
of stimulation with face and object images (:ref:`Haxby et al., 2011 <HGC+11>`).
Each dataset, after preprocessing, has one sample per category and run for each
of the eight runs and seven stimulus categories. Individual subject brains have
been aligned anatomically using a 12 dof linear transformation.
"""

verbose(1, "Loading data...")
filepath = os.path.join(pymvpa_datadbroot,
                        'hyperalignment_tutorial_data',
                        'hyperalignment_tutorial_data.hdf5.gz')
ds_all = h5load(filepath)
# zscore all datasets individually
_ = [zscore(ds) for ds in ds_all]
# inject the subject ID into all datasets
for i,sd in enumerate(ds_all):
    sd.sa['subject'] = np.repeat(i, len(sd))
# number of subjects
nsubjs = len(ds_all)
# number of categories
ncats = len(ds_all[0].UT)
# number of run
nruns = len(ds_all[0].UC)
verbose(2, "%d subjects" % len(ds_all))
verbose(2, "Per-subject dataset: %i samples with %i features" % ds_all[0].shape)
verbose(2, "Stimulus categories: %s" % ', '.join(ds_all[0].UT))


"""
Now we'll create a couple of building blocks for the intended analyses. We'll
use a linear SVM classifier, and perform feature selection with a simple one-way
ANOVA selecting the ``nf`` highest scoring features.

"""

# use same classifier
clf = LinearCSVMC()

# feature selection helpers
nf = 100
fselector = FixedNElementTailSelector(nf, tail='upper',
                                      mode='select',sort=False)
sbfs = SensitivityBasedFeatureSelection(OneWayAnova(), fselector,
                                        enable_ca=['sensitivities'])
# create classifier with automatic feature selection
fsclf = FeatureSelectionClassifier(clf, sbfs)

"""
Within-subject classification
-----------------------------

We start off by running a cross-validated classification analysis for every
subject's dataset individually. Data folding will be performed by leaving out
one run to serve as the testing dataset. ANOVA-based features selection will be
performed automatically on training dataset and applied to testing dataset.
"""

verbose(1, "Performing classification analyses...")
verbose(2, "within-subject...", cr=False, lf=False)
wsc_start_time = time.time()
cv = CrossValidation(fsclf,
                     NFoldPartitioner(attr='chunks'),
                     errorfx=mean_match_accuracy)
# store results in a sequence
wsc_results = [cv(sd) for sd in ds_all]
wsc_results = vstack(wsc_results)
verbose(2, " done in %.1f seconds" % (time.time() - wsc_start_time,))


"""
Between-subject classification using anatomically aligned data
--------------------------------------------------------------

For between-subject classification with MNI-aligned voxels, we can stack up
all individual datasets into a single one, as (anatomical!) feature
correspondence is given. The crossvalidation analysis using the feature
selection classifier will automatically perform the desired ANOVA-based feature
selection on every training dataset partition. However, data folding will now
be done by leaving out a complete subject for testing.

"""

verbose(2, "between-subject (anatomically aligned)...", cr=False, lf=False)
ds_mni = vstack(ds_all)
mni_start_time = time.time()
cv = CrossValidation(fsclf,
                     NFoldPartitioner(attr='subject'),
                     errorfx=mean_match_accuracy)
bsc_mni_results = cv(ds_mni)
verbose(2, "done in %.1f seconds" % (time.time() - mni_start_time,))

"""
Between-subject classification with Hyperalignment(TM)
------------------------------------------------------

Between-subject classification using Hyperalignment is very similar
to the previous analysis. However, now we no longer assume feature
correspondence (or aren't satisfied with anatomical alignment anymore).
Consequently, we have to transform the individual datasets into a common space
before performing the classification analysis. To avoid introducing
circularity problems to the analysis, we perform leave-one-run-out
data folding manually, and train Hyperalignment only on the training
dataset partitions. Subsequently, we will apply the derived transformation
to the full datasets, stack them up across individual subjects, as before,
and run the classification analysis. ANOVA-based feature selection is done
in the same way as before (but also manually).
"""

verbose(2, "between-subject (hyperaligned)...", cr=False, lf=False)
hyper_start_time = time.time()
bsc_hyper_results = []
# same cross-validation over subjects as before
cv = CrossValidation(clf, NFoldPartitioner(attr='subject'), 
                     errorfx=mean_match_accuracy)

# leave-one-run-out for hyperalignment training
for test_run in range(nruns):
    # split in training and testing set
    ds_train = [sd[sd.sa.chunks != test_run,:] for sd in ds_all]
    ds_test = [sd[sd.sa.chunks == test_run,:] for sd in ds_all]

    # manual feature selection for every individual dataset in the list
    anova = OneWayAnova()
    fscores = [anova(sd) for sd in ds_train]
    featsels = [StaticFeatureSelection(fselector(fscore)) for fscore in fscores]
    ds_train_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_train)]


    # Perform hyperalignment on the training data with default parameters.
    # Computing hyperalignment parameters is as simple as calling the
    # hyperalignment object with a list of datasets. All datasets must have the
    # same number of samples and time-locked responses are assumed.
    # Hyperalignment returns a list of mappers corresponding to subjects in the
    # same order as the list of datasets we passed in.


    hyper = Hyperalignment()
    hypmaps = hyper(ds_train_fs)

    # Applying hyperalignment parameters is similar to applying any mapper in
    # PyMVPA. We start by selecting the voxels that we used to derive the
    # hyperalignment parameters. And then apply the hyperalignment parameters
    # by running the test dataset through the forward() function of the mapper.

    ds_test_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_test)]
    ds_hyper = [ hypmaps[i].forward(sd) for i, sd in enumerate(ds_test_fs)]

    # Now, we have a list of datasets with feature correspondence in a common
    # space derived from the training data. Just as in the between-subject
    # analyses of anatomically aligned data we can stack them all up and run the
    # crossvalidation analysis.

    ds_hyper = vstack(ds_hyper)
    # zscore each subject individually after transformation for optimal
    # performance
    zscore(ds_hyper, chunks_attr='subject')
    res_cv = cv(ds_hyper)
    bsc_hyper_results.append(res_cv)

bsc_hyper_results = hstack(bsc_hyper_results)
verbose(2, "done in %.1f seconds" % (time.time() - hyper_start_time,))

"""
Comparing the results
---------------------

Performance
^^^^^^^^^^^

First we take a look at the classification performance (or accuracy) of all
three analysis approaches.
"""

verbose(1, "Average classification accuracies:")
verbose(2, "within-subject: %.2f +/-%.3f"
        % (np.mean(wsc_results),
           np.std(wsc_results) / np.sqrt(nsubjs - 1)))
verbose(2, "between-subject (anatomically aligned): %.2f +/-%.3f"
        % (np.mean(bsc_mni_results),
           np.std(np.mean(bsc_mni_results, axis=1)) / np.sqrt(nsubjs - 1)))
verbose(2, "between-subject (hyperaligned): %.2f +/-%.3f" \
        % (np.mean(bsc_hyper_results),
           np.std(np.mean(bsc_hyper_results, axis=1)) / np.sqrt(nsubjs - 1)))

"""
The output of this demo looks like this::

 Loading data...
  10 subjects
  Per-subject dataset: 56 samples with 3509 features
  Stimulus categories: Chair, DogFace, FemaleFace, House, MaleFace, MonkeyFace, Shoe
 Performing classification analyses...
  within-subject... done in 4.3 seconds
  between-subject (anatomically aligned)...done after 3.2 seconds
  between-subject (hyperaligned)...done in 10.5 seconds
 Average classification accuracies:
  within-subject: 0.57 +/-0.063
  between-subject (anatomically aligned): 0.42 +/-0.035
  between-subject (hyperaligned): 0.62 +/-0.046

It is obvious that the between-subject classification using anatomically
aligned data has significantly worse performance when compared to
within-subject classification. Clearly the group classification model is
inferior to individual classifiers fitted to a particular subject's data.
However, a group classifier trained on hyperaligned data is performing at least
as good as the within-subject classifiers -- possibly even slightly better due
to the increased size of the training dataset.


Similarity structures
^^^^^^^^^^^^^^^^^^^^^

To get a better understanding of how hyperalignment transforms the structure
of the data, we compare the similarity structures of the corresponding input
datasets of all three analysis above (and one in addition).

These are respectively:

1. Average similarity structure of the individual data.
2. Similarity structure of the averaged hyperaligned data.
3. Average similarity structure of the individual data after hyperalignment.
4. Similarity structure of the averaged anatomically-aligned data.

Similarity structure in this case is the correlation matrix of multivariate
response patterns for all seven stimulus categories in the datasets. For
the sake of simplicity, all similarity structures are computed on the full
dataset without data folding.

"""

# feature selection as above
anova = OneWayAnova()
fscores = [anova(sd) for sd in ds_all]
fscores = np.mean(np.asarray(vstack(fscores)), axis=0)
# apply to full datasets
ds_fs = [sd[:,fselector(fscores)] for i,sd in enumerate(ds_all)]
#run hyperalignment on full datasets
hyper = Hyperalignment()
mappers = hyper(ds_fs)
ds_hyper = [ mappers[i].forward(ds_) for i,ds_ in enumerate(ds_fs)]
# similarity of original data samples
sm_orig = [np.corrcoef(
                sd.get_mapped(
                    mean_group_sample(['targets'])).samples)
                        for sd in ds_fs]
# mean across subjects
sm_orig_mean = np.mean(sm_orig, axis=0)
# same individual average but this time for hyperaligned data
sm_hyper_mean = np.mean(
                    [np.corrcoef(
                        sd.get_mapped(mean_group_sample(['targets'])).samples)
                            for sd in ds_hyper], axis=0)
# similarity for averaged hyperaligned data
ds_hyper = vstack(ds_hyper)
sm_hyper = np.corrcoef(ds_hyper.get_mapped(mean_group_sample(['targets'])))
# similarity for averaged anatomically aligned data
ds_fs = vstack(ds_fs)
sm_anat = np.corrcoef(ds_fs.get_mapped(mean_group_sample(['targets'])))

"""
We then plot the respective similarity strucures.
"""

# class labels should be in more meaningful order for visualization
# (human faces, animals faces, objects)
intended_label_order = [2,4,1,5,3,0,6]
labels = ds_all[0].UT
labels = labels[intended_label_order]

pl.figure(figsize=(6,6))
# plot all three similarity structures
for i, sm_t in enumerate((
    (sm_orig_mean, "Average within-subject\nsimilarity"),
    (sm_anat, "Similarity of group average\ndata (anatomically aligned)"),
    (sm_hyper_mean, "Average within-subject\nsimilarity (hyperaligned data)"),
    (sm_hyper, "Similarity of group average\ndata (hyperaligned)"),
                      )):
    sm, title = sm_t
    # reorder matrix columns to match label order
    sm = sm[intended_label_order][:,intended_label_order]
    pl.subplot(2, 2, i+1)
    pl.imshow(sm, vmin=-1.0, vmax=1.0, interpolation='nearest')
    pl.colorbar(shrink=.4, ticks=[-1,0,1])
    pl.title(title, size=12)
    ylim = pl.ylim()
    pl.xticks(range(ncats), labels, size='small', stretch='ultra-condensed',
              rotation=45)
    pl.yticks(range(ncats), labels, size='small', stretch='ultra-condensed',
              rotation=45)
    pl.ylim(ylim)

"""
.. figure:: ../pics/ex_hyperalignment_similarity.*

   Fig. 1: Correlation of category-specific response patterns using the 100 most
   informative voxels (based on ANOVA F-score ranking).

We can clearly see that averaging anatomically aligned data has a negative
effect on the similarity structure, as the fine category structure is diminished
and only the coarse structure (faces vs. objects) is preserved. Moreover, we can
see that after hyperalignment the average similarity structure of individual
data is essentially identical to the similarity structure of averaged data --
reflecting the feature correspondence in the common high-dimensional space.


Regularized Hyperalignment
--------------------------

According to :ref:`Xu et al. 2012 <XLR2012>`, Hyperalignment can be
reformulated to a regularized algorithm that can span the whole continuum
between `canonical correlation analysis (CCA)`_ and regular hyperalignment by
varying a regularization parameter (alpha).  Here, we repeat the above
between-subject hyperalignment and classification analyses with varying values
of alpha from 0 (CCA) to 1.0 (regular hyperalignment).

.. _`canonical correlation analysis (CCA)`: http://en.wikipedia.org/wiki/Canonical_correlation

The following code is essentially identical to the implementation of
between-subject classification shown above. The only difference is an addition
``for`` loop doing the alpha value sweep for each cross-validation fold.
"""

alpha_levels = np.concatenate(
                    (np.linspace(0.0, 0.7, 8),
                     np.linspace(0.8, 1.0, 9)))
# to collect the results for later visualization
bsc_hyper_results = np.zeros((nsubjs, len(alpha_levels), nruns))
# same cross-validation over subjects as before
cv = CrossValidation(clf, NFoldPartitioner(attr='subject'), 
                     errorfx=mean_match_accuracy)

# leave-one-run-out for hyperalignment training
for test_run in range(nruns):
    # split in training and testing set
    ds_train = [sd[sd.sa.chunks != test_run,:] for sd in ds_all]
    ds_test = [sd[sd.sa.chunks == test_run,:] for sd in ds_all]

    # manual feature selection for every individual dataset in the list
    anova = OneWayAnova()
    fscores = [anova(sd) for sd in ds_train]
    featsels = [StaticFeatureSelection(fselector(fscore)) for fscore in fscores]
    ds_train_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_train)]

    for alpha_level, alpha in enumerate(alpha_levels):
        hyper = Hyperalignment(alignment=ProcrusteanMapper(svd='dgesvd',
                                                           space='commonspace'),
                               alpha=alpha)
        hypmaps = hyper(ds_train_fs)
        ds_test_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_test)]
        ds_hyper = [ hypmaps[i].forward(sd) for i, sd in enumerate(ds_test_fs)]
        ds_hyper = vstack(ds_hyper)
        zscore(ds_hyper, chunks_attr='subject')
        res_cv = cv(ds_hyper)
        bsc_hyper_results[:, alpha_level, test_run] = res_cv.samples.T

"""
Now we can plot the classification accuracy as a function of regularization
intensity.
"""

bsc_hyper_results = np.mean(bsc_hyper_results, axis=2)
pl.figure()
plot_err_line(bsc_hyper_results, alpha_levels)
pl.xlabel('Regularization parameter: alpha')
pl.ylabel('Average BSC using hyperalignment +/- SEM')
pl.title('Using regularized hyperalignment with varying alpha values')

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""
.. figure:: ../pics/ex_hyperalignment_alphasweep.*

   Fig. 2: Mean between-subject classification accuracies using regularized
   hyperalignment with alpha value ranging from 0 (CCA) to 1 (vanilla
   hyperalignment).

We can clearly see that the regular hyperalignment performs best for this
dataset. However, please refer to :ref:`Xu et al. 2012 <XLR2012>` for an
example showing that this is not always the case.
"""

########NEW FILE########
__FILENAME__ = hyperplane_demo
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
# Originally written by Rajeev Raizada in 2010 for Matlab and with permission
# licensed under the terms of the PyMVPA's license.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Separating hyperplane tutorial
==============================

This is a very introductory tutorial, showing how a classification task (in
this case, deciding whether people are sumo wrestlers or basketball players,
based on their height and weight) can be viewed as drawing a decision boundary
in a feature space. It shows how to plot the data, calculate the weights of a
simple linear classifier, and see how the resulting classifier carves up the
feature space into two categories.

.. note::

  This tutorial was originally written by Rajeev Raizada for Matlab and was
  ported to Python by the PyMVPA authors. The original Matlab code is available
  from: http://www.dartmouth.edu/~raj/Matlab/fMRI/classification_plane_tutorial.m

Let's look at a toy example: classifying people as either 
sumo wrestlers or basketball players, depending on their height and weight.
Let's call the x-axis height and the y-axis weight
"""

sumo_wrestlers_height = [ 4, 2, 2, 3, 4 ]
sumo_wrestlers_weight = [ 8, 6, 2, 5, 7 ]
basketball_players_height = [ 3, 4, 5, 5, 3 ]
basketball_players_weight = [ 2, 5, 3, 7, 3 ]

"""
Let's plot this.
"""

import pylab as pl
pl.plot(sumo_wrestlers_height, sumo_wrestlers_weight, 'ro',
       linewidth=2, label="Sumo wrestlers")
pl.plot(basketball_players_height, basketball_players_weight, 'bx',
       linewidth=2, label="Basketball players")
pl.xlim(0, 6)
pl.ylim(0, 10)
pl.xlabel('Height')
pl.ylabel('Weight')
pl.legend()

"""
Let's stack up the sumo data on top of the basketball players data.
"""

import numpy as np

# transpose to have observations along the first axis
sumo_data = np.vstack((sumo_wrestlers_height,
                      sumo_wrestlers_weight)).T
# same for the baskball data
basketball_data = np.vstack((basketball_players_height,
                            basketball_players_weight)).T
# now stack them all together
all_data = np.vstack((sumo_data, basketball_data))

"""
In order to be able to train a classifier on the input vectors, we need to know
what the desired output categories are for each one.  Let's set this to be `+1`
for sumo wrestlers, and `-1` for basketball players.
"""

# creates: [  1,  1,  1,  1,  1, -1, -1, -1, -1, -1]
all_desired_output = np.repeat([1, -1], 5)

"""
We want to find a linear decision boundary,
i.e. simply a straight line, such that all the data points
on one side of the line get classified as sumo wrestlers,
i.e. get mapped onto the desired output of `+1`,
and all the data points on the other side get classified
as basketball players, i.e. get mapped onto the desired output of `-1`.

The equation for a straight line has this form:

.. math:: \vec{w} \mathbf{D} - \vec{b} = 0

were :math:`\vec{w}` is a weight vector, :math:`\mathbf{D}` is the data matrix,
and :math:`\vec{b}` is the offset of the dataset form the origin, or the bias.
We're not so interested for now in :math:`\vec{b}`,
so we can get rid of that by subtracting the mean from our data to get
:math:`\mathbf{D_{C}}` the per-column (i.e. variable) demeaned data that is now
centered around the origin.
"""

zero_meaned_data = all_data - all_data.mean(axis=0)

"""
Now, having gotten rid of that annoying bias term,
we want to find a weight vector which gives us the best solution
that we can find to this equation:

.. math:: \mathbf{D_{C}} \vec{w} = \vec{o}

were :math:`\vec{o}` is the desired output, or the class labels.  But, there is
no such perfect set of weights.  We can only get a best fit, such that

.. math:: \mathbf{D_{C}} \vec{w} = \vec{o} + \vec{e}

where the error term :math:`\vec{e}` is as small as possible.

Note that our equation 

.. math:: \mathbf{D_{C}} \vec{w} = \vec{o}

has exactly the same form as the equation
from the tutorial code in 
http://www.dartmouth.edu/~raj/Matlab/fMRI/design_matrix_tutorial.m
which is:

.. math:: \mathbf{X} \vec{\beta} = \vec{y}

where :math:`\mathbf{X}` was the design matrix, :math:`\vec{\beta}` the
sensitivity vector, and :math:`\vec{y}` the voxel response.

The way we solve the equation is exactly the same, too.
If we could find a matrix-inverse of the data matrix,
then we could pre-multiply both sides by that inverse,
and that would give us the weights:

.. math:: \mathbf{D_{C}^{-1}} \mathbf{D_{C}} \vec{w} = \mathbf{D_{C}^{-1}} \vec{o}

The :math:`\mathbf{D_{C}^{-1}}` and :math:`\mathbf{D_{C}}` terms on the left
would cancel each other out, and we would be left with:

.. math:: \vec{w} = \mathbf{D_{C}^{-1}} \vec{o}

However, unfortunately there will in general not exist any matrix-inverse of
the data matrix :math:`\mathbf{D_{C}}`.  Only square matrices have inverses,
and not even all of them do.  Luckily, however, we can use something that plays
a similar role, called a pseudo-inverse. In Numpy, this is given by the command
`pinv`.  The pseudo-inverse won't give us a perfect solution to the above
equation but it will give us the best approximate solution, which is what we
want.

So, instead of

.. math:: \vec{w} = \mathbf{D_{C}^{-1}} \vec{o}

we have this equation:
"""

# compute pseudo-inverse as a matrix
pinv = np.linalg.pinv(np.mat(zero_meaned_data))
# column-vector of observations
y = all_desired_output[np.newaxis].T

weights = pinv * y

"""
Let's have a look at how these weights carve up the input space
A useful command for making grids of points
which span a particular 2D space is called "meshgrid"
"""

gridspec = np.linspace(-4, 4, 20)
input_space_X, input_space_Y = np.meshgrid(gridspec, gridspec)

# for the rest it is easier to have `weights` as a simple array, instead
# of a matrix
weights = weights.A

weighted_output_Z = input_space_X * weights[0] + input_space_Y * weights[1]

"""
The weighted output gets turned into the category-decision `+1`
if it is greater than 0, and `-1` if it is less than zero.
Let's plot the decision surface color-coded and then plot the zero-meaned
sumo and basketball data on top.
"""

pl.figure()
pl.pcolor(input_space_X, input_space_Y, weighted_output_Z,
         cmap=pl.cm.Spectral)
pl.plot(zero_meaned_data[all_desired_output == 1, 0],
       zero_meaned_data[all_desired_output == 1, 1],
       'ro', linewidth=2, label="Sumo wrestlers")
pl.plot(zero_meaned_data[all_desired_output == -1, 0],
       zero_meaned_data[all_desired_output == -1, 1],
       'bx', linewidth=2, label="Basketball players")
pl.xlim(-4, 4)
pl.ylim(-4, 4)
pl.colorbar()
pl.xlabel('Demeaned height')
pl.ylabel('Demeaned weight')
pl.title('Decision output')
pl.legend()


from mvpa2.base import cfg
if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = kerneldemo
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Kernel-Demo
===========

This is an example demonstrating various kernel implementation in PyMVPA.
"""

import numpy as np
from mvpa2.support.pylab import pl

#from mvpa2.suite import *
from mvpa2.base import cfg
from mvpa2.kernels.np import *


# np.random.seed(1)
data = np.random.rand(4, 2)

for kernel_class, kernel_args in (
    (ConstantKernel, {'sigma_0':1.0}),
    (ConstantKernel, {'sigma_0':1.0}),
    (GeneralizedLinearKernel, {'Sigma_p':np.eye(data.shape[1])}),
    (GeneralizedLinearKernel, {'Sigma_p':np.ones(data.shape[1])}),
    (GeneralizedLinearKernel, {'Sigma_p':2.0}),
    (GeneralizedLinearKernel, {}),
    (ExponentialKernel, {}),
    (SquaredExponentialKernel, {}),
    (Matern_3_2Kernel, {}),
    (Matern_5_2Kernel, {}),
    (RationalQuadraticKernel, {}),
    ):
    kernel = kernel_class(**kernel_args)
    print kernel
    result = kernel.compute(data)

# In the following we draw some 2D functions at random from the
# distribution N(O,kernel) defined by each available kernel and
# plot them. These plots shows the flexibility of a given kernel
# (with default parameters) when doing interpolation. The choice
# of a kernel defines a prior probability over the function space
# used for regression/classfication with GPR/GPC.
count = 1
for k in kernel_dictionary.keys():
    pl.subplot(3, 4, count)
    # X = np.random.rand(size)*12.0-6.0
    # X.sort()
    X = np.arange(-1, 1, .02)
    X = X[:, np.newaxis]
    ker = kernel_dictionary[k]()
    ker.compute(X, X)
    print k
    K = np.asarray(ker)
    for i in range(10):
        f = np.random.multivariate_normal(np.zeros(X.shape[0]), K)
        pl.plot(X[:, 0], f, "b-")

    pl.title(k)
    pl.axis('tight')
    count += 1

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = knn_plot
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
kNN -- Model Flexibility in Pictures
====================================

.. index:: kNN

TODO

"""

import numpy as np


"""

"""

import mvpa2
from mvpa2.base import cfg
from mvpa2.misc.data_generators import *
from mvpa2.clfs.knn import kNN
from mvpa2.misc.plot import *

mvpa2.seed(0)                            # to reproduce the plot

dataset_kwargs = dict(nfeatures=2, nchunks=10,
    snr=2, nlabels=4, means=[ [0,1], [1,0], [1,1], [0,0] ])

dataset_train = normal_feature_dataset(**dataset_kwargs)
dataset_plot = normal_feature_dataset(**dataset_kwargs)


# make a new figure
pl.figure(figsize=(9, 9))

for i,k in enumerate((1, 3, 9, 20)):
    knn = kNN(k)

    print "Processing kNN(%i) problem..." % k
    pl.subplot(2, 2, i+1)

    """
    """

    knn.train(dataset_train)

    plot_decision_boundary_2d(
        dataset_plot, clf=knn, maps='targets')

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = match_distribution
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Determine the Distribution of some Variable
===========================================

This is an example demonstrating discovery of the distribution
facility.
"""

from mvpa2.suite import *

verbose.level = 2
if __debug__:
    # report useful debug information for the example
    debug.active += ['STAT', 'STAT_']

"""

While doing distribution matching, this example also demonstrates
infrastructure within PyMVPA to log a progress report not only on the
screen, but also into external files, such as

- simple text file,
- PDF file including all text messages and pictures which were rendered.

For PDF report you need to have ``reportlab`` external available.

"""

report = Report(name='match_distribution_report',
                title='PyMVPA Example: match_distribution.py')
verbose.handlers += [report]     # Lets add verbose output to the report.
                                 # Similar action could be done to 'debug'

# Also append verbose output into a log file we care about
verbose.handlers += [open('match_distribution_report.log', 'a')]

#
# Figure for just normal distribution
#

# generate random signal from normal distribution
verbose(1, "Random signal with normal distribution")
data = np.random.normal(size=(1000, 1))

# find matching distributions
# NOTE: since kstest is broken in older versions of scipy
#       p-roc testing is done here, which aims to minimize
#       false positives/negatives while doing H0-testing
test = 'p-roc'
figsize = (15, 10)
verbose(1, "Find matching datasets")
matches = match_distribution(data, test=test, p=0.05)

pl.figure(figsize=figsize)
pl.subplot(2, 1, 1)
plot_distribution_matches(data, matches, legend=1, nbest=5)
pl.title('Normal: 5 best distributions')

pl.subplot(2, 1, 2)
plot_distribution_matches(data, matches, nbest=5, p=0.05,
                        tail='any', legend=4)
pl.title('Accept regions for two-tailed test')

# we are done with the figure -- add it to report
report.figure()

#
# Figure for fMRI data sample we have
#
verbose(1, "Load sample fMRI dataset")
attr = SampleAttributes(os.path.join(pymvpa_dataroot, 'attributes.txt'))
dataset = fmri_dataset(samples=os.path.join(pymvpa_dataroot, 'bold.nii.gz'),
                        targets=attr.targets,
                        chunks=attr.chunks,
                        mask=os.path.join(pymvpa_dataroot, 'mask.nii.gz'))
# select random voxel
dataset = dataset[:, int(np.random.uniform()*dataset.nfeatures)]

verbose(2, "Minimal preprocessing to remove the bias per each voxel")
poly_detrend(dataset, chunks_attr='chunks', polyord=1)
zscore(dataset, chunks_attr='chunks', param_est=('targets', ['0']),
       dtype='float32')

# on all voxels at once, just for the sake of visualization
data = dataset.samples.ravel()
verbose(2, "Find matching distribution")
matches = match_distribution(data, test=test, p=0.05)

pl.figure(figsize=figsize)
pl.subplot(2, 1, 1)
plot_distribution_matches(data, matches, legend=1, nbest=5)
pl.title('Random voxel: 5 best distributions')

pl.subplot(2, 1, 2)
plot_distribution_matches(data, matches, nbest=5, p=0.05,
                        tail='any', legend=4)
pl.title('Accept regions for two-tailed test')
report.figure()

if cfg.getboolean('examples', 'interactive', True):
    # store the report
    report.save()
    # show the cool figure
    pl.show()

"""
Example output for a random voxel is

.. image:: ../pics/ex_match_distribution.*
   :align: center
   :alt: Matching distributions for a random voxel

"""

########NEW FILE########
__FILENAME__ = mdp_mnist
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""

.. index:: MDP

Classifying the MNIST handwritten digits with MDP
=================================================

This example will demonstrate how to embed MDP_'s flows_ into a PyMVPA-based
analysis. We will perform a classification of a large number of images of
handwritten digits from the :ref:`MNIST <datadb_mnist>` database. To get a
better sense of how MDP blends into PyMVPA, we will do the same analysis with
MDP only first, and then redo it in PyMVPA -- only using particular bits from
MDP.

.. _MDP: http://mdp-toolkit.sourceforge.net
.. _flows: http://mdp-toolkit.sourceforge.net/tutorial/flows.html

But first we need some helper to load the MNIST data. The following function
will load four NumPy arrays from an HDF5 file in the PyMVPA Data DB. These
arrays are the digit images and the numerical labels for training and testing
dataset respectively. All 28x28 pixel images are stored as flattened vectors.

"""

import os
from mvpa2.base.hdf5 import h5load
from mvpa2 import pymvpa_datadbroot

def load_data():
    data = h5load(os.path.join(pymvpa_datadbroot, 'mnist', "mnist.hdf5"))
    traindata = data['train'].samples
    trainlabels = data['train'].sa.labels
    testdata = data['test'].samples
    testlabels = data['test'].sa.labels
    return traindata, trainlabels, testdata, testlabels

"""

MDP-style classification
------------------------

Here is how to get the classification of the digit images done in MDP.  The
data is preprocessed by whitening, followed by polynomial expansion, and a
subsequent projection on a nine-dimensional discriminant analysis solution.
There is absolutely no need to do this particular pre-processing, it is just
done to show off some MDP features. The actual classification is performed
by a Gaussian classifier. The training data needs to be fed in different ways
to the individual nodes of the flow. The whitening needs only the images,
polynomial expansion needs no training at all, and FDA as well as the
classifier also need the labels. Moreover, a custom iterator is used to feed
data in chunks to the last two nodes of the flow.

"""

import numpy as np
import mdp

class DigitsIterator:
    def __init__(self, digits, labels):
        self.digits = digits
        self.targets = labels
    def __iter__(self):
        frac = 10
        ll = len(self.targets)
        for i in xrange(frac):
            yield self.digits[i*ll/frac:(i+1)*ll/frac], \
                  self.targets[i*ll/frac:(i+1)*ll/frac]

traindata, trainlabels, testdata, testlabels = load_data()

fdaclf = (mdp.nodes.WhiteningNode(output_dim=10, dtype='d') +
          mdp.nodes.PolynomialExpansionNode(2) +
          mdp.nodes.FDANode(output_dim=9) +
          mdp.nodes.GaussianClassifier())

fdaclf.verbose = True

fdaclf.train([[traindata],
               None,
               DigitsIterator(traindata,
                              trainlabels),
               DigitsIterator(traindata,
                              trainlabels)
               ])

"""
After training, we feed the test data through the flow to obtain the
predictions. First through the pre-processing nodes and then through
the classifier, extracting the predicted labels only. Finally, the
prediction error is computed.
"""

feature_space = fdaclf[:-1](testdata)
guess = fdaclf[-1].label(feature_space)
err = 1 - np.mean(guess == testlabels)
print 'Test error:', err

"""

Doing it the PyMVPA way
-----------------------

Analog to the previous approach we load the data first. This time, however,
we convert it into a PyMVPA dataset. Training and testing data are initially
created as two separate datasets, get tagged as 'train' and 'test' respectively,
and are finally stacked into a single Dataset of 70000 images and their
numerical labels.
"""

import pylab as pl
from mvpa2.suite import *

traindata, trainlabels, testdata, testlabels = load_data()
train = dataset_wizard(
        traindata,
        targets=trainlabels,
        chunks='train')
test = dataset_wizard(
        testdata,
        targets=testlabels,
        chunks='test')
# merge the datasets into on
ds = vstack((train, test))
ds.init_origids('samples')

"""
For this analysis we will use the exact same pre-processing as in the MDP
code above, by using the same MDP nodes, in an MDP flow that is shortened only
by the Gaussian classifier. The glue between these MDP nodes and PyMVPA is the
:class:`~mvpa2.mappers.mdp_adaptor.MDPFlowMapper`. This mapper is able to supply
nodes with optional arguments for their training. In this example a
:class:`~mvpa2.base.dataset.DatasetAttributeExtractor` is used to feed the
labels of the training dataset to the FDA node (in addition to the training
data itself).
"""

fdaflow = (mdp.nodes.WhiteningNode(output_dim=10, dtype='d') +
           mdp.nodes.PolynomialExpansionNode(2) +
           mdp.nodes.FDANode(output_dim=9))
fdaflow.verbose = True

mapper = MDPFlowMapper(fdaflow,
                       ([], [], [DatasetAttributeExtractor('sa', 'targets')]))

"""
The :class:`~mvpa2.mappers.mdp_adaptor.MDPFlowMapper` can represent any MDP flow
as a PyMVPA mapper. In this example, we attach the MDP-based pre-processing
flow, wrapped in the mapper, to a classifier (arbitrarily chosen to be SMLR)
via a :class:`~mvpa2.clfs.meta.MappedClassifier`. In doing so we achieve that
the training data is automatically pre-processed before it is used to train the
classifier, and later on the same pre-processing it applied to the testing data,
before the classifier is asked to make its predictions.

At last we wrap the MappedClassifier into a
:class:`~mvpa2.measures.base.TransferMeasure` that splits the dataset into a
training and testing part. In this particular case this is not really
necessary, as we could have left training and testing data separate in the
first place, and could have called the classifier's ``train()`` and
``predict()`` manually. However, when doing repeated train/test cycles as, for
example, in a cross-validation this is not very useful. In this particular case
the TransferMeasure computes a number of performance measures for us that we
only need to extract.
"""

tm = TransferMeasure(MappedClassifier(SMLR(), mapper),
                     Splitter('chunks', attr_values=['train', 'test']),
                     enable_ca=['stats', 'samples_error'])
tm(ds)
print 'Test error:', 1 - tm.ca.stats.stats['ACC']

"""
Visualizing data and results
----------------------------

The analyses are already done. But for the sake of completeness we take a final
look at both data and results. First and few examples of the training data.

"""

examples = [3001 + 5940 * i for i in range(10)]

pl.figure(figsize=(2, 5))

for i, id_ in enumerate(examples):
    ax = pl.subplot(2, 5, i+1)
    ax.axison = False
    pl.imshow(traindata[id_].reshape(28, 28).T, cmap=pl.cm.gist_yarg,
             interpolation='nearest', aspect='equal')

pl.subplots_adjust(left=0, right=1, bottom=0, top=1,
                  wspace=0.05, hspace=0.05)
pl.draw()

"""
And finally we take a peak at the result of pre-processing for a number of
example images for each digit. The following plot shows the training data on
hand-picked three-dimensional subset of the original nine FDA dimension the
data was projected on.

"""

if externals.exists('matplotlib') \
   and externals.versions['matplotlib'] >= '0.99':
    from mpl_toolkits.mplot3d import Axes3D
    pts = []
    for i, ex in enumerate(examples):
        pts.append(mapper.forward(ds.samples[ex:ex+200])[:, :3])

    fig = pl.figure()

    ax = Axes3D(fig)
    colors = ('r','g','b','k','c','m','y','burlywood','chartreuse','gray')
    clouds = []
    for i, p in enumerate(pts):
        print i
        clouds.append(ax.plot(p[:, 0], p[:, 1], p[:, 2], 'o', c=colors[i],
                              label=str(i), alpha=0.6))

    ax.legend([str(i) for i in range(10)])
    pl.draw()

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""
.. image:: ../pics/ex_mdp_fda.*

Note: The data visualization depends on the 3D matplotlib features, which are
available only from version 0.99.
"""

########NEW FILE########
__FILENAME__ = mri_plot
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Basic (f)MRI plotting
=====================

.. index:: plotting

When running an fMRI data analysis it is often necessary to visualize results
in their original dataspace, typically as an overlay on some anatomical brain
image. PyMVPA has the ability to export results into the NIfTI format, and via
this data format it is compatible with virtually any MRI visualization software.

However, sometimes having a scriptable plotting facility within Python is
desired. There are a number of candidate tools for this purpose (e.g. Mayavi_),
but also PyMVPA itself offers some basic MRI plotting.

.. _Mayavi: http://code.enthought.com/projects/mayavi/

In this example, we are showing a quick-and-dirty plot of a voxel-wise
ANOVA measure, overlaid on the respective brain anatomy. Note that the plotting
is not specific to ANOVAs. Any feature-wise measure can be plotted this way.

We start with basic steps: loading PyMVPA and the example fMRI dataset, only
select voxels that correspond to some pre-computed gray matter mask, do basic
preprocessing, and estimate ANOVA scores. This has already been described
elsewhere, hence we only provide the code here for the sake of completeness.
"""

from mvpa2.suite import *

# load PyMVPA example dataset
datapath = os.path.join(pymvpa_datadbroot,
                        'tutorial_data', 'tutorial_data', 'data')
attr = SampleAttributes(os.path.join(datapath, 'attributes.txt'))
dataset = fmri_dataset(samples=os.path.join(datapath, 'bold.nii.gz'),
                       targets=attr.targets,
                       chunks=attr.chunks,
                       mask=os.path.join(datapath, 'mask_gray.nii.gz'))

# do chunkswise linear detrending on dataset
poly_detrend(dataset, chunks_attr='chunks')

# exclude the rest conditions from the dataset, since that should be
# quite different from the 'active' conditions, and make the computation
# below pointless
dataset = dataset[dataset.sa.targets != 'rest']

# define sensitivity analyzer to compute ANOVA F-scores on the remaining
# samples
sensana = OneWayAnova()
sens = sensana(dataset)

"""
The measure is computed, and we can look at the actual plotting. Typically, it
is useful to pre-define some common plotting arguments, for example to ensure
consistency throughout multiple figures. This following sets up which backround
image to use (``background``), which portions of the image to plot
(``background_mask``), and which portions of the overlay images to plot
(``overlay_mask``). All these arguments are actually NIfTI images of the same
dimensions and orientation as the to be plotted F-scores image. the remaining
settings configure the colormaps to be used for plotting and trigger
interactive plotting.
"""

mri_args = {
    'background' : os.path.join(datapath, 'anat.nii.gz'),
    'background_mask' : os.path.join(datapath, 'mask_brain.nii.gz'),
    'overlay_mask' : os.path.join(datapath, 'mask_gray.nii.gz'),
    'cmap_bg' : 'gray',
    'cmap_overlay' : 'autumn', # YlOrRd_r # pl.cm.autumn
    'interactive' : cfg.getboolean('examples', 'interactive', True),
    }

"""
All that remains to do is a single call to `plot_lightbox()`. We pass it the
F-score vector. `map2nifti` uses the mapper in our original dataset to project
it back into the functional MRI volume space. We treshold the data with the
interval [0, +inf] (i.e. all possible values and F-Score can have), and select
a subset of slices to be plotted. That's it.
"""


fig = plot_lightbox(overlay=map2nifti(dataset, sens),
              vlim=(0, None), slices=range(25,29), **mri_args)

"""
The resulting figure would look like this:

.. image:: ../pics/ex_plot_lightbox.*
   :align: center
   :alt: Simple plotting facility for (f)MRI. F-scores

In interactive mode it is possible to click on the histogram to adjust the
thresholding of the overlay volumes. Left-click sets the value corresponding
to the lowest value in the colormap, and right-click set the value for the upper
end of the colormap. Try right-clicking somewhere at the beginning of the x-axis
and left on the end of the x-axis.
"""

########NEW FILE########
__FILENAME__ = nested_cv
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Nested Cross-Validation
=======================

.. index:: model selection, cross-validation

Often it is desired to explore multiple models (classifiers,
parameterizations) but it becomes an easy trap for introducing an
optimistic bias into generalization estimate.  The easiest but
computationally intensive solution to overcome such a bias is to carry
model selection by estimating the same (or different) performance
characteristic while operating only on training data.  If such
performance is a cross-validation, then it leads to the so called
"nested cross-validation" procedure.

This example will demonstrate on how to implement such nested
cross-validation while selecting the best performing classifier from
the warehouse of available within PyMVPA.
"""

from mvpa2.suite import *
# increase verbosity a bit for now
verbose.level = 3
# pre-seed RNG if you want to investigate the effects, thus
# needing reproducible results
#mvpa2.seed(3)

"""
For this simple example lets generate some fresh random data with 2
relevant features and low SNR.
"""

dataset = normal_feature_dataset(perlabel=24, nlabels=2, nchunks=3,
                                 nonbogus_features=[0, 1],
                                 nfeatures=100, snr=3.0)

"""
For the demonstration of model selection benefit, lets first compute
cross-validated error using simple and popular kNN.
"""

clf_sample = kNN()
cv_sample = CrossValidation(clf_sample, NFoldPartitioner())

verbose(1, "Estimating error using a sample classifier")
error_sample = np.mean(cv_sample(dataset))

"""
For the convenience lets define a helpful function which we will use
twice -- once within cross-validation, and once on the whole dataset
"""

def select_best_clf(dataset_, clfs):
    """Select best model according to CVTE

    Helper function which we will use twice -- once for proper nested
    cross-validation, and once to see how big an optimistic bias due
    to model selection could be if we simply provide an entire dataset.

    Parameters
    ----------
    dataset_ : Dataset
    clfs : list of Classifiers
      Which classifiers to explore

    Returns
    -------
    best_clf, best_error
    """
    best_error = None
    for clf in clfs:
        cv = CrossValidation(clf, NFoldPartitioner())
        # unfortunately we don't have ability to reassign clf atm
        # cv.transerror.clf = clf
        try:
            error = np.mean(cv(dataset_))
        except LearnerError, e:
            # skip the classifier if data was not appropriate and it
            # failed to learn/predict at all
            continue
        if best_error is None or error < best_error:
            best_clf = clf
            best_error = error
        verbose(4, "Classifier %s cv error=%.2f" % (clf.descr, error))
    verbose(3, "Selected the best out of %i classifiers %s with error %.2f"
            % (len(clfs), best_clf.descr, best_error))
    return best_clf, best_error

"""
First lets select a classifier within cross-validation, thus
eliminating model-selection bias
"""

best_clfs = {}
confusion = ConfusionMatrix()
verbose(1, "Estimating error using nested CV for model selection")
partitioner = NFoldPartitioner()
splitter = Splitter('partitions')
for isplit, partitions in enumerate(partitioner.generate(dataset)):
    verbose(2, "Processing split #%i" % isplit)
    dstrain, dstest = list(splitter.generate(partitions))
    best_clf, best_error = select_best_clf(dstrain, clfswh['!gnpp'])
    best_clfs[best_clf.descr] = best_clfs.get(best_clf.descr, 0) + 1
    # now that we have the best classifier, lets assess its transfer
    # to the testing dataset while training on entire training
    tm = TransferMeasure(best_clf, splitter,
                         postproc=BinaryFxNode(mean_mismatch_error,
                                               space='targets'),
                         enable_ca=['stats'])
    tm(partitions)
    confusion += tm.ca.stats

"""
And for comparison, lets assess what would be the best performance if
we simply explore all available classifiers, providing all the data at
once
"""


verbose(1, "Estimating error via fishing expedition (best clf on entire dataset)")
cheating_clf, cheating_error = select_best_clf(dataset, clfswh['!gnpp'])

print """Errors:
 sample classifier (kNN): %.2f
 model selection within cross-validation: %.2f
 model selection via fishing expedition: %.2f with %s
 """ % (error_sample, 1 - confusion.stats['ACC'],
        cheating_error, cheating_clf.descr)

print "# of times following classifiers were selected within " \
      "nested cross-validation:"
for c, count in sorted(best_clfs.items(), key=lambda x:x[1], reverse=True):
    print " %i times %s" % (count, c)

print "\nConfusion table for the nested cross-validation results:"
print confusion

########NEW FILE########
__FILENAME__ = permutation_test
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Monte-Carlo testing of Classifier-based Analyses
================================================

.. index:: statistical testing, monte-carlo, permutation

It is often desirable to be able to make statements like *"Performance is
significantly above chance-level"* and to help with that PyMVPA supports *Null*
hypothesis (aka *H0*) testing for any :class:`~mvpa2.measures.base.Measure`.
Measures take an optional constructor argument ``null_dist`` that can be used
to provide an instance of some :class:`~mvpa2.clfs.stats.NullDist` estimator.
If the properties of the expected *Null* distribution are known a-priori, it is
possible to use any distribution specified in SciPy's ``stats`` module for this
purpose (see e.g. :class:`~mvpa2.clfs.stats.FixedNullDist`).

However, as with other applications of statistics in classifier-based analyses
there is the problem that we typically do not know the distribution of a
variable like error or performance under the *Null* hypothesis (i.e. the
probability of a result given that there is no signal), hence we cannot easily
assign the adored p-values. Even worse, the chance-level or guess probability
of a classifier depends on the content of a validation dataset, e.g. balanced
or unbalanced number of samples per label and total number of labels.

One approach to deal with this situation is to *estimate* the *Null*
distribution using permutation testing. The *Null* distribution is then
estimated by computing the measure of interest multiple times using original
data samples but with permuted targets.  Since quite often the exploration of
all permutations is unfeasible, Monte-Carlo testing (see :ref:`Nichols
et al. (2006) <NH02>`) allows to obtain stable estimate with only a
limited number of random permutations.

Given the results computed using permuted targets one can now determine the
probability of the empirical result (i.e. the one computed from the
original training dataset) under the *no signal* condition. This is
simply the fraction of results from the permutation runs that is
larger or smaller than the empirical (depending on whether one is
looking at performances or errors).

Here is how this looks for a simple cross-validated classification in PyMVPA.
We start by generated a dataset with 200 samples and 3 features of which 2 carry
some relevant signal.
"""

# lazy import
from mvpa2.suite import *

# enable progress output for MC estimation
if __debug__:
    debug.active += ["STATMC"]

# some example data with signal
ds = normal_feature_dataset(perlabel=100, nlabels=2, nfeatures=3,
                            nonbogus_features=[0,1], snr=0.3, nchunks=2)

"""
Now we can start collecting the pieces that play a role in this analysis. We
need a classifier.
"""

clf = LinearCSVMC()

"""
We need a :term:`generator` than will produce partitioned datasets, one for each
fold of the cross-validation. A partitioned dataset is basically the same as the
original dataset, but has an additional samples attribute that indicates whether
particular samples will be the *part* of the data that is used for training the
classifier, or for testing it. By default, the
:class:`~mvpa2.generators.partition.NFoldPartitioner` will create a sample
attribute ``partitions`` that will label one :term:`chunk` in each fold
differently from all others (hence mark it as taken-out for testing).
"""

partitioner = NFoldPartitioner()

"""
We need two pieces for the Monte Carlo shuffling. The first of them is
an instance of an
:class:`~mvpa2.generators.permutation.AttributePermutator` that will
permute the target attribute of the dataset for each iteration.  We
will instruct it to perform 200 permutations. In a real analysis the
number of permutations should be larger to get stable estimates.
"""

permutator = AttributePermutator('targets', count=200)

"""
The second mandatory piece for a Monte-Carlo-style estimation of
the *Null* distribution is the actual "estimator".
:class:`~mvpa2.clfs.stats.MCNullDist` will use the
constructed ``permutator`` to shuffle the targets and later on report
p-value from the left tail of the *Null* distribution, because we are
going to compute errors and are interested in them being *lower* than
chance. Finally we also ask for all results from Monte-Carlo shuffling
to be stored for subsequent visualization of the distribution.
"""

distr_est = MCNullDist(permutator, tail='left', enable_ca=['dist_samples'])

"""

Now we have all pieces and can conduct the actual cross-validation. We assign
a post-processing :term:`node` ``mean_sample`` that will take care of averaging
error values across all cross-validation fold. Consequently, the *Null*
distribution of *average cross-validated classification error* will be estimated
and used for statistical evaluation.
"""

cv = CrossValidation(clf, partitioner,
                     errorfx=mean_mismatch_error,
                     postproc=mean_sample(),
                     null_dist=distr_est,
                     enable_ca=['stats'])
# run
err = cv(ds)

"""
Now we have a usual cross-validation error and ``cv`` stores
:term:`conditional attribute`s such as confusion matrices`:
"""

print 'CV-error:', 1 - cv.ca.stats.stats['ACC']

"""
However, in addition it also provides the results of the statistical
evaluation. The :term:`conditional attribute` ``null_prob`` has a
dataset that contains the p-values representing the likelihood of an
error equal or lower to the output one under the *Null* hypothesis,
i.e. no actual relevant signal in the data. For a reason that will
appear sensible later on, the p-value is contained in a dataset.
"""

p = cv.ca.null_prob
# should be exactly one p-value
assert(p.shape == (1,1))
print 'Corresponding p-value:',  np.asscalar(p)

"""
We can now look at the distribution of the errors under *H0* and plot the
expected chance level as well as the empirical error.
"""

# make new figure
pl.figure()
# histogram of all computed errors from permuted data
pl.hist(np.ravel(cv.null_dist.ca.dist_samples), bins=20)
# empirical error
pl.axvline(np.asscalar(err), color='red')
# chance-level for a binary classification with balanced samples
pl.axvline(0.5, color='black', ls='--')
# scale x-axis to full range of possible error values
pl.xlim(0,1)
pl.xlabel('Average cross-validated classification error')

"""
We can see that the *Null* or chance distribution is centered around the
expected chance-level and the empirical error value is in the far left tail,
thus unlikely to belong to *Null* distribution, and hence the low p-value.

This could be the end, but sometimes one needs to have a closer look. Let's say your
data is not that homogeneous. Let's say that some :term:`chunk <Chunk>` may be very
different from others. You might want to look at the error value probability for
specific cross-validation folds. Sounds complicated? Luckily it is very simple.
It only needs a tiny change in the cross-validation setup -- the removal of the
``mean_sample`` post-processing :term:`node`.
"""

cv = CrossValidation(clf, partitioner,
                     errorfx=mean_mismatch_error,
                     null_dist=distr_est,
                     enable_ca=['stats'])
# run
err = cv(ds)

assert (err.shape == (2,1))
print 'CV-errors:', np.ravel(err)

"""
Now we get two errors -- one for each cross-validation fold and
most importantly, we also get the two associated p-values.
"""

p = cv.ca.null_prob
assert(p.shape == (2,1))
print 'Corresponding p-values:',  np.ravel(p)

"""
What happened is that a dedicated *Null* distribution has been estimated for
each element in the measure results. Without ``mean_sample`` an error is
reported for each CV-fold, hence a separate distributions are estimated for
each CV-fold too. And because we have also asked for all distribution samples
to be reported, we can now plot both distribution and both empirical errors.
But how do we figure out with value is which?

As mentioned earlier all results are returned in Datasets. All datasets have
compatible sample and feature axes, hence corresponding elements.
"""

assert(err.shape == p.shape == cv.null_dist.ca.dist_samples.shape[:2])

# let's make a function this time
def plot_cv_results(cv, err, title):
    # make new figure
    pl.figure()
    colors = ['green', 'blue']
    # null distribution samples
    dist_samples = np.asarray(cv.null_dist.ca.dist_samples)
    for i in range(len(err)):
        # histogram of all computed errors from permuted data per CV-fold
        pl.hist(np.ravel(dist_samples[i]), bins=20, color=colors[i],
                label='CV-fold %i' %i, alpha=0.5,
                range=(dist_samples.min(), dist_samples.max()))
        # empirical error
        pl.axvline(np.asscalar(err[i]), color=colors[i])

    # chance-level for a binary classification with balanced samples
    pl.axvline(0.5, color='black', ls='--')
    # scale x-axis to full range of possible error values
    pl.xlim(0,1)
    pl.xlabel(title)

plot_cv_results(cv, err, 'Per CV-fold classification error')

"""
We have already seen that the statistical evaluation is pretty flexible.
However, we haven't yet seen whether it is flexible enough. To illustrate that
think about what was done in the above Monte Carlo analyses.

A dataset was shuffled repeatedly, and for each iteration a full
cross-validation of classification error was performed. However, the shuffling
was done on the *full* dataset, hence target were permuted in both training
*and* testing dataset portions in each CV-fold. This basically means that for
each Monte Carlo iteration the classifier was tested on a new data/signal.
However, we may be more interested in what the classifier has to say on the
*actual* data, but when it was trained on randomly permuted data.

As you can guess this is possible too and goes like this. The most important
difference is that we are going to use  now a dedicate measure to estimate the
*Null* distribution. That measure is very similar to the cross-validation we
have used before, but differs in an important bit. Let's look at the pieces.
"""

# how often do we want to shuffle the data
repeater = Repeater(count=200)
# permute the training part of a dataset exactly ONCE
permutator = AttributePermutator('targets', limit={'partitions': 1}, count=1)
# CV with null-distribution estimation that permutes the training data for
# each fold independently
null_cv = CrossValidation(
            clf,
            ChainNode([partitioner, permutator], space=partitioner.get_space()),
            errorfx=mean_mismatch_error)
# Monte Carlo distribution estimator
distr_est = MCNullDist(repeater, tail='left', measure=null_cv,
                       enable_ca=['dist_samples'])
# actual CV with H0 distribution estimation
cv = CrossValidation(clf, partitioner, errorfx=mean_mismatch_error,
                     null_dist=distr_est, enable_ca=['stats'])

"""
The ``repeater`` is a simple node that returns any given dataset a
configurable number of times. We use the helper to configure the number of
Monte Carlo iterations. The new ``permutator`` is again configured to shuffle
the ``targets`` attribute, but only *once* and only for samples that were
labeled as being part of the training set in a particular CV-fold (the
``partitions`` sample attribute will be created by the NFoldPartitioner that we
have configured earlier).

The most important difference is a new dedicated measure that will be used to
perform a cross-validation analysis under the *H0* hypotheses. To this end
we set up a standard CV procedure with a twist: we use a chained generator
(comprising of the typical partitioner and the new one-time permutator).
This will cause the CV to permute the training set for each CV-fold internally
(and that is what we wanted).

Now we assign the *H0* cross-validation procedure to the distribution
estimator and use the ``repeater`` to set the number of iterations. Lastly, we
plug everything into a standard CV analysis with, again, a non-permuting
``partitioner`` and the pimped *Null* distribution estimator.

Now we just need to run it, and plot the results the same way we did before.
"""

err = cv(ds)
print 'CV-errors:', np.ravel(err)
p = cv.ca.null_prob
print 'Corresponding p-values:',  np.ravel(p)
# plot
plot_cv_results(cv, err,
                'Per CV-fold classification error (only training permutation)')

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""

There a many ways to futher tweak the statistical evaluation. For example, if
the family of the distribution is known (e.g. Gaussian/Normal) and provided
with the ``dist_class`` parameter of ``MCNullDist``, then permutation tests
done by ``MCNullDist`` allow determining the distribution parameters. Under the
(strong) assumption of Gaussian distribution, 20-30 permutations should be
sufficient to get sensible estimates of the distribution parameters.

But that would be another story...

"""

########NEW FILE########
__FILENAME__ = projections
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Visualization of Data Projection Methods
========================================
"""

from mvpa2.support.pylab import pl
from mvpa2.misc.data_generators import noisy_2d_fx
from mvpa2.mappers.svd import SVDMapper
from mvpa2.mappers.mdp_adaptor import ICAMapper, PCAMapper
from mvpa2 import cfg

center = [10, 20]
axis_range = 7

##REF: Name was automagically refactored
def plot_proj_dir(p):
    pl.plot([0, p[0,0]], [0, p[0,1]],
           linewidth=3, hold=True, color='y')
    pl.plot([0, p[1,0]], [0, p[1,1]],
           linewidth=3, hold=True, color='k')

mappers = {
            'PCA': PCAMapper(),
            'SVD': SVDMapper(),
            'ICA': ICAMapper(alg='CuBICA'),
          }
datasets = [
    noisy_2d_fx(100, lambda x: x, [lambda x: x],
                center, noise_std=0.5),
    noisy_2d_fx(50, lambda x: x, [lambda x: x, lambda x: -x],
                center, noise_std=0.5),
    noisy_2d_fx(50, lambda x: x, [lambda x: x, lambda x: 0],
                center, noise_std=0.5),
   ]

ndatasets = len(datasets)
nmappers = len(mappers.keys())

pl.figure(figsize=(8,8))
fig = 1

for ds in datasets:
    for mname, mapper in mappers.iteritems():
        mapper.train(ds)

        dproj = mapper.forward(ds.samples)
        mproj = mapper.proj
        pl.subplot(ndatasets, nmappers, fig)
        if fig <= 3:
            pl.title(mname)
        pl.axis('equal')

        pl.scatter(ds.samples[:, 0] - center[0],
                  ds.samples[:, 1] - center[1],
                  s=30, c=(ds.sa.targets) * 200)
        plot_proj_dir(mproj)
        fig += 1


if cfg.getboolean('examples', 'interactive', True):
    pl.show()

"""
Output of the example:

.. image:: ../pics/ex_projections.*
   :align: center
   :alt: SVD/ICA/PCA projections

"""

########NEW FILE########
__FILENAME__ = pylab_2d
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Simple Plotting of Classifier Behavior
======================================

.. index:: plotting example

This example runs a number of classifiers on a simple 2D dataset and plots the
decision surface of each classifier.

First compose some sample data -- no PyMVPA involved.
"""

import numpy as np

# set up the labeled data
# two skewed 2-D distributions
num_dat = 200
dist = 4
# Absolute max value allowed. Just to assure proper plots
xyamax = 10
feat_pos=np.random.randn(2, num_dat)
feat_pos[0, :] *= 2.
feat_pos[1, :] *= .5
feat_pos[0, :] += dist
feat_pos = feat_pos.clip(-xyamax, xyamax)
feat_neg=np.random.randn(2, num_dat)
feat_neg[0, :] *= .5
feat_neg[1, :] *= 2.
feat_neg[0, :] -= dist
feat_neg = feat_neg.clip(-xyamax, xyamax)

# set up the testing features
npoints = 101
x1 = np.linspace(-xyamax, xyamax, npoints)
x2 = np.linspace(-xyamax, xyamax, npoints)
x,y = np.meshgrid(x1, x2);
feat_test = np.array((np.ravel(x), np.ravel(y)))

"""Now load PyMVPA and convert the data into a proper
:class:`~mvpa2.datasets.base.Dataset`."""

from mvpa2.suite import *

# create the pymvpa dataset from the labeled features
patternsPos = dataset_wizard(samples=feat_pos.T, targets=1)
patternsNeg = dataset_wizard(samples=feat_neg.T, targets=0)
ds_lin = vstack((patternsPos, patternsNeg))

"""Let's add another dataset: XOR. This problem is not linear separable
and therefore need a non-linear classifier to be solved. The dataset is
provided by the PyMVPA dataset warehouse.
"""

# 30 samples per condition, SNR 2
ds_nl = pure_multivariate_signal(30, 2)
l1 = ds_nl.sa['targets'].unique[1]

datasets = {'linear': ds_lin, 'non-linear': ds_nl}

"""This demo utilizes a number of classifiers. The instantiation of a
classifier involves almost no runtime costs, so it is easily possible
compile a long list, if necessary."""

# set up classifiers to try out
clfs = {
        'Ridge Regression': RidgeReg(),
        'Linear SVM': LinearNuSVMC(probability=1,
                      enable_ca=['probabilities']),
        'RBF SVM': RbfNuSVMC(probability=1,
                      enable_ca=['probabilities']),
        'SMLR': SMLR(lm=0.01),
        'Logistic Regression': PLR(criterion=0.00001),
        '3-Nearest-Neighbour': kNN(k=3),
        '10-Nearest-Neighbour': kNN(k=10),
        'GNB': GNB(common_variance=True),
        'GNB(common_variance=False)': GNB(common_variance=False),
        'LDA': LDA(),
        'QDA': QDA(),
        }

# How many rows/columns we need
nx = int(ceil(np.sqrt(len(clfs))))
ny = int(ceil(len(clfs)/float(nx)))

"""Now we are ready to run the classifiers. The following loop trains
and queries each classifier to finally generate a nice plot showing
the decision surface of each individual classifier, both for the linear and
the non-linear dataset."""

for id, ds in datasets.iteritems():
    # loop over classifiers and show how they do
    fig = 0

    # make a new figure
    pl.figure(figsize=(nx*4, ny*4))

    print "Processing %s problem..." % id

    for c in sorted(clfs):
        # tell which one we are doing
        print "Running %s classifier..." % (c)

        # make a new subplot for each classifier
        fig += 1
        pl.subplot(ny, nx, fig)

        # select the clasifier
        clf = clfs[c]

        # enable saving of the estimates used for the prediction
        clf.ca.enable('estimates')

        # train with the known points
        clf.train(ds)

        # run the predictions on the test values
        pre = clf.predict(feat_test.T)

        # if ridge, use the prediction, otherwise use the values
        if c == 'Ridge Regression':
            # use the prediction
            res = np.asarray(pre)
        elif 'Nearest-Ne' in c:
            # Use the dictionaries with votes
            res = np.array([e[l1] for e in clf.ca.estimates]) \
                  / np.sum([e.values() for e in clf.ca.estimates], axis=1)
        elif c == 'Logistic Regression':
            # get out the values used for the prediction
            res = np.asarray(clf.ca.estimates)
        elif c in ['SMLR']:
            res = np.asarray(clf.ca.estimates[:, 1])
        elif c in ['LDA', 'QDA'] or c.startswith('GNB'):
            # Since probabilities are logprobs -- just for
            # visualization of trade-off just plot relative
            # "trade-off" which determines decision boundaries if an
            # alternative log-odd value was chosen for a cutoff
            res = np.asarray(clf.ca.estimates[:, 1]
                             - clf.ca.estimates[:, 0])
            # Scale and position around 0.5
            res = 0.5 + res/max(np.abs(res))
        else:
            # get the probabilities from the svm
            res = np.asarray([(q[1][1] - q[1][0] + 1) / 2
                    for q in clf.ca.probabilities])

        # reshape the results
        z = np.asarray(res).reshape((npoints, npoints))

        # plot the predictions
        pl.pcolor(x, y, z, shading='interp')
        pl.clim(0, 1)
        pl.colorbar()
        # plot decision surfaces at few levels to emphasize the
        # topology
        pl.contour(x, y, z, [0.1, 0.4, 0.5, 0.6, 0.9],
                   linestyles=['dotted', 'dashed', 'solid', 'dashed', 'dotted'],
                   linewidths=1, colors='black', hold=True)

        # plot the training points
        pl.plot(ds.samples[ds.targets == 1, 0],
               ds.samples[ds.targets == 1, 1],
               "r.")
        pl.plot(ds.samples[ds.targets == 0, 0],
               ds.samples[ds.targets == 0, 1],
               "b.")

        pl.axis('tight')
        # add the title
        pl.title(c)

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = rsa_fmri
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Representational similarity analysis (RSA) on fMRI data
=======================================================

.. index:: rsa_fmri

In this example we are going to take a look at representational similarity
analysis (RSA). This term was coined by :ref:`Kriegeskorte et al. (2008)
<KMB08>` and refers to a technique were data samples are converted into a
self-referential distance space, in order to aid comparsion across domains. The
premise is that whenever no appropriate transformation is known to directly
compare two types of data directly (1st-level), it is still useful to compare
similarities computed in individual domains (2nd-level). For example, this
analysis technique has been used to identify inter-species commonalities in
brain response pattern variations during stimulation with visual objects
(single-cell recordings in monkeys compared to human fMRI, Krigeskorte et al.,
2008), and to relate brain response pattern similarities to predictions of
computational models of perception (Connolly et al., 2012).
"""

import numpy as np
import pylab as pl
from mvpa2 import cfg

"""
In this example we use a dataset from :ref:`Haxby et al. (2001) <HGF+01>` were
participants watched pictures of eight different visual objects, while fMRI was
recorded. The following snippet load a portion of this dataset (single subject)
from regions on the ventral and occipital surface of the brain.
"""

# load dataset -- ventral and occipital ROIs
from mvpa2.misc.data_generators import load_datadb_tutorial_data
ds = load_datadb_tutorial_data(roi=(15, 16, 23, 24, 36, 38, 39, 40, 48))

"""
We only do minimal pre-processing: linear trend removal and Z-scoring all voxel
time-series with respect to the mean and standard deviation of the "rest"
condition.
"""

# only minial detrending
from mvpa2.mappers.detrend import poly_detrend
poly_detrend(ds, polyord=1, chunks_attr='chunks')
# z-scoring with respect to the 'rest' condition
from mvpa2.mappers.zscore import zscore
zscore(ds, chunks_attr='chunks', param_est=('targets', 'rest'))
# now remove 'rest' samples
ds = ds[ds.sa.targets != 'rest']

"""
RSA is all about so-called dissimilarity matrices: square, symetric matrices
with a zero diagonal that encode the (dis)similarity between all pairs of
data samples or conditions in a dataset. We compose a little helper function
to plot such matrices, including a color-scale and proper labeling of matrix
rows and columns.
"""

# little helper function to plot dissimilarity matrices
def plot_mtx(mtx, labels, title):
    pl.figure()
    pl.imshow(mtx, interpolation='nearest')
    pl.xticks(range(len(mtx)), labels, rotation=-45)
    pl.yticks(range(len(mtx)), labels)
    pl.title(title)
    pl.clim((0,1))
    pl.colorbar()

"""
As a start, we want to inspect the dissimilarity structure of the stimulation
conditions in the entire ROI. For this purpose, we average all samples of
each conditions into a single examplar, using an FxMapper() instance.
"""

# compute a dataset with the mean samples for all conditions
from mvpa2.mappers.fx import mean_group_sample
mtgs = mean_group_sample(['targets'])
mtds = mtgs(ds)

"""
After these preparations we can use the PDist() measure to compute the desired
distance matrix -- by default using correlation distance as a metric. The
``square`` argument will cause a ful square matrix to be
produced, instead of a leaner upper-triangular matrix in vector form.
"""

# basic ROI RSA -- dissimilarity matrix for the entire ROI
from mvpa2.measures import rsa
dsm = rsa.PDist(square=True)
res = dsm(mtds)
plot_mtx(res, mtds.sa.targets, 'ROI pattern correlation distances')

"""
Inspecting the figure we can see that there is not much structure in the matrix,
except for the face and the house condition being slightly more dissimilar than
others.

Now, let's take a look at the variation of similarity structure through the
brain. We can plug the PDist() measure into a searchlight to quickly scan the
brain and harvest this information.
"""

# same as above, but done in a searchlight fashion
from mvpa2.measures.searchlight import sphere_searchlight
dsm = rsa.PDist(square=False)
sl = sphere_searchlight(dsm, 2)
slres = sl(mtds)

"""
The result is a compact distance matrix in vector form for each searchlight
location. We can now try to score each matrix. Let's find the distance matrix
with the largest overall distances across all stimulation conditions, i.e.
the location in the brain where brain response patterns are most dissimilar.
"""

# score each searchlight sphere result wrt global pattern dissimilarity
distinctiveness = np.sum(np.abs(slres), axis=0)
print 'Most dissimilar patterns around', \
        mtds.fa.voxel_indices[distinctiveness.argmax()]
# take a look at the this dissimilarity structure
from scipy.spatial.distance import squareform
plot_mtx(squareform(slres.samples[:, distinctiveness.argmax()]),
         mtds.sa.targets,
         'Maximum distinctive searchlight pattern correlation distances')

"""
That looks confusing. But how do we know that this is not just noise (it
probably is)? One way would be to look at how stable a distance matrix is,
when computed for different portions of a dataset.

To perform this analysis, we use another FxMapper() instance that averages
all data into a single sample per stimulation conditions, per ``chunk``. A
chunk in this context indicates a complete fMRI recording run.
"""

# more interesting: let's look at the stability of similarity sturctures
# across experiment runs
# mean condition samples, as before, but now individually for each run
mtcgs = mean_group_sample(['targets', 'chunks'])
mtcds = mtcgs(ds)

"""
With this dataset we can use PDistConsistency() to compute the similarity
of dissimilarity matrices computes from different chunks. And, of course,
it can be done in a searchlight.
"""

# searchlight consistency measure -- how correlated are the structures
# across runs
dscm = rsa.PDistConsistency()
sl_cons = sphere_searchlight(dscm, 2)
slres_cons = sl_cons(mtcds)

"""
Now we can determine the most brain location with the most stable
dissimilarity matrix.
"""

# mean correlation
mean_consistency = np.mean(slres_cons, axis=0)
print 'Most stable dissimilarity patterns around', \
        mtds.fa.voxel_indices[mean_consistency.argmax()]
# Look at this pattern
plot_mtx(squareform(slres.samples[:, mean_consistency.argmax()]),
         mtds.sa.targets,
         'Most consistent searchlight pattern correlation distances')

"""
It is all in the face!

It would be interesting to know where in the brain dissimilarity structures
can be found that are similar to this one. PDistTargetSimilarity() can
be used to discover this kind of information with any kind of target
dissimilarity structure. We need to transpose the result for aggregation
into a searchlight map, as PDistTargetSimilarity can return more features
than just the correlation coefficient.
"""

# let's see where in the brain we find dissimilarity structures that are
# similar to our most stable one
tdsm = rsa.PDistTargetSimilarity(
            slres.samples[:, mean_consistency.argmax()])
# using a searchlight
from mvpa2.base.learner import ChainLearner
from mvpa2.mappers.shape import TransposeMapper
sl_tdsm = sphere_searchlight(ChainLearner([tdsm, TransposeMapper()]), 2)
slres_tdsm = sl_tdsm(mtds)

"""
Lastly, we can map this result back onto the 3D voxel grid, and overlay
it onto the brain anatomy.
"""

# plot the spatial distribution using NiPy
vol = ds.a.mapper.reverse1(slres_tdsm.samples[0])
import nibabel as nb
anat = nb.load('datadb/tutorial_data/tutorial_data/data/anat.nii.gz')

from nipy.labs.viz_tools.activation_maps import plot_map
pl.figure(figsize=(15,4))
sp = pl.subplot(121)
pl.title('Distribution of target similarity structure correlation')
slices = plot_map(
            vol,
            ds.a.imghdr.get_best_affine(),
             cut_coords=np.array((12,-42,-20)),
             threshold=.5,
             cmap="bwr",
             vmin=0,
             vmax=1.,
             axes=sp,
             anat=anat.get_data(),
             anat_affine=anat.get_affine(),
         )
img = pl.gca().get_images()[1]
cax = pl.axes([.05, .05, .05, .9])
pl.colorbar(img, cax=cax)

sp = pl.subplot(122)
pl.hist(slres_tdsm.samples[0],
        #range=(0,410),
        normed=False,
        bins=30,
        color='0.6')
pl.ylabel("Number of voxels")
pl.xlabel("Target similarity structure correlation")

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = searchlight
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Searchlight on fMRI data
========================

.. index:: Searchlight

The original idea of a spatial searchlight algorithm stems from a paper by
:ref:`Kriegeskorte et al. (2006) <KGB06>`, and has subsequently been used in a
number of studies. The most common use for a searchlight is to compute a full
cross-validation analysis in each spherical region of interest (ROI) in the
brain. This analysis yields a map of (typically) classification accuracies that
are often interpreted or post-processed similar to a GLM statistics output map
(e.g. subsequent analysis with inferential statistics). In this example we look
at how this type of analysis can be conducted in PyMVPA.

As always, we first have to import PyMVPA.
"""

from mvpa2.suite import *

"""As searchlight analyses are usually quite expensive in term of computational
resources, we are going to enable some progress output to entertain us while
we are waiting."""

# enable debug output for searchlight call
if __debug__:
    debug.active += ["SLC"]

"""The next few calls load an fMRI dataset, while assigning associated class
targets and chunks (experiment runs) to each volume in the 4D timeseries.  One
aspect is worth mentioning. When loading the fMRI data with
:func:`~mvpa2.datasets.mri.fmri_dataset()` additional feature attributes can be
added, by providing a dictionary with names and source pairs to the `add_fa`
arguments. In this case we are loading a thresholded zstat-map of a category
selectivity contrast for voxels ventral temporal cortex."""

# data path
datapath = os.path.join(pymvpa_datadbroot,
                        'tutorial_data', 'tutorial_data', 'data')
# source of class targets and chunks definitions
attr = SampleAttributes(os.path.join(datapath, 'attributes.txt'))

dataset = fmri_dataset(
                samples=os.path.join(datapath, 'bold.nii.gz'),
                targets=attr.targets,
                chunks=attr.chunks,
                mask=os.path.join(datapath, 'mask_brain.nii.gz'),
                add_fa={'vt_thr_glm': os.path.join(datapath, 'mask_vt.nii.gz')})

"""The dataset is now loaded and contains all brain voxels as features, and all
volumes as samples. To precondition this data for the intended analysis we have
to perform a few preprocessing steps (please note that the data was already
motion-corrected). The first step is a chunk-wise (run-wise) removal of linear
trends, typically caused by the acquisition equipment."""

poly_detrend(dataset, polyord=1, chunks_attr='chunks')

"""Now that the detrending is done, we can remove parts of the timeseries we
are not interested in. For this example we are only considering volumes acquired
during a stimulation block with images of houses and scrambled pictures, as well
as rest periods (for now). It is important to perform the detrending before
this selection, as otherwise the equal spacing of fMRI volumes is no longer
guaranteed."""

dataset = dataset[np.array([l in ['rest', 'house', 'scrambledpix']
                           for l in dataset.targets], dtype='bool')]

"""The final preprocessing step is data-normalization. This is a required step
for many classification algorithms. It scales all features (voxels)
into approximately the same range and removes the mean. In this example, we
perform a chunk-wise normalization and compute standard deviation and mean for
z-scoring based on the volumes corresponding to rest periods in the experiment.
The resulting features could be interpreted as being voxel salience relative
to 'rest'."""

zscore(dataset, chunks_attr='chunks', param_est=('targets', ['rest']), dtype='float32')

"""After normalization is completed, we no longer need the 'rest'-samples and
remove them."""

dataset = dataset[dataset.sa.targets != 'rest']

"""But now for the interesting part: Next we define the measure that shall be
computed for each sphere. Theoretically, this can be anything, but here we
choose to compute a full leave-one-out cross-validation using a linear Nu-SVM
classifier."""

# choose classifier
clf = LinearNuSVMC()

# setup measure to be computed by Searchlight
# cross-validated mean transfer using an N-fold dataset splitter
cv = CrossValidation(clf, NFoldPartitioner())

"""In this example, we do not want to compute full-brain accuracy maps, but
instead limit ourselves to a specific subset of voxels. We'll select all voxel
that have a non-zero z-stats value in the localizer mask we loaded above, as
center coordinates for a searchlight sphere. These spheres will still include
voxels that did not pass the threshold. the localizer merely define the
location of all to be processed spheres."""

# get ids of features that have a nonzero value
center_ids = dataset.fa.vt_thr_glm.nonzero()[0]

"""Finally, we can run the searchlight. We'll perform the analysis for three
different radii, each time computing an error for each sphere. To achieve this,
we simply use the :func:`~mvpa2.measures.searchlight.sphere_searchlight` class,
which takes any :term:`processing object` and a radius as arguments. The
:term:`processing object` has to compute the intended measure, when called with
a dataset. The :func:`~mvpa2.measures.searchlight.sphere_searchlight` object
will do nothing more than generate small datasets for each sphere, feeding them
to the processing object, and storing the result."""

# setup plotting parameters (not essential for the analysis itself)
plot_args = {
    'background' : os.path.join(datapath, 'anat.nii.gz'),
    'background_mask' : os.path.join(datapath, 'mask_brain.nii.gz'),
    'overlay_mask' : os.path.join(datapath, 'mask_vt.nii.gz'),
    'do_stretch_colors' : False,
    'cmap_bg' : 'gray',
    'cmap_overlay' : 'autumn', # YlOrRd_r # pl.cm.autumn
    'interactive' : cfg.getboolean('examples', 'interactive', True),
    }

for radius in [0, 1, 3]:
    # tell which one we are doing
    print "Running searchlight with radius: %i ..." % (radius)

    """
    Here we actually setup the spherical searchlight by configuring the
    radius, and our selection of sphere center coordinates. Moreover, via the
    `space` argument we can instruct the searchlight which feature attribute
    shall be used to determine the voxel neighborhood. By default,
    :func:`~mvpa2.datasets.mri.fmri_dataset()` creates a corresponding attribute
    called `voxel_indices`.  Using the `mapper` argument it is possible to
    post-process the results computed for each sphere. Cross-validation will
    compute an error value per each fold, but here we are only interested in
    the mean error across all folds. Finally, on multi-core machines `nproc`
    can be used to enabled parallelization by setting it to the number of
    processes utilized by the searchlight (default value of `nproc`=`None` utilizes
    all available local cores).
    """

    sl = sphere_searchlight(cv, radius=radius, space='voxel_indices',
                            center_ids=center_ids,
                            postproc=mean_sample())

    """
    Since we care about efficiency, we are stripping all attributes from the
    dataset that are not required for the searchlight analysis. This will offers
    some speedup, since it reduces the time that is spent on dataset slicing.
    """

    ds = dataset.copy(deep=False,
                      sa=['targets', 'chunks'],
                      fa=['voxel_indices'],
                      a=['mapper'])

    """
    Finally, we actually run the analysis. The result is returned as a
    dataset. For the upcoming plots, we are transforming the returned error
    maps into accuracies.
    """

    sl_map = sl(ds)
    sl_map.samples *= -1
    sl_map.samples += 1

    """
    The result dataset is fully aware of the original dataspace. Using this
    information we can map the 1D accuracy maps back into "brain-space" (using
    NIfTI image header information from the original input timeseries.
    """

    niftiresults = map2nifti(sl_map, imghdr=dataset.a.imghdr)

    """
    PyMVPA comes with a convenient plotting function to visualize the
    searchlight amps. We are only looking at fMRI slices that are covered
    by the mask of ventral temproal cortex.
    """

    fig = pl.figure(figsize=(12, 4), facecolor='white')
    subfig = plot_lightbox(overlay=niftiresults,
                           vlim=(0.5, None), slices=range(23,31),
                           fig=fig, **plot_args)
    pl.title('Accuracy distribution for radius %i' % radius)


if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""The following figures show the resulting accuracy maps for the slices
covered by the ventral temporal cortex mask. Note that each voxel value
represents the accuracy of a sphere centered around this voxel.

.. figure:: ../pics/ex_searchlight_vt_r0.*
   :align: center

   Searchlight (single element; univariate) accuracy maps for binary
   classification *house* vs. *scrambledpix*.

.. figure:: ../pics/ex_searchlight_vt_r1.*
   :align: center

   Searchlight (sphere of neighboring voxels; 9 elements) accuracy maps for
   binary classification *house* vs.  *scrambledpix*.

.. figure:: ../pics/ex_searchlight_vt_r3.*
   :align: center

   Searchlight (radius 3 elements; 123 voxels) accuracy maps for binary
   classification *house* vs.  *scrambledpix*.

With radius 0 (only the center voxel is part of the part the sphere) there is a
clear distinction between two distributions. The *chance distribution*,
relatively symetric and centered around the expected chance-performance at 50%.
The second distribution, presumambly of voxels with univariate signal, is nicely
segregated from that. Increasing the searchlight size significantly blurrs the
accuracy map, but also lead to an increase in classification accuracy.
"""

########NEW FILE########
__FILENAME__ = searchlight_app
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Parameterizable Python Scripting: Searchlight Example.
======================================================

Example demonstrating composition of analysis script with optional
command line parameters and arguments to make the computation easily
parameterizable.  That would allow you to process multiple datasets
and vary classifiers and/or parameters of the algorithm within some
batch system scheduler.  Searchlight analysis on an fMRI dataset is
taken for the example of actual computation to be done.  Run
`searchlight.py --help` to see the list of available command line
options.
"""

from mvpa2.suite import *


def main():
    """ Wrapped into a function call for easy profiling later on
    """

    parser.usage = """\
    %s [options] <NIfTI samples> <targets+blocks> <NIfTI mask> [<output>]

    where targets+blocks is a text file that lists the class label and the
    associated block of each data sample/volume as a tuple of two integer
    values (separated by a single space). -- one tuple per line.""" \
    % sys.argv[0]

    parser.option_groups = [opts.SVM, opts.KNN, opts.general, opts.common]

    # Set a set of available classifiers for this example
    opt.clf.choices=['knn', 'lin_nu_svmc', 'rbf_nu_svmc']
    opt.clf.default='lin_nu_svmc'

    parser.add_options([opt.clf, opt.zscore])

    (options, files) = parser.parse_args()

    if not len(files) in [3, 4]:
        parser.error("Please provide 3 or 4 files in the command line")
        sys.exit(1)

    verbose(1, "Loading data")

    # data filename
    dfile = files[0]
    # text file with targets and block definitions (chunks)
    cfile = files[1]
    # mask volume filename
    mfile = files[2]

    ofile = None
    if len(files)>=4:
        # outfile name
        ofile = files[3]

    # read conditions into an array (assumed to be two columns of integers)
    # TODO: We need some generic helper to read conditions stored in some
    #       common formats
    verbose(2, "Reading conditions from file %s" % cfile)
    attrs = SampleAttributes(cfile, literallabels=True)

    verbose(2, "Loading volume file %s" % dfile)
    data = fmri_dataset(dfile,
                         targets=attrs.targets,
                         chunks=attrs.chunks,
                         mask=mfile)

    # do not try to classify baseline condition
    # XXX this is only valid for our haxby8 example dataset and should
    # probably be turned into a proper --baselinelabel option that can
    # be used for zscoring as well.
    data = data[data.targets != 'rest']

    if options.zscore:
        verbose(1, "Zscoring data samples")
        zscore(data, chunks_attr='chunks', dtype='float32')

    if options.clf == 'knn':
        clf = kNN(k=options.knearestdegree)
    elif options.clf == 'lin_nu_svmc':
        clf = LinearNuSVMC(nu=options.svm_nu)
    elif options.clf == 'rbf_nu_svmc':
        clf = RbfNuSVMC(nu=options.svm_nu)
    else:
        raise ValueError, 'Unknown classifier type: %s' % `options.clf`
    verbose(3, "Using '%s' classifier" % options.clf)

    verbose(1, "Computing")

    verbose(3, "Assigning a measure to be CrossValidation")
    # compute N-1 cross-validation with the selected classifier in each sphere
    cv = CrossValidation(clf, NFoldPartitioner(cvtype=options.crossfolddegree))

    verbose(3, "Generating Searchlight instance")
    # contruct searchlight with 5mm radius
    # this assumes that the spatial pixdim values in the source NIfTI file
    # are specified in mm
    sl = sphere_searchlight(cv, radius=options.radius)

    # run searchlight
    verbose(3, "Running searchlight on loaded data")
    results = sl(data)

    if not ofile is None:
        # map the result vector back into a nifti image
        rimg = map2nifti(data, results)

        # save to file
        rimg.save(ofile)
    else:
        print results

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = searchlight_minimal
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Minimal Searchlight Example
===========================

.. index:: searchlight, cross-validation

The term :class:`~mvpa2.measures.searchlight.Searchlight` refers to an
algorithm that runs a scalar :class:`~mvpa2.measures.base.Measure` on
all possible spheres of a certain size within a dataset (that provides
information about distances between feature locations).  The measure
typically computed is a cross-validation of a classifier performance
(see :ref:`CrossValidation <sec_tutorial_crossvalidation>` section in
the tutorial). The idea to use a searchlight as a sensitivity analyzer
on fMRI datasets stems from :ref:`Kriegeskorte et al. (2006) <KGB06>`.

A searchlight analysis is can be easily performed. This examples shows a minimal
draft of a complete analysis.

First import a necessary pieces of PyMVPA -- this time each bit individually.
"""

import numpy as np

from mvpa2.generators.partition import OddEvenPartitioner
from mvpa2.clfs.svm import LinearCSVMC
from mvpa2.measures.base import CrossValidation
from mvpa2.measures.searchlight import sphere_searchlight
from mvpa2.testing.datasets import datasets
from mvpa2.mappers.fx import mean_sample

"""For the sake of simplicity, let's use a small artificial dataset."""

# Lets just use our tiny 4D dataset from testing battery
dataset = datasets['3dlarge']

"""Now it only takes three lines for a searchlight analysis."""

# setup measure to be computed in each sphere (cross-validated
# generalization error on odd/even splits)
cv = CrossValidation(LinearCSVMC(), OddEvenPartitioner())

# setup searchlight with 2 voxels radius and measure configured above
sl = sphere_searchlight(cv, radius=2, space='myspace',
                        postproc=mean_sample())

# run searchlight on dataset
sl_map = sl(dataset)

print 'Best performing sphere error:', np.min(sl_map.samples)

"""
If this analysis is done on a fMRI dataset using `NiftiDataset` the resulting
searchlight map (`sl_map`) can be mapped back into the original dataspace
and viewed as a brain overlay. :ref:`Another example <example_searchlight>`
shows a typical application of this algorithm.

.. Mention the fact that it also is a special `SensitivityAnalyzer`
"""

########NEW FILE########
__FILENAME__ = searchlight_surf
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Surface-based searchlight on fMRI data
======================================

.. index:: surface, searchlight, cross-validation

This example employs a surface-based searchlight as described in
:ref:`Oosterhof et al. (2011) <OWD+11>` (with a minor difference that distances
are currently computed using a Dijkstra distance metric rather than a geodesic
one). For more details, see the `Surfing <http://surfing.sourceforge.net>`_
website.

Surfaces used in this example are available in the tutorial dataset files;
either the tutorial_data_surf_minimal or tutorial_data_surf_complete version.
The surfaces were reconstructed using FreeSurfer and
subsequently preprocessed with AFNI and SUMA using the
pymvpa2-prep-afni-surf wrapper script in PyMVPA's 'bin' directory, which
resamples the surfaces to standard topologies (with different resolutions)
using MapIcosehedron, aligns surfaces to a reference functional volume, and
merges left and right hemispheres into single surface files. A more detailed
description of the steps that this script takes is provided in the
documentation on the `Surfing <http://surfing.sourceforge.net>`_
website.

If you use the surface-based searchlight code for a publication, please cite
both :ref:`PyMVPA (2009) <HHS+09a>` and :ref:`Oosterhof et al. (2011) <OWD+11>`.


As always, we first have to import PyMVPA.
"""

from mvpa2.suite import *
from mvpa2.clfs.svm import LinearCSVMC
from mvpa2.base.hdf5 import h5save, h5load
"""As searchlight analyses are usually quite expensive in term of computational
resources, we are going to enable some progress output to entertain us while
we are waiting."""

if __debug__:
    from mvpa2.base import debug
    debug.active += ["SVS", "SLC"]

"""Define surface and volume data paths:"""

rootpath = os.path.join(pymvpa_datadbroot,
                        'tutorial_data', 'tutorial_data')

datapath = os.path.join(rootpath, 'data')
surfpath = os.path.join(rootpath, 'suma_surfaces')

"""Define functional data volume filename:"""

epi_fn = os.path.join(datapath, 'bold.nii.gz')

"""
In this example we are concerned with the left hemisphere only.
(Other possible values are 'r' for the right hemisphere and 'm' for merged
hemispheres; the latter contains the nodes from the left and right
hemispheres in a single file. Both the 'r' and 'm' options require
the tutorial_data_surf_complete tutorial data.)
"""

hemi = 'l'

"""
Define the surfaces that enclose the grey matter, which are used to
delineate the grey matter. The pial surface is the 'outside' border of the
grey matter; the white surface is the 'inside' border.

The surfaces in this example were resampled using AFNI's MapIcosahedron
(for more details, see the top of this script). ld refers to the number of
linear divisions of the twenty 'large' triangles of the original
icosahedron; ld=x means there are 10*x**2+2 nodes (a.k.a. vertices)
and 20*x**2 triangles (a.k.a. faces).
"""

highres_ld = 128 # 64 or 128 is reasonable

pial_surf_fn = os.path.join(surfpath, "ico%d_%sh.pial_al.asc"
                                     % (highres_ld, hemi))
white_surf_fn = os.path.join(surfpath, "ico%d_%sh.smoothwm_al.asc"
                                      % (highres_ld, hemi))

"""
Define the surface on which the nodes are centers of the searchlight. This
surface should be an 'intermediate' surface, which is formed by the
node-wise average spatial coordinates of the inner (white) and outer (pial)
surfaces.

In this example a surface coarser (fewer nodes) than the grey matter-enclosing
surfaces is employed. This reduces the number of searchlights and therefore
the script's execution time. Of course one could also use a surface that has
the same number of nodes as the grey-matter enclosing surfaces; this is
actually the default and used when souce_surf_fn (assigned below) is set
to None.

It is required that highres_ld is an integer multiple of lowres_ld, so that
all nodes in the low-res surface have a corresponding node (i.e., with the
same, or almost the same, spatial coordinate) on the high-res surface.

Choice of lowres_ld and highres_ld is somewhat arbitrary and a trade-off
between spatial specificity and execution speed. For highres_ld a value of at
least 64 is be advisable as this ensures enough anatomical detail is available
to select voxels in the grey matter accurately. Typical values for lowres_ld
range from 8 to 64.

Note that the data in tutorial_data_surf_minimal only contains
all necessary surfaces for visualization for lowres_ld=16. For other values
of lowres_ld (4, 8, 32, 64 and 128) the surfaces in
tutorial_data_surf_complete are required.

"""

lowres_ld = 16 # 16, 32 or 64 is reasonable. 4 and 8 are really fast

source_surf_fn = os.path.join(surfpath, "ico%d_%sh.intermediate_al.asc"
                                             % (lowres_ld, hemi))

"""
Radius is specified as either an int (referring to a fixed number of voxels
across searchlights, with a variable radius in millimeters (or whatever unit
is used in the files that define the surfaces), or a float (referring to the
radius in millimeters, with a variable number of voxels).

Note that "a fixed number of voxels" in this context actually means an
approximation, in that on average that number of voxels is selected but the
actual number will vary slightly (typically in the range +/- 2 voxels)
"""

radius = 100


"""We're all set to go to create a query engine that performs 'voxel
selection', that is determines, for each node, which voxels are near it
(that is, in the corresponding searchlight disc).

As a reminder, the only essential values we have set so far are the
filenames of three surfaces (high-res inner and outer,
and low-res source surface), the functional volume, and the searchlight
radius.

Note that if the functional data was preprocessed and subsequently masked,
voxel selection should take into account this mask. To do so, the
instantiation of the query engine below takes an optional argument
'volume_mask' (which can be a PyMVPA dataset, a numpy array, a Nifti
volume, or a string representing the file name of a Nifti volume). It is,
however, recommended to *not* mask the functional data prior to voxel
selection, because the voxel selection uses (implicitly) a mask based on the
grey-matter enclosing surfaces already, and this mask is assumed to be more
precise than typical volume-based masking implementations.

Also note that, as described above, the argument defining the low-res source
surface can be omitted, in which case it is computed as the node-wise
average of the white and pial surface.)
"""

qe = disc_surface_queryengine(radius, epi_fn,
                              white_surf_fn, pial_surf_fn,
                              source_surf_fn)

"""
Voxel selection is now completed; each node has been assigned a list of
linear voxel indices in the searchlight. These result are stored in
'qe.voxsel' and can be saved with h5save for later re-use.

(Linear voxel indices mean that each voxel is indexed by a value between
0 (inclusive) and N (exclusive), where N is the number of voxels in the
volume (N = NX * NY * NZ, where NX, NY and NZ are the number of voxels in
the three spatial dimensions). For certain analyses one may want to index
voxels by 'sub indices' (triples (i,j,k) with 0<=i<NX, 0<=j<=NY,
and 0<=k<NZ) or spatial coordinates; conversions amongst
linear and sub indices and spatial coordinates is provided by
functions in the  VolGeom (volume geometry) instance stored in
'qe.voxsel.volgeom'.)

From now on we follow the example as in doc/examples/searchlight.py.

First, cross-validation is defined using a (SVM) classifier.
"""

clf = LinearCSVMC()

cv = CrossValidation(clf, NFoldPartitioner(),
                     errorfx=lambda p, t: np.mean(p == t),
                     enable_ca=['stats'])

"""
Set the roi_ids, that is the node indices that serve as searchlight
center. In this example it is set to None, meaning that all nodes are used
as a searchlight center. It is also possible to restrict the nodes that serve
as a searchlight center: setting roi_ids=np.arange(400,800) means that only
nodes in the range from 400 (inclusive) to 800 (exclusive) are used as a
searchlight center, and the result would be a partial brain map.
"""

roi_ids = None

"""
Combining the query-engine and the cross-validation defines the
searchlight. The postproc-step averages the classification accuracies
in each cross-validation fold to a single overall classification accuracy.

Because roi_ids is None is this example it could be omitted - it is only
included for instructive purposes.
"""

sl = Searchlight(cv, queryengine=qe, postproc=mean_sample(), roi_ids=roi_ids)



'''
In the next step the functional data is loaded. We can reduce
memory requirements significantly by considering which voxels to load:
since the searchlight analysis will only use data from voxels that
were selected (at least once) by the voxel selection step, a mask is
derived from the voxel selection results and used when loading the
functional data.
'''

mask = qe.voxsel.get_mask()

"""
Load the functional data. Note that we use the
mask that came from the voxel selection.
"""

attr = SampleAttributes(os.path.join(datapath, 'attributes.txt'))

dataset = fmri_dataset(
                samples=epi_fn,
                targets=attr.targets,
                chunks=attr.chunks,
                mask=mask)


"""
Apply some typical preprocessing steps
"""

poly_detrend(dataset, polyord=1, chunks_attr='chunks')

dataset = dataset[np.array([l in ['rest', 'house', 'scrambledpix']
                           for l in dataset.targets], dtype='bool')]

zscore(dataset, chunks_attr='chunks', param_est=('targets', ['rest']),
        dtype='float32')

dataset = dataset[dataset.sa.targets != 'rest']

"""
Run the searchlight on the dataset.
"""

sl_dset = sl(dataset)

"""
Searchlight results are now stored in sl_dset. As sl_dset is just like
any other PyMVPA dataset, it can be stored with h5save for future use.

The remainder of this example provides a data file that
can be visualized using AFNI's SUMA. This is achieved by storing the dataset
as an NIML (NeuroImaging Markup Language) dataset that can be viewed by
AFNI's SUMA. sl_dset contains a feature attribute 'center_ids' that is
automagically used to define the node indices of the searchlight centers in
this NIML dataset.

Note that this conversion will not preserve all information in sl_dset but
only the samples and (feature, sample, dataset) attributes that behave
like arrays or strings or scalars. For example, in this example sl_dset has a
dataset attribute 'mapper' which is not stored in the NIML dataset (and
a warning message is printed during the conversion, which can be ignored
savely). As mentioned above, using h5save will preserve this information
(but its output cannot be viewed in SUMA).

Before saving the dataset, first the labels are set for each sample (in
this case, only one) so that they show up in SUMA.
"""

sl_dset.sa['labels'] = ['HOUSvsSCRM']

"""
Set the filename for output.
Searchlight results are stored in the surface directory for easy
visualization. Finally print an informative message on how the
generated data can be visualized using SUMA.
"""

fn = 'ico%d-%d_%sh_%dvx.niml.dset' % (lowres_ld, highres_ld, hemi, radius)
path_fn = os.path.join(surfpath, fn)

niml.write(path_fn, sl_dset)

print ("To view results in SUMA, cd to '%s', run 'suma -spec "
      "%sh_ico%d_al.spec', press ctrl+s, "
       "click on 'Load Dset', and select %s" %
       (surfpath, hemi, lowres_ld, fn))

########NEW FILE########
__FILENAME__ = sensanas
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Sensitivity Measure
===================

.. index:: sensitivity

Run some basic and meta sensitivity measures on the example fMRI dataset that
comes with PyMVPA and plot the computed featurewise measures for each.  The
generated figure shows sensitivity maps computed by six sensitivity analyzers.

We start by loading PyMVPA and the example fMRI dataset.
"""

from mvpa2.suite import *

# load PyMVPA example dataset
attr = SampleAttributes(os.path.join(pymvpa_dataroot, 'attributes_literal.txt'))
dataset = fmri_dataset(samples=os.path.join(pymvpa_dataroot, 'bold.nii.gz'),
                       targets=attr.targets,
                       chunks=attr.chunks,
                       mask=os.path.join(pymvpa_dataroot, 'mask.nii.gz'))

"""As with classifiers it is easy to define a bunch of sensitivity
analyzers. It is usually possible to simply call `get_sensitivity_analyzer()`
on any classifier to get an instance of an appropriate sensitivity analyzer
that uses this particular classifier to compute and extract sensitivity scores.
"""

# define sensitivity analyzer
sensanas = {
    'a) ANOVA': OneWayAnova(postproc=absolute_features()),
    'b) Linear SVM weights': LinearNuSVMC().get_sensitivity_analyzer(
                                               postproc=absolute_features()),
    'c) I-RELIEF': IterativeRelief(postproc=absolute_features()),
    'd) Splitting ANOVA (odd-even)':
        RepeatedMeasure(
            OneWayAnova(postproc=absolute_features()),
            OddEvenPartitioner()),
    'e) Splitting SVM (odd-even)':
        RepeatedMeasure(
            LinearNuSVMC().get_sensitivity_analyzer(postproc=absolute_features()),
            OddEvenPartitioner()),
    'f) I-RELIEF Online':
        IterativeReliefOnline(postproc=absolute_features()),
    'g) Splitting ANOVA (nfold)':
        RepeatedMeasure(
            OneWayAnova(postproc=absolute_features()),
            NFoldPartitioner()),
    'h) Splitting SVM (nfold)':
        RepeatedMeasure(
            LinearNuSVMC().get_sensitivity_analyzer(postproc=absolute_features()),
            NFoldPartitioner()),
    'i) GNB Searchlight':
        sphere_gnbsearchlight(GNB(), NFoldPartitioner(cvtype=1),
                              radius=0, errorfx=mean_match_accuracy)
           }

"""Now, we are performing some a more or less standard preprocessing steps:
detrending, selecting a subset of the experimental conditions, normalization
of each feature to a standard mean and variance."""

# do chunkswise linear detrending on dataset
poly_detrend(dataset, polyord=1, chunks_attr='chunks')

# only use 'rest', 'shoe' and 'bottle' samples from dataset
dataset = dataset[np.array([l in ['rest', 'shoe', 'bottle']
                    for l in dataset.sa.targets], dtype='bool')]

# zscore dataset relative to baseline ('rest') mean
zscore(dataset, chunks_attr='chunks',
       param_est=('targets', ['rest']), dtype='float32')

# remove baseline samples from dataset for final analysis
dataset = dataset[dataset.sa.targets != 'rest']

"""Finally, we will loop over all defined analyzers and let them compute
the sensitivity scores. The resulting vectors are then mapped back into the
dataspace of the original fMRI samples, which are then plotted."""

fig = 0
pl.figure(figsize=(14, 8))

keys = sensanas.keys()
keys.sort()

for s in keys:
    # tell which one we are doing
    print "Running %s ..." % (s)

    sana = sensanas[s]
    # compute sensitivies
    sens = sana(dataset)
    # I-RELIEF assigns zeros, which corrupts voxel masking for pylab's
    # imshow, so adding some epsilon :)
    smap = sens.samples[0] + 0.00001

    # map sensitivity map into original dataspace
    orig_smap = dataset.mapper.reverse1(smap)
    masked_orig_smap = np.ma.masked_array(orig_smap, mask=orig_smap == 0)

    # make a new subplot for each classifier
    fig += 1
    pl.subplot(3, 3, fig)

    pl.title(s)

    pl.imshow(masked_orig_smap[..., 0].T,
             interpolation='nearest',
             aspect=1.25,
             cmap=pl.cm.autumn)

    # uniform scaling per base sensitivity analyzer
    ## if s.count('ANOVA'):
    ##     pl.clim(0, 30)
    ## elif s.count('SVM'):
    ##     pl.clim(0, 0.055)
    ## else:
    ##     pass

    pl.colorbar(shrink=0.6)

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""
Output of the example analysis:

.. image:: ../pics/ex_sensanas.*
   :align: center
   :alt: Various sensitivity analysis results

"""

########NEW FILE########
__FILENAME__ = skl_classifier_demo
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
"""
============================================
 Using scikit-learn classifiers with PyMVPA
============================================

Scikit-learn is a rich library of algorithms, many of them implementing the
`estimator and predictor API`_. PyMVPA provides the wrapper class,
:class:`~mvpa2.clfs.skl.base.SKLLearnerAdapter` that enables the use
of all of these algorithms within the PyMVPA framework. With this adaptor
these aspects of the scikit-learn API are presented through a PyMVPA
learner interface that is fully compatible with all other building blocks of
PyMVPA.

In this example we demonstrate this interface by mimicking the "`Nearest 
Neighbors Classification`_" example from the scikit-learn documentation --
applying the minimal modifications necessary to run two variants of the 
scikit-learn k-nearest neighbors algorithm implementation on PyMVPA datasets.

.. _estimator and predictor API: http://scikit-learn.org/stable/developers/#apis-of-scikit-learn-objects
.. _Nearest Neighbors Classification: http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html
"""

print(__doc__)

import numpy as np
import pylab as pl
from matplotlib.colors import ListedColormap
from sklearn import neighbors

n_neighbors = 15

"""
So far the code has been identical. The first difference is the import of the
adaptor class. We also load the scikit-learn demo dataset, but also with the
help of a wrapper function that yields a PyMVPA dataset.
"""


# this first import is only required to run the example a part of the test suite
from mvpa2 import cfg
from mvpa2.clfs.skl.base import SKLLearnerAdapter

# load the iris dataset
from mvpa2.datasets.sources.skl_data import skl_iris
iris = skl_iris()
# compact dataset summary
print iris

"""
The original example uses only the first two features of the dataset, 
since it intends to visualize learned classification boundaries in 2-D.
We can do the same slicing directly on our PyMVPA dataset.
"""

iris=iris[:,[0,1]]

d = {'setosa':0, 'versicolor':1, 'virginica':2}

"""
For visualization we will later map the literal class labels onto
numerival values. Besides that, we continue with practically identical code.
"""

h = .02  # step size in the mesh

cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

for weights in ['uniform', 'distance']:
    # create an instance of the algorithm from scikit-learn,
    # wrap it by SKLLearnerAdapter and finally train it
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    wrapped_clf=SKLLearnerAdapter(clf)
    wrapped_clf.train(iris)

    """
    The following lines are an example of the only significant modification
    with respect to a pure scikit-learn implementation: the classifier is
    wrapped into the adaptor.  The result is a PyMVPA classifier, hence can 
    be called with a dataset that contains both samples and targets.
    """

    # shortcut
    X = iris.samples
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = wrapped_clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # to put the result into a color plot we now need numerical values
    # this can be done nicely in PyMVPA
    from mvpa2.datasets.formats import AttributeMap
    Z = AttributeMap().to_numeric(Z)
    Z = Z.reshape(xx.shape)

    pl.figure()
    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # For plotting the training points we convert to numerical values again
    pl.scatter(X[:, 0], X[:, 1], c=AttributeMap().to_numeric(iris.targets), 
                                 cmap=cmap_bold)
    pl.xlim(xx.min(), xx.max())
    pl.ylim(yy.min(), yy.max())
    pl.title("3-Class classification (k = %i, weights = '%s')"
             % (n_neighbors, weights))

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""
This example shows that a PyMVPA classifier can be used in pretty much the
same way as the corresponding scikit-learn API. What this example does not show
is that with the :class:`~mvpa2.clfs.skl.base.SKLLearnerAdapter` class any
scikit-learn classifier can be employed in arbitrarily complex PyMVPA
processing pipelines and is enhanced with automatic training and all other
functionality of PyMVPA classifier implementations.
"""

########NEW FILE########
__FILENAME__ = skl_regression_demo
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
"""
============================================
 Using scikit-learn regressions with PyMVPA
============================================

This is practically part two of the example on `using scikit-learn classifiers
with PyMVPA <example_skl_classifier_demo>`. Just like classifiers,
implementations of regression algorithms in scikit-learn use the
`estimator and predictor API`_. Consequently, the same wrapper class
(:class:`~mvpa2.clfs.skl.base.SKLLearnerAdapter`) as before is applicable
when using scikit-learn regressions in PyMVPA.

The example demonstrates this by mimicking the "`Decision Tree Regression`_"
example from the scikit-learn documentation -- applying the minimal
modifications necessary to the scikit-learn decision tree regression algorithm
(with two different parameter settings) implementation on a PyMVPA dataset.

.. _estimator and predictor API: http://scikit-learn.org/stable/developers/#apis-of-scikit-learn-objects
.. _Decision Tree Regression: http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html 
"""

print(__doc__)

import numpy as np
from sklearn.tree import DecisionTreeRegressor

rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))

"""
So far the code has been identical. The first difference is the import of the
adaptor class. We also use a convenient way to convert the data into a proper
:class:`~mvpa2.datasets.base.Dataset`.
"""

# this first import is only required to run the example a part of the test suite
from mvpa2 import cfg
from mvpa2.clfs.skl.base import SKLLearnerAdapter
from mvpa2.datasets import dataset_wizard
ds_train=dataset_wizard(samples=X, targets=y)


"""
The following lines are an example of the only significant modification
with respect to a pure scikit-learn implementation: the regression is
wrapped into the adaptor. The result is a PyMVPA learner, hence can 
be called with a dataset that contains both samples and targets.
"""

clf_1 = SKLLearnerAdapter(DecisionTreeRegressor(max_depth=2))
clf_2 = SKLLearnerAdapter(DecisionTreeRegressor(max_depth=5))

clf_1.train(ds_train)
clf_2.train(ds_train)

X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_1 = clf_1.predict(X_test)
y_2 = clf_2.predict(X_test)


# plot the results
# which clearly show the overfitting for the second depth setting  
import pylab as pl

pl.figure()
pl.scatter(X, y, c="k", label="data")
pl.plot(X_test, y_1, c="g", label="max_depth=2", linewidth=2)
pl.plot(X_test, y_2, c="r", label="max_depth=5", linewidth=2)
pl.xlabel("data")
pl.ylabel("target")
pl.title("Decision Tree Regression")
pl.legend()

if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = skl_transformer_demo
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
"""
=============================================
 Using scikit-learn transformers with PyMVPA
=============================================

Scikit-learn is a rich library of algorithms, many of them implementing the
`transformer API`_. PyMVPA provides a wrapper class,
:class:`~mvpa2.mappers.skl_adaptor.SKLTransformer` that enables the use
of all of these algorithms within the PyMVPA framework. With this adaptor
the transformer API is presented as a PyMVPA mapper interface that is fully
compatible with all other building blocks of PyMVPA.

In this example we demonstrate this interface by mimicking the "`Comparison of
Manifold Learning methods`_" example from the scikit-learn documentation --
applying the minimal modifications necessary to run a variety of scikit-learn
algorithm implementation on PyMVPA datasets.

This script also prints the same timing information as the original.

.. _transformer API: http://scikit-learn.org/stable/developers/#apis-of-scikit-learn-objects
.. _Comparison of Manifold Learning methods: http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html

"""

print(__doc__)

from time import time

import pylab as pl
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter

from sklearn import manifold
# Next line to silence pyflakes. This import is needed.
Axes3D

n_points = 1000
n_neighbors = 10
n_components = 2

"""
So far the code has been identical. The first difference is the import of the
adaptor class. We also load the scikit-learn demo dataset, but also with the
help of a wrapper function that yields a PyMVPA dataset.
"""

# this first import is only required to run the example a part of the test suite
from mvpa2 import cfg
from mvpa2.mappers.skl_adaptor import SKLTransformer

# load the S-curve dataset 
from mvpa2.datasets.sources.skl_data import skl_s_curve
ds = skl_s_curve(n_points)

"""
And we continue with practically identical code.
"""

fig = pl.figure(figsize=(15, 8))
pl.suptitle("Manifold Learning with %i points, %i neighbors"
            % (1000, n_neighbors), fontsize=14)

try:
    # compatibility matplotlib < 1.0
    X = ds.samples
    ax = fig.add_subplot(241, projection='3d')
    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=ds.targets, cmap=pl.cm.Spectral)
    ax.view_init(4, -72)
except:
    X = ds.samples
    ax = fig.add_subplot(241, projection='3d')
    pl.scatter(X[:, 0], X[:, 2], c=ds.targets, cmap=pl.cm.Spectral)

methods = ['standard', 'ltsa', 'hessian', 'modified']
labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']

for i, method in enumerate(methods):
    t0 = time()
    # create an instance of the algorithm from scikit-learn
    # and wrap it by SKLTransformer

    """
    The following lines are an example of the only significant modification
    with respect to a pure scikit-learn implementation: the transformer is
    wrapped into the adaptor.  The result is a mapper, hence can be called
    with a dataset that contains both samples and targets -- without explcitly
    calling ``fit()`` and ``transform()``.
    """

    lle = SKLTransformer(manifold.LocallyLinearEmbedding(n_neighbors, 
                                                         n_components,
                                                         eigen_solver='auto',
                                                         method=method))
    # call the SKLTransformer instance on the input dataset
    Y = lle(ds)

    """
    The rest of the example is unmodified except for the wrapping of the
    respective transformer into the Mapper adaptor.
    """

    t1 = time()
    print("%s: %.2g sec" % (methods[i], t1 - t0))

    ax = fig.add_subplot(242 + i)
    pl.scatter(Y[:, 0], Y[:, 1], c=ds.targets, cmap=pl.cm.Spectral)
    pl.title("%s (%.2g sec)" % (labels[i], t1 - t0))
    ax.xaxis.set_major_formatter(NullFormatter())
    ax.yaxis.set_major_formatter(NullFormatter())
    pl.axis('tight')

t0 = time()
# create an instance of the algorithm from scikit-learn
# and wrap it by SKLTransformer
iso = SKLTransformer(manifold.Isomap(n_neighbors=10, n_components=2))
# call the SKLTransformer instance on the input dataset
Y = iso(ds)
t1 = time()
print("Isomap: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(246)
pl.scatter(Y[:, 0], Y[:, 1], c=ds.targets, cmap=pl.cm.Spectral)
pl.title("Isomap (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
pl.axis('tight')


t0 = time()
# create an instance of the algorithm from scikit-learn
# and wrap it by SKLTransformer
mds = SKLTransformer(manifold.MDS(n_components=2, max_iter=100, 
                                  n_init=1, dissimilarity='euclidean')) 
# call the SKLTransformer instance on the input dataset
Y = mds(ds)
t1 = time()
print("MDS: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(247)
pl.scatter(Y[:, 0], Y[:, 1], c=ds.targets, cmap=pl.cm.Spectral)
pl.title("MDS (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
pl.axis('tight')


t0 = time()
# create an instance of the algorithm from scikit-learn
# and wrap it by SKLTransformer
se = SKLTransformer(manifold.SpectralEmbedding(n_components=n_components,
                                               n_neighbors=n_neighbors))
# call the SKLTransformer instance on the input dataset
Y = se(ds)
t1 = time()
print("SpectralEmbedding: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(248)
pl.scatter(Y[:, 0], Y[:, 1], c=ds.targets, cmap=pl.cm.Spectral)
pl.title("SpectralEmbedding (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
pl.axis('tight')



if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

########NEW FILE########
__FILENAME__ = smellit
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Simple Data-Exploration
=======================

Example showing some possibilities of data exploration
(i.e. to 'smell' data).
"""

from mvpa2.suite import *

# load example fmri dataset
ds = load_example_fmri_dataset()

# only use the first 5 chunks to save some cpu-cycles
ds = ds[ds.chunks < 5]

"""
It is always useful to have a quick look at the summary of the dataset and
verify that statistics (mean, standard deviation) are in the expected range,
that there is balance among targets/chunks, and that order is balanced (where
appropriate).
"""

print ds.summary()

"""
Now we can take a look at the distribution of the feature values in all
sample categories and chunks.
"""

pl.figure(figsize=(14, 14)) # larger figure
hist(ds, xgroup_attr='chunks', ygroup_attr='targets', noticks=None,
     bins=20, normed=True)

# next only works with floating point data
ds.samples = ds.samples.astype('float')

# look at sample similarity
# Note, the decreasing similarity with increasing temporal distance
# of the samples
pl.figure(figsize=(14, 6))
pl.subplot(121)
plot_samples_distance(ds, sortbyattr='chunks')
pl.title('Sample distances (sorted by chunks)')

# similar distance plot, but now samples sorted by their
# respective targets, i.e. samples with same targets are plotted
# in adjacent columns/rows.
# Note, that the first and largest group corresponds to the
# 'rest' condition in the dataset
pl.subplot(122)
plot_samples_distance(ds, sortbyattr='targets')
pl.title('Sample distances (sorted by targets)')

# z-score features individually per chunk
print 'Detrending data'
poly_detrend(ds, polyord=2, chunks_attr='chunks')
print 'Z-Scoring data'
zscore(ds)

pl.figure(figsize=(14, 6))
pl.subplot(121)
plot_samples_distance(ds, sortbyattr='chunks')
pl.title('Distances: z-scored, detrended (sorted by chunks)')
pl.subplot(122)
plot_samples_distance(ds, sortbyattr='targets')
pl.title('Distances: z-scored, detrended (sorted by targets)');
if cfg.getboolean('examples', 'interactive', True):
    pl.show()

# XXX add some more, maybe show effect of preprocessing

"""
Outputs of the example script. Data prior to preprocessing

.. image:: ../pics/ex_smellit2.*
   :align: center
   :alt: Data prior preprocessing

Data after minimal preprocessing

.. image:: ../pics/ex_smellit3.*
   :align: center
   :alt: Data after z-scoring and detrending

"""

########NEW FILE########
__FILENAME__ = smlr
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Compare SMLR to Linear SVM Classifier
=====================================

.. index:: SMLR, SVM

Runs both classifiers on the the same dataset and compare their performance.
This example also shows an example usage of confusion matrices and how two
classifers can be combined.
"""

from mvpa2.suite import *

if __debug__:
    debug.active.append('SMLR_')

# features of sample data
print "Generating samples..."
nfeat = 10000
nsamp = 100
ntrain = 90
goodfeat = 10
offset = .5

# create the sample datasets
samp1 = np.random.randn(nsamp,nfeat)
samp1[:,:goodfeat] += offset

samp2 = np.random.randn(nsamp,nfeat)
samp2[:,:goodfeat] -= offset

# create the pymvpa training dataset from the labeled features
patternsPos = dataset_wizard(samples=samp1[:ntrain,:], targets=1)
patternsNeg = dataset_wizard(samples=samp2[:ntrain,:], targets=0)
trainpat = vstack((patternsPos, patternsNeg))

# create patters for the testing dataset
patternsPos = dataset_wizard(samples=samp1[ntrain:,:], targets=1)
patternsNeg = dataset_wizard(samples=samp2[ntrain:,:], targets=0)
testpat = vstack((patternsPos, patternsNeg))

# set up the SMLR classifier
print "Evaluating SMLR classifier..."
smlr = SMLR(fit_all_weights=True)

# enable saving of the estimates used for the prediction
smlr.ca.enable('estimates')

# train with the known points
smlr.train(trainpat)

# run the predictions on the test values
pre = smlr.predict(testpat.samples)

# calculate the confusion matrix
smlr_confusion = ConfusionMatrix(
    labels=trainpat.UT, targets=testpat.targets,
    predictions=pre)

# now do the same for a linear SVM
print "Evaluating Linear SVM classifier..."
lsvm = LinearNuSVMC(probability=1)

# enable saving of the estimates used for the prediction
lsvm.ca.enable('estimates')

# train with the known points
lsvm.train(trainpat)

# run the predictions on the test values
pre = lsvm.predict(testpat.samples)

# calculate the confusion matrix
lsvm_confusion = ConfusionMatrix(
    labels=trainpat.UT, targets=testpat.targets,
    predictions=pre)

# now train SVM with selected features
print "Evaluating Linear SVM classifier with SMLR's features..."

keepInd = (np.abs(smlr.weights).mean(axis=1)!=0)
newtrainpat = trainpat[:, keepInd]
newtestpat = testpat[:, keepInd]

# train with the known points
lsvm.train(newtrainpat)

# run the predictions on the test values
pre = lsvm.predict(newtestpat.samples)

# calculate the confusion matrix
lsvm_confusion_sparse = ConfusionMatrix(
    labels=newtrainpat.UT, targets=newtestpat.targets,
    predictions=pre)


print "SMLR Percent Correct:\t%g%% (Retained %d/%d features)" % \
    (smlr_confusion.percent_correct,
     (smlr.weights!=0).sum(), np.prod(smlr.weights.shape))
print "linear-SVM Percent Correct:\t%g%%" % \
    (lsvm_confusion.percent_correct)
print "linear-SVM Percent Correct (with %d features from SMLR):\t%g%%" % \
    (keepInd.sum(), lsvm_confusion_sparse.percent_correct)

########NEW FILE########
__FILENAME__ = som
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Self-organizing Maps
====================

.. index:: mapper, self-organizing map, SOM, SimpleSOMMapper

This is a demonstration of how a self-organizing map (SOM), also known
as a Kohonen network, can be used to map high-dimensional data into a
two-dimensional representation. For the sake of an easy visualization
'high-dimensional' in this case is 3D.

In general, SOMs might be useful for visualizing high-dimensional data
in terms of its similarity structure. Especially large SOMs (i.e. with
large number of Kohonen units) are known to perform mappings that
preserve the topology of the original data, i.e. neighboring data
points in input space will also be represented in adjacent locations
on the SOM.

The following code shows the 'classic' color mapping example, i.e. the
SOM will map a number of colors into a rectangular area.
"""

from mvpa2.suite import *

"""
First, we define some colors as RGB values from the interval (0,1),
i.e. with white being (1, 1, 1) and black being (0, 0, 0). Please
note, that a substantial proportion of the defined colors represent
variations of 'blue', which are supposed to be represented in more
detail in the SOM.
"""

colors = np.array(
         [[0., 0., 0.],
          [0., 0., 1.],
          [0., 0., 0.5],
          [0.125, 0.529, 1.0],
          [0.33, 0.4, 0.67],
          [0.6, 0.5, 1.0],
          [0., 1., 0.],
          [1., 0., 0.],
          [0., 1., 1.],
          [1., 0., 1.],
          [1., 1., 0.],
          [1., 1., 1.],
          [.33, .33, .33],
          [.5, .5, .5],
          [.66, .66, .66]])

# store the names of the colors for visualization later on
color_names = \
        ['black', 'blue', 'darkblue', 'skyblue',
         'greyblue', 'lilac', 'green', 'red',
         'cyan', 'violet', 'yellow', 'white',
         'darkgrey', 'mediumgrey', 'lightgrey']

"""
Now we can instantiate the mapper. It will internally use a so-called
Kohonen layer to map the data onto. We tell the mapper to use a
rectangular layer with 20 x 30 units. This will be the output space of
the mapper. Additionally, we tell it to train the network using 400
iterations and to use custom learning rate.
"""

som = SimpleSOMMapper((20, 30), 400, learning_rate=0.05)

"""
Finally, we train the mapper with the previously defined 'color' dataset.
"""

som.train(colors)

"""
Each unit in the Kohonen layer can be treated as a pointer into the
high-dimensional input space, that can be queried to inspect which
input subspaces the SOM maps onto certain sections of its 2D output
space.  The color-mapping generated by this example's SOM can be shown
with a single matplotlib call:
"""

pl.imshow(som.K, origin='lower')

"""
And now, let's take a look onto which coordinates the initial training
prototypes were mapped to. The get those coordinates we can simply feed
the training data to the mapper and plot the output.
"""

mapped = som(colors)

pl.title('Color SOM')
# SOM's kshape is (rows x columns), while matplotlib wants (X x Y)
for i, m in enumerate(mapped):
    pl.text(m[1], m[0], color_names[i], ha='center', va='center',
           bbox=dict(facecolor='white', alpha=0.5, lw=0))

"""
The text labels of the original training colors will appear at the 'mapped'
locations in the SOM -- and should match with the underlying color.
"""

# show the figure
if cfg.getboolean('examples', 'interactive', True):
    pl.show()

"""
The following figure shows an exemplary solution of the SOM mapping of the
3D color-space onto the 2D SOM node layer:

.. image:: ../pics/ex_som.*
   :align: center
   :alt: Color-space mapping by a self-organizing map.

"""

########NEW FILE########
__FILENAME__ = start_easy
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
A simple start
==============

Here we show how to perform a simple cross-validated classification analysis
with PyMVPA. This script is the exact equivalent of the
:ref:`example_cmdline_start_easy` example, but using the Python API instead of
the command line interface.

First, we import the PyMVPA suite to enable all PyMVPA building blocks
"""

from mvpa2.suite import *

"""
Now we load an fMRI dataset with some attributes for each volume, only
considering voxels that are non-zero in a mask image.
"""

attr = SampleAttributes(os.path.join(pymvpa_dataroot,
                        'attributes_literal.txt'))
dataset = fmri_dataset(samples=os.path.join(pymvpa_dataroot, 'bold.nii.gz'),
                       targets=attr.targets, chunks=attr.chunks,
                       mask=os.path.join(pymvpa_dataroot, 'mask.nii.gz'))

"""
Next we remove linear trends by polynomial regression for each voxel and
each chunk (recording run) of the dataset individually.
"""

poly_detrend(dataset, polyord=1, chunks_attr='chunks')

"""
For this example we are only interested in data samples that correspond
to the ``face`` or to the ``house`` condition.
"""

dataset = dataset[np.array([l in ['face', 'house'] for l in dataset.sa.targets],
                          dtype='bool')]

"""
The setup for our cross-validation analysis include the selection of a
classifier, and a partitioning scheme, and an error function
to convert literal predictions into a quantitative performance metric.
"""

cv = CrossValidation(SMLR(), OddEvenPartitioner(), errorfx=mean_mismatch_error)
error = cv(dataset)

"""
The resulting dataset contains the computed accuracy.
"""

# UC: unique chunks, UT: unique targets
print "Error for %i-fold cross-validation on %i-class problem: %f" \
      % (len(dataset.UC), len(dataset.UT), np.mean(error))

########NEW FILE########
__FILENAME__ = svdclf
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Classification of SVD-mapped Datasets
=====================================

.. index:: mapper, SVD, MappedClassifier

Demonstrate the usage of a dataset mapper performing data projection
onto singular value components within a cross-validation -- for *any*
classifier.
"""

from mvpa2.suite import *

if __debug__:
    debug.active += ["REPM"]

#
# load PyMVPA example dataset
#
attr = SampleAttributes(os.path.join(pymvpa_dataroot,
                        'attributes_literal.txt'))
dataset = fmri_dataset(os.path.join(pymvpa_dataroot, 'bold.nii.gz'),
                       targets=attr.targets, chunks=attr.chunks,
                       mask=os.path.join(pymvpa_dataroot, 'mask.nii.gz'))

#
# preprocessing
#

# do chunkswise linear detrending on dataset
poly_detrend(dataset, polyord=1, chunks_attr='chunks')

# only use 'rest', 'cats' and 'scissors' samples from dataset
dataset = dataset[np.array([ l in ['rest', 'cat', 'scissors']
                    for l in dataset.targets], dtype='bool')]

# zscore dataset relative to baseline ('rest') mean
zscore(dataset, chunks_attr='chunks', param_est=('targets', ['rest']), dtype='float32')

# remove baseline samples from dataset for final analysis
dataset = dataset[dataset.sa.targets != 'rest']

# Specify the class of a base classifier to be used
Clf = LinearCSVMC
# And create the instance of SVDMapper to be reused
svdmapper = SVDMapper()

"""Lets create a generator of a `ChainMapper` which would first perform
SVD and then subselect the desired range of components."""

get_SVD_sliced = lambda x: ChainMapper([svdmapper,
                                        StaticFeatureSelection(x)])

"""Now we can define a list of some classifiers: a simple one and several
classifiers with built-in SVD transformation and selection of
corresponding SVD subspaces"""

clfs = [('All orig.\nfeatures (%i)' % dataset.nfeatures, Clf()),
        ('All Comps\n(%i)' % (dataset.nsamples \
                 - (dataset.nsamples / len(dataset.UC)),),
                        MappedClassifier(Clf(), svdmapper)),
        ('First 5\nComp.', MappedClassifier(Clf(),
                        get_SVD_sliced(slice(0, 5)))),
        ('First 30\nComp.', MappedClassifier(Clf(),
                        get_SVD_sliced(slice(0, 30)))),
        ('Comp.\n6-30', MappedClassifier(Clf(),
                        get_SVD_sliced(slice(5, 30))))]


# run and visualize in barplot
results = []
labels = []

for desc, clf in clfs:
    print desc.replace('\n', ' ')
    cv = CrossValidation(clf, NFoldPartitioner())
    res = cv(dataset)
    # there is only one 'feature' i.e. the error in the returned
    # dataset
    results.append(res.samples[:,0])
    labels.append(desc)

plot_bars(results, labels=labels,
         title='Linear C-SVM classification (cats vs. scissors)',
         ylabel='Mean classification error (N-1 cross-validation, 12-fold)',
         distance=0.5)

if cfg.getboolean('examples', 'interactive', True):
    pl.show()

"""
Output of the example analysis:

.. image:: ../pics/ex_svdclf.*
   :align: center
   :alt: Generalization performance on the selected PCs.

"""

########NEW FILE########
__FILENAME__ = svm_margin
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Analysis of the margin width in a soft-margin SVM
=================================================

.. index:: SVM

Width of the margin of soft-margin SVM
(:class:`mvpa2.clfs.svm.LinearCSVMC`) is not monotonic in its relation
with SNR of the data.  In case of not perfectly separable classes
margin would first shrink with the increase of SNR, and then start to
expand again after learning error becomes sufficiently small.

This brief examples provides a demonstration.

"""

import mvpa2
import pylab as pl
import numpy as np
from mvpa2.misc.data_generators import normal_feature_dataset
from mvpa2.clfs.svm import LinearCSVMC
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.measures.base import CrossValidation
from mvpa2.mappers.zscore import zscore

"""
Generate a binary dataset without any signal (snr=0).
"""

mvpa2.seed(1);
ds_noise = normal_feature_dataset(perlabel=100, nlabels=2, nfeatures=2, snr=0,
                                  nonbogus_features=[0,1])

# signal levels
sigs = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]

"""

To mimic behavior of hard-margin SVM whenever classes become
separable, which is easier to comprehend, we are intentionally setting
very high C value.

"""

clf = LinearCSVMC(C=1000, enable_ca=['training_stats'])
cve = CrossValidation(clf, NFoldPartitioner(), enable_ca='stats')
sana = clf.get_sensitivity_analyzer(postproc=None)

rs = []
errors, training_errors = [], []

for sig in sigs:
    ds = ds_noise.copy()
    # introduce signal into the first feature
    ds.samples[ds.T == 'L1', 0] += sig

    error = np.mean(cve(ds))
    sa = sana(ds)
    training_error = 1-clf.ca.training_stats.stats['ACC']

    errors.append(error)
    training_errors.append(training_error)

    w = sa.samples[0]
    b = np.asscalar(sa.sa.biases)
    # width each way
    r = 1./np.linalg.norm(w)

    msg = "SIGNAL: %.2f training_error: %.2f error: %.2f |w|: %.2f r=%.2f" \
      %(sig, training_error, error, np.linalg.norm(w), r)
    print msg

    # Drawing current data and SVM hyperplane+margin
    xmin = np.min(ds[:,0], axis=0)
    xmax = np.max(ds[:,0], axis=0)
    x = np.linspace(xmin, xmax, 20)
    y  =    -(w[0] * x - b) /w[1]
    y1 = ( 1-(w[0] * x - b))/w[1]
    y2 = (-1-(w[0] * x - b))/w[1]

    pl.figure(figsize=(10,4))

    for t,c in zip(ds.UT, ['r', 'b']):
        ds_ = ds[ds.T == t]
        pl.scatter(ds_[:, 0], ds_[:, 1], c=c)
    # draw the hyperplane
    pl.plot(x, y)
    pl.plot(x, y1, '--')
    pl.plot(x, y2, '--')
    pl.title(msg)
    ca = pl.gca()
    ca.set_xlim((-2, 4))
    ca.set_ylim((-1.2, 1.2))
    pl.show()
    rs.append(r)

"""

So what would be our dependence between signal level and errors/width
of the margin?

"""

pl.figure()
pl.plot(sigs, rs, label="Margin width of %s" % clf)
pl.plot(sigs, errors, label="CV error")
pl.plot(sigs, training_errors, label="Training error")
pl.xlabel("Signal")
pl.legend()
pl.show()

"""
And this is how it looks like.

.. image:: ../pics/ex_svm_margin.*
   :align: center
   :alt: Relation between signal level, errors and the width of the soft-margin SVM's margin

"""

########NEW FILE########
__FILENAME__ = topo_plot
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""
Generating Topography plots
===========================

Example demonstrating a topography plot."""

from mvpa2.suite import *

# Sanity check if we have griddata available
externals.exists("griddata", raise_=True)

# EEG example splot
pl.subplot(1, 2, 1)

# load the sensor information from their definition file.
# This file has sensor names, as well as their 3D coordinates
sensors=XAVRSensorLocations(os.path.join(pymvpa_dataroot, 'xavr1010.dat'))

# make up some artifical topography
# 'enable' to channels, all others set to off ;-)
topo = np.zeros(len(sensors.names))
topo[sensors.names.index('O1')] = 1
topo[sensors.names.index('F4')] = 1

# plot with sensor locations shown
plot_head_topography(topo, sensors.locations(), plotsensors=True)


# MEG example plot
pl.subplot(1, 2, 2)

# load MEG sensor locations
sensors=TuebingenMEGSensorLocations(
            os.path.join(pymvpa_dataroot, 'tueb_meg_coord.xyz'))

# random values this time
topo = np.random.randn(len(sensors.names))

# plot without additional interpolation
plot_head_topography(topo, sensors.locations(),
                   interpolation='nearest')


if cfg.getboolean('examples', 'interactive', True):
    # show all the cool figures
    pl.show()

"""
The ouput of the provided example should look like

.. image:: ../pics/ex_topo_plot.*
   :align: center
   :alt: Topography plot of MEG data

"""

########NEW FILE########
__FILENAME__ = header
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
""""""

__docformat__ = 'restructuredtext'


########NEW FILE########
__FILENAME__ = ipy_profile_pymvpa
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""PyMVPA mode for IPython.
"""

__docformat__ = 'restructuredtext'

from IPython import ipapi

# The import below effectively obsoletes your old-style ipythonrc[.ini],
# so consider yourself warned!
import ipy_defaults

import mvpa2

def main():
    ip = ipapi.get()

    # PyMVPA specific
    ip.ex('import mvpa2')

    # and now the whole suite
    # but no, since ipython segfaults (tested with version 0.8.4)
    # the whole things seems to be related to RPy and friends
    # running the same command after IPython startup is completed
    # is no problem, though.
    #ip.ex('from mvpa2.suite import *')

    print """
###########################
# Welcome to PyMVPA %s #
###########################
""" % mvpa2.__version__

main()

########NEW FILE########
__FILENAME__ = conf
# emacs: -*- coding: utf-8; mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
#
# PyMVPA documentation build configuration file, created by
# sphinx-quickstart on Tue Dec 29 10:32:00 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os, re
import numpy as np
import mvpa2

# To troubleshoot buildbot documentation build problem
print "D: PyMVPA path %s" % mvpa2.__path__
print "D: PYTHONPATHs", '\n   '.join([]+sys.path)

# We need to know sphinx version for decisions below
import sphinx
from distutils.version import LooseVersion
sphinx_version = LooseVersion(sphinx.__version__)

from mvpa2.base import externals

try:
    import matplotlib
    # Disable warning from matplotlib
    import warnings
    warnings.filterwarnings(
        'ignore', 'This call to matplotlib.use() has no effect.*',
        UserWarning)
    matplotlib.use('svg')
except:
    pass

##################################################
# Config settings are at the bottom of the file! #
##################################################

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath('..'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc',
              'sphinx.ext.doctest',
              'sphinx.ext.intersphinx',
              'sphinx.ext.todo',
              'sphinx.ext.coverage',
              'sphinx.ext.ifconfig',
              'sphinx.ext.inheritance_diagram',
              'sphinx.ext.pngmath',
              # we have a local copy of the extension, imported from NumPy 1.3
              # this also includes the docscrape* extensions
              externals.exists('numpydoc') and 'numpydoc.numpydoc' or 'sphinxext.numpydoc',
              # finally our own little thingie to display tasks
              'sphinxext.exercise_directive']

# we have a local copy of autosummary from the unreleased sphinx
# 1.0 -- reason: the 0.6 extension creates half-empty summaries
extensions += [sphinx_version < '1.1.2'
               and 'sphinxext.autosummary'
               or 'sphinx.ext.autosummary']

# the following doesn't work with sphinx < 1.0, but will make a separate
# sphinx-autogen run obsolete in the future
#autosummary_generate = True

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General substitutions.
project = 'PyMVPA'
copyright = '2006-2013, PyMVPA Authors'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = mvpa2.__version__
# The full version, including alpha/beta/rc tags.
release = mvpa2.__version__

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# what to put into API doc (just class doc, just init, or both
autoclass_content = 'both'

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None
#default_role = "autolink" # causes actual running of code and crashes
# the problem with this setting is that is also confused things
# `Dataset` might lead to a link to the h5py.Dataset` docs
default_role = "obj"	   # seems to be sufficient to provide basic hyperlinking


# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'pymvpa_offline'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_themes']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = 'pics/pymvpa_logo.jpg'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
html_sidebars = {'index': 'indexsidebar.html'}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {'index': 'index.html'}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = False

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'PyMVPAdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
latex_paper_size = 'a4'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('pdfmanual', 'PyMVPA-Manual.tex', 'PyMVPA Manual',
   'PyMVPA Authors',
   'manual'),
  ('devguide', 'PyMVPA-DevGuide.tex', 'PyMVPA Developer Guidelines',
   'PyMVPA Authors',
   'manual'),
#  ('modref', 'PyMVPA-Reference.tex', 'PyMVPA Reference',
#   'PyMVPA Authors',
#   'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
latex_logo = 'pics/pymvpa_logo.pdf'

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
latex_preamble = r"""
\usepackage{enumitem}
\setdescription{style=nextline,font=\normalfont}

% to get proper single quotes
% source: http://stackoverflow.com/questions/5757630/sphinx-pdf-output-apostrophes-in-python-source-are-replaced-by-right-single-quo
\usepackage{upquote}

% more table of contents
\setcounter{tocdepth}{3}

% Have gray background for notes and exercises
\definecolor{MyBluishGray}{rgb}{0.90,0.90,1.00}

\makeatletter\newenvironment{graybox}{%
   \begin{lrbox}{\@tempboxa}\begin{minipage}{\columnwidth}}{\end{minipage}\end{lrbox}%
   \colorbox{MyBluishGray}{\usebox{\@tempboxa}}
}\makeatother

\makeatletter
\renewenvironment{notice}[2]{
  \begin{graybox}
  \bf\it
  \def\py@noticetype{#1}
  \par\strong{#2}
  \csname py@noticestart@#1\endcsname
}
{
  \csname py@noticeend@\py@noticetype\endcsname
  \end{graybox}
}
\makeatother

"""

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

# -----------------------------------------------------------------------------
# Intersphinx configuration
# -----------------------------------------------------------------------------
# to link to related projects
intersphinx_mapping = {'http://docs.python.org/': None,
                       'http://nipy.sourceforge.net/nipype': None,
                       'http://nipy.sourceforge.net/nipy/stable': None,
                       'http://h5py.alfven.org/docs': None,
                       'http://docs.scipy.org/doc/scipy/reference': None,
                       'http://docs.scipy.org/doc/numpy/': None,
                       'http://matplotlib.sourceforge.net/': None,
                       }

########NEW FILE########
__FILENAME__ = generate
# -*- coding: utf-8 -*-
"""
    sphinx.ext.autosummary.generate
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Usable as a library or script to generate automatic RST source files for
    items referred to in autosummary:: directives.

    Each generated RST file contains a single auto*:: directive which
    extracts the docstring of the referred item.

    Example Makefile rule::

       generate:
               sphinx-autogen source/*.rst source/generated

    :copyright: Copyright 2007-2010 by the Sphinx team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""
import os
import re
import sys
import pydoc
import optparse

from jinja2 import FileSystemLoader, TemplateNotFound
from jinja2.sandbox import SandboxedEnvironment

from sphinx.ext.autosummary import import_by_name, get_documenter
from sphinx.jinja2glue import BuiltinTemplateLoader
from sphinx.util.osutil import ensuredir

def main(argv=sys.argv):
    usage = """%prog [OPTIONS] SOURCEFILE ..."""
    p = optparse.OptionParser(usage.strip())
    p.add_option("-o", "--output-dir", action="store", type="string",
                 dest="output_dir", default=None,
                 help="Directory to place all output in")
    p.add_option("-s", "--suffix", action="store", type="string",
                 dest="suffix", default="rst",
                 help="Default suffix for files (default: %default)")
    p.add_option("-t", "--templates", action="store", type="string",
                 dest="templates", default=None,
                 help="Custom template directory (default: %default)")
    options, args = p.parse_args(argv[1:])

    if len(args) < 1:
        p.error('no input files given')

    generate_autosummary_docs(args, options.output_dir,
                              "." + options.suffix,
                              template_dir=options.templates)

def _simple_info(msg):
    print msg

def _simple_warn(msg):
    print >> sys.stderr, 'WARNING: ' + msg

# -- Generating output ---------------------------------------------------------

def generate_autosummary_docs(sources, output_dir=None, suffix='.rst',
                              warn=_simple_warn, info=_simple_info,
                              base_path=None, builder=None, template_dir=None):

    showed_sources = list(sorted(sources))
    if len(showed_sources) > 20:
        showed_sources = showed_sources[:10] + ['...'] + showed_sources[-10:]
    info('[autosummary] generating autosummary for: %s' %
         ', '.join(showed_sources))

    if output_dir:
        info('[autosummary] writing to %s' % output_dir)

    if base_path is not None:
        sources = [os.path.join(base_path, filename) for filename in sources]

    # create our own templating environment
    template_dirs = [os.path.join(os.path.dirname(__file__), 'templates')]
    if builder is not None:
        # allow the user to override the templates
        template_loader = BuiltinTemplateLoader()
        template_loader.init(builder, dirs=template_dirs)
    else:
        if template_dir:
            template_dirs.insert(0, template_dir)
        template_loader = FileSystemLoader(template_dirs)
    template_env = SandboxedEnvironment(loader=template_loader)

    # read
    items = find_autosummary_in_files(sources)

    # remove possible duplicates
    items = dict([(item, True) for item in items]).keys()

    # keep track of new files
    new_files = []

    # write
    for name, path, template_name in sorted(items):
        if path is None:
            # The corresponding autosummary:: directive did not have
            # a :toctree: option
            continue

        path = output_dir or os.path.abspath(path)
        ensuredir(path)

        try:
            obj, name = import_by_name(name)
        except ImportError, e:
            warn('[autosummary] failed to import %r: %s' % (name, e))
            continue

        fn = os.path.join(path, name + suffix)

        # skip it if it exists
        if os.path.isfile(fn):
            continue

        new_files.append(fn)

        f = open(fn, 'w')

        try:
            doc = get_documenter(obj)

            if template_name is not None:
                template = template_env.get_template(template_name)
            else:
                try:
                    template = template_env.get_template('autosummary/%s.rst'
                                                         % doc.objtype)
                except TemplateNotFound:
                    template = template_env.get_template('autosummary/base.rst')

            def get_members(obj, typ, include_public=[]):
                items = []
                for name in dir(obj):
                    try:
                        if get_documenter(getattr(obj, name)).objtype == typ:
                            items.append(name)
                    except AttributeError:
                        warn("[autosummary] problem accessing attribute "
                             "'%s' in '%s'." % (name, obj))
                public = [x for x in items
                          if x in include_public or not x.startswith('_')]
                return public, items

            ns = {}

            if doc.objtype == 'module':
                ns['members'] = dir(obj)
                ns['functions'], ns['all_functions'] = \
                                   get_members(obj, 'function')
                ns['classes'], ns['all_classes'] = \
                                 get_members(obj, 'class')
                ns['exceptions'], ns['all_exceptions'] = \
                                   get_members(obj, 'exception')
            elif doc.objtype == 'class':
                ns['members'] = dir(obj)
                ns['methods'], ns['all_methods'] = \
                                 get_members(obj, 'method', ['__init__'])
                ns['attributes'], ns['all_attributes'] = \
                                 get_members(obj, 'attribute')

            parts = name.split('.')
            if doc.objtype in ('method', 'attribute'):
                mod_name = '.'.join(parts[:-2])
                cls_name = parts[-2]
                obj_name = '.'.join(parts[-2:])
                ns['class'] = cls_name
            else:
                mod_name, obj_name = '.'.join(parts[:-1]), parts[-1]

            ns['fullname'] = name
            ns['module'] = mod_name
            ns['objname'] = obj_name
            ns['name'] = parts[-1]

            ns['objtype'] = doc.objtype
            ns['underline'] = len(name) * '='

            rendered = template.render(**ns)
            f.write(rendered)
        finally:
            f.close()

    # descend recursively to new files
    if new_files:
        generate_autosummary_docs(new_files, output_dir=output_dir,
                                  suffix=suffix, warn=warn, info=info,
                                  base_path=base_path, builder=builder,
                                  template_dir=template_dir)


# -- Finding documented entries in files ---------------------------------------

def find_autosummary_in_files(filenames):
    """
    Find out what items are documented in source/*.rst.
    See `find_autosummary_in_lines`.
    """
    documented = []
    for filename in filenames:
        f = open(filename, 'r')
        lines = f.read().splitlines()
        documented.extend(find_autosummary_in_lines(lines, filename=filename))
        f.close()
    return documented

def find_autosummary_in_docstring(name, module=None, filename=None):
    """
    Find out what items are documented in the given object's docstring.
    See `find_autosummary_in_lines`.
    """
    try:
        obj, real_name = import_by_name(name)
        lines = pydoc.getdoc(obj).splitlines()
        return find_autosummary_in_lines(lines, module=name, filename=filename)
    except AttributeError:
        pass
    except ImportError, e:
        print "Failed to import '%s': %s" % (name, e)
    return []

def find_autosummary_in_lines(lines, module=None, filename=None):
    """
    Find out what items appear in autosummary:: directives in the given lines.

    Returns a list of (name, toctree, template) where *name* is a name
    of an object and *toctree* the :toctree: path of the corresponding
    autosummary directive (relative to the root of the file name), and
    *template* the value of the :template: option. *toctree* and
    *template* ``None`` if the directive does not have the
    corresponding options set.
    """
    autosummary_re = re.compile(r'^\s*\.\.\s+autosummary::\s*')
    automodule_re = re.compile(
        r'^\s*\.\.\s+automodule::\s*([A-Za-z0-9_.]+)\s*$')
    module_re = re.compile(
        r'^\s*\.\.\s+(current)?module::\s*([a-zA-Z0-9_.]+)\s*$')
    autosummary_item_re = re.compile(r'^\s+(~?[_a-zA-Z][a-zA-Z0-9_.]*)\s*.*?')
    toctree_arg_re = re.compile(r'^\s+:toctree:\s*(.*?)\s*$')
    template_arg_re = re.compile(r'^\s+:template:\s*(.*?)\s*$')

    documented = []

    toctree = None
    template = None
    current_module = module
    in_autosummary = False

    for line in lines:
        if in_autosummary:
            m = toctree_arg_re.match(line)
            if m:
                toctree = m.group(1)
                if filename:
                    toctree = os.path.join(os.path.dirname(filename),
                                           toctree)
                continue

            m = template_arg_re.match(line)
            if m:
                template = m.group(1).strip()
                continue

            if line.strip().startswith(':'):
                continue # skip options

            m = autosummary_item_re.match(line)
            if m:
                name = m.group(1).strip()
                if name.startswith('~'):
                    name = name[1:]
                if current_module and \
                       not name.startswith(current_module + '.'):
                    name = "%s.%s" % (current_module, name)
                documented.append((name, toctree, template))
                continue

            if not line.strip():
                continue

            in_autosummary = False

        m = autosummary_re.match(line)
        if m:
            in_autosummary = True
            toctree = None
            template = None
            continue

        m = automodule_re.search(line)
        if m:
            current_module = m.group(1).strip()
            # recurse into the automodule docstring
            documented.extend(find_autosummary_in_docstring(
                current_module, filename=filename))
            continue

        m = module_re.match(line)
        if m:
            current_module = m.group(2)
            continue

    return documented


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = docscrape
"""Extract reference documentation from the NumPy source tree.

"""

import inspect
import textwrap
import re
import pydoc
from StringIO import StringIO
from warnings import warn
4
class Reader(object):
    """A line-based string reader.

    """
    def __init__(self, data):
        """
        Parameters
        ----------
        data : str
           String with lines separated by '\n'.

        """
        if isinstance(data,list):
            self._str = data
        else:
            self._str = data.split('\n') # store string as list of lines

        self.reset()

    def __getitem__(self, n):
        return self._str[n]

    def reset(self):
        self._l = 0 # current line nr

    def read(self):
        if not self.eof():
            out = self[self._l]
            self._l += 1
            return out
        else:
            return ''

    def seek_next_non_empty_line(self):
        for l in self[self._l:]:
            if l.strip():
                break
            else:
                self._l += 1

    def eof(self):
        return self._l >= len(self._str)

    def read_to_condition(self, condition_func):
        start = self._l
        for line in self[start:]:
            if condition_func(line):
                return self[start:self._l]
            self._l += 1
            if self.eof():
                return self[start:self._l+1]
        return []

    def read_to_next_empty_line(self):
        self.seek_next_non_empty_line()
        def is_empty(line):
            return not line.strip()
        return self.read_to_condition(is_empty)

    def read_to_next_unindented_line(self):
        def is_unindented(line):
            return (line.strip() and (len(line.lstrip()) == len(line)))
        return self.read_to_condition(is_unindented)

    def peek(self,n=0):
        if self._l + n < len(self._str):
            return self[self._l + n]
        else:
            return ''

    def is_empty(self):
        return not ''.join(self._str).strip()


class NumpyDocString(object):
    def __init__(self,docstring):
        docstring = textwrap.dedent(docstring).split('\n')

        self._doc = Reader(docstring)
        self._parsed_data = {
            'Signature': '',
            'Summary': [''],
            'Extended Summary': [],
            'Parameters': [],
            'Returns': [],
            'Raises': [],
            'Warns': [],
            'Other Parameters': [],
            'Attributes': [],
            'Methods': [],
            'See Also': [],
            'Notes': [],
            'Warnings': [],
            'References': '',
            'Examples': '',
            'index': {}
            }

        self._parse()

    def __getitem__(self,key):
        return self._parsed_data[key]

    def __setitem__(self,key,val):
        if not self._parsed_data.has_key(key):
            warn("Unknown section %s" % key)
        else:
            self._parsed_data[key] = val

    def _is_at_section(self):
        self._doc.seek_next_non_empty_line()

        if self._doc.eof():
            return False

        l1 = self._doc.peek().strip()  # e.g. Parameters

        if l1.startswith('.. index::'):
            return True

        l2 = self._doc.peek(1).strip() #    ---------- or ==========
        return l2.startswith('-'*len(l1)) or l2.startswith('='*len(l1))

    def _strip(self,doc):
        i = 0
        j = 0
        for i,line in enumerate(doc):
            if line.strip(): break

        for j,line in enumerate(doc[::-1]):
            if line.strip(): break

        return doc[i:len(doc)-j]

    def _read_to_next_section(self):
        section = self._doc.read_to_next_empty_line()

        while not self._is_at_section() and not self._doc.eof():
            if not self._doc.peek(-1).strip(): # previous line was empty
                section += ['']

            section += self._doc.read_to_next_empty_line()

        return section

    def _read_sections(self):
        while not self._doc.eof():
            data = self._read_to_next_section()
            name = data[0].strip()

            if name.startswith('..'): # index section
                yield name, data[1:]
            elif len(data) < 2:
                yield StopIteration
            else:
                yield name, self._strip(data[2:])

    def _parse_param_list(self,content):
        r = Reader(content)
        params = []
        while not r.eof():
            header = r.read().strip()
            if ' : ' in header:
                arg_name, arg_type = header.split(' : ')[:2]
            else:
                arg_name, arg_type = header, ''

            desc = r.read_to_next_unindented_line()
            desc = dedent_lines(desc)

            params.append((arg_name,arg_type,desc))

        return params

    
    _name_rgx = re.compile(r"^\s*(:(?P<role>\w+):`(?P<name>[a-zA-Z0-9_.-]+)`|"
                           r" (?P<name2>[a-zA-Z0-9_.-]+))\s*", re.X)
    def _parse_see_also(self, content):
        """
        func_name : Descriptive text
            continued text
        another_func_name : Descriptive text
        func_name1, func_name2, :meth:`func_name`, func_name3

        """
        items = []

        def parse_item_name(text):
            """Match ':role:`name`' or 'name'"""
            m = self._name_rgx.match(text)
            if m:
                g = m.groups()
                if g[1] is None:
                    return g[3], None
                else:
                    return g[2], g[1]
            raise ValueError("%s is not a item name" % text)

        def push_item(name, rest):
            if not name:
                return
            name, role = parse_item_name(name)
            items.append((name, list(rest), role))
            del rest[:]

        current_func = None
        rest = []
        
        for line in content:
            if not line.strip(): continue

            m = self._name_rgx.match(line)
            if m and line[m.end():].strip().startswith(':'):
                push_item(current_func, rest)
                current_func, line = line[:m.end()], line[m.end():]
                rest = [line.split(':', 1)[1].strip()]
                if not rest[0]:
                    rest = []
            elif not line.startswith(' '):
                push_item(current_func, rest)
                current_func = None
                if ',' in line:
                    for func in line.split(','):
                        push_item(func, [])
                elif line.strip():
                    current_func = line
            elif current_func is not None:
                rest.append(line.strip())
        push_item(current_func, rest)
        return items

    def _parse_index(self, section, content):
        """
        .. index: default
           :refguide: something, else, and more

        """
        def strip_each_in(lst):
            return [s.strip() for s in lst]

        out = {}
        section = section.split('::')
        if len(section) > 1:
            out['default'] = strip_each_in(section[1].split(','))[0]
        for line in content:
            line = line.split(':')
            if len(line) > 2:
                out[line[1]] = strip_each_in(line[2].split(','))
        return out
    
    def _parse_summary(self):
        """Grab signature (if given) and summary"""
        if self._is_at_section():
            return

        summary = self._doc.read_to_next_empty_line()
        summary_str = " ".join([s.strip() for s in summary]).strip()
        if re.compile('^([\w., ]+=)?\s*[\w\.]+\(.*\)$').match(summary_str):
            self['Signature'] = summary_str
            if not self._is_at_section():
                self['Summary'] = self._doc.read_to_next_empty_line()
        else:
            self['Summary'] = summary

        if not self._is_at_section():
            self['Extended Summary'] = self._read_to_next_section()
    
    def _parse(self):
        self._doc.reset()
        self._parse_summary()

        for (section,content) in self._read_sections():
            if not section.startswith('..'):
                section = ' '.join([s.capitalize() for s in section.split(' ')])
            if section in ('Parameters', 'Attributes', 'Methods',
                           'Returns', 'Raises', 'Warns'):
                self[section] = self._parse_param_list(content)
            elif section.startswith('.. index::'):
                self['index'] = self._parse_index(section, content)
            elif section == 'See Also':
                self['See Also'] = self._parse_see_also(content)
            else:
                self[section] = content

    # string conversion routines

    def _str_header(self, name, symbol='-'):
        return [name, len(name)*symbol]

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        if self['Signature']:
            return [self['Signature'].replace('*','\*')] + ['']
        else:
            return ['']

    def _str_summary(self):
        if self['Summary']:
            return self['Summary'] + ['']
        else:
            return []

    def _str_extended_summary(self):
        if self['Extended Summary']:
            return self['Extended Summary'] + ['']
        else:
            return []

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            for param,param_type,desc in self[name]:
                out += ['%s : %s' % (param, param_type)]
                out += self._str_indent(desc)
            out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += self[name]
            out += ['']
        return out

    def _str_see_also(self, func_role):
        if not self['See Also']: return []
        out = []
        out += self._str_header("See Also")
        last_had_desc = True
        for func, desc, role in self['See Also']:
            if role:
                link = ':%s:`%s`' % (role, func)
            elif func_role:
                link = ':%s:`%s`' % (func_role, func)
            else:
                link = "`%s`_" % func
            if desc or last_had_desc:
                out += ['']
                out += [link]
            else:
                out[-1] += ", %s" % link
            if desc:
                out += self._str_indent([' '.join(desc)])
                last_had_desc = True
            else:
                last_had_desc = False
        out += ['']
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        out += ['.. index:: %s' % idx.get('default','')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            out += ['   :%s: %s' % (section, ', '.join(references))]
        return out

    def __str__(self, func_role=''):
        out = []
        out += self._str_signature()
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters','Returns','Raises'):
            out += self._str_param_list(param_list)
        out += self._str_section('Warnings')
        out += self._str_see_also(func_role)
        for s in ('Notes','References','Examples'):
            out += self._str_section(s)
        out += self._str_index()
        return '\n'.join(out)


def indent(str,indent=4):
    indent_str = ' '*indent
    if str is None:
        return indent_str
    lines = str.split('\n')
    return '\n'.join(indent_str + l for l in lines)

def dedent_lines(lines):
    """Deindent a list of lines maximally"""
    return textwrap.dedent("\n".join(lines)).split("\n")

def header(text, style='-'):
    return text + '\n' + style*len(text) + '\n'


class FunctionDoc(NumpyDocString):
    def __init__(self, func, role='func', doc=None):
        self._f = func
        self._role = role # e.g. "func" or "meth"
        if doc is None:
            doc = inspect.getdoc(func) or ''
        try:
            NumpyDocString.__init__(self, doc)
        except ValueError, e:
            print '*'*78
            print "ERROR: '%s' while parsing `%s`" % (e, self._f)
            print '*'*78
            #print "Docstring follows:"
            #print doclines
            #print '='*78

        if not self['Signature']:
            func, func_name = self.get_func()
            try:
                # try to read signature
                argspec = inspect.getargspec(func)
                argspec = inspect.formatargspec(*argspec)
                argspec = argspec.replace('*','\*')
                signature = '%s%s' % (func_name, argspec)
            except TypeError, e:
                signature = '%s()' % func_name
            self['Signature'] = signature

    def get_func(self):
        func_name = getattr(self._f, '__name__', self.__class__.__name__)
        if inspect.isclass(self._f):
            func = getattr(self._f, '__call__', self._f.__init__)
        else:
            func = self._f
        return func, func_name
            
    def __str__(self):
        out = ''

        func, func_name = self.get_func()
        signature = self['Signature'].replace('*', '\*')

        roles = {'func': 'function',
                 'meth': 'method'}

        if self._role:
            if not roles.has_key(self._role):
                print "Warning: invalid role %s" % self._role
            out += '.. %s:: %s\n    \n\n' % (roles.get(self._role,''),
                                             func_name)

        out += super(FunctionDoc, self).__str__(func_role=self._role)
        return out


class ClassDoc(NumpyDocString):
    def __init__(self,cls,modulename='',func_doc=FunctionDoc,doc=None):
        if not inspect.isclass(cls):
            raise ValueError("Initialise using a class. Got %r" % cls)
        self._cls = cls

        if modulename and not modulename.endswith('.'):
            modulename += '.'
        self._mod = modulename
        self._name = cls.__name__
        self._func_doc = func_doc

        if doc is None:
            doc = pydoc.getdoc(cls)

        NumpyDocString.__init__(self, doc)

    @property
    def methods(self):
        return [name for name,func in inspect.getmembers(self._cls)
                if not name.startswith('_') and callable(func)]

    def __str__(self):
        out = ''
        out += super(ClassDoc, self).__str__()
        out += "\n\n"

        #for m in self.methods:
        #    print "Parsing `%s`" % m
        #    out += str(self._func_doc(getattr(self._cls,m), 'meth')) + '\n\n'
        #    out += '.. index::\n   single: %s; %s\n\n' % (self._name, m)

        return out



########NEW FILE########
__FILENAME__ = docscrape_sphinx
import re, inspect, textwrap, pydoc
from docscrape import NumpyDocString, FunctionDoc, ClassDoc

class SphinxDocString(NumpyDocString):
    # string conversion routines
    def _str_header(self, name, symbol='`'):
        return ['.. rubric:: ' + name, '']

    def _str_field_list(self, name):
        return [':' + name + ':']

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        return ['']
        if self['Signature']:
            return ['``%s``' % self['Signature']] + ['']
        else:
            return ['']

    def _str_summary(self):
        return self['Summary'] + ['']

    def _str_extended_summary(self):
        return self['Extended Summary'] + ['']

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_field_list(name)
            out += ['']
            for param,param_type,desc in self[name]:
                out += self._str_indent(['**%s** : %s' % (param.strip(),
                                                          param_type)])
                out += ['']
                out += self._str_indent(desc,8)
                out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += ['']
            content = textwrap.dedent("\n".join(self[name])).split("\n")
            out += content
            out += ['']
        return out

    def _str_see_also(self, func_role):
        out = []
        if self['See Also']:
            see_also = super(SphinxDocString, self)._str_see_also(func_role)
            out = ['.. seealso::', '']
            out += self._str_indent(see_also[2:])
        return out

    def _str_warnings(self):
        out = []
        if self['Warnings']:
            out = ['.. warning::', '']
            out += self._str_indent(self['Warnings'])
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        if len(idx) == 0:
            return out

        out += ['.. index:: %s' % idx.get('default','')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            elif section == 'refguide':
                out += ['   single: %s' % (', '.join(references))]
            else:
                out += ['   %s: %s' % (section, ','.join(references))]
        return out

    def _str_references(self):
        out = []
        if self['References']:
            out += self._str_header('References')
            if isinstance(self['References'], str):
                self['References'] = [self['References']]
            out.extend(self['References'])
            out += ['']
        return out

    def __str__(self, indent=0, func_role="obj"):
        out = []
        out += self._str_signature()
        out += self._str_index() + ['']
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters', 'Attributes', 'Methods',
                           'Returns','Raises'):
            out += self._str_param_list(param_list)
        out += self._str_warnings()
        out += self._str_see_also(func_role)
        out += self._str_section('Notes')
        out += self._str_references()
        out += self._str_section('Examples')
        out = self._str_indent(out,indent)
        return '\n'.join(out)

class SphinxFunctionDoc(SphinxDocString, FunctionDoc):
    pass

class SphinxClassDoc(SphinxDocString, ClassDoc):
    pass

def get_doc_object(obj, what=None, doc=None):
    if what is None:
        if inspect.isclass(obj):
            what = 'class'
        elif inspect.ismodule(obj):
            what = 'module'
        elif callable(obj):
            what = 'function'
        else:
            what = 'object'
    if what == 'class':
        return SphinxClassDoc(obj, '', func_doc=SphinxFunctionDoc, doc=doc)
    elif what in ('function', 'method'):
        return SphinxFunctionDoc(obj, '', doc=doc)
    else:
        if doc is None:
            doc = pydoc.getdoc(obj)
        return SphinxDocString(doc)


########NEW FILE########
__FILENAME__ = exercise_directive
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Sphinx extension add a 'exercise' directive similar to 'seealso' and 'note'.

This directive can be used to add exercize boxes to tutorials.
"""

__docformat__ = 'restructuredtext'

from sphinx import addnodes
from sphinx.util.compat import Directive, make_admonition

from docutils import nodes

class exercise(nodes.Admonition, nodes.Element):
    pass

def visit_exercise_node(self, node):
    self.visit_admonition(node)

def depart_exercise_node(self, node):
    self.depart_admonition(node)


class TaskDirective(Directive):
    """
    An admonition mentioning a exercise to perform (e.g. in a tutorial).
    """

    has_content = True
    required_arguments = 0
    optional_arguments = 1
    final_argument_whitespace = True
    option_spec = {}

    def run(self):
        ret = make_admonition(
            exercise, self.name, ['Exercise'], self.options,
            self.content, self.lineno, self.content_offset, self.block_text,
            self.state, self.state_machine)
        if self.arguments:
            argnodes, msgs = self.state.inline_text(self.arguments[0],
                                                    self.lineno)
            para = nodes.paragraph()
            para += argnodes
            para += msgs
            ret[0].insert(1, para)
        return ret


def setup(app):
    app.add_node(exercise,
                 html=(visit_exercise_node, depart_exercise_node),
                 latex=(visit_exercise_node, depart_exercise_node),
                 text=(visit_exercise_node, depart_exercise_node))
    app.add_directive('exercise', TaskDirective)

########NEW FILE########
__FILENAME__ = numpydoc
"""
========
numpydoc
========

Sphinx extension that handles docstrings in the Numpy standard format. [1]

It will:

- Convert Parameters etc. sections to field lists.
- Convert See Also section to a See also entry.
- Renumber references.
- Extract the signature from the docstring, if it can't be determined otherwise.

.. [1] http://projects.scipy.org/numpy/wiki/CodingStyleGuidelines#docstring-standard

"""

import os, re, pydoc
from docscrape_sphinx import get_doc_object, SphinxDocString
import inspect

def mangle_docstrings(app, what, name, obj, options, lines,
                      reference_offset=[0]):
    if what == 'module':
        # Strip top title
        title_re = re.compile(r'^\s*[#*=]{4,}\n[a-z0-9 -]+\n[#*=]{4,}\s*',
                              re.I|re.S)
        lines[:] = title_re.sub('', "\n".join(lines)).split("\n")
    else:
        doc = get_doc_object(obj, what, "\n".join(lines))
        lines[:] = str(doc).split("\n")

    if app.config.numpydoc_edit_link and hasattr(obj, '__name__') and \
           obj.__name__:
        if hasattr(obj, '__module__'):
            v = dict(full_name="%s.%s" % (obj.__module__, obj.__name__))
        else:
            v = dict(full_name=obj.__name__)
        lines += ['', '.. htmlonly::', '']
        lines += ['    %s' % x for x in
                  (app.config.numpydoc_edit_link % v).split("\n")]

    # replace reference numbers so that there are no duplicates
    references = []
    for l in lines:
        l = l.strip()
        if l.startswith('.. ['):
            try:
                references.append(int(l[len('.. ['):l.index(']')]))
            except ValueError:
                print "WARNING: invalid reference in %s docstring" % name

    # Start renaming from the biggest number, otherwise we may
    # overwrite references.
    references.sort()
    if references:
        for i, line in enumerate(lines):
            for r in references:
                new_r = reference_offset[0] + r
                lines[i] = lines[i].replace('[%d]_' % r,
                                            '[%d]_' % new_r)
                lines[i] = lines[i].replace('.. [%d]' % r,
                                            '.. [%d]' % new_r)

    reference_offset[0] += len(references)

def mangle_signature(app, what, name, obj, options, sig, retann):
    # Do not try to inspect classes that don't define `__init__`
    if (inspect.isclass(obj) and
        'initializes x; see ' in pydoc.getdoc(obj.__init__)):
        return '', ''

    if not (callable(obj) or hasattr(obj, '__argspec_is_invalid_')): return
    if not hasattr(obj, '__doc__'): return

    doc = SphinxDocString(pydoc.getdoc(obj))
    if doc['Signature']:
        sig = re.sub("^[^(]*", "", doc['Signature'])
        return sig, ''

def initialize(app):
    try:
        app.connect('autodoc-process-signature', mangle_signature)
    except:
        monkeypatch_sphinx_ext_autodoc()

def setup(app, get_doc_object_=get_doc_object):
    global get_doc_object
    get_doc_object = get_doc_object_
    
    app.connect('autodoc-process-docstring', mangle_docstrings)
    app.connect('builder-inited', initialize)
    app.add_config_value('numpydoc_edit_link', None, True)

#------------------------------------------------------------------------------
# Monkeypatch sphinx.ext.autodoc to accept argspecless autodocs (Sphinx < 0.5)
#------------------------------------------------------------------------------

def monkeypatch_sphinx_ext_autodoc():
    global _original_format_signature
    import sphinx.ext.autodoc

    if sphinx.ext.autodoc.format_signature is our_format_signature:
        return

    print "[numpydoc] Monkeypatching sphinx.ext.autodoc ..."
    _original_format_signature = sphinx.ext.autodoc.format_signature
    sphinx.ext.autodoc.format_signature = our_format_signature

def our_format_signature(what, obj):
    r = mangle_signature(None, what, None, obj, None, None, None)
    if r is not None:
        return r[0]
    else:
        return _original_format_signature(what, obj)

########NEW FILE########
__FILENAME__ = hyperalignment
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Transformation of individual feature spaces into a common space

The :class:`Hyperalignment` class in this module implements an algorithm
published in :ref:`Haxby et al., Neuron (2011) <HGC+11>` *A common,
high-dimensional model of the representational space in human ventral temporal
cortex.*

"""

__docformat__ = 'restructuredtext'

# don't leak the world
__all__ = ['Hyperalignment']

from mvpa2.support.copy import deepcopy

import numpy as np

from mvpa2.base.state import ConditionalAttribute, ClassWithCollections
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import *
from mvpa2.mappers.procrustean import ProcrusteanMapper
from mvpa2.datasets import Dataset
from mvpa2.mappers.base import ChainMapper
from mvpa2.mappers.zscore import zscore, ZScoreMapper
from mvpa2.mappers.staticprojection import StaticProjectionMapper

if __debug__:
    from mvpa2.base import debug

__all__ = [ "Hyperalignment" ]

class Hyperalignment(ClassWithCollections):
    """Align the features across multiple datasets into a common feature space.

    This is a three-level algorithm. In the first level, a series of input
    datasets is projected into a common feature space using a configurable
    mapper. The common space is initially defined by a chosen exemplar from the
    list of input datasets, but is subsequently refined by iteratively combining
    the common space with the projected input datasets.

    In the second (optional) level, the original input datasets are again
    aligned with (or projected into) the intermediate first-level common
    space. Through a configurable number of iterations the common space is
    further refined by repeated projections of the input datasets and
    combination/aggregation of these projections into an updated common space.

    In the third level, the input datasets are again aligned with the, now
    final, common feature space. The output of this algorithm are trained
    mappers (one for each input dataset) that transform the individual features
    spaces into the common space.

    Level 1 and 2 are performed by the ``train()`` method, and level 3 is
    performed when the trained Hyperalignment instance is called with a list of
    datasets. This dataset list may or may not be identical to the training
    datasets.

    The default values for the parameters of the algorithm (e.g. projection via
    Procrustean transformation, common space aggregation by averaging) resemble
    the setup reported in :ref:`Haxby et al., Neuron (2011) <HGC+11>` *A common,
    high-dimensional model of the representational space in human ventral
    temporal cortex.*

    Examples
    --------
    >>> # get some example data
    >>> from mvpa2.testing.datasets import datasets
    >>> from mvpa2.misc.data_generators import random_affine_transformation
    >>> ds4l = datasets['uni4large']
    >>> # generate a number of distorted variants of this data
    >>> dss = [random_affine_transformation(ds4l) for i in xrange(4)]
    >>> ha = Hyperalignment()
    >>> ha.train(dss)
    >>> mappers = ha(dss)
    >>> len(mappers)
    4
    """

    training_residual_errors = ConditionalAttribute(enabled=False,
            doc="""Residual error (norm of the difference between common space
                and projected data) per each training dataset at each level. The
                residuals are stored in a dataset with one row per level, and
                one column per input dataset. The first row corresponds to the
                error 1st-level of hyperalignment the remaining rows store the
                residual errors for each 2nd-level iteration.""")

    residual_errors = ConditionalAttribute(enabled=False,
            doc="""Residual error (norm of the difference between common space
                and projected data) per each dataset. The residuals are stored
                in a single-row dataset with one column per input dataset.""")

    # XXX Who cares whether it was chosen, or specified? This should be just
    # 'ref_ds'
    chosen_ref_ds = ConditionalAttribute(enabled=True,
            doc="""Index of the input dataset used as 1st-level reference
                dataset.""")

    # Lets use built-in facilities to specify parameters which
    # constructor should accept
    # the ``space`` of the mapper determines where the algorithm places the
    # common space definition in the datasets
    alignment = Parameter(ProcrusteanMapper(space='commonspace'), # might provide allowedtype
	    # XXX Currently, there's no way to handle this with connstraints  
            doc="""The multidimensional transformation mapper. If
            `None` (default) an instance of
            :class:`~mvpa2.mappers.procrustean.ProcrusteanMapper` is
            used.""")

    alpha = Parameter(1, constraints=EnsureFloat() & EnsureRange(min=0, max=1),
            doc="""Regularization parameter to traverse between (Shrinkage)-CCA
                (canonical correlation analysis) and regular hyperalignment.
                Setting alpha to 1 makes the algorithm identical to
                hyperalignment and alpha of 0 makes it CCA. By default,
                it is 1, therefore hyperalignment. """)

    level2_niter = Parameter(1, constraints=EnsureInt() & EnsureRange(min=0),
            doc="Number of 2nd-level iterations.")

    ref_ds = Parameter(None, constraints=(EnsureInt() & EnsureRange(min=0) 
                                          | EnsureNone()),
            doc="""Index of a dataset to use as 1st-level common space
                reference.  If `None`, then the dataset with the maximum
                number of features is used.""")

    zscore_all = Parameter(False, constraints='bool',
            doc="""Flag to Z-score all datasets prior hyperalignment.
            Turn it off if Z-scoring is not desired or was already performed.
            If True, returned mappers are ChainMappers with the Z-scoring
            prepended to the actual projection.""")

    zscore_common = Parameter(True, constraints='bool',
            doc="""Flag to Z-score the common space after each adjustment.
                This should be left enabled in most cases.""")

    combiner1 = Parameter(lambda x,y: 0.5*(x+y), #
            doc="""How to update common space in the 1st-level loop. This must
                be a callable that takes two arguments. The first argument is
                one of the input datasets after projection onto the 1st-level
                common space. The second argument is the current 1st-level
                common space. The 1st-level combiner is called iteratively for
                each projected input dataset, except for the reference dataset.
                By default the new common space is the average of the current
                common space and the recently projected dataset.""")

    combiner2 = Parameter(lambda l: np.mean(l, axis=0),
            doc="""How to combine all individual spaces to common space. This
            must be a callable that take a sequence of datasets as an argument.
            The callable must return a single array. This combiner is called
            once with all datasets after 1st-level projection to create an
            updated common space, and is subsequently called again after each
            2nd-level iteration.""")


    def __init__(self, **kwargs):
        ClassWithCollections.__init__(self, **kwargs)
        self.commonspace = None


    def train(self, datasets):
        """Derive a common feature space from a series of datasets.

        Parameters
        ----------
        datasets : sequence of datasets

        Returns
        -------
        A list of trained Mappers matching the number of input datasets.
        """
        params = self.params            # for quicker access ;)
        ca = self.ca
        ndatasets = len(datasets)
        nfeatures = [ds.nfeatures for ds in datasets]
        alpha = params.alpha
        
        residuals = None
        if ca['training_residual_errors'].enabled:
            residuals = np.zeros((1 + params.level2_niter, ndatasets))
            ca.training_residual_errors = Dataset(
                samples = residuals,
                sa = {'levels' :
                       ['1'] +
                       ['2:%i' % i for i in xrange(params.level2_niter)]})

        if __debug__:
            debug('HPAL', "Hyperalignment %s for %i datasets"
                  % (self, ndatasets))

        if params.ref_ds is None:
            ref_ds = np.argmax(nfeatures)
        else:
            ref_ds = params.ref_ds
            if ref_ds < 0 and ref_ds >= ndatasets:
                raise ValueError, "Requested reference dataset %i is out of " \
                      "bounds. We have only %i datasets provided" \
                      % (ref_ds, ndatasets)
        ca.chosen_ref_ds = ref_ds
        # zscore all data sets
        # ds = [ zscore(ds, chunks_attr=None) for ds in datasets]

        # TODO since we are doing in-place zscoring create deep copies
        # of the datasets with pruned targets and shallow copies of
        # the collections (if they would come needed in the transformation)
        # TODO: handle floats and non-floats differently to prevent
        #       waste of memory if there is no need (e.g. no z-scoring)
        #otargets = [ds.sa.targets for ds in datasets]
        datasets = [ds.copy(deep=False) for ds in datasets]
        #datasets = [Dataset(ds.samples.astype(float), sa={'targets': [None] * len(ds)})
        #datasets = [Dataset(ds.samples, sa={'targets': [None] * len(ds)})
        #            for ds in datasets]

        if params.zscore_all:
            if __debug__:
                debug('HPAL', "Z-scoring all datasets")
            for ids in xrange(len(datasets)):
                zmapper = ZScoreMapper(chunks_attr=None)
                zmapper.train(datasets[ids])
                datasets[ids] = zmapper.forward(datasets[ids])

        if alpha < 1:
            datasets, wmappers = self._regularize(datasets, alpha)

        # initial common space is the reference dataset
        commonspace = datasets[ref_ds].samples
        # the reference dataset might have been zscored already, don't do it
        # twice
        if params.zscore_common and not params.zscore_all:
            if __debug__:
                debug('HPAL_',
                      "Creating copy of a commonspace and assuring "
                      "it is of a floating type")
            commonspace = commonspace.astype(float)
            zscore(commonspace, chunks_attr=None)

        # create a mapper per dataset
        # might prefer some other way to initialize... later
        mappers = [deepcopy(params.alignment) for ds in datasets]

        #
        # Level 1 -- initial projection
        #
        lvl1_projdata = self._level1(datasets, commonspace, ref_ds, mappers,
                                     residuals)
        #
        # Level 2 -- might iterate multiple times
        #
        # this is the final common space
        self.commonspace = self._level2(datasets, lvl1_projdata, mappers,
                                        residuals)


    def __call__(self, datasets):
        """Derive a common feature space from a series of datasets.

        Parameters
        ----------
        datasets : sequence of datasets

        Returns
        -------
        A list of trained Mappers matching the number of input datasets.
        """
        if self.commonspace is None:
            self.train(datasets)

        # place datasets into a copy of the list since items
        # will be reassigned
        datasets = list(datasets)

        params = self.params            # for quicker access ;)
        alpha = params.alpha             # for letting me be lazy ;)
        if params.zscore_all:
            if __debug__:
                debug('HPAL', "Z-scoring all datasets")
            # zscore them once while storing corresponding ZScoreMapper's
            # so we can assemble a comprehensive mapper at the end
            # (together with procrustes)
            zmappers = []
            for ids in xrange(len(datasets)):
                zmapper = ZScoreMapper(chunks_attr=None)
                zmappers.append(zmapper)
                zmapper.train(datasets[ids])
                datasets[ids] = zmapper.forward(datasets[ids])

        if alpha < 1:
            datasets, wmappers = self._regularize(datasets, alpha)

        #
        # Level 3 -- final, from-scratch, alignment to final common space
        #
        mappers = self._level3(datasets)
        # return trained mappers for projection from all datasets into the
        # common space
        if params.zscore_all:
            # We need to construct new mappers which would chain
            # zscore and then final transformation
            if params.alpha < 1:
                return [ChainMapper([zm, wm, m]) for zm, wm, m in zip(zmappers, wmappers, mappers)]
            else:
                return [ChainMapper([zm, m]) for zm, m in zip(zmappers, mappers)]
        else:
            if params.alpha < 1:
                return [ChainMapper([wm, m]) for wm, m in zip(wmappers, mappers)]
            else:
                return mappers


    def _regularize(self, datasets, alpha):
        if __debug__:
            debug('HPAL', "Using regularized hyperalignment with alpha of %d"
                    % alpha)
        wmappers = []
        for ids in xrange(len(datasets)):
            U, S, Vh = np.linalg.svd(datasets[ids])
            S = 1/np.sqrt( (1-alpha)*np.square(S) + alpha )
            S.resize(len(Vh))
            S = np.matrix(np.diag(S))
            W = np.matrix(Vh.T)*S*np.matrix(Vh)
            wmapper = StaticProjectionMapper(proj=W)
            wmappers.append(wmapper)
            datasets[ids] = wmapper.forward(datasets[ids])
        return datasets, wmappers


    def _level1(self, datasets, commonspace, ref_ds, mappers, residuals):
        params = self.params            # for quicker access ;)
        data_mapped = [ds.samples for ds in datasets]
        for i, (m, ds_new) in enumerate(zip(mappers, datasets)):
            if __debug__:
                debug('HPAL_', "Level 1: ds #%i" % i)
            if i == ref_ds:
                continue
            # assign common space to ``space`` of the mapper, because this is
            # where it will be looking for it
            ds_new.sa[m.get_space()] = commonspace
            # find transformation of this dataset into the current common space
            m.train(ds_new)
            # remove common space attribute again to save on memory when the
            # common space is updated for the next iteration
            del ds_new.sa[m.get_space()]
            # project this dataset into the current common space
            ds_ = m.forward(ds_new.samples)
            if params.zscore_common:
                zscore(ds_, chunks_attr=None)
            # replace original dataset with mapped one -- only the reference
            # dataset will remain unchanged
            data_mapped[i] = ds_

            # compute first-level residuals wrt to the initial common space
            if residuals is not None:
                residuals[0, i] = np.linalg.norm(ds_ - commonspace)

            # Update the common space. This is an incremental update after
            # processing each 1st-level dataset. Maybe there should be a flag
            # to make a batch update after processing all 1st-level datasets
            # to an identical 1st-level common space
            # TODO: make just a function so we dont' waste space
            commonspace = params.combiner1(ds_, commonspace)
            if params.zscore_common:
                zscore(commonspace, chunks_attr=None)
        return data_mapped


    def _level2(self, datasets, lvl1_data, mappers, residuals):
        params = self.params            # for quicker access ;)
        data_mapped = lvl1_data
        # aggregate all processed 1st-level datasets into a new 2nd-level
        # common space
        commonspace = params.combiner2(data_mapped)

        # XXX Why is this commented out? Who knows what combiner2 is doing and
        # whether it changes the distribution of the data
        #if params.zscore_common:
        #zscore(commonspace, chunks_attr=None)

        ndatasets = len(datasets)
        for loop in xrange(params.level2_niter):
            # 2nd-level alignment starts from the original/unprojected datasets
            # again
            for i, (m, ds_new) in enumerate(zip(mappers, datasets)):
                if __debug__:
                    debug('HPAL_', "Level 2 (%i-th iteration): ds #%i" % (loop, i))

                # Optimization speed up heuristic
                # Slightly modify the common space towards other feature
                # spaces and reduce influence of this feature space for the
                # to-be-computed projection
                temp_commonspace = (commonspace * ndatasets - data_mapped[i]) \
                                    / (ndatasets - 1)

                if params.zscore_common:
                    zscore(temp_commonspace, chunks_attr=None)
                # assign current common space
                ds_new.sa[m.get_space()] = temp_commonspace
                # retrain the mapper for this dataset
                m.train(ds_new)
                # remove common space attribute again to save on memory when the
                # common space is updated for the next iteration
                del ds_new.sa[m.get_space()]
                # obtain the 2nd-level projection
                ds_ =  m.forward(ds_new.samples)
                if params.zscore_common:
                    zscore(ds_, chunks_attr=None)
                # store for 2nd-level combiner
                data_mapped[i] = ds_
                # compute residuals
                if residuals is not None:
                    residuals[1+loop, i] = np.linalg.norm(ds_ - commonspace)

            commonspace = params.combiner2(data_mapped)

        # and again
        if params.zscore_common:
            zscore(commonspace, chunks_attr=None)

        # return the final common space
        return commonspace


    def _level3(self, datasets):
        params = self.params            # for quicker access ;)
        # create a mapper per dataset
        mappers = [deepcopy(params.alignment) for ds in datasets]

        # key different from level-2; the common space is uniform
        #temp_commonspace = commonspace

        residuals = None
        if self.ca['residual_errors'].enabled:
            residuals = np.zeros((1, len(datasets)))
            self.ca.residual_errors = Dataset(samples=residuals)

        # start from original input datasets again
        for i, (m, ds_new) in enumerate(zip(mappers, datasets)):
            if __debug__:
                debug('HPAL_', "Level 3: ds #%i" % i)

            # retrain mapper on final common space
            ds_new.sa[m.get_space()] = self.commonspace
            m.train(ds_new)
            # remove common space attribute again to save on memory
            del ds_new.sa[m.get_space()]

            if residuals is not None:
                # obtain final projection
                data_mapped = m.forward(ds_new.samples)
                residuals[0, i] = np.linalg.norm(data_mapped - self.commonspace)

        return mappers


########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Base classes for Anatomy atlases support

TODOs:
======

 * major optimization. Now code is sloppy and slow -- plenty of checks etc

Module Organization
===================
mvpa2.atlases.base module contains support for various atlases

:group Base: BaseAtlas XMLBasedAtlas Label Level LabelsLevel
:group Talairach: PyMVPAAtlas LabelsAtlas ReferencesAtlas
:group Exceptions: XMLAtlasException

"""

import os.path as osp
from mvpa2.base import externals

if externals.exists('lxml', raise_=True, exception=ImportError):
    from lxml import etree, objectify

from mvpa2.base.dochelpers import enhanced_doc_string

import re
import numpy as np
from numpy.linalg import norm

from mvpa2.atlases.transformation import SpaceTransformation, Linear
from mvpa2.misc.support import reuse_absolute_path

if externals.exists('nibabel', raise_=True):
    import nibabel as nb

from mvpa2.base import warning
if __debug__:
    from mvpa2.base import debug


def check_range(coord, range):
    """Check if coordinates are within range (0,0,0) - (range)

    Returns
    -------
    bool
      Success status
    """
    # TODO: optimize
    if len(coord) != len(range):
        raise ValueError("Provided coordinate %r and given range %r" % \
                         (coord, range) + \
                         " have different dimensionality"
                         )
    for c, r in zip(coord, range):
        if c < 0 or c >= r:
            return False
    return True

#
# Base classes
#

class XMLAtlasException(Exception):
    """Exception to be thrown if smth goes wrong dealing with XML based atlas
    """
    pass

class BaseAtlas(object):
    """Base class for the atlases.
    """
    pass



class XMLBasedAtlas(BaseAtlas):
    """Base class for atlases using XML as the definition backend
    """

    def __init__(self, filename=None,
                 resolution=None, image_file=None,
                 coordT=None, default_levels=None, load_maps=True):
        """
        Parameters
        ----------
        filename : str
          Filename for the xml definition of the atlas
        resolution : None or float
          Some atlases link to multiple images at different
          resolutions. if None -- best resolution is selected
          using 0th dimension resolution
        image_file : None or str
          If None, overrides filename for the used imagefile, so
          it could load a custom (re-registered) atlas maps
        coordT
          Optional transformation to apply first
        default_levels : None or slice or list of int
          What levels by default to operate on
        load_maps : bool
          Load spatial maps for the atlas.
        """
        BaseAtlas.__init__(self)

        self.__atlas = None

        self._image_file = None
        self._filename = filename
        # TODO: think about more generalizable way?
        self._resolution = resolution
        self._force_image_file = image_file
        self.default_levels = default_levels

        if filename:
            self.load_atlas(filename)

        # common sanity checks
        if not self._check_version(self.version):
            raise IOError(
                "Version %s is not recognized to be native to class %s" % \
                (self.version, self.__name__))

        if not set(['header', 'data']) \
               == set([i.tag for i in self.getchildren()]):
            raise IOError("No header or data were defined in %s" % filename)

        header = self.header
        headerChildrenTags = XMLBasedAtlas._children_tags(header)
        if not ('images' in headerChildrenTags) or \
           not ('imagefile' in XMLBasedAtlas._children_tags(header.images)):
            raise XMLAtlasException(
                "Atlas requires image/imagefile header fields")

        # Load and post-process images
        self._image = None
        if load_maps:
            self._load_images()
            if self._image is not None:
                # Get extent and voxel dimensions, limiting to 3D
                self._extent = np.abs(np.asanyarray(self._image.shape[:3]))
                self._voxdim = np.asanyarray(self._image.get_header().get_zooms()[:3])
                self.relativeToOrigin = True
        # Assign transformation to get into voxel coordinates,
        # spaceT will be set accordingly
        self.set_coordT(coordT)
        self._load_metadata()


    def _check_range(self, c):
        """ check and adjust the voxel coordinates"""
        # check range
        if __debug__:
            debug('ATL__', "Querying for voxel %r" % (c,))
        if not check_range(c, self.extent):
            warning("Coordinates %r are not within the extent %r." \
                    " Reseting to (0,0,0)" % (c, self.extent))
            # assume that voxel [0,0,0] is blank, i.e. carries
            # no labels which could possibly result in evil outcome
            c = [0]*3
        return c


    @staticmethod
    def _check_version(version):
        """To be overriden in the derived classes. By default anything is good"""
        return True

    def _load_images(self):
        """To be overriden in the derived classes. By default does nothing"""
        pass

    def _load_metadata(self):
        """To be overriden in the derived classes. By default does nothing"""
        pass

    def load_atlas(self, filename):
        """Load atlas from a file
        """
        if __debug__:
            debug('ATL_', "Loading atlas definition xml file " + filename)
        # Create objectify parser first
        parser = etree.XMLParser(remove_blank_text=True)
        lookup = objectify.ObjectifyElementClassLookup()
        parser.setElementClassLookup(lookup)
        try:
            self.__atlas = etree.parse(filename, parser).getroot()
        except IOError:
            raise XMLAtlasException("Failed to load XML file %s" % filename)

    @property
    def version(self):
        if not self.__atlas is None \
               and ("version" in self.__atlas.attrib.keys()):
            return self.__atlas.get("version")
        else:
            return None

    @staticmethod
    def _children_tags(root):
        """Little helper to return tags for the children of the node
        """
        return [i.tag for i in root.getchildren()]


    def __getattr__(self, attr):
        """
        Lazy way to provide access to the definitions in the atlas
        """
        if not self.__atlas is None:
            return getattr(self.__atlas, attr)
        else:
            raise XMLAtlasException(
                "Atlas in " + self.__name__ + " was not read yet")


    def set_coordT(self, coordT):
        """Set coordT transformation.

        spaceT needs to be adjusted since we glob those two
        transformations together
        """
        self._coordT = coordT           # lets store for debugging etc
        if self._image is not None:
            # Combine with the image's qform
            coordT = Linear(np.linalg.inv(self._image.get_header().get_qform()),
                            previous=coordT)
        self._spaceT = SpaceTransformation(
            previous=coordT, to_real_space=False)


    ##REF: Name was automagically refactored
    def label_point(self, coord, levels=None):
        """Return labels for the given spatial point at specified levels

        Function takes care about first transforming the point into
        the voxel space

        Parameters
        ----------
        coord : tuple
          Coordinates of the point (xyz)
        levels : None or list of int
          At what levels to return the results
        """
        coord_ = np.asarray(coord)          # or we would alter what should be constant
        #if not isinstance(coord, np.numpy):
        #c = self.getVolumeCoordinate(coord)
        #c = self.spaceT.to_voxel_space(coord_)
        #if self.coordT:
        #   coord_t = self.coordT[coord_]
        #else:
        #   coord_t = coord_

        c = self.spaceT(coord_)

        result = self.label_voxel(c, levels)
        result['coord_queried'] = coord
        #result['coord_trans'] = coord_t
        result['voxel_atlas'] = c
        return result


    def levels_listing(self):
        lkeys = range(self.nlevels)
        return '\n'.join(['%d: ' % k + str(self._levels[k])
                          for k in lkeys])


    def _get_selected_levels(self, levels=None):
        """Helper to provide list of levels to operate on

        Depends on given `levels` as well as self.default_levels
        """
        if levels is None:
            levels = [ i for i in xrange(self.nlevels) ]
        elif (isinstance(levels, slice)):
            # levels are given as a range
            if levels.step: step = levels.step
            else: step = 1

            if levels.start: start = levels.start
            else: start = 0

            if levels.stop: stop = levels.stop
            else: stop = self.nlevels

            levels = [ i for i in xrange(start, stop, step) ]

        elif isinstance(levels, list) or isinstance(levels, tuple):
            # levels given as list
            levels = list(levels)

        elif isinstance(levels, int):
            levels = [ levels ]

        else:
            raise TypeError('Given levels "%s" are of unsupported type' % `levels`)

        selected_levels = levels
        # test given values
        levels = self.levels
        for level in selected_levels:
            if not level in levels:
                raise ValueError, \
                      "Level %r is not known (out of range?). Known levels are:\n%s" \
                      % (level, self.levels_listing())

        return selected_levels


    def query(self, index, query_voxel=False):
        """Generic query method.

        Use shortcuts `__getitem__` for querying by voxel indices and
        `__call__` for querying by space coordinates.

        Parameters
        ----------
        index : tuple or list
          Arguments of the query, such as coordinates and optionally
          levels
        query_voxel : bool
          Either at the end query a voxel indexes or point coordinates

        Allows to access the elements via simple indexing. Examples::

            print atlas[ 0, -7, 20, [1,2,3] ]
            print atlas[ (0, -7, 20), 1:2 ]
            print atlas[ (0, -7, 20) ]
            print atlas[ (0, -7, 20), : ]
        """
        if len(index) in [2, 4]:
            levels_slice = index[-1]
        else:
            if self.default_levels is None:
                levels_slice = slice(None, None, None)
            else:
                levels_slice = self.default_levels

        levels = self._get_selected_levels(levels=levels_slice)

        if len(index) in [3, 4]:
            # we got coordinates 1 by 1 + may be a level
            coord = index[0:3]

        elif len(index) in [1, 2]:
            coord = index[0]
            if isinstance(coord, list) or isinstance(coord, tuple):
                if len(coord) != 3:
                    raise TypeError("Given coordinates must be in 3D")
            else:
                raise TypeError("Given coordinates must be a list or a tuple")

        else:
            raise TypeError("Unknown shape of parameters `%s`" % `index`)

        if query_voxel:
            return self.label_voxel(coord, levels)
        else:
            return self.label_point(coord, levels)

    #
    ## Shortcuts for `query`
    #
    def __getitem__(self, index):
        """Query atlas with voxel indexes

        Examples
        --------

        ::
            print atlas[ 0, -7, 20, [1,2,3] ]
            print atlas[ (0, -7, 20), 1:2 ]
            print atlas[ (0, -7, 20) ]
            print atlas[ (0, -7, 20), : ]
        """
        return self.query(index, True)

    def __call__(self, *args):
        return self.query(args, False)

    # REDO in some sane fashion so referenceatlas returns levels for the base
    def _get_levels(self):
        return self._get_levels_virtual()

    ##REF: Name was automagically refactored
    def _get_levels_virtual(self):
        return self._levels

    levels = property(fget=_get_levels)


    resolution = property(fget=lambda self:self._resolution)
    origin = property(fget=lambda self:self._origin)
    extent = property(fget=lambda self:self._extent)
    voxdim = property(fget=lambda self:self._voxdim)
    spaceT = property(fget=lambda self:self._spaceT)
    coordT = property(fget=lambda self:self._spaceT,
                      fset=set_coordT)

class Label(object):
    """Represents a label. Just to bring all relevant information together
    """
    def __init__ (self, text, abbr=None, coord=(None, None, None),
                  count=0, index=0):
        """
        Parameters
        ----------
        text : str
          Fullname of the label
        abbr : str, optional
          Abbreviated name.
        coord : tuple of float, optional
          Coordinates.
        count : int, optional
          Count of those labels in the atlas
        """
        self.text = text.strip()
        if abbr is not None:
            abbr = abbr.strip()
        self.coord = coord
        self.count = count
        self.__abbr = abbr
        self.__index = int(index)


    @property
    def index(self):
        return self.__index

    def __repr__(self):
        return "Label(%r%s, coord=%r, count=%r, index=%r)" % \
               (self.text,
                (', abbr=%s' % repr(self.__abbr), '')[int(self.__abbr is None)],
                self.coord, self.count, self.__index)

    def __str__(self):
        return self.text

    @staticmethod
    def from_xml(Elabel):
        """Create label from an XML node
        """
        kwargs = {}
        if Elabel.attrib.has_key('x'):
            kwargs['coord'] = ( Elabel.attrib.get('x'),
                                Elabel.attrib.get('y'),
                                Elabel.attrib.get('z') )
        for l in ('count', 'abbr', 'index'):
            if Elabel.attrib.has_key(l):
                kwargs[l] = Elabel.attrib.get(l)
        return Label(Elabel.text.strip(), **kwargs)

    @property
    def abbr(self):
        """Returns abbreviated version if such is available
        """
        if self.__abbr in [None, ""]:
            return self.text
        else:
            return self.__abbr


class Level(object):
    """Represents a level. Just to bring all relevant information together
    """
    def __init__ (self, description):
        self.description = description
        self._type = "Base"

    def __repr__(self):
        return "%s Level: %s" % \
               (self.level_type, self.description)

    def __str__(self):
        return self.description

    @staticmethod
    ##REF: Name was automagically refactored
    def from_xml(Elevel, level_type=None):
        """Simple factory of levels
        """
        if level_type is None:
            if not Elevel.attrib.has_key("type"):
                raise XMLAtlasException("Level must have type specified. Level: " + `Elevel`)
            level_type = Elevel.get("type")

        levelTypes = { 'label':     LabelsLevel,
                       'reference': ReferencesLevel }

        if levelTypes.has_key(level_type):
            return levelTypes[level_type].from_xml(Elevel)
        else:
            raise XMLAtlasException("Unknown level type " + level_type)

    level_type = property(lambda self: self._type)


class LabelsLevel(Level):
    """Level of labels.

	XXX extend
    """
    def __init__ (self, description, index=None, labels=[]):
        Level.__init__(self, description)
        self.__index = index
        self.__labels = labels
        self._type = "Labels"

    def __repr__(self):
        return Level.__repr__(self) + " [%d] " % \
               (self.__index)

    @staticmethod
    ##REF: Name was automagically refactored
    def from_xml(Elevel, levelIndex=[0]):
        # XXX this is just for label type of level. For distance we need to ...
        # we need to assure the right indexing

        index = 0
        if Elevel.attrib.has_key("index"):
            index = int(Elevel.get("index"))

        maxindex = max([int(i.get('index')) \
                        for i in Elevel.label[:]])
        labels = [ None for i in xrange(maxindex+1) ]
        for label in Elevel.label[:]:
            labels[ int(label.get('index')) ] = Label.from_xml(label)

        levelIndex[0] = max(levelIndex[0], index) + 1 # assign next one

        return LabelsLevel(Elevel.get('description'),
                           index,
                           labels)

    @property
    def index(self): return self.__index

    @property
    def labels(self): return self.__labels

    def __getitem__(self, index):
        return self.__labels[index]

    def find(self, target, unique=True):
        """Return labels descr of which matches the string

        Parameters
        ----------
        target : str or re._pattern_type
          Substring in abbreviation to be searched for, or compiled
          regular expression to be searched or matched if anchored.
        unique : bool, optional
          If True, raise exception if none or more than 1 was found. Return
          just a single item if found (not list).
        """
        if isinstance(target, re._pattern_type):
            res = [l for l in self.__labels if target.search(l.abbr)]
        else:
            res = [l for l in self.__labels if target in l.abbr]

        if unique:
            if len(res) != 1:
                raise ValueError, "Got %d matches whenever just 1 was " \
                      "looked for (target was %s)." % (len(res), target)
            return res[0]
        else:
            return res


class ReferencesLevel(Level):
    """Level which carries reference points
    """
    def __init__ (self, description, indexes=[]):
        Level.__init__(self, description)
        self.__indexes = indexes
        self._type = "References"

    @staticmethod
    ##REF: Name was automagically refactored
    def from_xml(Elevel):
        # XXX should probably do the same for the others?
        requiredAttrs = ['x', 'y', 'z', 'type', 'description']
        if not set(requiredAttrs) == set(Elevel.attrib.keys()):
            raise XMLAtlasException("ReferencesLevel has to have " +
                                    "following attributes defined " +
                                    `requiredAttrs`)

        indexes = tuple(int(Elevel.get(a)) for a in ('x', 'y', 'z'))

        return ReferencesLevel(Elevel.get('description'),
                               indexes)

    @property
    def indexes(self):
        return self.__indexes


class PyMVPAAtlas(XMLBasedAtlas):
    """Base class for PyMVPA atlases, such as LabelsAtlas and ReferenceAtlas
    """

    source = 'PyMVPA'

    def __init__(self, *args, **kwargs):
        XMLBasedAtlas.__init__(self, *args, **kwargs)

        # sanity checks
        header = self.header
        headerChildrenTags = XMLBasedAtlas._children_tags(header)
        if not ('space' in headerChildrenTags) or \
           not ('space-flavor' in headerChildrenTags):
            raise XMLAtlasException("PyMVPA Atlas requires specification of" +
                                    " the space in which atlas resides")

        self.__space = header.space.text
        self.__spaceFlavor = header['space-flavor'].text


    __doc__ = enhanced_doc_string('PyMVPAAtlas', locals(), XMLBasedAtlas)


    ##REF: Name was automagically refactored
    def _load_images(self):
        # shortcut
        imagefile = self.header.images.imagefile
        #self.nlevels = len(self._levels_by_id)

        # Set offset if defined in XML file
        # XXX: should just take one from the qoffset... now that one is
        #       defined... this origin might be misleading actually
        self._origin = np.array( (0, 0, 0) )
        if imagefile.attrib.has_key('offset'):
            self._origin = np.array( [int(x) for x in
                                     imagefile.get('offset').split(',')] )

        # Load the image file which has labels
        if self._force_image_file is not None:
            imagefilename = self._force_image_file
        else:
            imagefilename = imagefile.text
        imagefilename = reuse_absolute_path(self._filename, imagefilename)

        try:
            self._image = None
            for ext in ['', '.nii.gz']:
                try:
                    self._image  = nb.load(imagefilename + ext)
                    break
                except Exception, e:
                    pass
            if self._image is None:
                raise e
        except RuntimeError, e:
            raise RuntimeError, \
                  " Cannot open file %s due to %s" % (imagefilename, e)

        self._data = self._image.get_data()
        # we get the data as x,y,z[,t] but we want to have the time axis first
        # if any
        if len(self._data.shape) == 4:
            self._data = np.rollaxis(self._data, -1)

        # remove bogus dimensions on top of 4th
        if len(self._data.shape[0:-4]) > 0:
            bogus_dims = self._data.shape[0:-4]
            if max(bogus_dims)>1:
                raise RuntimeError, "Atlas %s has more than 4 of non-singular" \
                      "dimensions" % imagefilename
            new_shape = self._data.shape[-4:]
            self._data.reshape(new_shape)

        #if self._image.extent[3] != self.nlevels:
        #   raise XMLAtlasException("Atlas %s has %d levels defined whenever %s has %d volumes" % \
        #                           ( filename, self.nlevels, imagefilename, self._image.extent[3] ))


    ##REF: Name was automagically refactored
    def _load_metadata(self):
        # Load levels
        self._levels = {}
        # preprocess labels for different levels
        self._Nlevels = 0
        index_incr = 0
        for index, child in enumerate(self.data.getchildren()):
            if child.tag == 'level':
                level = Level.from_xml(child)
                self._levels[level.description] = level
                if hasattr(level, 'index'):
                    index = level.index
                else:
                    # to avoid collision if some levels do
                    # have indexes
                    while index_incr in self._levels:
                        index_incr += 1
                    index, index_incr = index_incr, index_incr+1
                self._levels[index] = level
            else:
                raise XMLAtlasException(
                    "Unknown child '%s' within data" % child.tag)
            self._Nlevels += 1


    ##REF: Name was automagically refactored
    def _get_nlevels_virtual(self):
        return self._Nlevels

    ##REF: Name was automagically refactored
    def _get_nlevels(self):
        return self._get_nlevels_virtual()

    @staticmethod
    ##REF: Name was automagically refactored
    def _check_version(version):
        # For compatibility lets support "RUMBA" atlases
        return version.startswith("pymvpa-") or version.startswith("rumba-")


    space = property(fget=lambda self:self.__space)
    space_flavor = property(fget=lambda self:self.__spaceFlavor)
    nlevels = property(fget=_get_nlevels)


class LabelsAtlas(PyMVPAAtlas):
    """
    Atlas which provides labels for the given coordinate
    """

    ##REF: Name was automagically refactored
    def label_voxel(self, c, levels=None):
        """
        Return labels for the given voxel at specified levels specified by index
        """
        levels = self._get_selected_levels(levels=levels)

        result = {'voxel_queried' : c}

        # check range
        c = self._check_range(c)

        resultLevels = []
        for level in levels:
            if self._levels.has_key(level):
                level_ = self._levels[ level ]
            else:
                raise IndexError(
                    "Unknown index or description for level %d" % level)

            resultIndex =  int(self._data[ level_.index, \
                                            c[0], c[1], c[2] ])

            resultLevels += [ {'index': level_.index,
                               'id': level_.description,
                               'label' : level_[ resultIndex ]} ]

        result['labels'] = resultLevels
        return result

    __doc__ = enhanced_doc_string('LabelsAtlas', locals(), PyMVPAAtlas)



class ReferencesAtlas(PyMVPAAtlas):
    """
    Atlas which provides references to the other atlases.

    Example: the atlas which has references to the closest points
    (closest Gray, etc) in another atlas.
    """

    def __init__(self, distance=0, reference_level=None, *args, **kwargs):
        """Initialize `ReferencesAtlas`
        """
        PyMVPAAtlas.__init__(self, *args, **kwargs)
        # sanity checks
        if not ('reference-atlas' in XMLBasedAtlas._children_tags(self.header)):
            raise XMLAtlasException(
                "ReferencesAtlas must refer to a some other atlas")

        referenceAtlasName = self.header["reference-atlas"].text

        # uff -- another evil import but we better use the factory method
        from mvpa2.atlases.warehouse import Atlas
        self.__referenceAtlas = Atlas(filename=reuse_absolute_path(
            self._filename, referenceAtlasName))

        if self.__referenceAtlas.space != self.space or \
           self.__referenceAtlas.space_flavor != self.space_flavor:
            raise XMLAtlasException(
                "Reference and original atlases should be in the same space")

        self.__referenceLevel = None    # pylint shut up
        if reference_level is not None:
            self.set_reference_level(reference_level)
        self.set_distance(distance)

    __doc__ = enhanced_doc_string('ReferencesAtlas', locals(), PyMVPAAtlas)

    # number of levels must be of the referenced atlas due to
    # handling of that in __getitem__
    #nlevels = property(fget=lambda self:self.__referenceAtlas.nlevels)
    ##REF: Name was automagically refactored
    def _get_nlevels_virtual(self):
        return self.__referenceAtlas.nlevels


    ##REF: Name was automagically refactored
    def set_reference_level(self, level):
        """
        Set the level which will be queried
        """
        if self._levels.has_key(level):
            self.__referenceLevel = self._levels[level]
        else:
            raise IndexError, \
                  "Unknown reference level %r. " % level + \
                  "Known are %r" % (self._levels.keys(), )


    ##REF: Name was automagically refactored
    def label_voxel(self, c, levels = None):

        if self.__referenceLevel is None:
            warning("You did not provide what level to use "
                    "for reference. Assigning 0th level -- '%s'"
                    % (self._levels[0],))
            self.set_reference_level(0)
            # return self.__referenceAtlas.label_voxel(c, levels)

        c = self._check_range(c)

        # obtain coordinates of the closest voxel
        cref = self._data[ self.__referenceLevel.indexes, c[0], c[1], c[2] ]
        dist = norm( (cref - c) * self.voxdim )
        if __debug__:
            debug('ATL__', "Closest referenced point for %r is "
                  "%r at distance %3.2f" % (c, cref, dist))
        if (self.distance - dist) >= 1e-3: # neglect everything smaller
            result = self.__referenceAtlas.label_voxel(cref, levels)
            result['voxel_referenced'] = c
            result['distance'] = dist
        else:
            result = self.__referenceAtlas.label_voxel(c, levels)
            if __debug__:
                debug('ATL__', "Closest referenced point is "
                      "further than desired distance %.2f" % self.distance)
            result['voxel_referenced'] = None
            result['distance'] = 0
        return result


    ##REF: Name was automagically refactored
    def levels_listing(self):
        return self.__referenceAtlas.levels_listing()

    ##REF: Name was automagically refactored
    def _get_levels_virtual(self):
        return self.__referenceAtlas.levels

    ##REF: Name was automagically refactored
    def set_distance(self, distance):
        """Set desired maximal distance for the reference
        """
        if distance < 0:
            raise ValueError("Distance should not be negative. "
                             " Thus '%f' is not a legal value" % distance)
        if __debug__:
            debug('ATL__',
                  "Setting maximal distance for queries to be %d" % distance)
        self.__distance = distance

    distance = property(fget=lambda self:self.__distance, fset=set_distance)
    reference_level = property(fget=lambda self:self.__referenceLevel, fset=set_reference_level)


########NEW FILE########
__FILENAME__ = fsl
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""FSL atlases interfaces

"""

from mvpa2.base import externals

if externals.exists('nibabel', raise_=True):
    import nibabel as nb

import re
import os.path
import numpy as np

from mvpa2.misc.support import reuse_absolute_path
from mvpa2.base.dochelpers import enhanced_doc_string

from mvpa2.atlases.base import XMLBasedAtlas, LabelsLevel

if __debug__:
	from mvpa2.base import debug

__all__ = [ "FSLAtlas", "FSLLabelsAtlas", "FSLProbabilisticAtlas" ]

#
# Atlases from FSL
#

class FSLAtlas(XMLBasedAtlas):
    """Base class for FSL atlases

    """
    source = 'FSL'


    def __init__(self, *args, **kwargs):
        """
        """
        XMLBasedAtlas.__init__(self, *args, **kwargs)
        self.space = 'MNI'


    __doc__ = enhanced_doc_string('FSLAtlas', locals(), XMLBasedAtlas)


    ##REF: Name was automagically refactored
    def _load_images(self):
        resolution = self._resolution
        header = self.header
        images = header.images
        # Load present images
        # XXX might be refactored to avoid duplication of
        #     effort with PyMVPAAtlas
        ni_image = None
        resolutions = []
        if self._force_image_file is None:
            imagefile_candidates = [
                reuse_absolute_path(self._filename, i.imagefile.text, force=True)
                for i in images]
        else:
            imagefile_candidates = [self._force_image_file]

        for imagefilename in imagefile_candidates:
            try:
                if not os.path.exists(imagefilename):
                    # try with extension if filename doesn't exist
                    imagefilename += '.nii.gz'
                ni_image_  = nb.load(imagefilename)
            except RuntimeError, e:
                raise RuntimeError, " Cannot open file " + imagefilename

            resolution_ = ni_image_.get_header().get_zooms()[0]
            if resolution is None:
                # select this one if the best
                if ni_image is None or \
                       resolution_ < ni_image.get_header().get_zooms()[0]:
                    ni_image = ni_image_
                    self._image_file = imagefilename
            else:
                if resolution_ == resolution:
                    ni_image = ni_image_
                    self._image_file = imagefilename
                    break
                else:
                    resolutions += [resolution_]
            # TODO: also make use of summaryimagefile may be?

        if ni_image is None:
            msg = "Could not find an appropriate atlas among %d atlases." \
                  % len(imagefile_candidates)
            if resolution is not None:
                msg += " Atlases had resolutions %s" % \
                      (resolutions,)
            raise RuntimeError, msg
        if __debug__:
            debug('ATL__', "Loading atlas data from %s" % self._image_file)
        self._image = ni_image
        self._resolution = ni_image.get_header().get_zooms()[0]
        self._origin = np.abs(ni_image.get_header().get_qform()[:3,3])  # XXX

        self._data   = self._image.get_data()
        if len(self._data.shape) == 4:
            # want to have volume axis first
            self._data = np.rollaxis(self._data, -1)


    def _load_metadata(self):
        """   """
        # Load levels
        self._levels = {}
        # preprocess labels for different levels
        self.nlevels = 1
        #level = Level.from_xml(self.data, level_type='label')
        level = LabelsLevel.from_xml(self.data)#, level_type='label')
        level.description = self.header.name.text
        self._levels = {0: level}
        #for index, child in enumerate(self.data.getchildren()):
        #   if child.tag == 'level':
        #       level = Level.from_xml(child)
        #       self._levels[level.description] = level
        #       try:
        #           self._levels[level.index] = level
        #       except:
        #           pass
        #   else:
        #       raise XMLAtlasException("Unknown child '%s' within data" % child.tag)
        #   self.nlevels += 1
        #pass


    @staticmethod
    ##REF: Name was automagically refactored
    def _check_version(version):
        return re.search('^[0-9]+\.[0-9]', version) is not None


class FSLProbabilisticAtlas(FSLAtlas):
    """Probabilistic FSL atlases
    """

    def __init__(self, thr=0.0, strategy='all', sort=True, *args, **kwargs):
        """

        Parameters
        ----------
        thr : float
          Value to threshold at
        strategy : str
          Possible values
            all - all entries above thr
            max - entry with maximal value
        sort : bool
          Either to sort entries for 'all' strategy according to
          probability
        """

        FSLAtlas.__init__(self, *args, **kwargs)
        self.thr = thr
        self.strategy = strategy
        self.sort = sort

    __doc__ = enhanced_doc_string('FSLProbabilisticAtlas', locals(), FSLAtlas)

    ##REF: Name was automagically refactored
    def label_voxel(self, c, levels=None):
        """Return labels for the voxel

        Parameters
        ----------
        c : tuple of coordinates (xyz)
        - levels : just for API consistency (heh heh). Must be 0 for FSL atlases
        """

        if levels is not None and not (levels in [0, [0], (0,)]):
            raise ValueError, \
                  "I guess we don't support levels other than 0 in FSL atlas." \
                  " Got levels=%s" % (levels,)
        # check range
        c = self._check_range(c)

        # XXX think -- may be we should better assign each map to a
        # different level
        level = 0
        resultLabels = []
        for index, area in enumerate(self._levels[level]):
            prob =  int(self._data[index, c[0], c[1], c[2]])
            if prob > self.thr:
                resultLabels += [dict(index=index,
                                      #id=
                                      label=area.text,
                                      prob=prob)]

        if self.sort or self.strategy == 'max':
            resultLabels.sort(cmp=lambda x,y: cmp(x['prob'], y['prob']),
                              reverse=True)

        if self.strategy == 'max':
            resultLabels = resultLabels[:1]
        elif self.strategy == 'all':
            pass
        else:
            raise ValueError, 'Unknown strategy %s' % self.strategy

        result = {'voxel_queried' : c,
                  # in the list since we have only single level but
                  # with multiple entries
                  'labels': [resultLabels]}

        return result

    def find(self, *args, **kwargs):
        """Just a shortcut to the only level.

        See :class:`~mvpa2.atlases.base.Level.find` for more info
        """
        return self.levels[0].find(*args, **kwargs)

    def get_map(self, target, strategy='unique', axes_order='xyz'):
        """Return a probability map as an array

        Parameters
        ----------
        target : int or str or re._pattern_type
          If int, map for given index is returned. Otherwise, .find is called
          with ``unique=True`` to find matching area
        strategy : str in ('unique', 'max')
          If 'unique', then if multiple areas match, exception would be raised.
          In case of 'max', each voxel would get maximal value of probabilities
          from all matching areas
        axes_order : str in ('xyz', 'zyx')
          In what order axes of the returned array should follow.
        """
        if isinstance(target, int):
            res = self._data[target]
            # since we no longer support pynifti all is XYZ
            if axes_order == 'xyz':
                return res
            elif axes_order == 'zyx':
                return res.T
            else:
                raise ValueError, \
                      "Unknown axes_order=%r provided" % (axes_order,)
        else:
            lev = self.levels[0]       # we have just 1 here
            if strategy == 'unique':
                return self.get_map(lev.find(target, unique=True).index,
                                    axes_order=axes_order)
            else:
                maps_dict = self.get_maps(target, axes_order=axes_order)
                maps = np.array(maps_dict.values())
                return np.max(maps, axis=0)

    def get_maps(self, target, axes_order='xyz', key_attr=None,
                 overlaps=None):
        """Return a dictionary of probability maps for the target

        Each key is a `Label` instance, and value is the probability map

        Parameters
        ----------
        target : str or re._pattern_type
          .find is called with a target and unique=False to find all matches
        axes_order : str in ('xyz', 'zyx')
          In what order axes of the returned array should follow.
        key_attr : None or str
          What to use for the keys of the dictionary.  If None,
          `Label` instance would be used as a key.  If some attribute
          provided (e.g. 'text', 'abbr', 'index'), corresponding
          attribute of the `Label` instance would be taken as a key.
        overlaps : None or {'max'}
          How to treat overlaps in maps.  If None, nothing is done and maps
          might have overlaps.  If 'max', then maps would not overlap and
          competing maps will be resolved based on maximal value (e.g. if
          maps contain probabilities).
        """
        lev = self.levels[0]       # we have just 1 here
        if key_attr is None:
            key_gen = lambda x: x
        else:
            key_gen = lambda x: getattr(x, key_attr)

        res = [[key_gen(l),
                self.get_map(l.index, axes_order=axes_order)]
               for l in lev.find(target, unique=False)]

        if overlaps == 'max':
            # not efficient since it places all maps back into a single
            # ndarray... but well
            maps = np.array([x[1] for x in res])
            maximums = np.argmax(maps, axis=0)
            overlaps = np.sum(maps != 0, axis=0)>1
            # now lets go and infiltrate maps:
            # and do silly loop since we will reassign
            # the entries possibly
            for i in xrange(len(res)):
                n, m = res[i]
                loosers = np.logical_and(overlaps, ~(maximums == i))
                if len(loosers):
                    # copy and modify
                    m_new = m.copy()
                    m_new[loosers] = 0
                    res[i][1] = m_new
        elif overlaps is None:
            pass
        else:
            raise ValueError, \
                  "Incorrect value of overlaps argument %s" % overlaps
        return dict(res)

class FSLLabelsAtlas(XMLBasedAtlas):
    """Not sure what this one was for"""
    def __init__(self, *args, **kwargs):
        """not implemented"""
        FSLAtlas.__init__(self, *args, **kwargs)
        raise NotImplementedError



########NEW FILE########
__FILENAME__ = transformation
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Coordinate transformations"""

import numpy as np

if __debug__:
    from mvpa2.base import debug

class TypeProxy:
    """
    Simple class to convert from and then back to original type
    working with list, tuple, ndarray and having

    XXX Obsolete functionality ??
    """
    def __init__(self, value, toType=np.array):
        if   isinstance(value, list): self.__type = list
        elif isinstance(value, tuple): self.__type = tuple
        elif isinstance(value, np.ndarray): self.__type = np.array
        else:
            raise IndexError("Not understood format of coordinates '%s' for the transformation" % `coord`)

    def __call__(self, value):    return self.__type(value)
#   def __getitem__(self, value): return self.__type(value)


class TransformationBase:
    """
    Basic class to describe a transformation. Pretty much an interface
    """

    def __init__(self, previous=None):
        self.previous = previous

    def __getitem__(self, icoord):
        """
        Obtain coordinates, apply the transformation and spit out in the same
        format (list, tuple, numpy.array)
        """

        # remember original type
        #speed origType = TypeProxy(coord)

        # just in case it is not an ndarray, and to provide a copy to manipulate with
        coord = np.array(icoord)

        # apply previous transformation if such defined
        if self.previous:
            # if __debug__: debug('ATL__', "Applying previous transformation on `%s`" % `coord`)
            coord = self.previous[coord]

        #speed if __debug__: debug('ATL__', "Applying main transformation on `%s`" % `coord`)
        # apply transformation
        coord_out = self.apply(coord)
        #speed if __debug__: debug('ATL__', "Applied and got `%s`" % `coord_out`)

        #speed return origType(coord_out)
        return coord_out

    def __call__(self, coord):
        return self[coord]

    def apply(self, coord):
        return coord


class SpaceTransformation(TransformationBase):
    """
    To perform transformation from Voxel into Real Space.
    Simple one -- would subtract the origin and multiply by voxelSize.
    if to_real_space is True then on call/getitem converts to RealSpace
    """
    def __init__(self, voxelSize=None, origin=None, to_real_space=True,
                 *args, **kwargs):

        TransformationBase.__init__(self, *args, **kwargs)

        if not voxelSize is None: self.voxelSize = np.asarray(voxelSize)
        else: self.voxelSize = 1

        if not origin is None: self.origin = np.asarray(origin)
        else: self.origin = 0

        if to_real_space:
            self.apply = self.to_real_space
        else:
            self.apply = self.to_voxel_space

    ##REF: Name was automagically refactored
    def to_real_space(self, coord):
        #speed if not self.origin is None:
        coord -= self.origin
        #speed if not self.voxelSize is None:
        coord *= self.voxelSize
        return coord

    ##REF: Name was automagically refactored
    def to_voxel_space(self, coord):
        #speed if not self.voxelSize is None:
        coord /= self.voxelSize
        #speed if not self.origin is None:
        coord += self.origin
        return map(lambda x:int(round(x)), coord)


class Linear(TransformationBase):
    """
    Simple linear transformation defined by a matrix
    """
    def __init__(self, transf=np.eye(4), **kwargs):
        transf = np.asarray(transf)  # assure that we have arrays not matrices
        prev = kwargs.get('previous', None)
        if prev is not None and isinstance(prev, Linear):
            if prev.N == transf.shape[0] -1:
                if __debug__: debug('ATL__', "Colliding 2 linear transformations into 1")
                transf = np.dot(transf, prev.M)
                # reassign previous transformation to the current one
                kwargs['previous'] = prev.previous
        TransformationBase.__init__(self, **kwargs)
        self.M = transf
        self.N = self.M.shape[0] - 1

    def apply(self, coord):
        #speed if len(coord) != self.__N:
        #speed  raise ValueError("Transformation operates on %dD coordinates" \
        #speed                   % self.__N )
        #speed if __debug__: debug('ATL__', "Applying linear coord transformation + %s" % self.__M)
        # Might better come up with a linear transformation
        coord_ = np.r_[coord, [1.0]]
        result = np.dot(self.M, coord_)
        return result[0:-1]


class MNI2Tal_MatthewBrett(TransformationBase):
    """
    Transformation to bring MNI coordinates into MNI space

    Apparently it is due to Matthew Brett
    http://imaging.mrc-cbu.cam.ac.uk/imaging/MniTalairach
    """

    _UPPER = np.array([ [0.9900, 0, 0, 0 ],
                        [0, 0.9688, 0.0460, 0 ],
                        [0,-0.0485, 0.9189, 0 ],
                        [0, 0, 0, 1.0000] ] )
    _LOWER = np.array( [ [0.9900, 0, 0, 0 ],
                         [0, 0.9688, 0.0420, 0 ],
                         [0,-0.0485, 0.8390, 0 ],
                         [0, 0, 0, 1.0000] ] )

    def __init__(self, *args, **kwargs):
        TransformationBase.__init__(self, *args, **kwargs)
        self.__upper = Linear(self._UPPER)
        self.__lower = Linear(self._LOWER)

    def apply(self, coord):
        return {True: self.__upper,
                False: self.__lower}[coord[2]>=0][coord]


class Tal2MNI_MatthewBrett(TransformationBase):
    """
    Inverse of MNI2Tal_MatthewBrett

    """

    def __init__(self, *args, **kwargs):
        TransformationBase.__init__(self, *args, **kwargs)
        self.__upper = Linear(np.linalg.inv(MNI2Tal_MatthewBrett._UPPER))
        self.__lower = Linear(np.linalg.inv(MNI2Tal_MatthewBrett._LOWER))

    def apply(self, coord):

        return {True: self.__upper,
                False: self.__lower}[coord[2]>=0][coord]

def mni_to_tal_meyer_lindenberg98 (*args, **kwargs):
    """
    Due to Andreas Meyer-Lindenberg
    Taken from
    http://imaging.mrc-cbu.cam.ac.uk/imaging/MniTalairach
    """

    return Linear( np.array([
        [    0.88,   0,  0,  -0.8],
        [    0,   0.97,  0,  -3.32],
        [    0,   0.05,  0.88,   -0.44],
        [    0.00000,   0.00000,   0.00000,   1.00000] ]), *args, **kwargs )


def mni_to_tal_yohflirt (*args, **kwargs):
    """Transformations obtained using flirt from Talairach to Standard

    Transformations were obtained by registration of
    grey/white matter image from talairach atlas to FSL's standard
    volume. Following sequence of commands was used:

    fslroi /usr/share/rumba/atlases/data/talairach_atlas.nii.gz talairach_graywhite.nii.gz 3 1
    flirt -in talairach_graywhite.nii.gz \
     -ref /usr/apps/fsl.4.1/data/standard/MNI152_T1_1mm_brain.nii.gz \
     -out talairach2mni.nii.gz -omat talairach2mni.mat \
     -searchrx -20 20 -searchry -20 20 -searchrz -20 20 -coarsesearch 10 -finesearch 6 -v
    flirt -datatype float -in talairach_graywhite.nii.gz -init talairach2mni.mat \
     -ref /usr/apps/fsl.4.1/data/standard/MNI152_T1_1mm_brain.nii.gz \
     -out talairach2mni_fine1.nii.gz -omat talairach2mni_fine1.mat \
     -searchrx -10 10 -searchry -10 10 -searchrz -10 10 -coarsesearch 5 -finesearch 1 -v
    convert_xfm -inverse -omat mni2talairach.mat talairach2mni_fine1.mat
    """
    return Linear(
        t=np.array([
        [ 1.00448,  -0.00629625,  0.00741359,  0.70565,  ],
        [ 0.0130797,  0.978238,  0.0731315,  -3.8354,  ],
        [ 0.000248407,  -0.0374777,  0.838311,  18.6202,  ],
        [ 0,  0,  0,  1,  ],
        ])
                   , *args, **kwargs )


def tal_to_mni_yohflirt (*args, **kwargs):
    """See mni_to_tal_yohflirt doc
    """
    return Linear( np.array([
        [    1.00452,    0.00441281,  -0.011011,  -0.943886],
        [   -0.0141149,  1.00867,     -0.169177,  14.7016],
        [    0.00250222, 0.0920984,    1.18656,  -33.922],
        [    0.00000,   0.00000,   0.00000,   1.00000] ]), *args, **kwargs )



def mni_to_tal_lancaster07_fsl (*args, **kwargs):
    return Linear( np.array([
        [  0.9464, 0.0034, -0.0026, -1.0680],
        [ -0.0083, 0.9479, -0.0580, -1.0239],
        [  0.0053, 0.0617,  0.9010,  3.1883],
        [  0.0000, 0.0000,  0.0000,  1.0000] ]), *args, **kwargs )


def tal_to_mni_lancaster07_fsl (*args, **kwargs):
    return Linear( np.array([
        [ 1.056585, -0.003972,  0.002793,  1.115461],
        [ 0.008834,  1.050528,  0.067651,  0.869379],
        [-0.00682 , -0.071916,  1.105229, -3.60472 ],
        [ 0.      ,  0.      ,  0.      ,  1.      ]]), *args, **kwargs )


def mni_to_tal_lancaster07pooled (*args, **kwargs):
    return Linear( np.array([
        [    0.93570,   0.00290,  -0.00720,  -1.04230],
        [   -0.00650,   0.93960,  -0.07260,  -1.39400],
        [    0.01030,   0.07520,   0.89670,   3.64750],
        [    0.00000,   0.00000,   0.00000,   1.00000] ]), *args, **kwargs )


def tal_to_mni_lancaster07pooled (*args, **kwargs):
    return Linear( np.array([
        [  1.06860,  -0.00396,   0.00826,   1.07816],
        [  0.00640,   1.05741,   0.08566,   1.16824],
        [ -0.01281,  -0.08863,   1.10792,  -4.17805],
        [  0.00000,   0.00000,   0.00000,   1.00000] ]), *args, **kwargs )


if __name__ == '__main__':
    #t = Tal2Mni
    tl = tal_to_mni_lancaster07_fsl()
    tli = mni_to_tal_lancaster07_fsl()
    tml = mni_to_tal_meyer_lindenberg98()
    #print t[1,3,2]
    print tl[(1,3,2)]
    print tli[[1,3,2]]
    print tml[[1,3,2]]
    t = MNI2Tal_MatthewBrett()([10, 12, 14])
    print t, Tal2MNI_MatthewBrett()(t)
#   print t[(1,3,2,2)]

########NEW FILE########
__FILENAME__ = warehouse
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Collection of the known atlases"""

import os

from mvpa2.base import warning
from mvpa2.atlases.base import *
from mvpa2.atlases.fsl import *

__all__ = [ "KNOWN_ATLAS_FAMILIES", "KNOWN_ATLASES", "Atlas"]

KNOWN_ATLAS_FAMILIES = {
    'pymvpa': (["talairach", "talairach-dist"],
               r"/usr/share/rumba/atlases/data/%(name)s_atlas.xml"),
    'fsl': (["HarvardOxford-Cortical", "HarvardOxford-Subcortical",
             "JHU-tracts", "Juelich", "MNI", "Thalamus"],
            r"/usr/share/fsl/data/atlases/%(name)s.xml")
    # XXX make use of FSLDIR
    }

# map to go from the name to the path
KNOWN_ATLASES = dict(reduce(lambda x,y:x+[(yy,y[1]) for yy in y[0]],
                             KNOWN_ATLAS_FAMILIES.values(), []))


def Atlas(filename=None, name=None, *args, **kwargs):
    """A convinience factory for the atlases
    """
    if filename is None:
        if name is None:
            raise ValueError, \
                  "Please provide either path or name of the atlas to be used"
        atlaspath = KNOWN_ATLASES[name]
        filename = atlaspath % ( {'name': name} )
        if not os.path.exists(filename):
            raise IOError, \
                  "File %s for atlas %s was not found" % (filename, name)
    else:
        if name is not None:
            raise ValueError, "Provide only filename or name"

    try:
        # Just to guestimate what atlas that is
        tempAtlas = XMLBasedAtlas(filename=filename, load_maps=False) #, *args, **kwargs)
        version = tempAtlas.version
        atlas_source = None
        for cls in [PyMVPAAtlas, FSLAtlas]:
            if cls._check_version(version):
                atlas_source = cls.source
                break
        if atlas_source is None:
            if __debug__: debug('ATL_', "Unknown atlas " + filename)
            return tempAtlas

        atlasTypes = {
            'PyMVPA': {"Label" : LabelsAtlas,
                       "Reference": ReferencesAtlas},
            'FSL': {"Label" : FSLLabelsAtlas,
                    "Probabalistic": FSLProbabilisticAtlas,
                    "Probabilistic": FSLProbabilisticAtlas,
                    }
            }[atlas_source]
        atlasType = tempAtlas.header.type.text
        if atlasTypes.has_key(atlasType):
            if __debug__: debug('ATL_', "Creating %s Atlas" % atlasType)
            return atlasTypes[atlasType](filename=filename, *args, **kwargs)
            #return ReferencesAtlas(filename)
        else:
            warning("Unknown %s type '%s' of atlas in %s." " Known are %s" %
                    (atlas_source, atlasType, filename,
                     atlasTypes.keys()), 2)
            return tempAtlas
    except XMLAtlasException, e:
        print "File %s is not a valid XML based atlas due to %s" \
              % (filename, `e`)
        raise e


if __name__ == '__main__':
    from mvpa2.base import verbose
    verbose.level = 10
    for name in [
        #'data/talairach_atlas.xml',
        '/usr/share/fsl/data/atlases/HarvardOxford-Cortical.xml',
        '/usr/share/fsl/data/atlases/HarvardOxford-Subcortical.xml'
        ]:
        atlas = Atlas(name)
        #print isinstance(atlas.atlas, objectify.ObjectifiedElement)
        #print atlas.header.images.imagefile.get('offset')
        #print atlas.label_voxel( (0, -7, 20) )
        #print atlas[ 0, 0, 0 ]
        print atlas[ -63, -12, 22 ]
        #print atlas[ 0, -7, 20, [1,2,3] ]
        #print atlas[ (0, -7, 20), 1:2 ]
        #print atlas[ (0, -7, 20) ]
        #print atlas[ (0, -7, 20), : ]
        #   print atlas.get_labels(0)

########NEW FILE########
__FILENAME__ = attributes
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Module with some special objects to be used as magic attributes with
dedicated containers aka. `Collections`.
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.collections import Collectable

from mvpa2.misc.exceptions import UnknownStateError
import mvpa2.support.copy as copy

if __debug__:
    from mvpa2.base import debug



##################################################################
# Various attributes which will be collected into collections
#
class IndexedCollectable(Collectable):
    """Collectable with position information specified with index

    Derived classes will have specific semantics:

    * ConditionalAttribute: conditional storage
    * Parameter: attribute with validity ranges.

    `IndexedAttributes` instances are to be automagically grouped into
    corresponding collections for each class by `StateCollector`
    metaclass, i.e. it would be done on a class creation (i.e. not per
    each instance).  Upon instance creation those collection templates
    will be copied for the instance.
    """

    _instance_index = 0

    def __init__(self, index=None, *args, **kwargs):
        """
        Parameters
        ----------
        value : arbitrary (see derived implementations)
          The actual value of this attribute.
        **kwargs
          Passed to `Collectable`
        """
        if index is None:
            IndexedCollectable._instance_index += 1
            index = IndexedCollectable._instance_index
        else:
            # TODO: there can be collision between custom provided indexes
            #       and the ones automagically assigned.
            #       Check might be due
            pass
        self._instance_index = index

        self._isset = False
        self.reset()

        Collectable.__init__(self, *args, **kwargs)

        if __debug__ and 'COL' in debug.active:
            debug("COL", "Initialized new IndexedCollectable #%d:%s %r",
                  (index, self.name, self))

    # XXX shows how indexing was screwed up -- not copied etc
    #def __copy__(self):
    #    # preserve attribute type
    #    copied = self.__class__(name=self.name, doc=self.__doc__)
    #    # just get a view of the old data!
    #    copied.value = copy.copy(self.value)
    #    return copied

    def __reduce__(self):
        cr = Collectable.__reduce__(self)
        assert(len(cr) == 2)            # otherwise we need to change logic below
        res = (cr[0],
                (self._instance_index,) + cr[1],
                {'_isset' : self._isset})
        #if __debug__ and 'COL_RED' in debug.active:
        #    debug('COL_RED', 'Returning %s for %s' % (res, self))
        return res


    # XXX had to override due to _isset, init=
    def _set(self, val, init=False):
        """4Developers: Override this method in derived classes if you desire
           some logic (drop value in case of ca, or not allow to set value
           for read-only Parameters unless called with init=1) etc)
        """
        if __debug__: # Since this call is quite often, don't convert
            # values to strings here, rely on passing them # withing
            debug("COL", "%s %s to %s ",
                  ({True: 'Initializing', False: 'Setting'}[init],
                   self, val))
        self._value = val
        self._isset = True


    @property
    def is_set(self):
        return self._isset


    def reset(self):
        """Simply reset the flag"""
        if __debug__ and self._isset:
            debug("COL", "Reset %s to being non-modified", (self.name,))
        self._isset = False


    # TODO XXX unify all bloody __str__
    def __str__(self):
        res = "%s" % (self.name)
        if self.is_set:
            res += '*'          # so we have the value already
        return res

    # XXX  reports value depending on _isset
    def __repr__(self):
        if not self._isset:
            value = None
        else:
            value = self.value
        return "%s(value=%s, name=%s, doc=%s, index=%s)" % (
            self.__class__.__name__,
            repr(value),
            repr(self.name),
            repr(self.__doc__),
            self._instance_index,
            )


class ConditionalAttribute(IndexedCollectable):
    """Simple container intended to conditionally store the value
    """

    def __init__(self, enabled=True, *args, **kwargs):
        """
        Parameters
        ----------
        enabled : bool
          If a ConditionalAttribute is not enabled then assignment of any value has no
          effect, i.e. nothing is stored.
        **kwargs
          Passed to `IndexedCollectable`
        """
        # Force enabled state regardless of the input
        # to facilitate testing
        if __debug__ and 'ENFORCE_CA_ENABLED' in debug.active:
            enabled = True
        self.__enabled = enabled
        self._defaultenabled = enabled
        IndexedCollectable.__init__(self, *args, **kwargs)

    def __reduce__(self):
        icr = IndexedCollectable.__reduce__(self)
        icr[2].update({'_defaultenabled' : self._defaultenabled,
                       '_value': self._value})
        # kill the value from Collectable, because we have to put it in the dict
        # to prevent loosing it during reconstruction when the CA is disabled
        res = (icr[0], (self.__enabled, icr[1][0], None) + icr[1][2:], icr[2])
        #if __debug__ and 'COL_RED' in debug.active:
        #    debug('COL_RED', 'Returning %s for %s' % (res, self))
        return res

    def __str__(self):
        res = IndexedCollectable.__str__(self)
        if self.__enabled:
            res += '+'          # it is enabled but no value is assigned yet
        return res


    def _get(self):
        if not self.is_set:
            raise UnknownStateError("Unknown yet value of %s" % (self.name))
        return IndexedCollectable._get(self)


    def _set(self, val, init=False):
        if self.__enabled:
            # XXX may be should have left simple assignment
            # self._value = val
            IndexedCollectable._set(self, val)
        elif __debug__:
            debug("COL", "Not setting disabled %s to %s ",
                  (self, val))


    def reset(self):
        """Simply detach the value, and reset the flag"""
        IndexedCollectable.reset(self)
        self._value = None


    def _get_enabled(self):
        return self.__enabled


    def _set_enabled(self, value=False):
        if self.__enabled == value:
            # Do nothing since it is already in proper state
            return
        if __debug__:
            debug("STV", "%s %s",
                  ({True: 'Enabling', False: 'Disabling'}[value],
                   self))
        self.__enabled = value


    enabled = property(fget=_get_enabled, fset=_set_enabled)

########NEW FILE########
__FILENAME__ = collections
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Module with some special objects to be used as magic attributes with
dedicated containers aka. `Collections`.
"""

__docformat__ = 'restructuredtext'

import copy, re
import numpy as np

from mvpa2.base.dochelpers import _str, borrowdoc
from mvpa2.base.types import is_sequence_type

if __debug__:
    # we could live without, but it would be nicer with it
    try:
        from mvpa2.base import debug
        __mvpadebug__ = True
    except ImportError:
        __mvpadebug__ = False


_object_getattribute = dict.__getattribute__
_object_setattr = dict.__setattr__
_object_setitem = dict.__setitem__

# To validate fresh
_dict_api = set(dict.__dict__)

class Collectable(object):
    """Collection element.

    A named single item container that allows for type, or property checks of
    an assigned value, and also offers utility functionality.
    """
    def __init__(self, value=None, name=None, doc=None):
        """
        Parameters
        ----------
        value : arbitrary (see derived implementations)
          The actual value of this attribute.
        name : str
          Name of the collectable under which it should be available in its
          respective collection.
        doc : str
          Documentation about the purpose of this collectable.
        """
        if doc is not None:
            # to prevent newlines in the docstring
            try:
                doc = re.sub('[\n ]+', ' ', doc)
            except TypeError:
                # catch some old datasets stored in HDF5
                doc = re.sub('[\n ]+', ' ', np.asscalar(doc))

        self.__doc__ = doc
        self.__name = name
        self._value = None
        if not value is None:
            self._set(value)
        if __debug__ and __mvpadebug__:
            debug("COL", "Initialized %r", (self,))


    def __copy__(self):
        # preserve attribute type
        copied = self.__class__(name=self.name, doc=self.__doc__)
        # just get a view of the old data!
        copied.value = copy.copy(self.value)
        return copied

    ## def __deepcopy__(self, memo=None):
    ##     # preserve attribute type
    ##     copied = self.__class__(name=self.name, doc=self.__doc__)
    ##     # get a deepcopy of the old data!
    ##     copied._value = copy.deepcopy(self._value, memo)
    ##     return copied

    def _get(self):
        return self._value


    def _set(self, val):
        if __debug__ and __mvpadebug__:
            # Since this call is quite often, don't convert
            # values to strings here, rely on passing them
            # withing msgargs
            debug("COL", "Setting %s to %s ", (self, val))
        self._value = val


    def __str__(self):
        res = "%s" % (self.name)
        return res


    def __reduce__(self):
        return (self.__class__,
                    (self._value, self.name, self.__doc__))


    def __repr__(self):
        value = self.value
        return "%s(name=%s, doc=%s, value=%s)" % (self.__class__.__name__,
                                                  repr(self.name),
                                                  repr(self.__doc__),
                                                  repr(value))


    def _get_name(self):
        return self.__name


    def _set_name(self, name):
        """Set the name of parameter

        Notes
        -----
        Should not be called for an attribute which is already assigned
        to a collection
        """
        if name is not None:
            if isinstance(name, basestring):
                if name[0] == '_':
                    raise ValueError, \
                          "Collectable attribute name must not start " \
                          "with _. Got %s" % name
            else:
                raise ValueError, \
                      "Collectable attribute name must be a string. " \
                      "Got %s" % `name`
        self.__name = name


    # Instead of going for VProperty lets make use of virtual method
    def _get_virtual(self):
        return self._get()


    def _set_virtual(self, value):
        return self._set(value)


    value = property(_get_virtual, _set_virtual)
    name = property(_get_name, _set_name)


class SequenceCollectable(Collectable):
    """Collectable to handle sequences.

    It takes care about caching and recomputing unique values, as well as
    optional checking if assigned sequences have a desired length.
    """
    def __init__(self, value=None, name=None, doc="Sequence attribute",
                 length=None):
        """
        Parameters
        ----------
        value : arbitrary (see derived implementations)
          The actual value of this attribute.
        name : str
          Name of the attribute under which it should be available in its
          respective collection.
        doc : str
          Documentation about the purpose of this attribute.
        length : int
          If not None, enforce any array assigned as value of this collectable
          to be of this `length`. If an array does not match this requirement
          it is not modified, but a ValueError is raised.
        """
        # first configure the value checking, to enable it for the base class
        # init
        # XXX should we disallow empty Collectables??
        if not value is None and not hasattr(value, '__len__'):
            raise ValueError("%s only takes sequences as value."
                             % self.__class__.__name__)
        self._target_length = length
        Collectable.__init__(self, value=value, name=name, doc=doc)
        self._reset_unique()


    def __reduce__(self):
        return (self.__class__,
                    (self.value, self.name, self.__doc__, self._target_length))


    def __repr__(self):
        value = self.value
        return "%s(name=%s, doc=%s, value=%s, length=%s)" \
                    % (self.__class__.__name__,
                       repr(self.name),
                       repr(self.__doc__),
                       repr(value),
                       repr(self._target_length))


    def __len__(self):
        return self.value.__len__()


    def __getitem__(self, key):
        return self.value.__getitem__(key)


    def _set(self, val):
        # check if the new value has the desired length -- if length checking is
        # desired at all
        if not self._target_length is None \
           and len(val) != self._target_length:
            raise ValueError("Value length [%i] does not match the required "
                             "length [%i] of attribute '%s'."
                             % (len(val),
                                self._target_length,
                                str(self.name)))
        self._reset_unique()
        Collectable._set(self, val)


    def _reset_unique(self):
        self._unique_values = None


    @property
    def unique(self):
        """Return unique values
        """
        if self.value is None:
            return None
        if self._unique_values is None:
            try:
                self._unique_values = np.unique(self.value)
            except TypeError:
                # We are probably on Python 3 and value contains None's
                # or any other different type breaking the comparison
                # so operate through set()
                # See http://projects.scipy.org/numpy/ticket/2188

                # Get a 1-D array
                #  list around set is required for Python3
                value_unique = list(set(np.asanyarray(self.value).ravel()))
                try:
                    self._unique_values = np.array(value_unique)
                except ValueError:
                    # without forced dtype=object it might have failed due to
                    # something related to
                    # http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=679948
                    # which was fixed recently...
                    self._unique_values = np.array(value_unique, dtype=object)
        return self._unique_values


    def set_length_check(self, value):
        """Set a target length of the value in this collectable.

        Parameters
        ----------
        value : int
          If not None, enforce any array assigned as value of this collectable
          to be of this `length`. If an array does not match this requirement
          it is not modified, but a ValueError is raised.
        """
        self._target_length = value



class ArrayCollectable(SequenceCollectable):
    """Collectable embedding an array.

    When shallow-copied it includes a view of the array in the copy.
    """
    def __copy__(self):
        # preserve attribute type
        copied = self.__class__(name=self.name, doc=self.__doc__,
                                length=self._target_length)
        # just get a view of the old data!
        copied.value = self.value.view()
        return copied


    def _set(self, val):
        if not hasattr(val, 'view'):
            if is_sequence_type(val):
                try:
                    val = np.asanyarray(val)
                except ValueError, e:
                    if "setting an array element with a sequence" in str(e):
                        val = np.asanyarray(val, dtype=object)
                    else:
                        raise
            else:
                raise ValueError("%s only takes ndarrays (or array-likes "
                                 "providing view(), or sequence that can "
                                 "be converted into arrays (got '%s')."
                                 % (self.__class__.__name__,
                                    str(type(val))))
        SequenceCollectable._set(self, val)


class SampleAttribute(ArrayCollectable):
    """Per sample attribute in a dataset"""
    pass

class FeatureAttribute(ArrayCollectable):
    """Per feature attribute in a dataset"""
    pass

class DatasetAttribute(ArrayCollectable):
    """Dataset attribute"""
    pass



class Collection(dict):
    """Container of some Collectables.
    """
    def __init__(self, items=None):
        """
        Parameters
        ----------
        items : all types accepted by update()
        """
        dict.__init__(self)
        if not items is None:
            self.update(items)

    def copy(self, deep=True, a=None, memo=None):
        """Create a copy of a collection.

        By default this is going to return a deep copy of the
        collection, hence no data would be shared between the original
        dataset and its copy.

        Parameters
        ----------
        deep : boolean, optional
          If False, a shallow copy of the collection is return instead. The copy
          contains only views of the values.
        a : list or None
          List of attributes to include in the copy of the dataset. If
          `None` all attributes are considered. If an empty list is
          given, all attributes are stripped from the copy.
        memo : dict
          Developers only: This argument is only useful if copy() is called
          inside the __deepcopy__() method and refers to the dict-argument
          `memo` in the Python documentation.
        """

        # create the new collections of the right type derived classes
        # might like to assure correct setting of additional
        # attributes such as self._attr_length
        anew = self.__class__()

        # filter the attributes if necessary
        if a is None:
            aorig = self
        else:
            aorig = dict([(k, v) for k, v in self.iteritems() if k in a])

        # XXX copyvalues defaults to None which provides capability to
        #     just bind values (not even 'copy').  Might it need be
        #     desirable here?
        anew.update(aorig, copyvalues=deep and 'deep' or 'shallow',
                    memo=memo)

        if __debug__ and __mvpadebug__ and 'COL' in debug.active:
            debug("COL", "Copied %s into %s using args deep=%r a=%r",
                  (self, anew, deep, a))
            #if 'state2' in str(self):
            #    import pydb; pydb.debugger()
        return anew

    # XXX If enabled, then overrides dict.__reduce* leading to conditional
    #     attributes loosing their documentations in copying etc.
    #
    #def __copy__(self):
    #    return self.copy(deep=False)
    #
    #
    #def __deepcopy__(self, memo=None):
    #    return self.copy(deep=True, memo=memo)


    def __setitem__(self, key, value):
        """Add a new Collectable to the collection

        Parameters
        ----------
        key : str
          The name of the collectable under which it is available in the
          collection. This name is also stored in the item itself
        value : anything
          The actual item the should become part of the collection. If this is
          not an instance of `Collectable` or a subclass the value is
          automatically wrapped into it.
        """
        # Check if given key is not trying to override anything in
        # dict interface
        if key in _dict_api:
            raise ValueError, \
                  "Cannot add a collectable %r to collection %s since an " \
                  "attribute or a method with such a name is already present " \
                  "in dict interface.  Choose some other name." % (key, self)
        if not isinstance(value, Collectable):
            value = Collectable(value, name=key)
        else:
            if not value.name: # None assigned -- just use the Collectable
                value.name = key
            elif value.name == key: # assigned the same -- use the Collectable
                pass
            else: # use the copy and assign new name
                # see https://github.com/PyMVPA/PyMVPA/issues/149
                # for the original issue.  __copy__ directly to avoid copy.copy
                # doing the same + few more checks
                value = value.__copy__()
                # overwrite the Collectable's name with the given one
                value.name = key

        _object_setitem(self, key, value)


    def update(self, source, copyvalues=None, memo=None):
        """
        Parameters
        ----------
        source : list, Collection, dict
        copyvalues : None, shallow, deep
          If None, values will simply be bound to the collection items'
          values thus sharing the same instance. 'shallow' and 'deep' copies use
          'copy' and 'deepcopy' correspondingly.
        memo : dict
          Developers only: This argument is only useful if copy() is called
          inside the __deepcopy__() method and refers to the dict-argument
          `memo` in the Python documentation.
        """
        if isinstance(source, list):
            for a in source:
                if isinstance(a, tuple):
                    #list of tuples, e.g. from dict.items()
                    name = a[0]
                    value = a[1]
                else:
                    # list of collectables
                    name = a.name
                    value = a

                if copyvalues is None:
                    self[name] = value
                elif copyvalues == 'shallow':
                    self[name] = copy.copy(value)
                elif copyvalues == 'deep':
                    self[name] = copy.deepcopy(value, memo)
                else:
                    raise ValueError("Unknown value ('%s') for copy argument."
                                     % copy)
        elif isinstance(source, dict):
            for k, v in source.iteritems():
                # expand the docs
                if isinstance(v, tuple):
                    value = v[0]
                    doc = v[1]
                else:
                    value = v
                    doc = None
                # add the attribute with optional docs
                if copyvalues is None:
                    self[k] = v
                elif copyvalues == 'shallow':
                    self[k] = copy.copy(v)
                elif copyvalues == 'deep':
                    self[k] = copy.deepcopy(v, memo)
                else:
                    raise ValueError("Unknown value ('%s') for copy argument."
                                     % copy)
                # store documentation
                self[k].__doc__ = doc
        else:
            raise ValueError("Collection.update() cannot handle '%s'."
                             % str(type(source)))


    def __getattribute__(self, key):
        try:
            return self[key].value
        except KeyError:
            return _object_getattribute(self, key)


    def __setattr__(self, key, value):
        try:
            self[key].value = value
        except KeyError:
            _object_setattr(self, key, value)
        except Exception, e:
            # catch any other exception in order to provide a useful error message
            errmsg = "parameter '%s' cannot accept value `%r` (%s)" % (key, value, str(e))
            try:
                cdoc = self[key].constraints.long_description()
                if cdoc[0] == '(' and cdoc[-1] == ')':
                    cdoc = cdoc[1:-1]
                errmsg += " [%s]" % cdoc
            except:
                pass
            raise ValueError(errmsg)


    # TODO: unify with the rest of __repr__ handling
    def __repr__(self):
        return "%s(items=%r)" \
                  % (self.__class__.__name__, self.values())


    def __str__(self):
        return _str(self, ','.join([str(k) for k in sorted(self.keys())]))



class UniformLengthCollection(Collection):
    """Container for attributes with the same length.
    """
    def __init__(self, items=None, length=None):
        """
        Parameters
        ----------
        length : int
          When adding items to the collection, they are checked if the have this
          length.
        """
        # cannot call set_length(), since base class __getattribute__ goes wild
        # before its __init__ is called.
        self._uniform_length = length
        Collection.__init__(self, items)


    def __reduce__(self):
        return (self.__class__,
                    (self.items(), self._uniform_length))

    @borrowdoc(Collection)
    def copy(self, *args, **kwargs):
        # Create a generic copy of the collection
        anew = super(UniformLengthCollection, self).copy(*args, **kwargs)

        # if it had any attributes assigned, those should have set
        # attr_length already, otherwise lets assure that we copy the
        # correct one into the new instance
        if self.attr_length is not None and anew.attr_length is None:
            anew.set_length_check(self.attr_length)
        return anew


    def set_length_check(self, value):
        """
        Parameters
        ----------
        value : int
          When adding new items to the collection, they are checked if the have
          this length.
        """
        self._uniform_length = value
        for v in self.values():
            v.set_length_check(value)


    def __setitem__(self, key, value):
        """Add a new IndexedCollectable to the collection

        Parameters
        ----------
        item : IndexedCollectable
          or of derived class. Must have 'name' assigned.
        """
        # local binding
        ulength = self._uniform_length

        # XXX should we check whether it is some other Collectable?
        if not isinstance(value, ArrayCollectable):
            # if it is only a single element iterable, attempt broadcasting
            if is_sequence_type(value) and len(value) == 1 \
                    and not ulength is None:
                if ulength > 1:
                    # cannot use np.repeat, because it destroys dimensionality
                    value = [value[0]] * ulength
            value = ArrayCollectable(value)
        if ulength is None:
            ulength = len(value)
        elif not len(value.value) == ulength:
            raise ValueError("Collectable '%s' with length [%i] does not match "
                             "the required length [%i] of collection '%s'."
                             % (key,
                                len(value.value),
                                ulength,
                                str(self)))
        # tell the attribute to maintain the desired length
        value.set_length_check(ulength)
        Collection.__setitem__(self, key, value)


    attr_length = property(fget=lambda self:self._uniform_length,
                    doc="Uniform length of all attributes in a collection")



class SampleAttributesCollection(UniformLengthCollection):
    """Container for attributes of samples (i.e. labels, chunks...)
    """
    pass


class FeatureAttributesCollection(UniformLengthCollection):
    """Container for attributes of features
    """
    pass


class DatasetAttributesCollection(Collection):
    """Container for attributes of datasets (i.e. mappers, ...)
    """
    pass

########NEW FILE########
__FILENAME__ = config
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Registry-like monster"""

__docformat__ = 'restructuredtext'

from ConfigParser import SafeConfigParser
import os.path


class ConfigManager(SafeConfigParser):
    """Central configuration registry for PyMVPA.

    The purpose of this class is to collect all configurable settings used by
    various parts of PyMVPA. It is fairly simple and does only little more
    than the standard Python ConfigParser. Like ConfigParser it is blind to the
    data that it stores, i.e. not type checking is performed.

    Configuration files (INI syntax) in multiple location are passed when the
    class is instantiated or whenever `Config.reload()` is called later on.
    By default it looks for a config file named `pymvpa2.cfg` in the current
    directory and `.pymvpa2.cfg` in the user's home directory. Moreover, the
    constructor takes an optional argument with a list of additional file names
    to parse.

    In addition to configuration files, this class also looks for special
    environment variables to read settings from. Names of such variables have to
    start with `MVPA_` following by the an optional section name and the
    variable name itself ('_' as delimiter). If no section name is provided,
    the variables will be associated with section `general`. Some examples::

        MVPA_VERBOSE=1

    will become::

        [general]
        verbose = 1

    However, `MVPA_VERBOSE_OUTPUT=stdout` becomes::

        [verbose]
        output = stdout

    Any length of variable name as allowed, e.g. MVPA_SEC1_LONG_VARIABLE_NAME=1
    becomes::

        [sec1]
        long variable name = 1

    Settings from custom configuration files (specified by the constructor
    argument) have the highest priority and override settings found in the
    current directory. They in turn override user-specific settings and finally
    the content of any `MVPA_*` environment variables overrides all settings
    read from any file.
    """

    # things we want to count on to be available
    _DEFAULTS = {'general':
                  {
                    'verbose': '1',
                  }
                }


    def __init__(self, filenames=None):
        """Initialization reads settings from config files and env. variables.

        Parameters
        ----------
        filenames : list of filenames
        """
        SafeConfigParser.__init__(self)

        # store additional config file names
        if not filenames is None:
            self.__cfg_filenames = filenames
        else:
            self.__cfg_filenames = []

        # set critical defaults
        for sec, vars in ConfigManager._DEFAULTS.iteritems():
            self.add_section(sec)
            for key, value in vars.iteritems():
                self.set(sec, key, value)

        # now get the setting
        self.reload()


    def reload(self):
        """Re-read settings from all configured locations.
        """
        # listof filenames to parse (custom plus some standard ones)
        homedir = os.path.expanduser('~')
        user_configfile = os.path.join(homedir, '.pymvpa2.cfg')
        user_configfile_old = os.path.join(homedir, '.pymvpa.cfg')
        # first load user config and then overwrite by local and custom config
        # files.
        filenames = [user_configfile, 'pymvpa2.cfg'] + self.__cfg_filenames

        # Check if config for previous version exists, we need to
        # warn users since they might need to copy it over
        if not os.path.exists(user_configfile) and \
            os.path.exists(user_configfile_old):
            # but we can't use our 'warning' since it would not be
            # defined yet and import here would be circular
            # so use stock Python one
            from warnings import warn
            warn("You seems to have a configuration file %s for previous "
                 "version of PyMVPA but lacking configuration for PyMVPA2. "
                 "Consider copying it into %s"
                 % (user_configfile_old, user_configfile))

        # read local and user-specific config
        files = self.read(filenames)

        # no look for variables in the environment
        for var in [v for v in os.environ.keys() if v.startswith('MVPA_')]:
            # strip leading 'MVPA_' and lower case entries
            svar = var[5:].lower()

            # section is next element in name (or 'general' if simple name)
            if not svar.count('_'):
                sec = 'general'
            else:
                cut = svar.find('_')
                sec = svar[:cut]
                svar = svar[cut + 1:].replace('_', ' ')

            # check if section is already known and add it if not
            if not self.has_section(sec):
                self.add_section(sec)

            # set value
            self.set(sec, svar, os.environ[var])


    def __repr__(self):
        """Generate INI file content with current configuration.
        """
        # make adaptor to use str as file-like (needed for ConfigParser.write()
        class file2str(object):
            def __init__(self):
                self.__s = ''
            def write(self, val):
                self.__s += val
            def str(self):
                return self.__s

        r = file2str()
        self.write(r)

        return r.str()


    def save(self, filename):
        """Write current configuration to a file.
        """
        f = open(filename, 'w')
        self.write(f)
        f.close()


    def get(self, section, option, default=None, **kwargs):
        """Wrapper around SafeConfigParser.get() with a custom default value.

        This method simply wraps the base class method, but adds a `default`
        keyword argument. The value of `default` is returned whenever the
        config parser does not have the requested option and/or section.
        """
        if not self.has_option(section, option):
            return default

        try:
            return SafeConfigParser.get(self, section, option, **kwargs)
        except ValueError, e:
            # provide somewhat descriptive error
            raise ValueError, \
                  "Failed to obtain value from configuration for %s.%s. " \
                  "Original exception was: %s" % (section, option, e)


    def getboolean(self, section, option, default=None):
        """Wrapper around SafeConfigParser.getboolean() with a custom default.

        This method simply wraps the base class method, but adds a `default`
        keyword argument. The value of `default` is returned whenever the
        config parser does not have the requested option and/or section.
        """
        if not self.has_option(section, option):
            if isinstance(default, bool):
                return default
            else:
                # compatibility layer for py3 version of ConfigParser
                if hasattr(self, '_boolean_states'):
                    boolean_states = self._boolean_states
                else:
                    boolean_states = self.BOOLEAN_STATES
                if default.lower() not in boolean_states:
                    raise ValueError, 'Not a boolean: %s' % default
                return boolean_states[default.lower()]

        return SafeConfigParser.getboolean(self, section, option)


    ##REF: Name was automagically refactored
    def get_as_dtype(self, section, option, dtype, default=None):
        """Convenience method to query options with a custom default and type

        This method simply wraps the base class method, but adds a `default`
        keyword argument. The value of `default` is returned whenever the
        config parser does not have the requested option and/or section.

        In addition, the returned value is converted into the specified `dtype`.
        """
        if not self.has_option(section, option):
            return default
        try:
            return SafeConfigParser._get(self, section, dtype, option)
        except ValueError, e:
            # provide somewhat descriptive error
            raise ValueError, \
                  "Failed to obtain value from configuration for %s.%s. " \
                  "Original exception was: %s" % (section, option, e)

########NEW FILE########
__FILENAME__ = constraints
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##g
"""Helper for parameter validation, documentation and conversion"""

# TODO: __str__ / __repr__'s

__docformat__ = 'restructuredtext'

import numpy as np

if __debug__:
    from mvpa2.base import debug

class Constraint(object):
    """Base class for input value conversion/validation.

    These classes are also meant to be able to generate appropriate
    documentation on an appropriate parameter value.
    """

    def __and__(self, other):
        return Constraints(self, other)

    def __or__(self, other):
        return AltConstraints(self, other)

    def __call__(self, value):
        # do any necessary checks or conversions, potentially catch exceptions
        # and generate a meaningful error message
        raise NotImplementedError("abstract class")

    def long_description(self):
        # return meaningful docs or None
        # used as a comprehensive description in the parameter list
        return self.short_description()

    def short_description(self):
        # return meaningful docs or None
        # used as a condensed primer for the parameter lists
        raise NotImplementedError("abstract class")

class EnsureDType(Constraint):
    """Ensure that an input (or several inputs) are of a particular data type.
    """
    # TODO extend to support numpy-like dtype specs, e.g. 'int64'
    # in addition to functors
    def __init__(self, dtype):
        """
        Parameters
        ----------
        dtype : functor
        """
        self._dtype = dtype

    def __call__(self, value):
        if hasattr(value, '__array__'):
            return np.asanyarray(value, dtype=self._dtype)
        elif hasattr(value,'__iter__'):
            return map(self._dtype, value)
        else:
            return self._dtype(value)

    def short_description(self):
        dtype_descr = str(self._dtype)
        if dtype_descr[:7] == "<type '" and dtype_descr[-2:] == "'>":
            dtype_descr = dtype_descr[7:-2]
        return dtype_descr

    def long_description(self):
        return "value must be convertible to type '%s'" % self.short_description()


class EnsureInt(EnsureDType):
    """Ensure that an input (or several inputs) are of a data type 'int'.
    """
    def __init__(self):
        """Initializes EnsureDType with int"""
        EnsureDType.__init__(self, int)

class EnsureFloat(EnsureDType):
    """Ensure that an input (or several inputs) are of a data type 'float'.
    """
    def __init__(self):
        """Initializes EnsureDType with float"""
        EnsureDType.__init__(self, float)

class EnsureListOf(Constraint):
    """Ensure that an input is a list of a particular data type
    """
    def __init__(self, dtype):
        """
        Parameters
        ----------
        dtype : functor
        """
        self._dtype = dtype
        super(EnsureListOf, self).__init__()

    def __call__(self, value):
        return map(self._dtype, value)

    def short_description(self):
        dtype_descr = str(self._dtype)
        if dtype_descr[:7] == "<type '" and dtype_descr[-2:] == "'>":
            dtype_descr = dtype_descr[7:-2]
        return 'list(%s)' % dtype_descr

    def long_description(self):
        return "value must be convertible to %s" % self.short_description()

class EnsureTupleOf(Constraint):
    """Ensure that an input is a tuple of a particular data type
    """
    def __init__(self, dtype):
        """
        Parameters
        ----------
        dtype : functor
        """
        self._dtype = dtype
        super(EnsureTupleOf, self).__init__()

    def __call__(self, value):
        return tuple(map(self._dtype, value))

    def short_description(self):
        dtype_descr = str(self._dtype)
        if dtype_descr[:7] == "<type '" and dtype_descr[-2:] == "'>":
            dtype_descr = dtype_descr[7:-2]
        return 'tuple(%s)' % dtype_descr

    def long_description(self):
        return "value must be convertible to %s" % self.short_description()        
            
class EnsureBool(Constraint):
    """Ensure that an input is a bool.

    A couple of literal labels are supported, such as:
    False: '0', 'no', 'off', 'disable', 'false'
    True: '1', 'yes', 'on', 'enable', 'true'
    """
    def __call__(self, value):
        if isinstance(value, bool):
            return value
        elif isinstance(value, basestring):
            value = value.lower()
            if value in ('0', 'no', 'off', 'disable', 'false'):
                return False
            elif value in ('1', 'yes', 'on', 'enable', 'true'):
                return True
        raise ValueError("value must be converted to boolean")

    def long_description(self):
        return 'value must be convertible to type bool'

    def short_description(self):
        return 'bool'

class EnsureStr(Constraint):
    """Ensure an input is a string.

    No automatic conversion is attempted.
    """
    def __call__(self, value):
        if not isinstance(value, basestring):
            # do not perform a blind conversion ala str(), as almost
            # anything can be converted and the result is most likely
            # unintended
            raise ValueError("value is not a string")
        return value

    def long_description(self):
        return 'value must be a string'

    def short_description(self):
        return 'str'

class EnsureNone(Constraint):
    """Ensure an input is of value `None`"""
    def __call__(self, value):
        if value is None:
            return None
        else:
            raise ValueError("value must be `None`")

    def short_description(self):
        return 'None'

    def long_description(self):
        return 'value must be `None`'

class EnsureChoice(Constraint):
    """Ensure an input is element of a set of possible values"""

    def __init__(self, *values):
        """
        Parameters
        ----------
        *values
           Possible accepted values.
        """
        self._allowed = values
        super(EnsureChoice, self).__init__()

    def __call__(self, value):
        if value not in self._allowed:
            raise ValueError("value is not one of %s" % (self._allowed, ))
        return value

    def long_description(self):
        return 'value must be one of %s' % (str(self._allowed), )

    def short_description(self):
        return '{%s}' % ', '.join([str(c) for c in self._allowed])

class EnsureRange(Constraint):
    """Ensure an input is within a particular range

    No type checks are performed.
    """
    def __init__(self, min=None, max=None):
        """
        Parameters
        ----------
        min
            Minimal value to be accepted in the range
        max
            Maximal value to be accepted in the range
        """
        self._min = min
        self._max = max
        super(EnsureRange, self).__init__()

    def __call__(self, value):
        if self._min is not None:
            if value < self._min:
                raise ValueError("value must be at least %s" % (self._min, ))
        if self._max is not None:
            if value > self._max:
                raise ValueError("value must be at most %s" % (self._max, ))
        return value

    def long_description(self):
        min_str='-inf' if self._min is None else str(self._min)
        max_str='inf' if self._max is None else str(self._max)
        return 'value must be in range [%s, %s]' % (min_str, max_str)

    def short_description(self):
        None

class AltConstraints(Constraint):
    """Logical OR for constraints.

    An arbitrary number of constraints can be given. They are evaluated in the
    order in which they were specified. The value returned by the first
    constraint that does not raise an exception is the global return value.

    Documentation is aggregated for all alternative constraints.
    """
    def __init__(self, *constraints):
        """
        Parameters
        ----------
        *constraints
           Alternative constraints
        """
        super(AltConstraints, self).__init__()
        self.constraints = [EnsureNone() if c is None else c for c in constraints]

    def __or__(self, other):
        if isinstance(other, AltConstraints):
            self.constraints.extend(other.constraints)
        else:
            self.constraints.append(other)
        return self

    def __call__(self, value):
        e_list = []
        for c in self.constraints:
            try:
                return c(value)
            except Exception, e:
                e_list.append(e)
        raise ValueError("all alternative constraints violated")

    def long_description(self):
        cs = [c.long_description() for c in self.constraints if hasattr(c, 'long_description')]
        doc = ', or '.join(cs)
        if len(cs) > 1:
            return '(%s)' % doc
        else:
            return doc

    def short_description(self):
        cs = [c.short_description() for c in self.constraints
                    if hasattr(c, 'short_description') and not c.short_description() is None]
        doc = ' or '.join(cs)
        if len(cs) > 1:
            return '(%s)' % doc
        else:
            return doc



class Constraints(Constraint):
    """Logical AND for constraints.

    An arbitrary number of constraints can be given. They are evaluated in the
    order in which they were specified. The return value of each constraint is
    passed an input into the next. The return value of the last constraint
    is the global return value. No intermediate exceptions are caught.

    Documentation is aggregated for all constraints.
    """
    def __init__(self, *constraints):
        """
        Parameters
        ----------
        *constraints
           Constraints all of which must be satisfied
        """
        super(Constraints, self).__init__()
        self.constraints = [EnsureNone() if c is None else c for c in constraints]

    def __and__(self, other):
        if isinstance(other, Constraints):
            self.constraints.extend(other.constraints)
        else:
            self.constraints.append(other)
        return self

    def __call__(self, value):
        for c in (self.constraints):
            value = c(value)
        return value

    def long_description(self):
        cs = [c.long_description() for c in self.constraints if hasattr(c, 'long_description')]
        doc = ', and '.join(cs)
        if len(cs) > 1:
            return '(%s)' % doc
        else:
            return doc

    def short_description(self):
        cs = [c.short_description() for c in self.constraints
                    if hasattr(c, 'short_description') and not c.short_description() is None]
        doc = ' and '.join(cs)
        if len(cs) > 1:
            return '(%s)' % doc
        else:
            return doc

constraint_spec_map = {
    'float': EnsureFloat(),
    'int': EnsureInt(),
    'bool': EnsureBool(),
    'str': EnsureStr(),
    }

def expand_contraint_spec(spec):
    """Helper to translate literal contraint specs into functional ones

    e.g. 'float' -> EnsureFloat()
    """
    if spec is None or hasattr(spec, '__call__'):
        return spec
    else:
        try:
            return constraint_spec_map[spec]
        except KeyError:
            raise ValueError("unsupport constraint specification '%r'" % (spec,))

########NEW FILE########
__FILENAME__ = dataset
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Multi-purpose dataset container with support for attributes."""

__docformat__ = 'restructuredtext'

import numpy as np
import copy

from mvpa2.base import externals, cfg, warning
from mvpa2.base.collections import SampleAttributesCollection, \
        FeatureAttributesCollection, DatasetAttributesCollection
from mvpa2.base.types import is_datasetlike
from mvpa2.base.dochelpers import _str, _strid

if __debug__:
    from mvpa2.base import debug

__REPR_STYLE__ = cfg.get('datasets', 'repr', 'full')

if not __REPR_STYLE__ in ('full', 'str'):
    raise ValueError, "Incorrect value %r for option datasets.repr." \
          " Valid are 'full' and 'str'." % __REPR_STYLE__

class AttrDataset(object):
    """Generic storage class for datasets with multiple attributes.

    A dataset consists of four pieces.  The core is a two-dimensional
    array that has variables (so-called `features`) in its columns and
    the associated observations (so-called `samples`) in the rows.  In
    addition a dataset may have any number of attributes for features
    and samples.  Unsurprisingly, these are called 'feature attributes'
    and 'sample attributes'.  Each attribute is a vector of any datatype
    that contains a value per each item (feature or sample). Both types
    of attributes are organized in their respective collections --
    accessible via the `sa` (sample attribute) and `fa` (feature
    attribute) attributes.  Finally, a dataset itself may have any number
    of additional attributes (i.e. a mapper) that are stored in their
    own collection that is accessible via the `a` attribute (see
    examples below).

    Attributes
    ----------
    sa : Collection
      Access to all sample attributes, where each attribute is a named
      vector (1d-array) of an arbitrary datatype, with as many elements
      as rows in the `samples` array of the dataset.
    fa : Collection
      Access to all feature attributes, where each attribute is a named
      vector (1d-array) of an arbitrary datatype, with as many elements
      as columns in the `samples` array of the dataset.
    a : Collection
      Access to all dataset attributes, where each attribute is a named
      element of an arbitrary datatype.

    Notes
    -----
    Any dataset might have a mapper attached that is stored as a dataset
    attribute called `mapper`.

    Examples
    --------

    The simplest way to create a dataset is from a 2D array.

    >>> import numpy as np
    >>> from mvpa2.datasets import *
    >>> samples = np.arange(12).reshape((4,3))
    >>> ds = AttrDataset(samples)
    >>> ds.nsamples
    4
    >>> ds.nfeatures
    3
    >>> ds.samples
    array([[ 0,  1,  2],
           [ 3,  4,  5],
           [ 6,  7,  8],
           [ 9, 10, 11]])

    The above dataset can only be used for unsupervised machine-learning
    algorithms, since it doesn't have any targets associated with its
    samples. However, creating a labeled dataset is equally simple.

    >>> ds_labeled = dataset_wizard(samples, targets=range(4))

    Both the labeled and the unlabeled dataset share the same samples
    array. No copying is performed.

    >>> ds.samples is ds_labeled.samples
    True

    If the data should not be shared the samples array has to be copied
    beforehand.

    The targets are available from the samples attributes collection, but
    also via the convenience property `targets`.

    >>> ds_labeled.sa.targets is ds_labeled.targets
    True

    If desired, it is possible to add an arbitrary amount of additional
    attributes. Regardless if their original sequence type they will be
    converted into an array.

    >>> ds_labeled.sa['lovesme'] = [0,0,1,0]
    >>> ds_labeled.sa.lovesme
    array([0, 0, 1, 0])

    An alternative method to create datasets with arbitrary attributes
    is to provide the attribute collections to the constructor itself --
    which would also test for an appropriate size of the given
    attributes:

    >>> fancyds = AttrDataset(samples, sa={'targets': range(4),
    ...                                'lovesme': [0,0,1,0]})
    >>> fancyds.sa.lovesme
    array([0, 0, 1, 0])

    Exactly the same logic applies to feature attributes as well.

    Datasets can be sliced (selecting a subset of samples and/or
    features) similar to arrays. Selection is possible using boolean
    selection masks, index sequences or slicing arguments. The following
    calls for samples selection all result in the same dataset:

    >>> sel1 = ds[np.array([False, True, True])]
    >>> sel2 = ds[[1,2]]
    >>> sel3 = ds[1:3]
    >>> np.all(sel1.samples == sel2.samples)
    True
    >>> np.all(sel2.samples == sel3.samples)
    True

    During selection data is only copied if necessary. If the slicing
    syntax is used the resulting dataset will share the samples with the
    original dataset (here and below we compare .base against both ds.samples
    and its .base for compatibility with NumPy < 1.7)

    >>> sel1.samples.base in (ds.samples.base, ds.samples)
    False
    >>> sel2.samples.base in (ds.samples.base, ds.samples)
    False
    >>> sel3.samples.base in (ds.samples.base, ds.samples)
    True

    For feature selection the syntax is very similar they are just
    represented on the second axis of the samples array. Plain feature
    selection is achieved be keeping all samples and select a subset of
    features (all syntax variants for samples selection are also
    supported for feature selection).

    >>> fsel = ds[:, 1:3]
    >>> fsel.samples
    array([[ 1,  2],
           [ 4,  5],
           [ 7,  8],
           [10, 11]])

    It is also possible to simultaneously selection a subset of samples
    *and* features. Using the slicing syntax now copying will be
    performed.

    >>> fsel = ds[:3, 1:3]
    >>> fsel.samples
    array([[1, 2],
           [4, 5],
           [7, 8]])
    >>> fsel.samples.base in (ds.samples.base, ds.samples)
    True

    Please note that simultaneous selection of samples and features is
    *not* always congruent to array slicing.

    >>> ds[[0,1,2], [1,2]].samples
    array([[1, 2],
           [4, 5],
           [7, 8]])

    Whereas the call: 'ds.samples[[0,1,2], [1,2]]' would not be
    possible. In `AttrDatasets` selection of samples and features is always
    applied individually and independently to each axis.
    """
    def __init__(self, samples, sa=None, fa=None, a=None):
        """
        A Dataset might have an arbitrary number of attributes for samples,
        features, or the dataset as a whole. However, only the data samples
        themselves are required.

        Parameters
        ----------
        samples : ndarray
          Data samples.  This has to be a two-dimensional (samples x features)
          array. If the samples are not in that format, please consider one of
          the `AttrDataset.from_*` classmethods.
        sa : SampleAttributesCollection
          Samples attributes collection.
        fa : FeatureAttributesCollection
          Features attributes collection.
        a : DatasetAttributesCollection
          Dataset attributes collection.

        """
        # conversions
        if isinstance(samples, list):
            samples = np.array(samples)
        # Check all conditions we need to have for `samples` dtypes
        if not hasattr(samples, 'dtype'):
            raise ValueError(
                "AttrDataset only supports dtypes as samples that have a "
                "`dtype` attribute that behaves similar to the one of an "
                "array-like.")
        if not hasattr(samples, 'shape'):
            raise ValueError(
                "AttrDataset only supports dtypes as samples that have a "
                "`shape` attribute that behaves similar to the one of an "
                "array-like.")
        if not len(samples.shape):
            raise ValueError("Only `samples` with at least one axis are "
                    "supported (got: %i)" % len(samples.shape))

        # handling of 1D-samples
        # i.e. 1D is treated as multiple samples with a single feature
        if len(samples.shape) == 1:
            samples = np.atleast_2d(samples).T

        # that's all -- accepted
        self.samples = samples

        # Everything in a dataset (except for samples) is organized in
        # collections
        # Number of samples is .shape[0] for sparse matrix support
        self.sa = SampleAttributesCollection(length=len(self))
        if not sa is None:
            self.sa.update(sa)
        self.fa = FeatureAttributesCollection(length=self.nfeatures)
        if not fa is None:
            self.fa.update(fa)
        self.a = DatasetAttributesCollection()
        if not a is None:
            self.a.update(a)


    def init_origids(self, which, attr='origids', mode='new'):
        """Initialize the dataset's 'origids' attribute.

        The purpose of origids is that they allow to track the identity of
        a feature or a sample through the lifetime of a dataset (i.e. subsequent
        feature selections).

        Calling this method will overwrite any potentially existing IDs (of the
        XXX)

        Parameters
        ----------
        which : {'features', 'samples', 'both'}
          An attribute is generated for each feature, sample, or both that
          represents a unique ID.  This ID incorporates the dataset instance ID
          and should allow merging multiple datasets without causing multiple
          identical ID and the resulting dataset.
        attr : str
          Name of the attribute to store the generated IDs in.  By convention
          this should be 'origids' (the default), but might be changed for
          specific purposes.
        mode : {'existing', 'new', 'raise'}, optional
          Action if `attr` is already present in the collection.
          Default behavior is 'new' whenever new ids are generated and
          replace existing values if such are present.  With 'existing' it would
          not alter existing content.  With 'raise' it would raise
          `RuntimeError`.

        Raises
        ------
        `RuntimeError`
          If `mode` == 'raise' and `attr` is already defined
        """
        # now do evil to ensure unique ids across multiple datasets
        # so that they could be merged together
        thisid = str(id(self))
        legal_modes = ('raise', 'existing', 'new')
        if not mode in legal_modes:
            raise ValueError, "Incorrect mode %r. Known are %s." % \
                  (mode, legal_modes)
        if which in ('samples', 'both'):
            if attr in self.sa:
                if mode == 'existing':
                    return
                elif mode == 'raise':
                    raise RuntimeError, \
                          "Attribute %r already known to %s" % (attr, self.sa)
            ids = np.array(['%s-%i' % (thisid, i)
                                for i in xrange(self.samples.shape[0])])
            if self.sa.has_key(attr):
                self.sa[attr].value = ids
            else:
                self.sa[attr] = ids
        if which in ('features', 'both'):
            if attr in self.sa:
                if mode == 'existing':
                    return
                elif mode == 'raise':
                    raise RuntimeError, \
                          "Attribute %r already known to %s" % (attr, self.fa)
            ids = np.array(['%s-%i' % (thisid, i)
                                for i in xrange(self.samples.shape[1])])
            if self.fa.has_key(attr):
                self.fa[attr].value = ids
            else:
                self.fa[attr] = ids


    def __copy__(self):
        return self.copy(deep=False)


    def __deepcopy__(self, memo=None):
        return self.copy(deep=True, memo=memo)


    def __reduce__(self):
        return (self.__class__,
                    (self.samples,
                     dict(self.sa),
                     dict(self.fa),
                     dict(self.a)))


    def copy(self, deep=True, sa=None, fa=None, a=None, memo=None):
        """Create a copy of a dataset.

        By default this is going to return a deep copy of the dataset, hence no
        data would be shared between the original dataset and its copy.

        Parameters
        ----------
        deep : boolean, optional
          If False, a shallow copy of the dataset is return instead.  The copy
          contains only views of the samples, sample attributes and feature
          attributes, as well as shallow copies of all dataset
          attributes.
        sa : list or None
          List of attributes in the sample attributes collection to include in
          the copy of the dataset. If `None` all attributes are considered. If
          an empty list is given, all attributes are stripped from the copy.
        fa : list or None
          List of attributes in the feature attributes collection to include in
          the copy of the dataset. If `None` all attributes are considered If
          an empty list is given, all attributes are stripped from the copy.
        a : list or None
          List of attributes in the dataset attributes collection to include in
          the copy of the dataset. If `None` all attributes are considered If
          an empty list is given, all attributes are stripped from the copy.
        memo : dict
          Developers only: This argument is only useful if copy() is called
          inside the __deepcopy__() method and refers to the dict-argument
          `memo` in the Python documentation.
        """
        if __debug__:
            debug('DS_', "Duplicating samples shaped %s"
                         % str(self.samples.shape))
        if deep:
            samples = copy.deepcopy(self.samples, memo)
        else:
            samples = self.samples.view()

        if __debug__:
            debug('DS_', "Create new dataset instance for copy")
        # call the generic init
        out = self.__class__(samples,
                             sa=self.sa.copy(a=sa, deep=deep, memo=memo),
                             fa=self.fa.copy(a=fa, deep=deep, memo=memo),
                             a=self.a.copy(a=a, deep=deep, memo=memo))
        if __debug__:
            debug('DS_', "Return dataset copy %s of source %s"
                         % (_strid(out), _strid(self)))
        return out


    def append(self, other):
        """This method should not be used and will be removed in the future"""
        warning("AttrDataset.append() is deprecated and will be removed. "
                "Instead of ds.append(x) use: ds = vstack((ds, x), a=0)")

        if not self.nfeatures == other.nfeatures:
            raise DatasetError("Cannot merge datasets, because the number of "
                               "features does not match.")

        if not sorted(self.sa.keys()) == sorted(other.sa.keys()):
            raise DatasetError("Cannot merge dataset. This datasets samples "
                               "attributes %s cannot be mapped into the other "
                               "set %s" % (self.sa.keys(), other.sa.keys()))

        # concat the samples as well
        self.samples = np.concatenate((self.samples, other.samples), axis=0)

        # tell the collection the new desired length of all attributes
        self.sa.set_length_check(len(self.samples))
        # concat all samples attributes
        for k, v in other.sa.iteritems():
            self.sa[k].value = np.concatenate((self.sa[k].value, v.value),
                                             axis=0)


    def __getitem__(self, args):
        """
        """
        # uniformize for checks below; it is not a tuple if just single slicing
        # spec is passed
        if not isinstance(args, tuple):
            args = (args,)

        if len(args) > 2:
            raise ValueError("Too many arguments (%i). At most there can be "
                             "two arguments, one for samples selection and one "
                             "for features selection" % len(args))

        # simplify things below and always have samples and feature slicing
        if len(args) == 1:
            args = [args[0], slice(None)]
        else:
            args = [a for a in args]

        samples = None

        # get the intended subset of the samples array
        #
        # need to deal with some special cases to ensure proper behavior
        #
        # ints need to become lists to prevent silent dimensionality changes
        # of the arrays when slicing
        for i, a in enumerate(args):
            if isinstance(a, int):
                args[i] = [a]

        # for simultaneous slicing of numpy arrays we should
        # distinguish the case when one of the args is a slice, so no
        # ix_ is needed
        if __debug__:
            debug('DS_', "Selecting feature/samples of %s" % str(self.samples.shape))
        if isinstance(self.samples, np.ndarray):
            if np.any([isinstance(a, slice) for a in args]):
                samples = self.samples[args[0], args[1]]
            else:
                # works even with bool masks (although without
                # assurance/checking if mask is of actual length as
                # needed, so would work with bogus shorter
                # masks). TODO check in __debug__? or may be just do
                # enforcing of proper dimensions and order manually?
                samples = self.samples[np.ix_(*args)]
        else:
            # in all other cases we have to do the selection sequentially
            #
            # samples subset: only alter if subset is requested
            samples = self.samples[args[0]]
            # features subset
            if not args[1] is slice(None):
                samples = samples[:, args[1]]
        if __debug__:
            debug('DS_', "Selected feature/samples %s" % str(self.samples.shape))
        # and now for the attributes -- we want to maintain the type of the
        # collections
        sa = self.sa.__class__(length=samples.shape[0])
        fa = self.fa.__class__(length=samples.shape[1])
        a = self.a.__class__()

        # per-sample attributes; always needs to run even if slice(None), since
        # we need fresh SamplesAttributes even if they share the data
        for attr in self.sa.values():
            # preserve attribute type
            newattr = attr.__class__(doc=attr.__doc__)
            # slice
            newattr.value = attr.value[args[0]]
            # assign to target collection
            sa[attr.name] = newattr

        # per-feature attributes; always needs to run even if slice(None),
        # since we need fresh SamplesAttributes even if they share the data
        for attr in self.fa.values():
            # preserve attribute type
            newattr = attr.__class__(doc=attr.__doc__)
            # slice
            newattr.value = attr.value[args[1]]
            # assign to target collection
            fa[attr.name] = newattr

        # and finally dataset attributes: this time copying
        for attr in self.a.values():
            # preserve attribute type
            newattr = attr.__class__(name=attr.name, doc=attr.__doc__)
            # do a shallow copy here
            # XXX every DatasetAttribute should have meaningful __copy__ if
            # necessary -- most likely all mappers need to have one
            newattr.value = copy.copy(attr.value)
            # assign to target collection
            a[attr.name] = newattr

        # and after a long way instantiate the new dataset of the same type
        return self.__class__(samples, sa=sa, fa=fa, a=a)


    def __repr_full__(self):
        return "%s(%s, sa=%s, fa=%s, a=%s)" \
                % (self.__class__.__name__,
                   repr(self.samples),
                   repr(self.sa),
                   repr(self.fa),
                   repr(self.a))


    def __str__(self):
        samplesstr = 'x'.join(["%s" % x for x in self.shape])
        samplesstr += '@%s' % self.samples.dtype
        cols = [str(col).replace(col.__class__.__name__, label)
                    for col, label in [(self.sa, 'sa'),
                                       (self.fa, 'fa'),
                                       (self.a, 'a')] if len(col)]
        # include only collections that have content
        return _str(self, samplesstr, *cols)

    __repr__ = {'full' : __repr_full__,
                'str'  : __str__}[__REPR_STYLE__]

    def __array__(self, *args):
        """Provide an 'array' view or copy over dataset.samples

        Parameters
        ----------
        dtype: type, optional
          If provided, passed to .samples.__array__() call

        *args to mimique numpy.ndarray.__array__ behavior which relies
        on the actual number of arguments
        """
        # another possibility would be converting .todense() for sparse data
        # but that might easily kill the machine ;-)
        if not hasattr(self.samples, '__array__'):
            raise RuntimeError(
                "This AttrDataset instance cannot be used like a Numpy array "
                "since its data-container does not provide an '__array__' "
                "methods. Container type is %s." % type(self.samples))
        return self.samples.__array__(*args)


    def __len__(self):
        return self.shape[0]


    @classmethod
    def from_hdf5(cls, source, name=None):
        """Load a Dataset from HDF5 file

        Parameters
        ----------
        source : string or h5py.highlevel.File
          Filename or HDF5's File to load dataset from
        name : string, optional
          If file contains multiple entries at the 1st level, if
          provided, `name` specifies the group to be loaded as the
          AttrDataset.

        Returns
        -------
        AttrDataset

        Raises
        ------
        ValueError
        """
        if not externals.exists('h5py'):
            raise RuntimeError(
                "Missing 'h5py' package -- saving is not possible.")

        import h5py
        from mvpa2.base.hdf5 import hdf2obj

        # look if we got an hdf file instance already
        if isinstance(source, h5py.highlevel.File):
            own_file = False
            hdf = source
        else:
            own_file = True
            hdf = h5py.File(source, 'r')

        if not name is None:
            # some HDF5 subset is requested
            if not name in hdf:
                raise ValueError("Cannot find '%s' group in HDF file %s.  "
                                 "File contains groups: %s"
                                 % (name, source, hdf.keys()))

            # access the group that should contain the dataset
            dsgrp = hdf[name]
            res = hdf2obj(dsgrp)
            if not isinstance(res, AttrDataset):
                # TODO: unittest before committing
                raise ValueError, "%r in %s contains %s not a dataset.  " \
                      "File contains groups: %s." \
                      % (name, source, type(res), hdf.keys())
        else:
            # just consider the whole file
            res = hdf2obj(hdf)
            if not isinstance(res, AttrDataset):
                # TODO: unittest before committing
                raise ValueError, "Failed to load a dataset from %s.  " \
                      "Loaded %s instead." \
                      % (source, type(res))
        if own_file:
            hdf.close()
        return res


    # shortcut properties
    nsamples = property(fget=len)
    nfeatures = property(fget=lambda self:self.shape[1])
    shape = property(fget=lambda self:self.samples.shape)


def datasetmethod(func):
    """Decorator to easily bind functions to an AttrDataset class
    """
    if __debug__:
        debug("DS_",
              "Binding function %s to AttrDataset class" % func.func_name)

    # Bind the function
    setattr(AttrDataset, func.func_name, func)

    # return the original one
    return func


def vstack(datasets, a=None):
    """Stacks datasets vertically (appending samples).

    Feature attribute collections are merged incrementally, attribute with
    identical keys overwriting previous ones in the stacked dataset. All
    datasets must have an identical set of sample attributes (matching keys,
    not values), otherwise a ValueError will be raised.
    No dataset attributes from any source dataset will be transferred into the
    stacked dataset. If all input dataset have common dataset attributes that
    are also valid for the stacked dataset, they can be moved into the output
    dataset like this::

      ds_merged = vstack((ds1, ds2, ds3))
      ds_merged.a.update(ds1.a)

    Parameters
    ----------
    datasets : tuple
        Sequence of datasets to be stacked.
    a: {'unique','drop_nonunique','uniques','all'} or True or False or None (default: None)
        Indicates which dataset attributes from datasets are stored
        in merged_dataset. If an int k, then the dataset attributes from
        datasets[k] are taken. If 'unique' then it is assumed that any
        attribute common to more than one dataset in datasets is unique;
        if not an exception is raised. If 'drop_nonunique' then as 'unique',
        except that exceptions are not raised. If 'uniques' then, for each
        attribute,  any unique value across the datasets is stored in a tuple
        in merged_datasets. If 'all' then each attribute present in any
        dataset across datasets is stored as a tuple in merged_datasets;
        missing values are replaced by None. If None (the default) then no
        attributes are stored in merged_dataset. True is equivalent to
        'drop_nonunique'. False is equivalent to None.

    Returns
    -------
    AttrDataset (or respective subclass)
    """
    if not len(datasets):
        raise ValueError('concatenation of zero-length sequences is impossible')
    if not len(datasets) > 1:
        # trivial vstack
        return datasets[0]
    # fall back to numpy if it is not a dataset
    if not is_datasetlike(datasets[0]):
        return AttrDataset(np.vstack(datasets))

    if __debug__:
        target = sorted(datasets[0].sa.keys())
        if not np.all([sorted(ds.sa.keys()) == target for ds in datasets]):
            raise ValueError("Sample attributes collections of to be stacked "
                             "datasets have varying attributes.")
    # will puke if not equal number of features
    stacked_samp = np.concatenate([ds.samples for ds in datasets], axis=0)

    stacked_sa = {}
    for attr in datasets[0].sa:
        stacked_sa[attr] = np.concatenate(
            [ds.sa[attr].value for ds in datasets], axis=0)
    # create the dataset
    merged = datasets[0].__class__(stacked_samp, sa=stacked_sa)

    for ds in datasets:
        merged.fa.update(ds.fa)

    _stack_add_equal_dataset_attributes(merged, datasets, a)
    return merged


def hstack(datasets, a=None):
    """Stacks datasets horizontally (appending features).

    Sample attribute collections are merged incrementally, attribute with
    identical keys overwriting previous ones in the stacked dataset. All
    datasets must have an identical set of feature attributes (matching keys,
    not values), otherwise a ValueError will be raised.
    No dataset attributes from any source dataset will be transferred into the
    stacked dataset.

    Parameters
    ----------
    datasets : tuple
        Sequence of datasets to be stacked.
    a: {'unique','drop_nonunique','uniques','all'} or True or False or None (default: None)
        Indicates which dataset attributes from datasets are stored
        in merged_dataset. If an int k, then the dataset attributes from
        datasets[k] are taken. If 'unique' then it is assumed that any
        attribute common to more than one dataset in datasets is unique;
        if not an exception is raised. If 'drop_nonunique' then as 'unique',
        except that exceptions are not raised. If 'uniques' then, for each
        attribute,  any unique value across the datasets is stored in a tuple
        in merged_datasets. If 'all' then each attribute present in any
        dataset across datasets is stored as a tuple in merged_datasets;
        missing values are replaced by None. If None (the default) then no
        attributes are stored in merged_dataset. True is equivalent to
        'drop_nonunique'. False is equivalent to None.

    Returns
    -------
    AttrDataset (or respective subclass)
    """
    #
    # XXX Use CombinedMapper in here whenever it comes back
    #

    if not len(datasets):
        raise ValueError('concatenation of zero-length sequences is impossible')
    if not len(datasets) > 1:
        # trivial hstack
        return datasets[0]
    # fall back to numpy if it is not a dataset
    if not is_datasetlike(datasets[0]):
        # we might get a list of 1Ds that would yield wrong results when
        # turned into a dict (would run along samples-axis)
        return AttrDataset(np.atleast_2d(np.hstack(datasets)))

    if __debug__:
        target = sorted(datasets[0].fa.keys())
        if not np.all([sorted(ds.fa.keys()) == target for ds in datasets]):
            raise ValueError("Feature attributes collections of to be stacked "
                             "datasets have varying attributes.")
    # will puke if not equal number of samples
    stacked_samp = np.concatenate([ds.samples for ds in datasets], axis=1)

    stacked_fa = {}
    for attr in datasets[0].fa:
        stacked_fa[attr] = np.concatenate(
            [ds.fa[attr].value for ds in datasets], axis=0)
    # create the dataset
    merged = datasets[0].__class__(stacked_samp, fa=stacked_fa)

    for ds in datasets:
        merged.sa.update(ds.sa)

    _stack_add_equal_dataset_attributes(merged, datasets, a)

    return merged


def all_equal(x, y):
    '''General function that compares two values. Usually this function
    behaves like x==y and type(x)==type(y), but for numpy arrays it
    behaves like np.array_equal(x==y).

    Parameters
    ----------
    x, y : any type
        Elements to be compared

    Returns
    -------
    eq: bool
        True iff x and y are equal. If in the comparison of x and y
        and exception is thrown then False is returned
        This comparison is performed element-wise, if applicable, and
        in that case True is only returned if all elements are equal
    '''

    # an equality comparison that also works on numpy arrays
    try:
        eq = x == y
    except:
        return False

    # eq could be a numpy array or similar. See if it has a length
    try:
        len(eq) # that's fine, so we can zip x and y (below)
                # and compare by elements
    except TypeError:
        # if it's just a bool (or boolean-like, such as numpy.bool_)
        # then see if it is True or not
        if eq == True or eq == False:
            # also consider the case that eq is a numpy boolean array
            # with just a single element - so compare to True
            return eq == True
        else:
            # no idea what to do
            raise

    # because of numpy's broadcasting either x or y may
    # be a scaler yet eq could be an array
    try:
        same_length = len(x) == len(y)
        if not same_length:
            return False
    except:
        return False

    # do a recursive call on all elements
    return all(all_equal(xx, yy) for (xx, yy) in zip(x, y))

def _stack_add_equal_dataset_attributes(merged_dataset, datasets, a=None):
    """Helper function for vstack and hstack to find dataset
    attributes common to a set of datasets, and at them to the output.
    Note:by default this function does nothing because testing for equality
    may be messy for certain types; to override a value should be assigned
    to the add_keys argument.

    Parameters
    ----------
    merged_dataset: Dataset
        the output dataset to which attributes are added
    datasets: tuple of Dataset
        Sequence of datasets to be stacked. Only attributes present
        in all datasets and with identical values are put in
        merged_dataset
    a: {'unique','drop_nonunique','uniques','all'} or True or False or None (default: None).
        Indicates which dataset attributes from datasets are stored
        in merged_dataset. If an int k, then the dataset attributes from
        datasets[k] are taken. If 'unique' then it is assumed that any
        attribute common to more than one dataset in datasets is unique;
        if not an exception is raised. If 'drop_nonunique' then as 'unique',
        except that exceptions are not raised. If 'uniques' then, for each
        attribute,  any unique value across the datasets is stored in a tuple
        in merged_datasets. If 'all' then each attribute present in any
        dataset across datasets is stored as a tuple in merged_datasets;
        missing values are replaced by None. If None (the default) then no
        attributes are stored in merged_dataset. True is equivalent to
        'drop_nonunique'. False is equivalent to None.
    """
    if a is None or a is False:
        # do nothing
        return
    elif a is True:
        a = 'drop_nonunique'

    if not datasets:
        # empty - so nothing to do
        return

    if type(a) is int:
        base_dataset = datasets[a]

        for key in base_dataset.a.keys():
            merged_dataset.a[key] = base_dataset.a[key].value

        return

    allowed_values = ['unique', 'uniques', 'drop_nonunique', 'all']
    if not a in allowed_values:
        raise ValueError("a should be an int or one of "
                        "%r" % allowed_values)

    # consider all keys that are present in at least one dataset
    all_keys = set.union(*[set(dataset.a.keys()) for dataset in datasets])


    def _contains(xs, y, comparator=all_equal):
        for x in xs:
            if comparator(x, y):
                return True
        return False

    for key in all_keys:
        add_key = True
        values = []
        for i, dataset in enumerate(datasets):
            if not key in dataset.a:
                if a == 'all':
                    values.append(None)
                continue

            value = dataset.a[key].value

            if a in ('drop_nonunique', 'unique'):
                if not values:
                    values.append(value)
                elif not _contains(values, value):
                    if a == 'unique':
                        raise DatasetError("Not unique dataset attribute value "
                                         " for %s: %s and %s" %
                                            (key, values[0], value))
                    else:
                        add_key = False
                        break
            elif a == 'uniques':
                if not _contains(values, value):
                    values.append(value)
            elif a == 'all':
                values.append(value)
            else:
                raise ValueError("this should not happen: %s" % a)

        if add_key:
            if a in ('drop_nonunique', 'unique'):
                merged_dataset.a[key] = values[0]
            else:
                merged_dataset.a[key] = tuple(values)


def _expand_attribute(attr, length, attr_name):
    """Helper function to expand attributes to a desired length.

    If e.g. a sample attribute is given as a scalar expand/repeat it to a
    length matching the number of samples in the dataset.
    """
    try:
        # if we are initializing with a single string -- we should
        # treat it as a single label
        if isinstance(attr, basestring):
            raise TypeError
        if len(attr) != length:
            raise ValueError("Length of attribute '%s' [%d] has to be %d."
                             % (attr_name, len(attr), length))
        # sequence as array
        return np.asanyarray(attr)

    except TypeError:
        # make sequence of identical value matching the desired length
        return np.repeat(attr, length)

def stack_by_unique_sample_attribute(dataset, sa_label):
    """Performs hstack based on unique values in sa_label

    Parameters
    ----------
    dataset: Dataset
        input dataset.
    sa_label: str
        sample attribute label according which samples in dataset
        are stacked.

    Returns
    -------
    stacked_dataset: Dataset
        A dataset where matching features are joined (hstacked).
        If the number of matching features differs for values in sa_label
        and exception is raised.
    """

    unq, masks = _get_unique_attribute_masks(dataset.sa[sa_label].value)

    ds = []
    for i, mask in enumerate(masks):
        d = dataset[mask, :]
        d.fa[sa_label] = [unq[i]] * d.nfeatures
        ds.append(d)

    stacked_ds = hstack(ds, True)
    stacked_ds.sa.pop(sa_label)

    return stacked_ds


def stack_by_unique_feature_attribute(dataset, fa_label):
    """Performs vstack based on unique values in fa_label

    Parameters
    ----------
    dataset: Dataset
        input dataset.
    fa_label: str
        feature attribute label according which samples in dataset
        are stacked.

    Returns
    stacked_dataset: Dataset
        A dataset where matching samples are joined. This dataset has
        a sample attribute fa_label added and the feature attribute
        fa_label removed.
        If the number of matching features differs for values in sa_label
        and exception is raised.
    """

    unq, masks = _get_unique_attribute_masks(dataset.fa[fa_label].value)

    ds = []
    for i, mask in enumerate(masks):
        d = dataset[:, mask]
        d.sa[fa_label] = [unq[i]] * d.nsamples
        ds.append(d)

    stacked_ds = vstack(ds, True)
    stacked_ds.fa.pop(fa_label)

    return stacked_ds


def _get_unique_attribute_masks(xs, raise_unequal_count=True):
    '''Helper function to get masks for each unique value'''
    unq = np.unique(xs)
    masks = [x == xs for x in unq]

    if raise_unequal_count:
        hs = [np.sum(mask) for mask in masks]

        for i, h in enumerate(hs):
            if i == 0:
                h0 = h
            elif h != h0:
                raise ValueError('Value mismatch between input 0 and %d:'
                                 ' %s != %s' % (i, h, h0))
    return unq, masks

def split_by_sample_attribute(ds, sa_label, raise_unequal_count=True):
    '''Splits a dataset based on unique values of a sample attribute

    Parameters
    ----------
    d: Dataset
        input dataset
    sa_label: str or list of str
        sample attribute label(s) on which the split is based

    Returns
    -------
    ds: list of Dataset
        List with n datasets, if d.sa[sa_label] has n unique values
    '''
    if type(sa_label) in (list, tuple):
        label0 = sa_label[0]
        sas = split_by_sample_attribute(ds, label0, raise_unequal_count)
        if len(sa_label) == 1:
            return sas
        else:
            return sum([split_by_sample_attribute(sa, sa_label[1:],
                                                  raise_unequal_count)
                                for sa in sas], [])

    _, masks = _get_unique_attribute_masks(ds.sa[sa_label].value,
                                    raise_unequal_count=raise_unequal_count)

    return [ds[mask, :].copy(deep=False) for mask in masks]


def split_by_feature_attribute(ds, fa_label, raise_unequal_count=True):
    '''Splits a dataset based on unique values of a feature attribute

    Parameters
    ----------
    d: Dataset
        input dataset
    sa_label: str or list of str
        sample attribute label(s) on which the split is based

    Returns
    -------
    ds: list of Dataset
        List with n datasets, if d.fa[fa_label] has n unique values
    '''
    if type(fa_label) in (list, tuple):
        label0 = fa_label[0]
        fas = split_by_feature_attribute(ds, label0, raise_unequal_count)
        if len(fa_label) == 1:
            return fas
        else:
            return sum([split_by_feature_attribute(fa, fa_label[1:],
                                                   raise_unequal_count)
                                for fa in fas], [])

    _, masks = _get_unique_attribute_masks(ds.fa[fa_label].value,
                                    raise_unequal_count=raise_unequal_count)

    return [ds[:, mask].copy(deep=False) for mask in masks]



class DatasetError(Exception):
    """Thrown if there is a problem with the internal integrity of a Dataset.
    """
    # A ValueError exception is too generic to be used for any needed case,
    # thus this one is created
    pass


class DatasetAttributeExtractor(object):
    """Helper to extract arbitrary attributes from dataset collections.

    Examples
    --------
    >>> ds = AttrDataset(np.arange(12).reshape((4,3)),
    ...              sa={'targets': range(4)},
    ...              fa={'foo': [0,0,1]})
    >>> ext = DAE('sa', 'targets')
    >>> ext(ds)
    array([0, 1, 2, 3])

    >>> ext = DAE('fa', 'foo')
    >>> ext(ds)
    array([0, 0, 1])
    """
    def __init__(self, col, key):
        """
        Parameters
        ----------
        col : {'sa', 'fa', 'a'}
          The respective collection to extract an attribute from.
        key : arbitrary
          The name/key of the attribute in the collection.
        """
        self._col = col
        self._key = key

    def __call__(self, ds):
        """
        Parameters
        ----------
        ds : AttrDataset
        """
        return ds.__dict__[self._col][self._key].value

    def __repr__(self):
        return "%s(%s, %s)" % (self.__class__.__name__,
                               repr(self._col), repr(self._key))


# shortcut that allows for more finger/screen-friendly specification of
# attribute extraction
DAE = DatasetAttributeExtractor


@datasetmethod
def save(dataset, destination, name=None, compression=None):
    """Save Dataset into HDF5 file

    Parameters
    ----------
    dataset : `Dataset`
    destination : `h5py.highlevel.File` or str
    name : str, optional
    compression : None or int or {'gzip', 'szip', 'lzf'}, optional
      Level of compression for gzip, or another compression strategy.
    """
    if not externals.exists('h5py'):
        raise RuntimeError("Missing 'h5py' package -- saving is not possible.")

    import h5py
    from mvpa2.base.hdf5 import obj2hdf

    # look if we got an hdf file instance already
    if isinstance(destination, h5py.highlevel.File):
        own_file = False
        hdf = destination
    else:
        own_file = True
        hdf = h5py.File(destination, 'w')

    obj2hdf(hdf, dataset, name, compression=compression)

    # if we opened the file ourselves we close it now
    if own_file:
        hdf.close()
    return

########NEW FILE########
__FILENAME__ = dochelpers
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Various helpers to improve docstrings and textual output"""

__docformat__ = 'restructuredtext'

import re, textwrap

types = __import__('types')

# for table2string
import numpy as np
from math import ceil
from StringIO import StringIO
from mvpa2 import cfg

from mvpa2.base.externals import versions, exists
if __debug__:
    from mvpa2.base import debug

__add_init2doc = False
__in_ipython = exists('running ipython env')

# if ran within IPython -- might need to add doc to init
if __in_ipython:
    __rst_mode = False                       # either to do ReST links at all
    if versions['ipython'] <= '0.8.1':
        __add_init2doc = True
else:
    __rst_mode = True

#
# Predefine some sugarings depending on syntax convention to be used
#
# XXX Might need to be removed or become proper cfg parameter
__rst_conventions = 'numpy'
if __rst_conventions == 'epydoc':
    _rst_sep = "`"
    _rst_indentstr = "  "
    def _rst_section(section_name):
        """Provide section heading"""
        return ":%s:" % section_name
elif __rst_conventions == 'numpy':
    _rst_sep = ""
    _rst_indentstr = ""
    def _rst_section(section_name):
        """Provide section heading"""
        return "%s\n%s" % (section_name, '-'*len(section_name))
else:
    raise ValueError, "Unknown convention %s for RST" % __rst_conventions


def _rst(s, snotrst=''):
    """Produce s only in __rst mode"""
    if __rst_mode:
        return s
    else:
        return snotrst

def _rst_underline(text, markup):
    """Add and underline RsT string matching the length of the given string.
    """
    return text + '\n' + markup * len(text)


def single_or_plural(single, plural, n):
    """Little helper to spit out single or plural version of a word.
    """
    ni = int(n)
    if ni > 1 or ni == 0:
        # 1 forest, 2 forests, 0 forests
        return plural
    else:
        return single


def handle_docstring(text, polite=True):
    """Take care of empty and non existing doc strings."""
    if text is None or not len(text):
        if polite:
            return '' #No documentation found. Sorry!'
        else:
            return ''
    else:
        # Problem is that first line might often have no offset, so might
        # need to be ignored from dedent call
        if not text.startswith(' '):
            lines = text.split('\n')
            text2 = '\n'.join(lines[1:])
            return lines[0] + "\n" + textwrap.dedent(text2)
        else:
            return textwrap.dedent(text)


def _indent(text, istr=_rst_indentstr):
    """Simple indenter
    """
    return '\n'.join(istr + s for s in text.split('\n'))


__parameters_str_re = re.compile("[\n^]\s*:?Parameters?:?\s*\n(:?\s*-+\s*\n)?")
"""regexp to match :Parameter: and :Parameters: stand alone in a line
or
Parameters
----------
in multiple lines"""


def _split_out_parameters(initdoc):
    """Split documentation into (header, parameters, suffix)

    Parameters
    ----------
    initdoc : string
      The documentation string
    """

    # TODO: bind it to the only word in the line
    p_res = __parameters_str_re.search(initdoc)
    if p_res is None:
        return initdoc, "", ""
    else:
        # Could have been accomplished also via re.match

        # where new line is after :Parameters:
        # parameters header index
        ph_i = p_res.start()

        # parameters body index
        pb_i = p_res.end()

        # end of parameters
        try:
            pe_i = initdoc.index('\n\n', pb_i)
        except ValueError:
            pe_i = len(initdoc)

        result = initdoc[:ph_i].rstrip('\n '), \
                 initdoc[pb_i:pe_i], initdoc[pe_i:]

    # XXX a bit of duplication of effort since handle_docstring might
    # do splitting internally
    return handle_docstring(result[0], polite=False).strip('\n'), \
           textwrap.dedent(result[1]).strip('\n'), \
           textwrap.dedent(result[2]).strip('\n')


__re_params = re.compile('(?:\n\S.*?)+$')
__re_spliter1 = re.compile('(?:\n|\A)(?=\S)')
__re_spliter2 = re.compile('[\n:]')
def _parse_parameters(paramdoc):
    """Parse parameters and return list of (name, full_doc_string)

    It is needed to remove multiple entries for the same parameter
    like it could be with adding parameters from the parent class

    It assumes that previously parameters were unwrapped, so their
    documentation starts at the begining of the string, like what
    should it be after _split_out_parameters
    """
    entries = __re_spliter1.split(paramdoc)
    result = [(__re_spliter2.split(e)[0].strip(), e)
              for e in entries if e != '']
    if __debug__:
        debug('DOCH', 'parseParameters: Given "%s", we split into %s' %
              (paramdoc, result))
    return result

def get_docstring_split(f):
    """Given a function, break it up into portions

    Parameters
    ----------
    f : function

    Returns
    -------

    (initial doc string, params (as list of tuples), suffix string)
    """

    if not hasattr(f, '__doc__') or f.__doc__ in (None, ""):
        return None, None, None
    initdoc, params, suffix = _split_out_parameters(
        f.__doc__)
    params_list = _parse_parameters(params)
    return initdoc, params_list, suffix

def enhanced_doc_string(item, *args, **kwargs):
    """Generate enhanced doc strings for various items.

    Parameters
    ----------
    item : str or class
      What object requires enhancing of documentation
    *args : list
      Includes base classes to look for parameters, as well, first item
      must be a dictionary of locals if item is given by a string
    force_extend : bool
      Either to force looking for the documentation in the parents.
      By default force_extend = False, and lookup happens only if kwargs
      is one of the arguments to the respective function (e.g. item.__init__)
    skip_params : list of str
      List of parameters (in addition to [kwargs]) which should not
      be added to the documentation of the class.

    It is to be used from a collector, ie whenever class is already created
    """
    # Handling of arguments
    if len(kwargs):
        if set(kwargs.keys()).issubset(set(['force_extend'])):
            raise ValueError, "Got unknown keyword arguments (smth among %s)" \
                  " in enhanced_doc_string." % kwargs
    force_extend = kwargs.get('force_extend', False)
    skip_params = kwargs.get('skip_params', [])

    # XXX make it work also not only with classes but with methods as well
    if isinstance(item, basestring):
        if len(args)<1 or not isinstance(args[0], dict):
            raise ValueError, \
                  "Please provide locals for enhanced_doc_string of %s" % item
        name = item
        lcl = args[0]
        args = args[1:]
    elif hasattr(item, "im_class"):
        # bound method
        raise NotImplementedError, \
              "enhanced_doc_string is not yet implemented for methods"
    elif hasattr(item, "__name__"):
        name = item.__name__
        lcl = item.__dict__
    else:
        raise ValueError, "Don't know how to extend docstring for %s" % item

    # check whether docstring magic is requested or not
    if not cfg.getboolean('doc', 'pimp docstrings', True):
        return  lcl['__doc__']

    if __debug__:
        debug('DOCH', 'Processing docstrings of %s' % name)

    #return lcl['__doc__']
    rst_lvlmarkup = ["=", "-", "_"]

    # would then be called for any child... ok - ad hoc for SVM???
    if hasattr(item, '_customize_doc') and name=='SVM':
        item._customize_doc()

    initdoc = ""
    if lcl.has_key('__init__'):
        func = lcl['__init__']
        initdoc = func.__doc__

        skip_params += lcl.get('__init__doc__exclude__', [])

        # either to extend arguments
        # do only if kwargs is one of the arguments
        # in python 2.5 args are no longer in co_names but in varnames
        extend_args = force_extend or \
                      'kwargs' in (func.func_code.co_names +
                                   func.func_code.co_varnames)

        if __debug__ and not extend_args:
            debug('DOCH',
                  'Not extending parameters for __init__ of  %s',
                  (name,))

        if initdoc is None:
            initdoc = "Initialize instance of %s" % name

        initdoc, params, suffix = _split_out_parameters(initdoc)
        params_list = _parse_parameters(params)

        known_params = set([i[0] for i in params_list])

        # If there are additional ones:
        if lcl.has_key('_paramsdoc'):
            params_list += [i for i in lcl['_paramsdoc']
                            if not (i[0] in known_params)]
            known_params = set([i[0] for i in params_list])

        # no need for placeholders
        skip_params = set(skip_params + ['kwargs', '**kwargs'])

        # XXX we do evil check here, refactor code to separate
        #     regressions out of the classifiers, and making
        #     retrainable flag not available for those classes which
        #     can't actually do retraining. Although it is not
        #     actually that obvious for Meta Classifiers
        if hasattr(item, '__tags__'):
            clf_internals = item.__tags__
            skip_params.update([i for i in ('retrainable',)
                                if not (i in clf_internals)])

        known_params.update(skip_params)
        if extend_args:
            # go through all the parents and obtain their init parameters
            parent_params_list = []
            for i in args:
                if hasattr(i, '__init__'):
                    # XXX just assign within a class to don't redo without need
                    initdoc_ = i.__init__.__doc__
                    if initdoc_ is None:
                        continue
                    splits_ = _split_out_parameters(initdoc_)
                    params_ = splits_[1]
                    parent_params_list += _parse_parameters(params_.lstrip())

            # extend with ones which are not known to current init
            for i, v in parent_params_list:
                if not (i in known_params):
                    params_list += [(i, v)]
                    known_params.update([i])

        # if there are parameters -- populate the list
        if len(params_list):
            params_ = '\n'.join([i[1].rstrip() for i in params_list
                                 if not i[0] in skip_params])
            initdoc += "\n\n%s\n" \
                       % _rst_section('Parameters') + _indent(params_)

        if suffix != "":
            initdoc += "\n\n" + suffix

        initdoc = handle_docstring(initdoc)

        # Finally assign generated doc to the constructor
        lcl['__init__'].__doc__ = initdoc

    docs = [ handle_docstring(lcl['__doc__']) ]

    # Optionally populate the class documentation with it
    if __add_init2doc and initdoc != "":
        docs += [ _rst_underline('Constructor information for `%s` class'
                                 % name, rst_lvlmarkup[2]),
                  initdoc ]

    # Add information about the ca if available
    if lcl.has_key('_cadoc') and len(item._cadoc):
        # to don't conflict with Notes section if such was already
        # present
        lcldoc = lcl['__doc__'] or ''
        if not 'Notes' in lcldoc:
            section_name = _rst_section('Notes')
        else:
            section_name = '\n'         # just an additional newline
        # no indent is necessary since ca list must be already indented
        docs += ['%s\nAvailable conditional attributes:' % section_name,
                 handle_docstring(item._cadoc)]

    # Deprecated -- but actually we might like to have it in ipython
    # mode may be?
    if False: #len(args):
        bc_intro = _rst('  ') + 'Please refer to the documentation of the ' \
                   'base %s for more information:' \
                   % (single_or_plural('class', 'classes', len(args)))

        docs += ['\n' + _rst_section('See Also'),
                 bc_intro,
                 '  ' + ',\n  '.join(['%s%s.%s%s%s' % (_rst(':class:`~'),
                                                      i.__module__,
                                                      i.__name__,
                                                     _rst('`'),
                                                      _rst_sep)
                                      for i in args])
                ]

    itemdoc = '\n\n'.join(docs)
    # remove some bogus new lines -- never 3 empty lines in doc are useful
    result = re.sub("\s*\n\s*\n\s*\n", "\n\n", itemdoc)

    return result


def table2string(table, out=None):
    """Given list of lists figure out their common widths and print to out

    Parameters
    ----------
    table : list of lists of strings
      What is aimed to be printed
    out : None or stream
      Where to print. If None -- will print and return string

    Returns
    -------
    string if out was None
    """

    print2string = out is None
    if print2string:
        out = StringIO()

    # equalize number of elements in each row
    Nelements_max = len(table) \
                    and max(len(x) for x in table)

    for i, table_ in enumerate(table):
        table[i] += [''] * (Nelements_max - len(table_))

    # figure out lengths within each column
    atable = np.asarray(table)
    # eat whole entry while computing width for @w (for wide)
    markup_strip = re.compile('^@([lrc]|w.*)')
    col_width = [ max( [len(markup_strip.sub('', x))
                        for x in column] ) for column in atable.T ]
    string = ""
    for i, table_ in enumerate(table):
        string_ = ""
        for j, item in enumerate(table_):
            item = str(item)
            if item.startswith('@'):
                align = item[1]
                item = item[2:]
                if not align in ['l', 'r', 'c', 'w']:
                    raise ValueError, 'Unknown alignment %s. Known are l,r,c' % align
            else:
                align = 'c'

            NspacesL = max(ceil((col_width[j] - len(item))/2.0), 0)
            NspacesR = max(col_width[j] - NspacesL - len(item), 0)

            if align in ['w', 'c']:
                pass
            elif align == 'l':
                NspacesL, NspacesR = 0, NspacesL + NspacesR
            elif align == 'r':
                NspacesL, NspacesR = NspacesL + NspacesR, 0
            else:
                raise RuntimeError, 'Should not get here with align=%s' % align

            string_ += "%%%ds%%s%%%ds " \
                       % (NspacesL, NspacesR) % ('', item, '')
        string += string_.rstrip() + '\n'
    out.write(string)

    if print2string:
        value = out.getvalue()
        out.close()
        return value

def _saferepr(f):
    """repr which would not repr instances of bound methods since that might recurse

    See: bound methods to its instances
    https://github.com/PyMVPA/PyMVPA/issues/122
    """
    if type(f) == types.MethodType:
        objid = _strid(f.im_self) if __debug__ and 'ID_IN_REPR' in debug.active else ""
        return "<bound %s%s.%s>" % (f.im_class.__name__, objid, f.__func__.__name__)
    else:
        return repr(f)

def _repr_attrs(obj, attrs, default=None, error_value='ERROR'):
    """Helper to obtain a list of formatted attributes different from
    the default
    """
    out = []
    for a in attrs:
        v = getattr(obj, a, error_value)
        if not (v is default or isinstance(v, basestring) and v == default):
            out.append('%s=%s' % (a, _saferepr(v)))
    return out

def _repr(obj, *args, **kwargs):
    """Helper to get a structured __repr__ for all objects.

    Parameters
    ----------
    obj : object
      This will typically be `self` of the to be documented object.
    *args, **kwargs : str
      An arbitrary number of additional items. All of them must be of type
      `str`. All items will be appended comma separated to the class name.
      Keyword arguments will be appended as `key`=`value.

    Returns
    -------
    str
    """
    cls_name = obj.__class__.__name__
    truncate = cfg.get_as_dtype('verbose', 'truncate repr', int, default=200)
    # -5 to take (...) into account
    max_length = truncate - 5 - len(cls_name)
    if max_length < 0:
        max_length = 0
    auto_repr = ', '.join(list(args)
                   + ["%s=%s" % (k, v) for k, v in kwargs.iteritems()])


    if not truncate is None and len(auto_repr) > max_length:
        auto_repr = auto_repr[:max_length] + '...'

    # finally wrap in <> and return
    # + instead of '%s' for bits of speedup

    return "%s(%s)" % (cls_name, auto_repr)

def _strid(obj):
    """Helper for centralized string id representation in debug msgs
    """
    return "#%d" % (id(obj))

def _str(obj, *args, **kwargs):
    """Helper to get a structured __str__ for all objects.

    If an object has a `descr` attribute, its content will be used instead of
    an auto-generated description.

    Optional additional information might be added under certain debugging
    conditions (e.g. `id(obj)`).

    Parameters
    ----------
    obj : object
      This will typically be `self` of the to be documented object.
    *args, **kwargs : str
      An arbitrary number of additional items. All of them must be of type
      `str`. All items will be appended comma separated to the class name.
      Keyword arguments will be appended as `key`=`value.

    Returns
    -------
    str
    """
    truncate = cfg.get_as_dtype('verbose', 'truncate str', int, default=200)

    s = None
    # don't do descriptions for dicts like our collections as they might contain
    # an actual item 'descr'
    if hasattr(obj, 'descr') and not isinstance(obj, dict):
        s = obj.descr
    if s is None:
        s = obj.__class__.__name__
        auto_descr = ', '.join(list(args)
                       + ["%s=%s" % (k, v) for k, v in kwargs.iteritems()])
        if len(auto_descr):
            s = s + ': ' + auto_descr

    if not truncate is None and len(s) > truncate - 5:
        # -5 to take <...> into account
        s = s[:truncate-5] + '...'

    if __debug__ and 'DS_ID' in debug.active:
        # in case there was nothing but the class name
        if len(s):
            if s[-1]:
                s += ','
            s += ' '
        s += _strid(obj)

    # finally wrap in <> and return
    # + instead of '%s' for bits of speedup
    return '<' + s + '>'


def borrowdoc(cls, methodname=None):
    """Return a decorator to borrow docstring from another `cls`.`methodname`

    It should not be used for __init__ methods of classes derived from
    ClassWithCollections since __doc__'s of those are handled by the
    AttributeCollector anyways.

    Common use is to borrow a docstring from the class's method for an
    adapter function (e.g. sphere_searchlight borrows from Searchlight)

    Examples
    --------
    To borrow `__repr__` docstring from parent class `Mapper`, do::

       @borrowdoc(Mapper)
       def __repr__(self):
           ...

    Parameters
    ----------
    cls
      Usually a parent class
    methodname : None or str
      Name of the method from which to borrow.  If None, would use
      the same name as of the decorated method
    """

    def _borrowdoc(method):
        """Decorator which assigns to the `method` docstring from another
        """
        if methodname is None:
            other_method = getattr(cls, method.__name__)
        else:
            other_method = getattr(cls, methodname)
        if hasattr(other_method, '__doc__'):
            method.__doc__ = other_method.__doc__
        return method
    return _borrowdoc


def borrowkwargs(cls, methodname=None, exclude=None):
    """Return  a decorator which would borrow docstring for ``**kwargs``

    Notes
    -----
    TODO: take care about ``*args`` in  a clever way if those are also present

    Examples
    --------
    In the simplest scenario -- just grab all arguments from parent class::

           @borrowkwargs(A)
           def met1(self, bu, **kwargs):
               pass

    Parameters
    ----------
    methodname : None or str
      Name of the method from which to borrow.  If None, would use
      the same name as of the decorated method
    exclude : None or list of arguments to exclude
      If function does not pass all ``**kwargs``, you would need to list
      those here to be excluded from borrowed docstring
    """

    def _borrowkwargs(method):
        """Decorator which borrows docstrings for ``**kwargs`` for the `method`
        """
        if methodname is None:
            other_method = getattr(cls, method.__name__)
        else:
            other_method = getattr(cls, methodname)
        # TODO:
        # method.__doc__ = enhanced_from(other_method.__doc__)

        mdoc, odoc = method.__doc__, other_method.__doc__
        if mdoc is None:
            mdoc = ''

        mpreamble, mparams, msuffix = _split_out_parameters(mdoc)
        opreamble, oparams, osuffix = _split_out_parameters(odoc)
        mplist = _parse_parameters(mparams)
        oplist = _parse_parameters(oparams)
        known_params = set([i[0] for i in mplist])

        # !!! has to not rebind exclude variable
        skip_params = exclude or []         # handle None
        skip_params = set(['kwargs', '**kwargs'] + skip_params)

        # combine two and filter out items to skip
        aplist = [i for i in mplist if not i[0] in skip_params]
        aplist += [i for i in oplist
                   if not i[0] in skip_params.union(known_params)]

        docstring = mpreamble
        if len(aplist):
            params_ = '\n'.join([i[1].rstrip() for i in aplist])
            docstring += "\n\n%s\n" \
                         % _rst_section('Parameters') + _indent(params_)

        if msuffix != "":
            docstring += "\n\n" + msuffix

        docstring = handle_docstring(docstring)

        # Finally assign generated doc to the method
        method.__doc__ = docstring
        return method
    return _borrowkwargs

########NEW FILE########
__FILENAME__ = externals
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helper to verify presence of external libraries and modules
"""

__docformat__ = 'restructuredtext'
import os
import numpy as np                      # NumPy is required anyways

from mvpa2.base import warning
from mvpa2 import cfg
from mvpa2.misc.support import SmartVersion

if __debug__:
    from mvpa2.base import debug

class _VersionsChecker(dict):
    """Helper class to check the versions of the available externals
    """

    def __init__(self, *args, **kwargs):
        self._KNOWN = {}
        dict.__init__(self, *args, **kwargs)

    def __getitem__(self, key):
        if key not in self:
            if key in self._KNOWN:
                # run registered procedure to obtain versions
                self._KNOWN[key]()
            else:
                # just check for presence -- that function might set
                # the version information
                exists(key, force=True, raise_=True)
        return super(_VersionsChecker, self).__getitem__(key)

versions = _VersionsChecker()
"""Versions of available externals, as tuples
"""

def __assign_numpy_version():
    """Check if numpy is present (it must be) an if it is -- store its version
    """
    import numpy as np
    versions['numpy'] = SmartVersion(np.__version__)

def __check_numpy_correct_unique():
    """ndarray.unique fails to operate on heterogeneous object ndarrays
    See http://projects.scipy.org/numpy/ticket/2188
    """
    import numpy as np
    try:
        _ = np.unique(np.array([1, None, "str"]))
    except TypeError, e:
        raise RuntimeError("numpy.unique thrown %s" % e)

def __assign_scipy_version():
    # To don't allow any crappy warning to sneak in
    import warnings
    warnings.simplefilter('ignore', DeprecationWarning)
    try:
        import scipy as sp
    except:
        warnings.simplefilter('default', DeprecationWarning)
        raise
    warnings.simplefilter('default', DeprecationWarning)
    versions['scipy'] = SmartVersion(sp.__version__)

def __check_scipy():
    """Check if scipy is present an if it is -- store its version
    """
    exists('numpy', raise_=True)
    __assign_numpy_version()
    __assign_scipy_version()
    import scipy as sp

def _suppress_scipy_warnings():
    # Infiltrate warnings if necessary
    numpy_ver = versions['numpy']
    scipy_ver = versions['scipy']
    # There is way too much deprecation warnings spit out onto the
    # user. Lets assume that they should be fixed by scipy 0.7.0 time
    if scipy_ver >= "0.6.0" and scipy_ver < "0.7.0" \
        and numpy_ver > "1.1.0":
        import warnings
        if not __debug__ or (__debug__ and not 'PY' in debug.active):
            if __debug__:
                debug('EXT', "Setting up filters for numpy DeprecationWarnings")
            filter_lines = [
                ('NumpyTest will be removed in the next release.*',
                 DeprecationWarning),
                ('PyArray_FromDims: use PyArray_SimpleNew.',
                 DeprecationWarning),
                ('PyArray_FromDimsAndDataAndDescr: use PyArray_NewFromDescr.',
                 DeprecationWarning),
                # Trick re.match, since in warnings absent re.DOTALL in re.compile
                ('[\na-z \t0-9]*The original semantics of histogram is scheduled to be.*'
                 '[\na-z \t0-9]*', Warning) ]
            for f, w in filter_lines:
                warnings.filterwarnings('ignore', f, w)


def __assign_mdp_version():
    """Check if mdp is present (it must be) an if it is -- store its version
    """
    import mdp
    ver = mdp.__version__
    if SmartVersion(ver) == "2.5" and not hasattr(mdp.nodes, 'IdentityNode'):
        # Thanks to Yarik's shipment of svn snapshots into Debian we
        # can't be sure if that was already released version, since
        # mdp guys didn't use -dev suffix
        ver += '-dev'
    versions['mdp'] = SmartVersion(ver)

def __assign_nibabel_version():
    try:
        import nibabel
    except Exception, e:
        # FloatingError is defined in the same module which precludes
        # its specific except
        e_str = str(e)
        if "We had not expected long double type <type 'numpy.float128'>" in e_str:
            warning("Must be running under valgrind?  Available nibabel experiences "
                    "difficulty with float128 upon import and fails to work, thus is "
                    "report as N/A")
            raise ImportError("Fail to import nibabel due to %s" % e_str)
        raise
    versions['nibabel'] = SmartVersion(nibabel.__version__)

def __check_pywt(features=None):
    """Check for available functionality within pywt

    Parameters
    ----------
    features : list of str
      List of known features to check such as 'wp reconstruct',
      'wp reconstruct fixed'
    """
    import pywt
    import numpy as np
    data = np.array([ 0.57316901,  0.65292526,  0.75266733,  0.67020084,  0.46505364,
                     0.76478331,  0.33034164,  0.49165547,  0.32979941,  0.09696717,
                     0.72552711,  0.4138999 ,  0.54460628,  0.786351  ,  0.50096306,
                     0.72436454, 0.2193098 , -0.0135051 ,  0.34283984,  0.65596245,
                     0.49598417,  0.39935064,  0.26370727,  0.05572373,  0.40194438,
                     0.47004551,  0.60327258,  0.25628266,  0.32964893,  0.24009889,])
    mode = 'per'
    wp = pywt.WaveletPacket(data, 'sym2', mode)
    wp2 = pywt.WaveletPacket(data=None, wavelet='sym2', mode=mode)
    try:
        for node in wp.get_level(2): wp2[node.path] = node.data
    except:
        raise ImportError, \
               "Failed to reconstruct WP by specifying data in the layer"

    if 'wp reconstruct fixed' in features:
        rec = wp2.reconstruct()
        if np.linalg.norm(rec[:len(data)] - data) > 1e-3:
            raise ImportError, \
                  "Failed to reconstruct WP correctly"
    return True


def __check_libsvm_verbosity_control():
    """Check for available verbose control functionality
    """
    import mvpa2.clfs.libsvmc._svmc as _svmc
    try:
        _svmc.svm_set_verbosity(0)
    except:
        raise ImportError, "Provided version of libsvm has no way to control " \
              "its level of verbosity"

def __assign_shogun_version():
    """Assign shogun versions
    """
    if 'shogun' in versions:
        return
    import shogun.Classifier as __sc
    versions['shogun:rev'] = __sc.Version_get_version_revision()
    ver = __sc.Version_get_version_release().lstrip('v')
    versions['shogun:full'] = ver
    if '_' in ver:
        ver = ver[:ver.index('_')]
    versions['shogun'] = ver


def __check_shogun(bottom_version, custom_versions=[]):
    """Check if version of shogun is high enough (or custom known) to
    be enabled in the testsuite

    Parameters
    ----------
    bottom_version : int
      Bottom version which must be satisfied
    custom_versions : list of int
      Arbitrary list of versions which could got patched for
      a specific issue
    """
    import shogun.Classifier as __sc
    ver = __sc.Version_get_version_revision()
    __assign_shogun_version()
    if (ver in custom_versions) or (ver >= bottom_version):
        return True
    else:
        raise ImportError, 'Version %s is smaller than needed %s' % \
              (ver, bottom_version)

def __check_nipy_neurospin():
    from nipy.neurospin.utils import emp_nul

def __assign_skl_version():
    try:
        import sklearn as skl
    except ImportError:
        # Let's try older space
        import scikits.learn as skl
        if skl.__doc__ is None or skl.__doc__.strip() == "":
            raise ImportError("Verify your installation of scikits.learn. "
                              "Its docstring is empty -- could be that only -lib "
                              "was installed without the native Python modules")
    versions['skl'] = SmartVersion(skl.__version__)

def __check_weave():
    """Apparently presence of scipy is not sufficient since some
    versions experience problems. E.g. in Sep,Oct 2008 lenny's weave
    failed to work. May be some other converter could work (? See
    http://lists.debian.org/debian-devel/2008/08/msg00730.html for a
    similar report.

    Following simple snippet checks compilation of the basic code using
    weave
    """
    try:
        from scipy import weave
    except OSError, e:
        raise ImportError(
            "Weave cannot be used due to failure to import because of %s"
            % e)
    from scipy.weave import converters, build_tools
    import numpy as np
    # to shut weave up
    import sys
    # we can't rely on weave at all at the restoring argv. On etch box
    # restore_sys_argv() is apparently is insufficient
    oargv = sys.argv[:]
    ostdout = sys.stdout
    if not( __debug__ and 'EXT_' in debug.active):
        from StringIO import StringIO
        sys.stdout = StringIO()
        # *nix specific solution to shut weave up.
        # Some users must complain and someone
        # needs to fix this to become more generic.
        cargs = [">/dev/null", "2>&1"]
    else:
        cargs = []
    fmsg = None
    try:
        data = np.array([1,2,3])
        counter = weave.inline("data[0]=fabs(-1);", ['data'],
                               type_converters=converters.blitz,
                               verbose=0,
                               extra_compile_args=cargs,
                               compiler = 'gcc')
    except Exception, e:
        fmsg = "Failed to build simple weave sample." \
               " Exception was %s" % str(e)

    sys.stdout = ostdout
    # needed to fix sweave which might "forget" to restore sysv
    # build_tools.restore_sys_argv()
    sys.argv = oargv
    if fmsg is not None:
        raise ImportError, fmsg
    else:
        return "Everything is cool"


def __check_atlas_family(family):
    # XXX I guess pylint will dislike it a lot
    from mvpa2.atlases.warehouse import KNOWN_ATLAS_FAMILIES
    names, pathpattern = KNOWN_ATLAS_FAMILIES[family]
    filename = pathpattern % {'name':names[0]}
    if not os.path.exists(filename):
        raise ImportError, "Cannot find file %s for atlas family %s" \
              % (filename, family)
    pass


def __check_stablerdist():
    import scipy.stats
    import numpy as np
    ## Unfortunately 0.7.0 hasn't fixed the issue so no chance but to do
    ## a proper numerical test here
    try:
        scipy.stats.rdist(1.32, 0, 1).cdf(-1.0 + np.finfo(float).eps)
        # Actually previous test is insufficient for 0.6, so enabling
        # elderly test on top
        # ATM all known implementations which implement custom cdf for
        #     rdist are misbehaving, so there should be no _cdf
        distributions = scipy.stats.distributions
        if 'rdist_gen' in dir(distributions) \
            and ('_cdf' in distributions.rdist_gen.__dict__.keys()):
            raise ImportError, \
                  "scipy.stats carries misbehaving rdist distribution"
    except ZeroDivisionError:
        raise RuntimeError, "RDist in scipy is still unstable on the boundaries"


def __check_rv_discrete_ppf():
    """Unfortunately 0.6.0-12 of scipy pukes on simple ppf
    """
    import scipy.stats
    try:
        bdist = scipy.stats.binom(100, 0.5)
        bdist.ppf(0.9)
    except TypeError:
        raise RuntimeError, "pmf is broken in discrete dists of scipy.stats"

def __check_rv_continuous_reduce_func():
    """Unfortunately scipy 0.10.1 pukes when fitting with two params fixed
    """
    import scipy.stats as ss
    try:
        ss.t.fit(np.arange(6), floc=0.0, fscale=1.)
    except IndexError, e:
        raise RuntimeError("rv_continuous.fit can't candle 2 fixed params")

def __check_in_ipython():
    # figure out if ran within IPython
    if '__IPYTHON__' in globals()['__builtins__']:
        return
    raise RuntimeError, "Not running in IPython session"

def __assign_ipython_version():
    ipy_version = None
    try:
        # Development post 0.11 version finally carries
        # conventional one
        import IPython
        ipy_version = IPython.__version__
    except:
        try:
            from IPython import Release
            ipy_version = Release.version
        except:
            pass
        pass
    versions['ipython'] = SmartVersion(ipy_version)

def __check_openopt():
    m = None
    try:
        import openopt as m
    except ImportError:
        import scikits.openopt as m
    versions['openopt'] = m.__version__
    return True


def _set_matplotlib_backend():
    """Check if we have custom backend to set and it is different
    from current one
    """
    backend = cfg.get('matplotlib', 'backend')
    if backend:
        import matplotlib as mpl
        mpl_backend = mpl.get_backend().lower()
        if mpl_backend != backend.lower():
            if __debug__:
                debug('EXT_', "Trying to set matplotlib backend to %s" % backend)
            mpl.use(backend)
            import warnings
            # And disable useless warning from matplotlib in the future
            warnings.filterwarnings(
                'ignore', 'This call to matplotlib.use() has no effect.*',
                UserWarning)
        elif __debug__:
            debug('EXT_',
                  "Not trying to set matplotlib backend to %s since it was "
                  "already set" % backend)


def __assign_matplotlib_version():
    """Check for matplotlib version and set backend if requested."""
    import matplotlib
    versions['matplotlib'] = SmartVersion(matplotlib.__version__)
    _set_matplotlib_backend()

def __check_pylab():
    """Check if matplotlib is there and then pylab"""
    exists('matplotlib', raise_='always')
    import pylab as pl

def __check_pylab_plottable():
    """Simple check either we can plot anything using pylab.

    Primary use in unittests
    """
    try:
        exists('pylab', raise_='always')
        import pylab as pl
        fig = pl.figure()
        pl.plot([1,2], [1,2])
        pl.close(fig)
    except:
        raise RuntimeError, "Cannot plot in pylab"
    return True


def __check_griddata():
    """griddata might be independent module or part of mlab
    """

    try:
        from griddata import griddata as __
        return True
    except ImportError:
        if __debug__:
            debug('EXT_', 'No python-griddata available')

    from matplotlib.mlab import griddata as __
    return True


def __check_reportlab():
    import reportlab as rl
    versions['reportlab'] = SmartVersion(rl.Version)

def __check(name, a='__version__'):
    exec "import %s" % name
    try:
        exec "v = %s.%s" % (name, a)
        # it might be lxml.etree, so take only first module
        versions[name.split('.')[0]] = SmartVersion(v)
    except Exception, e:
        # we can't assign version but it is there
        if __debug__:
            debug('EXT', 'Failed to acquire a version of %(name)s: %(e)s'
                  % locals())
        pass
    return True

def __check_h5py():
    __check('h5py', 'version.version')
    import h5py
    versions['hdf5'] = SmartVersion(h5py.version.hdf5_version)

def __check_rpy():
    """Check either rpy is available and also set it for the sane execution
    """
    #import rpy_options
    #rpy_options.set_options(VERBOSE=False, SETUP_READ_CONSOLE=False) # SETUP_WRITE_CONSOLE=False)
    #rpy_options.set_options(VERBOSE=False, SETUP_WRITE_CONSOLE=False) # SETUP_WRITE_CONSOLE=False)
    #    if not cfg.get('rpy', 'read_console', default=False):
    #        print "no read"
    #        rpy_options.set_options(SETUP_READ_CONSOLE=False)
    #    if not cfg.get('rpy', 'write_console', default=False):
    #        print "no write"
    #        rpy_options.set_options(SETUP_WRITE_CONSOLE=False)
    import rpy
    if not cfg.getboolean('rpy', 'interactive', default=True) \
           and (rpy.get_rpy_input() is rpy.rpy_io.rpy_input):
        if __debug__:
            debug('EXT_', "RPy: providing dummy callback for input to return '1'")
        def input1(*args): return "1"      # which is "1: abort (with core dump, if enabled)"
        rpy.set_rpy_input(input1)

def _R_library(libname):
    import rpy2.robjects as ro

    try:
        if not tuple(ro.r(
            "suppressMessages(suppressWarnings(require(%r, quiet=TRUE)))"
            % libname))[0]:
            raise ImportError("It seems that R cannot load library %r"
                              % libname)
    except Exception, e:
        raise ImportError("Failed to load R library %r due to %s"
                          % (libname, e))

def __check_rpy2():
    """Check either rpy2 is available and also set it for the sane execution
    """
    import rpy2
    versions['rpy2'] = SmartVersion(rpy2.__version__)

    import rpy2.robjects
    r = rpy2.robjects.r
    r.options(warn=cfg.get_as_dtype('rpy', 'warn', dtype=int, default=-1))

    # To shut R up while it is importing libraries to do not ruin out
    # doctests
    r.library = _R_library

def __check_liblapack_so():
    """Check either we could load liblapack.so library via ctypes
    """
    from ctypes import cdll
    try:
        lapacklib = cdll.LoadLibrary('liblapack.so')
    except OSError, e:
        # reraise with exception type we catch/handle while testing externals
        raise RuntimeError("Failed to import liblapack.so: %s" % e)

# contains list of available (optional) external classifier extensions
_KNOWN = {'libsvm':'import mvpa2.clfs.libsvmc._svm as __; x=__.seq_to_svm_node',
          'libsvm verbosity control':'__check_libsvm_verbosity_control();',
          'nibabel':'__assign_nibabel_version()',
          'ctypes':'__check("ctypes")',
          'liblapack.so': "__check_liblapack_so()",
          'shogun':'__assign_shogun_version()',
          'shogun.krr': '__assign_shogun_version(); import shogun.Regression as __; x=__.KRR',
          'shogun.mpd': '__assign_shogun_version(); import shogun.Classifier as __; x=__.MPDSVM',
          'shogun.lightsvm': '__assign_shogun_version(); import shogun.Classifier as __; x=__.SVMLight',
          'shogun.svmocas': '__assign_shogun_version(); import shogun.Classifier as __; x=__.SVMOcas',
          'shogun.svrlight': '__assign_shogun_version(); from shogun.Regression import SVRLight as __',
          'numpy': "__assign_numpy_version()",
          'numpy_correct_unique': "__check_numpy_correct_unique()",
          'numpydoc': "import numpydoc",
          'scipy': "__check_scipy()",
          'good scipy.stats.rdist': "__check_stablerdist()",
          'good scipy.stats.rv_discrete.ppf': "__check_rv_discrete_ppf()",
          'good scipy.stats.rv_continuous._reduce_func(floc,fscale)': "__check_rv_continuous_reduce_func()",
          'weave': "__check_weave()",
          'pywt': "import pywt as __",
          'pywt wp reconstruct': "__check_pywt(['wp reconstruct'])",
          'pywt wp reconstruct fixed': "__check_pywt(['wp reconstruct fixed'])",
          #'rpy': "__check_rpy()",
          'rpy2': "__check_rpy2()",
          'lars': "exists('rpy2', raise_='always');" \
                  "import rpy2.robjects; rpy2.robjects.r.library('lars')",
          'mass': "exists('rpy2', raise_='always');" \
                  "import rpy2.robjects; rpy2.robjects.r.library('MASS')",
          'elasticnet': "exists('rpy2', raise_='always'); "\
                  "import rpy2.robjects; rpy2.robjects.r.library('elasticnet')",
          'glmnet': "exists('rpy2', raise_='always'); " \
                  "import rpy2.robjects; rpy2.robjects.r.library('glmnet')",
          'cran-energy': "exists('rpy2', raise_='always'); " \
                  "import rpy2.robjects; rpy2.robjects.r.library('energy')",
          'matplotlib': "__assign_matplotlib_version()",
          'pylab': "__check_pylab()",
          'pylab plottable': "__check_pylab_plottable()",
          'openopt': "__check_openopt()",
          'skl': "__assign_skl_version()",
          'mdp': "__assign_mdp_version()",
          'mdp ge 2.4': "from mdp.nodes import LLENode as __",
          'sg_fixedcachesize': "__check_shogun(3043, [2456])",
           # 3318 corresponds to release 0.6.4
          'sg ge 0.6.4': "__check_shogun(3318)",
           # 3377 corresponds to release 0.6.5
          'sg ge 0.6.5': "__check_shogun(3377)",
          'hcluster': "import hcluster as __",
          'griddata': "__check_griddata()",
          'cPickle': "import cPickle as __",
          'gzip': "import gzip as __",
          'lxml': "__check('lxml.etree', '__version__');"
                  "from lxml import objectify as __",
          'atlas_pymvpa': "__check_atlas_family('pymvpa')",
          'atlas_fsl': "__check_atlas_family('fsl')",
          'ipython': "__assign_ipython_version()",
          'running ipython env': "__check_in_ipython()",
          'reportlab': "__check('reportlab', 'Version')",
          'nose': "import nose as __",
          'pprocess': "__check('pprocess')",
          'pywt': "__check('pywt')",
          'h5py': "__check_h5py()",
          'hdf5': "__check_h5py()",
          'nipy': "__check('nipy')",
          'nipy.neurospin': "__check_nipy_neurospin()",
          'statsmodels': 'import statsmodels.api as __',
          'mock': "__check('mock')",
          }


def exists(dep, force=False, raise_=False, issueWarning=None,
           exception=RuntimeError):
    """
    Test whether a known dependency is installed on the system.

    This method allows us to test for individual dependencies without
    testing all known dependencies. It also ensures that we only test
    for a dependency once.

    Parameters
    ----------
    dep : string or list of string
      The dependency key(s) to test.
    force : boolean
      Whether to force the test even if it has already been
      performed.
    raise_ : boolean, str
      Whether to raise an exception if dependency is missing.
      If True, it is still conditioned on the global setting
      MVPA_EXTERNALS_RAISE_EXCEPTION, while would raise exception
      if missing despite the configuration if 'always'.
    issueWarning : string or None or True
      If string, warning with given message would be thrown.
      If True, standard message would be used for the warning
      text.
    exception : exception, optional
      What exception to raise.  Defaults to RuntimeError
    """
    # if we are provided with a list of deps - go through all of them
    if isinstance(dep, list) or isinstance(dep, tuple):
        results = [ exists(dep_, force, raise_) for dep_ in dep ]
        return bool(reduce(lambda x,y: x and y, results, True))

    # where to look in cfg
    cfgid = 'have ' + dep

    # pre-handle raise_ according to the global settings and local argument
    if isinstance(raise_, str):
        if raise_.lower() == 'always':
            raise_ = True
        else:
            raise ValueError("Unknown value of raise_=%s. "
                             "Must be bool or 'always'" % raise_)
    else: # must be bool conditioned on the global settings
        raise_ = raise_ \
                and cfg.getboolean('externals', 'raise exception', True)

    # prevent unnecessarry testing
    if cfg.has_option('externals', cfgid) \
       and not cfg.getboolean('externals', 'retest', default='no') \
       and not force:
        if __debug__:
            debug('EXT', "Skip retesting for '%s'." % dep)

        # check whether an exception should be raised, even though the external
        # was already tested previously
        if not cfg.getboolean('externals', cfgid) and raise_:
            raise exception, "Required external '%s' was not found" % dep
        return cfg.getboolean('externals', cfgid)


    # determine availability of external (non-cached)

    # default to 'not found'
    result = False

    if dep not in _KNOWN:
        raise ValueError, "%r is not a known dependency key." % (dep,)
    else:
        # try and load the specific dependency
        if __debug__:
            debug('EXT', "Checking for the presence of %s" % dep)

        # Exceptions which are silently caught while running tests for externals
        _caught_exceptions = [ImportError, AttributeError, RuntimeError]

        try:
            # Suppress NumPy warnings while testing for externals
            olderr = np.seterr(all="ignore")

            estr = ''
            try:
                exec _KNOWN[dep]
                result = True
            except tuple(_caught_exceptions), e:
                estr = ". Caught exception was: " + str(e)
            except Exception, e:
                # Add known ones by their names so we don't need to
                # actually import anything manually to get those classes
                if e.__class__.__name__ in ['RPy_Exception', 'RRuntimeError',
                                            'RPy_RException']:
                    _caught_exceptions += [e.__class__]
                    estr = ". Caught exception was: " + str(e)
                else:
                    raise
        finally:
            # And restore warnings
            np.seterr(**olderr)

        if __debug__:
            debug('EXT', "Presence of %s is%s verified%s" %
                  (dep, {True:'', False:' NOT'}[result], estr))

    if not result:
        if raise_:
            raise exception, "Required external '%s' was not found" % dep
        if issueWarning is not None \
               and cfg.getboolean('externals', 'issue warning', True):
            if issueWarning is True:
                warning("Required external '%s' was not found" % dep)
            else:
                warning(issueWarning)


    # store result in config manager
    if not cfg.has_section('externals'):
        cfg.add_section('externals')
    if result:
        cfg.set('externals', 'have ' + dep, 'yes')
    else:
        cfg.set('externals', 'have ' + dep, 'no')

    return result

# Bind functions for some versions checkings
versions._KNOWN.update({
    'shogun' : __assign_shogun_version,
    'shogun:rev' : __assign_shogun_version,
    'shogun:full' : __assign_shogun_version,
    })


##REF: Name was automagically refactored
def check_all_dependencies(force=False, verbosity=1):
    """
    Test for all known dependencies.

    Parameters
    ----------
    force : boolean
      Whether to force the test even if it has already been
      performed.

    """
    # loop over all known dependencies
    for dep in _KNOWN:
        if not exists(dep, force):
            if verbosity:
                warning("%s is not available." % dep)

    if __debug__:
        debug('EXT', 'The following optional externals are present: %s' \
                     % [ k[5:] for k in cfg.options('externals')
                            if k.startswith('have') \
                            and cfg.getboolean('externals', k) == True ])

########NEW FILE########
__FILENAME__ = hdf5
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""HDF5-based file IO for PyMVPA objects.

Based on the `h5py` package, this module provides two functions (`obj2hdf()`
and `hdf2obj()`, as well as the convenience functions `h5save()` and
`h5load()`) to store (in principle) arbitrary Python objects into HDF5 groups,
and using HDF5 as input, convert them back into Python object instances.

Similar to `pickle` a Python object is disassembled into its pieces, but instead
of serializing it into a byte-stream it is stored in chunks which type can be
natively stored in HDF5. That means basically everything that can be stored in
a NumPy array.

If an object is not readily storable, its `__reduce__()` method is called to
disassemble it into basic pieces.  The default implementation of
`object.__reduce__()` is typically sufficient. Hence, for any new-style Python
class there is, in general, no need to implement `__reduce__()`. However, custom
implementations might allow for leaner HDF5 representations and leaner files.
Basic types, such as `list`, and `dict`, whose `__reduce__()` method does not do
help with disassembling are also handled.

.. warning::

  Although, in principle, storage and reconstruction of arbitrary object types
  is possible, it might not be implemented yet. The current focus lies on
  storage of PyMVPA datasets and their attributes (e.g. Mappers).
"""

__docformat__ = 'restructuredtext'

import types
import numpy as np
import h5py

import os
import os.path as osp

import mvpa2
from mvpa2.base import externals
from mvpa2.base.types import asobjarray

if __debug__:
    from mvpa2.base import debug

# don't ask -- makes history re-education a breeze
universal_classname_remapper = {
    ('mvpa2.mappers.base', 'FeatureSliceMapper'):
        ('mvpa2.featsel.base', 'StaticFeatureSelection'),
}
# Comment: H5Py defines H5Error
class HDF5ConversionError(Exception):
    """Generic exception to be thrown while doing conversions to/from HDF5
    """
    pass

def hdf2obj(hdf, memo=None):
    """Convert an HDF5 group definition into an object instance.

    Obviously, this function assumes the conventions implemented in the
    `obj2hdf()` function. Those conventions will eventually be documented in
    the module docstring, whenever they are sufficiently stable.

    Parameters
    ----------
    hdf : HDF5 group instance
      HDF5 group instance. this could also be an HDF5 file instance.
    memo : dict
      Dictionary tracking reconstructed objects to prevent recursions (analog to
      deepcopy).

    Notes
    -----
    Although, this function uses a way to reconstruct object instances that is
    similar to unpickling, it should be *relatively* safe to open HDF files
    from untrusted sources. Only basic datatypes are stored in HDF files, and
    there is no foreign code that is executed during reconstructing. For that
    reason, any type that shall be reconstructed needs to be importable
    (importing is done be fully-qualified module names).

    Returns
    -------
    object instance
    """
    if memo is None:
        # init object tracker
        memo = {}
    # note, older file formats did not store objrefs
    if 'objref' in hdf.attrs:
        objref = hdf.attrs['objref']
    else:
        objref = None

    # if this HDF group has an objref that points to an already recontructed
    # object, simple return this object again
    if not objref is None and objref in memo:
        obj = memo[objref]
        if __debug__:
            debug('HDF5', "Use tracked object %s (%i)" % (type(obj), objref))
        return obj

    #
    # Actual data
    #
    if isinstance(hdf, h5py.Dataset):
        if __debug__:
            debug('HDF5', "Load from HDF5 dataset [%s]" % hdf.name)
        if 'is_scalar' in hdf.attrs:
            # extract the scalar from the 0D array
            obj = hdf[()]
            # and coerce it back into the native Python type if necessary
            if issubclass(type(obj), np.generic):
                obj = np.asscalar(obj)
        elif 'is_numpy_scalar' in hdf.attrs:
            # extract the scalar from the 0D array as is
            obj = hdf[()]
        else:
            # read array-dataset into an array
            obj = np.empty(hdf.shape, hdf.dtype)
            if obj.size:
                hdf.read_direct(obj)
    else:
        # check if we have a class instance definition here
        if not ('class' in hdf.attrs or 'recon' in hdf.attrs):
            raise LookupError("Found hdf group without class instance "
                    "information (group: %s). Cannot convert it into an "
                    "object (content: '%s', attributes: '%s')."
                    % (hdf.name, hdf.keys(), hdf.attrs.keys()))

        mod_name = hdf.attrs['module']

        if 'recon' in hdf.attrs:
            # Custom objects custom reconstructor
            obj = _recon_customobj_customrecon(hdf, memo)
        elif mod_name != '__builtin__':
            # Custom objects default reconstructor
            cls_name = hdf.attrs['class']
            if cls_name in ('function', 'type', 'builtin_function_or_method'):
                # Functions and types
                obj = _recon_functype(hdf)
            else:
                # Other custom objects
                obj = _recon_customobj_defaultrecon(hdf, memo)
        else:
            # Built-in objects
            cls_name = hdf.attrs['class']
            if __debug__:
                debug('HDF5', "Reconstructing built-in object '%s'." % cls_name)
            # built in type (there should be only 'list', 'dict' and 'None'
            # that would not be in a Dataset
            if cls_name == 'NoneType':
                obj = None
            elif cls_name == 'tuple':
                obj = _hdf_tupleitems_to_obj(hdf, memo)
            elif cls_name == 'list':
                # could be used also for storing object ndarrays
                if 'is_objarray' in hdf.attrs:
                    obj = _hdf_list_to_objarray(hdf, memo)
                else:
                    obj = _hdf_list_to_obj(hdf, memo)
            elif cls_name == 'dict':
                obj = _hdf_dict_to_obj(hdf, memo)
            elif cls_name == 'type':
                obj = eval(hdf.attrs['name'])
            elif cls_name == 'function':
                raise RuntimeError("Unhandled reconstruction of built-in "
                        "function (at '%s')." % hdf.name)
            else:
                raise RuntimeError("Found hdf group with a builtin type "
                        "that is not handled by the parser (group: %s). This "
                        "is a conceptual bug in the parser. Please report."
                        % hdf.name)
    #
    # Final post-processing
    #

    # track if desired
    if objref:
        if __debug__:
            debug('HDF5', "Placing %s objref '%s' to memo", (obj, objref))
        memo[objref] = obj
    if __debug__:
        debug('HDF5', "Done loading %s [%s]"
                      % (type(obj), hdf.name))
    return obj


def _recon_functype(hdf):
    """Reconstruct a function or type from HDF"""
    cls_name = hdf.attrs['class']
    mod_name = hdf.attrs['module']
    ft_name = hdf.attrs['name']
    if __debug__:
        debug('HDF5', "Load '%s.%s.%s' [%s]"
                      % (mod_name, cls_name, ft_name, hdf.name))
    mod, obj = _import_from_thin_air(mod_name, ft_name, cls_name=cls_name)
    return obj

def _get_subclass_entry(cls, clss, exc_msg="", exc=NotImplementedError):
    """In a list of tuples (cls, ...) return the entry for the first
    occurrence of the class of which `cls` is a subclass of.
    Otherwise raise `exc` with the given message"""

    for clstuple in clss:
        if issubclass(cls, clstuple[0]):
            return clstuple
    raise exc(exc_msg % locals())

def _update_obj_state_from_hdf(obj, hdf, memo):
    if 'state' in hdf:
        # insert the state of the object
        if __debug__:
            debug('HDF5', "Populating instance state.")
        if hasattr(obj, '__setstate__'):
            state = hdf2obj(hdf['state'], memo)
            obj.__setstate__(state)
        else:
            state = _hdf_dict_to_obj(hdf['state'], memo)
            if state:
                obj.__dict__.update(state)
        if __debug__:
            debug('HDF5', "Updated %i state items." % len(state))

def _recon_customobj_customrecon(hdf, memo):
    """Reconstruct a custom object from HDF using a custom recontructor"""
    # we found something that has some special idea about how it wants
    # to be reconstructed
    mod_name = hdf.attrs['module']
    recon_name = hdf.attrs['recon']
    if __debug__:
        debug('HDF5', "Load from custom reconstructor '%s.%s' [%s]"
                      % (mod_name, recon_name, hdf.name))
    # turn names into definitions
    mod, recon = _import_from_thin_air(mod_name, recon_name)

    obj = None
    if 'rcargs' in hdf:
        recon_args_hdf = hdf['rcargs']
        if __debug__:
            debug('HDF5', "Load reconstructor args in [%s]"
                          % recon_args_hdf.name)
        if 'objref' in hdf.attrs:
            # XXX TODO YYY ZZZ WHATEVER
            # yoh: the problem is that inside this beast might be references
            # to current, not yet constructed object, and if we follow
            # Python docs we should call recon with *recon_args, thus we
            # cannot initiate the beast witout them.  But if recon is a class
            # with __new__ we could may be first __new__ and then only __init__
            # with recon_args?
            if '__new__' in dir(recon):
                try:
                    # TODO: what if multiple inheritance?
                    obj = recon.__bases__[0].__new__(recon)
                except:
                    # try direct __new__
                    try:
                        obj = recon.__new__()
                    except:
                        # give up and hope for the best
                        obj = None
                if obj is not None:
                    memo[hdf.attrs['objref']] = obj
        recon_args = _hdf_tupleitems_to_obj(recon_args_hdf, memo)
    else:
        recon_args = ()

    # reconstruct
    if obj is None:
        obj = recon(*recon_args)
    else:
        # Let only to init it
        obj.__init__(*recon_args)
    # insert any stored object state
    _update_obj_state_from_hdf(obj, hdf, memo)
    return obj


def _import_from_thin_air(mod_name, importee, cls_name=None):
    if cls_name is None:
        cls_name = importee
    try:
        mod = __import__(mod_name, fromlist=[importee])
    except ImportError, e:
        if mod_name.startswith('mvpa') and not mod_name.startswith('mvpa2'):
            # try to be gentle on data that got stored with PyMVPA 0.5 or 0.6
            mod_name = mod_name.replace('mvpa', 'mvpa2', 1)
            mod = __import__(mod_name, fromlist=[cls_name])
        else:
            raise e
    try:
        imp = mod.__dict__[importee]
    except KeyError:
        mod_name, importee = universal_classname_remapper[(mod_name, importee)]
        mod = __import__(mod_name, fromlist=[cls_name])
        imp = mod.__dict__[importee]
    return mod, imp


def _recon_customobj_defaultrecon(hdf, memo):
    """Reconstruct a custom object from HDF using the default recontructor"""
    cls_name = hdf.attrs['class']
    mod_name = hdf.attrs['module']
    if __debug__:
        debug('HDF5', "Load class instance '%s.%s' instance [%s]"
                      % (mod_name, cls_name, hdf.name))
    mod, cls = _import_from_thin_air(mod_name, cls_name)

    # create the object
    # use specialized __new__ if necessary or beneficial
    pcls, = _get_subclass_entry(cls, ((dict,), (list,), (object,)),
                                "Do not know how to create instance of %(cls)s")
    obj = pcls.__new__(cls)
    # insert any stored object state
    _update_obj_state_from_hdf(obj, hdf, memo)

    # do we process a container?
    if 'items' in hdf:
        # charge the items -- handling depends on the parent class
        pcls, umeth, cfunc = _get_subclass_entry(
            cls,
            ((dict, 'update', _hdf_dict_to_obj),
             (list, 'extend', _hdf_list_to_obj)),
            "Unhandled container type (got: '%(cls)s').")
        if __debug__:
            debug('HDF5', "Populating %s object." % pcls)
        getattr(obj, umeth)(cfunc(hdf, memo))
        if __debug__:
            debug('HDF5', "Loaded %i items." % len(obj))

    return obj


def _hdf_dict_to_obj(hdf, memo, skip=None):
    if skip is None:
        skip = []
    # legacy compat code
    if not 'items' in hdf:
        items_container = hdf
    # end of legacy compat code
    else:
        items_container = hdf['items']

    if items_container.attrs.get('__keys_in_tuple__', 0):
        # pre-create the object so it could be correctly
        # objref'ed/used in memo
        d = dict()
        items = _hdf_list_to_obj(hdf, memo, target_container=d)
        # some time back we had attribute names stored as arrays
        for k, v in items:
            if k in skip:
                continue
            try:
                d[k] = v
            except TypeError:
                # fucked up dataset -- trying our best
                if isinstance(k, np.ndarray):
                    d[np.asscalar(k)] = v
                else:
                    # no idea, really
                    raise
        return d
    else:
        # legacy files had keys as group names
        return dict([(item, hdf2obj(items_container[item], memo=memo))
                        for item in items_container
                            if not item in skip])

def _hdf_list_to_objarray(hdf, memo):
    if not ('shape' in hdf.attrs):
        if __debug__:
            debug('HDF5', "Enountered objarray stored without shape (due to a bug "
                "in post 2.1 release).  Some nested structures etc might not be "
                "loaded incorrectly")
        # yoh: we have possibly a problematic case due to my fix earlier
        # resolve to old logic:  nested referencing might not work :-/
        obj = _hdf_list_to_obj(hdf, memo)
        # need to handle special case of arrays of objects
        if np.isscalar(obj):
            obj = np.array(obj, dtype=np.object)
        else:
            obj = asobjarray(obj)
    else:
        shape = tuple(hdf.attrs['shape'])
        # reserve space first
        if len(shape):
            obj = np.empty(np.prod(shape), dtype=object)
        else:
            # scalar
            obj = np.array(None, dtype=object)
        # now load the items from the list, noting existence of this
        # container
        obj_items = _hdf_list_to_obj(hdf, memo, target_container=obj)
        # assign to the object array
        for i, v in enumerate(obj_items):
            obj[i] = v
        if len(shape) and shape != obj.shape:
            obj = obj.reshape(shape)
    return obj

def _hdf_list_to_obj(hdf, memo, target_container=None):
    """Convert an HDF item sequence into a list

    Lists are used for storing also dicts.  To properly reference
    the actual items in memo, target_container could be specified
    to point to the actual data structure to be referenced, which
    later would get populated with list's items.
    """
    # new-style files have explicit length
    if 'length' in hdf.attrs:
        length = hdf.attrs['length']
        if __debug__:
            debug('HDF5', "Found explicit sequence length setting (%i)"
                          % length)
        hdf_items = hdf['items']
    elif 'items' in hdf:
        # not so legacy file, at least has an items container
        length = len(hdf['items'])
        if __debug__:
            debug('HDF5', "No explicit sequence length setting (guess: %i)"
                          % length)
        hdf_items = hdf['items']
    # legacy compat code
    else:
        length = len(hdf)
        if __debug__:
            debug('HDF5', "Ancient file, guessing sequence length (%i)"
                          % length)
        # really legacy file, not even items container
        hdf_items = hdf
    # end of legacy compat code

    # prepare item list
    items = [None] * length
    # need to put items list in memo before starting to parse to allow to detect
    # self-inclusion of this list in itself
    if 'objref' in hdf.attrs:
        objref = hdf.attrs['objref']
        if target_container is None:
            if __debug__:
                debug('HDF5', "Track sequence with %i elements under objref '%s'"
                              % (length, objref))
            memo[objref] = items
        else:
            if __debug__:
                debug('HDF5', "Track provided target_container under objref '%s'",
                      objref)
            memo[objref] = target_container
    # for all expected items
    for i in xrange(length):
        if __debug__:
            debug('HDF5', "Item %i" % i)
        str_i = str(i)
        obj = None
        objref = None
        # we need a separate flag, see below
        got_obj = False
        # do we have an item attribute for this item (which is the objref)
        if str_i in hdf_items.attrs:
            objref = hdf_items.attrs[str_i]
        # do we have an actual value for this item
        if str_i in hdf_items:
            obj = hdf2obj(hdf_items[str_i], memo=memo)
            # we need to signal that we got something, since it could as well
            # be None
            got_obj = True
        if not got_obj:
            # no actual value for item
            if objref is None:
                raise LookupError("Cannot find list item '%s'" % str_i)
            else:
                # no value but reference -> value should be in memo
                if objref in memo:
                    if __debug__:
                        debug('HDF5', "Use tracked object (%i)"
                                      % objref)
                    items[i] = memo[objref]
                else:
                    raise LookupError("No value for objref '%i'" % objref)
        else:
            # we have a value for this item
            items[i] = obj
            # store value for ref if present
            if not objref is None:
                memo[objref] = obj

    return items


def _hdf_tupleitems_to_obj(hdf, memo):
    """Same as _hdf_list_to_obj, but converts to tuple upon return"""
    return tuple(_hdf_list_to_obj(hdf, memo))


def _seqitems_to_hdf(obj, hdf, memo, noid=False, **kwargs):
    """Store a sequence as HDF item list"""
    hdf.attrs.create('length', len(obj))
    items = hdf.create_group('items')
    for i, item in enumerate(obj):
        if __debug__:
            debug('HDF5', "Item %i" % i)
        obj2hdf(items, item, name=str(i), memo=memo, noid=noid, **kwargs)


def obj2hdf(hdf, obj, name=None, memo=None, noid=False, **kwargs):
    """Store an object instance in an HDF5 group.

    A given object instance is (recursively) disassembled into pieces that are
    storable in HDF5. In general, any pickable object should be storable, but
    since the parser is not complete, it might not be possible (yet).

    .. warning::

      Currently, the parser does not track recursions. If an object contains
      recursive references all bets are off. Here be dragons...

    Parameters
    ----------
    hdf : HDF5 group instance
      HDF5 group instance. this could also be an HDF5 file instance.
    obj : object instance
      Object instance that shall be stored.
    name : str or None
      Name of the object. In case of a complex object that cannot be stored
      natively without disassembling them, this is going to be a new group,
      Otherwise the name of the dataset. If None, no new group is created.
    memo : dict
      Dictionary tracking stored objects to prevent recursions (analog to
      deepcopy).
    noid : bool
      If True, the to be processed object has no usable id. Set if storing
      objects that were created temporarily, e.g. during type conversions.
    **kwargs
      All additional arguments will be passed to `h5py.Group.create_dataset()`
    """
    if memo is None:
        # initialize empty recursion tracker
        memo = {}

    #
    # Catch recursions: just stored references to already known objects
    #
    if noid:
        # noid: tracking this particular object is not intended
        obj_id = 0
    else:
        obj_id = id(obj)
    if not noid and obj_id in memo:
        # already in here somewhere, nothing else but reference needed
        # this can also happen inside containers, so 'name' should not be None
        hdf.attrs.create(name, obj_id)
        if __debug__:
            debug('HDF5', "Store '%s' by objref: %i" % (type(obj), obj_id))
        # done
        return

    #
    # Ugly special case of arrays of objects
    #
    is_objarray = False                # assume the bright side ;-)
    is_ndarray = isinstance(obj, np.ndarray)
    if is_ndarray:
        if obj.dtype == np.object:
            shape = obj.shape
            if not len(obj.shape):
                # even worse: 0d array
                # we store 0d object arrays just by content
                if __debug__:
                    debug('HDF5', "0d array(object) -> object")
                obj = obj[()]
            else:
                # proper arrays can become lists
                if __debug__:
                    debug('HDF5', "array(objects) -> list(objects)")
                obj = list(obj.flatten())
                # make sure we don't ref this temporary list object
                # noid = True
                # yoh: obj_id is of the original obj here so should
                # be stored
            # flag that we messed with the original type
            is_objarray = True
            # and re-estimate the content's nd-array-ness
            is_ndarray = isinstance(obj, np.ndarray)

    # if it is something that can go directly into HDF5, put it there
    # right away
    is_scalar = np.isscalar(obj)
    if is_scalar or is_ndarray:
        is_numpy_scalar = issubclass(type(obj), np.generic)
        if name is None:
            # HDF5 cannot handle datasets without a name
            name = '__unnamed__'
        if __debug__:
            debug('HDF5', "Store '%s' (ref: %i) in [%s/%s]"
                          % (type(obj), obj_id, hdf.name, name))
        # the real action is here
        if 'compression' in kwargs \
               and (is_scalar or (is_ndarray and not len(obj.shape))):
            # recent (>= 2.0.0) h5py is strict not allowing
            # compression to be set for scalar types or anything with
            # shape==() ... TODO: check about is_objarrays ;-)
            kwargs = dict([(k, v) for (k, v) in kwargs.iteritems()
                           if k != 'compression'])
        hdf.create_dataset(name, None, None, obj, **kwargs)
        if not noid and not is_scalar:
            # objref for scalar items would be overkill
            hdf[name].attrs.create('objref', obj_id)
            # store object reference to be able to detect duplicates
            if __debug__:
                debug('HDF5', "Record objref in memo-dict (%i)" % obj_id)
            memo[obj_id] = obj

        ## yoh: was not sure why we have to assign here as well as below to grp
        ##      so commented out and seems to work just fine ;)
        ## yoh: because it never reaches grp! (see return below)
        if is_objarray:
            # we need to confess the true origin
            hdf[name].attrs.create('is_objarray', True)
            # it was of more than 1 dimension or it was a scalar
            if not len(shape) and externals.versions['hdf5'] < '1.8.7':
                if __debug__:
                    debug('HDF5', "Versions of hdf5 before 1.8.7 have problems with empty arrays")
            else:
                hdf[name].attrs.create('shape', shape)

        # handle scalars giving numpy scalars different flag
        if is_numpy_scalar:
            hdf[name].attrs.create('is_numpy_scalar', True)
        elif is_scalar:
            hdf[name].attrs.create('is_scalar', True)
        return

    #
    # Below handles stuff that cannot be natively stored in HDF5
    #
    if not name is None:
        if __debug__:
            debug('HDF5', "Store '%s' (ref: %i) in [%s/%s]"
                          % (type(obj), obj_id, hdf.name, name))
        grp = hdf.create_group(str(name))
    else:
        # XXX wouldn't it be more coherent to always have non-native objects in
        # a separate group
        if __debug__:
            debug('HDF5', "Store '%s' (ref: %i) in [%s]"
                          % (type(obj), obj_id, hdf.name))
        grp = hdf

    #
    # Store important flags and references in the group meta data
    #
    if not noid and not obj is None:
        # no refs for basic types
        grp.attrs.create('objref', obj_id)
        # we also note that we processed this object
        memo[obj_id] = obj

    if is_objarray:
        # we need to confess the true origin
        grp.attrs.create('is_objarray', True)
        grp.attrs.create('shape', shape)

    # standard containers need special treatment
    if not hasattr(obj, '__reduce__'):
        raise HDF5ConversionError("Cannot store class without __reduce__ "
                                  "implementation (%s)" % type(obj))
    # try disassembling the object
    try:
        pieces = obj.__reduce__()
        if __debug__:
            debug('HDF5', "Reduced '%s' (ref: %i) in [%s]"
                          % (type(obj), obj_id, hdf.name))
    except TypeError as te:
        # needs special treatment
        pieces = None
        if __debug__:
            debug('HDF5', "Failed to reduce '%s' (ref: %i) in [%s]: %s" # (%s)"
                          % (type(obj), obj_id, hdf.name, te)) #, obj))

    # common container handling, either __reduce__ was not possible
    # or it was the default implementation
    if pieces is None or pieces[0].__name__ == '_reconstructor':
        # figure out the source module
        if hasattr(obj, '__module__'):
            src_module = obj.__module__
        else:
            src_module = obj.__class__.__module__

        cls_name = obj.__class__.__name__
        # special case: metaclass types NOT instance of a class with metaclass
        if hasattr(obj, '__metaclass__') and hasattr(obj, '__base__'):
            cls_name = 'type'

        if src_module != '__builtin__':
            if hasattr(obj, '__name__'):
                if not obj.__name__ in dir(__import__(src_module,
                                                      fromlist=[obj.__name__])):
                    raise HDF5ConversionError("Cannot store locally defined "
                                              "function '%s'" % cls_name)
            else:
                if not cls_name in dir(__import__(src_module,
                                                  fromlist=[cls_name])):
                    raise HDF5ConversionError("Cannot store locally defined "
                                              "class '%s'" % cls_name)
        # store class info (fully-qualified)
        grp.attrs.create('class', cls_name)
        grp.attrs.create('module', src_module)

        if hasattr(obj, '__name__'):
            # for functions/types we need a name for reconstruction
            oname = obj.__name__
            if oname == '<lambda>':
                raise HDF5ConversionError(
                    "Can't obj2hdf lambda functions. Got %r" % (obj,))
            grp.attrs.create('name', oname)
        if isinstance(obj, list) or isinstance(obj, tuple):
            _seqitems_to_hdf(obj, grp, memo, **kwargs)
        elif isinstance(obj, dict):
            if __debug__:
                debug('HDF5', "Store dict as zipped list")
            # need to set noid since outer tuple containers are temporary
            _seqitems_to_hdf(zip(obj.keys(), obj.values()), grp, memo,
                             noid=True, **kwargs)
            grp['items'].attrs.create('__keys_in_tuple__', 1)

    else:
        if __debug__:
            debug('HDF5', "Use custom __reduce__ for storage: (%i arguments)."
                          % len(pieces[1]))
        grp.attrs.create('recon', pieces[0].__name__)
        grp.attrs.create('module', pieces[0].__module__)
        args = grp.create_group('rcargs')
        _seqitems_to_hdf(pieces[1], args, memo, **kwargs)

    # pull all remaining data from __reduce__
    if not pieces is None and len(pieces) > 2:
        # there is something in the state
        state = pieces[2]
        if __debug__:
            if state is not None:
                debug('HDF5', "Store object state (%i items)." % len(state))
            else:
                debug('HDF5', "Storing object with None state")
        # need to set noid since state dict is unique to an object
        obj2hdf(grp, state, name='state', memo=memo, noid=True,
                **kwargs)


def h5save(filename, data, name=None, mode='w', mkdir=True, **kwargs):
    """Stores arbitrary data in an HDF5 file.

    This is a convenience wrapper around `obj2hdf()`. Please see its
    documentation for more details -- especially the warnings!!

    Parameters
    ----------
    filename : str
      Name of the file the data shall be stored in.
    data : arbitrary
      Instance of an object that shall be stored in the file.
    name : str or None
      Name of the object. In case of a complex object that cannot be stored
      natively without disassembling them, this is going to be a new group,
      otherwise the name of the dataset. If None, no new group is created.
    mode : {'r', 'r+', 'w', 'w-', 'a'}
      IO mode of the HDF5 file. See `h5py.File` documentation for more
      information.
    mkdir : bool, optional
      Create target directory if it does not exist yet.
    **kwargs
      All additional arguments will be passed to `h5py.Group.create_dataset`.
      This could, for example, be `compression='gzip'`.
    """
    if mkdir:
        target_dir = osp.dirname(filename)
        if target_dir and not osp.exists(target_dir):
            os.makedirs(target_dir)
    hdf = h5py.File(filename, mode)
    hdf.attrs.create('__pymvpa_hdf5_version__', '2')
    hdf.attrs.create('__pymvpa_version__', mvpa2.__version__)
    try:
        obj2hdf(hdf, data, name, **kwargs)
    finally:
        hdf.close()


def h5load(filename, name=None):
    """Loads the content of an HDF5 file that has been stored by `h5save()`.

    This is a convenience wrapper around `hdf2obj()`. Please see its
    documentation for more details.

    Parameters
    ----------
    filename : str
      Name of the file to open and load its content.
    name : str
      Name of a specific object to load from the file.

    Returns
    -------
    instance
      An object of whatever has been stored in the file.
    """
    hdf = h5py.File(filename, 'r')
    try:
        if not name is None:
            if not name in hdf:
                raise ValueError("No object of name '%s' in file '%s'."
                                 % (name, filename))
            obj = hdf2obj(hdf[name])
        else:
            if not len(hdf) and not len(hdf.attrs):
                # there is nothing
                obj = None
            else:
                # stored objects can only by special groups or datasets
                if isinstance(hdf, h5py.Dataset) \
                   or ('class' in hdf.attrs or 'recon' in hdf.attrs):
                    # this is an object stored at the toplevel
                    obj = hdf2obj(hdf)
                else:
                    # no object into at the top-level, but maybe in the next one
                    # this would happen for plain mat files with arrays
                    if len(hdf) == 1 and '__unnamed__' in hdf:
                        # just a single with special name -> special case:
                        # return as is
                        obj = hdf2obj(hdf['__unnamed__'])
                    else:
                        # otherwise build dict with content
                        obj = {}
                        for k in hdf:
                            obj[k] = hdf2obj(hdf[k])
    finally:
        hdf.close()
    return obj

########NEW FILE########
__FILENAME__ = info
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Provide system and PyMVPA information useful while reporting bugs
"""

__docformat__ = 'restructuredtext'

import time, sys, os, subprocess
import platform as pl
from tempfile import mkstemp
from StringIO import StringIO

import mvpa2
from mvpa2.base import externals, cfg
from mvpa2.base.dochelpers import borrowkwargs

def _t2s(t):
    res = []
    for e in t:
        if isinstance(e, tuple):
            es = _t2s(e)
            if es != '':
                res += ['(%s)' % es]
        elif e != '':
            res += [e]
    return '/'.join(res)

__all__ = ['wtf', 'get_pymvpa_gitversion']


def get_pymvpa_gitversion():
    """PyMVPA version as reported by git.

    Returns
    -------
    None or str
      Version of PyMVPA according to git.
    """
    gitpath = os.path.join(os.path.dirname(mvpa2.__file__), os.path.pardir)
    gitpathgit = os.path.join(gitpath, '.git')
    if not os.path.exists(gitpathgit):
        return None
    ver = None
    try:
        (tmpd, tmpn) = mkstemp('mvpa', 'git')
        retcode = subprocess.call(['git',
                                   '--git-dir=%s' % gitpathgit,
                                   '--work-tree=%s' % gitpath,
                                   'describe', '--abbrev=4', 'HEAD'
                                   ],
                                  stdout=tmpd,
                                  stderr=subprocess.STDOUT)
        outline = open(tmpn, 'r').readlines()[0].strip()
        if outline.startswith('upstream/'):
            ver = outline.replace('upstream/', '')
    finally:
        os.remove(tmpn)
    return ver


class WTF(object):
    """Convenience class to contain information about PyMVPA and OS

    TODO: refactor to actually not contain just string representation
    but rather a dictionary (of dictionaries)
    """

    __knownitems__ = set(('process', 'runtime',
                          'externals', 'system', 'sources'))

    def __init__(self, include=None, exclude=None):
        """
        Parameters
        ----------
        include : list of str
          By default, all known (listed in `__knownitems__`) are reported.
          If only a limited set is needed to be reported -- specify it here.
        exclude : list of str
          If you want to exclude any item from being reported.
        """
        self._info = ''
        if include is None:
            report_items = self.__knownitems__.copy()
        else:
            # check first
            if not self.__knownitems__.issuperset(include):
                raise ValueError, \
                      "Items %s provided in exclude are not known to WTF." \
                      " Known are %s" % \
                      (str(set(include).difference(self.__knownitems__)),
                       self.__knownitems__)
            report_items = set(include)

        if exclude is not None:
            # check if all are known
            if not self.__knownitems__.issuperset(exclude):
                raise ValueError, \
                      "Items %s provided in exclude are not known to WTF." \
                      " Known are %s" % \
                      (str(set(exclude).difference(self.__knownitems__)),
                       self.__knownitems__)
            report_items = report_items.difference(exclude)
        self._report_items = report_items
        self._acquire()


    def _acquire_sources(self, out):
        out.write("PyMVPA:\n")
        out.write(" Version:       %s\n" % mvpa2.__version__)
        out.write(" Hash:          %s\n" % mvpa2.__hash__)
        out.write(" Path:          %s\n" % mvpa2.__file__)

        # Try to obtain git information if available
        out.write(" Version control (GIT):\n")
        try:
            gitpath = os.path.join(os.path.dirname(mvpa2.__file__), os.path.pardir)
            gitpathgit = os.path.join(gitpath, '.git')
            if os.path.exists(gitpathgit):
                for scmd, cmd in [
                    ('Status', ['status']),
                    ('Reference', 'show-ref -h HEAD'.split(' ')),
                    ('Difference from last release %s' % mvpa2.__version__,
                     ['diff', '--shortstat', 'upstream/%s...' % mvpa2.__version__])]:
                    try:
                        (tmpd, tmpn) = mkstemp('mvpa', 'git')
                        retcode = subprocess.call(['git',
                                                   '--git-dir=%s' % gitpathgit,
                                                   '--work-tree=%s' % gitpath] + cmd,
                                                  stdout=tmpd,
                                                  stderr=subprocess.STDOUT)
                    finally:
                        outlines = open(tmpn, 'r').readlines()
                        if len(outlines):
                            out.write('  %s:\n   %s' % (scmd, '   '.join(outlines)))
                        os.remove(tmpn)
                    #except Exception, e:
                    #    pass
            else:
                raise RuntimeError, "%s is not under GIT" % gitpath
        except Exception, e:
            out.write(' GIT information could not be obtained due "%s"\n' % e)


    def _acquire_system(self, out):
        out.write('SYSTEM:\n')
        out.write(' OS:            %s\n' %
                  ' '.join([os.name,
                            pl.system(),
                            pl.release(),
                            pl.version()]).rstrip())
        out.write(' Distribution:  %s\n' %
                  ' '.join([_t2s(pl.dist()),
                            _t2s(pl.mac_ver()),
                            _t2s(pl.win32_ver())]).rstrip())

    def _acquire_externals(self, out):
        # Test and list all dependencies:
        sdeps = {True: [], False: [], 'Error': []}
        for dep in sorted(externals._KNOWN):
            try:
                sdeps[externals.exists(dep, force=False)] += [dep]
            except:
                sdeps['Error'] += [dep]
        out.write('EXTERNALS:\n')
        out.write(' Present:       %s\n' % ', '.join(sdeps[True]))
        out.write(' Absent:        %s\n' % ', '.join(sdeps[False]))
        if len(sdeps['Error']):
            out.write(' Errors in determining: %s\n' % ', '.join(sdeps['Error']))

        SV = ('.__version__', )              # standard versioning
        out.write(' Versions of critical externals:\n')
        # First the ones known to externals,
        for k, v in sorted(externals.versions.iteritems()):
            out.write('  %-12s: %s\n' % (k, str(v)))
        try:
            if externals.exists('matplotlib'):
                import matplotlib
                out.write(' Matplotlib backend: %s\n'
                          % matplotlib.get_backend())
        except Exception, exc:
            out.write(' Failed to determine backend of matplotlib due to "%s"'
                      % str(exc))

    def _acquire_runtime(self, out):
        out.write("RUNTIME:\n")
        out.write(" PyMVPA Environment Variables:\n")
        out.write('  ' + '  '.join(
            ['%-20s: "%s"\n' % (str(k), str(v))
             for k, v in os.environ.iteritems()
             if (k.startswith('MVPA') or k.startswith('PYTHON'))]))

        out.write(" PyMVPA Runtime Configuration:\n")
        out.write('  ' + str(cfg).replace('\n', '\n  ').rstrip() + '\n')

    def _acquire_process(self, out):
        try:
            procstat = open('/proc/%d/status' % os.getpid()).readlines()
            out.write(' Process Information:\n')
            out.write('  ' + '  '.join(procstat))
        except:
            pass


    def _acquire(self):
        """
        TODO: refactor and redo ;)
        """
        out = StringIO()

        out.write("Current date:   %s\n" % time.strftime("%Y-%m-%d %H:%M"))

        # Little silly communicator/
        if 'sources' in self._report_items:
            self._acquire_sources(out)
        if 'system' in self._report_items:
            self._acquire_system(out)
        if 'externals' in self._report_items:
            self._acquire_externals(out)
        if 'runtime' in self._report_items:
            self._acquire_runtime(out)
        if 'process' in self._report_items:
            self._acquire_process(out)

        self._info = out.getvalue()


    def __repr__(self):
        if self._info is None:
            self._acquire()
        return self._info

    __str__ = __repr__


@borrowkwargs(WTF, '__init__')
def wtf(filename=None, **kwargs):
    """Report summary about PyMVPA and the system

    Parameters
    ----------
    filename : None or str
      If provided, information will be stored in a file, not printed
      to the screen
    **kwargs
      Passed to initialize `WTF` instance
    """

    info = WTF(**kwargs)
    if filename is not None:
        _ = open(filename, 'w').write(str(info))
    else:
        return info


if __name__ == '__main__':
    print wtf()

########NEW FILE########
__FILENAME__ = learner
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Implementation of a common trainable processing object (Learner)."""

__docformat__ = 'restructuredtext'

import time

from mvpa2.base.dataset import AttrDataset
from mvpa2.base.node import Node, ChainNode
from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.types import is_datasetlike
from mvpa2.base.dochelpers import _repr_attrs
from mvpa2.base.node import CompoundNode, CombinedNode, ChainNode

if __debug__:
    from mvpa2.base import debug


class LearnerError(Exception):
    """Base class for exceptions thrown by the Learners
    """
    pass


class DegenerateInputError(LearnerError):
    """Learner exception thrown if input data is not bogus

    i.e. no features or samples
    """
    pass


class FailedToTrainError(LearnerError):
    """Learner exception thrown if training failed"""
    pass


class FailedToPredictError(LearnerError):
    """Learner exception if it fails to predict.

    Usually happens if it was trained on degenerate data but without any
    complaints, or was not trained prior calling predict().
    """
    pass



class Learner(Node):
    """Common trainable processing object.

    A `Learner` is a `Node` that can (maybe has to) be trained on a dataset,
    before it can perform its function.
    """

    training_time = ConditionalAttribute(enabled=True,
        doc="Time (in seconds) it took to train the learner")

    trained_targets = ConditionalAttribute(enabled=True,
        doc="Set of unique targets (or any other space) it has"
            " been trained on (if present in the dataset trained on)")

    trained_nsamples = ConditionalAttribute(enabled=True,
        doc="Number of samples it has been trained on")

    trained_dataset = ConditionalAttribute(enabled=False,
        doc="The dataset it has been trained on")


    def __init__(self, auto_train=False, force_train=False, **kwargs):
        """
        Parameters
        ----------
        auto_train : bool
          Flag whether the learner will automatically train itself on the input
          dataset when called untrained.
        force_train : bool
          Flag whether the learner will enforce training on the input dataset
          upon every call.
        **kwargs
          All arguments are passed to the baseclass.
        """
        Node.__init__(self, **kwargs)
        self.__is_trained = False
        self.__auto_train = auto_train
        self.__force_train = force_train


    def __repr__(self, prefixes=[]):
        return super(Learner, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['auto_train', 'force_train'], default=False))

    def train(self, ds):
        """
        The default implementation calls ``_pretrain()``, ``_train()``, and
        finally ``_posttrain()``.

        Parameters
        ----------
        ds: Dataset
          Training dataset.

        Returns
        -------
        None
        """
        got_ds = is_datasetlike(ds)

        # TODO remove first condition if all Learners get only datasets
        if got_ds and (ds.nfeatures == 0 or len(ds) == 0):
            raise DegenerateInputError(
                    "Cannot train learner on degenerate data %s" % ds)
        if __debug__:
            debug("LRN", "Training learner %(lrn)s on dataset %(dataset)s",
                  msgargs={'lrn':self, 'dataset': ds})

        self._pretrain(ds)

        # remember the time when started training
        t0 = time.time()

        if got_ds:
            # things might have happened during pretraining
            if ds.nfeatures > 0:
                result = self._train(ds)
            else:
                warning("Trying to train on dataset with no features present")
                if __debug__:
                    debug("LRN",
                          "No features present for training, no actual training " \
                          "is called")
                result = None
        else:
            # in this case we claim to have no idea and simply try to train
            result = self._train(ds)

        # store timing
        self.ca.training_time = time.time() - t0

        # and post-proc
        result = self._posttrain(ds)

        # finally flag as trained
        self._set_trained()

        if __debug__:
            debug("LRN", "Finished training learner %(lrn)s on dataset %(dataset)s",
                  msgargs={'lrn':self, 'dataset': ds})


    def untrain(self):
        """Reverts changes in the state of this node caused by previous training
        """
        # flag the learner as untrained
        # important to do that before calling the implementation in the derived
        # class, as it might decide that an object remains trained
        self._set_trained(False)
        # call subclass untrain first to allow it to access current attributes
        self._untrain()
        # TODO evaluate whether this should also reset the nodes collections, or
        # whether that should be done by a more general reset() method
        self.reset()


    def _untrain(self):
        # nothing by default
        pass


    def _pretrain(self, ds):
        """Preparations prior training.

        By default, does nothing.

        Parameters
        ----------
        ds: Dataset
          Original training dataset.

        Returns
        -------
        None
        """
        pass


    def _train(self, ds):
        # nothing by default
        pass


    def _posttrain(self, ds):
        """Finalizing the training.

        By default, does nothing.

        Parameters
        ----------
        ds: Dataset
          Original training dataset.

        Returns
        -------
        None
        """
        ca = self.ca
        if ca.is_enabled('trained_targets') and isinstance(ds, AttrDataset):
            space = self.get_space()
            if space in ds.sa:
                ca.trained_targets = ds.sa[space].unique

        ca.trained_dataset = ds
        ca.trained_nsamples = len(ds)


    def _set_trained(self, status=True):
        """Set the Learner's training status

        Derived use this to set the Learner's status to trained (True) or
        untrained (False).
        """
        self.__is_trained = status


    def __call__(self, ds):
        # overwrite __call__ to perform a rigorous check whether the learner was
        # trained before use and auto-train
        if self.is_trained:
            # already trained
            if self.force_train:
                if __debug__:
                    debug('LRN', "Forcing training of %s on %s",
                          (self, ds))
                # but retraining is enforced
                self.train(ds)
            elif __debug__:
                debug('LRN', "Skipping training of already trained %s on %s",
                      (self, ds))
        else:
            # not trained
            if self.auto_train:
                # auto training requested
                if __debug__:
                    debug('LRN', "Auto-training %s on %s",
                          (self, ds))
                self.train(ds)
            else:
                # we always have to have trained before using a learner
                raise RuntimeError("%s needs to be trained before it can be "
                                   "used and auto training is disabled."
                                   % str(self))
        return super(Learner, self).__call__(ds)


    is_trained = property(fget=lambda x:x.__is_trained, fset=_set_trained,
                          doc="Whether the Learner is currently trained.")
    auto_train = property(fget=lambda x:x.__auto_train,
                          doc="Whether the Learner performs automatic training"
                              "when called untrained.")
    force_train = property(fget=lambda x:x.__force_train,
                          doc="Whether the Learner enforces training upon every"
                              "called.")


class CompoundLearner(Learner, CompoundNode):
    def __init__(self, learners, auto_train=False,
                    force_train=False, **kwargs):
        '''Initializes with measures

        Parameters
        ----------
        learners: list or tuple
            a list of Learner instances
        '''
        Learner.__init__(self, auto_train=auto_train,
                         force_train=force_train, **kwargs)
        CompoundNode.__init__(self, learners, **kwargs)

    is_trained = property(fget=lambda x:all(y.is_trained
                                            for y in x),
                          fset=lambda x:map(y._set_trained()
                                            for y in x),
                          doc="Whether the Learner is currently trained.")

    def train(self, ds):
        for learner in self:
            learner.train(ds)

    def untrain(self):
        for learner in self:
            learner.untrain()

    def _call(self, ds):
        raise NotImplementedError


class ChainLearner(ChainNode, CompoundLearner):
    '''Combines different learners into one in a chained fashion'''
    def __init__(self, learners, auto_train=False,
                    force_train=False, **kwargs):
        '''Initializes with measures

        Parameters
        ----------
        learners: list or tuple
            a list of Learner instances
        '''
        CompoundLearner.__init__(self, learners, auto_train=auto_train,
                         force_train=force_train, **kwargs)

    def _call(self, ds):
       return ChainNode._call(self, ds)

class CombinedLearner(CompoundLearner, CombinedNode):
    def __init__(self, learners, combine_axis, a=None, **kwargs):
        """
        Parameters
        ----------
        learners : list of Learner
        combine_axis : ['h', 'v']
        a: {'unique','drop_nonunique','uniques','all'} or True or False or None (default: None)
            Indicates which dataset attributes from datasets are stored
            in merged_dataset. If an int k, then the dataset attributes from
            datasets[k] are taken. If 'unique' then it is assumed that any
            attribute common to more than one dataset in datasets is unique;
            if not an exception is raised. If 'drop_nonunique' then as 'unique',
            except that exceptions are not raised. If 'uniques' then, for each
            attribute,  any unique value across the datasets is stored in a tuple
            in merged_datasets. If 'all' then each attribute present in any
            dataset across datasets is stored as a tuple in merged_datasets;
            missing values are replaced by None. If None (the default) then no
            attributes are stored in merged_dataset. True is equivalent to
            'drop_nonunique'. False is equivalent to None.
        """
        CompoundLearner.__init__(self, learners, **kwargs)
        self._combine_axis = combine_axis
        self._a = a

    def _call(self, ds):
        return CombinedNode._call(self, ds)



########NEW FILE########
__FILENAME__ = node
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Implementation of a common processing object (node)."""

__docformat__ = 'restructuredtext'

import time
import numpy as np
from mvpa2.support import copy

from mvpa2.base.dochelpers import _str, _repr, _repr_attrs
from mvpa2.base.state import ClassWithCollections, ConditionalAttribute

from mvpa2.base.collections import SampleAttributesCollection, \
     FeatureAttributesCollection, DatasetAttributesCollection

if __debug__:
    from mvpa2.base import debug

class Node(ClassWithCollections):
    """Common processing object.

    A `Node` is an object the processes datasets. It can be called with a
    `Dataset` and returns another dataset with the results. In addition, a node
    can also be used as a generator. Upon calling ``generate()`` with a datasets
    it yields (potentially) multiple result datasets.

    Node have a notion of ``space``. The meaning of this space may vary heavily
    across sub-classes. In general, this is a trigger that tells the node to
    compute and store information about the input data that is "interesting" in
    the context of the corresponding processing in the output dataset.
    """

    calling_time = ConditionalAttribute(enabled=True,
        doc="Time (in seconds) it took to call the node")

    raw_results = ConditionalAttribute(enabled=False,
        doc="Computed results before invoking postproc. " +
            "Stored only if postproc is not None.")

    # Work-around for "happily-broken-by-design" HDF5 storage
    # serialization: upon reconstruction, no __init__ is called
    # so no private attributes, introduced since the moment when
    # original structure was serialized, would get populated.
    # "More proper" solution would be finally to implement a full
    # chain of __reduce__ and __setstate__s for derived classes (and use
    # Parameters more).  For now we would simply define those with default
    # values also at class level.
    __pass_attr = None

    def __init__(self, space=None, pass_attr=None, postproc=None, **kwargs):
        """
        Parameters
        ----------
        space : str, optional
          Name of the 'processing space'. The actual meaning of this argument
          heavily depends on the sub-class implementation. In general, this is
          a trigger that tells the node to compute and store information about
          the input data that is "interesting" in the context of the
          corresponding processing in the output dataset.
        pass_attr : str, list of str|tuple, optional
          Additional attributes to pass on to an output dataset. Attributes can
          be taken from all three attribute collections of an input dataset
          (sa, fa, a -- see :meth:`Dataset.get_attr`), or from the collection
          of conditional attributes (ca) of a node instance. Corresponding
          collection name prefixes should be used to identify attributes, e.g.
          'ca.null_prob' for the conditional attribute 'null_prob', or
          'fa.stats' for the feature attribute stats. In addition to a plain
          attribute identifier it is possible to use a tuple to trigger more
          complex operations. The first tuple element is the attribute
          identifier, as described before. The second element is the name of the
          target attribute collection (sa, fa, or a). The third element is the
          axis number of a multidimensional array that shall be swapped with the
          current first axis. The fourth element is a new name that shall be
          used for an attribute in the output dataset.
          Example: ('ca.null_prob', 'fa', 1, 'pvalues') will take the
          conditional attribute 'null_prob' and store it as a feature attribute
          'pvalues', while swapping the first and second axes. Simplified
          instructions can be given by leaving out consecutive tuple elements
          starting from the end.
        postproc : Node instance, optional
          Node to perform post-processing of results. This node is applied
          in `__call__()` to perform a final processing step on the to be
          result dataset. If None, nothing is done.
        """
        ClassWithCollections.__init__(self, **kwargs)
        if __debug__:
            debug("NO",
                  "Init node '%s' (space: '%s', postproc: '%s')",
                  (self.__class__.__name__, space, str(postproc)))
        self.set_space(space)
        self.set_postproc(postproc)
        if isinstance(pass_attr, basestring):
            pass_attr = (pass_attr,)
        self.__pass_attr = pass_attr


    def __call__(self, ds):
        """
        The default implementation calls ``_precall()``, ``_call()``, and
        finally returns the output of ``_postcall()``.

        Parameters
        ----------
        ds: Dataset
          Input dataset.

        Returns
        -------
        Dataset
        """
        t0 = time.time()                # record the time when call initiated

        self._precall(ds)
        result = self._call(ds)
        result = self._postcall(ds, result)

        self.ca.calling_time = time.time() - t0 # set the calling_time
        return result


    def _precall(self, ds):
        """Preprocessing of data

        By default, does nothing.

        Parameters
        ----------
        ds: Dataset
          Original input dataset.

        Returns
        -------
        Dataset
        """
        return ds


    def _call(self, ds):
        raise NotImplementedError


    def _postcall(self, ds, result):
        """Postprocessing of results.

        By default, does nothing.

        Parameters
        ----------
        ds: Dataset
          Original input dataset.
        result: Dataset
          Preliminary result dataset (as produced by ``_call()``).

        Returns
        -------
        Dataset
        """
        result = self._pass_attr(ds, result)
        result = self._apply_postproc(ds, result)
        return result

    def _pass_attr(self, ds, result):
        """Pass a configured set of attributes on to the output dataset"""
        pass_attr = self.__pass_attr
        if pass_attr is not None:
            ca = self.ca
            ca_keys = self.ca.keys()
            for a in pass_attr:
                maxis = 0
                rcol = None
                attr_newname = None
                if isinstance(a, tuple):
                    if len(a) > 1:
                        # target collection is second element
                        colswitch = {'sa': result.sa, 'fa': result.fa, 'a': result.a}
                        rcol = colswitch[a[1]]
                    if len(a) > 2:
                        # major axis is third element
                        maxis = a[2]
                    if len(a) > 3:
                        # new attr name if fourth element
                        attr_newname = a[3]
                    # the attribute name is the first element
                    a = a[0]
                # It might come from .ca of this instance
                if a.startswith('ca.'):
                    a = a[3:]
                if a in ca_keys:
                    if rcol is None:
                        # We will assign it to .sa for now
                        rcol = result.sa
                    attr = ca[a]
                else:
                    # look in the ds
                    # find it in the original ds
                    attr, col = ds.get_attr(a)
                    if rcol is None:
                        # ONLY if there was no explicit output collection set
                        # deduce corresponding collection in results
                        # Since isinstance would take longer (eg 200 us vs 4)
                        # for now just use 'is' on the __class__
                        col_class = col.__class__
                        if col_class is SampleAttributesCollection:
                            rcol = result.sa
                        elif col_class is FeatureAttributesCollection:
                            rcol = result.fa
                        elif col_class is DatasetAttributesCollection:
                            rcol = result.a
                        else:
                            raise ValueError("Cannot determine origin of %s collection"
                                             % col)
                if attr_newname is None:
                    # go with previous name if no other is given
                    attr_newname = attr.name
                if maxis == 0:
                    # all good
                    value = attr.value
                else:
                    # move selected axis to the front
                    value = np.swapaxes(attr.value, 0, maxis)
                # "shallow copy" into the result
                # this way we also invoke checks for the correct length etc
                rcol[attr_newname] = value
        return result

    def _apply_postproc(self, ds, result):
        """Apply any post-processing to an output dataset"""
        if not self.__postproc is None:
            if __debug__:
                debug("NO",
                      "Applying post-processing node %s", (self.__postproc,))
            self.ca.raw_results = result
            result = self.__postproc(result)
        return result

    def generate(self, ds):
        """Yield processing results.

        This methods causes the node to behave like a generator. By default it
        simply yields a single result of its processing -- identical to the
        output of calling the node with a dataset. Subclasses might implement
        generators that yield multiple results.

        Parameters
        ----------
        ds: Dataset
          Input dataset

        Returns
        -------
        generator
          the generator yields the result of the processing.
        """
        yield self(ds)


    def get_space(self):
        """Query the processing space name of this node."""
        return self.__space


    def set_space(self, name):
        """Set the processing space name of this node."""
        self.__space = name


    def get_postproc(self):
        """Returns the post-processing node or None."""
        return self.__postproc


    def set_postproc(self, node):
        """Assigns a post-processing node

        Set to `None` to disable postprocessing.
        """
        self.__postproc = node


    def __str__(self):
        return _str(self)


    def __repr__(self, prefixes=[]):
        return super(Node, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['space', 'pass_attr', 'postproc']))

    space = property(get_space, set_space,
                     doc="Processing space name of this node")

    pass_attr = property(lambda self: self.__pass_attr,
                         doc="Which attributes of the dataset or self.ca "
                         "to pass into result dataset upon call")

    postproc = property(get_postproc, set_postproc,
                        doc="Node to perform post-processing of results")


class CompoundNode(Node):
    """List of nodes. 

    A CompoundNode behaves similar to a list container: Nodes can be appended,
    and the chain can be sliced like a list, etc ...
    
    Subclasses such as ChainNode and CombinedNode implement the _call
    method in different ways.
    """
    def __init__(self, nodes, **kwargs):
        """
        Parameters
        ----------
        nodes: list
          Node instances.
        """
        if not len(nodes):
            raise ValueError("%s needs at least one embedded node."
                             % self.__class__.__name__)

        self._nodes = nodes
        Node.__init__(self, **kwargs)


    def __copy__(self):
        # XXX how do we safely and exhaustively copy a node?
        return self.__class__([copy.copy(n) for n in self])


    def _call(self, ds):
        raise NotImplementedError("This is an abstract class.")


    def generate(self, ds, startnode=0):
        """
        Parameters
        ----------
        ds: Dataset
          To be processed dataset
        startnode: int
          First node in the chain that shall be considered. This argument is
          mostly useful for internal optimization.
        """
        first_node = self[startnode]
        if __debug__:
            debug('MAP', "%s: input (%s) -> generator (%i/%i): '%s'",
                  (self.__class__.__name__, ds.shape,
                   startnode + 1, len(self), first_node))
        # let the first node generator as many datasets as it wants
        for gds in first_node.generate(ds):
            if startnode == len(self) - 1:
                # if this is already the last node yield the result
                yield gds
            else:
                # otherwise feed them through the rest of the chain
                for rgds in self.generate(gds, startnode=startnode + 1):
                    yield rgds


    #
    # Behave as a container
    #
    def append(self, node):
        """Append a node to the chain."""
        # XXX and if a node is a ChainMapper itself -- should we just
        # may be loop and add all the entries?
        self._nodes.append(node)


    def __len__(self):
        return len(self._nodes)


    def __iter__(self):
        for n in self._nodes:
            yield n


    def __reversed__(self):
        return reversed(self._nodes)


    def __getitem__(self, key):
        # if just one is requested return just one, otherwise return a
        # NodeChain again
        if isinstance(key, int):
            return self._nodes[key]
        else:
            # operate on shallow copy of self
            sliced = copy.copy(self)
            sliced._nodes = self._nodes[key]
            return sliced


    def __repr__(self, prefixes=[]):
        return super(CompoundNode, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['nodes']))


    def __str__(self):
        return _str(self, '-'.join([str(n) for n in self]))

    nodes = property(fget=lambda self:self._nodes)


class ChainNode(CompoundNode):
    """
    This class allows to concatenate a list of nodes into a processing chain.
    When called with a dataset, it is sequentially fed through nodes in the
    chain. A ChainNode may also be used as a generator. In this case, all
    nodes in the chain are treated as generators too, and the ChainNode
    behaves as a single big generator that recursively calls all embedded
    generators and yield the results.
    """
    def __init__(self, nodes, **kwargs):
        """
        Parameters
        ----------
        nodes: list
          Node instances.
        """
        CompoundNode.__init__(self, nodes=nodes, **kwargs)

    def _call(self, ds):
        mp = ds
        for i, n in enumerate(self):
            if __debug__:
                debug('MAP', "%s: input (%s) -> node (%i/%i): '%s'",
                      (self.__class__.__name__,
                       hasattr(mp, 'shape') and mp.shape or '???',
                       i + 1, len(self),
                       n))
            mp = n(mp)
        if __debug__:
            debug('MAP', "%s: output (%s)", (self.__class__.__name__, mp.shape))
        return mp


class CombinedNode(CompoundNode):
    """Node to pass a dataset on to a set of nodes and combine there output.

    Output combination or aggregation is currently done by hstacking or
    vstacking the resulting datasets.
    """

    def __init__(self, nodes, combine_axis, a=None, **kwargs):
        """
        Parameters
        ----------
        mappers : list
        combine_axis : ['h', 'v']
        a: {'unique','drop_nonunique','uniques','all'} or True or False or None (default: None)
            Indicates which dataset attributes from datasets are stored 
            in merged_dataset. If an int k, then the dataset attributes from 
            datasets[k] are taken. If 'unique' then it is assumed that any
            attribute common to more than one dataset in datasets is unique;
            if not an exception is raised. If 'drop_nonunique' then as 'unique',
            except that exceptions are not raised. If 'uniques' then, for each 
            attribute,  any unique value across the datasets is stored in a tuple 
            in merged_datasets. If 'all' then each attribute present in any 
            dataset across datasets is stored as a tuple in merged_datasets; 
            missing values are replaced by None. If None (the default) then no 
            attributes are stored in merged_dataset. True is equivalent to
            'drop_nonunique'. False is equivalent to None.
        """
        CompoundNode.__init__(self, nodes=nodes, **kwargs)
        self._combine_axis = combine_axis
        self._a = a

    def __copy__(self):
        return self.__class__([copy.copy(n) for n in self],
                                         copy.copy(self._combine_axis),
                                         copy.copy(self._a))


    def _call(self, ds):
        out = [node(ds) for node in self]
        from mvpa2.datasets import hstack, vstack
        stacker = {'h': hstack, 'v': vstack}
        stacked = stacker[self._combine_axis](out, self._a)
        return stacked

    def __repr__(self, prefixes=[]):
        return super(CombinedNode, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['combine_axis', 'a']))



########NEW FILE########
__FILENAME__ = param
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##g
"""Parameter representation"""

__docformat__ = 'restructuredtext'

import re
import textwrap
import warnings
import numpy as np
from mvpa2.base.state import IndexedCollectable
from mvpa2.base.constraints import expand_contraint_spec

if __debug__:
    from mvpa2.base import debug

_whitespace_re = re.compile('\n\s+|^\s+')

__all__ = [ 'Parameter', 'KernelParameter' ]


class Parameter(IndexedCollectable):
    """This class shall serve as a representation of a parameter.

    It might be useful if a little more information than the pure parameter
    value is required (or even only useful).

    Each parameter must have a value. However additional attributes can be
    passed to the constructor and will be stored in the object.

    Notes
    -----
    BIG ASSUMPTION: stored values are not mutable, ie nobody should do

        cls.parameter1[:] = ...

    or we wouldn't know that it was changed
    Here is a list of possible additional attributes:

    step
      Increment/decrement step size hint for optimization
    """

    def __init__(self, default, constraints=None, ro=False,  index=None,  value=None,
                 name=None, doc=None, **kwargs):
        """Specify a Parameter with a default value and arbitrary
        number of additional attributes.

        Parameters
        ----------
        constraints : callable
          A functor that takes any input value, performs checks or type
          conversions and finally returns a value that is appropriate for a
          parameter or raises an exception.
        name : str
          Name of the parameter under which it should be available in its
          respective collection.
        doc : str
          Documentation about the purpose of this parameter.
        index : int or None
          Index of parameter among the others.  Determines order of listing
          in help.  If None, order of instantiation determines the index.
        ro : bool
          Either value which will be assigned in the constructor is read-only and
          cannot be changed
        value
          Actual value of the parameter to be assigned

        Examples
        --------
        -ensure the parameter to be of type float
        (None not allowed as value):
        constraints = EnsureFloat()
        >>> from mvpa2.base.param import Parameter
        >>> from mvpa2.base.constraints import (EnsureFloat, EnsureRange,
        ...                              AltConstraints, Constraints)
        >>> C = Parameter(23.0, constraints=EnsureFloat())

        -ensure the parameter to be of type float or to be None:
        >>> C = Parameter(23.0, constraints=AltConstraints(EnsureFloat(), None))

        -ensure the parameter to be None or to be of type float
        and lie in the inclusive range (7.0,44.0):
        >>> C = Parameter(23.0, AltConstraints(Constraints(EnsureFloat(),
        ...                                    EnsureRange(min=7.0,max=44.0)),
        ...                                    None))
        """
        allowedtype = kwargs.pop('allowedtype', None)
        if allowedtype is not None:
            warnings.warn(
                "allowedtype option was deprecated in favor of constraints. "
                "Adjust your code, provided value '%s' was ignored"
                % str(allowedtype), category=DeprecationWarning)
        # XXX probably is too generic...
        # and potentially dangerous...
        # let's at least keep track of what is passed
        self._additional_props = []
        for k, v in kwargs.iteritems():
            self.__setattr__(k, v)
            self._additional_props.append(k)

        self.__default = default
        self._ro = ro
        self.constraints = expand_contraint_spec(constraints)

        # needs to come after kwargs processing, since some debug statements
        # rely on working repr()
        # value is not passed since we need to invoke _set with init=True
        # below
        IndexedCollectable.__init__(self, index=index, # value=value,
                                    name=name, doc=doc)
        self._isset = False
        if value is None:
            self._set(self.__default, init=True)
        else:
            self._set(value, init=True)

        if __debug__:
            if 'val' in kwargs:
                raise ValueError, "'val' property name is illegal."


    def __reduce__(self):
        icr = IndexedCollectable.__reduce__(self)
        # Collect all possible additional properties which were passed
        # to the constructor
        state = dict([(k, getattr(self, k)) for k in self._additional_props])
        state['_additional_props'] = self._additional_props
        state.update(icr[2])
        res = (self.__class__, (self.__default, self.constraints, self._ro) + icr[1], state)
        #if __debug__ and 'COL_RED' in debug.active:
        #    debug('COL_RED', 'Returning %s for %s' % (res, self))
        return res


    def __str__(self):
        res = IndexedCollectable.__str__(self)
        # it is enabled but no value is assigned yet
        res += '=%s' % (self.value,)
        return res


    def __repr__(self):
        # cannot use IndexedCollectable's repr(), since the contructor
        # needs to handle the mandatory 'default' argument
        # TODO: so what? just tune it up ;)
        # TODO: think what to do with index parameter...
        s = "%s(%s, name=%s, doc=%s" % (self.__class__.__name__,
                                        self.__default,
                                        repr(self.name),
                                        repr(self.__doc__))
        plist = ["%s=%s" % (p, self.__getattribute__(p))
                    for p in self._additional_props]
        if len(plist):
            s += ', ' + ', '.join(plist)
        if self._ro:
            s += ', ro=True'
        if not self.is_default:
            s += ', value=%r' % (self.value, )
        s += ')'
        return s


    def _paramdoc(self, indent="  ", width=70):
        """Docstring for the parameter to be used in lists of parameters

        Returns
        -------
        string or list of strings (if indent is None)
        """
        paramsdoc = '%s' % self.name
        if not self.constraints is None:
            sdoc = self.constraints.short_description()
            if not sdoc is None:
                if sdoc[0] == '(' and sdoc[-1] == ')':
                    sdoc = sdoc[1:-1]
                # parameters are always optional
                paramsdoc += " : %s, optional" % sdoc
        paramsdoc = [paramsdoc]

        try:
            doc = self.__doc__.strip()
            if not doc.endswith('.'):
                doc += '.'
            if self.constraints is not None:
                cdoc = self.constraints.long_description()
                if cdoc[0] == '(' and cdoc[-1] == ')':
                    cdoc = cdoc[1:-1]
                doc += ' Constraints: %s.' % cdoc
            try:
                doc += " [Default: %r]" % (self.default,)
            except:
                pass
            # Explicitly deal with multiple spaces, for some reason
            # replace_whitespace is non-effective
            doc = _whitespace_re.sub(' ', doc)
            paramsdoc += [indent + x
                          for x in textwrap.wrap(doc, width=width-len(indent),
                                                 replace_whitespace=True)]
        except Exception, e:
            pass
        return '\n'.join(paramsdoc)


    # XXX should be named reset2default? correspondingly in
    #     ParameterCollection as well
    def reset_value(self):
        """Reset value to the default"""
        #IndexedCollectable.reset(self)
        if not self.is_default and not self._ro:
            self._isset = True
            self.value = self.__default

    def _set(self, val, init=False):
        if self.constraints is not None:
#            for c in self.constraints:
#                val = c(val)
#                #val = c.validate(val)
            val = self.constraints(val)    
        different_value = self._value != val
        isarray = isinstance(different_value, np.ndarray)
        if self._ro and not init:
            raise RuntimeError, \
                  "Attempt to set read-only parameter %s to %s" \
                  % (self.name, val)
        if (isarray and np.any(different_value)) or \
           ((not isarray) and different_value):
            if __debug__:
                debug("COL",
                      "Parameter: setting %s to %s " % (str(self), val))
            self._value = val
            # Set 'isset' only if not called from initialization routine
            self._isset = not init #True
        elif __debug__:
            debug("COL",
                  "Parameter: not setting %s since value is the same" \
                  % (str(self)))

    @property
    def is_default(self):
        """Returns True if current value is bound to default one"""
        return self._value is self.default

    @property
    def equal_default(self):
        """Returns True if current value is equal to default one"""
        return self._value == self.__default

    def _set_default(self, value):
        wasdefault = self.is_default
        self.__default = value
        if wasdefault:
            self.reset_value()
            self._isset = False

    # incorrect behavior
    #def reset(self):
    #    """Override reset so we don't clean the flag"""
    #    pass

    default = property(fget=lambda x:x.__default, fset=_set_default)
    value = property(fget=lambda x:x._value, fset=_set)


class KernelParameter(Parameter):
    """Just that it is different beast"""
    pass

########NEW FILE########
__FILENAME__ = progress
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helper to print pretty progress indicator.

"""

__docformat__ = 'restructuredtext'

import time
import datetime
import math

def seconds2prettystring(t, ndigits=0):
    '''Prints seconds in a pretty form

    Parameters
    ----------
    t: float
        time in seconds
    ndigits: int (default: 0)
        how many digits are used to show time in seconds (after
        the decimal sign '.')

    Returns
    s: str
        time represented as a string HH:MM:SS

    '''
    if t < 0:
        return '-' + seconds2prettystring(-t, ndigits)

    if t == t + 1.:
        return 'oo' # infinity

    seconds_per_day = 60 * 60 * 24
    ndays = int(t) // seconds_per_day
    nseconds = t - ndays * seconds_per_day
    sec_str = str(datetime.timedelta(seconds=nseconds))

    # split on decimal point
    sec_split = sec_str.split('.')

    if len(sec_split) != 1:
        big, small = sec_split # before and after dot
        if ndigits == 0:
            sec_str = big
        else:
            sec_str = '%s.%s' % (big, small[:min(len(small), ndigits)])

    return sec_str if ndays == 0 else '%d+%s' % (ndays, sec_str)


def eta_string(start_time, progress, msg=None,
                progress_bar_width=18, show_percentage=True):
    '''Simple linear extrapolation to estimate how much time is needed
    to complete a task.

    Parameters
    ----------
    starttime
        Time the tqsk started, from 'time.time()'
    progress: float
        Between 0 (nothing completed) and 1 (fully completed)
    msg: str (optional)
        Message that describes progress - is added to the output
    progress_bar_width: int (default: 18)
        Width of progress bar. If zero then no progress bar is shown.
    show_percentage: bool (default: True)
        Show progress in percentage?

    Returns
    -------
    eta
        Estimated time until completion formatter pretty,

    Notes
    -----
    ETA refers to "estimated time of arrival".
    '''
    SYM_DONE = '='
    SYM_TODO = '_'

    now = time.time()
    took = now - start_time

    legal_progress = 0 < progress <= 1
    eta = took * (1 - progress) / progress if legal_progress \
                                           else progress * float('inf')
    pct_str = '[%.0f%%]' % (progress * 100.)

    formatter = seconds2prettystring


    if progress_bar_width:
        n_done = int(math.floor(min(progress, 1.) * progress_bar_width))
        n_todo = progress_bar_width - n_done

        done = SYM_DONE * n_done
        todo = SYM_TODO * n_todo

        if show_percentage:
            # replace characters in 'done' or 'todo' (whichever is longer)
            # by a percentage indicator (e.g. "[42%]").
            n_pct = len(pct_str)
            margin = 2
            n = n_pct + margin # required length for pct string
            if n <= n_done and (n > n_todo or n_done >= n_todo):
                offset = (n_done - margin - n_pct // 2) // 2

                done = done[:offset] + pct_str + \
                        done[:(n_done - offset - n_pct)]
            elif n <= n_todo:
                offset = (n_todo - (n_pct + margin) // 2) // 2
                todo = todo[:offset] + pct_str + \
                        todo[:(n_todo - offset - n_pct)]
            else:
                pass # not enough space - don't show anything


        bar = done + todo
    else:
        bar = pct_str

    full_msg = '+%s %s %s' % (formatter(took), bar, formatter(-eta))
    if not msg is None:
        full_msg = '%s  %s' % (full_msg, msg)
    return full_msg

class ProgressBar(object):
    '''Simple progress bar in ASCII text'''
    def __init__(self, start_time=None, progress_bar_width=18,
                        show_percentage=True):
        '''
        Initializes the progress bar

        Parameters
        ----------
        start_time: float or None (default)
            Start time relative to the start of the Epoch. If None it takes
            the current time.
        progress_bar_width: int (default: 18)
            Width of progress bar. If zero then no progress bar is shown.
        show_percentage: bool (default: True)
            Show progress in percentage?
        '''

        self.start(start_time)
        self._progress_bar_width = progress_bar_width
        self._show_percentage = show_percentage

    def start(self, start_time=None):
        '''Resets the start time

        Parameters
        ----------
        start_time: float or None (default)
            Start time relative to the start of the Epoch. If None it takes
            the current time.
        '''
        if start_time is None:
            start_time = time.time()
        self._start_time = start_time

    def __call__(self, progress, msg=None):
        '''
        Returns a string representation of progress

        Parameters
        ----------
        progress: float
            Between 0 (nothing completed) and 1 (fully completed)
        msg: str (optional)
            Message that describes progress - is added to the output

        Returns
        -------
        bar: str
            A text representation of progress.
        '''
        return eta_string(self._start_time, progress, msg,
                          progress_bar_width=self._progress_bar_width,
                          show_percentage=self._show_percentage)

########NEW FILE########
__FILENAME__ = report
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Creating simple PDF reports using reportlab
"""

__docformat__ = 'restructuredtext'


import os
from datetime import datetime

import mvpa2
from mvpa2.base import externals, verbose
from mvpa2.base.dochelpers import borrowkwargs

if __debug__:
    from mvpa2.base import debug

if externals.exists('reportlab', raise_=True):
    import reportlab as rl
    import reportlab.platypus as rplp
    import reportlab.lib.styles as rpls
    import reportlab.lib.units as rplu

    # Actually current reportlab's Image can't deal directly with .pdf images
    # Lets use png for now
    if externals.versions['reportlab'] >= '1112.2':
        _fig_ext_default = 'pdf'
    else:
        _fig_ext_default = 'png'


__all__ = [ 'rl', 'Report', 'escape_xml' ]


def escape_xml(s):
    """Replaces &<> symbols with corresponding XML tokens
    """
    s = s.replace('&', '&amp;')
    s = s.replace('<', '&lt;')
    s = s.replace('>', '&gt;')
    return s

class Report(object):
    """Simple PDF reports using reportlab

    Named report 'report' generates 'report.pdf' and directory 'report/' with
    images which were requested to be included in the report

    You can attach report to the existing 'verbose' with

    >>> report = Report()
    >>> verbose.handlers += [report]

    and then all verbose messages present on the screen will also be recorded
    in the report.  Use

    >>> report.text("string")          #  to add arbitrary text
    >>> report.xml("<H1>skajdsf</H1>") # to add XML snippet

    or

    >>> report.figure()  # to add the current figure to the report.
    >>> report.figures() # to add existing figures to the report

    Note that in the later usecase, figures might not be properly
    interleaved with verbose messages if there were any between the
    creations of the figures.

    Inspired by Andy Connolly
    """

    def __init__(self, name='report', title=None, path=None,
                 author=None, style="Normal",
                 fig_ext=None, font='Helvetica',
                 pagesize=None):
        """Initialize report

        Parameters
        ----------
        name : string
          Name of the report
        title : string or None
          Title to start the report, if None, name will be used
        path : string
          Top directory where named report will be stored. Has to be
          set now to have correct path for storing image renderings.
          Default: current directory
        author : string or None
          Optional author identity to be printed
        style : string
          Default Paragraph to be used. Must be the one of the known
          to reportlab styles, e.g. Normal
        fig_ext : string
          What extension to use for figures by default. If None, a default
          will be used. Since versions prior 2.2 of reportlab might do not
          support pdf, 'png' is default for those, 'pdf' otherwise
        font : string
          Name of the font to use
        pagesize : tuple of floats
          Optional page size if not to be default
        """

        if pagesize is None:
            pagesize = rl.rl_config.defaultPageSize
        self.pagesize = pagesize

        self.name = name
        self.author = author
        self.font = font
        self.title = title
        if fig_ext is None:
            self.fig_ext = _fig_ext_default
        else:
            self.fig_ext = fig_ext

        if path is None:
            self._filename = name
        else:
            self._filename = os.path.join(path, name)

        self.__nfigures = 0

        try:
            styles = rpls.getSampleStyleSheet()
            self.style = styles.byName[style]
        except KeyError:
            raise ValueError, \
                  "Style %s is not know to reportlab. Known are %s" \
                  % (styles.keys())

        self._story = []


    @property
    def __preamble(self):
        """Compose the beginning of the report
        """
        date = datetime.today().isoformat(' ')

        owner = 'PyMVPA v. %s' % mvpa2.__version__
        if self.author is not None:
            owner += '   Author: %s' % self.author

        return [ rplp.Spacer(1, 0.8*rplu.inch),
                 rplp.Paragraph("Generated on " + date, self.style),
                 rplp.Paragraph(owner, self.style)] + self.__flowbreak


    def clear(self):
        """Clear the report
        """
        self._story = []


    def xml(self, line, style=None):
        """Adding XML string to the report
        """
        if __debug__ and not self in debug.handlers:
            debug("REP", "Adding xml '%s'" % line.strip())
        if style is None:
            style = self.style
        self._story.append(rplp.Paragraph(line, style=style))

    # Can't use here since Report isn't yet defined at this point
    #@borrowkwargs(Report, 'xml')
    def text(self, line, **kwargs):
        """Add a text string to the report
        """
        if __debug__ and not self in debug.handlers:
            debug("REP_", "Adding text '%s'" % line.strip())
        # we need to convert some of the characters to make it
        # legal XML
        line = escape_xml(line)
        self.xml(line, **kwargs)

    write = text
    """Just an alias for .text, so we could simply provide report
    as a handler for verbose
    """

    # can't do here once again since it needs to conditional on externals
    # TODO: workaround -- either passing symbolic names or assign
    #       post-class creation
    #@borrowkwargs(reportlab.platypus.Image, '__init__')
    def figure(self, fig=None, name=None, savefig_kwargs={}, **kwargs):
        """Add a figure to the report

        Parameters
        ----------
        fig : None or str or `figure.Figure`
          Figure to place into report: `str` is treated as a filename,
          `Figure` stores it into a file under directory and embeds
          into the report, and `None` takes the current figure
        savefig_kwargs : dict
          Additional keyword arguments to provide savefig with (e.g. dpi)
        **kwargs
          Passed to :class:`reportlab.platypus.Image` constructor
        """

        if externals.exists('pylab', raise_=True):
            import pylab as pl
            figure = pl.matplotlib.figure

        if fig is None:
            fig = pl.gcf()

        if isinstance(fig, figure.Figure):
            # Create directory if needed
            if not (os.path.exists(self._filename) and
                    os.path.isdir(self._filename)):
                os.makedirs(self._filename)

            # Figure out the name for image
            self.__nfigures += 1
            if name is None:
                name = 'Figure#'
            name = name.replace('#', str(self.__nfigures))

            # Save image
            fig_filename = os.path.join(self._filename,
                                        '%s.%s' % (name, self.fig_ext))
            if __debug__ and not self in debug.handlers:
                debug("REP_", "Saving figure '%s' into %s"
                      % (fig, fig_filename))

            fig.savefig(fig_filename, **savefig_kwargs)

            # adjust fig to the one to be included
            fig = fig_filename

        if __debug__ and not self in debug.handlers:
            debug("REP", "Adding figure '%s'" % fig)

        im = rplp.Image(fig, **kwargs)

        # If the inherent or provided width/height are too large -- shrink down
        imsize = (im.drawWidth, im.drawHeight)

        # Reduce the size if necessary so reportlab does not puke later on
        r = [float(d)/m for d,m in zip(imsize, self.pagesize)]
        maxr = max(r)
        if maxr > 1.0:
            if __debug__ and not self in debug.handlers:
                debug("REP_", "Shrinking figure by %.3g" % maxr)
            im.drawWidth  /= maxr
            im.drawHeight /= maxr

        self._story.append(im)


    def figures(self, *args, **kwargs):
        """Adds all present figures at once

        If called twice, it might add the same figure multiple times,
        so make sure to close all previous figures if you use
        figures() multiple times
        """
        if externals.exists('pylab', raise_=True):
            import pylab as pl
        figs = pl.matplotlib._pylab_helpers.Gcf.figs
        if __debug__ and not self in debug.handlers:
            debug('REP', "Saving all %d present figures" % len(figs))
        for fid, f in figs.iteritems():
            self.figure(f.canvas.figure, *args, **kwargs)

    @property
    def __flowbreak(self):
        return [rplp.Spacer(1, 0.2*rplu.inch),
                rplp.Paragraph("-" * 150, self.style),
                rplp.Spacer(1, 0.2*rplu.inch)]

    def flowbreak(self):
        """Just a marker for the break of the flow
        """
        if __debug__ and not self in debug.handlers:
            debug("REP", "Adding flowbreak")

        self._story.append(self.__flowbreak)


##     def __del__(self):
##         """Store report upon deletion
##         """
##         if __debug__ and not self in debug.handlers:
##             debug("REP", "Report is being deleted. Storing")
##         self.save()


    def save(self, add_preamble=True):
        """Saves PDF

        Parameters
        ----------
        add_preamble : bool
          Either to add preamble containing title/date/author information
        """

        if self.title is None:
            title = self.name + " report"
        else:
            title = self.title

        pageinfo = self.name + " data"

        ##REF: Name was automagically refactored
        def my_first_page(canvas, doc):
            canvas.saveState()
            canvas.setFont(self.font, 16)
            canvas.drawCentredString(self.pagesize[0]/2.0,
                                     self.pagesize[1]-108, title)
            canvas.setFont(self.font, 9)
            canvas.drawString(rplu.inch, 0.75 * rplu.inch,
                              "First Page / %s" % pageinfo)
            canvas.restoreState()

        ##REF: Name was automagically refactored
        def my_later_pages(canvas, doc):
            canvas.saveState()
            canvas.setFont(self.font, 9)
            canvas.drawString(rplu.inch, 0.75 * rplu.inch,
                              "Page %d %s" % (doc.page, pageinfo))
            canvas.restoreState()

        filename = self._filename + ".pdf"
        doc = rplp.SimpleDocTemplate(filename)

        story = self._story
        if add_preamble:
            story = self.__preamble + story

        if __debug__ and not self in debug.handlers:
            debug("REP", "Saving the report into %s" % filename)

        doc.build(story,
                  onFirstPage=my_first_page,
                  onLaterPages=my_later_pages)


########NEW FILE########
__FILENAME__ = report_dummy
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Dummy report class, to just be there in case if reportlab is not available.

"""

__docformat__ = 'restructuredtext'


from mvpa2.base import warning

if __debug__:
    from mvpa2.base import debug

def _dummy(*args, **kwargs):
    pass


class Report(object):
    """Dummy report class which does nothing but complains if used

    """

    def __init__(self, *args, **kwargs):
        """Initialize dummy report
        """
        warning("You are using DummyReport - no action will be taken. "
                "Please install reportlab to enjoy reporting facility "
                "within PyMVPA")

    def __getattribute__(self, index):
        """
        """
        # returns a dummy function
        return _dummy

########NEW FILE########
__FILENAME__ = state
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Classes to control and store state information.

It was devised to provide conditional storage 
"""

_DEV_DOC = """
TODO:
+ Use %r instead of %s in repr for ClassWithCollections
  Replaced few %s's... might be fixed
+ Check why __doc__ is not set in kernels
  Seems to be there now...
- puke if *args and **kwargs are provided and exceed necessary number
+ for Parameter add ability to make it 'final'/read-only
+ repr(instance.params) contains only default value -- not current or
  set in the constructor... not good
  Now if value is not default -- would be present
? check/possible assure order of parameters/ca to be as listed in the
  constructor
  There is _instance_index (could be set with 'index' parameter in
  Parameter). ATM it is used in documentation to list them in original
  order.
"""

# XXX: MH: The use of `index` as variable name confuses me. IMHO `index` refers
#          to a position in a container (i.e. list access). However, in this
#          file it is mostly used in the context of a `key` for dictionary
#          access. Can we refactor that?
# YOH: good point -- doing so... also we need to document somewhere that
#      names of Collectables are actually the keys in Collections
__docformat__ = 'restructuredtext'

from mvpa2.base.types import is_sequence_type
import mvpa2.support.copy as copy
from textwrap import TextWrapper

# Although not used here -- included into interface
from mvpa2.misc.exceptions import UnknownStateError
from mvpa2.base.attributes import IndexedCollectable, ConditionalAttribute
from mvpa2.base.dochelpers import enhanced_doc_string, borrowdoc, _repr_attrs, \
     get_docstring_split, _strid, _saferepr

from mvpa2.base import externals
# XXX local rename is due but later on
from mvpa2.base.collections import Collection as BaseCollection

if __debug__:
    from mvpa2.base import debug
    # XXX
    # To debug references on top level -- useful to keep around for now,
    # don't remove until refactoring is complete
    import sys
    _debug_references = 'ATTRREFER' in debug.active
    _debug_shits = []                   # remember all to don't complaint twice
    import traceback

_in_ipython = externals.exists('running ipython env')
# Separators around definitions, needed for ReST, but bogus for
# interactive sessions
_def_sep = ('`', '')[int(_in_ipython)]

_object_getattribute = object.__getattribute__
_object_setattr = object.__setattr__

###################################################################
# Collections
#
# TODO:
#  - refactor: use base.collections and unify this to that
#  - minimize interface


class Collection(BaseCollection):
    """Container of some IndexedCollectables.

     XXX Seems to be not used and duplicating functionality: `listing`
     (thus `listing` property)
    """

    def __init__(self, items=None, name=None):
        """Initialize the Collection

        Parameters
        ----------
        items : dict of IndexedCollectable's
          items to initialize with
        name : str
          name of the collection (as seen in the owner, e.g. 'ca')
        """
        # first set all stuff to nothing and later on charge it
        # this is important, since some of the stuff below relies in the
        # defaults
        self.name = name
        super(Collection, self).__init__(items)


    def __reduce__(self):
        #bcr = BaseCollection.__reduce__(self)
        res = (self.__class__, (self.items(), self.name,))
        #if __debug__ and 'COL_RED' in debug.active:
        #    debug('COL_RED', 'Returning %s for %s' % (res, self))
        return res


    @borrowdoc(BaseCollection)
    def copy(self, *args, **kwargs):
        # Create a generic copy of the collection
        anew = super(Collection, self).copy(*args, **kwargs)
        anew.name = self.name
        return anew


    def __str__(self):
        num = len(self)
        if __debug__ and "ST" in debug.active:
            maxnumber = 1000            # I guess all
        else:
            maxnumber = 4
        if self.name is not None:
            res = self.name
        else:
            res = ""
        res += "{"
        for i in xrange(min(num, maxnumber)):
            if i > 0:
                res += " "
            res += "%s" % str(self.values()[i])
        if len(self) > maxnumber:
            res += "..."
        res += "}"
        return res


    def _cls_repr(self):
        """Collection specific part of __repr__ for a class containing
        it, ie a part of __repr__ for the owner object

        Returns
        -------
        list
          list of items to be appended within __repr__ after a .join()
        """
        # XXX For now we do not expect any pure non-specialized
        # collection , thus just override in derived classes
        raise NotImplementedError, "Class %s should override _cls_repr" \
              % self.__class__.__name__

    def _is_initializable(self, key):
        """Checks if key could be assigned within collection via
        _initialize

        Returns
        -------
        bool
          value for a given `key`

        It is to facilitate dynamic assignment of collections' items
        within derived classes' __init__ depending on the present
        collections in the class.
        """
        # XXX Each collection has to provide what indexes it allows
        #     to be set within constructor. Custom handling of some
        #     arguments (like (dis|en)able_ca) is to be performed
        #     in _initialize
        # raise NotImplementedError, \
        #      "Class %s should override _is_initializable" \
        #      % self.__class__.__name__

        # YYY lets just check if it is in the keys
        return key in self.keys()


    def _initialize(self, key, value):
        """Initialize `key` (no check performed) with `value`
        """
        # by default we just set corresponding value
        self[key]._set(value, init=True)


    def __repr__(self):
        # do not include the owner arg, since that will most likely
        # cause recursions when the collection it self is included
        # into the repr() of the ClassWithCollections instance
        return "%s(items=%s, name=%s)" \
                  % (self.__class__.__name__,
                     repr(self.values()),
                     repr(self.name))

        items_s = ""
        sep = ""
        for item in self:
            try:
                itemvalue = "%r" % (self[item].value,)
                if len(itemvalue)>50:
                    itemvalue = itemvalue[:10] + '...' + itemvalue[-10:]
                items_s += "%s'%s':%s" % (sep, item, itemvalue)
                sep = ', '
            except:
                pass
        if items_s != "":
            s += "items={%s}" % items_s
        s += ")"
        return s


    def is_set(self, key=None):
        """If item (or any in the present or listed) was set

        Parameters
        ----------
        key : None or str or list of str
          What items to check if they were set in the collection
        """
        if not (key is None):
            if isinstance(key, basestring):
                return self[key].is_set
            else:
                items = key           # assume that we got some list
        else:
            items = self         # go through all the items

        for key in items:
            if self[key].is_set:
                return True
        return False


    def which_set(self):
        """Return list of keys which were set"""
        result = []
        # go through all members and if any is_set -- return True
        for key, v in self.iteritems():
            if v.is_set:
                result.append(key)
        return result


    def _action(self, key, func, missingok=False, **kwargs):
        """Run specific func either on a single item or on all of them

        Parameters
        ----------
        key : str
          Name of the conditional attribute
        func
          Function (not bound) to call given an item, and **kwargs
        missingok : bool
          If True - do not complain about wrong key
        """
        if isinstance(key, basestring):
            if key.lower() == 'all':
                for key_ in self:
                    self._action(key_, func, missingok=missingok, **kwargs)
            else:
                try:
                    func(self[key], **kwargs)
                except:
                    if missingok:
                        return
                    raise
        elif is_sequence_type(key):
            for item in key:
                self._action(item, func, missingok=missingok, **kwargs)
        else:
            raise ValueError, \
                  "Don't know how to handle  variable given by %s" % key


    def reset(self, key=None):
        """Reset the conditional attribute defined by `key`"""

        if not key is None:
            keys = [ key ]
        else:
            keys = self.keys()

        if len(self):
            for key in keys:
                # XXX Check if that works as desired
                self._action(key, self.values()[0].__class__.reset,
                             missingok=False)

    # XXX RF: not used anywhere / myself -- hence not worth it?
    @property
    def listing(self):
        """Return a list of registered ca along with the documentation"""

        # lets assure consistent litsting order
        items_ = self.items()
        items_.sort()
        return [ "%s%s%s: %s" % (_def_sep, str(x[1]), _def_sep, x[1].__doc__)
                 for x in items_ ]



class ParameterCollection(Collection):
    """Container of Parameters for a stateful object.
    """

#    def __init__(self, items=None, name=None):
#        """Initialize the conditional attributes of a derived class
#
#        Parameters
#        ----------
#        items : dict
#          dictionary of ca
#        """
#        Collection.__init__(self, items, name)
#

    def _cls_repr(self):
        """Part of __repr__ for the owner object
        """
        prefixes = []
        for k in self.keys():
            # list only params with not default values
            if self[k].is_default:
                continue
            prefixes.append("%s=%s" % (k, _saferepr(self[k].value)))
        return prefixes


    def reset_value(self, key, missingok=False):
        """Reset all parameters to default values"""
        from param import Parameter
        self._action(key, Parameter.reset_value, missingok=False)


class ConditionalAttributesCollection(Collection):
    """Container of ConditionalAttributes for a stateful object.

    :Groups:
     - `Public Access Functions`: `has_key`, `is_enabled`, `is_active`
     - `Access Implementors`: `listing`, `names`, `_get_enabled`
     - `Mutators`: `__init__`, `enable`, `disable`, `_set_enabled`
     - `R/O Properties`: `listing`, `names`, `items`
     - `R/W Properties`: `enabled`
    """

    def __init__(self, items=None, name=None):
        """Initialize the conditional attributes of a derived class

        Parameters
        ----------
        items : dict
          dictionary of ca
        name : str
          literal description. Usually just attribute name for the
          collection, e.g. 'ca'
        """
        Collection.__init__(self, items=items, name=name)

        self.__storedTemporarily = []
        """List to contain sets of enabled ca which were enabled
        temporarily.
        """

    #
    # XXX TODO: figure out if there is a way to define proper
    #           __copy__'s for a hierarchy of classes. Probably we had
    #           to define __getinitargs__, etc... read more...
    #
    #def __copy__(self):

    def _cls_repr(self):
        """Part of __repr__ for the owner object
        """
        prefixes = []
        for name, invert in ( ('enable', False), ('disable', True) ):
            ca = self._get_enabled(nondefault=False,
                                       invert=invert)
            if len(ca):
                prefixes.append("%s_ca=%s" % (name, str(ca)))
        return prefixes


    def _is_initializable(self, key):
        """Checks if key could be assigned within collection via
        setvalue
        """
        return key in ['enable_ca', 'disable_ca']


    def _initialize(self, key, value):
        if value is None:
            value = []
        if key == 'enable_ca':
            self.enable(value, missingok=True)
        elif key == 'disable_ca':
            self.disable(value)
        else:
            raise ValueError, "ConditionalAttributesCollection can accept only enable_ca " \
                  "and disable_ca arguments for the initialization. " \
                  "Got %s" % key

    # XXX RF: used only in meta -- those should become a bit tighter coupled
    #         and .copy / .update should only be used
    def _copy_ca_(self, fromstate, key=None, deep=False):
        """Copy known here ca from `fromstate` object into current object

        Parameters
        ----------
        fromstate : Collection or ClassWithCollections
          Source ca to copy from
        key : None or list of str
          If not to copy all set conditional attributes, key provides
          selection of what to copy
        deep : bool
          Optional control over the way to copy

        Crafted to overcome a problem mentioned above in the comment
        and is to be called from __copy__ of derived classes

        Probably sooner than later will get proper __getstate__,
        __setstate__
        """
        # Bad check... doesn't generalize well...
        # if not issubclass(fromstate.__class__, self.__class__):
        #     raise ValueError, \
        #           "Class  %s is not subclass of %s, " % \
        #           (fromstate.__class__, self.__class__) + \
        #           "thus not eligible for _copy_ca_"
        # TODO: FOR NOW NO TEST! But this beast needs to be fixed...
        operation = { True: copy.deepcopy,
                      False: copy.copy }[deep]

        if isinstance(fromstate, ClassWithCollections):
            fromstate = fromstate.ca

        if key is None:
            # copy all set ones
            for name in fromstate.which_set():#self.keys():
                #if fromstate.has_key(name):
                self[name].value = operation(fromstate[name].value)
        else:
            # workaround for supporting py2 and py3 dictionary interface
            try:
                has_key = fromstate.has_key
            except AttributeError:
                has_key = fromstate.__contains__
            for name in key:
                if has_key(name):
                    self[name].value = operation(fromstate[name].value)


    def is_enabled(self, key):
        """Returns `True` if state `key` is enabled"""
        return key in self and self[key].enabled


    def is_active(self, key):
        """Returns `True` if state `key` is known and is enabled"""
        return self.has_key(key) and self.is_enabled(key)


    def enable(self, key, value=True, missingok=False):
        """Enable  conditional attribute given in `key`"""
        self._action(key, ConditionalAttribute._set_enabled, missingok=missingok,
                     value=value)


    def disable(self, key):
        """Disable conditional attribute defined by `key` id"""
        self._action(key,
                     ConditionalAttribute._set_enabled, missingok=False, value=False)


    # TODO XXX think about some more generic way to grab temporary
    # snapshot of IndexedCollectables to be restored later on...
    def change_temporarily(self, enable_ca=None,
                           disable_ca=None, other=None):
        """Temporarily enable/disable needed ca for computation

        Enable or disable ca which are enabled in `other` and listed in
        `enable _ca`. Use `reset_enabled_temporarily` to reset
        to previous state of enabled.

        `other` can be a ClassWithCollections object or ConditionalAttributesCollection
        """
        if enable_ca == None:
            enable_ca = []
        if disable_ca == None:
            disable_ca = []
        self.__storedTemporarily.append(self.enabled)
        other_ = other
        if isinstance(other, ClassWithCollections):
            other = other.ca

        if not other is None:
            # lets take ca which are enabled in other but not in
            # self
            add_enable_ca = list(set(other.enabled).difference(
                 set(enable_ca)).intersection(self.keys()))
            if len(add_enable_ca)>0:
                if __debug__:
                    debug("ST",
                          "Adding ca %s from %s to be enabled temporarily "
                          "since they are not enabled in %s",
                          (add_enable_ca, other_, self))
                enable_ca += add_enable_ca

        # Lets go one by one enabling only disabled once... but could be as
        # simple as
        self.enable(enable_ca)
        self.disable(disable_ca)


    def reset_changed_temporarily(self):
        """Reset to previousely stored set of enabled ca"""
        if __debug__:
            debug("ST", "Resetting to previous set of enabled ca")
        if len(self.enabled)>0:
            self.enabled = self.__storedTemporarily.pop()
        else:
            raise ValueError("Trying to restore not-stored list of enabled " \
                             "ca")


    # XXX probably nondefault logic could be done at places?
    #     =False is used in __repr__ and _svmbase
    # XXX also may be we need enabled to return a subcollection
    #        with binds to ConditionalAttributes found to be enabled?
    def _get_enabled(self, nondefault=True, invert=False):
        """Return list of enabled ca

        Parameters
        ----------
        nondefault : bool
          Either to return also ca which are enabled simply by default
        invert : bool
          Would invert the meaning, ie would return disabled ca
        """
        if invert:
            fmatch = lambda y: not self.is_enabled(y)
        else:
            fmatch = self.is_enabled

        if nondefault:
            ffunc = fmatch
        else:
            ffunc = lambda y: fmatch(y) and \
                        self[y]._defaultenabled != self.is_enabled(y)
        return [n for n in self.keys() if ffunc(n)]


    def _set_enabled(self, keys):
        """Given `keys` make only those in the list enabled

        It might be handy to store set of enabled ca and then to restore
        it later on. It can be easily accomplished now::

        >>> from mvpa2.base.state import ClassWithCollections, ConditionalAttribute
        >>> class Blah(ClassWithCollections):
        ...   bleh = ConditionalAttribute(enabled=False, doc='Example')
        ...
        >>> blah = Blah()
        >>> ca_enabled = blah.ca.enabled
        >>> blah.ca.enabled = ['bleh']
        >>> blah.ca.enabled = ca_enabled
        """
        for key in self.keys():
            self.enable(key, key in keys)


    # Properties
    enabled = property(fget=_get_enabled, fset=_set_enabled)


##################################################################
# Base classes (and metaclass) which use collections
#


#
# Helper dictionaries for AttributesCollector
#
_known_collections = {
    # Quite a generic one but mostly in classifiers
    'ConditionalAttribute': ("ca", ConditionalAttributesCollection),
    # For classifiers only
    'Parameter': ("params", ParameterCollection),
    'KernelParameter': ("kernel_params", ParameterCollection),
#MH: no magic for datasets
#    # For datasets
#    # XXX custom collections needed?
#    'SampleAttribute':  ("sa", SampleAttributesCollection),
#    'FeatureAttribute': ("fa", SampleAttributesCollection),
#    'DatasetAttribute': ("dsa", SampleAttributesCollection),
    }


_col2class = dict(_known_collections.values())
"""Mapping from collection name into Collection class"""


#MH: no magic for datasets
#_COLLECTIONS_ORDER = ['sa', 'fa', 'dsa',
#                      'params', 'kernel_params', 'ca']
_COLLECTIONS_ORDER = ['params', 'kernel_params', 'ca']


class AttributesCollector(type):
    """Intended to collect and compose collections for any child
    class of ClassWithCollections
    """


    def __init__(cls, name, bases, dict):
        """
        Parameters
        ----------
        name : str
          Name of the class
        bases : iterable
          Base classes
        dict : dict
          Attributes.
        """
        if __debug__:
            debug(
                "COLR",
                "AttributesCollector call for %s.%s, where bases=%s, dict=%s ",
                (cls, name, bases, dict))

        super(AttributesCollector, cls).__init__(name, bases, dict)

        collections = {}
        for name, value in dict.iteritems():
            if isinstance(value, IndexedCollectable):
                baseclassname = value.__class__.__name__
                col = _known_collections[baseclassname][0]
                # XXX should we allow to throw exceptions here?
                if col not in collections:
                    collections[col] = {}
                collections[col][name] = value
                # and assign name if not yet was set
                if value.name is None:
                    value.name = name
                # !!! We do not keep copy of this attribute static in the class.
                #     Due to below traversal of base classes, we should be
                #     able to construct proper collections even in derived classes
                delattr(cls, name)

        # XXX can we first collect parent's ca and then populate with ours?
        # TODO

        for base in bases:
            if hasattr(base, "__class__") and \
                   base.__class__ == AttributesCollector:
                # TODO take care about overriding one from super class
                # for state in base.ca:
                #    if state[0] =
                newcollections = base._collections_template
                if len(newcollections) == 0:
                    continue
                if __debug__: # XXX RF:  and "COLR" in debug.active:
                    debug("COLR",
                          "Collect collections %s for %s from %s",
                          (newcollections, cls, base))
                for col, collection in newcollections.iteritems():
                    if col in collections:
                        collections[col].update(collection)
                    else:
                        collections[col] = collection


        if __debug__:
            debug("COLR",
                  "Creating ConditionalAttributesCollection template %s "
                  "with collections %s", (cls, collections.keys()))

        # if there is an explicit
        if hasattr(cls, "_ATTRIBUTE_COLLECTIONS"):
            for col in cls._ATTRIBUTE_COLLECTIONS:
                if not col in _col2class:
                    raise ValueError, \
                          "Requested collection %s is unknown to collector" % \
                          col
                if not col in collections:
                    collections[col] = None

        # TODO: check on conflict in names of Collections' items!  since
        # otherwise even order is not definite since we use dict for
        # collections.
        # XXX should we switch to tuple?

        for col, colitems in collections.iteritems():
            # so far we collected the collection items in a dict, but the new
            # API requires to pass a _list_ of collectables instead of a dict.
            # So, whenever there are items, we pass just the values of the dict.
            # There is no information last, since the keys of the dict are the
            # name attributes of each collectable in the list.
            if not colitems is None:
                collections[col] = _col2class[col](items=colitems.values())
            else:
                collections[col] = _col2class[col]()

        setattr(cls, "_collections_template", collections)

        #
        # Expand documentation for the class based on the listed
        # parameters an if it is stateful
        #
        # TODO -- figure nice way on how to alter __init__ doc directly...
        textwrapper = TextWrapper(subsequent_indent="    ",
                                  initial_indent="    ",
                                  width=70)

        # Parameters
        paramsdoc = []
        paramscols = []
        for col in ('params', 'kernel_params'):
            if col in collections:
                paramscols.append(col)
                # lets at least sort the parameters for consistent output
                col_items = collections[col]
                iparams = [(v._instance_index, k)
                           for k,v in col_items.iteritems()]
                iparams.sort()
                paramsdoc += [(col_items[iparam[1]].name,
                               col_items[iparam[1]]._paramdoc())
                              for iparam in iparams]

        # Parameters collection could be taked hash of to decide if
        # any were changed? XXX may be not needed at all?
        setattr(cls, "_paramscols", paramscols)

        # States doc
        cadoc = ""
        if 'ca' in collections:
            paramsdoc += [
                ('enable_ca',
                 "enable_ca : None or list of str\n  "
                 "Names of the conditional attributes which should "
                 "be enabled in addition\n  to the default ones"),
                ('disable_ca',
                 "disable_ca : None or list of str\n  "
                 "Names of the conditional attributes which should "
                 "be disabled""")]
            if len(collections['ca']):
                cadoc += '\n'.join(['* ' + x
                                    for x in collections['ca'].listing])
                cadoc += "\n\n(Conditional attributes enabled by default suffixed with `+`)"
            if __debug__:
                debug("COLR", "Assigning __cadoc to be %s", (cadoc,))
            setattr(cls, "_cadoc", cadoc)

        if paramsdoc != "":
            if __debug__ and 'COLR' in debug.active:
                debug("COLR", "Assigning __paramsdoc to be %s", (paramsdoc,))
            setattr(cls, "_paramsdoc", paramsdoc)

        if len(paramsdoc) or cadoc != "":
            cls.__doc__ = enhanced_doc_string(cls, *bases)



class ClassWithCollections(object):
    """Base class for objects which contain any known collection

    Classes inherited from this class gain ability to access
    collections and their items as simple attributes. Access to
    collection items "internals" is done via <collection_name> attribute
    and interface of a corresponding `Collection`.
    """

    _DEV__doc__ = """
    TODO: rename 'descr'? -- it should simply
          be 'doc' -- no need to drag classes docstring imho.
    """

    __metaclass__ = AttributesCollector

    def __new__(cls, *args, **kwargs):
        """Instantiate ClassWithCollections object
        """
        self = super(ClassWithCollections, cls).__new__(cls)

        s__dict__ = self.__dict__

        # init variable
        # XXX: Added as pylint complained (rightfully) -- not sure if false
        # is the proper default
        self.__params_set = False

        # need to check to avoid override of enabled ca in the case
        # of multiple inheritance, like both ClassWithCollectionsl and
        # Harvestable
        if '_collections' not in s__dict__:
            s__class__ = self.__class__

            collections = copy.deepcopy(s__class__._collections_template)
            s__dict__['_collections'] = collections
            s__dict__['_known_attribs'] = {}
            """Dictionary to contain 'links' to the collections from each
            known attribute. Is used to gain some speed up in lookup within
            __getattribute__ and __setattr__
            """

            # Assign owner to all collections
            for col, collection in collections.iteritems():
                if col in s__dict__:
                    raise ValueError, \
                          "Object %s has already attribute %s" % \
                          (self, col)
                s__dict__[col] = collection
                collection.name = col

            self.__params_set = False

        if __debug__:
            descr = kwargs.get('descr', None)
            debug("COL", "ClassWithCollections.__new__ was done "
                  "for %s%s with descr=%s",
                  (s__class__.__name__, _strid(self), descr))

        return self

    @classmethod
    def _custom_kwargs_sort_items(cls, **kwargs):
        """Custom sorting of kwargs to fulfill premises of pymvpa

        Some time ago the world was square and we could assume
        that enable_ca always comes before disable_ca.  But we
        were misguided, thus now we need to provide a custom
        sorting routine to sort disable_ca after enable_ca
        """

        def _key(x):
            k = x[0]
            if k == 'enable_ca':
                return 'ZZZZZZ'
            return k

        return sorted(kwargs.items(), key=_key)

    def __init__(self, descr=None, **kwargs):
        """Initialize ClassWithCollections object

        Parameters
        ----------
        descr : str
          Description of the instance
        """
        # Note: __params_set was initialized in __new__
        if not self.__params_set:
            self.__descr = descr
            """Set humane description for the object"""

            # To avoid double initialization in case of multiple inheritance
            self.__params_set = True

            collections = self._collections
            # Assign attributes values if they are given among
            # **kwargs

            for arg, argument in self._custom_kwargs_sort_items(**kwargs):
                isset = False
                for collection in collections.itervalues():
                    if collection._is_initializable(arg):
                        collection._initialize(arg, argument)
                        isset = True
                        break
                if isset:
                    _ = kwargs.pop(arg)
                else:
                    known_params = reduce(
                       lambda x,y:x+y,
                        [x.keys()
                         for x in collections.itervalues()
                         if not isinstance(x, ConditionalAttributesCollection)
                         ], [])
                    _, kwargs_list, _ = get_docstring_split(self.__init__)
                    known_params = sorted(known_params
                                          + [x[0] for x in kwargs_list])
                    raise TypeError(
                        "Unexpected keyword argument %s=%s for %s."
                        % (arg, argument, self)
                        + "\n\tValid parameters are: %s" % ', '.join(known_params))

            ## Initialize other base classes
            ##  commented out since it seems to be of no use for now
            #if init_classes is not None:
            #    # return back stateful arguments since they might be
            #    # processed by underlying classes
            #    kwargs.update(kwargs_stateful)
            #    for cls in init_classes:
            #        cls.__init__(self, **kwargs)
            #else:
            #    if len(kwargs)>0:
            #        known_params = reduce(lambda x, y: x + y, \
            #                            [x.keys() for x in collections],
            #                            [])
            #        raise TypeError, \
            #              "Unknown parameters %s for %s." % (kwargs.keys(),
            #                                                 self) \
            #              + " Valid parameters are %s" % known_params
        if __debug__:
            debug("COL", "ClassWithCollections.__init__ was done "
                  "for %s%s with descr=%s",
                  (self.__class__.__name__, _strid(self), descr))


    #__doc__ = enhanced_doc_string('ClassWithCollections', locals())

    if __debug__ and _debug_references:
        def __debug_references_call(self, method, key):
            """Helper for debugging location of the call
            """
            s_dict = _object_getattribute(self, '__dict__')
            known_attribs = s_dict['_known_attribs']
            if key in known_attribs:
                clsstr = str(self.__class__)
                # Skip some False positives
                if 'mvpa2.datasets' in clsstr and 'Dataset' in clsstr and \
                       (key in ['targets', 'chunks', 'samples', 'mapper']):
                    return
                colname = known_attribs[key]
                # figure out and report invocation location
                ftb = traceback.extract_stack(limit=4)[-3]
                shit = '\n%s:%d:[%s %s.%s]: %s\n' % \
                       (ftb[:2] + (method, colname, key) + (ftb[3],))
                if not (shit in _debug_shits):
                    _debug_shits.append(shit)
                    sys.stderr.write(shit)


        def __getattribute__(self, key):
            # return all private ones first since smth like __dict__ might be
            # queried by copy before instance is __init__ed
            if key == '':
                raise AttributeError, "Silly to request attribute ''"

            if key[0] == '_':
                return _object_getattribute(self, key)

            s_dict = _object_getattribute(self, '__dict__')
            # check if it is a known collection
            collections = s_dict['_collections']
            if key in collections:
                return collections[key]

            # MH: No implicite outbreak of collection items into the namespace of
            #     the parent class
            ## check if it is a part of any collection
            #known_attribs = s_dict['_known_attribs']
            #if key in known_attribs:
            #    return collections[known_attribs[key]].getvalue(key)

            # Report the invocation location if applicable
            self.__debug_references_call('get', key)

            # just a generic return
            return _object_getattribute(self, key)


        def __setattr__(self, key, value):
            if key == '':
                raise AttributeError, "Silly to set attribute ''"

            if key[0] == '_':
                return _object_setattr(self, key, value)

            if __debug__ and _debug_references:
                # Report the invocation location if applicable
                self.__debug_references_call('set', key)

            ## YOH: if we are to disable access at instance level -- do it in
            ##      set as well ;)
            ##
            ## # Check if a part of a collection, and set appropriately
            ## s_dict = _object_getattribute(self, '__dict__')
            ## known_attribs = s_dict['_known_attribs']
            ## if key in known_attribs:
            ##     collections = s_dict['_collections']
            ##     return collections[known_attribs[key]].setvalue(key, value)

            # Generic setattr
            return _object_setattr(self, key, value)


    # XXX not sure if we shouldn't implement anything else...
    def reset(self):
        for collection in self._collections.values():
            collection.reset()


    def __str__(self):
        s = "%s:" % (self.__class__.__name__)
        if self.__descr is not None:
            s += "/%s " % self.__descr
        if hasattr(self, "_collections"):
            for col, collection in self._collections.iteritems():
                s += " %d %s:%s" % (len(collection), col, str(collection))
        return s


    def __repr__(self, prefixes=None, fullname=False):
        """String definition of the object of ClassWithCollections object

        Parameters
        ----------
        prefixes : list of str
          What other prefixes to prepend to list of arguments
        fullname : bool
          Either to include full name of the module
        """
        prefixes = prefixes or []
        prefixes = prefixes[:]          # copy list
        # filter using __init__doc__exclude__
        for f in getattr(self, '__init__doc__exclude__', []):
            prefixes = [x for x in prefixes if not x.startswith(f+'=')]
        id_str = ""
        module_str = ""

        if __debug__:
            if 'MODULE_IN_REPR' in debug.active:
                fullname = True
            if 'ID_IN_REPR' in debug.active:
                id_str = _strid(self)

        if fullname:
            modulename = '%s' % self.__class__.__module__
            if modulename != "__main__":
                module_str = "%s." % modulename

        # Collections' attributes
        collections = self._collections
        # we want them in this particular order
        for col in _COLLECTIONS_ORDER:
            collection = collections.get(col, None)
            if collection is None:
                continue
            prefixes += collection._cls_repr()

        # Description if present
        prefixes += _repr_attrs(self, ['descr'])

        out = "%s%s(%s)%s" % (module_str, self.__class__.__name__,
                               ', '.join(prefixes), id_str)
        # To possibly debug mass repr/str-fication
        # print str(self), ' REPR: ', out
        return out

    descr = property(lambda self: self.__descr,
                     doc="Description of the object if any")

########NEW FILE########
__FILENAME__ = types
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Things concerned with types and type-checking in PyMVPA"""

import sys
import numpy as np


def is_datasetlike(obj):
    """Check if an object looks like a Dataset."""
    if hasattr(obj, 'samples') and \
       hasattr(obj, 'sa') and \
       hasattr(obj, 'fa') and \
       hasattr(obj, 'a'):
        return True

    return False


def accepts_dataset_as_samples(fx):
    """Decorator to extract samples from Datasets.

    Little helper to allow methods to be written for plain data (if they
    don't need information from a Dataset), but at the same time also
    accept whole Datasets as input.
    """
    def extract_samples(obj, data):
        if is_datasetlike(data):
            return fx(obj, data.samples)
        else:
            return fx(obj, data)
    return extract_samples


def asobjarray(x):
    """Generates numpy.ndarray with dtype object from an iterable

    Is needed to assure object dtype, so first empty array of
    dtype=object needs to be constructed and then only items to be
    assigned.

    Parameters
    ----------
    x : list or tuple or ndarray
    """
    res = np.empty(len(x), dtype=object)
    res[:] = x
    return res

# compatibility layer for Python3
if sys.version_info[0] < 3:

    from operator import isSequenceType as is_sequence_type

    def as_char(x):
        """Identity mapping in python2"""
        return x

else:

    def is_sequence_type(inst):
        """Return True if an instance is of an iterable type

        Verified by wrapping with iter() call
        """
        try:
            _ = iter(inst)
            return True
        except:
            return False

    import codecs

    def as_char(x):
        """Return character representation for a unicode symbol"""
        return codecs.latin_1_encode(x)[0]

########NEW FILE########
__FILENAME__ = verbosity
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Verbose output and debugging facility

Examples:
from verbosity import verbose, debug; debug.active = [1,2,3]; debug(1, "blah")

"""

__docformat__ = 'restructuredtext'

from sys import stdout, stderr
# GOALS
#  any logger should be able
#   to log into a file or stdout/stderr
#   provide ability to log with/without a new line at the end
#
#  debug logger should be able
#    to log sets of debug statements
#    add/remove debug setid items
#    give verbose description about registered debugset items

class Logger(object):
    """Base class to provide logging
    """

    def __init__(self, handlers=None):
        """Initialize the logger with a set of handlers to use for output

        Each hanlder must have write() method implemented
        """
        if handlers == None:
            handlers = [stdout]
        self.__close_handlers = []
        self.__handlers = []            # pylint friendliness
        self._set_handlers(handlers)
        self.__lfprev = True
        self.__crprev = 0               # number of symbols in previous cr-ed

    def __del__(self):
        self._close_opened_handlers()

    ##REF: Name was automagically refactored
    def _set_handlers(self, handlers):
        """Set list of handlers for the log.

        A handler can be opened files, stdout, stderr, or a string, which
        will be considered a filename to be opened for writing
        """
        handlers_ = []
        self._close_opened_handlers()
        for handler in handlers:
            if isinstance(handler, basestring):
                try:
                    handler = {'stdout' : stdout,
                               'stderr' : stderr}[handler.lower()]
                except:
                    try:
                        handler = open(handler, 'w')
                        self.__close_handlers.append(handler)
                    except:
                        raise RuntimeError, \
                              "Cannot open file %s for writing by the logger" \
                              % handler
            handlers_.append(handler)
        self.__handlers = handlers_

    ##REF: Name was automagically refactored
    def _close_opened_handlers(self):
        """Close opened handlers (such as opened logfiles
        """
        for handler in self.__close_handlers:
            handler.close()

    ##REF: Name was automagically refactored
    def _get_handlers(self):
        """Return active handlers
        """
        return self.__handlers


    def __call__(self, msg, args=None, lf=True, cr=False, **kwargs):
        """Write msg to each of the handlers.

        It can append a newline (lf = Line Feed) or return
        to the beginning before output and to take care about
        cleaning previous message if present

        it appends a newline (lf = Line Feed) since most commonly each
        call is a separate message
        """

        if args is not None:
            try:
                msg = msg % args
            except Exception as e:
                msg = "%s [%% FAILED due to %s]" % (msg, e)

        if 'msgargs' in kwargs:
            msg = msg % kwargs['msgargs']

        if cr:
            msg_ = ""
            if self.__crprev > 0:
                # wipe out older line to make sure to see no ghosts
                msg_ = "\r%s" % (" "*self.__crprev)
            msg_ += "\r" + msg
            self.__crprev = len(msg)
            msg = msg_
            # since it makes no sense this days for cr and lf,
            # override lf
            lf = False
        else:
            self.__crprev += len(msg)

        if lf:
            msg = msg + "\n"
            self.__crprev = 0           # nothing to clear

        for handler in self.__handlers:
            try:
                handler.write(msg)
            except:
                print "Failed writing on handler %s" % handler
                raise
            try:
                handler.flush()
            except:
                # it might be not implemented..
                pass

        self.__lfprev = lf

    handlers = property(fget=_get_handlers, fset=_set_handlers)
    lfprev = property(fget=lambda self:self.__lfprev)



class LevelLogger(Logger):
    """Logger not to log anything with a level smaller than specified.
    """

    def __init__(self, level=0, indent=" ", *args, **kwargs):
        """
        Parameters
        ----------
        level : int, optional
          Level to consider be active.
        indent : str, optional
          String to use for indentation.
        """
        Logger.__init__(self, *args, **kwargs)
        self.__level = level            # damn pylint ;-)
        self.__indent = indent
        self._set_level(level)
        self._set_indent(indent)

    ##REF: Name was automagically refactored
    def _set_level(self, level):
        """Set logging level
        """
        if __debug__:
            try:
                from mvpa2.base import debug
                debug('VERBOSE', 'Setting verbosity to %r from %r',
                      (self.__level, level))
            except:
                pass
        ilevel = int(level)
        if ilevel < 0:
            raise ValueError, \
                  "Negative verbosity levels (got %d) are not supported" \
                  % ilevel
        self.__level = ilevel


    ##REF: Name was automagically refactored
    def _set_indent(self, indent):
        """Either to indent the lines based on message log level"""
        self.__indent = "%s" % indent


    def __call__(self, level, msg, *args, **kwargs):
        """Write msg and indent using self.indent it if it was requested.

        It appends a newline since most commonly each call is a separate
        message
        """
        if level <= self.level:
            if self.lfprev and self.indent:
                # indent if previous line ended with newline
                msg = self.indent * level + msg
            Logger.__call__(self, msg, *args, **kwargs)

    level = property(fget=lambda self: self.__level, fset=_set_level)
    indent = property(fget=lambda self: self.__indent, fset=_set_indent)


class OnceLogger(Logger):
    """Logger which prints a message for a given ID just once.

    It could be used for one-time warning to don't overfill the output
    with useless repeatative messages.
    """

    def __init__(self, *args, **kwargs):
        """Define once logger.
        """
        Logger.__init__(self, *args, **kwargs)
        self._known = {}


    def __call__(self, ident, msg, count=1, *args, **kwargs):
        """Write `msg` if `ident` occured less than `count` times by now.
        """
        if ident not in self._known:
            self._known[ident] = 0

        if count < 0 or self._known[ident] < count:
            self._known[ident] += 1
            Logger.__call__(self, msg, *args, **kwargs)


class SetLogger(Logger):
    """Logger which prints based on defined sets identified by Id.
    """

    def __init__(self, register=None, active=None, printsetid=True,
                 *args, **kwargs):
        """
        Parameters
        ----------
        register : dict or None
          What Ids are to be known. Each item dictionary contains consists
          of concise key and a description as the value.
        active : iterable
          What Ids to consider active upon initialization.
        printsetid : bool, optional
          Either to prefix each line with the target Id of a set in which
          the line was printed to (default behavior).
        """
        if register is None:
            register = {}
        if active == None:
            active = []
        Logger.__init__(self, *args, **kwargs)
        self.__printsetid = printsetid
        self.__registered = register    # all "registered" sets descriptions
        # which to output... pointless since __registered
        self._set_active(active)
        self._set_printsetid(printsetid)


    ##REF: Name was automagically refactored
    def _set_active(self, active):
        """Set active logging set
        """
        # just unique entries... we could have simply stored Set I guess,
        # but then smth like debug.active += ["BLAH"] would not work
        from mvpa2.base import verbose
        self.__active = []
        registered_keys = self.__registered.keys()
        for item in list(set(active)):
            if item == '':
                continue
            if isinstance(item, basestring):
                if item in ['?', 'list', 'help']:
                    self.print_registered(detailed=(item != '?'))
                    raise SystemExit(0)
                if item.upper() == "ALL":
                    verbose(2, "Enabling all registered debug handlers")
                    self.__active = registered_keys
                    break
                # try to match item as it is regexp
                regexp_str = "^%s$" % item
                try:
                    regexp = re.compile(regexp_str)
                except:
                    raise ValueError, \
                          "Unable to create regular expression out of  %s" % item
                matching_keys = filter(regexp.match, registered_keys)
                toactivate = matching_keys
                if len(toactivate) == 0:
                    ids = self.registered.keys()
                    ids.sort()
                    raise ValueError, \
                          "Unknown debug ID '%s' was asked to become active," \
                          " or regular expression '%s' did not get any match" \
                          " among known ids: %s" \
                          % (item, regexp_str, ids)
            else:
                toactivate = [item]

            # Lets check if asked items are known
            for item_ in toactivate:
                if not (item_ in registered_keys):
                    raise ValueError, \
                          "Unknown debug ID %s was asked to become active" \
                          % item_
            self.__active += toactivate

        self.__active = list(set(self.__active)) # select just unique ones
        self.__maxstrlength = max([len(str(x)) for x in self.__active] + [0])
        if len(self.__active):
            verbose(2, "Enabling debug handlers: %s" % `self.__active`)


    ##REF: Name was automagically refactored
    def _set_printsetid(self, printsetid):
        """Either to print set Id at each line"""
        self.__printsetid = printsetid


    def __call__(self, setid, msg, *args, **kwargs):
        """
        Write msg

        It appends a newline since most commonly each call is a separate
        message
        """

        if setid in self.__active:
            if len(msg) > 0 and self.__printsetid:
                msg = "[%%-%ds] " % self.__maxstrlength % (setid) + msg
            Logger.__call__(self, msg, *args, **kwargs)


    def register(self, setid, description):
        """ "Register" a new setid with a given description for easy finding
        """

        if setid in self.__registered:
            raise ValueError, \
                  "Setid %s is already known with description '%s'" % \
                  (`setid`, self.__registered[setid])
        self.__registered[setid] = description


    ##REF: Name was automagically refactored
    def set_active_from_string(self, value):
        """Given a string listing registered(?) setids, make then active
        """
        # somewhat evil but works since verbose must be initiated
        # by now
        self.active = value.split(",")


    def print_registered(self, detailed=True):
        print "Registered debug entries: ",
        kd = self.registered
        rks = sorted(kd.keys())
        maxl = max([len(k) for k in rks])
        if not detailed:
            # short list
            print ', '.join(rks)
        else:
            print
            for k in rks:
                print '%%%ds  %%s' % maxl % (k, kd[k])


    printsetid = property(fget=lambda self: self.__printsetid, \
                          fset=_set_printsetid)
    active = property(fget=lambda self: self.__active, fset=_set_active)
    registered = property(fget=lambda self: self.__registered)


if __debug__:

    import os, re
    import traceback
    import time
    from os import getpid
    from os.path import basename, dirname

    __pymvpa_pid__ = getpid()


    def parse_status(field='VmSize', value_only=False):
        """Return stat information on current process.

        Usually it is needed to know where the memory is gone, that is
        why VmSize is the default for the field to spit out

        TODO: Spit out multiple fields. Use some better way than parsing proc
        """
        regex = re.compile('^%s:' % field)
        match = None
        try:
            for l in open('/proc/%d/status' % __pymvpa_pid__):
                if regex.match(l):
                    match = l.strip()
                    break
            if match:
                match = re.sub('[ \t]+', ' ', match)
        except IOError:
            pass
        if match and value_only:
            match = match.split(':', 1)[1].lstrip()
        return match

    def get_vmem_from_status():
        """Return utilization of virtual memory

        Deprecated implementation which relied on parsing proc/PID/status
        """
        rss, vms = [parse_status(field=x, value_only=True)
                  for x in ['VmRSS', 'VmSize']]
        if rss is None or vms is None:
            # So not available on this system -- signal with negatives
            # but do not crash
            return (-1, -1)
        if rss[-3:] == vms[-3:] and rss[-3:] == ' kB':
            # the same units
            rss = int(rss[:-3])                # strip from rss
            vms = int(vms[:-3])
        return (rss, vms)

    try:
        # we prefer to use psutil if available
        # and let's stay away from "externals" module for now
        # Note: importing as __Process so it does not get
        #       'queried' by autodoc leading to an exception
        #       while being unable to get values for the properties
        from psutil import Process as __Process
        __pymvpa_process__ = __Process(__pymvpa_pid__)

        def get_vmem():
            """Return utilization of virtual memory

            Generic implementation using psutil
            """
            mi = __pymvpa_process__.get_memory_info()
            # in later versions of psutil mi is a named tuple.
            # but that is not the case on Debian squeeze with psutil 0.1.3
            rss = mi[0] / 1024
            vms = mi[1] / 1024
            return (rss, vms)

    except ImportError:
        get_vmem = get_vmem_from_status

    def get_vmem_str():
        """Return  a string summary about utilization of virtual_memory
        """
        vmem = get_vmem()
        try:
            return "RSS/VMS: %d/%d kB" % vmem
        except:
            return "RSS/VMS: %s" % str(vmem)

    def _get_vmem_max_str_gen():
        """Return peak vmem utilization so far.

        It is a generator, get_vmem_max_str later is bound to .next
        of it - to mimic static variables
        """
        rss_max = 0
        vms_max = 0

        while True:
            rss, vms = get_vmem()
            rss_max = max(rss, rss_max)
            vms_max = max(vms, vms_max)
            yield "max RSS/VMS: %d/%d kB" % (rss_max, vms_max)
    get_vmem_max_str = _get_vmem_max_str_gen().next

    def mbasename(s):
        """Custom function to include directory name if filename is too common

        Also strip .py at the end
        """
        base = basename(s)
        if base.endswith('.py'):
            base = base[:-3]
        if base in set(['base', '__init__']):
            base = basename(dirname(s)) + '.' + base
        return base

    class TraceBack(object):
        """Customized traceback to be included in debug messages
        """

        def __init__(self, collide=False):
            """Initialize TrackBack metric

            Parameters
            ----------
            collide : bool
              if True then prefix common with previous invocation gets
              replaced with ...
            """
            self.__prev = ""
            self.__collide = collide

        def __call__(self):
            ftb = traceback.extract_stack(limit=100)[:-2]
            entries = [[mbasename(x[0]), str(x[1])] for x in ftb]
            entries = [ e for e in entries if e[0] != 'unittest' ]

            # lets make it more consize
            entries_out = [entries[0]]
            for entry in entries[1:]:
                if entry[0] == entries_out[-1][0]:
                    entries_out[-1][1] += ',%s' % entry[1]
                else:
                    entries_out.append(entry)
            sftb = '>'.join(['%s:%s' % (mbasename(x[0]),
                                        x[1]) for x in entries_out])
            if self.__collide:
                # lets remove part which is common with previous invocation
                prev_next = sftb
                common_prefix = os.path.commonprefix((self.__prev, sftb))
                common_prefix2 = re.sub('>[^>]*$', '', common_prefix)

                if common_prefix2 != "":
                    sftb = '...' + sftb[len(common_prefix2):]
                self.__prev = prev_next

            return sftb


    class RelativeTime(object):
        """Simple helper class to provide relative time it took from previous
        invocation"""

        def __init__(self, format="%3.3f sec"):
            """
            Parameters
            ----------
            format : str
              String format to use for reporting time.
            """
            self.__prev = None
            self.__format = format

        def __call__(self):
            dt = 0.0
            ct = time.time()
            if not self.__prev is None:
                dt = ct - self.__prev
            self.__prev = ct
            return self.__format % dt


    class DebugLogger(SetLogger):
        """
        Logger for debugging purposes.

        Expands SetLogger with ability to print some interesting information
        (named Metric... XXX) about current process at each debug printout
        """

        _known_metrics = {
            # TODO: make up Windows-friendly version or pure Python platform
            # independent version (probably just make use of psutil)
            'vmem' : get_vmem_str,
            'vmem_max' : get_vmem_max_str,
            'pid' : getpid, # lambda : parse_status(field='Pid'),
            'asctime' : time.asctime,
            'tb' : TraceBack(),
            'tbc' : TraceBack(collide=True),
            }

        def __init__(self, metrics=None, offsetbydepth=True, *args, **kwargs):
            """
            Parameters
            ----------
            metrics : iterable of (func or str) or None
              What metrics (functions) to be reported.  If item is a string,
              it is matched against `_known_metrics` keys.
            offsetbydepth : bool, optional
              Either to offset lines depending on backtrace depth (default
              behavior).
            *args, **kwargs
              Passed to SetLogger initialization  XXX
            """
            if metrics == None:
                metrics = []
            SetLogger.__init__(self, *args, **kwargs)
            self.__metrics = []
            self._offsetbydepth = offsetbydepth
            self._reltimer = RelativeTime()
            self._known_metrics = DebugLogger._known_metrics
            self._known_metrics['reltime'] = self._reltimer
            for metric in metrics:
                self._registerMetric(metric)


        ##REF: Name was automagically refactored
        def register_metric(self, func):
            """Register some metric to report

            func can be either a function call or a string which should
            correspond to known metrics
            """

            if isinstance(func, basestring):
                if func in ['all', 'ALL']:
                    func = self._known_metrics.keys()

            if isinstance(func, basestring):
                if func in DebugLogger._known_metrics:
                    func = DebugLogger._known_metrics[func]
                else:
                    if func in ['?', 'list', 'help']:
                        print 'Known debug metrics: ', \
                              ', '.join(DebugLogger._known_metrics.keys())
                        raise SystemExit(0)
                    else:
                        raise ValueError, \
                              "Unknown name %s for metric in DebugLogger" % \
                              func + " Known metrics are " + \
                              `DebugLogger._known_metrics.keys()`
            elif isinstance(func, list):
                self.__metrics = []     # reset
                for item in func:
                    self.register_metric(item)
                return

            if not func in self.__metrics:
                try:
                    from mvpa2.base import debug
                    debug("DBG", "Registering metric %s" % func)
                    self.__metrics.append(func)
                except:
                    pass


        def __call__(self, setid, msg, *args, **kwargs):

            if setid not in self.registered:
                raise ValueError, "Not registered debug ID %s" % setid

            if not setid in self.active:
                # don't even compute the metrics, since they might
                # be statefull as RelativeTime
                return

            msg_ = ' / '.join([str(x()) for x in self.__metrics])

            if len(msg_) > 0:
                msg_ = "{%s}" % msg_

            if len(msg) > 0:
                # determine blank offset using backstacktrace
                if self._offsetbydepth:
                    level = len(traceback.extract_stack()) - 2
                else:
                    level = 1

                if len(msg) > 250 and 'DBG' in self.active and not setid.endswith('_TB'):
                    tb = traceback.extract_stack(limit=2)
                    msg += "  !!!2LONG!!!. From %s" % str(tb[0])

                msg = "DBG%s:%s%s" % (msg_, " "*level, msg)
                SetLogger.__call__(self, setid, msg, *args, **kwargs)
            else:
                msg = msg_
                Logger.__call__(self, msg, *args, **kwargs)




        ##REF: Name was automagically refactored
        def _set_offset_by_depth(self, b):
            self._offsetbydepth = b

        offsetbydepth = property(fget=lambda x:x._offsetbydepth,
                                 fset=_set_offset_by_depth)

        metrics = property(fget=lambda x:x.__metrics,
                           fset=register_metric)


if not __debug__:
    class BlackHoleLogger(SetLogger):
        '''A logger that does absolutely nothing - it is used as a fallback
        so that debug(...) can still be called even if not __debug__'''
        def __init__(self, metrics=None, offsetbydepth=True, *args, **kwargs):
            '''Initializes the logger - ignores all input arguments'''

            # do not be evil - initialize through the parent class
            SetLogger.__init__(self, *args, **kwargs)

        def __call__(self, setid, msg, *args, **kwargs):
            pass

        def register_metric(self, func):
            pass

        def register(self, setid, description):
            pass

        def set_active_from_string(self, value):
            pass

        def print_registered(self, detailed=True):
            print "BlackHoleLogger: nothing registered "

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Plumbing for all learners (classifiers and regressions)"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.support.copy import deepcopy

import time

from mvpa2.base.types import is_datasetlike, accepts_dataset_as_samples
from mvpa2.measures.base import Measure
from mvpa2.base.learner import Learner, FailedToPredictError
from mvpa2.datasets.base import Dataset
from mvpa2.misc.support import idhash
from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.param import Parameter
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.base.dochelpers import _str, _strid

from mvpa2.clfs.transerror import ConfusionMatrix, RegressionStatistics

from mvpa2.base import warning

if __debug__:
    from mvpa2.base import debug

__all__ = [ 'Classifier',
            'accepts_dataset_as_samples', 'accepts_samples_as_dataset']

def accepts_samples_as_dataset(fx):
    """Decorator to wrap samples into a Dataset.

    Little helper to allow methods to accept plain data whenever
    dataset is generally required.
    """
    def wrap_samples(obj, data, *args, **kwargs):
        if is_datasetlike(data):
            return fx(obj, data, *args, **kwargs)
        else:
            return fx(obj, Dataset(data), *args, **kwargs)
    return wrap_samples


class Classifier(Learner):
    """Abstract classifier class to be inherited by all classifiers
    """

    # Kept separate from doc to don't pollute help(clf), especially if
    # we including help for the parent class
    _DEV__doc__ = """
    Required behavior:

    For every classifier is has to be possible to be instantiated without
    having to specify the training pattern.

    Repeated calls to the train() method with different training data have to
    result in a valid classifier, trained for the particular dataset.

    It must be possible to specify all classifier parameters as keyword
    arguments to the constructor.

    Recommended behavior:

    Derived classifiers should provide access to *estimates* -- i.e. that
    information that is finally used to determine the predicted class label.

    Michael: Maybe it works well if each classifier provides a 'estimates'
             state member. This variable is a list as long as and in same order
             as Dataset.uniquetargets (training data). Each item in the list
             corresponds to the likelyhood of a sample to belong to the
             respective class. However the semantics might differ between
             classifiers, e.g. kNN would probably store distances to class-
             neighbors, where PLR would store the raw function value of the
             logistic function. So in the case of kNN low is predictive and for
             PLR high is predictive. Don't know if there is the need to unify
             that.

             As the storage and/or computation of this information might be
             demanding its collection should be switchable and off be default.

    Nomenclature
     * predictions  : result of the last call to .predict()
     * estimates : might be different from predictions if a classifier's predict()
                   makes a decision based on some internal value such as
                   probability or a distance.
    """
    # Dict that contains the parameters of a classifier.
    # This shall provide an interface to plug generic parameter optimizer
    # on all classifiers (e.g. grid- or line-search optimizer)
    # A dictionary is used because Michael thinks that access by name is nicer.
    # Additionally Michael thinks ATM that additional information might be
    # necessary in some situations (e.g. reasonably predefined parameter range,
    # minimal iteration stepsize, ...), therefore the value to each key should
    # also be a dict or we should use mvpa2.base.param.Parameter'...

    training_stats = ConditionalAttribute(enabled=False,
        doc="Confusion matrix of learning performance")

    predictions = ConditionalAttribute(enabled=True,
        doc="Most recent set of predictions")

    estimates = ConditionalAttribute(enabled=True,
        doc="Internal classifier estimates the most recent " +
            "predictions are based on")

    predicting_time = ConditionalAttribute(enabled=True,
        doc="Time (in seconds) which took classifier to predict")

    __tags__ = []
    """Describes some specifics about the classifier -- is that it is
    doing regression for instance...."""

    # TODO: make it available only for actually retrainable classifiers
    retrainable = Parameter(False, constraints='bool',
        doc="""Either to enable retraining for 'retrainable' classifier.""",
        index=1002)


    def __init__(self, space=None, **kwargs):
        # by default we want classifiers to use the 'targets' sample attribute
        # for training/testing
        if space is None:
            space = 'targets'
        Learner.__init__(self, space=space, **kwargs)

        # XXX
        # the place to map literal to numerical labels (and back)
        # this needs to be in the base class, since some classifiers also
        # have this nasty 'regression' mode, and the code in this class
        # needs to deal with converting the regression output into discrete
        # labels
        # however, preferably the mapping should be kept in the respective
        # low-level implementations that need it
        self._attrmap = AttributeMap()

        self.__trainednfeatures = 0
        """Stores number of features for which classifier was trained.
        If 0 -- it wasn't trained at all"""

        self._set_retrainable(self.params.retrainable, force=True)

        # deprecate
        #self.__trainedidhash = None
        #"""Stores id of the dataset on which it was trained to signal
        #in trained() if it was trained already on the same dataset"""

    @property
    def __summary_class__(self):
        if 'regression' in self.__tags__:
            return RegressionStatistics
        else:
            return ConfusionMatrix

    @property
    def __is_regression__(self):
        return 'regression' in self.__tags__

    def __str__(self, *args, **kwargs):
        if __debug__ and 'CLF_' in debug.active:
            return "%s / %s" % (repr(self), super(Classifier, self).__str__())
        else:
            return _str(self, *args, **kwargs)


    def _pretrain(self, dataset):
        """Functionality prior to training
        """
        # So we reset all conditional attributes and may be free up some memory
        # explicitly
        params = self.params
        if not params.retrainable:
            self.untrain()
        else:
            # just reset the ca, do not untrain
            self.ca.reset()
            if not self.__changedData_isset:
                self.__reset_changed_data()
                _changedData = self._changedData
                __idhashes = self.__idhashes
                __invalidatedChangedData = self.__invalidatedChangedData

                # if we don't know what was changed we need to figure
                # them out
                if __debug__:
                    debug('CLF_', "IDHashes are %s", (__idhashes,))

                # Look at the data if any was changed
                for key, data_ in (('traindata', dataset.samples),
                                   ('targets', dataset.sa[self.get_space()].value)):
                    _changedData[key] = self.__was_data_changed(key, data_)
                    # if those idhashes were invalidated by retraining
                    # we need to adjust _changedData accordingly
                    if __invalidatedChangedData.get(key, False):
                        if __debug__ and not _changedData[key]:
                            debug('CLF_', 'Found that idhash for %s was '
                                  'invalidated by retraining', (key,))
                        _changedData[key] = True

                # Look at the parameters
                for col in self._paramscols:
                    changedParams = self._collections[col].which_set()
                    if len(changedParams):
                        _changedData[col] = changedParams

                self.__invalidatedChangedData = {} # reset it on training

                if __debug__:
                    debug('CLF_', "Obtained _changedData is %s",
                          (self._changedData,))


    def _posttrain(self, dataset):
        """Functionality post training

        For instance -- computing confusion matrix.

        Parameters
        ----------
        dataset : Dataset
          Data which was used for training
        """
        super(Classifier, self)._posttrain(dataset)

        ca = self.ca

        # needs to be assigned first since below we use predict
        self.__trainednfeatures = dataset.nfeatures

        if __debug__ and 'CHECK_TRAINED' in debug.active:
            self.__trainedidhash = dataset.idhash

        if ca.is_enabled('training_stats') and \
               not ca.is_set('training_stats'):
            # we should not store predictions for training data,
            # it is confusing imho (yoh)
            ca.change_temporarily(
                disable_ca=["predictions"])
            if self.params.retrainable:
                # we would need to recheck if data is the same,
                # XXX think if there is a way to make this all
                # efficient. For now, probably, retrainable
                # classifiers have no chance but not to use
                # training_stats... sad
                self.__changedData_isset = False
            predictions = self.predict(dataset)
            ca.reset_changed_temporarily()
            targets = dataset.sa[self.get_space()].value
            if is_datasetlike(predictions) and (self.get_space() in predictions.fa):
                # e.g. in case of pair-wise uncombined results - provide
                # stats per each of the targets pairs
                prediction_targets = predictions.fa[self.get_space()].value
                ca.training_stats = dict(
                    (t, self.__summary_class__(
                        targets=targets, predictions=predictions.samples[:, i]))
                    for i, t in enumerate(prediction_targets))
            else:
                ca.training_stats = self.__summary_class__(
                    targets=targets, predictions=predictions)


    def summary(self):
        """Providing summary over the classifier"""

        s = "Classifier %s" % self
        ca = self.ca
        ca_enabled = ca.enabled

        if self.trained:
            s += "\n trained"
            if ca.is_set('training_time'):
                s += ' in %.3g sec' % ca.training_time
            s += ' on data with'
            if ca.is_set('trained_targets'):
                s += ' targets:%s' % list(ca.trained_targets)

            nsamples, nchunks = None, None
            if ca.is_set('trained_nsamples'):
                nsamples = ca.trained_nsamples
            if ca.is_set('trained_dataset'):
                td = ca.trained_dataset
                nsamples, nchunks = td.nsamples, len(td.sa['chunks'].unique)
            if nsamples is not None:
                s += ' #samples:%d' % nsamples
            if nchunks is not None:
                s += ' #chunks:%d' % nchunks

            s += " #features:%d" % self.__trainednfeatures
            if ca.is_set('training_stats'):
                s += ", training error:%.3g" % ca.training_stats.error
        else:
            s += "\n not yet trained"

        if len(ca_enabled):
            s += "\n enabled ca:%s" % ', '.join([str(ca[x])
                                                     for x in ca_enabled])
        return s


    def clone(self):
        """Create full copy of the classifier.

        It might require classifier to be untrained first due to
        present SWIG bindings.

        TODO: think about proper re-implementation, without enrollment of deepcopy
        """
        if __debug__:
            debug("CLF", "Cloning %s%s", (self, _strid(self)))
        try:
            return deepcopy(self)
        except:
            self.untrain()
            return deepcopy(self)


    def _train(self, dataset):
        """Function to be actually overridden in derived classes
        """
        raise NotImplementedError


    def _prepredict(self, dataset):
        """Functionality prior prediction
        """
        if not ('notrain2predict' in self.__tags__):
            # check if classifier was trained if that is needed
            if not self.trained:
                raise FailedToPredictError(
                      "Classifier %s wasn't yet trained, therefore can't "
                      "predict" % self)
            nfeatures = dataset.nfeatures #data.shape[1]
            # check if number of features is the same as in the data
            # it was trained on
            if nfeatures != self.__trainednfeatures:
                raise ValueError, \
                      "Classifier %s was trained on data with %d features, " % \
                      (self, self.__trainednfeatures) + \
                      "thus can't predict for %d features" % nfeatures


        if self.params.retrainable:
            if not self.__changedData_isset:
                self.__reset_changed_data()
                _changedData = self._changedData
                data = np.asanyarray(dataset.samples)
                _changedData['testdata'] = \
                                        self.__was_data_changed('testdata', data)
                if __debug__:
                    debug('CLF_', "prepredict: Obtained _changedData is %s",
                          (_changedData,))


    def _postpredict(self, dataset, result):
        """Functionality after prediction is computed
        """
        self.ca.predictions = result
        if self.params.retrainable:
            self.__changedData_isset = False

    def _predict(self, dataset):
        """Actual prediction
        """
        raise NotImplementedError

    @accepts_samples_as_dataset
    def predict(self, dataset):
        """Predict classifier on data

        Shouldn't be overridden in subclasses unless explicitly needed
        to do so. Also subclasses trying to call super class's predict
        should call _predict if within _predict instead of predict()
        since otherwise it would loop
        """
        ## ??? yoh: changed to asany from as without exhaustive check
        data = np.asanyarray(dataset.samples)
        if __debug__:
            # Verify that we have no NaN/Inf's which we do not "support" ATM
            if not np.all(np.isfinite(data)):
                raise ValueError(
                    "Some input data for predict is not finite (NaN or Inf)")
            debug("CLF", "Predicting classifier %s on ds %s",
                  (self, dataset))

        # remember the time when started computing predictions
        t0 = time.time()

        ca = self.ca
        # to assure that those are reset (could be set due to testing
        # post-training)
        ca.reset(['estimates', 'predictions'])

        self._prepredict(dataset)

        if self.__trainednfeatures > 0 \
               or 'notrain2predict' in self.__tags__:
            result = self._predict(dataset)
        else:
            warning("Trying to predict using classifier trained on no features")
            if __debug__:
                debug("CLF",
                      "No features were present for training, prediction is " \
                      "bogus")
            result = [None]*data.shape[0]

        ca.predicting_time = time.time() - t0

        # with labels mapping in-place, we also need to go back to the
        # literal labels
        if self._attrmap:
            try:
                result = self._attrmap.to_literal(result)
            except KeyError, e:
                raise FailedToPredictError, \
                      "Failed to convert predictions from numeric into " \
                      "literals: %s" % e

        self._postpredict(dataset, result)
        return result


    def _call(self, ds):
        # get the predictions
        # call with full dataset, since we might need it further down in
        # the stream, e.g. for caching...
        pred = self.predict(ds)
        tattr = self.get_space()
        # return the predictions and the targets in a dataset
        if isinstance(pred, Dataset):
            # it is already a dataset, e.g. as if we did not
            # use any combiner for MulticlassClassifier
            # to look at each pair
            pred.sa[tattr] = ds.sa[tattr]
            return pred
        else:
            return Dataset(pred, sa={tattr: ds.sa[tattr]})


    # XXX deprecate ???
    ##REF: Name was automagically refactored
    def is_trained(self, dataset=None):
        """Either classifier was already trained.

        MUST BE USED WITH CARE IF EVER"""
        if dataset is None:
            # simply return if it was trained on anything
            return not self.__trainednfeatures == 0
        else:
            res = (self.__trainednfeatures == dataset.nfeatures)
            if __debug__ and 'CHECK_TRAINED' in debug.active:
                res2 = (self.__trainedidhash == dataset.idhash)
                if res2 != res:
                    raise RuntimeError, \
                          "is_trained is weak and shouldn't be relied upon. " \
                          "Got result %b although comparing of idhash says %b" \
                          % (res, res2)
            return res


    @property
    def trained(self):
        """Either classifier was already trained"""
        return self.is_trained()

    def _untrain(self):
        """Reset trained state"""
        # any previous apping is obsolete now
        self._attrmap.clear()

        self.__trainednfeatures = 0
        # probably not needed... retrainable shouldn't be fully untrained
        # or should be???
        #if self.params.retrainable:
        #    # ??? don't duplicate the code ;-)
        #    self.__idhashes = {'traindata': None, 'targets': None,
        #                       'testdata': None, 'testtraindata': None}

        # no need to do this, as the Leaner class is doing it anyway
        #super(Classifier, self).reset()


    ##REF: Name was automagically refactored
    def get_sensitivity_analyzer(self, **kwargs):
        """Factory method to return an appropriate sensitivity analyzer for
        the respective classifier."""
        raise NotImplementedError


    #
    # Methods which are needed for retrainable classifiers
    #
    ##REF: Name was automagically refactored
    def _set_retrainable(self, value, force=False):
        """Assign value of retrainable parameter

        If retrainable flag is to be changed, classifier has to be
        untrained.  Also internal attributes such as _changedData,
        __changedData_isset, and __idhashes should be initialized if
        it becomes retrainable
        """
        pretrainable = self.params['retrainable']
        if (force or value != pretrainable.value) \
               and 'retrainable' in self.__tags__:
            if __debug__:
                debug("CLF_", "Setting retrainable to %s" % value)
            if 'meta' in self.__tags__:
                warning("Retrainability is not yet crafted/tested for "
                        "meta classifiers. Unpredictable behavior might occur")
            # assure that we don't drag anything behind
            if self.trained:
                self.untrain()
            ca = self.ca
            if not value and ca.has_key('retrained'):
                ca.pop('retrained')
                ca.pop('repredicted')
            if value:
                if not 'retrainable' in self.__tags__:
                    warning("Setting of flag retrainable for %s has no effect"
                            " since classifier has no such capability. It would"
                            " just lead to resources consumption and slowdown"
                            % self)
                ca['retrained'] = ConditionalAttribute(enabled=True,
                        doc="Either retrainable classifier was retrained")
                ca['repredicted'] = ConditionalAttribute(enabled=True,
                        doc="Either retrainable classifier was repredicted")

            pretrainable.value = value

            # if retrainable we need to keep track of things
            if value:
                self.__idhashes = {'traindata': None, 'targets': None,
                                   'testdata': None} #, 'testtraindata': None}
                if __debug__ and 'CHECK_RETRAIN' in debug.active:
                    # ??? it is not clear though if idhash is faster than
                    # simple comparison of (dataset != __traineddataset).any(),
                    # but if we like to get rid of __traineddataset then we
                    # should use idhash anyways
                    self.__trained = self.__idhashes.copy() # just same Nones
                self.__reset_changed_data()
                self.__invalidatedChangedData = {}
            elif 'retrainable' in self.__tags__:
                #self.__reset_changed_data()
                self.__changedData_isset = False
                self._changedData = None
                self.__idhashes = None
                if __debug__ and 'CHECK_RETRAIN' in debug.active:
                    self.__trained = None

    ##REF: Name was automagically refactored
    def __reset_changed_data(self):
        """For retrainable classifier we keep track of what was changed
        This function resets that dictionary
        """
        if __debug__:
            debug('CLF_',
                  'Retrainable: resetting flags on either data was changed')
        keys = self.__idhashes.keys() + self._paramscols
        # we might like to just reinit estimates to False???
        #_changedData = self._changedData
        #if isinstance(_changedData, dict):
        #    for key in _changedData.keys():
        #        _changedData[key] = False
        self._changedData = dict(zip(keys, [False]*len(keys)))
        self.__changedData_isset = False


    ##REF: Name was automagically refactored
    def __was_data_changed(self, key, entry, update=True):
        """Check if given entry was changed from what known prior.

        If so -- store only the ones needed for retrainable beastie
        """
        idhash_ = idhash(entry)
        __idhashes = self.__idhashes

        changed = __idhashes[key] != idhash_
        if __debug__ and 'CHECK_RETRAIN' in debug.active:
            __trained = self.__trained
            changed2 = entry != __trained[key]
            if isinstance(changed2, np.ndarray):
                changed2 = changed2.any()
            if changed != changed2 and not changed:
                raise RuntimeError, \
                  'idhash found to be weak for %s. Though hashid %s!=%s %s, '\
                  'estimates %s!=%s %s' % \
                  (key, idhash_, __idhashes[key], changed,
                   entry, __trained[key], changed2)
            if update:
                __trained[key] = entry

        if __debug__ and changed:
            debug('CLF_', "Changed %s from %s to %s.%s",
                  (key, __idhashes[key], idhash_,
                   ('','updated')[int(update)]))
        if update:
            __idhashes[key] = idhash_

        return changed


    # def __updateHashIds(self, key, data):
    #     """Is twofold operation: updates hashid if was said that it changed.
    #
    #     or if it wasn't said that data changed, but CHECK_RETRAIN and it found
    #     to be changed -- raise Exception
    #     """
    #
    #     check_retrain = __debug__ and 'CHECK_RETRAIN' in debug.active
    #     chd = self._changedData
    #
    #     # we need to updated idhashes
    #     if chd[key] or check_retrain:
    #         keychanged = self.__was_data_changed(key, data)
    #     if check_retrain and keychanged and not chd[key]:
    #         raise RuntimeError, \
    #               "Data %s found changed although wasn't " \
    #               "labeled as such" % key


    #
    # Additional API which is specific only for retrainable classifiers.
    # For now it would just puke if asked from not retrainable one.
    #
    # Might come useful and efficient for statistics testing, so if just
    # labels of dataset changed, then
    #  self.retrain(dataset, labels=True)
    # would cause efficient retraining (no kernels recomputed etc)
    # and subsequent self.repredict(data) should be also quite fase ;-)

    def retrain(self, dataset, **kwargs):
        """Helper to avoid check if data was changed actually changed

        Useful if just some aspects of classifier were changed since
        its previous training. For instance if dataset wasn't changed
        but only classifier parameters, then kernel matrix does not
        have to be computed.

        Words of caution: classifier must be previously trained,
        results always should first be compared to the results on not
        'retrainable' classifier (without calling retrain). Some
        additional checks are enabled if debug id 'CHECK_RETRAIN' is
        enabled, to guard against obvious mistakes.

        Parameters
        ----------
        kwargs
          that is what _changedData gets updated with. So, smth like
          `(params=['C'], targets=True)` if parameter C and targets
          got changed
        """
        # Note that it also demolishes anything for repredicting,
        # which should be ok in most of the cases
        if __debug__:
            if not self.params.retrainable:
                raise RuntimeError, \
                      "Do not use re(train,predict) on non-retrainable %s" % \
                      self

            if kwargs.has_key('params') or kwargs.has_key('kernel_params'):
                raise ValueError, \
                      "Retraining for changed params not working yet"

        self.__reset_changed_data()

        # local bindings
        chd = self._changedData
        ichd = self.__invalidatedChangedData

        chd.update(kwargs)
        # mark for future 'train()' items which are explicitely
        # mentioned as changed
        for key, value in kwargs.iteritems():
            if value:
                ichd[key] = True
        self.__changedData_isset = True

        # To check if we are not fooled
        if __debug__ and 'CHECK_RETRAIN' in debug.active:
            for key, data_ in (('traindata', dataset.samples),
                               ('targets', dataset.sa[self.get_space()].value)):
                # so it wasn't told to be invalid
                if not chd[key] and not ichd.get(key, False):
                    if self.__was_data_changed(key, data_, update=False):
                        raise RuntimeError, \
                              "Data %s found changed although wasn't " \
                              "labeled as such" % key

        # TODO: parameters of classifiers... for now there is explicit
        # 'forbidance' above

        # Below check should be superseeded by check above, thus never occur.
        # remove later on ???
        if __debug__ and 'CHECK_RETRAIN' in debug.active and self.trained \
               and not self._changedData['traindata'] \
               and self.__trained['traindata'].shape != dataset.samples.shape:
            raise ValueError, "In retrain got dataset with %s size, " \
                  "whenever previousely was trained on %s size" \
                  % (dataset.samples.shape, self.__trained['traindata'].shape)
        self.train(dataset)


    @accepts_samples_as_dataset
    def repredict(self, dataset, **kwargs):
        """Helper to avoid check if data was changed actually changed

        Useful if classifier was (re)trained but with the same data
        (so just parameters were changed), so that it could be
        repredicted easily (on the same data as before) without
        recomputing for instance train/test kernel matrix. Should be
        used with caution and always compared to the results on not
        'retrainable' classifier. Some additional checks are enabled
        if debug id 'CHECK_RETRAIN' is enabled, to guard against
        obvious mistakes.

        Parameters
        ----------
        dataset
          dataset which is conventionally given to predict
        kwargs
          that is what _changedData gets updated with. So, smth like
          `(params=['C'], targets=True)` if parameter C and targets
          got changed
        """
        if len(kwargs)>0:
            raise RuntimeError, \
                  "repredict for now should be used without params since " \
                  "it makes little sense to repredict if anything got changed"
        if __debug__ and not self.params.retrainable:
            raise RuntimeError, \
                  "Do not use retrain/repredict on non-retrainable classifiers"

        self.__reset_changed_data()
        chd = self._changedData
        chd.update(**kwargs)
        self.__changedData_isset = True


        # check if we are attempted to perform on the same data
        if __debug__ and 'CHECK_RETRAIN' in debug.active:
            for key, data_ in (('testdata', dataset.samples),):
                # so it wasn't told to be invalid
                #if not chd[key]:# and not ichd.get(key, False):
                if self.__was_data_changed(key, data_, update=False):
                    raise RuntimeError, \
                          "Data %s found changed although wasn't " \
                          "labeled as such" % key

        # Should be superseded by above
        # remove in future???
        if __debug__ and 'CHECK_RETRAIN' in debug.active \
               and not self._changedData['testdata'] \
               and self.__trained['testdata'].shape != dataset.samples.shape:
            raise ValueError, "In repredict got dataset with %s size, " \
                  "whenever previously was trained on %s size" \
                  % (dataset.samples.shape, self.__trained['testdata'].shape)

        return self.predict(dataset)


    # TODO: callback into retrainable parameter
    #retrainable = property(fget=_getRetrainable, fset=_set_retrainable,
    #                  doc="Specifies either classifier should be retrainable")

########NEW FILE########
__FILENAME__ = blr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com>
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Bayesian Linear Regression (BLR)."""

__docformat__ = 'restructuredtext'


import numpy as np

from mvpa2.base.state import ConditionalAttribute
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples

if __debug__:
    from mvpa2.misc import debug


class BLR(Classifier):
    """Bayesian Linear Regression (BLR).

    """

    predicted_variances = ConditionalAttribute(enabled=False,
        doc="Variance per each predicted value")

    log_marginal_likelihood = ConditionalAttribute(enabled=False,
        doc="Log Marginal Likelihood")


    __tags__ = [ 'blr', 'regression', 'linear' ]

    def __init__(self, sigma_p = None, sigma_noise=1.0, **kwargs):
        """Initialize a BLR regression analysis.

        Parameters
        ----------
        sigma_noise : float
          the standard deviation of the gaussian noise.
          (Defaults to 0.1)

        """
        # init base class first
        Classifier.__init__(self, **kwargs)

        # pylint happiness
        self.w = None

        # It does not make sense to calculate a confusion matrix for a
        # BLR:
        self.ca.enable('training_stats', False)

        # set the prior on w: N(0,sigma_p) , specifying the covariance
        # sigma_p on w:
        self.sigma_p = sigma_p

        # set noise level:
        self.sigma_noise = sigma_noise

        self.ca.predicted_variances = None
        self.ca.log_marginal_likelihood = None
        # Yarik: what was those about??? just for future in
        #        compute_log_marginal_likelihood ?
        # self.targets = None
        pass

    def __repr__(self):
        """String summary of the object
        """
        return """BLR(w=%s, sigma_p=%s, sigma_noise=%f, enable_ca=%s)""" % \
               (self.w, self.sigma_p, self.sigma_noise, str(self.ca.enabled))


    def compute_log_marginal_likelihood(self):
        """
        Compute log marginal likelihood using self.train_fv and self.targets.
        """
        # log_marginal_likelihood = None
        # return log_marginal_likelihood
        raise NotImplementedError


    def _train(self, data):
        """Train regression using `data` (`Dataset`).
        """
        # BLR relies on numerical labels
        train_labels = self._attrmap.to_numeric(data.sa[self.get_space()].value)
        # provide a basic (i.e. identity matrix) and correct prior
        # sigma_p, if not provided before or not compliant to 'data':
        if self.sigma_p == None: # case: not provided
            self.sigma_p = np.eye(data.samples.shape[1]+1)
        elif self.sigma_p.shape[1] != (data.samples.shape[1]+1): # case: wrong dimensions
            self.sigma_p = np.eye(data.samples.shape[1]+1)
        else:
            # ...then everything is OK :)
            pass

        # add one fake column of '1.0' to model the intercept:
        self.samples_train = np.hstack([data.samples,np.ones((data.samples.shape[0],1))])
        if type(self.sigma_p)!=type(self.samples_train): # if sigma_p is a number...
            self.sigma_p = np.eye(self.samples_train.shape[1])*self.sigma_p # convert in matrix
            pass

        self.A_inv = np.linalg.inv(1.0/(self.sigma_noise**2) *
                                  np.dot(self.samples_train.T,
                                        self.samples_train) +
                                  np.linalg.inv(self.sigma_p))
        self.w = 1.0/(self.sigma_noise**2) * np.dot(self.A_inv,
                                                   np.dot(self.samples_train.T,
                                                         train_labels))
        pass


    @accepts_dataset_as_samples
    def _predict(self, data):
        """
        Predict the output for the provided data.
        """

        data = np.hstack([data,np.ones((data.shape[0],1),dtype=data.dtype)])
        predictions = np.dot(data,self.w)

        if self.ca.is_enabled('predicted_variances'):
            # do computation only if conditional attribute was enabled
            self.ca.predicted_variances = np.dot(data, np.dot(self.A_inv, data.T)).diagonal()[:,np.newaxis]
        self.ca.estimates = predictions
        return predictions


    def set_hyperparameters(self,*args):
        """
        Set hyperparameters' values.

        Note that this is a list so the order of the values is
        important.
        """
        args=args[0]
        self.sigma_noise = args[0]
        if len(args)>1:
            self.sigma_p = np.array(args[1:]) # XXX check if this is ok
            pass
        return

    pass


if __name__ == "__main__":
    import pylab
    pylab.close("all")
    pylab.ion()

    from mvpa2.misc.data_generators import linear_awgn

    train_size = 10
    test_size = 100
    F = 1 # dimensions of the dataset

    # np.random.seed(1)

    slope = np.random.rand(F)
    intercept = np.random.rand(1)
    print "True slope:",slope
    print "True intercept:",intercept

    dataset_train = linear_awgn(train_size, intercept=intercept, slope=slope)

    dataset_test = linear_awgn(test_size, intercept=intercept, slope=slope, flat=True)

    regression = True
    logml = False

    b = BLR(sigma_p=np.eye(F+1), sigma_noise=0.1)
    b.ca.enable("predicted_variances")
    b.train(dataset_train)
    predictions = b.predict(dataset_test.samples)
    print "Predicted slope and intercept:",b.w

    if F==1:
        pylab.plot(dataset_train.samples,
                   dataset_train.sa[b.get_space()].value,
                   "ro", label="train")

        pylab.plot(dataset_test.samples, predictions, "b-", label="prediction")
        pylab.plot(dataset_test.samples,
                   predictions+np.sqrt(b.ca.predicted_variances),
                   "b--", label="pred(+/-)std")
        pylab.plot(dataset_test.samples,
                   predictions-np.sqrt(b.ca.predicted_variances),
                   "b--", label=None)
        pylab.legend()
        pylab.xlabel("samples")
        pylab.ylabel("labels")
        pylab.title("Bayesian Linear Regression on dataset 'linear_AWGN'")
        pass


########NEW FILE########
__FILENAME__ = distance
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Distance functions to be used in kernels and elsewhere
"""

__docformat__ = 'restructuredtext'

# TODO: Make all distance functions accept 2D matrices samples x features
#       and compute the distance matrix between all samples. They would
#       need to be capable of dealing with unequal number of rows!
#       If we would have that, we could make use of them in kNN.

import numpy as np
from mvpa2.base import externals

if __debug__:
    from mvpa2.base import debug, warning


def cartesian_distance(a, b):
    """Return Cartesian distance between a and b
    """
    return np.linalg.norm(a-b)


def absmin_distance(a, b):
    """Returns dinstance max(\|a-b\|)
    XXX There must be better name!
    XXX Actually, why is it absmin not absmax?

    Useful to select a whole cube of a given "radius"
    """
    return max(abs(a-b))


def manhatten_distance(a, b):
    """Return Manhatten distance between a and b
    """
    return sum(abs(a-b))


def mahalanobis_distance(x, y=None, w=None):
    """Calculate Mahalanobis distance of the pairs of points.

    Parameters
    ----------
    `x`
      first list of points. Rows are samples, columns are
      features.
    `y`
      second list of points (optional)
    `w` : np.ndarray
      optional inverse covariance matrix between the points. It is
      computed if not given

    Inverse covariance matrix can be calculated with the following

      w = np.linalg.solve(np.cov(x.T), np.identity(x.shape[1]))

    or

      w = np.linalg.inv(np.cov(x.T))
    """
    # see if pairwise between two matrices or just within a single matrix
    if y is None:
        # pairwise distances of single matrix
        # calculate the inverse correlation matrix if necessary
        if w is None:
            w = np.linalg.inv(np.cov(x.T))

        # get some shapes of the data
        mx, nx = x.shape
        #mw, nw = w.shape

        # allocate for the matrix to fill
        d = np.zeros((mx, mx), dtype=np.float32)
        for i in range(mx-1):
            # get the current row to compare
            xi = x[i, :]
            # replicate the row
            xi = xi[np.newaxis, :].repeat(mx-i-1, axis=0)
            # take the distance between all the matrices
            dc = x[i+1:mx, :] - xi
            # scale the distance by the correlation
            d[i+1:mx, i] = np.real(np.sum((np.inner(dc, w) * np.conj(dc)), 1))
            # fill the other direction of the matrix
            d[i, i+1:mx] = d[i+1:mx, i].T
    else:
        # is between two matrixes
        # calculate the inverse correlation matrix if necessary
        if w is None:
            # calculate over all points
            w = np.linalg.inv(np.cov(np.concatenate((x, y)).T))

        # get some shapes of the data
        mx, nx = x.shape
        my, ny = y.shape

        # allocate for the matrix to fill
        d = np.zeros((mx, my), dtype=np.float32)

        # loop over shorter of two dimensions
        if mx <= my:
            # loop over the x patterns
            for i in range(mx):
                # get the current row to compare
                xi = x[i, :]
                # replicate the row
                xi = xi[np.newaxis, :].repeat(my, axis=0)
                # take the distance between all the matrices
                dc = xi - y
                # scale the distance by the correlation
                d[i, :] = np.real(np.sum((np.inner(dc, w) * np.conj(dc)), 1))
        else:
            # loop over the y patterns
            for j in range(my):
                # get the current row to compare
                yj = y[j, :]
                # replicate the row
                yj = yj[np.newaxis, :].repeat(mx, axis=0)
                # take the distance between all the matrices
                dc = x - yj
                # scale the distance by the correlation
                d[:, j] = np.real(np.sum((np.inner(dc, w) * np.conj(dc)), 1))

    # return the dist
    return np.sqrt(d)


def squared_euclidean_distance(data1, data2=None, weight=None):
    """Compute weighted euclidean distance matrix between two datasets.


    Parameters
    ----------
    data1 : np.ndarray
        first dataset
    data2 : np.ndarray
        second dataset. If None, compute the euclidean distance between
        the first dataset versus itself.
        (Defaults to None)
    weight : np.ndarray
        vector of weights, each one associated to each dimension of the
        dataset (Defaults to None)
    """
    if __debug__:
        # check if both datasets are floating point
        if not np.issubdtype(data1.dtype, 'f') \
           or (data2 is not None and not np.issubdtype(data2.dtype, 'f')):
            warning('Computing euclidean distance on integer data ' \
                    'is not supported.')

    # removed for efficiency (see below)
    #if weight is None:
    #    weight = np.ones(data1.shape[1], 'd') # unitary weight

    # In the following you can find faster implementations of this
    # basic code:
    #
    # squared_euclidean_distance_matrix = \
    #           np.zeros((data1.shape[0], data2.shape[0]), 'd')
    # for i in range(size1):
    #     for j in range(size2):
    #         squared_euclidean_distance_matrix[i, j] = \
    #           ((data1[i, :]-data2[j, :])**2*weight).sum()
    #         pass
    #     pass

    # Fast computation of distance matrix in Python+NumPy,
    # adapted from Bill Baxter's post on [numpy-discussion].
    # Basically: (x-y)**2*w = x*w*x - 2*x*w*y + y*y*w

    # based on value of weight and data2 we might save on computation
    # and resources
    if weight is None:
        data1w = data1
        if data2 is None:
            data2, data2w = data1, data1w
        else:
            data2w = data2
    else:
        data1w = data1 * weight
        if data2 is None:
            data2, data2w = data1, data1w
        else:
            data2w = data2 * weight

    squared_euclidean_distance_matrix = \
        (data1w * data1).sum(1)[:, None] \
        -2 * np.dot(data1w, data2.T) \
        + (data2 * data2w).sum(1)

    # correction to some possible numerical instabilities:
    less0 = squared_euclidean_distance_matrix < 0
    if __debug__ and 'CHECK_STABILITY' in debug.active:
        less0num = np.sum(less0)
        if less0num > 0:
            norm0 = np.linalg.norm(squared_euclidean_distance_matrix[less0])
            totalnorm = np.linalg.norm(squared_euclidean_distance_matrix)
            if totalnorm != 0 and norm0 / totalnorm > 1e-8:
                warning("Found %d elements out of %d unstable (<0) in " \
                        "computation of squared_euclidean_distance_matrix. " \
                        "Their norm is %s when total norm is %s" % \
                        (less0num, np.sum(less0.shape), norm0, totalnorm))
    squared_euclidean_distance_matrix[less0] = 0
    return squared_euclidean_distance_matrix


def one_minus_correlation(X, Y):
    """Return one minus the correlation matrix between the rows of two matrices.

    This functions computes a matrix of correlations between all pairs of
    rows of two matrices. Unlike NumPy's corrcoef() this function will only
    considers pairs across matrices and not within, e.g. both elements of
    a pair never have the same source matrix as origin.

    Both arrays need to have the same number of columns.

    Parameters
    ----------
    X: 2D-array
    Y: 2D-array

    Examples
    --------

    >>> import numpy as np
    >>> from mvpa2.clfs.distance import one_minus_correlation
    >>> X = np.random.rand(20,80)
    >>> Y = np.random.rand(5,80)
    >>> C = one_minus_correlation(X, Y)
    >>> print C.shape
    (20, 5)

    """
    # check if matrices have same number of columns
    if __debug__:
        if not X.shape[1] == Y.shape[1]:
            raise ValueError, 'correlation() requires to matrices with the ' \
                              'same #columns (Got: %s and %s)' \
                              % (X.shape, Y.shape)

    # zscore each sample/row
    Zx = X - np.c_[X.mean(axis=1)]
    Zx /= np.c_[X.std(axis=1)]
    Zy = Y - np.c_[Y.mean(axis=1)]
    Zy /= np.c_[Y.std(axis=1)]

    C = ((np.matrix(Zx) * np.matrix(Zy).T) / Zx.shape[1]).A

    # let it behave like a distance, i.e. smaller is closer
    C -= 1.0

    return np.abs(C)


def pnorm_w_python(data1, data2=None, weight=None, p=2,
                   heuristic='auto', use_sq_euclidean=True):
    """Weighted p-norm between two datasets (pure Python implementation)

    ||x - x'||_w = (\sum_{i=1...N} (w_i*|x_i - x'_i|)**p)**(1/p)

    Parameters
    ----------
    data1 : np.ndarray
      First dataset
    data2 : np.ndarray or None
      Optional second dataset
    weight : np.ndarray or None
      Optional weights per 2nd dimension (features)
    p
      Power
    heuristic : str
      Which heuristic to use:
       * 'samples' -- python sweep over 0th dim
       * 'features' -- python sweep over 1st dim
       * 'auto' decides automatically. If # of features (shape[1]) is much
         larger than # of samples (shape[0]) -- use 'samples', and use
         'features' otherwise
    use_sq_euclidean : bool
      Either to use squared_euclidean_distance_matrix for computation if p==2
    """
    if weight == None:
        weight = np.ones(data1.shape[1], 'd')
        pass

    if p == 2 and use_sq_euclidean:
        return np.sqrt(squared_euclidean_distance(data1=data1, data2=data2,
                                                 weight=weight**2))

    if data2 == None:
        data2 = data1
        pass

    S1, F1 = data1.shape[:2]
    S2, F2 = data2.shape[:2]
    # sanity check
    if not (F1==F2==weight.size):
        raise ValueError, \
              "Datasets should have same #columns == #weights. Got " \
              "%d %d %d" % (F1, F2, weight.size)
    d = np.zeros((S1, S2), 'd')

    # Adjust local functions for specific p values
    # pf - power function
    # af - after function
    if p == 1:
        pf = lambda x:x
        af = lambda x:x
    else:
        pf = lambda x:x ** p
        af = lambda x:x ** (1.0/p)

    # heuristic 'auto' might need to be adjusted
    if heuristic == 'auto':
        heuristic = {False: 'samples',
                     True: 'features'}[(F1/S1) < 500]

    if heuristic == 'features':
        #  Efficient implementation if the feature size is little.
        for NF in range(F1):
            d += pf(np.abs(np.subtract.outer(data1[:, NF],
                                           data2[:, NF]))*weight[NF])
            pass
    elif heuristic == 'samples':
        #  Efficient implementation if the feature size is much larger
        #  than number of samples
        for NS in xrange(S1):
            dfw = pf(np.abs(data1[NS] - data2) * weight)
            d[NS] = np.sum(dfw, axis=1)
            pass
    else:
        raise ValueError, "Unknown heuristic '%s'. Need one of " \
              "'auto', 'samples', 'features'" % heuristic
    return af(d)


if externals.exists('weave'):
    from scipy import weave
    from scipy.weave import converters

    def pnorm_w(data1, data2=None, weight=None, p=2):
        """Weighted p-norm between two datasets (scipy.weave implementation)

        ||x - x'||_w = (\sum_{i=1...N} (w_i*|x_i - x'_i|)**p)**(1/p)

        Parameters
        ----------
        data1 : np.ndarray
          First dataset
        data2 : np.ndarray or None
          Optional second dataset
        weight : np.ndarray or None
          Optional weights per 2nd dimension (features)
        p
          Power
        """

        if weight == None:
            weight = np.ones(data1.shape[1], 'd')
            pass
        S1, F1 = data1.shape[:2]
        code = ""
        if data2 == None or id(data1)==id(data2):
            if not (F1==weight.size):
                raise ValueError, \
                      "Dataset should have same #columns == #weights. Got " \
                      "%d %d" % (F1, weight.size)
            F = F1
            d = np.zeros((S1, S1), 'd')
            try:
                code_peritem = \
                    {1.0 : "tmp = tmp+weight(t)*fabs(data1(i,t)-data1(j,t))",
                     2.0 : "tmp2 = weight(t)*(data1(i,t)-data1(j,t));" \
                     " tmp = tmp + tmp2*tmp2"}[p]
            except KeyError:
                code_peritem = "tmp = tmp+pow(weight(t)*fabs(data1(i,t)-data1(j,t)),p)"

            code = """
            int i,j,t;
            double tmp, tmp2;
            for (i=0; i<S1-1; i++) {
                for (j=i+1; j<S1; j++) {
                    tmp = 0.0;
                    for(t=0; t<F; t++) {
                        %s;
                        }
                    d(i,j) = tmp;
                    }
                }
            return_val = 0;
            """ % code_peritem


            counter = weave.inline(code,
                               ['data1', 'S1', 'F', 'weight', 'd', 'p'],
                               type_converters=converters.blitz,
                               compiler = 'gcc')
            d = d + np.triu(d).T # copy upper part to lower part
            return d**(1.0/p)

        S2, F2 = data2.shape[:2]
        if not (F1==F2==weight.size):
            raise ValueError, \
                  "Datasets should have same #columns == #weights. Got " \
                  "%d %d %d" % (F1, F2, weight.size)
        F = F1
        d = np.zeros((S1, S2), 'd')
        try:
            code_peritem = \
                {1.0 : "tmp = tmp+weight(t)*fabs(data1(i,t)-data2(j,t))",
                 2.0 : "tmp2 = weight(t)*(data1(i,t)-data2(j,t));" \
                 " tmp = tmp + tmp2*tmp2"}[p]
        except KeyError:
            code_peritem = "tmp = tmp+pow(weight(t)*fabs(data1(i,t)-data2(j,t)),p)"
            pass

        code = """
        int i,j,t;
        double tmp, tmp2;
        for (i=0; i<S1; i++) {
            for (j=0; j<S2; j++) {
                tmp = 0.0;
                for(t=0; t<F; t++) {
                    %s;
                    }
                d(i,j) = tmp;
                }
            }
        return_val = 0;

        """ % code_peritem

        counter = weave.inline(code,
                               ['data1', 'data2', 'S1', 'S2',
                                'F', 'weight', 'd', 'p'],
                               type_converters=converters.blitz,
                               compiler = 'gcc')
        return d**(1.0/p)

else:
    # Bind pure python implementation
    pnorm_w = pnorm_w_python
    pass


### XXX EO: This is code to compute streamline-streamline distance.
### Currently used just for testing purpose for the PrototypeMapper.

def mean_min(streamline1, streamline2):
    """Basic building block to compute several distances between
    streamlines.
    """
    d_e_12 = np.sqrt(squared_euclidean_distance(streamline1, streamline2))
    return np.array([d_e_12.min(1).mean(), d_e_12.min(0).mean()])


def corouge(streamline1, streamline2):
    """Mean of the mean min distances. See Zhang et al., Identifying
    White-Matter Fiber Bundles in DTI Data Using an Automated
    Proximity-Based Fiber-Clustering Method, 2008.
    """
    return mean_min(streamline1, streamline2).mean()

########NEW FILE########
__FILENAME__ = dummies
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Collection of dummy (e.g. random) classifiers.  Primarily for testing.
"""

__docformat__ = 'restructuredtext'

import numpy as np
import numpy.random as npr

from mvpa2.base.param import Parameter
from mvpa2.base.types import accepts_dataset_as_samples, is_datasetlike
from mvpa2.clfs.base import Classifier

__all__ = ['Classifier', 'SameSignClassifier', 'RandomClassifier',
           'Less1Classifier']

#
# Few silly classifiers
#
class SameSignClassifier(Classifier):
    """Dummy classifier which reports +1 class if both features have
    the same sign, -1 otherwise"""

    __tags__ = ['notrain2predict']
    def __init__(self, **kwargs):
        Classifier.__init__(self, **kwargs)

    def _train(self, data):
        # we don't need that ;-)
        pass

    @accepts_dataset_as_samples
    def _predict(self, data):
        data = np.asanyarray(data)
        datalen = len(data)
        estimates = []
        for d in data:
            estimates.append(2*int( (d[0]>=0) == (d[1]>=0) )-1)
        self.ca.predictions = estimates
        self.ca.estimates = estimates            # just for the sake of having estimates
        return estimates


class RandomClassifier(Classifier):
    """Dummy classifier deciding on labels absolutely randomly
    """

    __tags__ = ['random', 'non-deterministic']

    same = Parameter(
        False, constraints='bool',
        doc="If a dataset arrives to predict, assign identical (but random) label "
            "to all samples having the same label in original, thus mimiquing the "
            "situation where testing samples are not independent.")

    def __init__(self, **kwargs):
        Classifier.__init__(self, **kwargs)
        self._ulabels = None

    def _train(self, data):
        self._ulabels = data.sa[self.get_space()].unique

    @accepts_dataset_as_samples
    def _predict(self, data):
        l = len(self._ulabels)
        # oh those lovely random estimates, for now just an estimate
        # per sample. Since we are random after all -- keep it random
        self.ca.estimates = np.random.normal(size=len(data))
        if is_datasetlike(data) and self.params.same:
            # decide on mapping between original labels
            labels_map = dict(
                (t, rt) for t, rt in zip(self._ulabels,
                                         self._ulabels[npr.randint(0, l, size=l)]))
            return [labels_map[t] for t in data.sa[self.get_space()].value]
        else:
            # random one per each
            return self._ulabels[npr.randint(0, l, size=len(data))]


class Less1Classifier(SameSignClassifier):
    """Dummy classifier which reports +1 class if abs value of max less than 1"""
    def _predict(self, data):
        datalen = len(data)
        estimates = []
        for d in data:
            estimates.append(2*int(max(d)<=1)-1)
        self.predictions = estimates
        return estimates

########NEW FILE########
__FILENAME__ = enet
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Elastic-Net (ENET) regression classifier."""

__docformat__ = 'restructuredtext'

# system imports
import numpy as np

import mvpa2.base.externals as externals

# do conditional to be able to build module reference
if externals.exists('elasticnet', raise_=True):
    import rpy2.robjects
    import rpy2.robjects.numpy2ri
    if hasattr(rpy2.robjects.numpy2ri,'activate'):
        rpy2.robjects.numpy2ri.activate()
    RRuntimeError = rpy2.robjects.rinterface.RRuntimeError
    r = rpy2.robjects.r
    r.library('elasticnet')
    from mvpa2.support.rpy2_addons import Rrx2


# local imports
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples, \
        FailedToPredictError
from mvpa2.base.learner import FailedToTrainError
from mvpa2.measures.base import Sensitivity

if __debug__:
    from mvpa2.base import debug

class ENET(Classifier):
    """Elastic-Net regression (ENET) `Classifier`.

    Elastic-Net is the model selection algorithm from:

    :ref:`Zou and Hastie (2005) <ZH05>` 'Regularization and Variable
    Selection via the Elastic Net' Journal of the Royal Statistical
    Society, Series B, 67, 301-320.

    Similar to SMLR, it performs a feature selection while performing
    classification, but instead of starting with all features, it
    starts with none and adds them in, which is similar to boosting.

    Unlike LARS it has both L1 and L2 regularization (instead of just
    L1).  This means that while it tries to sparsify the features it
    also tries to keep redundant features, which may be very very good
    for fMRI classification.

    In the true nature of the PyMVPA framework, this algorithm was
    actually implemented in R by Zou and Hastie and wrapped via RPy.
    To make use of ENET, you must have R and RPy installed as well as
    both the lars and elasticnet contributed package. You can install
    the R and RPy with the following command on Debian-based machines:

    sudo aptitude install python-rpy python-rpy-doc r-base-dev

    You can then install the lars and elasticnet package by running R
    as root and calling:

    install.packages()

    """

    __tags__ = [ 'enet', 'regression', 'linear', 'has_sensitivity',
                 'does_feature_selection', 'rpy2' ]

    def __init__(self, lm=1.0, trace=False, normalize=True,
                 intercept=True, max_steps=None, **kwargs):
        """
        Initialize ENET.

        See the help in R for further details on the following parameters:

        Parameters
        ----------
        lm : float
          Penalty parameter.  0 will perform LARS with no ridge regression.
          Default is 1.0.
        trace : boolean
          Whether to print progress in R as it works.
        normalize : boolean
          Whether to normalize the L2 Norm.
        intercept : boolean
          Whether to add a non-penalized intercept to the model.
        max_steps : None or int
          If not None, specify the total number of iterations to run. Each
          iteration adds a feature, but leaving it none will add until
          convergence.
        """
        # init base class first
        Classifier.__init__(self, **kwargs)

        # set up the params
        self.__lm = lm
        self.__normalize = normalize
        self.__intercept = intercept
        self.__trace = trace
        self.__max_steps = max_steps

        # pylint friendly initializations
        self.__weights = None
        """The beta weights for each feature."""
        self.__trained_model = None
        """The model object after training that will be used for
        predictions."""

        # It does not make sense to calculate a confusion matrix for a
        # regression
        self.ca.enable('training_stats', False)

    def __repr__(self):
        """String summary of the object
        """
        return """ENET(lm=%s, normalize=%s, intercept=%s, trace=%s, max_steps=%s, enable_ca=%s)""" % \
               (self.__lm,
                self.__normalize,
                self.__intercept,
                self.__trace,
                self.__max_steps,
                str(self.ca.enabled))


    def _train(self, data):
        """Train the classifier using `data` (`Dataset`).
        """
        targets = data.sa[self.get_space()].value[:, np.newaxis]
        enet_kwargs = {}
        if self.__max_steps is not None:
            enet_kwargs['max.steps'] = self.__max_steps

        try:
            self.__trained_model = trained_model = \
                r.enet(data.samples,
                       targets,
                       self.__lm,
                       normalize=self.__normalize,
                       intercept=self.__intercept,
                       trace=self.__trace,
                       **enet_kwargs)
        except RRuntimeError, e:
            raise FailedToTrainError, \
                  "Failed to predict on %s using %s. Exceptions was: %s" \
                  % (data, self, e)

        # find the step with the lowest Cp (risk)
        # it is often the last step if you set a max_steps
        # must first convert dictionary to array
#         Cp_vals = np.asarray([trained_model['Cp'][str(x)]
#                              for x in range(len(trained_model['Cp']))])
#         self.__lowest_Cp_step = Cp_vals.argmin()

        # set the weights to the last step
        beta_pure = np.asanyarray(Rrx2(trained_model, 'beta.pure'))
        self.__beta_pure_shape = beta_pure.shape
        self.__weights = np.zeros(data.nfeatures,
                                 dtype=beta_pure.dtype)
        ind = np.asanyarray(Rrx2(trained_model, 'allset'))-1
        self.__weights[ind] = beta_pure[-1,:]
#         # set the weights to the final state
#         self.__weights = trained_model['beta'][-1,:]


    @accepts_dataset_as_samples
    def _predict(self, data):
        """Predict the output for the provided data.
        """
        # predict with the final state (i.e., the last step)
        try:
            res = r.predict(self.__trained_model,
                            data,
                            mode='step',
                            type='fit',
                            s=rpy2.robjects.IntVector(self.__beta_pure_shape))
            fit = np.asanyarray(Rrx2(res, 'fit'))[:, -1]
        except RRuntimeError, e:
            raise FailedToPredictError, \
                  "Failed to predict on %s using %s. Exceptions was: %s" \
                  % (data, self, e)

        if len(fit.shape) == 0:
            # if we just got 1 sample with a scalar
            fit = fit.reshape( (1,) )
        self.ca.estimates = fit     # charge conditional attribute
        return fit


    ##REF: Name was automagically refactored
    def _get_feature_ids(self):
        """Return ids of the used features
        """
        return np.where(np.abs(self.__weights)>0)[0]



    ##REF: Name was automagically refactored
    def get_sensitivity_analyzer(self, **kwargs):
        """Returns a sensitivity analyzer for ENET."""
        return ENETWeights(self, **kwargs)

    weights = property(lambda self: self.__weights)



class ENETWeights(Sensitivity):
    """`SensitivityAnalyzer` that reports the weights ENET trained
    on a given `Dataset`.
    """

    _LEGAL_CLFS = [ ENET ]

    def _call(self, dataset=None):
        """Extract weights from ENET classifier.

        ENET always has weights available, so nothing has to be computed here.
        """
        clf = self.clf
        weights = clf.weights

        if __debug__:
            debug('ENET',
                  "Extracting weights for ENET - "+
                  "Result: min=%f max=%f" %\
                  (np.min(weights), np.max(weights)))

        return weights


########NEW FILE########
__FILENAME__ = gda
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Gaussian Discriminant Analyses: LDA and QDA

   Basic implementation at the moment: no data sphering, nor
   dimensionality reduction tricks are in place ATM
"""

"""
TODO:

 * too much in common with GNB -- LDA/QDA/GNB could reuse much of machinery
 * provide actual probabilities computation as in GNB
 * LDA/QDA -- make use of data sphering and may be operating in the
              subspace of centroids

Was based on GNB code
"""

__docformat__ = 'restructuredtext'

import numpy as np

from numpy import ones, zeros, sum, abs, isfinite, dot
from mvpa2.base import warning, externals
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.base.learner import DegenerateInputError
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import EnsureChoice
from mvpa2.base.state import ConditionalAttribute
#from mvpa2.measures.base import Sensitivity


if __debug__:
    from mvpa2.base import debug

__all__ = [ "LDA", "QDA" ]

class GDA(Classifier):
    """Gaussian Discriminant Analysis -- base for LDA and QDA

    """

    __tags__ = ['binary', 'multiclass']


    prior = Parameter('laplacian_smoothing',
             constraints=EnsureChoice('laplacian_smoothing', 'uniform', 'ratio'),
             doc="""How to compute prior distribution.""")

    allow_pinv = Parameter(True,
             constraints='bool',
             doc="""Allow pseudo-inverse in case of degenerate covariance(s).""")

    def __init__(self, **kwargs):
        """Initialize a GDA classifier.
        """

        # init base class first
        Classifier.__init__(self, **kwargs)

        # pylint friendly initializations
        self.means = None
        """Means of features per class"""
        self.cov = None
        """Co-variances per class, but "vars" is taken ;)"""
        self.ulabels = None
        """Labels classifier was trained on"""
        self.priors = None
        """Class probabilities"""
        self.nsamples_per_class = None
        """Number of samples per class - used by derived classes"""

        # Define internal state of classifier
        self._norm_weight = None

    def _get_priors(self, nlabels, nsamples, nsamples_per_class):
        """Return prior probabilities given data
        """
        prior = self.params.prior
        if prior == 'uniform':
            priors = np.ones((nlabels,))/nlabels
        elif prior == 'laplacian_smoothing':
            priors = (1+np.squeeze(nsamples_per_class)) \
                          / (float(nsamples) + nlabels)
        elif prior == 'ratio':
            priors = np.squeeze(nsamples_per_class) / float(nsamples)
        else:
            raise ValueError, \
                  "No idea on how to handle '%s' way to compute priors" \
                  % self.params.prior
        return priors


    def _train(self, dataset):
        """Train the classifier using `dataset` (`Dataset`).
        """
        params = self.params
        targets_sa_name = self.get_space()
        targets_sa = dataset.sa[targets_sa_name]

        # get the dataset information into easy vars
        X = dataset.samples
        labels = targets_sa.value
        self.ulabels = ulabels = targets_sa.unique
        nlabels = len(ulabels)
        label2index = dict((l, il) for il, l in enumerate(ulabels))

        # set the feature dimensions
        nsamples = len(X)
        nfeatures = dataset.nfeatures

        self.means = means = \
                     np.zeros((nlabels, nfeatures))
        # degenerate dimension are added for easy broadcasting later on
        # XXX might want to remove -- for now taken from GNB as is
        self.nsamples_per_class = nsamples_per_class \
                                  = np.zeros((nlabels, 1))
        self.cov = cov = \
                     np.zeros((nlabels, nfeatures, nfeatures))


        # Estimate cov
        # better loop than repmat! ;)
        for l, il in label2index.iteritems():
            Xl = X[labels == l]
            nsamples_per_class[il] = len(Xl)
            # TODO: degenerate case... no samples for known label for
            #       some reason?
            means[il] = np.mean(Xl, axis=0)
            # since we have means already lets do manually cov here
            Xldm = Xl - means[il]
            cov[il] = np.dot(Xldm.T, Xldm)
            # scaling will be done correspondingly in LDA or QDA

        # Store prior probabilities
        self.priors = self._get_priors(nlabels, nsamples, nsamples_per_class)

        if __debug__ and 'GDA' in debug.active:
            debug('GDA', "training finished on data.shape=%s " % (X.shape, )
                  + "min:max(data)=%f:%f" % (np.min(X), np.max(X)))


    def _untrain(self):
        """Untrain classifier and reset all learnt params
        """
        self.means = None
        self.cov = None
        self.ulabels = None
        self.priors = None
        super(GDA, self)._untrain()


    @accepts_dataset_as_samples
    def _predict(self, data):
        """Predict the output for the provided data.
        """
        params = self.params

        self.ca.estimates = prob_cs_cp = self._g_k(data)

        # Take the class with maximal (log)probability
        # XXX in GNB it is axis=0, i.e. classes were first
        winners = prob_cs_cp.argmax(axis=1)
        predictions = [self.ulabels[c] for c in winners]

        if __debug__ and 'GDA' in debug.active:
            debug('GDA', "predict on data.shape=%s min:max(data)=%f:%f " %
                  (data.shape, np.min(data), np.max(data)))

        return predictions

    def _inv(self, cov):
        try:
            return np.linalg.inv(cov)
        except Exception, e:
            if self.params.allow_pinv:
                try:
                    return np.linalg.pinv(cov)
                except Exception, e:
                    pass
            raise DegenerateInputError, \
              "Data is probably singular, since inverse fails. Got %s"\
              % (e,)


class LDA(GDA):
    """Linear Discriminant Analysis.
    """

    __tags__ = GDA.__tags__ + ['linear', 'lda']


    def _untrain(self):
        self._w = None
        self._b = None
        super(LDA, self)._untrain()


    def _train(self, dataset):
        super(LDA, self)._train(dataset)
        nlabels = len(self.ulabels)
        # Sum and scale the covariance
        self.cov = cov = \
            np.sum(self.cov, axis=0) \
            / (np.sum(self.nsamples_per_class) - nlabels)

        # For now as simple as that -- see notes on top
        covi = self._inv(cov)

        # Precompute and store the actual separating hyperplane and offset
        self._w = np.dot(covi, self.means.T)
        self._b = b = np.zeros((nlabels,))
        for il in xrange(nlabels):
            m = self.means[il]
            b[il] = np.log(self.priors[il]) - 0.5 * np.dot(np.dot(m.T, covi), m)

    def _g_k(self, data):
        """Return decision function values"""
        return np.dot(data, self._w) + self._b


class QDA(GDA):
    """Quadratic Discriminant Analysis.
    """

    __tags__ = GDA.__tags__ + ['non-linear', 'qda']

    def _untrain(self):
        # XXX theoretically we could use the same _w although with
        # different "content"
        self._icov = None
        self._b = None
        super(QDA, self)._untrain()

    def _train(self, dataset):
        super(QDA, self)._train(dataset)

        # XXX should we drag cov around at all then?
        self._icov = np.zeros(self.cov.shape)

        for ic, cov in enumerate(self.cov):
            cov /= float(self.nsamples_per_class[ic])
            self._icov[ic] = self._inv(cov)

        self._b = np.array([np.log(p) - 0.5 * np.log(np.linalg.det(c))
                            for p,c in zip(self.priors, self.cov)])

    def _g_k(self, data):
        """Return decision function values"""
        res = []
        for m, covi, b in zip(self.means, self._icov, self._b):
            dm = data - m
            res.append(b - 0.5 * np.sum(np.dot(dm, covi) * dm, axis=1))
        return np.array(res).T

########NEW FILE########
__FILENAME__ = glmnet
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""GLM-Net (GLMNET) regression and classifier."""

__docformat__ = 'restructuredtext'

# system imports
import numpy as np

import mvpa2.base.externals as externals

# do conditional to be able to build module reference
if externals.exists('glmnet', raise_=True):
    import rpy2.robjects
    import rpy2.robjects.numpy2ri
    if hasattr(rpy2.robjects.numpy2ri,'activate'):
        rpy2.robjects.numpy2ri.activate()
    RRuntimeError = rpy2.robjects.rinterface.RRuntimeError
    r = rpy2.robjects.r
    r.library('glmnet')
    from mvpa2.support.rpy2_addons import Rrx2

# local imports
from mvpa2.base import warning
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.base.learner import FailedToTrainError
from mvpa2.measures.base import Sensitivity
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import *
from mvpa2.datasets.base import Dataset

if __debug__:
    from mvpa2.base import debug

def _label2indlist(labels, ulabels):
    """Convert labels to list of unique label indicies starting at 1.
    """

    # allocate for the new one-of-M labels
    new_labels = np.zeros(len(labels), dtype=np.int)

    # loop and convert to one-of-M
    for i, c in enumerate(ulabels):
        new_labels[labels == c] = i+1

    return [str(l) for l in new_labels.tolist()]


def _label2oneofm(labels, ulabels):
    """Convert labels to one-of-M form.

    TODO: Might be useful elsewhere so could migrate into misc/
    """

    # allocate for the new one-of-M labels
    new_labels = np.zeros((len(labels), len(ulabels)))

    # loop and convert to one-of-M
    for i, c in enumerate(ulabels):
        new_labels[labels == c, i] = 1

    return new_labels


class _GLMNET(Classifier):
    """GLM-Net regression (GLMNET) `Classifier`.

    GLM-Net is the model selection algorithm from:

    Friedman, J., Hastie, T. and Tibshirani, R. (2008) Regularization
    Paths for Generalized Linear Models via Coordinate
    Descent. http://www-stat.stanford.edu/~hastie/Papers/glmnet.pdf

    To make use of GLMNET, you must have R and RPy2 installed as well
    as the glmnet contributed package. You can install the R and RPy2
    with the following command on Debian-based machines::

      sudo aptitude install python-rpy2 r-base-dev

    You can then install the glmnet package by running R
    as root and calling::

      install.packages()

    """

    __tags__ = [ 'glmnet', 'linear', 'has_sensitivity',
                 'does_feature_selection', 'rpy2'
                 ]

    family = Parameter('gaussian',
                       constraints=EnsureChoice('gaussian', 'multinomial'),
                       ro=True,
                       doc="""Response type of your targets (either 'gaussian'
                       for regression or 'multinomial' for classification).""")

    alpha = Parameter(1.0, constraints=EnsureFloat() & EnsureRange(min=0.01, max=1.0),
                      doc="""The elastic net mixing parameter.
                      Larger values will give rise to
                      less L2 regularization, with alpha=1.0
                      as a true LASSO penalty.""")

    nlambda = Parameter(100, constraints=EnsureInt() & EnsureRange(min=1),
                        doc="""Maximum number of lambdas to calculate
                        before stopping if not converged.""")

    standardize = Parameter(True, constraints='bool',
                            doc="""Whether to standardize the variables
                            prior to fitting.""")

    thresh = Parameter(1e-4, constraints=EnsureFloat() & EnsureRange(min=1e-10, max=1.0),
             doc="""Convergence threshold for coordinate descent.""")

    pmax = Parameter(None, 
             constraints=((EnsureInt() & EnsureRange(min=1)) | EnsureNone()),
             doc="""Limit the maximum number of variables ever to be
             nonzero.""")

    maxit = Parameter(100, constraints=EnsureInt() & EnsureRange(min=10),
             doc="""Maximum number of outer-loop iterations for
             'multinomial' families.""")

    model_type = Parameter('covariance',
                           constraints=EnsureChoice('covariance', 'naive'),
             doc="""'covariance' saves all inner-products ever
             computed and can be much faster than 'naive'. The
             latter can be more efficient for
             nfeatures>>nsamples situations.""")

    def __init__(self, **kwargs):
        """
        Initialize GLM-Net.

        See the help in R for further details on the parameters
        """
        # init base class first
        Classifier.__init__(self, **kwargs)

        # pylint friendly initializations
        self._utargets = None
        self.__weights = None
        """The beta weights for each feature."""
        self.__trained_model = None
        """The model object after training that will be used for
        predictions."""
        self.__last_lambda = None
        """Lambda obtained on the last step"""

#     def __repr__(self):
#         """String summary of the object
#         """
#         return """ENET(lm=%s, normalize=%s, intercept=%s, trace=%s, max_steps=%s, enable_ca=%s)""" % \
#                (self.__lm,
#                 self.__normalize,
#                 self.__intercept,
#                 self.__trace,
#                 self.__max_steps,
#                 str(self.ca.enabled))

    def _train(self, dataset):
        """Train the classifier using `data` (`Dataset`).
        """
        # process targets based on the model family
        targets = dataset.sa[self.get_space()].value
        if self.params.family == 'gaussian':
            # do nothing, just save the targets as a list
            #targets = targets.tolist()
            self._utargets = None
        elif self.params.family == 'multinomial':
            # turn lables into list of range values starting at 1
            #targets = _label2indlist(dataset.targets,
            #                        dataset.uniquetargets)
            targets_unique = dataset.sa[self.get_space()].unique
            targets = _label2oneofm(targets, targets_unique)

            # save some properties of the data/classification
            self._utargets = targets_unique.copy()

        # process the pmax
        if self.params.pmax is None:
            # set it to the num features
            pmax = dataset.nfeatures
        else:
            # use the value
            pmax = self.params.pmax

        try:
            self.__trained_model = trained_model = \
                r.glmnet(dataset.samples,
                         targets,
                         family=self.params.family,
                         alpha=self.params.alpha,
                         nlambda=self.params.nlambda,
                         standardize=self.params.standardize,
                         thresh=self.params.thresh,
                         pmax=pmax,
                         maxit=self.params.maxit,
                         type=self.params.model_type)
        except RRuntimeError, e:
            raise FailedToTrainError, \
                  "Failed to train %s on %s. Got '%s' during call r.glmnet()." \
                  % (self, dataset, e)

        self.__last_lambda = last_lambda = \
                             np.asanyarray(Rrx2(trained_model, 'lambda'))[-1]

        # set the weights to the last step
        weights = r.coef(trained_model, s=last_lambda)
        if self.params.family == 'multinomial':
            self.__weights = np.hstack([np.array(r['as.matrix'](weights[i]))[1:]
                                       for i in range(len(weights))])
        elif self.params.family == 'gaussian':
            self.__weights = np.array(r['as.matrix'](weights))[1:, 0]
        else:
            raise NotImplementedError, \
                  "Somehow managed to get here with family %s." % \
                  (self.params.family,)

    @accepts_dataset_as_samples
    def _predict(self, data):
        """
        Predict the output for the provided data.
        """
        # predict with standard method
        values = np.array(r.predict(self.__trained_model,
                                   newx=data,
                                   type='link',
                                   s=self.__last_lambda))

        # predict with the final state (i.e., the last step)
        classes = None
        if self.params.family == 'multinomial':
            # remove last dimension of values
            values = values[:, :, 0]

            # get the classes too (they are 1-indexed)
            class_ind = np.array(r.predict(self.__trained_model,
                                          newx=data,
                                          type='class',
                                          s=self.__last_lambda))

            # convert to 0-based ints
            class_ind = (class_ind-1).astype('int')

            # convert to actual targets
            # XXX If just one sample is predicted, the converted predictions
            # array is just 1D, hence it yields an IndexError on [:,0]
            # Modified to .squeeze() which should do the same.
            # Please acknowledge and remove this comment.
            #classes = self._utargets[class_ind][:,0]
            classes = self._utargets[class_ind].squeeze()
        else:
            # is gaussian, so just remove last dim of values
            values = values[:, 0]

        # values need to be set anyways if values state is enabled
        self.ca.estimates = values
        if classes is not None:
            # set the values and return none
            return classes
        else:
            # return the values as predictions
            return values


    def _init_internals(self):
        """Reinitialize all internals
        """
        self._utargets = None
        self.__weights = None
        """The beta weights for each feature."""
        self.__trained_model = None
        """The model object after training that will be used for
        predictions."""
        self.__last_lambda = None
        """Lambda obtained on the last step"""

    def _untrain(self):
        super(_GLMNET, self)._untrain()
        self._init_internals()


    ##REF: Name was automagically refactored
    def _get_feature_ids(self):
        """Return ids of the used features
        """
        return np.where(np.abs(self.__weights)>0)[0]


    ##REF: Name was automagically refactored
    def get_sensitivity_analyzer(self, **kwargs):
        """Returns a sensitivity analyzer for GLMNET."""
        return GLMNETWeights(self, **kwargs)

    weights = property(lambda self: self.__weights)



class GLMNETWeights(Sensitivity):
    """`SensitivityAnalyzer` that reports the weights GLMNET trained
    on a given `Dataset`.
    """

    _LEGAL_CLFS = [ _GLMNET ]

    def _call(self, dataset=None):
        """Extract weights from GLMNET classifier.

        GLMNET always has weights available, so nothing has to be computed here.
        """
        clf = self.clf
        weights = clf.weights

        if __debug__:
            debug('GLMNET',
                  "Extracting weights for GLMNET - "+
                  "Result: min=%f max=%f" %\
                  (np.min(weights), np.max(weights)))

        #return weights
        if clf.params.family == 'multinomial':
            return Dataset(weights.T, sa={clf.get_space(): clf._utargets})
        else:
            return Dataset(weights[np.newaxis])


class GLMNET_R(_GLMNET):
    """
    GLM-NET Gaussian Regression Classifier.

    This is the GLM-NET algorithm from

    Friedman, J., Hastie, T. and Tibshirani, R. (2008) Regularization
    Paths for Generalized Linear Models via Coordinate
    Descent. http://www-stat.stanford.edu/~hastie/Papers/glmnet.pdf

    parameterized to be a regression.

    See GLMNET_C for the multinomial classifier version.

    """

    __tags__ = _GLMNET.__tags__ + ['regression']

    def __init__(self,  **kwargs):
        """
        Initialize GLM-Net.

        See the help in R for further details on the parameters
        """
        # make sure they didn't specify incompatible model
        regr_family = 'gaussian'
        family = kwargs.pop('family', regr_family).lower()
        if family != regr_family:
            warning('You specified the parameter family=%s, but we '
                    'force this to be "%s" for regression.'
                    % (family, regr_family))
            family = regr_family

        # init base class first, forcing regression
        _GLMNET.__init__(self, family=family, **kwargs)


class GLMNET_C(_GLMNET):
    """
    GLM-NET Multinomial Classifier.

    This is the GLM-NET algorithm from

    Friedman, J., Hastie, T. and Tibshirani, R. (2008) Regularization
    Paths for Generalized Linear Models via Coordinate
    Descent. http://www-stat.stanford.edu/~hastie/Papers/glmnet.pdf

    parameterized to be a multinomial classifier.

    See GLMNET_Class for the gaussian regression version.

    """

    __tags__ = _GLMNET.__tags__ + ['multiclass', 'binary']

    def __init__(self,  **kwargs):
        """
        Initialize GLM-Net multinomial classifier.

        See the help in R for further details on the parameters
        """
        # make sure they didn't specify regression
        if not kwargs.pop('family', None) is None:
            warning('You specified the "family" parameter, but we '
                    'force this to be "multinomial".')

        # init base class first, forcing regression
        _GLMNET.__init__(self, family='multinomial', **kwargs)


########NEW FILE########
__FILENAME__ = gnb
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Gaussian Naive Bayes Classifier

   Basic implementation of Gaussian Naive Bayes classifier.
"""

"""
TODO: for now all estimates are allocated at the first level of GNB
instance (e.g. self.priors, etc) -- move them deeper or into a
corresponding "Collection"?
The same for GNBSearchlight
"""

__docformat__ = 'restructuredtext'

import numpy as np

from numpy import ones, zeros, sum, abs, isfinite, dot
from mvpa2.base import warning, externals
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.base.param import Parameter
from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.constraints import EnsureChoice
#from mvpa2.measures.base import Sensitivity


if __debug__:
    from mvpa2.base import debug

__all__ = [ "GNB" ]

class GNB(Classifier):
    """Gaussian Naive Bayes `Classifier`.

    `GNB` is a probabilistic classifier relying on Bayes rule to
    estimate posterior probabilities of labels given the data.  Naive
    assumption in it is an independence of the features, which allows
    to combine per-feature likelihoods by a simple product across
    likelihoods of "independent" features.
    See http://en.wikipedia.org/wiki/Naive_bayes for more information.

    Provided here implementation is "naive" on its own -- various
    aspects could be improved, but it has its own advantages:

    - implementation is simple and straightforward
    - no data copying while considering samples of specific class
    - provides alternative ways to assess prior distribution of the
      classes in the case of unbalanced sets of samples (see parameter
      `prior`)
    - makes use of NumPy broadcasting mechanism, so should be
      relatively efficient
    - should work for any dimensionality of samples

    `GNB` is listed both as linear and non-linear classifier, since
    specifics of separating boundary depends on the data and/or
    parameters: linear separation is achieved whenever samples are
    balanced (or ``prior='uniform'``) and features have the same
    variance across different classes (i.e. if
    ``common_variance=True`` to enforce this).

    Whenever decisions are made based on log-probabilities (parameter
    ``logprob=True``, which is the default), then conditional
    attribute `values`, if enabled, would also contain
    log-probabilities.  Also mention that normalization by the
    evidence (P(data)) is disabled by default since it has no impact
    per se on classification decision.  You might like to set
    parameter normalize to True if you want to access properly scaled
    probabilities in `values` conditional attribute.
    """
    # XXX decide when should we set corresponding internal,
    #     since it depends actually on the data -- no clear way,
    #     so set both linear and non-linear
    __tags__ = [ 'gnb', 'linear', 'non-linear',
                       'binary', 'multiclass' ]

    common_variance = Parameter(False, constraints='bool',
             doc="""Use the same variance across all classes.""")

    prior = Parameter('laplacian_smoothing',
             constraints=EnsureChoice('laplacian_smoothing', 'uniform', 'ratio'),
             doc="""How to compute prior distribution.""")

    logprob = Parameter(True, constraints='bool',
             doc="""Operate on log probabilities.  Preferable to avoid unneeded
             exponentiation and loose precision.
             If set, logprobs are stored in `values`""")

    normalize = Parameter(False, constraints='bool',
             doc="""Normalize (log)prob by P(data).  Requires probabilities thus
             for `logprob` case would require exponentiation of 'logprob's, thus
             disabled by default since does not impact classification output.
             """)

    def __init__(self, **kwargs):
        """Initialize an GNB classifier.
        """

        # init base class first
        Classifier.__init__(self, **kwargs)

        # pylint friendly initializations
        self.means = None
        """Means of features per class"""
        self.variances = None
        """Variances per class, but "vars" is taken ;)"""
        self.ulabels = None
        """Labels classifier was trained on"""
        self.priors = None
        """Class probabilities"""

        # Define internal state of classifier
        self._norm_weight = None

    def _get_priors(self, nlabels, nsamples, nsamples_per_class):
        """Return prior probabilities given data
        """
        # helper function - squash all dimensions but 1
        squash = lambda x: np.atleast_1d(x.squeeze())

        prior = self.params.prior
        if prior == 'uniform':
            priors = np.ones((nlabels,))/nlabels
        elif prior == 'laplacian_smoothing':
            priors = (1+squash(nsamples_per_class)) \
                          / (float(nsamples) + nlabels)
        elif prior == 'ratio':
            priors = squash(nsamples_per_class) / float(nsamples)
        else:
            raise ValueError(
                "No idea on how to handle '%s' way to compute priors"
                % self.params.prior)
        return priors

    def _train(self, dataset):
        """Train the classifier using `dataset` (`Dataset`).
        """
        params = self.params
        targets_sa_name = self.get_space()
        targets_sa = dataset.sa[targets_sa_name]

        # get the dataset information into easy vars
        X = dataset.samples
        labels = targets_sa.value
        self.ulabels = ulabels = targets_sa.unique
        nlabels = len(ulabels)
        label2index = dict((l, il) for il, l in enumerate(ulabels))

        # set the feature dimensions
        nsamples = len(X)
        s_shape = X.shape[1:]           # shape of a single sample

        self.means = means = \
                     np.zeros((nlabels, ) + s_shape)
        self.variances = variances = \
                     np.zeros((nlabels, ) + s_shape)
        # degenerate dimension are added for easy broadcasting later on
        nsamples_per_class = np.zeros((nlabels,) + (1,)*len(s_shape))

        # Estimate means and number of samples per each label
        for s, l in zip(X, labels):
            il = label2index[l]         # index of the label
            nsamples_per_class[il] += 1
            means[il] += s

        # helper function - squash all dimensions but 1
        squash = lambda x: np.atleast_1d(x.squeeze())
        ## Actually compute the means
        non0labels = (squash(nsamples_per_class) != 0)
        means[non0labels] /= nsamples_per_class[non0labels]

        # Store prior probabilities
        self.priors = self._get_priors(nlabels, nsamples, nsamples_per_class)

        # Estimate variances
        # better loop than repmat! ;)
        for s, l in zip(X, labels):
            il = label2index[l]         # index of the label
            variances[il] += (s - means[il])**2

        ## Actually compute the variances
        if params.common_variance:
            # we need to get global std
            cvar = np.sum(variances, axis=0)/nsamples # sum across labels
            # broadcast the same variance across labels
            variances[:] = cvar
        else:
            variances[non0labels] /= nsamples_per_class[non0labels]

        # Precompute and store weighting coefficient for Gaussian
        if params.logprob:
            # it would be added to exponent
            self._norm_weight = -0.5 * np.log(2*np.pi*variances)
        else:
            self._norm_weight = 1.0/np.sqrt(2*np.pi*variances)

        if __debug__ and 'GNB' in debug.active:
            debug('GNB', "training finished on data.shape=%s " % (X.shape, )
                  + "min:max(data)=%f:%f" % (np.min(X), np.max(X)))


    def _untrain(self):
        """Untrain classifier and reset all learnt params
        """
        self.means = None
        self.variances = None
        self.ulabels = None
        self.priors = None
        super(GNB, self)._untrain()


    @accepts_dataset_as_samples
    def _predict(self, data):
        """Predict the output for the provided data.
        """
        params = self.params
        # argument of exponentiation
        scaled_distances = \
            -0.5 * (((data - self.means[:, np.newaxis, ...])**2) \
                          / self.variances[:, np.newaxis, ...])
        if params.logprob:
            # if self.params.common_variance:
            # XXX YOH:
            # For decision there is no need to actually compute
            # properly scaled p, ie 1/sqrt(2pi * sigma_i) could be
            # simply discarded since it is common across features AND
            # classes
            # For completeness -- computing everything now even in logprob
            lprob_csfs = self._norm_weight[:, np.newaxis, ...] \
                         + scaled_distances

            # XXX for now just cut/paste with different operators, but
            #     could just bind them and reuse in the same equations
            # Naive part -- just a product of probabilities across features
            ## First we need to reshape to get class x samples x features
            lprob_csf = lprob_csfs.reshape(
                lprob_csfs.shape[:2] + (-1,))
            ## Now -- sum across features
            lprob_cs = lprob_csf.sum(axis=2)

            # Incorporate class probabilities:
            prob_cs_cp = lprob_cs + np.log(self.priors[:, np.newaxis])

        else:
            # Just a regular Normal distribution with per
            # feature/class mean and variances
            prob_csfs = \
                 self._norm_weight[:, np.newaxis, ...] \
                 * np.exp(scaled_distances)

            # Naive part -- just a product of probabilities across features
            ## First we need to reshape to get class x samples x features
            prob_csf = prob_csfs.reshape(
                prob_csfs.shape[:2] + (-1,))
            ## Now -- product across features
            prob_cs = prob_csf.prod(axis=2)

            # Incorporate class probabilities:
            prob_cs_cp = prob_cs * self.priors[:, np.newaxis]

        # Normalize by evidence P(data)
        if params.normalize:
            if params.logprob:
                prob_cs_cp_real = np.exp(prob_cs_cp)
            else:
                prob_cs_cp_real = prob_cs_cp
            prob_s_cp_marginals = np.sum(prob_cs_cp_real, axis=0)
            if params.logprob:
                prob_cs_cp -= np.log(prob_s_cp_marginals)
            else:
                prob_cs_cp /= prob_s_cp_marginals

        # Take the class with maximal (log)probability
        winners = prob_cs_cp.argmax(axis=0)
        predictions = [self.ulabels[c] for c in winners]

        # set to the probabilities per class
        self.ca.estimates = prob_cs_cp.T

        if __debug__ and 'GNB' in debug.active:
            debug('GNB', "predict on data.shape=%s min:max(data)=%f:%f " %
                  (data.shape, np.min(data), np.max(data)))

        return predictions


    # XXX Later come up with some
    #     could be a simple t-test maps using distributions
    #     per each class
    #def get_sensitivity_analyzer(self, **kwargs):
    #    """Returns a sensitivity analyzer for GNB."""
    #    return GNBWeights(self, **kwargs)


    # XXX Is there any reason to use properties?
    #means = property(lambda self: self.__biases)
    #variances = property(lambda self: self.__weights)



## class GNBWeights(Sensitivity):
##     """`SensitivityAnalyzer` that reports the weights GNB trained
##     on a given `Dataset`.

##     """

##     _LEGAL_CLFS = [ GNB ]

##     def _call(self, dataset=None):
##         """Extract weights from GNB classifier.

##         GNB always has weights available, so nothing has to be computed here.
##         """
##         clf = self.clf
##         means = clf.means
##           XXX we can do something better ;)
##         return means


########NEW FILE########
__FILENAME__ = gpr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com>
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Gaussian Process Regression (GPR)."""

__docformat__ = 'restructuredtext'


import numpy as np

from mvpa2.base import externals, warning

from mvpa2.base.state import ConditionalAttribute
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import EnsureFloat, EnsureNone, EnsureRange
from mvpa2.kernels.np import SquaredExponentialKernel, GeneralizedLinearKernel, \
     LinearKernel
from mvpa2.measures.base import Sensitivity
from mvpa2.misc.exceptions import InvalidHyperparameterError
from mvpa2.datasets import Dataset, dataset_wizard

if externals.exists("scipy", raise_=True):
    from scipy.linalg import cho_solve as SLcho_solve
    from scipy.linalg import cholesky as SLcholesky
    import scipy.linalg as SL
    # Some local binding for bits of speed up
    SLAError = SL.basic.LinAlgError

if __debug__:
    from mvpa2.base import debug

# Some local bindings for bits of speed up
from numpy import array, asarray
Nlog = np.log
Ndot = np.dot
Ndiag = np.diag
NLAcholesky = np.linalg.cholesky
NLAsolve = np.linalg.solve
NLAError = np.linalg.linalg.LinAlgError
eps64 = np.finfo(np.float64).eps

# Some precomputed items. log is relatively expensive
_halflog2pi = 0.5 * Nlog(2 * np.pi)

def _SLcholesky_autoreg(C, nsteps=None, **kwargs):
    """Simple wrapper around cholesky to incrementally regularize the
    matrix until successful computation.

    For `nsteps` we boost diagonal 10-fold each time from the
    'epsilon' of the respective dtype. If None -- would proceed until
    reaching 1.
    """
    if nsteps is None:
        nsteps = -int(np.floor(np.log10(np.finfo(float).eps)))
    result = None
    for step in xrange(nsteps):
        epsilon_value = (10**step) * np.finfo(C.dtype).eps
        epsilon = epsilon_value * np.eye(C.shape[0])
        try:
            result = SLcholesky(C + epsilon, lower=True)
        except SLAError, e:
            warning("Cholesky decomposition lead to failure: %s.  "
                    "As requested, performing auto-regularization but "
                    "for better control you might prefer to regularize "
                    "yourself by providing lm parameter to GPR" % e)
            if step < nsteps-1:
                if __debug__:
                    debug("GPR", "Failed to obtain cholesky on "
                          "auto-regularization step %d value %g. Got %s."
                          " Boosting lambda more to reg. C."
                          % (step, epsilon_value, e))
                continue
            else:
                raise

    if result is None:
        # no loop was done for some reason
        result = SLcholesky(C, lower=True)

    return result


class GPR(Classifier):
    """Gaussian Process Regression (GPR).

    """

    predicted_variances = ConditionalAttribute(enabled=False,
        doc="Variance per each predicted value")

    log_marginal_likelihood = ConditionalAttribute(enabled=False,
        doc="Log Marginal Likelihood")

    log_marginal_likelihood_gradient = ConditionalAttribute(enabled=False,
        doc="Log Marginal Likelihood Gradient")

    __tags__ = [ 'gpr', 'regression', 'retrainable' ]


    # NOTE XXX Parameters of the classifier. Values available as
    # clf.parameter or clf.params.parameter, or as
    # clf.params['parameter'] (as the full Parameter object)
    #
    # __doc__ and __repr__ for class is conviniently adjusted to
    # reflect values of those params

    # Kernel machines/classifiers should be refactored also to behave
    # the same and define kernel parameter appropriately... TODO, but SVMs
    # already kinda do it nicely ;-)

    sigma_noise = Parameter(0.001, 
        constraints=EnsureFloat() & EnsureRange(min=1e-10),
        doc="the standard deviation of the gaussian noise.")

    # XXX For now I don't introduce kernel parameter since yet to unify
    # kernel machines
    #kernel = Parameter(None, allowedtype='Kernel',
    #    doc="Kernel object defining the covariance between instances. "
    #        "(Defaults to KernelSquaredExponential if None in arguments)")

    lm = Parameter(None,
        constraints=((EnsureFloat() & EnsureRange(min=0.0)) | EnsureNone()),
        doc="""The regularization term lambda.
        Increase this when the kernel matrix is not positive definite. If None,
        some regularization will be provided upon necessity""")


    def __init__(self, kernel=None, **kwargs):
        """Initialize a GPR regression analysis.

        Parameters
        ----------
        kernel : Kernel
          a kernel object defining the covariance between instances.
          (Defaults to SquaredExponentialKernel if None in arguments)
        """
        # init base class first
        Classifier.__init__(self, **kwargs)

        # It does not make sense to calculate a confusion matrix for a GPR
        # XXX it does ;) it will be a RegressionStatistics actually ;-)
        # So if someone desires -- let him have it
        # self.ca.enable('training_stats', False)

        # set kernel:
        if kernel is None:
            kernel = SquaredExponentialKernel()
            debug("GPR",
                  "No kernel was provided, falling back to default: %s"
                  % kernel)
        self.__kernel = kernel

        # append proper clf_internal depending on the kernel
        # TODO: add "__tags__" to kernels since the check
        #       below does not scale
        if isinstance(kernel, GeneralizedLinearKernel) or \
           isinstance(kernel, LinearKernel):
            self.__tags__ += ['linear']
        else:
            self.__tags__ += ['non-linear']

        if externals.exists('openopt') \
               and not 'has_sensitivity' in self.__tags__:
            self.__tags__ += ['has_sensitivity']

        # No need to initialize conditional attributes. Unless they got set
        # they would raise an exception self.predicted_variances =
        # None self.log_marginal_likelihood = None
        self._init_internals()
        pass


    def _init_internals(self):
        """Reset some internal variables to None.

        To be used in constructor and untrain()
        """
        self._train_fv = None
        self._labels = None
        self._km_train_train = None
        self._train_labels = None
        self._alpha = None
        self._L = None
        self._LL = None
        # XXX EO: useful for model selection but not working in general
        # self.__kernel.reset()
        pass


    def __repr__(self):
        """String summary of the object
        """
        return super(GPR, self).__repr__(
            prefixes=['kernel=%s' % self.__kernel])


    def compute_log_marginal_likelihood(self):
        """
        Compute log marginal likelihood using self.train_fv and self.targets.
        """
        if __debug__:
            debug("GPR", "Computing log_marginal_likelihood")
        self.ca.log_marginal_likelihood = \
                                 -0.5*Ndot(self._train_labels, self._alpha) - \
                                  Nlog(self._L.diagonal()).sum() - \
                                  self._km_train_train.shape[0] * _halflog2pi
        return self.ca.log_marginal_likelihood


    def compute_gradient_log_marginal_likelihood(self):
        """Compute gradient of the log marginal likelihood. This
        version use a more compact formula provided by Williams and
        Rasmussen book.
        """
        # XXX EO: check whether the precomputed self.alpha self.Kinv
        # are actually the ones corresponding to the hyperparameters
        # used to compute this gradient!
        # YYY EO: currently this is verified outside gpr.py but it is
        # not an efficient solution.
        # XXX EO: Do some memoizing since it could happen that some
        # hyperparameters are kept constant by user request, so we
        # don't need (somtimes) to recompute the corresponding
        # gradient again. COULD THIS BE TAKEN INTO ACCOUNT BY THE
        # NEW CACHED KERNEL INFRASTRUCTURE?

        # self.Kinv = np.linalg.inv(self._C)
        # Faster:
        Kinv = SLcho_solve(self._LL, np.eye(self._L.shape[0]))

        alphalphaT = np.dot(self._alpha[:,None], self._alpha[None,:])
        tmp = alphalphaT - Kinv
        # Pass tmp to __kernel and let it compute its gradient terms.
        # This scales up to huge number of hyperparameters:
        grad_LML_hypers = self.__kernel.compute_lml_gradient(
            tmp, self._train_fv)
        grad_K_sigma_n = 2.0*self.params.sigma_noise*np.eye(tmp.shape[0])
        # Add the term related to sigma_noise:
        # grad_LML_sigma_n = 0.5 * np.trace(np.dot(tmp,grad_K_sigma_n))
        # Faster formula: tr(AB) = (A*B.T).sum()
        grad_LML_sigma_n = 0.5 * (tmp * (grad_K_sigma_n).T).sum()
        lml_gradient = np.hstack([grad_LML_sigma_n, grad_LML_hypers])
        self.log_marginal_likelihood_gradient = lml_gradient
        return lml_gradient


    def compute_gradient_log_marginal_likelihood_logscale(self):
        """Compute gradient of the log marginal likelihood when
        hyperparameters are in logscale. This version use a more
        compact formula provided by Williams and Rasmussen book.
        """
        # Kinv = np.linalg.inv(self._C)
        # Faster:
        Kinv = SLcho_solve(self._LL, np.eye(self._L.shape[0]))
        alphalphaT = np.dot(self._alpha[:,None], self._alpha[None,:])
        tmp = alphalphaT - Kinv
        grad_LML_log_hypers = \
            self.__kernel.compute_lml_gradient_logscale(tmp, self._train_fv)
        grad_K_log_sigma_n = 2.0 * self.params.sigma_noise ** 2 * np.eye(Kinv.shape[0])
        # Add the term related to sigma_noise:
        # grad_LML_log_sigma_n = 0.5 * np.trace(np.dot(tmp, grad_K_log_sigma_n))
        # Faster formula: tr(AB) = (A * B.T).sum()
        grad_LML_log_sigma_n = 0.5 * (tmp * (grad_K_log_sigma_n).T).sum()
        lml_gradient = np.hstack([grad_LML_log_sigma_n, grad_LML_log_hypers])
        self.log_marginal_likelihood_gradient = lml_gradient
        return lml_gradient


    ##REF: Name was automagically refactored
    def get_sensitivity_analyzer(self, flavor='auto', **kwargs):
        """Returns a sensitivity analyzer for GPR.

        Parameters
        ----------
        flavor : str
          What sensitivity to provide. Valid values are
          'linear', 'model_select', 'auto'.
          In case of 'auto' selects 'linear' for linear kernel
          and 'model_select' for the rest. 'linear' corresponds to
          GPRLinearWeights and 'model_select' to GRPWeights
        """
        # XXX The following two lines does not work since
        # self.__kernel is instance of LinearKernel and not
        # just LinearKernel. How to fix?
        # YYY yoh is not sure what is the problem... LinearKernel is actually
        #     kernel.LinearKernel so everything shoudl be ok
        if flavor == 'auto':
            flavor = ('model_select', 'linear')\
                     [int(isinstance(self.__kernel, GeneralizedLinearKernel)
                          or
                          isinstance(self.__kernel, LinearKernel))]
            if __debug__:
                debug("GPR", "Returning '%s' sensitivity analyzer" % flavor)

        # Return proper sensitivity
        if flavor == 'linear':
            return GPRLinearWeights(self, **kwargs)
        elif flavor == 'model_select':
            # sanity check
            if not ('has_sensitivity' in self.__tags__):
                raise ValueError, \
                      "model_select flavor is not available probably " \
                      "due to not available 'openopt' module"
            return GPRWeights(self, **kwargs)
        else:
            raise ValueError, "Flavor %s is not recognized" % flavor


    def _train(self, data):
        """Train the classifier using `data` (`Dataset`).
        """

        # local bindings for faster lookup
        params = self.params
        retrainable = params.retrainable
        if retrainable:
            newkernel = False
            newL = False
            _changedData = self._changedData

        self._train_fv = train_fv = data.samples
        # GRP relies on numerical labels
        # yoh: yeah -- GPR now is purely regression so no conversion
        #      is necessary
        train_labels = data.sa[self.get_space()].value
        self._train_labels = train_labels

        if not retrainable or _changedData['traindata'] \
               or _changedData.get('kernel_params', False):
            if __debug__:
                debug("GPR", "Computing train train kernel matrix")
            self.__kernel.compute(train_fv)
            self._km_train_train = km_train_train = asarray(self.__kernel)
            newkernel = True
            if retrainable:
                self._km_train_test = None # reset to facilitate recomputation
        else:
            if __debug__:
                debug("GPR", "Not recomputing kernel since retrainable and "
                      "nothing has changed")
            km_train_train = self._km_train_train # reuse

        if not retrainable or newkernel or _changedData['params']:
            if __debug__:
                debug("GPR", "Computing L. sigma_noise=%g" \
                             % params.sigma_noise)
            # XXX it seems that we do not need binding to object, but may be
            # commented out code would return?
            self._C = km_train_train + \
                  params.sigma_noise ** 2 * \
                  np.identity(km_train_train.shape[0], 'd')
            # The following decomposition could raise
            # np.linalg.linalg.LinAlgError because of numerical
            # reasons, due to the too rapid decay of 'self._C'
            # eigenvalues. In that case we try adding a small constant
            # to self._C, e.g. epsilon=1.0e-20. It should be a form of
            # Tikhonov regularization. This is equivalent to adding
            # little white gaussian noise to data.
            #
            # XXX EO: how to choose epsilon?
            #
            # Cholesky decomposition is provided by three different
            # NumPy/SciPy routines (fastest first):
            # 1) self._LL = scipy.linalg.cho_factor(self._C, lower=True)
            #    self._L = L = np.tril(self._LL[0])
            # 2) self._L = scipy.linalg.cholesky(self._C, lower=True)
            # 3) self._L = numpy.linalg.cholesky(self._C)
            # Even though 1 is the fastest we choose 2 since 1 does
            # not return a clean lower-triangular matrix (see docstring).

            # PBS: I just made it so the KernelMatrix is regularized
            # all the time.  I figured that if ever you were going to
            # use regularization, you would want to set it yourself
            # and use the same value for all folds of your data.
            # YOH: Ideally so, but in real "use cases" some might have no
            #      clue, also our unittests (actually clfs_examples) might
            #      fail without any good reason.  So lets return a magic with
            #      an option to forbid any regularization (if lm is None)
            try:
                # apply regularization
                lm, C = params.lm, self._C
                if lm is not None:
                    epsilon = lm * np.eye(C.shape[0])
                    self._L = SLcholesky(C + epsilon, lower=True)
                else:
                    # do 10 attempts to raise each time by 10
                    self._L = _SLcholesky_autoreg(C, nsteps=None, lower=True)
                self._LL = (self._L, True)
            except SLAError:
                raise SLAError("Kernel matrix is not positive, definite. "
                               "Try increasing the lm parameter.")
                pass
            newL = True
        else:
            if __debug__:
                debug("GPR", "Not computing L since kernel, data and params "
                      "stayed the same")

        # XXX we leave _alpha being recomputed, although we could check
        #   if newL or _changedData['targets']
        #
        if __debug__:
            debug("GPR", "Computing alpha")
        # L = self._L                 # reuse
        # self._alpha = NLAsolve(L.transpose(),
        #                              NLAsolve(L, train_labels))
        # Faster:
        self._alpha = SLcho_solve(self._LL, train_labels)

        # compute only if the state is enabled
        if self.ca.is_enabled('log_marginal_likelihood'):
            self.compute_log_marginal_likelihood()
            pass

        if retrainable:
            # we must assign it only if it is retrainable
            self.ca.retrained = not newkernel or not newL

        if __debug__:
            debug("GPR", "Done training")

        pass


    @accepts_dataset_as_samples
    def _predict(self, data):
        """
        Predict the output for the provided data.
        """
        retrainable = self.params.retrainable
        ca = self.ca

        if not retrainable or self._changedData['testdata'] \
               or self._km_train_test is None:
            if __debug__:
                debug('GPR', "Computing train test kernel matrix")
            self.__kernel.compute(self._train_fv, data)
            km_train_test = asarray(self.__kernel)
            if retrainable:
                self._km_train_test = km_train_test
                ca.repredicted = False
        else:
            if __debug__:
                debug('GPR', "Not recomputing train test kernel matrix")
            km_train_test = self._km_train_test
            ca.repredicted = True


        predictions = Ndot(km_train_test.transpose(), self._alpha)

        if ca.is_enabled('predicted_variances'):
            # do computation only if conditional attribute was enabled
            if not retrainable or self._km_test_test is None \
                   or self._changedData['testdata']:
                if __debug__:
                    debug('GPR', "Computing test test kernel matrix")
                self.__kernel.compute(data)
                km_test_test = asarray(self.__kernel)
                if retrainable:
                    self._km_test_test = km_test_test
            else:
                if __debug__:
                    debug('GPR', "Not recomputing test test kernel matrix")
                km_test_test = self._km_test_test

            if __debug__:
                debug("GPR", "Computing predicted variances")
            L = self._L
            # v = NLAsolve(L, km_train_test)
            # Faster:
            piv = np.arange(L.shape[0])
            v = SL.lu_solve((L.T, piv), km_train_test, trans=1)
            # self.predicted_variances = \
            #     Ndiag(km_test_test - Ndot(v.T, v)) \
            #     + self.sigma_noise**2
            # Faster formula: np.diag(Ndot(v.T, v)) = (v**2).sum(0):
            ca.predicted_variances = Ndiag(km_test_test) - (v ** 2).sum(0) \
                                       + self.params.sigma_noise ** 2
            pass

        if __debug__:
            debug("GPR", "Done predicting")
        ca.estimates = predictions
        return predictions


    ##REF: Name was automagically refactored
    def _set_retrainable(self, value, force=False):
        """Internal function : need to set _km_test_test
        """
        super(GPR, self)._set_retrainable(value, force)
        if force or (value and value != self.params.retrainable):
            self._km_test_test = None


    def _untrain(self):
        super(GPR, self)._untrain()
        # XXX might need to take special care for retrainable. later
        self._init_internals()


    def set_hyperparameters(self, hyperparameter):
        """
        Set hyperparameters' values.

        Note that 'hyperparameter' is a sequence so the order of its
        values is important. First value must be sigma_noise, then
        other kernel's hyperparameters values follow in the exact
        order the kernel expect them to be.
        """
        if hyperparameter[0] < self.params['sigma_noise'].min:
            raise InvalidHyperparameterError()
        self.params.sigma_noise = hyperparameter[0]
        if hyperparameter.size > 1:
            self.__kernel.set_hyperparameters(hyperparameter[1:])
            pass
        return

    kernel = property(fget=lambda self:self.__kernel)
    pass


class GPRLinearWeights(Sensitivity):
    """`SensitivityAnalyzer` that reports the weights GPR trained
    on a given `Dataset`.

    In case of LinearKernel compute explicitly the coefficients
    of the linear regression, together with their variances (if
    requested).

    Note that the intercept is not computed.
    """

    variances = ConditionalAttribute(enabled=False,
        doc="Variances of the weights (for GeneralizedLinearKernel)")

    _LEGAL_CLFS = [ GPR ]


    def _call(self, dataset):
        """Extract weights from GPR
        """

        clf = self.clf
        kernel = clf.kernel
        train_fv = clf._train_fv
        if isinstance(kernel, LinearKernel):
            Sigma_p = 1.0
        else:
            Sigma_p = kernel.params.Sigma_p

        weights = Ndot(Sigma_p,
                        Ndot(train_fv.T, clf._alpha))

        if self.ca.is_enabled('variances'):
            # super ugly formulas that can be quite surely improved:
            tmp = np.linalg.inv(clf._L)
            Kyinv = Ndot(tmp.T, tmp)
            # XXX in such lengthy matrix manipulations you might better off
            #     using np.matrix where * is a matrix product
            self.ca.variances = Ndiag(
                Sigma_p -
                Ndot(Sigma_p,
                      Ndot(train_fv.T,
                            Ndot(Kyinv,
                                  Ndot(train_fv, Sigma_p)))))
        return Dataset(np.atleast_2d(weights))


if externals.exists('openopt'):

    from mvpa2.clfs.model_selector import ModelSelector

    class GPRWeights(Sensitivity):
        """`SensitivityAnalyzer` that reports the weights GPR trained
        on a given `Dataset`.
        """

        _LEGAL_CLFS = [ GPR ]

        def _call(self, ds_):
            """Extract weights from GPR

            .. note:
              Input dataset is not actually used. New dataset is
              constructed from what is known to the classifier
            """

            clf = self.clf
            # normalize data:
            clf._train_labels = (clf._train_labels - clf._train_labels.mean()) \
                                / clf._train_labels.std()
            # clf._train_fv = (clf._train_fv-clf._train_fv.mean(0)) \
            #                  /clf._train_fv.std(0)
            ds = dataset_wizard(samples=clf._train_fv, targets=clf._train_labels)
            clf.ca.enable("log_marginal_likelihood")
            ms = ModelSelector(clf, ds)
            # Note that some kernels does not have gradient yet!
            # XXX Make it initialize to clf's current hyperparameter values
            #     or may be add ability to specify starting points in the constructor
            sigma_noise_initial = 1.0e-5
            sigma_f_initial = 1.0
            length_scale_initial = np.ones(ds.nfeatures)*1.0e4
            # length_scale_initial = np.random.rand(ds.nfeatures)*1.0e4
            hyp_initial_guess = np.hstack([sigma_noise_initial,
                                          sigma_f_initial,
                                          length_scale_initial])
            fixedHypers = array([0]*hyp_initial_guess.size, dtype=bool)
            fixedHypers = None
            problem =  ms.max_log_marginal_likelihood(
                hyp_initial_guess=hyp_initial_guess,
                optimization_algorithm="scipy_lbfgsb",
                ftol=1.0e-3, fixedHypers=fixedHypers,
                use_gradient=True, logscale=True)
            if __debug__ and 'GPR_WEIGHTS' in debug.active:
                problem.iprint = 1
            lml = ms.solve()
            weights = 1.0/ms.hyperparameters_best[2:] # weight = 1/length_scale
            if __debug__:
                debug("GPR",
                      "%s, train: shape %s, labels %s, min:max %g:%g, "
                      "sigma_noise %g, sigma_f %g" %
                      (clf, clf._train_fv.shape, np.unique(clf._train_labels),
                       clf._train_fv.min(), clf._train_fv.max(),
                       ms.hyperparameters_best[0], ms.hyperparameters_best[1]))

            return weights


########NEW FILE########
__FILENAME__ = knn
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""k-Nearest-Neighbour classifier."""

__docformat__ = 'restructuredtext'

import sys
# not worthy of externals checking
_dict_has_key = sys.version_info >= (2, 5)

import numpy as np

from mvpa2.base import warning
from mvpa2.datasets.base import Dataset
from mvpa2.misc.support import indent_doc
from mvpa2.base.state import ConditionalAttribute

from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.clfs.distance import squared_euclidean_distance

__all__ = [ 'kNN' ]

if __debug__:
    from mvpa2.base import debug


class kNN(Classifier):
    """
    k-Nearest-Neighbour classifier.

    This is a simple classifier that bases its decision on the distances
    between the training dataset samples and the test sample(s). Distances
    are computed using a customizable distance function. A certain number
    (`k`)of nearest neighbors is selected based on the smallest distances
    and the labels of this neighboring samples are fed into a voting
    function to determine the labels of the test sample.

    Training a kNN classifier is extremely quick, as no actual training
    is performed as the training dataset is simply stored in the
    classifier. All computations are done during classifier prediction.

    Ties
    ----

    In case if voting procedure results in a tie, it is broken by
    choosing a class with minimal mean distance to the corresponding
    k-neighbors.

    Notes
    -----
    If enabled, kNN stores the votes per class in the 'values' state after
    calling predict().

    """

    distances = ConditionalAttribute(enabled=False,
        doc="Distances computed for each sample")


    __tags__ = ['knn', 'non-linear', 'binary', 'multiclass']

    def __init__(self, k=2, dfx=squared_euclidean_distance,
                 voting='weighted', **kwargs):
        """
        Parameters
        ----------
        k : unsigned integer
          Number of nearest neighbours to be used for voting.
        dfx : functor
          Function to compute the distances between training and test samples.
          Default: squared euclidean distance
        voting : str
          Voting method used to derive predictions from the nearest neighbors.
          Possible values are 'majority' (simple majority of classes
          determines vote) and 'weighted' (votes are weighted according to the
          relative frequencies of each class in the training data).
        **kwargs
          Additional arguments are passed to the base class.
        """

        # init base class first
        Classifier.__init__(self, **kwargs)

        self.__k = k
        self.__dfx = dfx
        self.__voting = voting
        self.__data = None
        self.__weights = None


    def __repr__(self, prefixes=[]): # pylint: disable-msg=W0102
        """Representation of the object
        """
        return super(kNN, self).__repr__(
            ["k=%d" % self.__k, "dfx=%s" % self.__dfx,
             "voting=%s" % repr(self.__voting)]
            + prefixes)


    ## def __str__(self):
    ##     return "%s\n data: %s" % \
    ##         (Classifier.__str__(self), indent_doc(self.__data))


    def _train(self, data):
        """Train the classifier.

        For kNN it is degenerate -- just stores the data.
        """
        self.__data = data
        labels = data.sa[self.get_space()].value
        uniquelabels = data.sa[self.get_space()].unique
        Nuniquelabels = len(uniquelabels)

        if __debug__:
            if str(data.samples.dtype).startswith('uint') \
                or str(data.samples.dtype).startswith('int'):
                warning("kNN: input data is in integers. " + \
                        "Overflow on arithmetic operations might result in"+\
                        " errors. Please convert dataset's samples into" +\
                        " floating datatype if any error is reported.")
        if self.__voting == 'weighted':
            self.__labels = labels.copy()
            Nlabels = len(labels)

            # TODO: To get proper speed up for the next line only,
            #       histogram should be computed
            #       via sorting + counting "same" elements while reducing.
            #       Guaranteed complexity is NlogN whenever now it is N^2
            # compute the relative proportion of samples belonging to each
            # class (do it in one loop to improve speed and reduce readability
            weights = \
                [ 1.0 - ((labels == label).sum() / Nlabels) \
                    for label in uniquelabels ]
            self.__weights = dict(zip(uniquelabels, weights))
        else:
            self.__weights = None

        # create dictionary with an item for each condition
        self.__votes_init = dict(zip(uniquelabels,
                                     [0] * Nuniquelabels))


    @accepts_dataset_as_samples
    def _predict(self, data):
        """Predict the class labels for the provided data.

        Returns a list of class labels (one for each data sample).
        """
        # make sure we're talking about arrays
        data = np.asanyarray(data)

        targets_sa_name = self.get_space()
        targets_sa = self.__data.sa[targets_sa_name]
        labels = targets_sa.value
        uniquelabels = targets_sa.unique

        # checks only in debug mode
        if __debug__:
            if not data.ndim == 2:
                raise ValueError, "Data array must be two-dimensional."

            if not data.shape[1] == self.__data.nfeatures:
                raise ValueError, "Length of data samples (features) does " \
                                  "not match the classifier."

        # compute the distance matrix between training and test data with
        # distances stored row-wise, i.e. distances between test sample [0]
        # and all training samples will end up in row 0
        dists = self.__dfx(self.__data.samples, data).T
        if self.ca.is_enabled('distances'):
            # .sa.copy() now does deepcopying by default
            self.ca.distances = Dataset(dists, fa=self.__data.sa.copy())

        # determine the k nearest neighbors per test sample
        knns = dists.argsort(axis=1)[:, :self.__k]

        # predictions and votes for all samples
        all_votes, predictions = [], []
        for inns, nns in enumerate(knns):
            votes = self.__votes_init.copy()
            # TODO: optimize!
            for nn in nns:
                votes[labels[nn]] += 1

            # optionally weight votes
            if self.__voting == 'majority':
                pass
            elif self.__voting == 'weighted':
                # TODO: optimize!
                for ul in uniquelabels:
                    votes[ul] *= self.__weights[ul]
            else:
                raise ValueError, "kNN told to perform unknown voting '%s'." \
                      % self.__voting

            # reverse dictionary items and sort them to get the
            # winners
            # It would be more expensive than just to look for
            # the maximum, but this piece should be the least
            # cpu-intensive while distances computation should consume
            # the most. Also it would allow to look and break the ties
            votes_reversed = sorted([(v, k) for k, v in votes.iteritems()],
                                    reverse=True)
            # check for ties
            max_vote, max_vote_label = votes_reversed[0]

            if len(votes_reversed) > 1 and max_vote == votes_reversed[1][0]:
                # figure out all ties and break them based on the mean
                # distance
                # TODO: theoretically we could break out of the loop earlier
                ties = [x[1] for x in votes_reversed if x[0] == max_vote]

                # compute mean distances to the corresponding clouds
                # restrict analysis only to k-nn's
                nns_labels = labels[nns]
                nns_dists = dists[inns][nns]
                ties_dists = [np.mean(nns_dists[nns_labels == t]) for t in ties]
                max_vote_label = ties[np.argmin(ties_dists)]
                if __debug__:
                    debug('KNN',
                          'Ran into the ties: %s with votes: %s, dists: %s, max_vote %r',
                          (ties, votes_reversed, ties_dists, max_vote_label))

            all_votes.append(votes)
            predictions.append(max_vote_label)

        # store the predictions in the state. Relies on State._setitem to do
        # nothing if the relevant state member is not enabled
        self.ca.predictions = predictions
        self.ca.estimates = all_votes # np.array([r[1] for r in results])

        return predictions

    def _untrain(self):
        """Reset trained state"""
        self.__data = None
        self.__weights = None
        super(kNN, self)._untrain()

    dfx = property(fget=lambda self: self.__dfx)

########NEW FILE########
__FILENAME__ = lars
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Least angle regression (LARS)."""

__docformat__ = 'restructuredtext'

# system imports
import numpy as np

import mvpa2.base.externals as externals

# do conditional to be able to build module reference
if externals.exists('lars', raise_=True):
    import rpy2.robjects
    import rpy2.robjects.numpy2ri
    if hasattr(rpy2.robjects.numpy2ri,'activate'):
        rpy2.robjects.numpy2ri.activate()
    RRuntimeError = rpy2.robjects.rinterface.RRuntimeError
    r = rpy2.robjects.r
    r.library('lars')
    from mvpa2.support.rpy2_addons import Rrx2

# local imports
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples, \
        FailedToPredictError
from mvpa2.base.learner import FailedToTrainError
from mvpa2.measures.base import Sensitivity
from mvpa2.datasets.base import Dataset

from mvpa2.base import warning
if __debug__:
    from mvpa2.base import debug

known_models = ('lasso', 'stepwise', 'lar', 'forward.stagewise')

class LARS(Classifier):
    """Least angle regression (LARS).

    LARS is the model selection algorithm from:

    Bradley Efron, Trevor Hastie, Iain Johnstone and Robert
    Tibshirani, Least Angle Regression Annals of Statistics (with
    discussion) (2004) 32(2), 407-499. A new method for variable
    subset selection, with the lasso and 'epsilon' forward stagewise
    methods as special cases.

    Similar to SMLR, it performs a feature selection while performing
    classification, but instead of starting with all features, it
    starts with none and adds them in, which is similar to boosting.

    This learner behaves more like a ridge regression in that it
    returns prediction values and it treats the training labels as
    continuous.

    In the true nature of the PyMVPA framework, this algorithm is
    actually implemented in R by Trevor Hastie and wrapped via RPy.
    To make use of LARS, you must have R and RPy installed as well as
    the LARS contributed package. You can install the R and RPy with
    the following command on Debian-based machines:

    sudo aptitude install python-rpy python-rpy-doc r-base-dev

    You can then install the LARS package by running R as root and
    calling:

    install.packages()

    """

    # XXX from yoh: it is linear, isn't it?
    __tags__ = [ 'lars', 'regression', 'linear', 'has_sensitivity',
                 'does_feature_selection', 'rpy2' ]

    def __init__(self, model_type="lasso", trace=False, normalize=True,
                 intercept=True, max_steps=None, use_Gram=False, **kwargs):
        """
        Initialize LARS.

        See the help in R for further details on the following parameters:

        Parameters
        ----------
        model_type : string
          Type of LARS to run. Can be one of ('lasso', 'lar',
          'forward.stagewise', 'stepwise').
        trace : boolean
          Whether to print progress in R as it works.
        normalize : boolean
          Whether to normalize the L2 Norm.
        intercept : boolean
          Whether to add a non-penalized intercept to the model.
        max_steps : None or int
          If not None, specify the total number of iterations to run. Each
          iteration adds a feature, but leaving it none will add until
          convergence.
        use_Gram : boolean
          Whether to compute the Gram matrix (this should be false if you
          have more features than samples.)
        """
        # init base class first
        Classifier.__init__(self, **kwargs)

        if not model_type in known_models:
            raise ValueError('Unknown model %s for LARS is specified. Known' %
                             model_type + 'are %s' % `known_models`)

        # set up the params
        self.__type = model_type
        self.__normalize = normalize
        self.__intercept = intercept
        self.__trace = trace
        self.__max_steps = max_steps
        self.__use_Gram = use_Gram

        # pylint friendly initializations
        self.__lowest_Cp_step = None
        self.__weights = None
        """The beta weights for each feature."""
        self.__trained_model = None
        """The model object after training that will be used for
        predictions."""


    def __repr__(self):
        """String summary of the object
        """
        return "LARS(type='%s', normalize=%s, intercept=%s, trace=%s, " \
               "max_steps=%s, use_Gram=%s, " \
               "enable_ca=%s)" % \
               (self.__type,
                self.__normalize,
                self.__intercept,
                self.__trace,
                self.__max_steps,
                self.__use_Gram,
                str(self.ca.enabled))


    def _train(self, data):
        """Train the classifier using `data` (`Dataset`).
        """
        targets = data.sa[self.get_space()].value[:, np.newaxis]
        # some non-Python friendly R-lars arguments
        lars_kwargs = {'use.Gram': self.__use_Gram}
        if self.__max_steps is not None:
            lars_kwargs['max.steps'] = self.__max_steps

        trained_model = r.lars(data.samples,
                               targets,
                               type=self.__type,
                               normalize=self.__normalize,
                               intercept=self.__intercept,
                               trace=self.__trace,
                               **lars_kwargs
                               )
        #import pydb
        #pydb.debugger()
        # find the step with the lowest Cp (risk)
        # it is often the last step if you set a max_steps
        # must first convert dictionary to array
        Cp_vals = None
        try:
            Cp_vals = np.asanyarray(Rrx2(trained_model, 'Cp'))
        except TypeError, e:
            raise FailedToTrainError, \
                  "Failed to train %s on %s. Got '%s' while trying to access " \
                  "trained model %s" % (self, data, e, trained_model)

        if Cp_vals is None:
            # if there were no any -- just choose 0th
            lowest_Cp_step = 0
        elif np.isnan(Cp_vals[0]):
            # sometimes may come back nan, so just pick the last one
            lowest_Cp_step = len(Cp_vals)-1
        else:
            # determine the lowest
            lowest_Cp_step = Cp_vals.argmin()

        self.__lowest_Cp_step = lowest_Cp_step
        # set the weights to the lowest Cp step
        self.__weights = np.asanyarray(
            Rrx2(trained_model, 'beta'))[lowest_Cp_step]

        self.__trained_model = trained_model # bind to an instance
#         # set the weights to the final state
#         self.__weights = self.__trained_model['beta'][-1,:]


    @accepts_dataset_as_samples
    def _predict(self, data):
        """
        Predict the output for the provided data.
        """
        # predict with the final state (i.e., the last step)
        # predict with the lowest Cp step
        try:
            res = r.predict(self.__trained_model,
                            data,
                            mode='step',
                            s=self.__lowest_Cp_step)
                            #s=self.__trained_model['beta'].shape[0])
            fit = np.atleast_1d(Rrx2(res, 'fit'))
        except RRuntimeError, e:
            raise FailedToPredictError, \
                  "Failed to predict on %s using %s. Exceptions was: %s" \
                  % (data, self, e)

        self.ca.estimates = fit
        return fit


    def _init_internals(self):
        """Reinitialize all internals
        """
        self.__lowest_Cp_step = None
        self.__weights = None
        """The beta weights for each feature."""
        self.__trained_model = None
        """The model object after training that will be used for
        predictions."""

    def _untrain(self):
        super(LARS, self)._untrain()
        self._init_internals()


    ##REF: Name was automagically refactored
    def _get_feature_ids(self):
        """Return ids of the used features
        """
        return np.where(np.abs(self.__weights)>0)[0]



    ##REF: Name was automagically refactored
    def get_sensitivity_analyzer(self, **kwargs):
        """Returns a sensitivity analyzer for LARS."""
        return LARSWeights(self, **kwargs)

    weights = property(lambda self: self.__weights)



class LARSWeights(Sensitivity):
    """`SensitivityAnalyzer` that reports the weights LARS trained
    on a given `Dataset`.
    """

    _LEGAL_CLFS = [ LARS ]

    def _call(self, dataset=None):
        """Extract weights from LARS classifier.

        LARS always has weights available, so nothing has to be computed here.
        """
        clf = self.clf
        weights = clf.weights

        if __debug__:
            debug('LARS',
                  "Extracting weights for LARS - "+
                  "Result: min=%f max=%f" %\
                  (np.min(weights), np.max(weights)))

        return Dataset(np.atleast_2d(weights))


########NEW FILE########
__FILENAME__ = ctypes_helper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helpers for wrapping C libraries with ctypes."""

import numpy as np
import ctypes as C

# define an array type to help with wrapping
c_darray = np.ctypeslib.ndpointer(dtype=np.float64, flags='ALIGNED,CONTIGUOUS')
c_larray = np.ctypeslib.ndpointer(dtype=np.int64, flags='ALIGNED,CONTIGUOUS')
c_farray = np.ctypeslib.ndpointer(dtype=np.float32, flags='ALIGNED,CONTIGUOUS')
c_iarray = np.ctypeslib.ndpointer(dtype=np.int32, flags='ALIGNED,CONTIGUOUS')

def extend_args(*args):
    """Turn ndarray arguments into dims and arrays."""
    arglist = []
    for arg in args:
        if isinstance(arg, np.ndarray):
            # add the dimensions
            arglist.extend(arg.shape)

        # just append the arg
        arglist.append(arg)

    return arglist

#############################################################
# I'm not sure the rest is helpful, but I'll keep it for now.
#############################################################

# incomplete type conversion
typemap = {
    np.float64: C.c_double,
    np.float32: C.c_float,
    np.int64: C.c_int64,
    np.int32: C.c_int32}

def process_args(*args):
    """Turn ndarray arguments into dims and array pointers for calling
    a ctypes-wrapped function."""
    arglist = []
    for arg in args:
        if isinstance(arg, np.ndarray):
            # add the dimensions
            arglist.extend(arg.shape)

            # add the pointer to the ndarray
            arglist.append(arg.ctypes.data_as(
                C.POINTER(typemap[arg.dtype.type])))
        else:
            # just append the arg
            arglist.append(arg)

    return arglist

def get_argtypes(*args):
    argtypes = []
    for arg in args:
        if isinstance(arg, np.ndarray):
            # add the dimensions
            argtypes.extend([C.c_int]*len(arg.shape))

            # add the pointer to the ndarray
            argtypes.append(np.ctypeslib.ndpointer(dtype=arg.dtype))
        else:
            # try and figure out the type
            argtypes.append(arg)
    return argtypes



########NEW FILE########
__FILENAME__ = sens
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Provide sensitivity measures for libsvm's SVM."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import warning
from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.param import Parameter
from mvpa2.base.types import asobjarray
from mvpa2.measures.base import Sensitivity
from mvpa2.datasets.base import Dataset

if __debug__:
    from mvpa2.base import debug

class LinearSVMWeights(Sensitivity):
    """`SensitivityAnalyzer` for the LIBSVM implementation of a linear SVM.
    """

    _ATTRIBUTE_COLLECTIONS = ['params']

    split_weights = Parameter(False, constraints='bool',
                  doc="If binary classification either to sum SVs per each "
                      "class separately.  Note: be careful with interpretation"
                      " of the values")

    def __init__(self, clf, **kwargs):
        """Initialize the analyzer with the classifier it shall use.

        Parameters
        ----------
        clf : LinearSVM
          classifier to use. Only classifiers sub-classed from
          `LinearSVM` may be used.
        """
        # init base classes first
        Sensitivity.__init__(self, clf, **kwargs)


    def _call(self, dataset, callables=[]):
        # local bindings
        clf = self.clf
        model = clf.model

        # Labels for sensitivities to be returned
        sens_labels = None

        if clf.__is_regression__:
            nr_class = None
            svm_labels = None           # shouldn't bother to provide "targets" for regressions
        else:
            nr_class = model.nr_class
            svm_labels = model.labels

        # No need to warn since now we by default we do not do
        # anything evil and provide labels -- so it is up for a user
        # to decide either he wants to do something silly
        #if nr_class != 2:
        #    warning("You are estimating sensitivity for SVM %s trained on %d" %
        #            (str(clf), nr_class) +
        #            " classes. Make sure that it is what you intended to do" )

        svcoef = np.matrix(model.get_sv_coef())
        svs = np.matrix(model.get_sv())
        rhos = np.asarray(model.get_rho())

        if self.params.split_weights:
            if nr_class != 2:
                raise NotImplementedError, \
                      "Cannot compute per-class weights for" \
                      " non-binary classification task"
            # libsvm might have different idea on the ordering
            # of labels, so we would need to map them back explicitely
            ds_labels = list(dataset.sa[clf.get_space()].unique) # labels in the dataset
            senses = [None for i in ds_labels]
            # first label is given positive value
            for i, (c, l) in enumerate( [(svcoef > 0, lambda x: x),
                                         (svcoef < 0, lambda x: x*-1)] ):
                # convert to array, and just take the meaningful dimension
                c_ = c.A[0]
                # NOTE svm_labels are numerical; ds_labels are literal
                senses[ds_labels.index(
                            clf._attrmap.to_literal(svm_labels[i]))] = \
                                (l(svcoef[:, c_] * svs[c_, :])).A[0]
            weights = np.array(senses)
            sens_labels = svm_labels
        else:
            # XXX yoh: .mean() is effectively
            # averages across "sensitivities" of all paired classifiers (I
            # think). See more info on this topic in svm.py on how sv_coefs
            # are stored
            #
            # First multiply SV coefficients with the actual SVs to get
            # weighted impact of SVs on decision, then for each feature
            # take mean across SVs to get a single weight value
            # per feature
            if nr_class is None or nr_class <= 2:
                # as simple as this
                weights = (svcoef * svs).A
                # and only in case of classification
                if nr_class:
                    # ??? First label seems corresponds to positive
                    sens_labels = [tuple(svm_labels[::-1])]
            else:
                # we need to compose correctly per each pair of classifiers.
                # See docstring for get_sv_coef for more details on internal
                # structure of bloody storage

                # total # of pairs
                npairs = nr_class * (nr_class-1)/2
                # # of SVs in each class
                NSVs_perclass = model.get_n_sv()
                # indices where each class starts in each row of SVs
                # name is after similar variable in libsvm internals
                nz_start = np.cumsum([0] + NSVs_perclass[:-1])
                nz_end = nz_start + NSVs_perclass
                # reserve storage
                weights = np.zeros((npairs, svs.shape[1]))
                ipair = 0               # index of the pair
                """
                // classifier (i,j): coefficients with
				// i are in sv_coef[j-1][nz_start[i]...],
				// j are in sv_coef[i][nz_start[j]...]
                """
                sens_labels = []
                for i in xrange(nr_class):
                    for j in xrange(i+1, nr_class):
                        weights[ipair, :] = np.asarray(
                            svcoef[j-1, nz_start[i]:nz_end[i]]
                            * svs[nz_start[i]:nz_end[i]]
                            +
                            svcoef[i, nz_start[j]:nz_end[j]]
                            * svs[nz_start[j]:nz_end[j]]
                            )
                        # ??? First label corresponds to positive
                        # that is why [j], [i]
                        sens_labels += [(svm_labels[j], svm_labels[i])]
                        ipair += 1      # go to the next pair
                assert(ipair == npairs)

        if __debug__ and 'SVM' in debug.active:
            if nr_class:
                nsvs = model.get_n_sv()
            else:
                nsvs = model.get_total_n_sv()
            if clf.__is_regression__:
                svm_type = clf._svm_impl # type of regression
            else:
                svm_type = '%d-class SVM(%s)' % (nr_class, clf._svm_impl)
            debug('SVM',
                  "Extracting weights for %s: #SVs=%s, " % \
                  (svm_type, nsvs) + \
                  " SVcoefshape=%s SVs.shape=%s Rhos=%s." % \
                  (svcoef.shape, svs.shape, rhos) + \
                  " Result: min=%f max=%f" % (np.min(weights), np.max(weights)))

        ds_kwargs = {}
        if nr_class:          # for classification only
            # and we should have prepared the labels
            assert(sens_labels is not None)

            if len(clf._attrmap):
                if isinstance(sens_labels[0], tuple):
                    sens_labels = asobjarray(sens_labels)
                sens_labels = clf._attrmap.to_literal(sens_labels, recurse=True)

            # NOTE: `weights` is already and always 2D
            ds_kwargs = dict(sa={clf.get_space(): sens_labels})

        weights_ds = Dataset(weights, **ds_kwargs)
        weights_ds.sa['biases'] = rhos
        return weights_ds

    _customizeDocInherit = True

########NEW FILE########
__FILENAME__ = svm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wrap the libsvm package into a very simple class interface."""

__docformat__ = 'restructuredtext'

import numpy as np

import operator

from mvpa2.base import warning
from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.learner import FailedToTrainError

from mvpa2.clfs.base import accepts_dataset_as_samples, \
     accepts_samples_as_dataset
from mvpa2.clfs._svmbase import _SVM

from mvpa2.clfs.libsvmc import _svm
from mvpa2.kernels.libsvm import LinearLSKernel
from mvpa2.clfs.libsvmc.sens import LinearSVMWeights

if __debug__:
    from mvpa2.base import debug

# we better expose those since they are mentioned in docstrings
# although pylint would not be happy
from mvpa2.clfs.libsvmc._svmc import \
     C_SVC, NU_SVC, EPSILON_SVR, \
     NU_SVR, LINEAR, POLY, RBF, SIGMOID, \
     PRECOMPUTED, ONE_CLASS

def _data2ls(data):
    return np.asarray(data).astype(float)

class SVM(_SVM):
    """Support Vector Machine Classifier.

    This is a simple interface to the libSVM package.
    """

    # Since this is internal feature of LibSVM, this conditional attribute is present
    # here
    probabilities = ConditionalAttribute(enabled=False,
        doc="Estimates of samples probabilities as provided by LibSVM")

    # TODO p is specific for SVR
    _KNOWN_PARAMS = [ 'epsilon', 'probability', 'shrinking',
                      'weight_label', 'weight']

    #_KNOWN_KERNEL_PARAMS = [ 'cache_size' ]

    _KNOWN_SENSITIVITIES = {'linear':LinearSVMWeights,
                            }
    _KNOWN_IMPLEMENTATIONS = {
        'C_SVC' : (_svm.svmc.C_SVC, ('C',),
                   ('binary', 'multiclass'), 'C-SVM classification'),
        'NU_SVC' : (_svm.svmc.NU_SVC, ('nu',),
                    ('binary', 'multiclass'), 'nu-SVM classification'),
        'ONE_CLASS' : (_svm.svmc.ONE_CLASS, (),
                       ('oneclass',), 'one-class-SVM'),
        'EPSILON_SVR' : (_svm.svmc.EPSILON_SVR, ('C', 'tube_epsilon'),
                         ('regression',), 'epsilon-SVM regression'),
        'NU_SVR' : (_svm.svmc.NU_SVR, ('nu', 'tube_epsilon'),
                    ('regression',), 'nu-SVM regression')
        }

    __default_kernel_class__ = LinearLSKernel
    __tags__ = _SVM.__tags__ + [ 'libsvm' ]

    def __init__(self,
                 **kwargs):
        # XXX Determine which parameters depend on each other and implement
        # safety/simplifying logic around them
        # already done for: nr_weight
        # thought: weight and weight_label should be a dict
        """Interface class to LIBSVM classifiers and regressions.

        Default implementation (C/nu/epsilon SVM) is chosen depending
        on the given parameters (C/nu/tube_epsilon).
        """

        svm_impl = kwargs.get('svm_impl', None)
        # Depending on given arguments, figure out desired SVM
        # implementation
        if svm_impl is None:
            for arg, impl in [ ('tube_epsilon', 'EPSILON_SVR'),
                               ('C', 'C_SVC'),
                               ('nu', 'NU_SVC') ]:
                if kwargs.has_key(arg):
                    svm_impl = impl
                    if __debug__:
                        debug('SVM', 'No implementation was specified. Since '
                              '%s is given among arguments, assume %s' %
                              (arg, impl))
                    break
            if svm_impl is None:
                svm_impl = 'C_SVC'
                if __debug__:
                    debug('SVM', 'Assign C_SVC "by default"')
        kwargs['svm_impl'] = svm_impl

        # init base class
        _SVM.__init__(self, **kwargs)

        self._svm_type = self._KNOWN_IMPLEMENTATIONS[svm_impl][0]

        if 'nu' in self._KNOWN_PARAMS and 'epsilon' in self._KNOWN_PARAMS:
            # overwrite eps param with new default value (information
            # taken from libSVM docs
            self.params['epsilon']._set_default(0.001)

        self.__model = None
        """Holds the trained SVM."""



    def _train(self, dataset):
        """Train SVM
        """
        targets_sa_name = self.get_space()    # name of targets sa
        targets_sa = dataset.sa[targets_sa_name] # actual targets sa

        # libsvm needs doubles
        src = _data2ls(dataset)

        # libsvm cannot handle literal labels
        labels = self._attrmap.to_numeric(targets_sa.value).tolist()

        svmprob = _svm.SVMProblem(labels, src )

        # Translate few params
        TRANSLATEDICT = {'epsilon': 'eps',
                         'tube_epsilon': 'p'}
        args = []
        for paramname, param in self.params.items() \
                + self.kernel_params.items():
            if paramname in TRANSLATEDICT:
                argname = TRANSLATEDICT[paramname]
            elif paramname in _svm.SVMParameter.default_parameters:
                argname = paramname
            else:
                if __debug__:
                    debug("SVM_", "Skipping parameter %s since it is not known "
                          "to libsvm" % paramname)
                continue
            args.append( (argname, param.value) )

        # ??? All those parameters should be fetched if present from
        # **kwargs and create appropriate parameters within .params or
        # .kernel_params
        libsvm_param = _svm.SVMParameter(
            kernel_type=self.params.kernel.as_raw_ls(),# Just an integer ID
            svm_type=self._svm_type,
            **dict(args))

        """Store SVM parameters in libSVM compatible format."""

        if self.params.has_key('C'):#svm_type in [_svm.svmc.C_SVC]:
            Cs = self._get_cvec(dataset)
            if len(Cs)>1:
                C0 = abs(Cs[0])
                scale = 1.0/(C0)#*np.sqrt(C0))
                # so we got 1 C per label
                uls = self._attrmap.to_numeric(targets_sa.unique)
                if len(Cs) != len(uls):
                    raise ValueError, "SVM was parameterized with %d Cs but " \
                          "there are %d labels in the dataset" % \
                          (len(Cs), len(targets_sa.unique))
                weight = [ c*scale for c in Cs ]
                # All 3 need to be set to take an effect
                libsvm_param._set_parameter('weight', weight)
                libsvm_param._set_parameter('nr_weight', len(weight))
                libsvm_param._set_parameter('weight_label', uls)
            libsvm_param._set_parameter('C', Cs[0])

        try:
            self.__model = _svm.SVMModel(svmprob, libsvm_param)
        except Exception, e:
            raise FailedToTrainError(str(e))


    @accepts_samples_as_dataset
    def _predict(self, data):
        """Predict values for the data
        """
        # libsvm needs doubles
        src = _data2ls(data)
        ca = self.ca

        predictions = [ self.model.predict(p) for p in src ]

        if ca.is_enabled('estimates'):
            if self.__is_regression__:
                estimates = [ self.model.predict_values_raw(p)[0] for p in src ]
            else:
                # if 'trained_targets' are literal they have to be mapped
                if ( np.issubdtype(self.ca.trained_targets.dtype, 'c') or
                     np.issubdtype(self.ca.trained_targets.dtype, 'U') ):
                    trained_targets = self._attrmap.to_numeric(
                            self.ca.trained_targets)
                else:
                    trained_targets = self.ca.trained_targets
                nlabels = len(trained_targets)
                # XXX We do duplicate work. model.predict calls
                # predict_values_raw internally and then does voting or
                # thresholding. So if speed becomes a factor we might
                # want to move out logic from libsvm over here to base
                # predictions on obtined values, or adjust libsvm to
                # spit out values from predict() as well
                if nlabels == 2:
                    # Apperently libsvm reorders labels so we need to
                    # track (1,0) values instead of (0,1) thus just
                    # lets take negative reverse
                    estimates = [ self.model.predict_values(p)[(trained_targets[1],
                                                            trained_targets[0])]
                               for p in src ]
                    if len(estimates) > 0:
                        if __debug__:
                            debug("SVM",
                                  "Forcing estimates to be ndarray and reshaping"
                                  " them into 1D vector")
                        estimates = np.asarray(estimates).reshape(len(estimates))
                else:
                    # In multiclass we return dictionary for all pairs
                    # of labels, since libsvm does 1-vs-1 pairs
                    estimates = [ self.model.predict_values(p) for p in src ]
            ca.estimates = estimates

        if ca.is_enabled("probabilities"):
            # XXX Is this really necesssary? yoh don't think so since
            # assignment to ca is doing the same
            #self.probabilities = [ self.model.predict_probability(p)
            #                       for p in src ]
            try:
                ca.probabilities = [ self.model.predict_probability(p)
                                         for p in src ]
            except TypeError:
                warning("Current SVM %s doesn't support probability " %
                        self + " estimation.")
        return predictions


    def summary(self):
        """Provide quick summary over the SVM classifier
        """
        s = super(SVM, self).summary()
        if self.trained:
            s += '\n # of SVs: %d' % self.__model.get_total_n_sv()
            try:
                prm = _svm.svmc.svm_model_param_get(self.__model.model)
                C = _svm.svmc.svm_parameter_C_get(prm)
                # extract information of how many SVs sit inside the margin,
                # i.e. so called 'bounded SVs'
                inside_margin = np.sum(
                    # take 0.99 to avoid rounding issues
                    np.abs(self.__model.get_sv_coef())
                          >= 0.99*_svm.svmc.svm_parameter_C_get(prm))
                s += ' #bounded SVs:%d' % inside_margin
                s += ' used C:%5g' % C
            except:
                pass
        return s


    def _untrain(self):
        """Untrain libsvm's SVM: forget the model
        """
        if __debug__ and "SVM" in debug.active:
            debug("SVM", "Untraining %s and destroying libsvm model" % self)
        super(SVM, self)._untrain()
        del self.__model
        self.__model = None

    model = property(fget=lambda self: self.__model)
    """Access to the SVM model."""


# try to configure libsvm 'noise reduction'. Due to circular imports,
# we can't check externals here since it would not work.
try:
    # if externals.exists('libsvm verbosity control'):
    if __debug__ and "LIBSVM" in debug.active:
        debug("LIBSVM", "Setting verbosity for libsvm to 255")
        _svm.svmc.svm_set_verbosity(255)
    else:
        _svm.svmc.svm_set_verbosity(0)
except AttributeError:
    warning("Available LIBSVM has no way to control verbosity of the output")

# Assign SVM class to limited set of LinearSVMWeights
LinearSVMWeights._LEGAL_CLFS = [SVM]

########NEW FILE########
__FILENAME__ = _svm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Python interface to the SWIG-wrapped libsvm"""

__docformat__ = 'restructuredtext'


from math import exp, fabs
import re, copy

import numpy as np

from mvpa2.base.types import is_sequence_type

from mvpa2.clfs.libsvmc import _svmc as svmc
from mvpa2.clfs.libsvmc._svmc import C_SVC, NU_SVC, ONE_CLASS, EPSILON_SVR, \
                                  NU_SVR, LINEAR, POLY, RBF, SIGMOID, \
                                  PRECOMPUTED

if __debug__:
    from mvpa2.base import debug

##REF: Name was automagically refactored
def int_array(seq):
    size = len(seq)
    array = svmc.new_int(size)
    for i, item in enumerate(seq):
        svmc.int_setitem(array, i, int(item))
    return array


##REF: Name was automagically refactored
def double_array(seq):
    size = len(seq)
    array = svmc.new_double(size)
    for i, item in enumerate(seq):
        svmc.double_setitem(array, i, item)
    return array


##REF: Name was automagically refactored
def free_int_array(x):
    if x != 'NULL' and x != None:
        svmc.delete_int(x)


##REF: Name was automagically refactored
def free_double_array(x):
    if x != 'NULL' and x != None:
        svmc.delete_double(x)


def int_array_to_list(x, n):
    return [svmc.int_getitem(x, i) for i in xrange(n)]


def double_array_to_list(x, n):
    return [svmc.double_getitem(x, i) for i in xrange(n)]


class SVMParameter(object):
    """
    SVMParameter class safe to be deepcopied.
    """
    # default values
    default_parameters = {
    'svm_type' : C_SVC,
    'kernel_type' : RBF,
    'degree' : 3,
    'gamma' : 0,        # 1/k
    'coef0' : 0,
    'nu' : 0.5,
    'cache_size' : 100,
    'C' : 1,
    'eps' : 1e-3,
    'p' : 0.1,
    'shrinking' : 1,
    'nr_weight' : 0,
    'weight_label' : [],
    'weight' : [],
    'probability' : 0
    }

    class _SVMCParameter(object):
        """Internal class to to avoid memory leaks returning away svmc's params"""

        def __init__(self, params):
            self.param = svmc.new_svm_parameter()
            for attr, val in params.items():
                # adjust val if necessary
                if attr == 'weight_label':
                    #self.__weight_label_len = len(val)
                    val = int_array(val)
                    # no need?
                    #free_int_array(self.weight_label)
                elif attr == 'weight':
                    #self.__weight_len = len(val)
                    val = double_array(val)
                    # no need?
                    # free_double_array(self.weight)
                # set the parameter through corresponding call
                set_func = getattr(svmc, 'svm_parameter_%s_set' % (attr))
                set_func(self.param, val)

        def __del__(self):
            if __debug__:
                debug('SVM_', 'Destroying libsvm._SVMCParameter %s' % str(self))
            free_int_array(svmc.svm_parameter_weight_label_get(self.param))
            free_double_array(svmc.svm_parameter_weight_get(self.param))
            svmc.delete_svm_parameter(self.param)


    def __init__(self, **kw):
        self._orig_params = kw
        self.untrain()

    def untrain(self):
        self._params = {}
        self._params.update(self.default_parameters) # kinda copy.copy ;-)
        self._params.update(**self._orig_params)       # update with new values
        self.__svmc_params = None       # none is computed 
        self.__svmc_recompute = False   # thus none to recompute

    def __repr__(self):
        return self._params

    def __str__(self):
        return "SVMParameter: %s" % `self._params`

    def __copy__(self):
        out = SVMParameter()
        out._params = copy.copy(self._params)
        return out

    def __deepcopy__(self, memo):
        out = SVMParameter()
        out._params = copy.deepcopy(self._params)
        return out

    def _clear_svmc_params(self):
        if not self.__svmc_params is None:
            del self.__svmc_params
        self.__svmc_params = None

    @property
    def param(self):
        if self.__svmc_recompute:
            self._clear_svmc_params()
        if self.__svmc_params is None:
            self.__svmc_params = SVMParameter._SVMCParameter(self._params)
            self.__svmc_recompute = False
        return self.__svmc_params.param

    def __del__(self):
        if __debug__:
            debug('SVM_', 'Destroying libsvm.SVMParameter %s' % str(self))
        self._clear_svmc_params()

    ##REF: Name was automagically refactored
    def _set_parameter(self, key, value):
        """Not exactly proper one -- if lists are svmc_recompute, would fail anyways"""
        self.__svmc_recompute = True
        self._params[key] = value

    @classmethod
    def _register_properties(cls):
        for key in cls.default_parameters.keys():
            exec "%s.%s = property(fget=%s, fset=%s)"  % \
                 (cls.__name__, key,
                  "lambda self:self._params['%s']" % key,
                  "lambda self,val:self._set_parameter('%s', val)" % key)


SVMParameter._register_properties()

##REF: Name was automagically refactored
def seq_to_svm_node(x):
    """convert a sequence or mapping to an SVMNode array"""

    length = len(x)

    # make two lists, one of indices, one of values
    # YYY Use isinstance  instead of type...is so we could
    #     easily use derived subclasses
    if isinstance(x, np.ndarray):
        iter_range = range(length)
        iter_values = x
    elif isinstance(x, dict):
        iter_range = list(x).sort()
        iter_values = np.ndarray(x.values())
    elif is_sequence_type(x):
        iter_range = range(length)
        iter_values = np.asarray(x)
    else:
        raise TypeError, "data must be a mapping or an ndarray or a sequence"

    # allocate c struct
    data = svmc.svm_node_array(length + 1)
    # insert markers into the c struct
    svmc.svm_node_array_set(data, length, -1, 0.0)
    # pass the list and the ndarray to the c struct
    svmc.svm_node_array_set(data, iter_range, iter_values)

    return data



class SVMProblem:
    def __init__(self, y, x):
        assert len(y) == len(x)
        self.prob = prob = svmc.new_svm_problem()
        self.size = size = len(y)

        self.y_array = y_array = svmc.new_double(size)
        for i in xrange(size):
            svmc.double_setitem(y_array, i, y[i])

        self.x_matrix = x_matrix = svmc.svm_node_matrix(size)
        data = [None for i in xrange(size)]
        maxlen = 0
        for i in xrange(size):
            x_i = x[i]
            lx_i = len(x_i)
            data[i] = d = seq_to_svm_node(x_i)
            svmc.svm_node_matrix_set(x_matrix, i, d)
            if isinstance(x_i, dict):
                if (lx_i > 0):
                    maxlen = max(maxlen, max(x_i.keys()))
            else:
                maxlen = max(maxlen, lx_i)

        # bind to instance
        self.data = data
        self.maxlen = maxlen
        svmc.svm_problem_l_set(prob, size)
        svmc.svm_problem_y_set(prob, y_array)
        svmc.svm_problem_x_set(prob, x_matrix)


    def __repr__(self):
        return "<SVMProblem: size = %s>" % (self.size)


    def __del__(self):
        if __debug__:
            debug('SVM_', 'Destroying libsvm.SVMProblem %s' % `self`)

        svmc.delete_svm_problem(self.prob)
        svmc.delete_double(self.y_array)
        for i in range(self.size):
            svmc.svm_node_array_destroy(self.data[i])
        svmc.svm_node_matrix_destroy(self.x_matrix)



class SVMModel:
    def __init__(self, arg1, arg2=None):
        if arg2 == None:
            # create model from file
            filename = arg1
            self.model = svmc.svm_load_model(filename)
        else:
            # create model from problem and parameter
            prob, param = arg1, arg2
            self.prob = prob
            if param.gamma == 0:
                param.gamma = 1.0/prob.maxlen
            msg = svmc.svm_check_parameter(prob.prob, param.param)
            if msg:
                raise ValueError, msg
            self.model = svmc.svm_train(prob.prob, param.param)

        #setup some classwide variables
        self.nr_class = svmc.svm_get_nr_class(self.model)
        self.svm_type = svmc.svm_get_svm_type(self.model)
        #create labels(classes)
        intarr = svmc.new_int(self.nr_class)
        svmc.svm_get_labels(self.model, intarr)
        self.labels = int_array_to_list(intarr, self.nr_class)
        svmc.delete_int(intarr)
        #check if valid probability model
        self.probability = svmc.svm_check_probability_model(self.model)


    def __repr__(self):
        """
        Print string representation of the model or easier comprehension
        and some statistics
        """
        ret = '<SVMModel:'
        try:
            ret += ' type = %s, ' % `self.svm_type`
            ret += ' number of classes = %d (%s), ' \
                   % ( self.nr_class, `self.labels` )
        except:
            pass
        return ret+'>'


    def predict(self, x):
        data = seq_to_svm_node(x)
        ret = svmc.svm_predict(self.model, data)
        svmc.svm_node_array_destroy(data)
        return ret


    ##REF: Name was automagically refactored
    def get_nr_class(self):
        return self.nr_class


    ##REF: Name was automagically refactored
    def get_labels(self):
        if self.svm_type == NU_SVR \
           or self.svm_type == EPSILON_SVR \
           or self.svm_type == ONE_CLASS:
            raise TypeError, "Unable to get label from a SVR/ONE_CLASS model"
        return self.labels


    #def getParam(self):
    #    return SVMParameter(
    #                svmc_parameter=svmc.svm_model_param_get(self.model))


    ##REF: Name was automagically refactored
    def predict_values_raw(self, x):
        #convert x into SVMNode, allocate a double array for return
        n = self.nr_class*(self.nr_class-1)//2
        data = seq_to_svm_node(x)
        dblarr = svmc.new_double(n)
        svmc.svm_predict_values(self.model, data, dblarr)
        ret = double_array_to_list(dblarr, n)
        svmc.delete_double(dblarr)
        svmc.svm_node_array_destroy(data)
        return ret


    ##REF: Name was automagically refactored
    def predict_values(self, x):
        v = self.predict_values_raw(x)
        if self.svm_type == NU_SVR \
           or self.svm_type == EPSILON_SVR \
           or self.svm_type == ONE_CLASS:
            return v[0]
        else: #self.svm_type == C_SVC or self.svm_type == NU_SVC
            count = 0
            d = {}
            for i in range(len(self.labels)):
                for j in range(i+1, len(self.labels)):
                    d[self.labels[i], self.labels[j]] = v[count]
                    d[self.labels[j], self.labels[i]] = -v[count]
                    count += 1
            return  d


    ##REF: Name was automagically refactored
    def predict_probability(self, x):
        #c code will do nothing on wrong type, so we have to check ourself
        if self.svm_type == NU_SVR or self.svm_type == EPSILON_SVR:
            raise TypeError, "call get_svr_probability or get_svr_pdf " \
                             "for probability output of regression"
        elif self.svm_type == ONE_CLASS:
            raise TypeError, "probability not supported yet for one-class " \
                             "problem"
        #only C_SVC, NU_SVC goes in
        if not self.probability:
            raise TypeError, "model does not support probability estimates"

        #convert x into SVMNode, alloc a double array to receive probabilities
        data = seq_to_svm_node(x)
        dblarr = svmc.new_double(self.nr_class)
        pred = svmc.svm_predict_probability(self.model, data, dblarr)
        pv = double_array_to_list(dblarr, self.nr_class)
        svmc.delete_double(dblarr)
        svmc.svm_node_array_destroy(data)
        p = {}
        for i in range(len(self.labels)):
            p[self.labels[i]] = pv[i]
        return pred, p


    ##REF: Name was automagically refactored
    def get_svr_probability(self):
        #leave the Error checking to svm.cpp code
        ret = svmc.svm_get_svr_probability(self.model)
        if ret == 0:
            raise TypeError, "not a regression model or probability " \
                             "information not available"
        return ret


    ##REF: Name was automagically refactored
    def get_svr_pdf(self):
        #get_svr_probability will handle error checking
        sigma = self.get_svr_probability()
        return lambda z: exp(-fabs(z)/sigma)/(2*sigma)


    def save(self, filename):
        svmc.svm_save_model(filename, self.model)


    def __del__(self):
        if __debug__:
            # TODO: place libsvm versioning information into externals
            debug('SVM_', 'Destroying libsvm v. %s SVMModel %s',
                  (hasattr(svmc, '__version__') \
                   and svmc.__version__ or "unknown",
                   `self`))
        try:
            svmc.svm_destroy_model_helper(self.model)
        except Exception, e:
            # blind way to overcome problem of already deleted model and
            # "SVMModel instance has no attribute 'model'" in  ignored
            if __debug__:
                debug('SVM_', 'Failed to destroy libsvm.SVMModel due to %s' % (e,))
            pass


    ##REF: Name was automagically refactored
    def get_total_n_sv(self):
        return svmc.svm_model_l_get(self.model)


    ##REF: Name was automagically refactored
    def get_n_sv(self):
        """Returns a list with the number of support vectors per class.
        """
        return [ svmc.int_getitem(svmc.svm_model_nSV_get( self.model ), i) 
                    for i in range( self.nr_class ) ]


    ##REF: Name was automagically refactored
    def get_sv(self):
        """Returns an array with the all support vectors.

        array( nSV x <nFeatures>)
        """
        return svmc.svm_node_matrix2numpy_array(
                    svmc.svm_model_SV_get(self.model),
                    self.get_total_n_sv(),
                    self.prob.maxlen)


    ##REF: Name was automagically refactored
    def get_sv_coef(self):
        """Return coefficients for SVs... Needs to be used directly with caution!

        Summary on what is happening in libsvm internals with sv_coef

        svm_model's sv_coef (especially) are "cleverly" packed into a matrix
        nr_class - 1 x #SVs_total which stores
        coefficients for
        nr_class x (nr_class-1) / 2
        binary classifiers' SV coefficients.

        For classifier i-vs-j
        General packing rule can be described as:

          i-th row contains sv_coefficients for SVs of class i it took
          in all i-vs-j or j-vs-i classifiers.

        Another useful excerpt from svm.cpp is

                // classifier (i,j): coefficients with
                // i are in sv_coef[j-1][nz_start[i]...],
                // j are in sv_coef[i][nz_start[j]...]

        It can also be described as j-th column lists coefficients for SV # j which
        belongs to some class C, which it took (if it was an SV, ie != 0)
        in classifiers i vs C (iff i<C), or C vs i+1 (iff i>C)

        This way no byte of storage is wasted but imho such setup is quite convolved
        """
        return svmc.doubleppcarray2numpy_array(
                    svmc.svm_model_sv_coef_get(self.model),
                    self.nr_class - 1,
                    self.get_total_n_sv())


    ##REF: Name was automagically refactored
    def get_rho(self):
        """Return constant(s) in decision function(s) (if multi-class)"""
        return double_array_to_list(svmc.svm_model_rho_get(self.model),
                                self.nr_class * (self.nr_class-1)//2)

########NEW FILE########
__FILENAME__ = mass
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Generic wrappers for learners (classifiers) provided by R's MASS

Highly experimental and ad-hoc -- primary use was to verify LDA/QDA
results, thus not included in the mvpa2.suite ATM.
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import warning, externals
from mvpa2.base.state import ConditionalAttribute
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.base.learner import FailedToTrainError, FailedToPredictError


# do conditional to be able to build module reference
if externals.exists('mass', raise_=True):
    import rpy2.robjects
    import rpy2.robjects.numpy2ri
    if hasattr(rpy2.robjects.numpy2ri,'activate'):
        rpy2.robjects.numpy2ri.activate()
    RRuntimeError = rpy2.robjects.rinterface.RRuntimeError
    r = rpy2.robjects.r
    r.library('MASS')
    from mvpa2.support.rpy2_addons import Rrx, Rrx2


class MASSLearnerAdapter(Classifier):
    """Generic adapter for instances of learners provided by R's MASS

    Provides basic adaptation of interface for classifiers from MASS
    library (e.g. QDA, LDA), by adapting interface.

    Examples
    --------
    >>> if externals.exists('mass'):
    ...    from mvpa2.testing.datasets import datasets
    ...    mass_qda = MASSLearnerAdapter('qda', tags=['non-linear', 'multiclass'], enable_ca=['posterior'])
    ...    mass_qda.train(datasets['uni2large'])
    ...    mass_qda.predict(datasets['uni2large']) # doctest: +SKIP
    """

    __tags__ = ['mass', 'rpy2']

    posterior = ConditionalAttribute(enabled=False,
        doc='Posterior probabilities if provided by classifier')

    def __init__(self, learner, kwargs=None, kwargs_predict=None,
                 tags=None, **kwargs_):
        """
        Parameters
        ----------
        learner : string
        kwargs : dict, optional
        kwargs_predict : dict, optional
        tags : list of string
          What additional tags to attach to this classifier.  Tags are
          used in the queries to classifier or regression warehouses.
        """

        self._learner = learner

        self._kwargs = kwargs or {}
        self._kwargs_predict = kwargs_predict or {}

        if tags:
            # So we make a per-instance copy
            self.__tags__ = self.__tags__ + tags

        Classifier.__init__(self, **kwargs_)


    def __repr__(self):
        """String representation of `SKLLearnerWrapper`
        """
        return Classifier.__repr__(self,
            prefixes=[repr(self._learner),
                      'kwargs=%r' % (self._kwargs,)])


    def _train(self, dataset):
        """Train the skl learner using `dataset` (`Dataset`).
        """
        targets_sa = dataset.sa[self.get_space()]
        targets = targets_sa.value
        if not 'regression' in self.__tags__:
            targets = self._attrmap.to_numeric(targets)

        try:
            self._R_model = r[self._learner](
                dataset.samples,
                targets,
                **self._kwargs)
        except RRuntimeError, e:
            raise FailedToTrainError, \
                  "Failed to train %s on %s. Got '%s' during call to fit()." \
                  % (self, dataset, e)


    @accepts_dataset_as_samples
    def _predict(self, data):
        """Predict using the trained MASS learner
        """
        try:
            output = r.predict(self._R_model,
                               data,
                               **self._kwargs_predict)
            # TODO: access everything computed, and assign to
            #       ca's: res.names
            classes = Rrx2(output, 'class')
            # TODO: move to helper function to be used generically
            if classes.rclass[0] == 'factor':
                classes = [int(classes.levels[i-1]) for i in classes]
            if 'posterior' in output.names:
                self.ca.posterior = np.asarray(Rrx2(output, 'posterior'))
            res = np.asarray(classes)
        except Exception, e:
            raise FailedToPredictError, \
                  "Failed to predict %s on data of shape %s. Got '%s' during" \
                  " call to predict()." % (self, data.shape, e)

        return res

########NEW FILE########
__FILENAME__ = meta
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Meta classifiers -- classifiers which use other classifiers or preprocessing

Meta Classifiers can be grouped according to their function as

:group BoostedClassifiers: CombinedClassifier MulticlassClassifier
  SplitClassifier
:group ProxyClassifiers: ProxyClassifier BinaryClassifier MappedClassifier
  FeatureSelectionClassifier
:group PredictionsCombiners for CombinedClassifier: PredictionsCombiner
  MaximalVote MeanPrediction

"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.misc.args import group_kwargs
from mvpa2.base.types import is_sequence_type, asobjarray
from mvpa2.base.param import Parameter

from mvpa2.datasets import Dataset

from mvpa2.generators.splitters import Splitter
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.datasets.miscfx import get_samples_by_attr
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.base.dochelpers import _str, _repr_attrs
from mvpa2.base.state import ConditionalAttribute, ClassWithCollections

from mvpa2.clfs.base import Classifier
from mvpa2.clfs.distance import cartesian_distance
from mvpa2.misc.transformers import first_axis_mean

from mvpa2.measures.base import \
    BoostedClassifierSensitivityAnalyzer, ProxyClassifierSensitivityAnalyzer, \
    MappedClassifierSensitivityAnalyzer, \
    FeatureSelectionClassifierSensitivityAnalyzer, \
    RegressionAsClassifierSensitivityAnalyzer, \
    BinaryClassifierSensitivityAnalyzer, \
    _dont_force_slaves

from mvpa2.base import warning

if __debug__:
    from mvpa2.base import debug


class BoostedClassifier(Classifier):
    """Classifier containing the farm of other classifiers.

    Should rarely be used directly. Use one of its children instead
    """

    # should not be needed if we have prediction_estimates upstairs
    raw_predictions = ConditionalAttribute(enabled=False,
        doc="Predictions obtained from each classifier")

    raw_estimates = ConditionalAttribute(enabled=False,
        doc="Estimates obtained from each classifier")


    def __init__(self, clfs=None, propagate_ca=True,
                 **kwargs):
        """Initialize the instance.

        Parameters
        ----------
        clfs : list
          list of classifier instances to use (slave classifiers)
        propagate_ca : bool
          either to propagate enabled ca into slave classifiers.
          It is in effect only when slaves get assigned - so if state
          is enabled not during construction, it would not necessarily
          propagate into slaves
        kwargs : dict
          dict of keyworded arguments which might get used
          by State or Classifier
        """
        if clfs == None:
            clfs = []

        Classifier.__init__(self, **kwargs)

        self.__clfs = None
        """Pylint friendly definition of __clfs"""

        self.__propagate_ca = propagate_ca
        """Enable current enabled ca in slave classifiers"""

        self._set_classifiers(clfs)
        """Store the list of classifiers"""


    def __repr__(self, prefixes=[]):
        if self.__clfs is None or len(self.__clfs)==0:
            #prefix_ = "clfs=%s" % repr(self.__clfs)
            prefix_ = []
        else:
            prefix_ = ["clfs=[%s,...]" % repr(self.__clfs[0])]
        return super(BoostedClassifier, self).__repr__(prefix_ + prefixes)


    def _train(self, dataset):
        """Train `BoostedClassifier`
        """
        for clf in self.__clfs:
            clf.train(dataset)


    def _posttrain(self, dataset):
        """Custom posttrain of `BoostedClassifier`

        Harvest over the trained classifiers if it was asked to so
        """
        Classifier._posttrain(self, dataset)
        if self.params.retrainable:
            self.__changedData_isset = False


    def _get_feature_ids(self):
        """Custom _get_feature_ids for `BoostedClassifier`
        """
        # return union of all used features by slave classifiers
        feature_ids = set([])
        for clf in self.__clfs:
            feature_ids = feature_ids.union(set(clf.ca.feature_ids))
        return list(feature_ids)


    def _predict(self, dataset):
        """Predict using `BoostedClassifier`
        """
        raw_predictions = [ clf.predict(dataset) for clf in self.__clfs ]
        self.ca.raw_predictions = raw_predictions
        assert(len(self.__clfs)>0)
        if self.ca.is_enabled("estimates"):
            if np.array([x.ca.is_enabled("estimates")
                        for x in self.__clfs]).all():
                estimates = [ clf.ca.estimates for clf in self.__clfs ]
                self.ca.raw_estimates = estimates
            else:
                warning("One or more classifiers in %s has no 'estimates' state" %
                        self + "enabled, thus BoostedClassifier can't have" +
                        " 'raw_estimates' conditional attribute defined")

        return raw_predictions


    def _set_classifiers(self, clfs):
        """Set the classifiers used by the boosted classifier

        We have to allow to set list of classifiers after the object
        was actually created. It will be used by
        MulticlassClassifier
        """
        # tuple to guarantee immutability since we are asssigning
        # __tags__ below and rely on having clfs populated already
        self.__clfs = tuple(clfs) if clfs is not None else tuple()
        """Classifiers to use"""

        if len(clfs):
            # enable corresponding ca in the slave-classifiers
            if self.__propagate_ca:
                for clf in self.__clfs:
                    clf.ca.enable(self.ca.enabled, missingok=True)

        # adhere to their capabilities + 'multiclass'
        # XXX do intersection across all classifiers!
        # TODO: this seems to be wrong since it can be regression etc
        self.__tags__ = [ 'binary', 'multiclass', 'meta' ]
        if len(clfs)>0:
            self.__tags__ += self.__clfs[0].__tags__

    def _untrain(self):
        """Untrain `BoostedClassifier`

        Has to untrain any known classifier
        """
        if not self.trained:
            return
        for clf in self.clfs:
            clf.untrain()
        super(BoostedClassifier, self)._untrain()

    def get_sensitivity_analyzer(self, **kwargs):
        """Return an appropriate SensitivityAnalyzer"""
        return BoostedClassifierSensitivityAnalyzer(
                self,
                **kwargs)


    clfs = property(fget=lambda x:x.__clfs,
                    fset=_set_classifiers,
                    doc="Used classifiers")



class ProxyClassifier(Classifier):
    """Classifier which decorates another classifier

    Possible uses:

     - modify data somehow prior training/testing:
       * normalization
       * feature selection
       * modification

     - optimized classifier?

    """

    __sa_class__ = ProxyClassifierSensitivityAnalyzer
    """Sensitivity analyzer to use for a generic ProxyClassifier"""

    def __init__(self, clf, **kwargs):
        """Initialize the instance of ProxyClassifier

        Parameters
        ----------
        clf : Classifier
          Classifier to proxy, i.e. to use after decoration
        """

        # Is done before parents __init__ since we need
        # it for _set_retrainable called during __init__
        self.__clf = clf
        """Store the classifier to use."""

        Classifier.__init__(self, **kwargs)

        # adhere to slave classifier capabilities
        # TODO: unittest
        self.__tags__ = self.__tags__[:] + ['meta']
        if clf is not None:
            self.__tags__ += clf.__tags__


    def __repr__(self, prefixes=[]):
        return super(ProxyClassifier, self).__repr__(
            ["clf=%s" % repr(self.__clf)] + prefixes)

    def __str__(self, *args, **kwargs):
        return super(ProxyClassifier, self).__str__(
            str(self.__clf), *args, **kwargs)

    def summary(self):
        s = super(ProxyClassifier, self).summary()
        if self.trained:
            s += "\n Slave classifier summary:" + \
                 '\n + %s' % \
                 (self.__clf.summary().replace('\n', '\n |'))
        return s

    def _set_retrainable(self, value, force=False):
        # XXX Lazy implementation
        self.clf._set_retrainable(value, force=force)
        super(ProxyClassifier, self)._set_retrainable(value, force)
        if value and not (self.ca['retrained']
                          is self.clf.ca['retrained']):
            if __debug__:
                debug("CLFPRX",
                      "Rebinding conditional attributes from slave clf %s", (self.clf,))
            self.ca['retrained'] = self.clf.ca['retrained']
            self.ca['repredicted'] = self.clf.ca['repredicted']


    def _train(self, dataset):
        """Train `ProxyClassifier`
        """
        # base class does nothing much -- just proxies requests to underlying
        # classifier
        self.__clf.train(dataset)

        # for the ease of access
        # TODO: if to copy we should exclude some ca which are defined in
        #       base Classifier (such as training_time, predicting_time)
        # YOH: for now _copy_ca_ would copy only set ca variables. If
        #      anything needs to be overriden in the parent's class, it is
        #      welcome to do so
        #self.ca._copy_ca_(self.__clf, deep=False)


    def _predict(self, dataset):
        """Predict using `ProxyClassifier`
        """
        clf = self.__clf
        if self.ca.is_enabled('estimates'):
            clf.ca.enable(['estimates'])

        result = clf.predict(dataset)
        # for the ease of access
        self.ca._copy_ca_(self.__clf, ['estimates'], deep=False)
        return result


    def _untrain(self):
        """Untrain ProxyClassifier
        """
        if not self.__clf is None:
            self.__clf.untrain()
        super(ProxyClassifier, self)._untrain()


    @group_kwargs(prefixes=['slave_'], passthrough=True)
    def get_sensitivity_analyzer(self, slave_kwargs, **kwargs):
        """Return an appropriate SensitivityAnalyzer

        Parameters
        ----------
        slave_kwargs : dict
          Arguments to be passed to the proxied (slave) classifier
        **kwargs
          Specific additional arguments for the sensitivity analyzer
          for the class.  See documentation of a corresponding `.__sa_class__`.
        """
        return self.__sa_class__(
                self,
                analyzer=self.__clf.get_sensitivity_analyzer(**slave_kwargs),
                **kwargs)


    clf = property(lambda x:x.__clf, doc="Used `Classifier`")



#
# Various combiners for CombinedClassifier
#

class PredictionsCombiner(ClassWithCollections):
    """Base class for combining decisions of multiple classifiers"""

    def train(self, clfs, dataset):
        """PredictionsCombiner might need to be trained

        Parameters
        ----------
        clfs : list of Classifier
          List of classifiers to combine. Has to be classifiers (not
          pure predictions), since combiner might use some other
          conditional attributes (value's) instead of pure prediction's
        dataset : Dataset
          training data in this case
        """
        # TODO: implement stacking to help with resolving ties
        pass


    def __call__(self, clfs, dataset):
        """Call function

        Parameters
        ----------
        clfs : list of Classifier
          List of classifiers to combine. Has to be classifiers (not
          pure predictions), since combiner might use some other
          conditional attributes (value's) instead of pure prediction's
        """
        raise NotImplementedError



class MaximalVote(PredictionsCombiner):
    """Provides a decision using maximal vote rule"""

    predictions = ConditionalAttribute(enabled=True,
        doc="Voted predictions")
    estimates = ConditionalAttribute(enabled=False,
        doc="Estimates keep counts across classifiers for each label/sample")

    # TODO: Might get a parameter to use raw decision estimates if
    # voting is not unambigous (ie two classes have equal number of
    # votes

    def __init__(self, **kwargs):
        PredictionsCombiner.__init__(self, **kwargs)


    def __call__(self, clfs, dataset):
        """Actuall callable - perform voting

        Extended functionality which might not be needed actually:
        Since `BinaryClassifier` might return a list of possible
        predictions (not just a single one), we should consider all of those

        MaximalVote doesn't care about dataset itself
        """
        if len(clfs)==0:
            return []                   # to don't even bother

        all_label_counts = None
        for clf in clfs:
            # Lets check first if necessary conditional attribute is enabled
            if not clf.ca.is_enabled("predictions"):
                raise ValueError, "MaximalVote needs classifiers (such as " + \
                      "%s) with state 'predictions' enabled" % clf
            predictions = clf.ca.predictions
            if all_label_counts is None:
                all_label_counts = [ {} for i in xrange(len(predictions)) ]

            # for every sample
            for i in xrange(len(predictions)):
                prediction = predictions[i]
                # XXX fishy location due to literal labels,
                # TODO simplify assumptions and logic
                if isinstance(prediction, basestring) or \
                       not is_sequence_type(prediction):
                    prediction = (prediction,)
                for label in prediction: # for every label
                    # XXX we might have multiple labels assigned
                    # but might not -- don't remember now
                    if not all_label_counts[i].has_key(label):
                        all_label_counts[i][label] = 0
                    all_label_counts[i][label] += 1

        predictions = []
        # select maximal vote now for each sample
        for i in xrange(len(all_label_counts)):
            label_counts = all_label_counts[i]
            # lets do explicit search for max so we know
            # if it is unique
            maxk = []                   # labels of elements with max vote
            maxv = -1
            for k, v in label_counts.iteritems():
                if v > maxv:
                    maxk = [k]
                    maxv = v
                elif v == maxv:
                    maxk.append(k)

            assert len(maxk) >= 1, \
                   "We should have obtained at least a single key of max label"

            if len(maxk) > 1:
                warning("We got multiple labels %s which have the " % maxk +
                        "same maximal vote %d. XXX disambiguate. " % maxv +
                        "Meanwhile selecting the first in sorted order")
            predictions.append(sorted(maxk)[0])

        ca = self.ca
        ca.estimates = all_label_counts
        ca.predictions = predictions
        return predictions



class MeanPrediction(PredictionsCombiner):
    """Provides a decision by taking mean of the results
    """

    predictions = ConditionalAttribute(enabled=True,
        doc="Mean predictions")

    estimates = ConditionalAttribute(enabled=True,
        doc="Predictions from all classifiers are stored")

    def __call__(self, clfs, dataset):
        """Actual callable - perform meaning

        """
        if len(clfs)==0:
            return []                   # to don't even bother

        all_predictions = []
        for clf in clfs:
            # Lets check first if necessary conditional attribute is enabled
            if not clf.ca.is_enabled("predictions"):
                raise ValueError, "MeanPrediction needs learners (such " \
                      " as %s) with state 'predictions' enabled" % clf
            all_predictions.append(clf.ca.predictions)

        # compute mean
        all_predictions = np.asarray(all_predictions)
        predictions = np.mean(all_predictions, axis=0)

        ca = self.ca
        ca.estimates = all_predictions
        ca.predictions = predictions
        return predictions


class ClassifierCombiner(PredictionsCombiner):
    """Provides a decision using training a classifier on predictions/estimates

    TODO: implement
    """

    predictions = ConditionalAttribute(enabled=True,
        doc="Trained predictions")


    def __init__(self, clf, variables=None):
        """Initialize `ClassifierCombiner`

        Parameters
        ----------
        clf : Classifier
          Classifier to train on the predictions
        variables : list of str
          List of conditional attributes stored in 'combined' classifiers, which
          to use as features for training this classifier
        """
        PredictionsCombiner.__init__(self)

        self.__clf = clf
        """Classifier to train on `variables` ca of provided classifiers"""

        if variables == None:
            variables = ['predictions']
        self.__variables = variables
        """What conditional attributes of the classifiers to use"""


    def _untrain(self):
        """It might be needed to untrain used classifier"""
        if self.__clf:
            self.__clf._untrain()

    def __call__(self, clfs, dataset):
        """
        """
        if len(clfs)==0:
            return []                   # to don't even bother

        raise NotImplementedError



class CombinedClassifier(BoostedClassifier):
    """`BoostedClassifier` which combines predictions using some
    `PredictionsCombiner` functor.
    """

    def __init__(self, clfs=None, combiner='auto', **kwargs):
        """Initialize the instance.

        Parameters
        ----------
        clfs : list of Classifier
          list of classifier instances to use
        combiner : PredictionsCombiner, optional
          callable which takes care about combining multiple results into a single
          one. If default ('auto') chooses `MaximalVote` for classification and
          `MeanPrediction` for regression. If None is provided -- no combination is
          done
        kwargs : dict
          dict of keyworded arguments which might get used
          by State or Classifier

        NB: `combiner` might need to operate not on 'predictions' discrete
            labels but rather on raw 'class' estimates classifiers
            estimate (which is pretty much what is stored under
            `estimates`)
        """
        if clfs == None:
            clfs = []

        BoostedClassifier.__init__(self, clfs, **kwargs)

        self.__combiner = combiner
        """Input argument describing which "combiner" to use to combine results of multiple classifiers"""

        self._combiner = None
        """Actual combiner which would be decided upon later"""


    def __repr__(self, prefixes=[]):
        """Literal representation of `CombinedClassifier`.
        """
        return super(CombinedClassifier, self).__repr__(
            ["combiner=%s" % repr(self.__combiner)] + prefixes)

    @property
    def combiner(self):
        # Decide either we are dealing with regressions
        # by looking at 1st learner
        if self._combiner is None:
            if isinstance(self.__combiner, basestring) and self.__combiner == 'auto':
                self._combiner = (
                    MaximalVote,
                    MeanPrediction)[int(self.clfs[0].__is_regression__)]()
            else:
                self._combiner = self.__combiner
        return self._combiner

    def summary(self):
        """Provide summary for the `CombinedClassifier`.
        """
        s = super(CombinedClassifier, self).summary()
        if self.trained:
            s += "\n Slave classifiers summaries:"
            for i, clf in enumerate(self.clfs):
                s += '\n + %d clf: %s' % \
                     (i, clf.summary().replace('\n', '\n |'))
        return s


    def _untrain(self):
        """Untrain `CombinedClassifier`
        """
        try:
            self._combiner.untrain()
        except:
            pass
        finally:
            self._combiner = None
        super(CombinedClassifier, self)._untrain()


    def _train(self, dataset):
        """Train `CombinedClassifier`
        """
        BoostedClassifier._train(self, dataset)

        # combiner might need to be defined and trained as well at this point
        if self.combiner is not None:
            self.combiner.train(self.clfs, dataset)


    def _predict(self, dataset):
        """Predict using `CombinedClassifier`
        """
        ca = self.ca
        predictions = BoostedClassifier._predict(self, dataset)
        if self.combiner is not None:
            cca = self.combiner.ca
            if ca.is_enabled("estimates"):
                cca.enable('estimates')

            # combiner will make use of conditional attributes instead of only predictions
            # returned from _predict
            predictions = self.combiner(self.clfs, dataset)
        else:
            cca = None

        ca.predictions = predictions

        if ca.is_enabled("estimates") and cca is not None:
            if cca.is_active("estimates"):
                # XXX or may be we could leave simply up to accessing .combiner?
                ca.estimates = cca.estimates
            else:
                if __debug__:
                    warning("Boosted classifier %s has 'estimates' state enabled,"
                            " but combiner doesn't have 'estimates' active, thus "
                            " .estimates cannot be provided directly, access .clfs"
                            % self)
        return predictions


class TreeClassifier(ProxyClassifier):
    """`TreeClassifier` which allows to create hierarchy of classifiers

    Functions by grouping some labels into a single "meta-label" and training
    classifier first to separate between meta-labels.  Then
    each group further proceeds with classification within each group.

    Possible scenarios::

      TreeClassifier(SVM(),
       {'animate':  ((1,2,3,4),
                     TreeClassifier(SVM(),
                         {'human': (('male', 'female'), SVM()),
                          'animals': (('monkey', 'dog'), SMLR())})),
        'inanimate': ((5,6,7,8), SMLR())})

    would create classifier which would first do binary classification
    to separate animate from inanimate, then for animate result it
    would separate to classify human vs animal and so on::

                                   SVM
                                 /     \
                            animate  inanimate
                             /            \
                           SVM            SMLR
                         /     \         / | \ \
                    human    animal     5  6 7  8
                     |          |
                    SVM        SVM
                   /   \       /  \
                 male female monkey dog
                  1      2    3      4

    If it is desired to have a trailing node with a single label and
    thus without any classification, such as in

                       SVM
                      /   \
                     g1   g2
                     /     \
                    1     SVM
                          /  \
                         2    3

    then just specify None as the classifier to use::

        TreeClassifier(SVM(),
           {'g1':  ((1,), None),
            'g2':  ((1,2,3,4), SVM())})

    """

    _DEV__doc = """
    Questions:
     * how to collect confusion matrices at a particular layer if such
       classifier is given to SplitClassifier or CVTE

     * What additional ca to add, something like
        clf_labels  -- store remapped labels for the dataset
        clf_estimates  ...

     * What do we store into estimates ? just estimates from the clfs[]
       for corresponding samples, or top level clf estimates as well?

     * what should be SensitivityAnalyzer?  by default it would just
       use top slave classifier (i.e. animate/inanimate)

    Problems?
     *  .clf is not actually "proxied" per se, so not sure what things
        should be taken care of yet...

    TODO:
     * Allow a group to be just a single category, so no further
        classifier is needed, it just should stay separate from the
        other groups

    Possible TODO:
     *  Add ability to provide results of clf.estimates as features into
        input of clfs[]. This way we could provide additional 'similarity'
        information to the "other" branch

    """

    def __init__(self, clf, groups, **kwargs):
        """Initialize TreeClassifier

        Parameters
        ----------
        clf : Classifier
          Classifier to separate between the groups
        groups : dict of meta-label: tuple of (tuple of labels, classifier)
          Defines the groups of labels and their classifiers.
          See :class:`~mvpa2.clfs.meta.TreeClassifier` for example
        """

        # Basic initialization
        ProxyClassifier.__init__(self, clf, **kwargs)

        # XXX RF: probably create internal structure with dictionary,
        # not just a tuple, and store all information in there
        # accordingly

        self._groups = groups
        self._index2group = groups.keys()

        # All processing of groups needs to be handled within _train
        # since labels_map is not available here and definition
        # is allowed to carry both symbolic and numeric values for
        # labels
        # XXX TODO due to abandoning of labels_map -- may be this is
        #     no longer the case?

        # We can only assign respective classifiers
        self.clfs = dict([(gk, c) for gk, (ls, c) in groups.iteritems()])
        """Dictionary of classifiers used by the groups"""


    def __repr__(self, prefixes=[]):
        """String representation of TreeClassifier
        """
        prefix = "groups=%s" % repr(self._groups)
        return super(TreeClassifier, self).__repr__([prefix] + prefixes)

    def __str__(self, *args, **kwargs):
        return super(TreeClassifier, self).__str__(
            ', '.join(['%s: %s' % i for i in self.clfs.iteritems()]),
            *args, **kwargs)

    def summary(self):
        """Provide summary for the `TreeClassifier`.
        """
        s = super(TreeClassifier, self).summary()
        if self.trained:
            s += "\n Node classifiers summaries:"
            for i, (clfname, clf) in enumerate(self.clfs.iteritems()):
                s += '\n + %d %s clf: %s' % \
                     (i, clfname, clf.summary().replace('\n', '\n |'))
        return s


    def _train(self, dataset):
        """Train TreeClassifier

        First train .clf on groupped samples, then train each of .clfs
        on a corresponding subset of samples.
        """
        # Local bindings
        targets_sa_name = self.get_space()    # name of targets sa
        targets_sa = dataset.sa[targets_sa_name] # actual targets sa
        clf, clfs, index2group = self.clf, self.clfs, self._index2group

        # Handle groups of labels
        groups = self._groups
        groups_labels = {}              # just groups with numeric indexes
        label2index = {}                # how to map old labels to new
        known = set()
        for gi, gk in enumerate(index2group):
            ls = groups[gk][0]
            known_already = known.intersection(ls)
            if len(known_already):
                raise ValueError, "Grouping of labels is not appropriate. " \
                      "Got labels %s already among known in %s. " % \
                       (known_already, known  )
            groups_labels[gk] = ls      # needed? XXX
            for l in ls :
                label2index[l] = gi
            known = known.union(ls )
        # TODO: check if different literal labels weren't mapped into
        #       same numerical but here asked to belong to different groups
        #  yoh: actually above should catch it

        # Check if none of the labels is missing from known groups
        dsul = set(targets_sa.unique)
        if known.intersection(dsul) != dsul:
            raise ValueError, \
                  "Dataset %s had some labels not defined in groups: %s. " \
                  "Known are %s" % \
                  (dataset, dsul.difference(known), known)

        # We can operate on the same dataset here 
        # Nope: doesn't work nicely with the classifier like kNN
        #      which links to the dataset used in the training,
        #      so whenever if we simply restore labels back, we
        #      would get kNN confused in _predict()
        #      Therefore we need to create a shallow copy of
        #      dataset and provide it with new labels
        ds_group = dataset.copy(deep=False)
        # assign new labels group samples into groups of labels
        ds_group.sa[targets_sa_name].value = [label2index[l]
                                              for l in targets_sa.value]

        # train primary classifier
        if __debug__:
            debug('CLFTREE', "Training primary %s on %s with targets %s",
                  (clf, ds_group, ds_group.sa[targets_sa_name].unique))
        clf.train(ds_group)

        # ??? should we obtain values for anything?
        #     may be we could training values of .clfs to be added
        #     as features to the next level -- i.e. .clfs

        # Proceed with next 'layer' and train all .clfs on corresponding
        # selection of samples
        # ??? should we may be allow additional 'the other' category, to
        #     signal contain all the other categories data? probably not
        #     since then it would lead to undetermined prediction (which
        #     might be not a bad thing altogether...)
        for gk in groups.iterkeys():
            clf = clfs[gk]
            group_labels = groups_labels[gk]
            if clf is None: # Trailing node
                if len(group_labels) != 1:
                    raise ValueError(
                        "Trailing nodes with no classifier assigned must have "
                        "only a single label associated. Got %s defined in "
                        "group %r of %s"
                        % (group_labels, gk, self))
            else:
                # select samples per each group
                ids = get_samples_by_attr(dataset, targets_sa_name, groups_labels[gk])
                ds_group = dataset[ids]
                if __debug__:
                    debug('CLFTREE', "Training %s for group %s on %s",
                          (clfs[gk], gk, ds_group))
                # and train corresponding slave clf
                clf.train(ds_group)


    def _untrain(self):
        """Untrain TreeClassifier
        """
        super(TreeClassifier, self)._untrain()
        for clf in self.clfs.values():
            if clf is not None:
                clf.untrain()


    def _predict(self, dataset):
        """
        """
        # Local bindings
        clfs, index2group, groups = self.clfs, self._index2group, self._groups
        clf_predictions = np.asanyarray(ProxyClassifier._predict(self, dataset))
        if __debug__:
                debug('CLFTREE',
                      'Predictions %s',
                      (clf_predictions))
        # assure that predictions are indexes, ie int
        clf_predictions = clf_predictions.astype(int)

        # now for predictions pointing to specific groups go into
        # corresponding one
        # defer initialization since dtype would depend on predictions
        predictions = None
        for pred_group in set(clf_predictions):
            gk = index2group[pred_group]
            clf_ = clfs[gk]
            group_indexes = (clf_predictions == pred_group)
            if __debug__:
                debug('CLFTREE',
                      'Predicting for group %s using %s on %d samples',
                      (gk, clf_, np.sum(group_indexes)))
            if clf_ is None:
                p = groups[gk][0] # our only label
            else:
                p = clf_.predict(dataset[group_indexes])

            if predictions is None:
                predictions = np.zeros((len(dataset),),
                                       dtype=np.asanyarray(p).dtype)
            predictions[group_indexes] = p
        return predictions


class BinaryClassifier(ProxyClassifier):
    """`ProxyClassifier` which maps set of two labels into +1 and -1
    """

    __sa_class__ = BinaryClassifierSensitivityAnalyzer

    def __init__(self, clf, poslabels, neglabels, **kwargs):
        """
        Parameters
        ----------
        clf : Classifier
          classifier to use
        poslabels : list
          list of labels which are treated as +1 category
        neglabels : list
          list of labels which are treated as -1 category
        """

        ProxyClassifier.__init__(self, clf, **kwargs)

        # Handle labels

        # TODO: move to use AttributeMap
        #self._attrmap = AttributeMap(dict([(l, -1) for l in sneglabels] +
        #                                  [(l, +1) for l in sposlabels]))

        # check if there is no overlap
        overlap = set(poslabels).intersection(neglabels)
        if len(overlap)>0:
            raise ValueError("Sets of positive and negative labels for " +
                "BinaryClassifier must not overlap. Got overlap " %
                overlap)

        self.__poslabels = poslabels
        self.__neglabels = neglabels

        # define what values will be returned by predict: if there is
        # a single label - return just it alone, otherwise - whole
        # list
        # Such approach might come useful if we use some classifiers
        # over different subsets of data with some voting later on
        # (1-vs-therest?)

        if len(self.__poslabels) > 1:
            self.__predictpos = self.__poslabels
        else:
            self.__predictpos = self.__poslabels[0]

        if len(self.__neglabels) > 1:
            self.__predictneg = self.__neglabels
        else:
            self.__predictneg = self.__neglabels[0]

    @property
    def poslabels(self):
        return self.__poslabels

    @property
    def neglabels(self):
        return self.__neglabels

    def __repr__(self, prefixes=[]):
        prefix = "poslabels=%s, neglabels=%s" % (
            repr(self.__poslabels), repr(self.__neglabels))
        return super(BinaryClassifier, self).__repr__([prefix] + prefixes)


    def _train(self, dataset):
        """Train `BinaryClassifier`
        """
        targets_sa_name = self.get_space()
        idlabels = [(x, +1) for x in get_samples_by_attr(dataset, targets_sa_name,
                                                         self.__poslabels)] + \
                    [(x, -1) for x in get_samples_by_attr(dataset, targets_sa_name,
                                                          self.__neglabels)]
        # XXX we have to sort ids since at the moment Dataset.select_samples
        #     doesn't take care about order
        idlabels.sort()

        # If we need all samples, why simply not perform on original
        # data, an just store/restore labels. But it really should be done
        # within Dataset.select_samples
        if len(idlabels) == dataset.nsamples \
            and [x[0] for x in idlabels] == range(dataset.nsamples):
            # the last condition is not even necessary... just overly
            # cautious
            datasetselected = dataset.copy(deep=False)   # no selection is needed
            if __debug__:
                debug('CLFBIN',
                      "Created shallow copy with %d samples for binary "
                      "classification among labels %s/+1 and %s/-1",
                      (dataset.nsamples, self.__poslabels, self.__neglabels))
        else:
            datasetselected = dataset[[ x[0] for x in idlabels ]]
            if __debug__:
                debug('CLFBIN',
                      "Selected %d samples out of %d samples for binary "
                      "classification among labels %s/+1 and %s/-1. Selected %s",
                      (len(idlabels), dataset.nsamples,
                       self.__poslabels, self.__neglabels, datasetselected))

        # adjust the labels
        datasetselected.sa[targets_sa_name].value = [ x[1] for x in idlabels ]

        # now we got a dataset with only 2 labels
        if __debug__:
            assert(set(datasetselected.sa[targets_sa_name].unique) ==
                   set([-1, 1]))

        self.clf.train(datasetselected)


    def _predict(self, dataset):
        """Predict the labels for a given `dataset`

        Predicts using binary classifier and spits out list (for each sample)
        where with either poslabels or neglabels as the "label" for the sample.
        If there was just a single label within pos or neg labels then it would
        return not a list but just that single label.
        """
        binary_predictions = ProxyClassifier._predict(self, dataset)
        self.ca.estimates = binary_predictions
        predictions = [ {-1: self.__predictneg,
                         +1: self.__predictpos}[x] for x in binary_predictions]
        self.ca.predictions = predictions
        return predictions



class MulticlassClassifier(CombinedClassifier):
    """Perform multiclass classification using a list of binary classifiers.

    Based on a `CombinedClassifier` for which it constructs a list of
    binary 1-vs-1 (ie in pairs like LIBSVM does) or 1-vs-all (which is
    yet to think about) classifiers.
    """

    raw_predictions_ds = ConditionalAttribute(enabled=False,
        doc="Wraps raw_predictions into a Dataset with .fa.(neg,pos) "
        "describing actual labels used in each binary classification task "
        "and samples containing actual decision labels per each input "
        "sample")

    def __init__(self, clf, bclf_type="1-vs-1", **kwargs):
        """Initialize the instance

        Parameters
        ----------
        clf : Classifier
          classifier based on which multiple classifiers are created
          for multiclass
        bclf_type
          "1-vs-1" or "1-vs-all", determines the way to generate binary
          classifiers
        """
        CombinedClassifier.__init__(self, **kwargs)

        self.__clf = clf
        """Store sample instance of basic classifier"""

        # adhere to slave classifier capabilities
        if clf is not None:
            self.__tags__ += clf.__tags__
        if not 'multiclass' in self.__tags__:
            self.__tags__ += ['multiclass']

        # Some checks on known ways to do multiclass
        if bclf_type == "1-vs-1":
            pass
        elif bclf_type == "1-vs-all": # TODO
            raise NotImplementedError
        else:
            raise ValueError(
                  "Unknown type of classifier %s for " % bclf_type +
                  "MulticlassClassifier")
        self.__bclf_type = bclf_type

    # XXX fix it up a bit... it seems that MulticlassClassifier should
    # be actually ProxyClassifier and use BoostedClassifier internally
    def __repr__(self, prefixes=[]):
        prefix = "bclf_type=%s, clf=%s" % (repr(self.__bclf_type),
                                            repr(self.__clf))
        return super(MulticlassClassifier, self).__repr__([prefix] + prefixes)

    def _get_binary_pairs(self, dataset):
        """Return a list of pairs of categories lists to be used in binary classification
        """
        targets_sa_name = self.get_space()

        # construct binary classifiers
        ulabels = dataset.sa[targets_sa_name].unique

        if self.__bclf_type == "1-vs-1":
            # generate pairs and corresponding classifiers
            # could use _product but let's stay inline with previuos
            # implementation
            label_pairs = [([ulabels[i]], [ulabels[j]])
                           for i in xrange(len(ulabels))
                           for j in xrange(i+1, len(ulabels))]
            if __debug__:
                debug("CLFMC", "Created %d label pairs for original %d labels",
                      (len(label_pairs), len(ulabels)))
        elif self.__bclf_type == "1-vs-all":
            raise NotImplementedError

        return label_pairs

    def _train(self, dataset):
        """Train classifier
        """
        # construct binary classifiers
        biclfs = []
        for poslabels, neglabels in self._get_binary_pairs(dataset):
            biclfs.append(
                BinaryClassifier(self.__clf.clone(),
                                 poslabels=poslabels,
                                 neglabels=neglabels))
        self.clfs = biclfs                # need to be set after, not operated in-place
        # perform actual training
        CombinedClassifier._train(self, dataset)

    def _predict(self, dataset):
        ca = self.ca
        if ca.is_enabled("raw_predictions_ds"):
            ca.enable("raw_predictions")

        predictions = super(MulticlassClassifier, self)._predict(dataset)

        if ca.is_enabled("raw_predictions_ds") or self.combiner is None:
            if self.combiner is None:
                raw_predictions = predictions
            else:
                # we should fetch those from ca
                raw_predictions = ca.raw_predictions

            # assign pos and neg to fa while squeezing out
            # degenerate dimensions which are there to possibly accomodate
            # 1-vs-all cases

            # for consistency -- place into object array of tuples
            # (Sensitivity analyzers already do the same)
            pairs = zip(np.array([np.squeeze(clf.neglabels) for clf in self.clfs]).tolist(),
                        np.array([np.squeeze(clf.poslabels) for clf in self.clfs]).tolist())
            ca.raw_predictions_ds = raw_predictions_ds = \
                Dataset(np.array(raw_predictions).T, fa={self.space: asobjarray(pairs)})
        if self.combiner is None:
            return raw_predictions_ds
        else:
            return predictions


class SplitClassifier(CombinedClassifier):
    """`BoostedClassifier` to work on splits of the data

    """

    """
    TODO: SplitClassifier and MulticlassClassifier have too much in
          common -- need to refactor: just need a splitter which would
          split dataset in pairs of class labels. MulticlassClassifier
          does just a tiny bit more which might be not necessary at
          all: map sets of labels into 2 categories...
    """

    stats = ConditionalAttribute(enabled=False,
        doc="Resultant confusion whenever classifier trained " +
            "on 1 part and tested on 2nd part of each split")

    splits = ConditionalAttribute(enabled=False, doc=
       """Store the actual splits of the data. Can be memory expensive""")

    # ??? couldn't be training_stats since it has other meaning
    #     here, BUT it is named so within CrossValidatedTransferError
    #     -- unify
    #  decided to go with overriding semantics tiny bit. For split
    #     classifier training_stats would correspond to summary
    #     over training errors across all splits. Later on if need comes
    #     we might want to implement global_training_stats which would
    #     correspond to overall confusion on full training dataset as it is
    #     done in base Classifier
    #global_training_stats = ConditionalAttribute(enabled=False,
    #    doc="Summary over training confusions acquired at each split")

    def __init__(self, clf, partitioner=NFoldPartitioner(),
                 splitter=Splitter('partitions', count=2), **kwargs):
        """Initialize the instance

        Parameters
        ----------
        clf : Classifier
          classifier based on which multiple classifiers are created
          for multiclass
        splitter : Splitter
          `Splitter` to use to split the dataset prior training
        """

        CombinedClassifier.__init__(self, **kwargs)
        self.__clf = clf
        """Store sample instance of basic classifier"""

        if isinstance(splitter, type):
            raise ValueError, \
                  "Please provide an instance of a splitter, not a type." \
                  " Got %s" % splitter

        self.__partitioner = partitioner
        self.__splitter = splitter


    def _train(self, dataset):
        """Train `SplitClassifier`
        """
        targets_sa_name = self.get_space()

        # generate pairs and corresponding classifiers
        bclfs = []

        # local binding
        ca = self.ca

        clf_template = self.__clf
        if ca.is_enabled('stats'):
            ca.stats = clf_template.__summary_class__()
        if ca.is_enabled('training_stats'):
            clf_template.ca.enable(['training_stats'])
            ca.training_stats = clf_template.__summary_class__()

        clf_hastestdataset = hasattr(clf_template, 'testdataset')

        self.ca.splits = []

        for i, pset in enumerate(self.__partitioner.generate(dataset)):
            if __debug__:
                debug("CLFSPL_", "Deepcopying %s for %s",
                      (clf_template, self))
            clf = clf_template.clone()
            bclfs.append(clf)

            if __debug__:
                debug("CLFSPL", "Training classifier for split %d", (i,))

            # split partitioned dataset
            split = [d for d in self.__splitter.generate(pset)]

            if ca.is_enabled("splits"):
                self.ca.splits.append(split)

            clf = bclfs[i]

            # assign testing dataset if given classifier can digest it
            if clf_hastestdataset:
                clf.testdataset = split[1]

            clf.train(split[0])

            # unbind the testdataset from the classifier
            if clf_hastestdataset:
                clf.testdataset = None

            if ca.is_enabled("stats"):
                predictions = clf.predict(split[1])
                self.ca.stats.add(split[1].sa[targets_sa_name].value,
                                          predictions,
                                          clf.ca.get('estimates', None))
                if __debug__:
                    dact = debug.active
                    if 'CLFSPL_' in dact:
                        debug('CLFSPL_', 'Split %d:\n%s',
                              (i, self.ca.stats))
                    elif 'CLFSPL' in dact:
                        debug('CLFSPL', 'Split %d error %.2f%%',
                              (i, self.ca.stats.summaries[-1].error))

            if ca.is_enabled("training_stats"):
                # XXX this is broken, as it cannot deal with not yet set ca
                ca.training_stats += clf.ca.training_stats
        # need to be assigned after the entire list populated since
        # _set_classifiers places them into a tuple
        self.clfs = bclfs


    @group_kwargs(prefixes=['slave_'], passthrough=True)
    def get_sensitivity_analyzer(self, slave_kwargs={}, **kwargs):
        """Return an appropriate SensitivityAnalyzer for `SplitClassifier`

        Parameters
        ----------
        combiner
          If not provided, `first_axis_mean` is assumed
        """
        return BoostedClassifierSensitivityAnalyzer(
                self, sa_attr='splits',
                analyzer=self.__clf.get_sensitivity_analyzer(
                    **_dont_force_slaves(slave_kwargs)),
                **kwargs)

    partitioner = property(fget=lambda x:x.__partitioner,
                        doc="Partitioner used by SplitClassifier")
    splitter = property(fget=lambda x:x.__splitter,
                        doc="Splitter used by SplitClassifier")


class MappedClassifier(ProxyClassifier):
    """`ProxyClassifier` which uses some mapper prior training/testing.

    `MaskMapper` can be used just a subset of features to
    train/classify.
    Having such classifier we can easily create a set of classifiers
    for BoostedClassifier, where each classifier operates on some set
    of features, e.g. set of best spheres from SearchLight, set of
    ROIs selected elsewhere. It would be different from simply
    applying whole mask over the dataset, since here initial decision
    is made by each classifier and then later on they vote for the
    final decision across the set of classifiers.
    """

    __sa_class__ = MappedClassifierSensitivityAnalyzer

    def __init__(self, clf, mapper, **kwargs):
        """Initialize the instance

        Parameters
        ----------
        clf : Classifier
          classifier based on which mask classifiers is created
        mapper
          whatever `Mapper` comes handy
        """
        ProxyClassifier.__init__(self, clf, **kwargs)

        self.__mapper = mapper
        """mapper to help us our with prepping data to
        training/classification"""


    def _train(self, dataset):
        """Train `MappedClassifier`
        """
        # first train the mapper
        # XXX: should training be done using whole dataset or just samples
        # YYY: in some cases labels might be needed, thus better full dataset
        self.__mapper.train(dataset)

        # for train() we have to provide dataset -- not just samples to train!
        wdataset = dataset.get_mapped(self.__mapper)
        if __debug__:
            debug('CLF', "Training %s having mapped dataset into %s",
                  (self, wdataset))
        ProxyClassifier._train(self, wdataset)


    def _untrain(self):
        """Untrain `FeatureSelectionClassifier`

        Has to untrain any known classifier
        """
        # untrain the mapper
        if self.__mapper is not None:
            self.__mapper.untrain()
        # let base class untrain as well
        super(MappedClassifier, self)._untrain()


    def _predict(self, dataset):
        """Predict using `MappedClassifier`
        """
        return ProxyClassifier._predict(self, self.__mapper.forward(dataset))


    def __repr__(self, prefixes=[]):
        return super(MappedClassifier, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['mapper']))

    def __str__(self, *args, **kwargs):
        return super(MappedClassifier, self).__str__(
              str(self.mapper), *args, **kwargs)

    mapper = property(lambda x:x.__mapper, doc="Used mapper")



class FeatureSelectionClassifier(MappedClassifier):
    """This is nothing but a `MappedClassifier`.

    This class is only kept for (temporary) compatibility with old code.
    """
    __tags__ = [ 'does_feature_selection', 'meta' ]


class RegressionAsClassifier(ProxyClassifier):
    """Allows to use arbitrary regression for classification.

    Possible usecases:

     Binary Classification
      Any regression could easily be extended for binary
      classification. For instance using labels -1 and +1, regression
      results are quantized into labels depending on their signs
     Multiclass Classification
      Although most of the time classes are not ordered and do not
      have a corresponding distance matrix among them it might often
      be the case that there is a hypothesis that classes could be
      well separated in a projection to single dimension (non-linear
      manifold, or just linear projection).  For such use regression
      might provide necessary means of classification
    """

    distances = ConditionalAttribute(enabled=False,
        doc="Distances obtained during prediction")

    __sa_class__ = RegressionAsClassifierSensitivityAnalyzer

    def __init__(self, clf, centroids=None, distance_measure=None, **kwargs):
        """
        Parameters
        ----------
        clf : Classifier XXX Should become learner
          Regression to be used as a classifier.  Although it would
          accept any Learner, only providing regressions would make
          sense.
        centroids : None or dict of (float or iterable)
          Hypothesis or prior information on location/distance of
          centroids for each category, provide them.  If None -- during
          training it will choose equidistant points starting from 0.0.
          If dict -- keys should be a superset of labels of dataset
          obtained during training and each value should be numeric value
          or iterable if centroids are multidimensional and regression
          can do multidimensional regression.
        distance_measure : function or None
          What distance measure to use to find closest class label
          from continuous estimates provided by regression.  If None,
          will use Cartesian distance.
        """
        ProxyClassifier.__init__(self, clf, **kwargs)
        self.centroids = centroids
        self.distance_measure = distance_measure

        # Adjust tags which were copied from slave learner
        if self.__is_regression__:
            self.__tags__.pop(self.__tags__.index('regression'))

        # We can do any number of classes, although in most of the scenarios
        # multiclass performance would suck, unless there is a strong
        # hypothesis
        self.__tags__ += ['binary', 'multiclass', 'regression_based']

        # XXX No support for retrainable in RegressionAsClassifier yet
        if 'retrainable' in self.__tags__:
            self.__tags__.remove('retrainable')

        # Pylint/user friendliness
        #self._trained_ul = None
        self._trained_attrmap = None
        self._trained_centers = None


    def __repr__(self, prefixes=[]):
        if self.centroids is not None:
            prefixes = prefixes + ['centroids=%r'
                                   % self.centroids]
        if self.distance_measure is not None:
            prefixes = prefixes + ['distance_measure=%r'
                                   % self.distance_measure]
        return super(RegressionAsClassifier, self).__repr__(prefixes)


    def _train(self, dataset):
        targets_sa_name = self.get_space()
        targets_sa = dataset.sa[targets_sa_name]

        # May be it is an advanced one needing training.
        if hasattr(self.distance_measure, 'train'):
            self.distance_measure.train(dataset)

        # Centroids
        ul = dataset.sa[targets_sa_name].unique
        if self.centroids is None:
            # setup centroids -- equidistant points
            # XXX we might preferred -1/+1 for binary...
            centers = np.arange(len(ul), dtype=float)
        else:
            # verify centroids and assign
            if not set(self.centroids.keys()).issuperset(ul):
                raise ValueError, \
                      "Provided centroids with keys %s do not cover all " \
                      "labels provided during training: %s" \
                      % (self.centroids.keys(), ul)
            # override with superset
            ul = self.centroids.keys()
            centers = np.array([self.centroids[k] for k in ul])

        #self._trained_ul = ul
        # Map labels into indexes (not centers)
        # since later on we would need to get back (see ??? below)
        self._trained_attrmap = AttributeMap(
            map=dict([(l, i) for i,l in enumerate(ul)]),
            mapnumeric=True)
        self._trained_centers = centers

        # Create a shallow copy of dataset, and override labels
        # TODO: we could just bind .a, .fa, and copy only .sa
        dataset_relabeled = dataset.copy(deep=False)
        # ???:  may be we could just craft a monster attrmap
        #       which does min distance search upon to_literal ?
        dataset_relabeled.sa[targets_sa_name].value = \
            self._trained_attrmap.to_numeric(targets_sa.value)

        ProxyClassifier._train(self, dataset_relabeled)


    def _predict(self, dataset):
        # TODO: Probably we should forwardmap labels for target
        #       dataset so slave has proper statistics attached
        self.ca.estimates = regr_predictions \
                           = ProxyClassifier._predict(self, dataset)

        # Local bindings
        #ul = self._trained_ul
        attrmap = self._trained_attrmap
        centers = self._trained_centers
        distance_measure = self.distance_measure
        if distance_measure is None:
            distance_measure = cartesian_distance

        # Compute distances
        self.ca.distances = distances \
            = np.array([[distance_measure(s, c) for c in centers]
                       for s in regr_predictions])

        predictions = attrmap.to_literal(np.argmin(distances, axis=1))
        if __debug__:
            debug("CLF_", "Converted regression distances %s "
                  "into labels %s for %s", (distances, predictions, self))

        return predictions


    def _set_retrainable(self, value, **kwargs):
        if value:
            raise NotImplementedError, \
                  "RegressionAsClassifier wrappers are not yet retrainable"
        ProxyClassifier._set_retrainable(self, value, **kwargs)

########NEW FILE########
__FILENAME__ = model_selector
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com>
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Model selction."""

__docformat__ = 'restructuredtext'


import numpy as np
from mvpa2.base import externals
from mvpa2.misc.exceptions import InvalidHyperparameterError

if externals.exists("scipy", raise_=True):
    import scipy.linalg as SL

# no sense to import this module if openopt is not available
if externals.exists("openopt", raise_=True):
    try:
        from openopt import NLP
    except ImportError:
        from scikits.openopt import NLP

if __debug__:
    from mvpa2.base import debug

def _openopt_debug():
    # shut up or make verbose OpenOpt
    # (-1 = no logs, 0 = small log, 1 = verbose)
    if __debug__:
        da = debug.active
        if 'OPENOPT' in da:
            return 1
        elif 'MOD_SEL' in da:
            return 0
    return -1


class ModelSelector(object):
    """Model selection facility.

    Select a model among multiple models (i.e., a parametric model,
    parametrized by a set of hyperparamenters).
    """

    def __init__(self, parametric_model, dataset):
        """TODO:
        """
        self.parametric_model = parametric_model
        self.dataset = dataset
        self.hyperparameters_best = None
        self.log_marginal_likelihood_best = None
        self.problem = None
        pass

    def max_log_marginal_likelihood(self, hyp_initial_guess, maxiter=1,
            optimization_algorithm="scipy_cg", ftol=1.0e-3, fixedHypers=None,
            use_gradient=False, logscale=False):
        """
        Set up the optimization problem in order to maximize
        the log_marginal_likelihood.

        Parameters
        ----------
        parametric_model : Classifier
          the actual parameteric model to be optimized.
        hyp_initial_guess : numpy.ndarray
          set of hyperparameters' initial values where to start
          optimization.
        optimization_algorithm : string
          actual name of the optimization algorithm. See
          http://scipy.org/scipy/scikits/wiki/NLP
          for a comprehensive/updated list of available NLP solvers.
          (Defaults to 'ralg')
        ftol : float
          threshold for the stopping criterion of the solver,
          which is mapped in OpenOpt NLP.ftol
          (Defaults to 1.0e-3)
        fixedHypers : numpy.ndarray (boolean array)
          boolean vector of the same size of hyp_initial_guess;
          'False' means that the corresponding hyperparameter must
          be kept fixed (so not optimized).
          (Defaults to None, which during means all True)

        Notes
        -----
        The maximization of log_marginal_likelihood is a non-linear
        optimization problem (NLP). This fact is confirmed by Dmitrey,
        author of OpenOpt.
        """
        self.problem = None
        self.use_gradient = use_gradient
        self.logscale = logscale # use log-scale on hyperparameters to enhance numerical stability
        self.optimization_algorithm = optimization_algorithm
        self.hyp_initial_guess = np.array(hyp_initial_guess)
        self.hyp_initial_guess_log = np.log(self.hyp_initial_guess)
        if fixedHypers is None:
            fixedHypers = np.zeros(self.hyp_initial_guess.shape[0],dtype=bool)
            pass
        self.freeHypers = -fixedHypers
        if self.logscale:
            self.hyp_running_guess = self.hyp_initial_guess_log.copy()
        else:
            self.hyp_running_guess = self.hyp_initial_guess.copy()
            pass
        self.f_last_x = None

        def f(x):
            """
            Wrapper to the log_marginal_likelihood to be
            maximized.
            """
            # XXX EO: since some OpenOpt NLP solvers does not
            # implement lower bounds the hyperparameters bounds are
            # implemented inside PyMVPA: (see dmitrey's post on
            # [SciPy-user] 20080628).
            #
            # XXX EO: OpenOpt does not implement optimization of a
            # subset of the hyperparameters so it is implemented here.
            #
            # XXX EO: OpenOpt does not implement logrithmic scale of
            # the hyperparameters (to enhance numerical stability), so
            # it is implemented here.
            self.f_last_x = x.copy()
            self.hyp_running_guess[self.freeHypers] = x
            # REMOVE print "guess:",self.hyp_running_guess,x
            try:
                if self.logscale:
                    self.parametric_model.set_hyperparameters(np.exp(self.hyp_running_guess))
                else:
                    self.parametric_model.set_hyperparameters(self.hyp_running_guess)
                    pass
            except InvalidHyperparameterError:
                if __debug__: debug("MOD_SEL","WARNING: invalid hyperparameters!")
                return -np.inf
            try:
                self.parametric_model.train(self.dataset)
            except (np.linalg.linalg.LinAlgError, SL.basic.LinAlgError, ValueError):
                # Note that ValueError could be raised when Cholesky gets Inf or Nan.
                if __debug__: debug("MOD_SEL", "WARNING: Cholesky failed! Invalid hyperparameters!")
                return -np.inf
            log_marginal_likelihood = self.parametric_model.compute_log_marginal_likelihood()
            # REMOVE print log_marginal_likelihood
            return log_marginal_likelihood

        def df(x):
            """
            Proxy to the log_marginal_likelihood first
            derivative. Necessary for OpenOpt when using derivatives.
            """
            self.hyp_running_guess[self.freeHypers] = x
            # REMOVE print "df guess:",self.hyp_running_guess,x
            # XXX EO: Most of the following lines can be skipped if
            # df() is computed just after f() with the same
            # hyperparameters. The partial results obtained during f()
            # are what is needed for df(). For now, in order to avoid
            # bugs difficult to trace, we keep this redunundancy. A
            # deep check with how OpenOpt works or using memoization
            # should solve this issue.
            try:
                if self.logscale:
                    self.parametric_model.set_hyperparameters(np.exp(self.hyp_running_guess))
                else:
                    self.parametric_model.set_hyperparameters(self.hyp_running_guess)
                    pass
            except InvalidHyperparameterError:
                if __debug__: debug("MOD_SEL", "WARNING: invalid hyperparameters!")
                return -np.inf
            # Check if it is possible to avoid useless computations
            # already done in f(). According to tests and information
            # collected from OpenOpt people, it is sufficiently
            # unexpected that the following test succeed:
            if np.any(x!=self.f_last_x):
                if __debug__: debug("MOD_SEL","UNEXPECTED: recomputing train+log_marginal_likelihood.")
                try:
                    self.parametric_model.train(self.dataset)
                except (np.linalg.linalg.LinAlgError, SL.basic.LinAlgError, ValueError):
                    if __debug__: debug("MOD_SEL", "WARNING: Cholesky failed! Invalid hyperparameters!")
                    # XXX EO: which value for the gradient to return to
                    # OpenOpt when hyperparameters are wrong?
                    return np.zeros(x.size)
                log_marginal_likelihood = self.parametric_model.compute_log_marginal_likelihood() # recompute what's needed (to be safe) REMOVE IN FUTURE!
                pass
            if self.logscale:
                gradient_log_marginal_likelihood = self.parametric_model.compute_gradient_log_marginal_likelihood_logscale()
            else:
                gradient_log_marginal_likelihood = self.parametric_model.compute_gradient_log_marginal_likelihood()
                pass
            # REMOVE print "grad:",gradient_log_marginal_likelihood
            return gradient_log_marginal_likelihood[self.freeHypers]


        if self.logscale:
            # vector of hyperparameters' values where to start the search
            x0 = self.hyp_initial_guess_log[self.freeHypers]
        else:
            x0 = self.hyp_initial_guess[self.freeHypers]
            pass
        self.contol = 1.0e-20 # Constraint tolerance level
        # XXX EO: is it necessary to use contol when self.logscale is
        # True and there is no lb? Ask dmitrey.
        if self.use_gradient:
            # actual instance of the OpenOpt non-linear problem
            self.problem = NLP(f, x0, df=df, contol=self.contol, goal='maximum')
        else:
            self.problem = NLP(f, x0, contol=self.contol, goal='maximum')
            pass
        self.problem.name = "Max LogMargLikelihood"
        if not self.logscale:
             # set lower bound for hyperparameters: avoid negative
             # hyperparameters. Note: problem.n is the size of
             # hyperparameters' vector
            self.problem.lb = np.zeros(self.problem.n)+self.contol
            pass
        # max number of iterations for the optimizer.
        self.problem.maxiter = maxiter
        # check whether the derivative of log_marginal_likelihood converged to
        # zero before ending optimization
        self.problem.checkdf = True
         # set increment of log_marginal_likelihood under which the optimizer stops
        self.problem.ftol = ftol
        self.problem.iprint = _openopt_debug()
        return self.problem


    def solve(self, problem=None):
        """Solve the maximization problem, check outcome and collect results.
        """
        # XXX: this method can be made more abstract in future in the
        # sense that it could work not only for
        # log_marginal_likelihood but other measures as well
        # (e.g. cross-valideted error).

        if np.all(self.freeHypers==False): # no optimization needed
            self.hyperparameters_best = self.hyp_initial_guess.copy()
            try:
                self.parametric_model.set_hyperparameters(self.hyperparameters_best)
            except InvalidHyperparameterError:
                if __debug__: debug("MOD_SEL", "WARNING: invalid hyperparameters!")
                self.log_marginal_likelihood_best = -np.inf
                return self.log_marginal_likelihood_best
            self.parametric_model.train(self.dataset)
            self.log_marginal_likelihood_best = self.parametric_model.compute_log_marginal_likelihood()
            return self.log_marginal_likelihood_best

        result = self.problem.solve(self.optimization_algorithm) # perform optimization!
        if result.stopcase == -1:
            # XXX: should we use debug() for the following messages?
            # If so, how can we track the missing convergence to a
            # solution?
            print "Unable to find a maximum to log_marginal_likelihood"
        elif result.stopcase == 0:
            print "Limits exceeded"
        elif result.stopcase == 1:
            self.hyperparameters_best = self.hyp_initial_guess.copy()
            if self.logscale:
                self.hyperparameters_best[self.freeHypers] = np.exp(result.xf) # best hyperparameters found # NOTE is it better to return a copy?
            else:
                self.hyperparameters_best[self.freeHypers] = result.xf
                pass
            self.log_marginal_likelihood_best = result.ff # actual best vuale of log_marginal_likelihood
            pass
        self.stopcase = result.stopcase
        return self.log_marginal_likelihood_best

    pass

########NEW FILE########
__FILENAME__ = plr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Penalized logistic regression classifier."""

__docformat__ = 'restructuredtext'


import numpy as np

from mvpa2.misc.exceptions import ConvergenceError
from mvpa2.base.learner import FailedToTrainError
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples

if __debug__:
    from mvpa2.base import debug


class PLR(Classifier):
    """Penalized logistic regression `Classifier`.
    """

    __tags__ = [ 'plr', 'binary', 'linear', 'has_sensitivity' ]

    def __init__(self, lm=1, criterion=1, reduced=0.0, maxiter=20, **kwargs):
        """
        Initialize a penalized logistic regression analysis

        Parameters
        ----------
        lm : int
          the penalty term lambda.
        criterion : int
          the criterion applied to judge convergence.
        reduced : float
          if not 0, the rank of the data is reduced before
          performing the calculations. In that case, reduce is taken
          as the fraction of the first singular value, at which a
          dimension is not considered significant anymore. A
          reasonable criterion is reduced=0.01
        maxiter : int
          maximum number of iterations. If no convergence occurs
          after this number of iterations, an exception is raised.
        """
        # init base class first
        Classifier.__init__(self, **kwargs)

        self.__lm   = lm
        self.__criterion = criterion
        self.__reduced = reduced
        self.__maxiter = maxiter


    def __repr__(self):
        """String summary over the object
        """
        return """PLR(lm=%f, criterion=%d, reduced=%s, maxiter=%d, enable_ca=%s)""" % \
               (self.__lm, self.__criterion, self.__reduced, self.__maxiter,
                str(self.ca.enabled))


    def _train(self, data):
        """Train the classifier using `data` (`Dataset`).
        """
        # Set up the environment for fitting the data
        X = data.samples.T
        d = self._attrmap.to_numeric(data.sa[self.get_space()].value)
        if set(d) != set([0, 1]):
            raise ValueError, \
                  "Regressors for logistic regression should be [0,1]. Got %s" \
                  %(set(d),)

        if self.__reduced != 0 :
            # Data have reduced rank
            from scipy.linalg import svd

            # Compensate for reduced rank:
            # Select only the n largest eigenvectors
            U, S, V = svd(X.T)
            if S[0] == 0:
                raise FailedToTrainError(
                    "Data provided to PLR seems to be degenerate -- "
                    "0-th singular value is 0")
            S /= S[0]
            V = np.matrix(V[:, :np.max(np.where(S > self.__reduced)) + 1])
            # Map Data to the subspace spanned by the eigenvectors
            X = (X.T * V).T

        nfeatures, npatterns = X.shape

        # Weighting vector
        w  = np.matrix(np.zeros( (nfeatures + 1, 1), 'd'))
        # Error for convergence criterion
        dw = np.matrix(np.ones(  (nfeatures + 1, 1), 'd'))
        # Patterns of interest in the columns
        X = np.matrix( \
                np.concatenate((X, np.ones((1, npatterns), 'd')), 0) \
                )
        p = np.matrix(np.zeros((1, npatterns), 'd'))
        # Matrix implementation of penalty term
        Lambda = self.__lm * np.identity(nfeatures + 1, 'd')
        Lambda[nfeatures, nfeatures] = 0
        # Gradient
        g = np.matrix(np.zeros((nfeatures + 1, 1), 'd'))
        # Fisher information matrix
        H = np.matrix(np.identity(nfeatures + 1, 'd'))

        # Optimize
        k = 0
        while np.sum(np.ravel(dw.A ** 2)) > self.__criterion:
            p[:, :] = self.__f(w.T * X)
            g[:, :] = X * (d - p).T - Lambda * w
            H[:, :] = X * np.diag(p.A1 * (1 - p.A1)) * X.T + Lambda
            dw[:, :] = H.I * g
            w += dw
            k += 1
            if k > self.__maxiter:
                raise ConvergenceError, \
                      "More than %d Iterations without convergence" % \
                      (self.__maxiter)

        if __debug__:
            debug("PLR", \
                  "PLR converged after %d steps. Error: %g" % \
                  (k, np.sum(np.ravel(dw.A ** 2))))

        if self.__reduced:
            # We have computed in rank reduced space ->
            # Project to original space
            self.w = V * w[:-1]
            self.bias = w[-1]
        else:
            self.w = w[:-1]
            self.bias = w[-1]


    def __f(self, y):
        """This is the logistic function f, that is used for determination of
        the vector w"""
        return 1. / (1 + np.exp(-y))


    @accepts_dataset_as_samples
    def _predict(self, data):
        """
        Predict the class labels for the provided data

        Returns a list of class labels
        """
        # make sure the data are in matrix form
        data = np.matrix(np.asarray(data))

        # get the values and then predictions
        values = np.ravel(self.__f(self.bias + data * self.w))
        predictions = (values > 0.5).astype(int)

        # save the state if desired, relying on State._setitem_ to
        # decide if we will actually save the values
        self.ca.predictions = predictions
        self.ca.estimates = values

        return predictions

    def get_sensitivity_analyzer(self, **kwargs):
        """Returns a sensitivity analyzer for PLR."""
        return PLRWeights(self, **kwargs)



from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.types import asobjarray
from mvpa2.measures.base import Sensitivity
from mvpa2.datasets.base import Dataset


class PLRWeights(Sensitivity):
    """`Sensitivity` reporting linear weights of PLR"""

    _LEGAL_CLFS = [ PLR ]

    def _call(self, dataset=None):
        """Extract weights from PLR classifier.

        PLR always has weights available, so nothing has to be computed here.
        """
        clf = self.clf
        attrmap = clf._attrmap

        if attrmap:
            # labels (values of the corresponding space) which were used
            # for mapping Here we rely on the fact that they are sorted
            # originally (just an arange())
            labels_num = attrmap.values()
            labels = attrmap.to_literal(asobjarray([tuple(sorted(labels_num))]),
                                        recurse=True)
        else:
            labels = [(0, 1)]           # we just had our good old numeric ones

        ds = Dataset(clf.w.T, sa={clf.get_space(): labels,
                                  'biases' : [clf.bias]})
        return ds

########NEW FILE########
__FILENAME__ = ridge
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Ridge regression classifier."""

__docformat__ = 'restructuredtext'


import numpy as np
from mvpa2.base import externals

if externals.exists("scipy", raise_=True):
    from scipy.linalg import lstsq

from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples

class RidgeReg(Classifier):
    """Ridge regression `Classifier`.

    This ridge regression adds an intercept term so your labels do not
    have to be zero-centered.
    """

    __tags__ = ['ridge', 'regression', 'linear']

    def __init__(self, lm=None, **kwargs):
        """
        Initialize a ridge regression analysis.

        Parameters
        ----------
        lm : float
          the penalty term lambda.
          (Defaults to .05*nFeatures)
        """
        # init base class first
        Classifier.__init__(self, **kwargs)

        # pylint happiness
        self.w = None

        # It does not make sense to calculate a confusion matrix for a
        # ridge regression
        self.ca.enable('training_stats', False)

        # verify that they specified lambda
        self.__lm = lm

        # store train method config
        self.__implementation = 'direct'


    def __repr__(self):
        """String summary of the object
        """
        if self.__lm is None:
            return """Ridge(lm=.05*nfeatures, enable_ca=%s)""" % \
                (str(self.ca.enabled))
        else:
            return """Ridge(lm=%f, enable_ca=%s)""" % \
                (self.__lm, str(self.ca.enabled))


    def _train(self, data):
        """Train the classifier using `data` (`Dataset`).
        """

        if self.__implementation == "direct":
            # create matrices to solve with additional penalty term
            # determine the lambda matrix
            if self.__lm is None:
                # Not specified, so calculate based on .05*nfeatures
                Lambda = .05*data.nfeatures*np.eye(data.nfeatures)
            else:
                # use the provided penalty
                Lambda = self.__lm*np.eye(data.nfeatures)

            # add the penalty term
            a = np.concatenate( \
                (np.concatenate((data.samples, np.ones((data.nsamples, 1))), 1),
                    np.concatenate((Lambda, np.zeros((data.nfeatures, 1))), 1)))
            b = np.concatenate((data.sa[self.get_space()].value,
                               np.zeros(data.nfeatures)))

            # perform the least sq regression and save the weights
            self.w = lstsq(a, b)[0]
        else:
            raise ValueError, "Unknown implementation '%s'" \
                              % self.__implementation


    @accepts_dataset_as_samples
    def _predict(self, data):
        """
        Predict the output for the provided data.
        """
        # predict using the trained weights
        pred = np.dot(np.concatenate((data, np.ones((len(data), 1))), 1),
                     self.w)
        # estimates equal predictions in this case
        self.ca.estimates = pred
        return pred


########NEW FILE########
__FILENAME__ = sens
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Provide sensitivity measures for sg's SVM."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import externals
if externals.exists('shogun', raise_=True):
    import shogun.Classifier
    _shogun_exposes_slavesvm_labels = externals.versions['shogun:rev'] < 4633

from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.types import asobjarray
from mvpa2.measures.base import Sensitivity
from mvpa2.datasets.base import Dataset

if __debug__:
    from mvpa2.base import debug


class LinearSVMWeights(Sensitivity):
    """`Sensitivity` that reports the weights of a linear SVM trained
    on a given `Dataset`.
    """

    def __init__(self, clf, **kwargs):
        """Initialize the analyzer with the classifier it shall use.

        Parameters
        ----------
        clf : LinearSVM
          classifier to use. Only classifiers sub-classed from
          `LinearSVM` may be used.
        """
        # init base classes first
        Sensitivity.__init__(self, clf, **kwargs)


    def __sg_helper(self, svm):
        """Helper function to compute sensitivity for a single given SVM"""
        bias = svm.get_bias()
        # if it has get_w (linear ones like SVMOcas) -- use it,
        # otherwise resort to recomputing
        if hasattr(svm, 'get_w'):
            res = svm.get_w()
        else:
            svcoef = np.matrix(svm.get_alphas())
            svnums = svm.get_support_vectors()
            svs = self.clf.traindataset.samples[svnums,:]
            res = (svcoef * svs).mean(axis=0).A1
        return res, bias


    def _call(self, dataset):
        # XXX Hm... it might make sense to unify access functions
        # naming across our swig libsvm wrapper and sg access
        # functions for svm
        clf = self.clf
        sgsvm = clf.svm
        sens_labels = None
        if isinstance(sgsvm, shogun.Classifier.MultiClassSVM):
            sens, biases = [], []
            nsvms = sgsvm.get_num_svms()
            clabels = sorted(clf._attrmap.values())
            nclabels = len(clabels)
            sens_labels = []
            isvm = 0                    # index for svm among known

            for i in xrange(nclabels):
                for j in xrange(i+1, nclabels):
                    sgsvmi = sgsvm.get_svm(isvm)
                    labels_tuple = (clabels[i], clabels[j])
                    # Since we gave the labels in incremental order,
                    # we always should be right - but it does not
                    # hurt to check if set of labels is the same
                    if __debug__ and _shogun_exposes_slavesvm_labels:
                        if not sgsvmi.get_labels():
                            # We need to call classify() so labels get assigned
                            # to the multiclass SVM
                            sgsvm.classify()
                        assert(set([sgsvmi.get_label(int(x))
                                    for x in sgsvmi.get_support_vectors()])
                               == set(labels_tuple))
                    sens1, bias = self.__sg_helper(sgsvmi)
                    sens.append(sens1)
                    biases.append(bias)
                    sens_labels += [labels_tuple[::-1]] # ??? positive first
                    isvm += 1
            assert(len(sens) == nsvms)  # we should have  covered all
        else:
            sens1, bias = self.__sg_helper(sgsvm)
            biases = np.atleast_1d(bias)
            sens = np.atleast_2d(sens1)
            if not clf.__is_regression__:
                assert(set(clf._attrmap.values()) == set([-1.0, 1.0]))
                assert(sens.shape[0] == 1)
                sens_labels = [(-1.0, 1.0)]

        ds = Dataset(np.atleast_2d(sens))
        if sens_labels is not None:
            if isinstance(sens_labels[0], tuple):
                # Need to have them in array of dtype object
                sens_labels = asobjarray(sens_labels)

            if len(clf._attrmap):
                sens_labels = clf._attrmap.to_literal(sens_labels, recurse=True)
            ds.sa[clf.get_space()] = sens_labels
        ds.sa['biases'] = biases

        return ds

########NEW FILE########
__FILENAME__ = svm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wrap the libsvm package into a very simple class interface."""

__docformat__ = 'restructuredtext'


_DEV__doc__ = """

TODOs:
 * dual-license under GPL for use of SG?
 * for recent versions add ability to specify/parametrize normalization
   scheme for the kernel, and reuse 'scale' now for the normalizer
 * Add support for simplified linear classifiers (which do not require
   storing all training SVs/samples to make classification in predict())
"""

import numpy as np

from mvpa2 import _random_seed

# Rely on SG
from mvpa2.base import externals, warning
if externals.exists('shogun', raise_=True):
    import shogun.Features
    import shogun.Classifier
    import shogun.Regression
    #import shogun.Kernel
    import shogun.Library
    from mvpa2.kernels.sg import SGKernel, LinearSGKernel
    # set the default kernel here, to be able to import this module
    # when building the docs without SG
    _default_kernel_class_ = LinearSGKernel

    # Figure out debug IDs once and for all
    if hasattr(shogun.Classifier, 'M_DEBUG'):
        _M_DEBUG = shogun.Classifier.M_DEBUG
        _M_ERROR = shogun.Classifier.M_ERROR
        _M_GCDEBUG = None
    elif hasattr(shogun.Classifier, 'MSG_DEBUG'):
        _M_DEBUG = shogun.Classifier.MSG_DEBUG
        _M_ERROR = shogun.Classifier.MSG_ERROR
    else:
        _M_DEBUG, _M_ERROR = None, None
        warning("Could not figure out debug IDs within shogun. "
                "No control over shogun verbosity would be provided")
    # Highest level
    if hasattr(shogun.Classifier, 'MSG_GCDEBUG'):
        _M_GCDEBUG = shogun.Classifier.MSG_GCDEBUG
    else:
        _M_GCDEBUG = None

else:
    # set a fake default kernel here, to be able to import this module
    # when building the docs without SG
    _default_kernel_class_ = None


import operator

from mvpa2.base.param import Parameter
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.base import warning

from mvpa2.clfs.base import accepts_dataset_as_samples, \
     accepts_samples_as_dataset
from mvpa2.base.learner import FailedToTrainError
from mvpa2.clfs.meta import MulticlassClassifier
from mvpa2.clfs._svmbase import _SVM
from mvpa2.base.state import ConditionalAttribute
from mvpa2.measures.base import Sensitivity

from sens import *

if __debug__:
    from mvpa2.base import debug


def seed(random_seed):
    if __debug__:
        debug('SG', "Seeding shogun's RNG with %s" % random_seed)
    try:
        # reuse the same seed for shogun
        shogun.Library.Math_init_random(random_seed)
    except Exception, e:
        warning('Shogun cannot be seeded due to %s' % (e,))

seed(_random_seed)

def _setdebug(obj, partname):
    """Helper to set level of debugging output for SG
    Parameters
    ----------
    obj
      In SG debug output seems to be set per every object
    partname : str
      For what kind of object we are talking about... could be automated
      later on (TODO)
    """
    if _M_DEBUG is None:
        return
    debugname = "SG_%s" % partname.upper()

    switch = {True: (_M_DEBUG, 'M_DEBUG', "enable"),
              False: (_M_ERROR, 'M_ERROR', "disable"),
              'GCDEBUG': (_M_GCDEBUG, 'M_GCDEBUG', "enable")}

    if __debug__:
        if 'SG_GC' in debug.active:
            key = 'GCDEBUG'
        else:
            key = debugname in debug.active
    else:
        key = False

    sglevel, slevel, progressfunc = switch[key]

    if __debug__ and 'SG_' in debug.active:
        debug("SG_", "Setting verbosity for shogun.%s instance: %s to %s" %
              (partname, `obj`, slevel))
    if sglevel is not None:
        obj.io.set_loglevel(sglevel)
    if __debug__ and 'SG_LINENO' in debug.active:
        try:
            obj.io.enable_file_and_line()
        except AttributeError, e:
            warning("Cannot enable SG_LINENO debug target for shogun %s"
                    % externals.versions['shogun'])
    try:
        exec "obj.io.%s_progress()" % progressfunc
    except:
        warning("Shogun version %s has no way to enable progress" +
                " reports" % externals.versions['shogun'])


# Still in use by non-kernel classifiers, e.g. SVMOcas
def _tosg(data):
    """Draft helper function to convert data we have into SG suitable format

    TODO: Remove once kernels are implemented here (or, possibly for non-kernel
    solvers, modify?)
    """

    if __debug__:
        debug("SG_", "Converting data for shogun into RealFeatures")

    features = shogun.Features.RealFeatures(data.astype('double').T)

    if __debug__:
        debug("SG__", "Done converting data for shogun into RealFeatures")
    _setdebug(features, 'Features')
    return features


class SVM(_SVM):
    """Support Vector Machine Classifier(s) based on Shogun

    This is a simple base interface
    """
    __default_kernel_class__ = _default_kernel_class_
    num_threads = Parameter(1,
                            min=1,
                            doc='Number of threads to utilize')

    _KNOWN_PARAMS = [ 'epsilon' ]

    __tags__ = _SVM.__tags__ + [ 'sg', 'retrainable' ]

    # Some words of wisdom from shogun author:
    # XXX remove after proper comments added to implementations
    """
    If you'd like to train linear SVMs use SGD or OCAS. These are (I am
    serious) the fastest linear SVM-solvers to date. (OCAS cannot do SVMs
    with standard additive bias, but will L2 reqularize it - though it
    should not matter much in practice (although it will give slightly
    different solutions)). Note that SGD has no stopping criterion (you
    simply have to specify the number of iterations) and that OCAS has a
    different stopping condition than svmlight for example which may be more
    tight and more loose depending on the problem - I sugeest 1e-2 or 1e-3
    for epsilon.

    If you would like to train kernel SVMs use libsvm/gpdt/svmlight -
    depending on the problem one is faster than the other (hard to say when,
    I *think* when your dataset is very unbalanced chunking methods like
    svmlight/gpdt are better), for smaller problems definitely libsvm.

    If you use string kernels then gpdt/svmlight have a special 'linadd'
    speedup for this (requires sg 0.6.2 - there was some inefficiency in the
    code for python-modular before that). This is effective for big datasets
    and (I trained on 10 million strings based on this).

    And yes currently we only implemented parallel training for svmlight,
    however all SVMs can be evaluated in parallel.
    """
    _KNOWN_SENSITIVITIES={'linear':LinearSVMWeights,
                          }
    _KNOWN_IMPLEMENTATIONS = {}
    if externals.exists('shogun', raise_=True):
        _KNOWN_IMPLEMENTATIONS = {
            "libsvm" : (shogun.Classifier.LibSVM, ('C',),
                       ('multiclass', 'binary'),
                        "LIBSVM's C-SVM (L2 soft-margin SVM)"),
            "gmnp" : (shogun.Classifier.GMNPSVM, ('C',),
                     ('multiclass', 'binary'),
                      "Generalized Nearest Point Problem SVM"),
            # XXX should have been GPDT, shogun has it fixed since some version
            "gpbt" : (shogun.Classifier.GPBTSVM, ('C',), ('binary',),
                      "Gradient Projection Decomposition Technique for " \
                      "large-scale SVM problems"),
            "gnpp" : (shogun.Classifier.GNPPSVM, ('C',), ('binary',),
                      "Generalized Nearest Point Problem SVM"),

            ## TODO: Needs sparse features...
            # "svmlin" : (shogun.Classifier.SVMLin, ''),
            # "liblinear" : (shogun.Classifier.LibLinear, ''),
            # "subgradient" : (shogun.Classifier.SubGradientSVM, ''),
            ## good 2-class linear SVMs
            # "ocas" : (shogun.Classifier.SVMOcas, ''),
            # "sgd" : ( shogun.Classifier.SVMSGD, ''),

            # regressions
            "libsvr": (shogun.Regression.LibSVR, ('C', 'tube_epsilon',),
                      ('regression',),
                       "LIBSVM's epsilon-SVR"),
            }


    def __init__(self, **kwargs):
        """Interface class to Shogun's classifiers and regressions.

        Default implementation is 'libsvm'.
        """


        svm_impl = kwargs.get('svm_impl', 'libsvm').lower()
        kwargs['svm_impl'] = svm_impl

        # init base class
        _SVM.__init__(self, **kwargs)

        self.__svm = None
        """Holds the trained svm."""
        self.__svm_apply = None
        """Compatibility convenience to bind to the classify/apply method
           of __svm"""
        # Need to store original data...
        # TODO: keep 1 of them -- just __traindata or __traindataset
        # For now it is needed for computing sensitivities
        self.__traindataset = None

        # internal SG swig proxies
        self.__traindata = None
        self.__kernel = None
        self.__kernel_test = None
        self.__testdata = None

        # remove kernel-based for some
        # TODO RF: provide separate handling for non-kernel machines
        if svm_impl in ['svmocas']:
            if not (self.__kernel is None
                    or self.__kernel.__kernel_name__ == 'linear'):
                raise ValueError(
                    "%s is inherently linear, thus provided kernel %s "
                    "is of no effect" % (svm_impl, self.__kernel))
            self.__tags__.pop(self.__tags__.index('kernel-based'))
            self.__tags__.pop(self.__tags__.index('retrainable'))

    # TODO: integrate with kernel framework
    #def __condition_kernel(self, kernel):
        ## XXX I thought that it is needed only for retrainable classifier,
        ##     but then krr gets confused, and svrlight needs it to provide
        ##     meaningful results even without 'retraining'
        #if self._svm_impl in ['svrlight', 'lightsvm']:
            #try:
                #kernel.set_precompute_matrix(True, True)
            #except Exception, e:
                ## N/A in shogun 0.9.1... TODO: RF
                #if __debug__:
                    #debug('SG_', "Failed call to set_precompute_matrix for %s: %s"
                          #% (self, e))


    def _train(self, dataset):
        """Train SVM
        """

        # XXX watchout
        # self.untrain()
        newkernel, newsvm = False, False
        # local bindings for faster lookup
        params = self.params
        retrainable = self.params.retrainable

        targets_sa_name = self.get_space()    # name of targets sa
        targets_sa = dataset.sa[targets_sa_name] # actual targets sa

        if retrainable:
            _changedData = self._changedData

        # LABELS
        ul = None
        self.__traindataset = dataset


        # OK -- we have to map labels since
        #  binary ones expect -1/+1
        #  Multiclass expect labels starting with 0, otherwise they puke
        #   when ran from ipython... yikes
        if __debug__:
            debug("SG_", "Creating labels instance")

        if self.__is_regression__:
            labels_ = np.asarray(targets_sa.value, dtype='double')
        else:
            ul = targets_sa.unique
            # ul.sort()

            if len(ul) == 2:
                # assure that we have -1/+1
                _labels_dict = {ul[0]:-1.0, ul[1]:+1.0}
            elif len(ul) < 2:
                raise FailedToTrainError, \
                      "We do not have 1-class SVM brought into SG yet"
            else:
                # can't use plain enumerate since we need them swapped
                _labels_dict = dict([ (ul[i], i) for i in range(len(ul))])

            # Create SG-customized attrmap to assure -1 / +1 if necessary
            self._attrmap = AttributeMap(_labels_dict, mapnumeric=True)

            if __debug__:
                debug("SG__", "Mapping labels using dict %s" % _labels_dict)
            labels_ = self._attrmap.to_numeric(targets_sa.value).astype(float)

        labels = shogun.Features.Labels(labels_)
        _setdebug(labels, 'Labels')


        # KERNEL

        # XXX cruel fix for now... whole retraining business needs to
        # be rethought
        if retrainable:
            _changedData['kernel_params'] = _changedData.get('kernel_params', False)

        # TODO: big RF to move non-kernel classifiers away
        if 'kernel-based' in self.__tags__ and (not retrainable
               or _changedData['traindata'] or _changedData['kernel_params']):
            # If needed compute or just collect arguments for SVM and for
            # the kernel

            if retrainable and __debug__:
                if _changedData['traindata']:
                    debug("SG",
                          "Re-Creating kernel since training data has changed")

                if _changedData['kernel_params']:
                    debug("SG",
                          "Re-Creating kernel since params %s has changed" %
                          _changedData['kernel_params'])


            k = self.params.kernel
            k.compute(dataset)
            self.__kernel = kernel = k.as_raw_sg()

            newkernel = True
            self.kernel_params.reset()  # mark them as not-changed
            #_setdebug(kernel, 'Kernels')

            #self.__condition_kernel(kernel)
            if retrainable:
                if __debug__:
                    debug("SG_", "Resetting test kernel for retrainable SVM")
                self.__kernel_test = None

        # TODO -- handle _changedData['params'] correctly, ie without recreating
        # whole SVM
        Cs = None
        if not retrainable or self.__svm is None or _changedData['params']:
            # SVM
            if self.params.has_key('C'):
                Cs = self._get_cvec(dataset)

                # XXX do not jump over the head and leave it up to the user
                #     ie do not rescale automagically by the number of samples
                #if len(Cs) == 2 and not ('regression' in self.__tags__) and len(ul) == 2:
                #    # we were given two Cs
                #    if np.max(C) < 0 and np.min(C) < 0:
                #        # and both are requested to be 'scaled' TODO :
                #        # provide proper 'features' to the parameters,
                #        # so we could specify explicitely if to scale
                #        # them by the number of samples here
                #        nl = [np.sum(labels_ == _labels_dict[l]) for l in ul]
                #        ratio = np.sqrt(float(nl[1]) / nl[0])
                #        #ratio = (float(nl[1]) / nl[0])
                #        Cs[0] *= ratio
                #        Cs[1] /= ratio
                #        if __debug__:
                #            debug("SG_", "Rescaled Cs to %s to accomodate the "
                #                  "difference in number of training samples" %
                #                  Cs)

            # Choose appropriate implementation
            svm_impl_class = self.__get_implementation(ul)

            if __debug__:
                debug("SG", "Creating SVM instance of %s" % `svm_impl_class`)

            if self._svm_impl in ['libsvr', 'svrlight']:
                # for regressions constructor a bit different
                self.__svm = svm_impl_class(Cs[0], self.params.tube_epsilon, self.__kernel, labels)
                # we need to set epsilon explicitly
                self.__svm.set_epsilon(self.params.epsilon)
            elif self._svm_impl in ['krr']:
                self.__svm = svm_impl_class(self.params.tau, self.__kernel, labels)
            elif 'kernel-based' in self.__tags__:
                self.__svm = svm_impl_class(Cs[0], self.__kernel, labels)
                self.__svm.set_epsilon(self.params.epsilon)
            else:
                traindata_sg = _tosg(dataset.samples)
                self.__svm = svm_impl_class(Cs[0], traindata_sg, labels)
                self.__svm.set_epsilon(self.params.epsilon)

            # To stay compatible with versions across API changes in sg 1.0.0
            self.__svm_apply = externals.versions['shogun'] >= '1' \
                               and self.__svm.apply \
                               or  self.__svm.classify # the last one for old API

            # Set shrinking
            if 'shrinking' in params:
                shrinking = params.shrinking
                if __debug__:
                    debug("SG_", "Setting shrinking to %s" % shrinking)
                self.__svm.set_shrinking_enabled(shrinking)

            if Cs is not None and len(Cs) == 2:
                if __debug__:
                    debug("SG_", "Since multiple Cs are provided: %s, assign them" % Cs)
                self.__svm.set_C(Cs[0], Cs[1])

            self.params.reset()  # mark them as not-changed
            newsvm = True
            _setdebug(self.__svm, 'SVM')
            # Set optimization parameters
            if self.params.has_key('tube_epsilon') and \
                   hasattr(self.__svm, 'set_tube_epsilon'):
                self.__svm.set_tube_epsilon(self.params.tube_epsilon)
            self.__svm.parallel.set_num_threads(self.params.num_threads)
        else:
            if __debug__:
                debug("SG_", "SVM instance is not re-created")
            if _changedData['targets']:          # labels were changed
                if __debug__: debug("SG__", "Assigning new labels")
                self.__svm.set_labels(labels)
            if newkernel:               # kernel was replaced
                if __debug__: debug("SG__", "Assigning new kernel")
                self.__svm.set_kernel(self.__kernel)
            assert(_changedData['params'] is False)  # we should never get here

        if retrainable:
            # we must assign it only if it is retrainable
            self.ca.retrained = not newsvm or not newkernel

        # Train
        if __debug__ and 'SG' in debug.active:
            if not self.__is_regression__:
                lstr = " with labels %s" % targets_sa.unique
            else:
                lstr = ""
            debug("SG", "%sTraining %s on data%s" %
                  (("","Re-")[retrainable and self.ca.retrained],
                   self, lstr))

        self.__svm.train()

        if __debug__:
            debug("SG_", "Done training SG_SVM %s" % self)

        # Report on training
        if (__debug__ and 'SG__' in debug.active) or \
           self.ca.is_enabled('training_stats'):
            if __debug__:
                debug("SG_", "Assessing predictions on training data")
            trained_targets = self.__svm_apply().get_labels()

        else:
            trained_targets = None

        if __debug__ and "SG__" in debug.active:
            debug("SG__", "Original labels: %s, Trained labels: %s" %
                  (targets_sa.value, trained_targets))

        # Assign training confusion right away here since we are ready
        # to do so.
        # XXX TODO use some other conditional attribute like 'trained_targets' and
        #     use it within base Classifier._posttrain to assign predictions
        #     instead of duplicating code here
        # XXX For now it can be done only for regressions since labels need to
        #     be remapped and that becomes even worse if we use regression
        #     as a classifier so mapping happens upstairs
        if self.__is_regression__ and self.ca.is_enabled('training_stats'):
            self.ca.training_stats = self.__summary_class__(
                targets=targets_sa.value,
                predictions=trained_targets)


    # XXX actually this is the beast which started this evil conversion
    #     so -- make use of dataset here! ;)
    @accepts_samples_as_dataset
    def _predict(self, dataset):
        """Predict values for the data
        """

        retrainable = self.params.retrainable

        if retrainable:
            changed_testdata = self._changedData['testdata'] or \
                               self.__kernel_test is None

        if not retrainable:
            if __debug__:
                debug("SG__",
                      "Initializing SVMs kernel of %s with training/testing samples"
                      % self)
            self.params.kernel.compute(self.__traindataset, dataset)
            self.__kernel_test = self.params.kernel.as_sg()._k
            # We can just reuse kernel used for training
            #self.__condition_kernel(self.__kernel)

        else:
            if changed_testdata:
                #if __debug__:
                    #debug("SG__",
                          #"Re-creating testing kernel of %s giving "
                          #"arguments %s" %
                          #(`self._kernel_type`, self.__kernel_args))
                self.params.kernel.compute(self.__traindataset, dataset)

                #_setdebug(kernel_test, 'Kernels')

                #_setdebug(kernel_test_custom, 'Kernels')
                self.__kernel_test = self.params.kernel.as_raw_sg()

            elif __debug__:
                debug("SG__", "Re-using testing kernel")

        assert(self.__kernel_test is not None)

        if 'kernel-based' in self.__tags__:
            self.__svm.set_kernel(self.__kernel_test)
            # doesn't do any good imho although on unittests helps tiny bit... hm
            #self.__svm.init_kernel_optimization()
            values_ = self.__svm_apply()
        else:
            testdata_sg = _tosg(dataset.samples)
            self.__svm.set_features(testdata_sg)
            values_ = self.__svm_apply()

        if __debug__:
            debug("SG_", "Classifying testing data")

        if values_ is None:
            raise RuntimeError, "We got empty list of values from %s" % self

        values = values_.get_labels()

        if retrainable:
            # we must assign it only if it is retrainable
            self.ca.repredicted = repredicted = not changed_testdata
            if __debug__:
                debug("SG__", "Re-assigning learing kernel. Repredicted is %s"
                      % repredicted)
            # return back original kernel
            if 'kernel-based' in self.__tags__:
                self.__svm.set_kernel(self.__kernel)

        if __debug__:
            debug("SG__", "Got values %s" % values)

        if (self.__is_regression__):
            predictions = values
        else:
            if len(self._attrmap.keys()) == 2:
                predictions = np.sign(values)
                # since np.sign(0) == 0
                predictions[predictions==0] = 1
            else:
                predictions = values

            # remap labels back adjusting their type
            # XXX YOH: This is done by topclass now (needs RF)
            #predictions = self._attrmap.to_literal(predictions)

            if __debug__:
                debug("SG__", "Tuned predictions %s" % predictions)

        # store conditional attribute
        # TODO: extract values properly for multiclass SVMs --
        #       ie 1 value per label or pairs for all 1-vs-1 classifications
        self.ca.estimates = values

        ## to avoid leaks with not yet properly fixed shogun
        if not retrainable:
            try:
                testdata.free_features()
            except:
                pass

        return predictions


    def _untrain(self):
        super(SVM, self)._untrain()
        # untrain/clean the kernel -- we might not allow to drag SWIG
        # instance around BUT XXX -- make it work fine with
        # CachedKernel -- we might not want to fully "untrain" in such
        # case
        self.params.kernel.cleanup()    # XXX unify naming
        if not self.params.retrainable:
            if __debug__:
                debug("SG__", "Untraining %(clf)s and destroying sg's SVM",
                      msgargs={'clf':self})

            # to avoid leaks with not yet properly fixed shogun
            # XXX make it nice... now it is just stable ;-)
            if True: # not self.__traindata is None:
                if True:
                # try:
                    if self.__kernel is not None:
                        del self.__kernel
                        self.__kernel = None

                    if self.__kernel_test is not None:
                        del self.__kernel_test
                        self.__kernel_test = None

                    if self.__svm is not None:
                        del self.__svm
                        self.__svm = None
                        self.__svm_apply = None

                    if self.__traindata is not None:
                        # Let in for easy demonstration of the memory leak in shogun
                        #for i in xrange(10):
                        #    debug("SG__", "cachesize pre free features %s" %
                        #          (self.__svm.get_kernel().get_cache_size()))
                        self.__traindata.free_features()
                        del self.__traindata
                        self.__traindata = None

                    self.__traindataset = None


                #except:
                #    pass

            if __debug__:
                debug("SG__",
                      "Done untraining %(self)s and destroying sg's SVM",
                      msgargs=locals())
        elif __debug__:
            debug("SG__", "Not untraining %(self)s since it is retrainable",
                  msgargs=locals())


    def __get_implementation(self, ul):
        if self.__is_regression__ or len(ul) == 2:
            svm_impl_class = SVM._KNOWN_IMPLEMENTATIONS[self._svm_impl][0]
        else:
            if self._svm_impl == 'libsvm':
                svm_impl_class = shogun.Classifier.LibSVMMultiClass
            elif self._svm_impl == 'gmnp':
                svm_impl_class = shogun.Classifier.GMNPSVM
            else:
                raise RuntimeError, \
                      "Shogun: Implementation %s doesn't handle multiclass " \
                      "data. Got labels %s. Use some other classifier" % \
                      (self._svm_impl,
                       self.__traindataset.sa[self.get_space()].unique)
            if __debug__:
                debug("SG_", "Using %s for multiclass data of %s" %
                      (svm_impl_class, self._svm_impl))

        return svm_impl_class


    svm = property(fget=lambda self: self.__svm)
    """Access to the SVM model."""

    traindataset = property(fget=lambda self: self.__traindataset)
    """Dataset which was used for training

    TODO -- might better become conditional attribute I guess"""



# Conditionally make some of the implementations available if they are
# present in the present shogun
for name, item, params, descr in \
        [('mpd', "shogun.Classifier.MPDSVM", "('C',), ('binary',)",
          "MPD classifier from shogun"),
         ('lightsvm', "shogun.Classifier.SVMLight", "('C',), ('binary',)",
          "SVMLight classification http://svmlight.joachims.org/"),
         ('svrlight', "shogun.Regression.SVRLight", "('C','tube_epsilon',), ('regression',)",
          "SVMLight regression http://svmlight.joachims.org/"),
         ('krr', "shogun.Regression.KRR", "('tau',), ('regression',)",
          "Kernel Ridge Regression"),
         ('svmocas', "shogun.Classifier.SVMOcas", "('C',), ('binary', 'linear')",
          "SVM with OCAS (Optimized Cutting Plane Algorithm) solver"),
         ]:
    if externals.exists('shogun.%s' % name):
        exec "SVM._KNOWN_IMPLEMENTATIONS[\"%s\"] = (%s, %s, \"%s\")" % (name, item, params, descr)

# Assign SVM class to limited set of LinearSVMWeights
LinearSVMWeights._LEGAL_CLFS = [SVM]

########NEW FILE########
__FILENAME__ = similarity
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com>
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Similarity functions for prototype-based projection."""

import numpy as np

from mvpa2.clfs.distance import squared_euclidean_distance

if __debug__:
    from mvpa2.base import debug

class Similarity(object):
    """Similarity function base class.

    """

    def __repr__(self):
        return "Similarity()"

    def computed(self, data1, data2=None):
        raise NotImplementedError


class SingleDimensionSimilarity(Similarity):
    """TODO

    .. math:: e^{(-|data1_j - data2_j|_2)}

    """
    def __init__(self, d=0, **kwargs):
        """
        Parameters
        ----------
        d : int
          Dimension (feature) across which to compute similarity
        **kwargs
          Passed to Similarity
        """
        Similarity.__init__(self, **kwargs)
        self.d = d

    def computed(self, data1, data2=None):
        if data2 == None: data2 = data1
        self.similarity_matrix = np.exp(-np.abs(data1[:, self.d],
                                              data2[:, self.d]))
        return self.similarity_matrix


class StreamlineSimilarity(Similarity):
    """Compute similarity between two streamlines.
    """

    def __init__(self, distance, gamma=1.0):
        """
        Parameters
        ----------
        distance : func
          Distance measure
        gamma : float
          Exponent coefficient
        """
        Similarity.__init__(self)
        self.distance = distance
        self.gamma = gamma


    def computed(self, data1, data2=None):
        if data2 == None:
            data2 = data1
        self.distance_matrix = np.zeros((len(data1), len(data2)))

        # setup helpers to pull out content of object-type arrays
        if isinstance(data1, np.ndarray) and np.issubdtype(data1.dtype, np.object):
            d1extract = _pass_obj_content
        else:
            d1extract = lambda x: x

        if isinstance(data2, np.ndarray) and np.issubdtype(data2.dtype, np.object):
            d2extract = _pass_obj_content
        else:
            d2extract = lambda x: x

        # TODO: use np.fromfunction
        for i, d1 in enumerate(data1):
            for j, d2 in enumerate(data2):
                self.distance_matrix[i,j] = self.distance(d1extract(data1[i]),
                                                          d2extract(data2[j]))

        self.similarity_matrix = np.exp(-self.gamma*self.distance_matrix)
        return self.similarity_matrix


def _pass_obj_content(data):
    """Helper that can be used to return the content of a single-element
    array of type 'object' to access its real content.
    """
    return data[0]

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Generic wrappers for learners (classifiers) provided by scikit-learn (AKA sklearn)"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import warning, externals
from mvpa2.base.dochelpers import _repr_attrs
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.base.learner import FailedToTrainError, FailedToPredictError, \
        DegenerateInputError


# do conditional to be able to build module reference
externals.exists('skl', raise_=True)


class SKLLearnerAdapter(Classifier):
    """Generic adapter for instances of learners provided by scikits.learn

    Provides basic adaptation of interface (e.g. train -> fit) and
    wraps the constructed instance of a learner from skl, so it looks
    like any other learner present within PyMVPA (so obtains all the
    conditional attributes defined at the base level of a
    `Classifier`)

    Examples
    --------

    TODO
    """

    __tags__ = ['skl']

    def __init__(self, skl_learner, tags=None, enforce_dim=None,
                 **kwargs):
        """
        Parameters
        ----------
        skl_learner
          Existing instance of a learner from skl.  It should
          implement `fit` and `predict`.  If `predict_proba` is
          available in the interface, then conditional attribute
          `probabilities` becomes available as well
        tags : list of string
          What additional tags to attach to this learner.  Tags are
          used in the queries to classifier or regression warehouses.
        enforce_dim : None or int, optional
          If not None, it would enforce given dimensionality for
          ``predict`` call, if all other trailing dimensions are
          degenerate.
        """

        self._skl_learner = skl_learner
        self.enforce_dim = enforce_dim
        if tags:
            # So we make a per-instance copy
            self.__tags__ = self.__tags__ + tags
        Classifier.__init__(self, **kwargs)


    def __repr__(self):
        """String representation of `SKLLearnerWrapper`
        """
        prefixes = [repr(self._skl_learner)]
        if self.__tags__ != ['skl']:
            prefixes += ['tags=%r' % [t for t in self.__tags__ if t != 'skl']]
        prefixes += _repr_attrs(self, ['enforce_dim'])
        return Classifier.__repr__(self, prefixes=prefixes)


    def _train(self, dataset):
        """Train the skl learner using `dataset` (`Dataset`).
        """
        targets_sa = dataset.sa[self.get_space()]
        targets = targets_sa.value
        # Some sanity checking so some classifiers such as LDA do not
        # puke meaningless exceptions
        if 'lda' in self.__tags__:
            if not dataset.nsamples > len(targets_sa.unique):
                raise DegenerateInputError, \
                      "LDA requires # of samples exceeding # of classes"

        # we better map into numeric labels if it is not a regression
        if not 'regression' in self.__tags__:
            targets = self._attrmap.to_numeric(targets)

        try:
            # train underlying learner
            self._skl_learner.fit(dataset.samples, targets)
        except (ValueError, np.linalg.LinAlgError), e:
            raise FailedToTrainError, \
                  "Failed to train %s on %s. Got '%s' during call to fit()." \
                  % (self, dataset, e)

    @accepts_dataset_as_samples
    def _predict(self, data):
        """Predict using the skl learner
        """
        try:
            res = self._skl_learner.predict(data)
        except Exception, e:
            raise FailedToPredictError, \
                  "Failed to predict %s on data of shape %s. Got '%s' during" \
                  " call to predict()." % (self, data.shape, e)

        if self.enforce_dim:
            res_dim = len(res.shape)
            if res_dim > self.enforce_dim:
                # would throw meaningful exception if not possible
                res = res.reshape(res.shape[:self.enforce_dim])
            elif res_dim < self.enforce_dim:
                # broadcast
                res = res.reshape(res.shape + (1,)* (self.enforce_dim - res_dim))
        # Estimate estimates after predict, so if something goes
        # wrong, above exception handling occurs
        if self.ca.is_enabled('probabilities'):
            if hasattr(self._skl_learner, 'predict_proba'):
                # Duplication of computation, since in many scenarios
                # predict() calls predict_proba()
                self.ca.probabilities = self._skl_learner.predict_proba(data)
            else:
                warning("%s has no predict_proba() defined, so no probability"
                        " estimates could be extracted" % self._skl_learner)
        self.ca.estimates = res
        return res

########NEW FILE########
__FILENAME__ = smlr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Sparse Multinomial Logistic Regression classifier."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2 import _random_seed
from mvpa2.base import warning, externals
from mvpa2.clfs.base import Classifier, accepts_dataset_as_samples
from mvpa2.measures.base import Sensitivity
from mvpa2.misc.exceptions import ConvergenceError
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import *
from mvpa2.base.state import ConditionalAttribute
from mvpa2.datasets.base import Dataset

__all__ = [ "SMLR", "SMLRWeights" ]


_DEFAULT_IMPLEMENTATION = "Python"
if externals.exists('ctypes'):
    # Uber-fast C-version of the stepwise regression
    try:
        from mvpa2.clfs.libsmlrc import stepwise_regression as _cStepwiseRegression
        _DEFAULT_IMPLEMENTATION = "C"
    except OSError, e:
        warning("Failed to load fast implementation of SMLR.  May be you "
                "forgotten to build it.  We will use much slower pure-Python "
                "version. Original exception was %s" % (e,))
        _cStepwiseRegression = None
else:
    _cStepwiseRegression = None
    warning("SMLR implementation without ctypes is overwhelmingly slow."
            " You are strongly advised to install python-ctypes")

if __debug__:
    from mvpa2.base import debug

def _label2oneofm(labels, ulabels):
    """Convert labels to one-of-M form.

    TODO: Might be useful elsewhere so could migrate into misc/
    """

    # allocate for the new one-of-M labels
    new_labels = np.zeros((len(labels), len(ulabels)))

    # loop and convert to one-of-M
    for i, c in enumerate(ulabels):
        new_labels[labels == c, i] = 1

    return new_labels



class SMLR(Classifier):
    """Sparse Multinomial Logistic Regression `Classifier`.

    This is an implementation of the SMLR algorithm published in
    :ref:`Krishnapuram et al., 2005 <KCF+05>` (2005, IEEE Transactions
    on Pattern Analysis and Machine Intelligence).  Be sure to cite
    that article if you use this classifier for your work.
    """

    __tags__ = [ 'smlr', 'linear', 'has_sensitivity', 'binary',
                 'multiclass', 'does_feature_selection',
                 'random_tie_breaking']
    # XXX: later 'kernel-based'?

    lm = Parameter(.1, constraints=EnsureFloat() & EnsureRange(min=1e-10),
             doc="""The penalty term lambda.  Larger values will give rise to
             more sparsification.""")

    convergence_tol = Parameter(1e-3, 
             constraints=EnsureFloat() & EnsureRange(min=1e-10, max=1.0),
             doc="""When the weight change for each cycle drops below this value
             the regression is considered converged.  Smaller values
             lead to tighter convergence.""")

    resamp_decay = Parameter(0.5, 
             constraints=EnsureFloat() & EnsureRange(min=0.0, max=1.0),
             doc="""Decay rate in the probability of resampling a zero weight.
             1.0 will immediately decrease to the min_resamp from 1.0, 0.0
             will never decrease from 1.0.""")

    min_resamp = Parameter(0.001, 
             constraints=EnsureFloat() & EnsureRange(min=1e-10, max=1.0),
             doc="Minimum resampling probability for zeroed weights")

    maxiter = Parameter(10000, constraints=EnsureInt() & EnsureRange(min=1),
             doc="""Maximum number of iterations before stopping if not
             converged.""")

    has_bias = Parameter(True, constraints='bool',
             doc="""Whether to add a bias term to allow fits to data not through
             zero""")

    fit_all_weights = Parameter(True, constraints='bool',
             doc="""Whether to fit weights for all classes or to the number of
            classes minus one.  Both should give nearly identical results, but
            if you set fit_all_weights to True it will take a little longer
            and yield weights that are fully analyzable for each class.  Also,
            note that the convergence rate may be different, but convergence
            point is the same.""")

    implementation = Parameter(_DEFAULT_IMPLEMENTATION,
             constraints=EnsureChoice('C', 'Python'),
             doc="""Use C or Python as the implementation of
             stepwise_regression. C version brings significant speedup thus is
             the default one.""")

    ties = Parameter('random', constraints='str',
                     doc="""Resolve ties which could occur.  At the moment
                     only obvious ties resulting in identical weights
                     per two classes are detected and resolved
                     randomly by injecting small amount of noise into
                     the estimates of tied categories.
                     Set to False to avoid this behavior""")

    seed = Parameter(_random_seed, constraints=EnsureNone() | EnsureInt(),
             doc="""Seed to be used to initialize random generator, might be
             used to replicate the run""")

    unsparsify = Parameter(False, constraints='bool',
             doc="""***EXPERIMENTAL*** Whether to unsparsify the weights via
             regression. Note that it likely leads to worse classifier
             performance, but more interpretable weights.""")

    std_to_keep = Parameter(2.0, constraints='float',
             doc="""Standard deviation threshold of weights to keep when
             unsparsifying.""")

    def __init__(self, **kwargs):
        """Initialize an SMLR classifier.
        """

        """
        TODO:
         # Add in likelihood calculation
         # Add kernels, not just direct methods.
         """
        # init base class first
        Classifier.__init__(self, **kwargs)

        if _cStepwiseRegression is None and self.params.implementation == 'C':
            warning('SMLR: C implementation is not available.'
                    ' Using pure Python one')
            self.params.implementation = 'Python'

        # pylint friendly initializations
        self._ulabels = None
        """Unigue labels from the training set."""
        self.__weights_all = None
        """Contains all weights including bias values"""
        self.__weights = None
        """Just the weights, without the biases"""
        self.__biases = None
        """The biases, will remain none if has_bias is False"""


    ##REF: Name was automagically refactored
    def _python_stepwise_regression(self, w, X, XY, Xw, E,
                                  auto_corr,
                                  lambda_over_2_auto_corr,
                                  S,
                                  M,
                                  maxiter,
                                  convergence_tol,
                                  resamp_decay,
                                  min_resamp,
                                  verbose,
                                  seed = None):
        """The (much slower) python version of the stepwise
        regression.  I'm keeping this around for now so that we can
        compare results."""

        # get the data information into easy vars
        ns, nd = X.shape

        # initialize the iterative optimization
        converged = False
        incr = np.finfo(np.float).max
        non_zero, basis, m, wasted_basis, cycles = 0, 0, 0, 0, 0
        sum2_w_diff, sum2_w_old, w_diff = 0.0, 0.0, 0.0
        p_resamp = np.ones(w.shape, dtype=np.float)

        if seed is not None:
            # set the random seed
            np.random.seed(seed)

            if __debug__:
                debug("SMLR_", "random seed=%s" % seed)

        # perform the optimization
        while not converged and cycles < maxiter:
            # get the starting weight
            w_old = w[basis, m]

            # see if we're gonna update
            if (w_old != 0) or np.random.rand() < p_resamp[basis, m]:
                # let's do it
                # get the probability
                P = E[:, m]/S

                # set the gradient
                grad = XY[basis, m] - np.dot(X[:, basis], P)

                # calculate the new weight with the Laplacian prior
                w_new = w_old + grad/auto_corr[basis]

                # keep weights within bounds
                if w_new > lambda_over_2_auto_corr[basis]:
                    w_new -= lambda_over_2_auto_corr[basis]
                    changed = True
                    # unmark from being zero if necessary
                    if w_old == 0:
                        non_zero += 1
                        # reset the prob of resampling
                        p_resamp[basis, m] = 1.0
                elif w_new < -lambda_over_2_auto_corr[basis]:
                    w_new += lambda_over_2_auto_corr[basis]
                    changed = True
                    # unmark from being zero if necessary
                    if w_old == 0:
                        non_zero += 1
                        # reset the prob of resampling
                        p_resamp[basis, m] = 1.0
                else:
                    # gonna zero it out
                    w_new = 0.0

                    # decrease the p_resamp
                    p_resamp[basis, m] -= (p_resamp[basis, m] - \
                                           min_resamp) * resamp_decay

                    # set number of non-zero
                    if w_old == 0:
                        changed = False
                        wasted_basis += 1
                    else:
                        changed = True
                        non_zero -= 1

                # process any changes
                if changed:
                    #print "w[%d, %d] = %g" % (basis, m, w_new)
                    # update the expected values
                    w_diff = w_new - w_old
                    Xw[:, m] = Xw[:, m] + X[:, basis]*w_diff
                    E_new_m = np.exp(Xw[:, m])
                    S += E_new_m - E[:, m]
                    E[:, m] = E_new_m

                    # update the weight
                    w[basis, m] = w_new

                    # keep track of the sqrt sum squared diffs
                    sum2_w_diff += w_diff*w_diff

                # add to the old no matter what
                sum2_w_old += w_old*w_old

            # update the class and basis
            m = np.mod(m+1, w.shape[1])
            if m == 0:
                # we completed a cycle of labels
                basis = np.mod(basis+1, nd)
                if basis == 0:
                    # we completed a cycle of features
                    cycles += 1

                    # assess convergence
                    incr = np.sqrt(sum2_w_diff) / \
                           (np.sqrt(sum2_w_old)+np.finfo(np.float).eps)

                    # save the new weights
                    converged = incr < convergence_tol

                    if __debug__:
                        debug("SMLR_", \
                              "cycle=%d ; incr=%g ; non_zero=%d ; " %
                              (cycles, incr, non_zero) +
                              "wasted_basis=%d ; " %
                              (wasted_basis) +
                              "sum2_w_old=%g ; sum2_w_diff=%g" %
                              (sum2_w_old, sum2_w_diff))

                    # reset the sum diffs and wasted_basis
                    sum2_w_diff = 0.0
                    sum2_w_old = 0.0
                    wasted_basis = 0


        if not converged:
            raise ConvergenceError, \
                "More than %d Iterations without convergence" % \
                (maxiter)

        # calcualte the log likelihoods and posteriors for the training data
        #log_likelihood = x

        return cycles


    def _train(self, dataset):
        """Train the classifier using `dataset` (`Dataset`).
        """
        targets_sa_name = self.get_space()    # name of targets sa
        targets_sa = dataset.sa[targets_sa_name] # actual targets sa

        # Process the labels to turn into 1 of N encoding
        uniquelabels = targets_sa.unique
        labels = _label2oneofm(targets_sa.value, uniquelabels)
        self._ulabels = uniquelabels.copy()

        Y = labels
        M = len(self._ulabels)

        # get the dataset information into easy vars
        X = dataset.samples

        # see if we are adding a bias term
        if self.params.has_bias:
            if __debug__:
                debug("SMLR_", "hstacking 1s for bias")

            # append the bias term to the features
            X = np.hstack((X, np.ones((X.shape[0], 1), dtype=X.dtype)))

        if self.params.implementation.upper() == 'C':
            _stepwise_regression = _cStepwiseRegression
            #
            # TODO: avoid copying to non-contig arrays, use strides in ctypes?
            if not (X.flags['C_CONTIGUOUS'] and X.flags['ALIGNED']):
                if __debug__:
                    debug("SMLR_",
                          "Copying data to get it C_CONTIGUOUS/ALIGNED")
                X = np.array(X, copy=True, dtype=np.double, order='C')

            # currently must be double for the C code
            if X.dtype != np.double:
                if __debug__:
                    debug("SMLR_", "Converting data to double")
                # must cast to double
                X = X.astype(np.double)

        # set the feature dimensions
        elif self.params.implementation.upper() == 'PYTHON':
            _stepwise_regression = self._python_stepwise_regression
        else:
            raise ValueError, \
                  "Unknown implementation %s of stepwise_regression" % \
                  self.params.implementation

        # set the feature dimensions
        ns, nd = X.shape

        # decide the size of weights based on num classes estimated
        if self.params.fit_all_weights:
            c_to_fit = M
        else:
            c_to_fit = M-1

        # Precompute what we can
        auto_corr = ((M-1.)/(2.*M))*(np.sum(X*X, 0))
        XY = np.dot(X.T, Y[:, :c_to_fit])
        lambda_over_2_auto_corr = (self.params.lm/2.)/auto_corr

        # set starting values
        w = np.zeros((nd, c_to_fit), dtype=np.double)
        Xw = np.zeros((ns, c_to_fit), dtype=np.double)
        E = np.ones((ns, c_to_fit), dtype=np.double)
        S = M*np.ones(ns, dtype=np.double)

        # set verbosity
        if __debug__:
            verbosity = int( "SMLR_" in debug.active )
            debug('SMLR_', 'Calling stepwise_regression. Seed %s' % self.params.seed)
        else:
            verbosity = 0

        # call the chosen version of stepwise_regression
        cycles = _stepwise_regression(w,
                                      X,
                                      XY,
                                      Xw,
                                      E,
                                      auto_corr,
                                      lambda_over_2_auto_corr,
                                      S,
                                      M,
                                      self.params.maxiter,
                                      self.params.convergence_tol,
                                      self.params.resamp_decay,
                                      self.params.min_resamp,
                                      verbosity,
                                      self.params.seed)

        if cycles >= self.params.maxiter:
            # did not converge
            raise ConvergenceError, \
                  "More than %d Iterations without convergence" % \
                  (self.params.maxiter)

        # see if unsparsify the weights
        if self.params.unsparsify:
            # unsparsify
            w = self._unsparsify_weights(X, w)

        # resolve ties if present
        self.__ties = None
        if self.params.ties:
            if self.params.ties == 'random':
                # check if there is a tie showing itself as absent
                # difference between two w's
                wdot = np.dot(w.T, -w)
                ties = np.where(np.max(np.abs(wdot), axis=0) == 0)[0]
                if len(ties):
                    warning("SMLR: detected ties in categories %s.  Small "
                            "amount of noise will be injected into result "
                            "estimates upon prediction to break the ties"
                            % self._ulabels[ties])
                    self.__ties = ties
                    ## w_non0 = np.nonzero(w)
                    ## w_non0_max = np.max(np.abs(w[w_non0]))
                    ## w_non0_idx = np.unique(w_non0[0])
                    ## w_non0_len = len(w_non0_idx)
                    ## for f_idx in np.where(ties)[0]:
                    ##     w[w_non0_idx, f_idx] += \
                    ##          0.001 * np.random.normal(size=(w_non0_len,))
            else:
                raise ValueError("Do not know how to treat ties=%r"
                                 % (self.params.ties,))

        # save the weights
        self.__weights_all = w
        self.__weights = w[:dataset.nfeatures, :]

        if self.ca.is_enabled('feature_ids'):
            self.ca.feature_ids = np.where(np.max(np.abs(w[:dataset.nfeatures, :]),
                                             axis=1)>0)[0]

        # and a bias
        if self.params.has_bias:
            self.__biases = w[-1, :]

        if __debug__:
            debug('SMLR', "train finished in %d cycles on data.shape=%s " %
                  (cycles, X.shape) +
                  "min:max(data)=%f:%f, got min:max(w)=%f:%f" %
                  (np.min(X), np.max(X), np.min(w), np.max(w)))

    def _unsparsify_weights(self, samples, weights):
        """Unsparsify weights via least squares regression."""
        # allocate for the new weights
        new_weights = np.zeros(weights.shape, dtype=np.double)

        # get the sample data we're predicting and the sum squared
        # total variance
        b = samples
        sst = np.power(b - b.mean(0),2).sum(0)

        # loop over each column
        for i in range(weights.shape[1]):
            w = weights[:,i]

            # get the nonzero ind
            ind = w!=0

            # get the features with non-zero weights
            a = b[:,ind]

            # predict all the data with the non-zero features
            betas = np.linalg.lstsq(a,b)[0]

            # determine the R^2 for each feature based on the sum
            # squared prediction error
            f = np.dot(a,betas)
            sse = np.power((b-f),2).sum(0)
            rsquare = np.zeros(sse.shape,dtype=sse.dtype)
            gind = sst>0
            rsquare[gind] = 1-(sse[gind]/sst[gind])

            # derrive new weights by combining the betas and weights
            # scaled by the rsquare
            new_weights[:,i] = np.dot(w[ind],betas)*rsquare

        # take the tails
        tozero = np.abs(new_weights) < self.params.std_to_keep*np.std(new_weights)
        orig_zero = weights==0.0
        if orig_zero.sum() < tozero.sum():
            # should not end up with fewer than start
            tozero = orig_zero
        new_weights[tozero] = 0.0

        debug('SMLR_', "Start nonzero: %d; Finish nonzero: %d" % \
              ((weights!=0).sum(), (new_weights!=0).sum()))

        return new_weights


    ##REF: Name was automagically refactored
    def _get_feature_ids(self):
        """Return ids of the used features
        """
        return np.where(np.max(np.abs(self.__weights), axis=1)>0)[0]

    @accepts_dataset_as_samples
    def _predict(self, data):
        """Predict the output for the provided data.
        """
        # see if we are adding a bias term
        if self.params.has_bias:
            # append the bias term to the features
            data = np.hstack((data,
                             np.ones((data.shape[0], 1), dtype=data.dtype)))

        # append the zeros column to the weights if necessary
        if self.params.fit_all_weights:
            w = self.__weights_all
        else:
            w = np.hstack((self.__weights_all,
                          np.zeros((self.__weights_all.shape[0], 1))))

        # determine the probability values for making the prediction
        dot_prod = np.dot(data, w)
        E = np.exp(dot_prod)
        if self.__ties is not None:
            # 1e-5 should be adequate since anyways this is done
            # already after exponentiation
            E[:, self.__ties] += \
                 1e-5 * np.random.normal(size=(len(E), len(self.__ties)))
        S = np.sum(E, 1)

        if __debug__:
            debug('SMLR', "predict on data.shape=%s min:max(data)=%f:%f " %
                  (`data.shape`, np.min(data), np.max(data)) +
                  "min:max(w)=%f:%f min:max(dot_prod)=%f:%f min:max(E)=%f:%f" %
                  (np.min(w), np.max(w), np.min(dot_prod), np.max(dot_prod),
                   np.min(E), np.max(E)))
        values = E / S[:, np.newaxis] #.repeat(E.shape[1], axis=1)
        self.ca.estimates = values

        # generate predictions
        predictions = np.asarray([self._ulabels[np.argmax(vals)]
                                 for vals in values])
        # no need to assign conditional attribute here -- would be done
        # in Classifier._postpredict anyway
        #self.predictions = predictions

        return predictions


    ##REF: Name was automagically refactored
    def get_sensitivity_analyzer(self, **kwargs):
        """Returns a sensitivity analyzer for SMLR."""
        return SMLRWeights(self, **kwargs)


    biases = property(lambda self: self.__biases)
    weights = property(lambda self: self.__weights)



class SMLRWeights(Sensitivity):
    """`SensitivityAnalyzer` that reports the weights SMLR trained
    on a given `Dataset`.

    By default SMLR provides multiple weights per feature (one per label in
    training dataset). By default, all weights are combined into a single
    sensitivity value. Please, see the `FeaturewiseMeasure` constructor
    arguments how to custmize this behavior.
    """

    _LEGAL_CLFS = [ SMLR ]


    def _call(self, dataset=None):
        """Extract weights from SMLR classifier.

        SMLR always has weights available, so nothing has to be computed here.
        """
        clf = self.clf
        # transpose to have the number of features on the second axis
        # (as usual)
        weights = clf.weights.T

        if __debug__:
            debug('SMLR',
                  "Extracting weights for %d-class SMLR" %
                  (len(weights) + 1) +
                  "Result: min=%f max=%f" %\
                  (np.min(weights), np.max(weights)))

        # limit the labels to the number of sensitivity sets, to deal
        # with the case of `fit_all_weights=False`
        ds = Dataset(weights,
                     sa={clf.get_space(): clf._ulabels[:len(weights)]})

        if clf.params.has_bias:
            ds.sa['biases'] = clf.biases
        return ds


########NEW FILE########
__FILENAME__ = stats
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Estimator for classifier error distributions."""

from __future__ import with_statement   # Let's start using with

__docformat__ = 'restructuredtext'

import warnings

import numpy as np

from mvpa2.base import externals, warning
from mvpa2.base.state import ClassWithCollections, ConditionalAttribute
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.base.types import is_datasetlike
from mvpa2.datasets import Dataset

if __debug__:
    from mvpa2.base import debug

if externals.exists('scipy'):
    import scipy.stats.distributions as ssd

    def _auto_rcdf(dist):
        dist_check = dist

        # which to check for continuous/discrete
        if isinstance(dist, ssd.rv_frozen):
            dist_check = dist.dist
        if isinstance(dist_check, ssd.rv_discrete):
            # we need to count the exact matches
            rcdf = lambda x, *args: 1 - dist.cdf(x, *args) + dist.pmf(x, *args)
        elif isinstance(dist_check, ssd.rv_continuous):
            # for continuous it is just as good
            rcdf = lambda x, *args: 1 - dist.cdf(x, *args)
        elif isinstance(dist_check, Nonparametric):
            rcdf = dist.rcdf
        else:
            raise ValueError("Do not know how to get 'right cdf' for %s" % (dist,))
        return rcdf

else:
    def _auto_rcdf(dist):
        if isinstance(dist, Nonparametric):
            rcdf = dist.rcdf
        else:
            raise ValueError("Do not know how to get 'right cdf' for %s" % (dist,))
        return rcdf

class Nonparametric(object):
    """Non-parametric 1d distribution -- derives cdf based on stored values.

    Introduced to complement parametric distributions present in scipy.stats.
    """

    def __init__(self, dist_samples, correction='clip'):
        """
        Parameters
        ----------
        dist_samples : ndarray
          Samples to be used to assess the distribution.
        correction : {'clip'} or None, optional
          Determines the behavior when .cdf is queried.  If None -- no
          correction is made.  If 'clip' -- values are clipped to lie
          in the range [1/(N+2), (N+1)/(N+2)] (simply because
          non-parametric assessment lacks the power to resolve with
          higher precision in the tails, so 'imagery' samples are
          placed in each of the two tails).
        """
        self._dist_samples = np.ravel(dist_samples)
        self._correction = correction

    def __repr__(self):
        return '%s(%r%s)' % (
            self.__class__.__name__,
            self._dist_samples,
            ('', ', correction=%r' % self._correction)
              [int(self._correction != 'clip')])

    @staticmethod
    def fit(dist_samples):
        return [dist_samples]

    def _cdf(self, x, operator):
        """Helper function to compute cdf proper or reverse (i.e. going from the right tail)
        """
        res = operator(x)
        if self._correction == 'clip':
            nsamples = len(self._dist_samples)
            np.clip(res, 1.0/(nsamples+2), (nsamples+1.0)/(nsamples+2), res)
        elif self._correction is None:
            pass
        else:
            raise ValueError, \
                  '%r is incorrect value for correction parameter of %s' \
                  % (self._correction, self.__class__.__name__)
        return res


    def cdf(self, x):
        """Returns the cdf value at `x`.
        """
        return self._cdf(x,
                         np.vectorize(lambda v: (self._dist_samples <= v).mean()))

    def rcdf(self, x):
        """Returns cdf of reversed distribution (i.e. if integrating from right tail)

        Necessary for hypothesis testing in the right tail.
        It is really just a 1 - cdf(x) + pmf(x) == sf(x)+pmf(x) for a discrete distribution
        """
        return self._cdf(x,
                         np.vectorize(lambda v: (self._dist_samples >= v).mean()))


def _pvalue(x, cdf_func, rcdf_func, tail, return_tails=False, name=None):
    """Helper function to return p-value(x) given cdf and tail

    Parameters
    ----------
    cdf_func : callable
      Function to be used to derive cdf values for x
    tail : str ('left', 'right', 'any', 'both')
      Which tail of the distribution to report. For 'any' and 'both'
      it chooses the tail it belongs to based on the comparison to
      p=0.5. In the case of 'any' significance is taken like in a
      one-tailed test.
    return_tails : bool
      If True, a tuple return (pvalues, tails), where tails contain
      1s if value was from the right tail, and 0 if the value was
      from the left tail.
    """
    is_scalar = np.isscalar(x)
    if is_scalar:
        x = [x]

    def stability_assurance(cdf):
        if __debug__ and 'CHECK_STABILITY' in debug.active:
            cdf_min, cdf_max = np.min(cdf), np.max(cdf)
            if cdf_min < 0 or cdf_max > 1.0:
                s = ('', ' for %s' % name)[int(name is not None)]
                warning('Stability check of cdf %s failed%s. Min=%s, max=%s' % \
                        (cdf_func, s, cdf_min, cdf_max))

    if tail == 'left':
        pvalues = cdf_func(x)
        if return_tails:
            right_tail = np.zeros(pvalues.shape, dtype=bool)
        stability_assurance(pvalues)
    elif tail == 'right':
        pvalues = rcdf_func(x)
        if return_tails:
            right_tail = np.ones(pvalues.shape, dtype=bool)
        stability_assurance(pvalues)
    elif tail in ('any', 'both'):
        pvalues = cdf_func(x)
        right_tail = (pvalues >= 0.5)

        if np.any(right_tail):
            # we must compute them all first ATM since otherwise
            # it would not work for "multiple" features with independent
            # distributions
            rcdf = rcdf_func(x)
            # and then assign the "interesting" ones
            pvalues[right_tail] = rcdf[right_tail]
        if tail == 'both':
            # we need report the area under both tails
            # XXX this is only meaningful for symmetric distributions
            pvalues *= 2

    # no escape but to assure that CDF is in the right range. Some
    # distributions from scipy tend to jump away from [0,1]
    # yoh: made inplace operation whenever RF into this function
    np.clip(pvalues, 0, 1.0, pvalues)

    # Assure that NaNs didn't get significant value
    # TODO: should be moved into corresponding cdf/rcdf computation
    #       since that is where x->pvalues relation can be assured
    x_nans = np.isnan(x)
    if np.any(x_nans):
        if (isinstance(x, np.ndarray) and x.shape == pvalues.shape) \
          or (pvalues.ndim == 1 and len(x) == len(pvalues)):
            pvalues[x_nans] = 1.0
        else:
            raise ValueError(
                "Input had NaN's but of different shape %s than output "
                "pvalues %s, so cannot deduce what needs to be done. Please "
                "make your input cleaner" % (x.shape, pvalues.shape))

    if is_scalar:
        pvalues = pvalues[0]

    if return_tails:
        return (pvalues, right_tail)
    else:
        return pvalues


class NullDist(ClassWithCollections):
    """Base class for null-hypothesis testing.

    """

    # Although base class is not benefiting from ca, derived
    # classes do (MCNullDist). For the sake of avoiding multiple
    # inheritance and associated headache -- let them all be ClassWithCollections,
    # performance hit should be negligible in most of the scenarios
    _ATTRIBUTE_COLLECTIONS = ['ca']

    def __init__(self, tail='both', **kwargs):
        """
        Parameters
        ----------
        tail : {'left', 'right', 'any', 'both'}
          Which tail of the distribution to report. For 'any' and 'both'
          it chooses the tail it belongs to based on the comparison to
          p=0.5. In the case of 'any' significance is taken like in a
          one-tailed test.
        """
        ClassWithCollections.__init__(self, **kwargs)

        self._set_tail(tail)

    def __repr__(self, prefixes=[]):
        return super(NullDist, self).__repr__(
            prefixes=["tail=%s" % `self.__tail`] + prefixes)


    ##REF: Name was automagically refactored
    def _set_tail(self, tail):
        # sanity check
        if tail not in ('left', 'right', 'any', 'both'):
            raise ValueError, 'Unknown value "%s" to `tail` argument.' \
                  % tail
        self.__tail = tail


    def fit(self, measure, ds):
        """Implement to fit the distribution to the data."""
        raise NotImplementedError


    def cdf(self, x):
        """Implementations return the value of the cumulative distribution
        function.
        """
        raise NotImplementedError

    def rcdf(self, x):
        """Implementations return the value of the reverse cumulative distribution
        function.
        """
        raise NotImplementedError

    def dists(self):
        """Implementations returns a sequence of the ``dist_class`` instances
        that were used to fit the distribution.
        """
        raise NotImplementedError

    def p(self, x, return_tails=False, **kwargs):
        """Returns the p-value for values of `x`.
        Returned values are determined left, right, or from any tail
        depending on the constructor setting.

        In case a `FeaturewiseMeasure` was used to estimate the
        distribution the method returns an array. In that case `x` can be
        a scalar value or an array of a matching shape.
        """
        peas = _pvalue(x, self.cdf, self.rcdf, self.__tail, return_tails=return_tails,
                       **kwargs)
        if is_datasetlike(x):
            # return the p-values in a dataset as well and assign the input
            # dataset attributes to the return dataset too
            pds = x.copy(deep=False)
            if return_tails:
                pds.samples = peas[0]
                return pds, peas[1]
            else:
                pds.samples = peas
                return pds
        return peas

    tail = property(fget=lambda x:x.__tail, fset=_set_tail)


class MCNullDist(NullDist):
    """Null-hypothesis distribution is estimated from randomly permuted data labels.

    The distribution is estimated by calling fit() with an appropriate
    `Measure` or `TransferError` instance and a training and a
    validation dataset (in case of a `TransferError`). For a customizable
    amount of cycles the training data labels are permuted and the
    corresponding measure computed. In case of a `TransferError` this is the
    error when predicting the *correct* labels of the validation dataset.

    The distribution can be queried using the `cdf()` method, which can be
    configured to report probabilities/frequencies from `left` or `right` tail,
    i.e. fraction of the distribution that is lower or larger than some
    critical value.

    This class also supports `FeaturewiseMeasure`. In that case `cdf()`
    returns an array of featurewise probabilities/frequencies.
    """

    _DEV_DOC = """
    TODO automagically decide on the number of samples/permutations needed
    Caution should be paid though since resultant distributions might be
    quite far from some conventional ones (e.g. Normal) -- it is expected to
    them to be bimodal (or actually multimodal) in many scenarios.
    """

    dist_samples = ConditionalAttribute(enabled=False,
                                 doc='Samples obtained for each permutation')
    skipped = ConditionalAttribute(enabled=True,
                  doc='# of the samples which were skipped because '
                      'measure has failed to evaluated at them')

    def __init__(self, permutator, dist_class=Nonparametric, measure=None,
                 **kwargs):
        """Initialize Monte-Carlo Permutation Null-hypothesis testing

        Parameters
        ----------
        permutator : Node
          Node instance that generates permuted datasets.
        dist_class : class
          This can be any class which provides parameters estimate
          using `fit()` method to initialize the instance, and
          provides `cdf(x)` method for estimating value of x in CDF.
          All distributions from SciPy's 'stats' module can be used.
        measure : Measure or None
          Optional measure that is used to compute results on permuted
          data. If None, a measure needs to be passed to ``fit()``.
        """
        NullDist.__init__(self, **kwargs)

        self._dist_class = dist_class
        self._dist = []                 # actual distributions
        self._measure = measure

        self.__permutator = permutator

    def __repr__(self, prefixes=[]):
        prefixes_ = ["%s" % self.__permutator]
        if self._dist_class != Nonparametric:
            prefixes_.insert(0, 'dist_class=%r' % (self._dist_class,))
        return super(MCNullDist, self).__repr__(
            prefixes=prefixes_ + prefixes)


    def fit(self, measure, ds):
        """Fit the distribution by performing multiple cycles which repeatedly
        permuted labels in the training dataset.

        Parameters
        ----------
        measure: Measure or None
          A measure used to compute the results from shuffled data. Can be None
          if a measure instance has been provided to the constructor.
        ds: `Dataset` which gets permuted and used to compute the
          measure/transfer error multiple times.
        """
        # TODO: place exceptions separately so we could avoid circular imports
        from mvpa2.base.learner import LearnerError

        # prefer the already assigned measure over anything the was passed to
        # the function.
        # XXX that is a bit awkward but is necessary to keep the code changes
        # in the rest of PyMVPA minimal till this behavior become mandatory
        if not self._measure is None:
            measure = self._measure
            measure.untrain()

        dist_samples = []
        """Holds the values for randomized labels."""

        # estimate null-distribution
        # TODO this really needs to be more clever! If data samples are
        # shuffled within a class it really makes no difference for the
        # classifier, hence the number of permutations to estimate the
        # null-distribution of transfer errors can be reduced dramatically
        # when the *right* permutations (the ones that matter) are done.
        skipped = 0                     # # of skipped permutations
        for p, permuted_ds in enumerate(self.__permutator.generate(ds)):
            # new permutation all the time
            # but only permute the training data and keep the testdata constant
            #
            if __debug__:
                debug('STATMC', "Doing %i permutations: %i" \
                      % (self.__permutator.count, p+1), cr=True)

            # compute and store the measure of this permutation
            # assume it has `TransferError` interface
            try:
                res = measure(permuted_ds)
                dist_samples.append(res.samples)
            except LearnerError, e:
                if __debug__:
                    debug('STATMC', " skipped", cr=True)
                warning('Failed to obtain value from %s due to %s.  Measurement'
                        ' was skipped, which could lead to unstable and/or'
                        ' incorrect assessment of the null_dist' % (measure, e))
                skipped += 1
                continue

        self.ca.skipped = skipped

        if __debug__:
            debug('STATMC', ' Skipped: %d permutations' % skipped)

        if not len(dist_samples) and skipped > 0:
            raise RuntimeError(
                'Failed to obtain any value from %s. %d measurements were '
                'skipped. Check above warnings, and your code/data'
                % (measure, skipped))
        # store samples as (npermutations x nsamples x nfeatures)
        dist_samples = np.asanyarray(dist_samples)
        # for the ca storage use a dataset with
        # (nsamples x nfeatures x npermutations) to make it compatible with the
        # result dataset of the measure
        self.ca.dist_samples = Dataset(np.rollaxis(dist_samples,
                                       0, len(dist_samples.shape)))

        # fit distribution per each element

        # to decide either it was done on scalars or vectors
        shape = dist_samples.shape
        nshape = len(shape)
        # if just 1 dim, original data was scalar, just create an
        # artif dimension for it
        if nshape == 1:
            dist_samples = dist_samples[:, np.newaxis]

        # fit per each element.
        # XXX could be more elegant? may be use np.vectorize?
        dist_samples_rs = dist_samples.reshape((shape[0], -1))
        dist = []
        for samples in dist_samples_rs.T:
            params = self._dist_class.fit(samples)
            if __debug__ and 'STAT__' in debug.active:
                debug('STAT', 'Estimated parameters for the %s are %s'
                      % (self._dist_class, str(params)))
            dist.append(self._dist_class(*params))
        self._dist = dist


    def _cdf(self, x, cdf_func):
        """Return value of the cumulative distribution function at `x`.
        """
        if self._dist is None:
            # XXX We might not want to descriminate that way since
            # usually generators also have .cdf where they rely on the
            # default parameters. But then what about Nonparametric
            raise RuntimeError, "Distribution has to be fit first"

        is_scalar = np.isscalar(x)
        if is_scalar:
            x = [x]

        x = np.asanyarray(x)
        xshape = x.shape
        # assure x is a 1D array now
        x = x.reshape((-1,))

        if len(self._dist) != len(x):
            raise ValueError, 'Distribution was fit for structure with %d' \
                  ' elements, whenever now queried with %d elements' \
                  % (len(self._dist), len(x))

        # extract cdf values per each element
        if cdf_func == 'cdf':
            cdfs = [ dist.cdf(v) for v, dist in zip(x, self._dist) ]
        elif cdf_func == 'rcdf':
            cdfs = [ _auto_rcdf(dist)(v) for v, dist in zip(x, self._dist) ]
        else:
            raise ValueError
        return np.array(cdfs).reshape(xshape)

    def cdf(self, x):
        return self._cdf(x, 'cdf')

    def rcdf(self, x):
        return self._cdf(x, 'rcdf')

    def dists(self):
        return self._dist

    def clean(self):
        """Clean stored distributions

        Storing all of the distributions might be too expensive
        (e.g. in case of Nonparametric), and the scope of the object
        might be too broad to wait for it to be destroyed. Clean would
        bind dist_samples to empty list to let gc revoke the memory.
        """
        self._dist = []



class FixedNullDist(NullDist):
    """Proxy/Adaptor class for SciPy distributions.

    All distributions from SciPy's 'stats' module can be used with this class.

    Examples
    --------

    >>> import numpy as np
    >>> from scipy import stats
    >>> from mvpa2.clfs.stats import FixedNullDist
    >>>
    >>> dist = FixedNullDist(stats.norm(loc=2, scale=4), tail='left')
    >>> dist.p(2)
    0.5
    >>>
    >>> dist.cdf(np.arange(5))
    array([ 0.30853754,  0.40129367,  0.5       ,  0.59870633,  0.69146246])
    >>>
    >>> dist = FixedNullDist(stats.norm(loc=2, scale=4), tail='right')
    >>> dist.p(np.arange(5))
    array([ 0.69146246,  0.59870633,  0.5       ,  0.40129367,  0.30853754])

    """
    def __init__(self, dist, **kwargs):
        """
        Parameters
        ----------
        dist : distribution object
          This can be any object the has a `cdf()` method to report the
          cumulative distribition function values.
        """
        NullDist.__init__(self, **kwargs)

        self._dist = dist
        # assign corresponding rcdf overloading NotImplemented one of
        # base class
        self.rcdf = _auto_rcdf(dist)

    def fit(self, measure, ds):
        """Does nothing since the distribution is already fixed."""
        pass


    def cdf(self, x):
        """Return value of the cumulative distribution function at `x`.
        """
        return self._dist.cdf(x)


    def __repr__(self, prefixes=[]):
        prefixes_ = ["dist=%s" % `self._dist`]
        return super(FixedNullDist, self).__repr__(
            prefixes=prefixes_ + prefixes)


class AdaptiveNullDist(FixedNullDist):
    """Adaptive distribution which adjusts parameters according to the data

    WiP: internal implementation might change
    """
    def fit(self, measure, wdata, vdata=None):
        """Cares about dimensionality of the feature space in measure
        """

        try:
            nfeatures = len(measure.feature_ids)
        except ValueError:              # XXX
            nfeatures = np.prod(wdata.shape[1:])

        dist_gen = self._dist
        if not hasattr(dist_gen, 'fit'): # frozen already
            dist_gen = dist_gen.dist     # rv_frozen at least has it ;)

        args, kwargs = self._adapt(nfeatures, measure, wdata, vdata)
        if __debug__:
            debug('STAT', 'Adapted parameters for %s to be %s, %s'
                  % (dist_gen, args, kwargs))
        self._dist = dist_gen(*args, **kwargs)


    def _adapt(self, nfeatures, measure, wdata, vdata=None):
        raise NotImplementedError


class AdaptiveRDist(AdaptiveNullDist):
    """Adaptive rdist: params are (nfeatures-1, 0, 1)
    """

    def _adapt(self, nfeatures, measure, wdata, vdata=None):
        return (nfeatures-1, 0, 1), {}

    # XXX: RDist has stability issue, just run
    #  python -c "import scipy.stats; print scipy.stats.rdist(541,0,1).cdf(0.72)"
    # to get some improbable value, so we need to take care about that manually
    # here
    def cdf(self, x):
        cdf_ = self._dist.cdf(x)
        bad_values = np.where(np.abs(cdf_)>1)
        # XXX there might be better implementation (faster/elegant) using np.clip,
        #     the only problem is that instability results might flip the sign
        #     arbitrarily
        if len(bad_values[0]):
            # in this distribution we have mean at 0, so we can take that easily
            # into account
            cdf_bad = cdf_[bad_values]
            x_bad = x[bad_values]
            cdf_bad[x_bad < 0] = 0.0
            cdf_bad[x_bad >= 0] = 1.0
            cdf_[bad_values] = cdf_bad
        return cdf_


class AdaptiveNormal(AdaptiveNullDist):
    """Adaptive Normal Distribution: params are (0, sqrt(1/nfeatures))
    """

    def _adapt(self, nfeatures, measure, wdata, vdata=None):
        return (0, 1.0/np.sqrt(nfeatures)), {}


if externals.exists('scipy'):
    from mvpa2.support.scipy.stats import scipy
    from scipy.stats import kstest

    """
    Thoughts:

    So we can use `scipy.stats.kstest` (Kolmogorov-Smirnov test) to
    check/reject H0 that samples come from a given distribution. But
    since it is based on a full range of data, we might better of with
    some ad-hoc checking by the detection power of the values in the
    tail of a tentative distribution.

    """

    # We need a way to fixate estimation of some parameters
    # (e.g. mean) so lets create a simple proxy, or may be class
    # generator later on, which would take care about punishing change
    # from the 'right' arguments

    import scipy

    class rv_semifrozen(object):
        """Helper proxy-class to fit distribution when some parameters are known

        It is an ugly hack with snippets of code taken from scipy, which is
        Copyright (c) 2001, 2002 Enthought, Inc.
        and is distributed under BSD license
        http://www.scipy.org/License_Compatibility
        """

        def __init__(self, dist, loc=None, scale=None, args=None):
            """
            Parameters
            ----------
            dist : rv_generic
              Distribution for which to freeze some of the parameters
            loc : array-like, optional
              Location parameter (default=0)
            scale : array-like, optional
              Scale parameter (default=1)
            args : iterable, optional
               Additional arguments to be passed to dist.

            Raises
            ------
            ValueError
              Arguments number mismatch
            """
            self._dist = dist
            # loc and scale
            theta = (loc, scale)
            # args
            Narg_ = dist.numargs
            if args is not None:
                Narg = len(args)
                if Narg > Narg_:
                    raise ValueError, \
                          'Distribution %s has only %d arguments. Got %d' \
                          % (dist, Narg_, Narg)
                args += (None,) * (Narg_ - Narg)
            else:
                args = (None,) * Narg_

            args_i = [i for i,v in enumerate(args) if v is None]
            self._fargs = (list(args+theta), args_i)
            """Arguments which should get some fixed value"""


        def __call__(self, *args, **kwargs):
            """Upon call mimic call to get actual rv_frozen distribution
            """
            return self._dist(*args, **kwargs)


        def nnlf(self, theta, x):
            # - sum (log pdf(x, theta),axis=0)
            #   where theta are the parameters (including loc and scale)
            #
            fargs, fargs_i = self._fargs
            try:
                i=-1
                if fargs[-1] is not None:
                    scale = fargs[-1]
                else:
                    scale = theta[i]
                    i -= 1

                if fargs[-2] is not None:
                    loc = fargs[-2]
                else:
                    loc = theta[i]
                    i -= 1

                args = theta[:i+1]
                # adjust args if there were fixed
                for i, a in zip(fargs_i, args):
                    fargs[i] = a
                args = fargs[:-2]

            except IndexError:
                raise ValueError, "Not enough input arguments."
            if not self._argcheck(*args) or scale <= 0:
                return np.inf
            x = np.asarray((x-loc) / scale)
            cond0 = (x <= self.a) | (x >= self.b)
            if (np.any(cond0)):
                return np.inf
            else:
                return self._nnlf(x, *args) + len(x)*np.log(scale)

        def fit(self, data, *args, **kwds):
            loc0, scale0 = map(kwds.get, ['loc', 'scale'], [0.0, 1.0])
            fargs, fargs_i = self._fargs
            Narg = len(args)
            Narg_ = self.numargs
            if Narg != Narg_:
                if Narg > Narg_:
                    raise ValueError, "Too many input arguments."
                else:
                    args += (1.0,)*(self.numargs-Narg)

            # Provide only those args which are not fixed, and
            # append location and scale (if not fixed) at the end
            if len(fargs_i) != Narg_:
                x0 = tuple([args[i] for i in fargs_i])
            else:
                x0 = args
            if fargs[-2] is None:
                x0 = x0 + (loc0,)
            if fargs[-1] is None:
                x0 = x0 + (scale0,)

            opt_x = scipy.optimize.fmin(
                self.nnlf, x0, args=(np.ravel(data),), disp=0)

            # reconstruct back
            i = 0
            loc, scale = fargs[-2:]
            if fargs[-1] is None:
                i -= 1
                scale = opt_x[i]
            if fargs[-2] is None:
                i -= 1
                loc = opt_x[i]

            # assign those which weren't fixed
            for i in fargs_i:
                fargs[i] = opt_x[i]

            #raise ValueError
            opt_x = np.hstack((fargs[:-2], (loc, scale)))
            return opt_x


        def __setattr__(self, a, v):
            if not a in ['_dist', '_fargs', 'fit', 'nnlf']:
                self._dist.__setattr__(a, v)
            else:
                object.__setattr__(self, a, v)


        def __getattribute__(self, a):
            """We need to redirect all queries correspondingly
            """
            if not a in ['_dist', '_fargs', 'fit', 'nnlf']:
                return getattr(self._dist, a)
            else:
                return object.__getattribute__(self, a)



    ##REF: Name was automagically refactored
    def match_distribution(data, nsamples=None, loc=None, scale=None,
                          args=None, test='kstest', distributions=None,
                          **kwargs):
        """Determine best matching distribution.

        Can be used for 'smelling' the data, as well to choose a
        parametric distribution for data obtained from non-parametric
        testing (e.g. `MCNullDist`).

        WiP: use with caution, API might change

        Parameters
        ----------
        data : np.ndarray
          Array of the data for which to deduce the distribution. It has
          to be sufficiently large to make a reliable conclusion
        nsamples : int or None
          If None -- use all samples in data to estimate parametric
          distribution. Otherwise use only specified number randomly selected
          from data.
        loc : float or None
          Loc for the distribution (if known)
        scale : float or None
          Scale for the distribution (if known)
        test : str
          What kind of testing to do. Choices:
           'p-roc'
             detection power for a given ROC. Needs two
             parameters: `p=0.05` and `tail='both'`
           'kstest'
             'full-body' distribution comparison. The best
             choice is made by minimal reported distance after estimating
             parameters of the distribution. Parameter `p=0.05` sets
             threshold to reject null-hypothesis that distribution is the
             same.
             **WARNING:** older versions (e.g. 0.5.2 in etch) of scipy have
             incorrect kstest implementation and do not function properly.
        distributions : None or list of str or tuple(str, dict)
          Distributions to check. If None, all known in scipy.stats
          are tested. If distribution is specified as a tuple, then
          it must contain name and additional parameters (name, loc,
          scale, args) in the dictionary. Entry 'scipy' adds all known
          in scipy.stats.
        **kwargs
          Additional arguments which are needed for each particular test
          (see above)

        Examples
        --------
        >>> from mvpa2.clfs.stats import match_distribution
        >>> data = np.random.normal(size=(1000,1));
        >>> matches = match_distribution(
        ...   data,
        ...   distributions=['rdist',
        ...                  ('rdist', {'name':'rdist_fixed',
        ...                             'loc': 0.0,
        ...                             'args': (10,)})],
        ...   nsamples=30, test='p-roc', p=0.05)

        """

        # Handle parameters
        _KNOWN_TESTS = ['p-roc', 'kstest']
        if not test in _KNOWN_TESTS:
            raise ValueError, 'Unknown kind of test %s. Known are %s' \
                  % (test, _KNOWN_TESTS)

        data = np.ravel(data)
        # data sampled
        if nsamples is not None:
            if __debug__:
                debug('STAT', 'Sampling %d samples from data for the ' \
                      'estimation of the distributions parameters' % nsamples)
            indexes_selected = (np.random.sample(nsamples)*len(data)).astype(int)
            data_selected = data[indexes_selected]
        else:
            indexes_selected = np.arange(len(data))
            data_selected = data

        p_thr = kwargs.get('p', 0.05)
        if test == 'p-roc':
            tail = kwargs.get('tail', 'both')
            npd = Nonparametric(data)
            data_p = _pvalue(data, npd.cdf, npd.rcdf, tail)
            data_p_thr = np.abs(data_p) <= p_thr
            true_positives = np.sum(data_p_thr)
            if true_positives == 0:
                raise ValueError, "Provided data has no elements in non-" \
                      "parametric distribution under p<=%.3f. Please " \
                      "increase the size of data or value of p" % p_thr
            if __debug__:
                debug('STAT_', 'Number of positives in non-parametric '
                      'distribution is %d' % true_positives)

        if distributions is None:
            distributions = ['scipy']

        # lets see if 'scipy' entry was in there
        try:
            scipy_ind = distributions.index('scipy')
            distributions.pop(scipy_ind)
            sp_dists = ssd.__all__
            sp_version = externals.versions['scipy']
            if sp_version >= '0.9.0':
                for d_ in ['ncf']:
                    if d_ in sp_dists:
                        warning("Not considering %s distribution because of "
                                "known issues in scipy %s" % (d_, sp_version))
                        _ = sp_dists.pop(sp_dists.index(d_))
            distributions += sp_dists
        except ValueError:
            pass

        results = []
        for d in distributions:
            dist_gen, loc_, scale_, args_ = None, loc, scale, args
            if isinstance(d, basestring):
                dist_gen = d
                dist_name = d
            elif isinstance(d, tuple):
                if not (len(d)==2 and isinstance(d[1], dict)):
                    raise ValueError,\
                          "Tuple specification of distribution must be " \
                          "(d, {params}). Got %s" % (d,)
                dist_gen = d[0]
                loc_ = d[1].get('loc', loc)
                scale_ = d[1].get('scale', scale)
                args_ = d[1].get('args', args)
                dist_name = d[1].get('name', str(dist_gen))
            else:
                dist_gen = d
                dist_name = str(d)

            # perform actions which might puke for some distributions
            try:
                dist_gen_ = getattr(scipy.stats, dist_gen)
                # specify distribution 'optimizer'. If loc or scale was provided,
                # use home-brewed rv_semifrozen
                if args_ is not None or loc_ is not None or scale_ is not None:
                    dist_opt = rv_semifrozen(dist_gen_,
                                             loc=loc_, scale=scale_, args=args_)
                else:
                    dist_opt = dist_gen_

                if __debug__:
                    debug('STAT__',
                          'Fitting %s distribution %r on data of size %s',
                          (dist_name, dist_opt, data_selected.shape))
                # suppress the warnings which might pop up while
                # matching "inappropriate" distributions
                with warnings.catch_warnings(record=True) as w:
                    dist_params = dist_opt.fit(data_selected)
                if __debug__:
                    debug('STAT__',
                          'Got distribution parameters %s for %s'
                          % (dist_params, dist_name))
                if test == 'p-roc':
                    cdf_func = lambda x: dist_gen_.cdf(x, *dist_params)
                    rcdf_func = _auto_rcdf(dist_gen_)
                    # We need to compare detection under given p
                    cdf_p = np.abs(_pvalue(data, cdf_func, rcdf_func, tail, name=dist_gen))
                    cdf_p_thr = cdf_p <= p_thr
                    D, p = (np.sum(np.abs(data_p_thr - cdf_p_thr))*1.0/true_positives, 1)
                    if __debug__:
                        res_sum = 'D=%.2f' % D
                elif test == 'kstest':
                    D, p = kstest(data, dist_gen, args=dist_params)
                    if __debug__:
                        res_sum = 'D=%.3f p=%.3f' % (D, p)
            except (TypeError, ValueError, AttributeError,
                    NotImplementedError), e:#Exception, e:
                if __debug__:
                    debug('STAT__',
                          'Testing for %s distribution failed due to %s',
                          (d, e))
                continue

            if p > p_thr and not np.isnan(D):
                results += [ (D, dist_gen, dist_name, dist_params) ]
                if __debug__:
                    debug('STAT_',
                          'Tested %s distribution: %s', (dist_name, res_sum))
            else:
                if __debug__:
                    debug('STAT__', 'Cannot consider %s dist. with %s',
                          (d, res_sum))
                continue

        # sort in ascending order, so smaller is better
        results.sort(key=lambda x:x[0])

        if __debug__ and 'STAT' in debug.active:
            # find the best and report it
            nresults = len(results)
            sresult = lambda r:'%s(%s)=%.2f' % (r[1],
                                                ', '.join(map(str, r[3])),
                                                r[0])
            if nresults > 0:
                nnextbest = min(2, nresults-1)
                snextbest = ', '.join(map(sresult, results[1:1+nnextbest]))
                debug('STAT', 'Best distribution %s. Next best: %s'
                          % (sresult(results[0]), snextbest))
            else:
                debug('STAT', 'Could not find suitable distribution')

        # return all the results
        return results


    if externals.exists('pylab'):
        import pylab as pl

        ##REF: Name was automagically refactored
        def plot_distribution_matches(data, matches, nbins=31, nbest=5,
                                    expand_tails=8, legend=2, plot_cdf=True,
                                    p=None, tail='both'):
            """Plot best matching distributions

            Parameters
            ----------
            data : np.ndarray
              Data which was used to obtain the matches
            matches : list of tuples
              Sorted matches as provided by match_distribution
            nbins : int
              Number of bins in the histogram
            nbest : int
              Number of top matches to plot
            expand_tails : int
              How many bins away to add to parametrized distributions
              plots
            legend : int
              Either to provide legend and statistics in the legend.
              1 -- just lists distributions.
              2 -- adds distance measure
              3 -- tp/fp/fn in the case if p is provided
            plot_cdf : bool
              Either to plot cdf for data using non-parametric distribution
            p : float or None
              If not None, visualize null-hypothesis testing (given p).
              Bars in the histogram which fall under given p are colored
              in red. False positives and false negatives are marked as
              triangle up and down symbols correspondingly
            tail : ('left', 'right', 'any', 'both')
              If p is not None, the choise of tail for null-hypothesis
              testing

            Returns
            -------
            histogram
            list of lines
            """

            # API changed since v0.99.0-641-ga7c2231
            halign = externals.versions['matplotlib'] >= '1.0.0' \
                     and 'mid' or 'center'
            hist = pl.hist(data, nbins, normed=1, align=halign)
            data_range = [np.min(data), np.max(data)]

            # x's
            x = hist[1]
            dx = x[expand_tails] - x[0] # how much to expand tails by
            x = np.hstack((x[:expand_tails] - dx, x, x[-expand_tails:] + dx))

            nonparam = Nonparametric(data)
            # plot cdf
            if plot_cdf:
                pl.plot(x, nonparam.cdf(x), 'k--', linewidth=1)

            data_p = _pvalue(data, nonparam.cdf, nonparam.rcdf, tail)

            npd = Nonparametric(data)
            x_p = _pvalue(x, npd.cdf, npd.rcdf, tail)

            if p is not None:
                data_p_thr = (data_p <= p).ravel()
                x_p_thr = np.abs(x_p) <= p

                # color bars which pass thresholding in red
                for thr, bar_ in zip(x_p_thr[expand_tails:], hist[2]):
                    bar_.set_facecolor(('w','r')[int(thr)])

            if not len(matches):
                # no matches were provided
                warning("No matching distributions were provided -- nothing to plot")
                return (hist, )

            lines = []
            labels = []
            for i in xrange(min(nbest, len(matches))):
                D, dist_gen, dist_name, params = matches[i]
                dist = getattr(scipy.stats, dist_gen)(*params)
                rcdf = _auto_rcdf(dist)
                label = '%s' % (dist_name)
                if legend > 1:
                    label += '(D=%.2f)' % (D)

                xcdf_p = np.abs(_pvalue(x, dist.cdf, rcdf, tail))
                if p is not None:
                    xcdf_p_thr = (xcdf_p <= p).ravel()

                if p is not None and legend > 2:
                    # We need to compare detection under given p
                    data_cdf_p = np.abs(_pvalue(data, dist.cdf, rcdf, tail))
                    data_cdf_p_thr = (data_cdf_p <= p).ravel()

                    # true positives
                    tp = np.logical_and(data_cdf_p_thr, data_p_thr)
                    # false positives
                    fp = np.logical_and(data_cdf_p_thr, ~data_p_thr)
                    # false negatives
                    fn = np.logical_and(~data_cdf_p_thr, data_p_thr)

                    label += ' tp/fp/fn=%d/%d/%d)' % \
                            tuple(map(np.sum, [tp, fp, fn]))

                pdf = dist.pdf(x)
                line = pl.plot(x, pdf, '-', linewidth=2, label=label)[0]
                color = line.get_color()

                if plot_cdf:
                    cdf = dist.cdf(x)
                    pl.plot(x, cdf, ':', linewidth=1, color=color, label=label)

                # TODO: decide on tp/fp/fn by not centers of the bins but
                #       by the values in data in the ranges covered by
                #       those bins. Then it would correspond to the values
                #       mentioned in the legend
                if p is not None:
                    # true positives
                    xtp = np.logical_and(xcdf_p_thr, x_p_thr)
                    # false positives
                    xfp = np.logical_and(xcdf_p_thr, ~x_p_thr)
                    # false negatives
                    xfn = np.logical_and(~xcdf_p_thr, x_p_thr)

                    # no need to plot tp explicitely -- marked by color of the bar
                    # pl.plot(x[xtp], pdf[xtp], 'o', color=color)
                    pl.plot(x[xfp], pdf[xfp], '^', color=color)
                    pl.plot(x[xfn], pdf[xfn], 'v', color=color)

                lines.append(line)
                labels.append(label)

            if legend:
                pl.legend(lines, labels)

            return (hist, lines)

    #if True:
    #    data = np.random.normal(size=(1000,1));
    #    matches = match_distribution(
    #        data,
    #        distributions=['scipy',
    #                       ('norm', {'name':'norm_known',
    #                                 'scale': 1.0,
    #                                 'loc': 0.0})],
    #        nsamples=30, test='p-roc', p=0.05)
    #    pl.figure(); plot_distribution_matches(data, matches, nbins=101,
    #                                        p=0.05, legend=4, nbest=5)


##REF: Name was automagically refactored
def auto_null_dist(dist):
    """Cheater for human beings -- wraps `dist` if needed with some
    NullDist

    tail and other arguments are assumed to be default as in
    NullDist/MCNullDist
    """
    if dist is None or isinstance(dist, NullDist):
        return dist
    elif hasattr(dist, 'fit'):
        if __debug__:
            debug('STAT', 'Wrapping %s into MCNullDist' % dist)
        return MCNullDist(dist)
    else:
        if __debug__:
            debug('STAT', 'Wrapping %s into FixedNullDist' % dist)
        return FixedNullDist(dist)


# if no scipy, we need nanmean
def _chk_asarray(a, axis):
    if axis is None:
        a = np.ravel(a)
        outaxis = 0
    else:
        a = np.asarray(a)
        outaxis = axis
    return a, outaxis

def nanmean(x, axis=0):
    """Compute the mean over the given axis ignoring NaNs.

    Parameters
    ----------
    x : ndarray
      input array
    axis : int
      axis along which the mean is computed.

    Returns
    -------
    m : float
      the mean.
    """
    x, axis = _chk_asarray(x, axis)
    x = x.copy()
    Norig = x.shape[axis]
    factor = 1.0 - np.sum(np.isnan(x), axis)*1.0/Norig

    x[np.isnan(x)] = 0
    return np.mean(x, axis)/factor

########NEW FILE########
__FILENAME__ = svm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Importer for the available SVM and SVR machines.

Multiple external libraries implementing Support Vector Machines
(Classification) and Regressions are available: LIBSVM, and shogun.
This module is just a helper to provide default implementation for SVM
depending on the availability of external libraries. By default LIBSVM
implementation is chosen by default, but in any case both libraries
are available through importing from this module::

 > from mvpa2.clfs.svm import sg, libsvm
 > help(sg.SVM)
 > help(libsvm.SVM)

Please refer to particular interface for more documentation about
parameterization and available kernels and implementations.
"""

__docformat__ = 'restructuredtext'

# take care of conditional import of external classifiers
from mvpa2.base import warning, cfg, externals
from mvpa2.clfs._svmbase import _SVM

if __debug__:
    from mvpa2.base import debug

# SVM implementation to be used "by default"
SVM = None
_NuSVM = None


# TODO: handle choices within cfg
_VALID_BACKENDS = ('libsvm', 'shogun', 'sg')
_svm_backend = cfg.get('svm', 'backend', default='libsvm').lower()
if _svm_backend == 'shogun':
    _svm_backend = 'sg'

if not _svm_backend in _VALID_BACKENDS:
    raise ValueError, 'Configuration option svm.backend got invalid value %s.' \
          ' Valid choices are %s' % (_svm_backend, _VALID_BACKENDS)

if __debug__:
    debug('SVM', 'SVM backend is %s' % _svm_backend)

if externals.exists('shogun'):
    from mvpa2.clfs import sg
    SVM = sg.SVM
    # Somewhat cruel hack -- define "SVM" family of kernels as binds
    # to specific default SVM implementation
    # XXX might need RF
    from mvpa2.kernels import sg as ksg
    LinearSVMKernel = ksg.LinearSGKernel
    RbfSVMKernel = ksg.RbfSGKernel

    #if not 'LinearCSVMC' in locals():
    #    from mvpa2.clfs.sg.svm import *

if externals.exists('libsvm'):
    # By default for now we want simply to import all SVMs from libsvm
    from mvpa2.clfs.libsvmc import svm as libsvm
    _NuSVM = libsvm.SVM
    if _svm_backend == 'libsvm' or SVM is None:
        if __debug__ and _svm_backend != 'libsvm' and SVM is None:
            debug('SVM', 'SVM backend %s was not found, so using libsvm'
                  % _svm_backend)
        SVM = libsvm.SVM
        from mvpa2.kernels import libsvm as kls
        LinearSVMKernel = kls.LinearLSKernel
        RbfSVMKernel = kls.RbfLSKernel
    #from mvpa2.clfs.libsvm.svm import *

if SVM is None:
    warning("None of SVM implementation libraries was found")
else:
    _defaultC = _SVM._SVM_PARAMS['C'].default
    _defaultNu = _SVM._SVM_PARAMS['nu'].default

    _edocs = []
    """List containing tuples of classes and docs to be extended"""

    # Define some convenience classes
    class LinearCSVMC(SVM):
        def __init__(self, C=_defaultC, **kwargs):
            SVM.__init__(self, C=C, kernel=LinearSVMKernel(), **kwargs)

    class RbfCSVMC(SVM):
        def __init__(self, C=_defaultC, **kwargs):
            SVM.__init__(self, C=C, kernel=RbfSVMKernel(), **kwargs)

    _edocs += [
        (LinearCSVMC, SVM, "C-SVM classifier using linear kernel."),
        (RbfCSVMC, SVM,
         "C-SVM classifier using a radial basis function kernel")]

    if _NuSVM is not None:
        class LinearNuSVMC(_NuSVM):
            def __init__(self, nu=_defaultNu, **kwargs):
                _NuSVM.__init__(self, nu=nu, kernel=LinearSVMKernel(), **kwargs)

        class RbfNuSVMC(_NuSVM):
            def __init__(self, nu=_defaultNu, **kwargs):
                _NuSVM.__init__(self, nu=nu, kernel=RbfSVMKernel(), **kwargs)

        _edocs += [
            (LinearNuSVMC, _NuSVM, "Nu-SVM classifier using linear kernel."),
            (RbfNuSVMC, _NuSVM,
             "Nu-SVM classifier using a radial basis function kernel")]

    for _c, _pc, _d in _edocs:
        _c.__doc__ = \
            "%s\n\nSee documentation of `%s` for more information" % \
            (_d, _pc.__class__.__name__)


########NEW FILE########
__FILENAME__ = transerror
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Utility class to compute the transfer error of classifiers."""

__docformat__ = 'restructuredtext'

import mvpa2.support.copy as copy

import numpy as np

from StringIO import StringIO
from math import log10, ceil

from mvpa2.base import externals
from mvpa2.base.node import Node

from mvpa2.misc.errorfx import mean_power_fx, root_mean_power_fx, rms_error, \
     relative_rms_error, mean_mismatch_error, auc_error
from mvpa2.base import warning
from mvpa2.datasets import Dataset
from mvpa2.base.collections import Collectable
from mvpa2.base.state import ConditionalAttribute, ClassWithCollections, \
     UnknownStateError
from mvpa2.base.dochelpers import enhanced_doc_string, table2string
from mvpa2.clfs.stats import auto_null_dist

if __debug__:
    from mvpa2.base import debug

if externals.exists('scipy'):
    from scipy.stats.stats import nanmean
    from mvpa2.misc.stats import chisquare
    from scipy.stats import linregress, friedmanchisquare
    from mvpa2.misc.errorfx import corr_error, corr_error_prob
else:
    from mvpa2.clfs.stats import nanmean
    chisquare = None
    linregress = None

def _p2(x, prec=2, formatting=''):
    """Helper to print depending on the type nicely. For some
    reason %.2g for 100 prints exponential form which is ugly
    """
    if isinstance(x, int):
        s = "%d" % x
    elif isinstance(x, float):
        s = ("%%.%df" % prec % x).rstrip('0').rstrip('.').lstrip()
        if s == '':
            s = '0'
    else:
        s = "%s" % x
    return formatting + s



class SummaryStatistics(object):
    """Basic class to collect targets/predictions and report summary statistics

    It takes care about collecting the sets, which are just tuples
    (targets, predictions, estimates). While 'computing' the matrix, all
    sets are considered together.  Children of the class are
    responsible for computation and display.
    """

    _STATS_DESCRIPTION = (
        ('# of sets',
         'number of target/prediction sets which were provided', None),
        )


    def __init__(self, targets=None, predictions=None, estimates=None, sets=None):
        """Initialize SummaryStatistics

        targets or predictions cannot be provided alone (ie targets
        without predictions)

        Parameters
        ----------
        targets
         Optional set of targets
        predictions
         Optional set of predictions
        estimates
         Optional set of estimates (which served for prediction)
        sets
         Optional list of sets
        """
        self._computed = False
        """Flag either it was computed for a given set of data"""

        self.__sets = (sets, [])[int(sets is None)]
        """Datasets (target, prediction) to compute confusion matrix on"""

        self._stats = {}
        """Dictionary to keep statistics. Initialized here to please pylint"""

        if not targets is None or not predictions is None:
            if not targets is None and not predictions is None:
                self.add(targets=targets, predictions=predictions,
                         estimates=estimates)
            else:
                raise ValueError, \
                      "Please provide none or both targets and predictions"


    def add(self, targets, predictions, estimates=None):
        """Add new results to the set of known results"""
        if len(targets) != len(predictions):
            raise ValueError, \
                  "Targets[%d] and predictions[%d]" % (len(targets),
                                                       len(predictions)) + \
                  " have different number of samples"

        # extract value if necessary
        if isinstance(estimates, Collectable):
            try:
                estimates = estimates.value
            except UnknownStateError, e:
                estimates = None        # reset to None to be safe below
                if __debug__:
                    debug('CM', "Cannot get yet uncollected estimates from %s",
                          estimates)

        if estimates is not None and len(targets) != len(estimates):
            raise ValueError, \
                  "Targets[%d] and estimates[%d]" % (len(targets),
                                                  len(estimates)) + \
                  " have different number of samples"

        # enforce labels in predictions to be of the same datatype as in
        # targets, since otherwise we are getting doubles for unknown at a
        # given moment labels
        nonetype = type(None)
        for i in xrange(len(targets)):
            t1, t2 = type(targets[i]), type(predictions[i])
            # if there were no prediction made - leave None, otherwise
            # convert to appropriate type
            if t1 != t2 and t2 != nonetype:
                #warning("Obtained target %s and prediction %s are of " %
                #       (t1, t2) + "different datatypes.")
                if isinstance(predictions, tuple):
                    predictions = list(predictions)
                predictions[i] = t1(predictions[i])

        if estimates is not None:
            # assure that we have a copy, or otherwise further in-place
            # modifications might screw things up (some classifiers share
            # estimates and spit out results)
            estimates = copy.deepcopy(estimates)

        self.__sets.append( (targets, predictions, estimates) )
        self._computed = False


    ##REF: Name was automagically refactored
    def as_string(self, short=False, header=True, summary=True,
                 description=False):
        """'Pretty print' the matrix

        Parameters
        ----------
        short : bool
          if True, ignores the rest of the parameters and provides consise
          1 line summary
        header : bool
          print header of the table
        summary : bool
          print summary (accuracy)
        description : bool
          print verbose description of presented statistics
        """
        raise NotImplementedError


    def __str__(self):
        """String summary over the `SummaryStatistics`

        It would print description of the summary statistics if 'CM'
        debug target is active
        """
        if __debug__:
            description = ('CM' in debug.active)
        else:
            description = False
        return self.as_string(short=False, header=True, summary=True,
                             description=description)


    def __iadd__(self, other):
        """Add the sets from `other` s `SummaryStatistics` to current one
        """
        #print "adding ", other, " to ", self
        # need to do shallow copy, or otherwise smth like "cm += cm"
        # would loop forever and exhaust memory eventually
        othersets = copy.copy(other.__sets)
        for set in othersets:
            self.add(*set)#[0], set[1])
        return self


    def __add__(self, other):
        """Add two `SummaryStatistics`s
        """
        result = copy.deepcopy(self)
        result += other
        return result


    def compute(self):
        """Actually compute the confusion matrix based on all the sets"""
        if self._computed:
            return

        self._compute()
        self._computed = True


    def _compute(self):
        """Compute basic statistics
        """
        self._stats = {'# of sets' : len(self.sets)}


    @property
    def summaries(self):
        """Return a list of separate summaries per each stored set"""
        return [ self.__class__(sets=[x]) for x in self.sets ]


    @property
    def error(self):
        raise NotImplementedError


    @property
    def stats(self):
        self.compute()
        return self._stats


    def reset(self):
        """Cleans summary -- all data/sets are wiped out
        """
        self.__sets = []
        self._computed = False


    sets = property(lambda self:self.__sets)


class ROCCurve(object):
    """Generic class for ROC curve computation and plotting
    """

    def __init__(self, labels, sets=None):
        """
        Parameters
        ----------
        labels : list
          labels which were used (in order of estimates if multiclass,
          or 1 per class for binary problems (e.g. in SMLR))
        sets : list of tuples
          list of sets for the analysis
        """
        self._labels = labels
        self._sets = sets
        self.__computed = False


    def _compute(self):
        """Lazy computation if needed
        """
        if self.__computed:
            return
        # local bindings
        labels = self._labels
        Nlabels = len(labels)
        sets = self._sets

        # Handle degenerate cases politely
        if Nlabels < 2:
            warning("ROC was asked to be evaluated on data with %i"
                    " labels which is a degenerate case." % Nlabels)
            self._ROCs = []
            self._aucs = []
            return

        # take sets which have values in the shape we can handle
        def _check_values(set_):
            """Check if values are 'acceptable'"""
            if len(set_)<3:
                return False
            x = set_[2]                 # estimates
            # TODO: OPT: need optimization
            if (x is None) or len(x) == 0:
                return False          # undefined
            for v in x:
                try:
                    if Nlabels <= 2 and np.isscalar(v):
                        continue
                    if (isinstance(v, dict) or # not dict for pairs
                        ((Nlabels >= 2) and len(v) != Nlabels)): # 1 per each label for multiclass
                        return False
                except Exception, e:
                    # Something else which is not supported, like
                    # in shogun interface we don't yet extract values per each label or
                    # in pairs in the case of built-in multiclass
                    if __debug__:
                        debug('ROC', "Exception %s while checking "
                              "either %s are valid labels" % (str(e), x))
                    return False
            return True

        sets_wv = [x for x in sets if _check_values(x)]
        # check if all had estimates, if not -- complain
        Nsets_wv = len(sets_wv)
        if Nsets_wv > 0 and len(sets) != Nsets_wv:
            warning("Only %d sets have estimates assigned from %d sets. "
                    "ROC estimates might be incorrect." %
                    (Nsets_wv, len(sets)))
        # bring all values to the same 'shape':
        #  1 value per each label. In binary classifier, if only a single
        #  value is provided, add '0' for 0th label 'value'... it should
        #  work taking drunk Yarik logic ;-)
        # yoh: apparently it caused problems whenever we had just a single
        #      unique label in the sets. Introduced handling for
        #      NLabels == 1
        for iset, s in enumerate(sets_wv):
            # we will do inplace modification, thus go by index
            estimates = s[2]
            # we would need it to be a list to reassign element with a list
            if isinstance(estimates, np.ndarray) and len(estimates.shape)==1:
                # XXX ??? so we are going away from inplace modifications?
                estimates = list(estimates)
            rangev = None
            for i in xrange(len(estimates)):
                v = estimates[i]
                if np.isscalar(v):
                    if Nlabels == 1:
                        # ensure the right dimensionality
                        estimates[i] = np.array(v, ndmin=2)
                    elif Nlabels == 2:
                        def last_el(x):
                            """Helper function. Returns x if x is scalar, and
                            last element if x is not (ie list/tuple)"""
                            if np.isscalar(x): return x
                            else:             return x[-1]
                        if rangev is None:
                            # we need to figure out min/max estimates
                            # to invert for the 0th label
                            estimates_ = [last_el(x) for x in estimates]
                            rangev = np.min(estimates_) + np.max(estimates_)
                        estimates[i] = [rangev - v, v]
                    else:
                        raise ValueError, \
                              "Cannot have a single 'value' for multiclass" \
                              " classification. Got %s" % (v)
                elif len(v) != Nlabels:
                    raise ValueError, \
                          "Got %d estimates whenever there is %d labels" % \
                          (len(v), Nlabels)
            # reassign possibly adjusted estimates
            sets_wv[iset] = (s[0], s[1], np.asarray(estimates))


        # we need to estimate ROC per each label
        # XXX order of labels might not correspond to the one among 'estimates'
        #     which were used to make a decision... check
        rocs, aucs = [], []             # 1 per label
        for i,label in enumerate(labels):
            aucs_pl = []
            ROCs_pl = []
            for s in sets_wv:
                targets_pl = (np.asanyarray(s[0]) == label).astype(int)
                # XXX we might unify naming between AUC/ROC
                ROC = auc_error
                aucs_pl += [ROC([np.asanyarray(x)[i] for x in s[2]], targets_pl)]
                ROCs_pl.append(ROC)
            if len(aucs_pl)>0:
                rocs += [ROCs_pl]
                aucs += [nanmean(aucs_pl)]
                #aucs += [np.mean(aucs_pl)]

        # store results within the object
        self._ROCs =  rocs
        self._aucs = aucs
        self.__computed = True


    @property
    def aucs(self):
        """Compute and return set of AUC values 1 per label
        """
        self._compute()
        return self._aucs


    @property
    ##REF: Name was automagically refactored
    def rocs(self):
        self._compute()
        return self._ROCs


    def plot(self, label_index=0):
        """

        TODO: make it friendly to labels given by values?
              should we also treat labels_map?
        """
        externals.exists("pylab", raise_=True)
        import pylab as pl

        self._compute()

        labels = self._labels
        # select only rocs for the given label
        rocs = self.rocs[label_index]

        fig = pl.gcf()
        ax = pl.gca()

        pl.plot([0, 1], [0, 1], 'k:')

        for ROC in rocs:
            pl.plot(ROC.fp, ROC.tp, linewidth=1)

        pl.axis((0.0, 1.0, 0.0, 1.0))
        pl.axis('scaled')
        pl.title('Label %s. Mean AUC=%.2f' % (label_index, self.aucs[label_index]))

        pl.xlabel('False positive rate')
        pl.ylabel('True positive rate')


class ConfusionMatrix(SummaryStatistics):
    """Class to contain information and display confusion matrix.

    Implementation of the `SummaryStatistics` in the case of
    classification problem. Actual computation of confusion matrix is
    delayed until all data is acquired (to figure out complete set of
    labels). If testing data doesn't have a complete set of labels,
    but you like to include all labels, provide them as a parameter to
    the constructor.

    Confusion matrix provides a set of performance statistics (use
    as_string(description=True) for the description of abbreviations),
    as well ROC curve (http://en.wikipedia.org/wiki/ROC_curve)
    plotting and analysis (AUC) in the limited set of problems:
    binary, multiclass 1-vs-all.
    """

    _STATS_DESCRIPTION = (
        ('TP', 'true positive (AKA hit)', None),
        ('TN', 'true negative (AKA correct rejection)', None),
        ('FP', 'false positive (AKA false alarm, Type I error)', None),
        ('FN', 'false negative (AKA miss, Type II error)', None),
        ('TPR', 'true positive rate (AKA hit rate, recall, sensitivity)',
                'TPR = TP / P = TP / (TP + FN)'),
        ('FPR', 'false positive rate (AKA false alarm rate, fall-out)',
                'FPR = FP / N = FP / (FP + TN)'),
        ('ACC', 'accuracy', 'ACC = (TP + TN) / (P + N)'),
        ('SPC', 'specificity', 'SPC = TN / (FP + TN) = 1 - FPR'),
        ('PPV', 'positive predictive value (AKA precision)',
                'PPV = TP / (TP + FP)'),
        ('NPV', 'negative predictive value', 'NPV = TN / (TN + FN)'),
        ('FDR', 'false discovery rate', 'FDR = FP / (FP + TP)'),
        ('MCC', "Matthews Correlation Coefficient",
                "MCC = (TP*TN - FP*FN)/sqrt(P N P' N')"),
        ('F1',  'F1 score',
                "F1 = 2TP / (P + P') = 2TP / (2TP + FP + FN)"),
        ('AUC', "Area under (AUC) curve", None),
        ('CHI^2', "Chi-square of confusion matrix", None),
        ('LOE(ACC)', "Linear Order Effect in ACC across sets", None),
        ## ('Friedman(TPR)',
        ##  "Friedman CHI^2 test of TPRs consistencies across sets", None),
        ## ('Friedman(CM)',
        ##  "Friedman CHI^2 test of full CM (normalized) consistencies across sets", None),
        ) + SummaryStatistics._STATS_DESCRIPTION


    def __init__(self, labels=None, labels_map=None, **kwargs):
        """Initialize ConfusionMatrix with optional list of `labels`

        Parameters
        ----------
        labels : list
         Optional set of labels to include in the matrix
        labels_map : None or dict
         Dictionary from original dataset to show mapping into
         numerical labels
        targets
         Optional set of targets
        predictions
         Optional set of predictions
         """

        SummaryStatistics.__init__(self, **kwargs)

        if labels == None:
            labels = []

        self.__labels = labels
        """List of known labels"""
        self.__labels_in_custom_order = bool(len(labels))
        """So we know later on either we could resort them"""
        self.__labels_map = labels_map
        """Mapping from original into given labels"""
        self.__matrix = None
        """Resultant confusion matrix"""


    def __call__(self, predictions, targets, estimates=None, store=False):
        """Computes confusion matrix (counts)

        It would rely on previously provided 'labels' to define columns/rows
        of the matrix to assure consistency across multiple invocations.

        Parameters
        ----------
        store : bool, optional
          By default, this function does not modify an existing
          instance of the ConfusionMatrix, and is just used merely to
          provide a resultant confusion matrix.  If 'store' set to
          True, provided set of predictions and targets would be added
          to the sets.

        Returns
        -------
        numpy.ndarray
           counts of hits with rows -- predictions, columns -- targets
        """
        labels = self.__labels
        if labels is None or not len(labels):
            raise RuntimeError("ConfusionMatrix must have labels assigned prior"
                               "__call__()")
        # verify that we know all the labels
        labels_set = set(labels)
        if not (labels_set.issuperset(targets)
                and labels_set.issuperset(predictions)):
            raise ValueError("Known labels %r does not include some labels "
                             "found in predictions %r or targets %r provided"
                             % (labels_set, set(predictions), set(targets)))

        Nlabels = len(labels_set)
        cm = np.zeros( (Nlabels, Nlabels), dtype=int )

        rev_map = dict([ (x[1], x[0]) for x in enumerate(labels)])
        for t,p in zip(targets, predictions):
            cm[rev_map[p], rev_map[t]] += 1

        if store:
            self.add(targets=targets, predictions=predictions, estimates=estimates)
        return cm

    # XXX might want to remove since summaries does the same, just without
    #     supplying labels
    @property
    def matrices(self):
        """Return a list of separate confusion matrix per each stored set"""
        return [ self.__class__(labels=self.labels,
                                labels_map=self.labels_map,
                                sets=[x]) for x in self.sets]


    def _compute(self):
        """Actually compute the confusion matrix based on all the sets"""

        super(ConfusionMatrix, self)._compute()

        if __debug__:
            if not self.__matrix is None:
                debug("LAZY",
                      "Have to recompute %s#%s" \
                        % (self.__class__.__name__, id(self)))


        # TODO: BinaryClassifier might spit out a list of predictions for each
        # value need to handle it... for now just keep original labels
        try:
            # figure out what labels we have
            labels = \
                list(reduce(lambda x, y: x.union(set(y[0]).union(set(y[1]))),
                            self.sets,
                            set(self.__labels)))
        except:
            labels = self.__labels


        # Check labels_map if it was provided if it covers all the labels
        labels_map = self.__labels_map
        if labels_map is not None:
            labels_set = set(labels)
            map_labels_set = set(labels_map.values())

            if not map_labels_set.issuperset(labels_set):
                warning("Provided labels_map %s is not coherent with labels "
                        "provided to ConfusionMatrix. No reverse mapping "
                        "will be provided" % labels_map)
                labels_map = None

        # Create reverse map
        labels_map_rev = None
        if labels_map is not None:
            labels_map_rev = {}
            for k,v in labels_map.iteritems():
                v_mapping = labels_map_rev.get(v, [])
                v_mapping.append(k)
                labels_map_rev[v] = v_mapping
        self.__labels_map_rev = labels_map_rev

        labels.sort()

        if self.__labels is None or not len(self.__labels):
            self.__labels = labels          # just store the recomputed labels
        else:
            # we should append them to already known ones
            # Otherwise order of labels known before might be altered
            add_labels = [x for x in labels if not (x in self.__labels)]
            if len(add_labels):
                self.__labels += add_labels
            labels = self.__labels      # and use them later on

            if not self.__labels_in_custom_order:
                labels.sort()

        Nlabels, Nsets = len(labels), len(self.sets)

        if __debug__:
            debug("CM", "Got labels %s" % labels)

        # Create a matrix for all votes
        mat_all = np.zeros( (Nsets, Nlabels, Nlabels), dtype=int )

        # create total number of samples of each label counts
        # just for convinience I guess since it can always be
        # computed from mat_all
        counts_all = np.zeros( (Nsets, Nlabels) )

        # reverse mapping from label into index in the list of labels
        rev_map = dict([ (x[1], x[0]) for x in enumerate(labels)])
        for iset, set_ in enumerate(self.sets):
            for t,p in zip(*set_[:2]):
                mat_all[iset, rev_map[p], rev_map[t]] += 1


        # for now simply compute a sum of votes across different sets
        # we might do something more sophisticated later on, and this setup
        # should easily allow it
        self.__matrix = np.sum(mat_all, axis=0)
        self.__Nsamples = np.sum(self.__matrix, axis=0)
        self.__Ncorrect = sum(np.diag(self.__matrix))

        TP = np.diag(self.__matrix)
        offdiag = self.__matrix - np.diag(TP)
        stats = {
            '# of labels' : Nlabels,
            'TP' : TP,
            'FP' : np.sum(offdiag, axis=1),
            'FN' : np.sum(offdiag, axis=0)}

        stats['CORR']  = np.sum(TP)
        stats['TN']  = stats['CORR'] - stats['TP']
        stats['P']  = stats['TP'] + stats['FN']
        stats['N']  = np.sum(stats['P']) - stats['P']
        stats["P'"] = stats['TP'] + stats['FP']
        stats["N'"] = stats['TN'] + stats['FN']
        stats['TPR'] = stats['TP'] / (1.0*stats['P'])
        # reset nans in TPRs to 0s whenever there is no entries
        # for those labels among the targets
        stats['TPR'][stats['P'] == 0] = 0
        stats['PPV'] = stats['TP'] / (1.0*stats["P'"])
        stats['NPV'] = stats['TN'] / (1.0*stats["N'"])
        stats['FDR'] = stats['FP'] / (1.0*stats["P'"])
        stats['SPC'] = (stats['TN']) / (1.0*stats['FP'] + stats['TN'])
        stats['F1'] = 2.*stats['TP'] / (stats["P"] + stats["P'"])

        MCC_denom = np.sqrt(1.0*stats['P']*stats['N']*stats["P'"]*stats["N'"])
        nz = MCC_denom!=0.0
        stats['MCC'] = np.zeros(stats['TP'].shape)
        stats['MCC'][nz] = \
                 (stats['TP'] * stats['TN'] - stats['FP'] * stats['FN'])[nz] \
                  / MCC_denom[nz]

        stats['ACC'] = np.sum(TP)/(1.0*np.sum(stats['P']))
        # TODO: STD of accuracy and corrected one according to
        #    Nadeau and Bengio [50]
        stats['ACC%'] = stats['ACC'] * 100.0
        if chisquare:
            # indep_rows to assure reasonable handling of disbalanced
            # cases
            stats['CHI^2'] = chisquare(self.__matrix, exp='indep_rows')
        if linregress and Nsets > 3:
            # Lets see if there is possible order effect in accuracy
            # (e.g. it goes down through splits)

            # simple linear regression
            ACC_per_set = [np.sum(np.diag(m))/np.sum(m).astype(float)
                           for m in mat_all]
            stats['LOE(ACC):slope'], stats['LOE(ACC):inter'], \
                stats['LOE(ACC):r'], stats['LOE(ACC):p'], _ = \
                linregress(np.arange(Nsets), ACC_per_set)

            TPRs_per_set = [np.diag(m)/np.sum(m, axis=0).astype(float)
                            for m in mat_all]
            # Confusion ratios (both TPs or FPs)
            # we want to divide each column but sum in the column
            CM_per_set = [np.ravel(m/np.sum(m, axis=0).astype(float)[None, :])
                          for m in mat_all]

            ## stats['Friedman(TPR):chi^2'], stats['Friedman(TPR):p'] = \
            ##                               friedmanchisquare(*TPRs_per_set)
            ## stats['Friedman(CM):chi^2'], stats['Friedman(CM):p'] = \
            ##                              friedmanchisquare(*CM_per_set)

        #
        # ROC computation if available
        ROC = ROCCurve(labels=labels, sets=self.sets)
        aucs = ROC.aucs
        if len(aucs)>0:
            stats['AUC'] = aucs
            if len(aucs) != Nlabels:
                raise RuntimeError, \
                      "We must got a AUC per label. Got %d instead of %d" % \
                      (len(aucs), Nlabels)
            self.ROC = ROC
        else:
            # we don't want to provide ROC if it is bogus
            stats['AUC'] = [np.nan] * Nlabels
            self.ROC = None


        # compute mean stats
        for k,v in stats.items():
            stats['mean(%s)' % k] = np.mean(v)

        self._stats.update(stats)


    ##REF: Name was automagically refactored
    def as_string(self, short=False, header=True, summary=True,
                 description=False):
        """'Pretty print' the matrix

        Parameters
        ----------
        short : bool
          if True, ignores the rest of the parameters and provides consise
          1 line summary
        header : bool
          print header of the table
        summary : bool
          print summary (accuracy)
        description : bool
          print verbose description of presented statistics
        """
        if len(self.sets) == 0:
            return "Empty"

        self.compute()

        # some shortcuts
        labels = self.__labels
        labels_map_rev = self.__labels_map_rev
        matrix = self.__matrix

        labels_rev = []
        if labels_map_rev is not None:
            labels_rev = [','.join([str(x) for x in labels_map_rev[l]])
                                   for l in labels]

        out = StringIO()
        # numbers of different entries
        Nlabels = len(labels)
        Nsamples = self.__Nsamples.astype(int)

        stats = self._stats
        if short:
            return "%(# of sets)d sets %(# of labels)d labels " \
                   " ACC:%(ACC).2f" \
                   % stats

        Ndigitsmax = int(ceil(log10(max(Nsamples))))
        Nlabelsmax = max( [len(str(x)) for x in labels] )

        # length of a single label/value
        L = max(Ndigitsmax+2, Nlabelsmax) #, len("100.00%"))
        res = ""

        stats_perpredict = ["P'", "N'", 'FP', 'FN', 'PPV', 'NPV', 'TPR',
                            'SPC', 'FDR', 'MCC', 'F1']
        # print AUC only if ROC was computed
        if self.ROC is not None: stats_perpredict += [ 'AUC' ]
        stats_pertarget = ['P', 'N', 'TP', 'TN']
        stats_summary = ['ACC', 'ACC%', '# of sets']
        # 'Friedman(TPR):p', 'Friedman(CM):p'


        #prefixlen = Nlabelsmax + 2 + Ndigitsmax + 1
        prefixlen = Nlabelsmax + 1
        pref = ' '*(prefixlen) # empty prefix

        if matrix.shape != (Nlabels, Nlabels):
            raise ValueError, \
                  "Number of labels %d doesn't correspond the size" + \
                  " of a confusion matrix %s" % (Nlabels, matrix.shape)

        # list of lists of what is printed
        printed = []
        underscores = [" %s" % ("-" * L)] * Nlabels
        if header:
            # labels
            printed.append(['@l----------.        '] + labels_rev)
            printed.append(['@lpredictions\\targets'] + labels)
            # underscores
            printed.append(['@l            `------'] \
                           + underscores + stats_perpredict)

        # matrix itself
        for i, line in enumerate(matrix):
            l = labels[i]
            if labels_rev != []:
                l = '@r%10s / %s' % (labels_rev[i], l)
            printed.append(
                [l] +
                [ str(x) for x in line ] +
                [ _p2(stats[x][i]) for x in stats_perpredict])

        if summary:
            ## Various alternative schemes ;-)
            # printed.append([''] + underscores)
            # printed.append(['@lPer target \ Means:'] + underscores + \
            #               [_p2(x) for x in mean_stats])
            # printed.append(['Means:'] + [''] * len(labels)
            #                + [_p2(x) for x in mean_stats])
            printed.append(['@lPer target:'] + underscores)
            for stat in stats_pertarget:
                printed.append([stat] + [
                    _p2(stats[stat][i]) for i in xrange(Nlabels)])

            # compute mean stats
            # XXX refactor to expose them in stats as well, as
            #     mean(FCC)
            mean_stats = np.mean(np.array([stats[k] for k in stats_perpredict]),
                                axis=1)
            printed.append(['@lSummary \ Means:'] + underscores
                           + [_p2(stats['mean(%s)' % x])
                              for x in stats_perpredict])

            if 'CHI^2' in self.stats:
                chi2t = stats['CHI^2']
                printed.append(['CHI^2'] + [_p2(chi2t[0])]
                               + ['@wp=%.2g' % chi2t[1]])

            for stat in stats_summary:
                if stat in stats:
                    printed.append([stat] + [_p2(stats[stat])])

            if 'LOE(ACC):inter' in stats:
                # Inject into sets line
                printed[-1] += [
                    '@w ACC(i) = %(LOE(ACC):inter).2g%(LOE(ACC):slope)+.2g*i'
                    % stats +
                    ' p=' + _p2(stats['LOE(ACC):p']) +
                    ' r=' + _p2(stats['LOE(ACC):r']) +
                    ' r^2=' + _p2(stats['LOE(ACC):r']**2)]

        table2string(printed, out)

        if description:
            out.write("\nStatistics computed in 1-vs-rest fashion per each " \
                      "target.\n")
            out.write("Abbreviations (for details see " \
                      "http://en.wikipedia.org/wiki/ROC_curve):\n")
            for d, val, eq in self._STATS_DESCRIPTION:
                out.write(" %-3s: %s\n" % (d, val))
                if eq is not None:
                    out.write("      " + eq + "\n")

        #out.write("%s" % printed)
        result = out.getvalue()
        out.close()
        return result


    def plot(self, labels=None, numbers=False, origin='upper',
             numbers_alpha=None, xlabels_vertical=True, numbers_kwargs={},
             **kwargs):
        """Provide presentation of confusion matrix in image

        Parameters
        ----------
        labels : list of int or str
          Optionally provided labels guarantee the order of
          presentation. Also value of None places empty column/row,
          thus provides visual groupping of labels (Thanks Ingo)
        numbers : bool
          Place values inside of confusion matrix elements
        numbers_alpha : None or float
          Controls textual output of numbers. If None -- all numbers
          are plotted in the same intensity. If some float -- it controls
          alpha level -- higher value would give higher contrast. (good
          value is 2)
        origin : str
          Which left corner diagonal should start
        xlabels_vertical : bool
          Either to plot xlabels vertical (benefitial if number of labels
          is large)
        numbers_kwargs : dict
          Additional keyword parameters to be added to numbers (if numbers
          is True)
        **kwargs
          Additional arguments given to imshow (\eg me cmap)

        Returns
        -------
         (fig, im, cb) -- figure, imshow, colorbar
        """

        externals.exists("pylab", raise_=True)
        import pylab as pl

        self.compute()
        labels_order = labels

        # some shortcuts
        labels = self.__labels
        labels_map = self.__labels_map
        labels_map_rev = self.__labels_map_rev
        matrix = self.__matrix

        # craft original mapping from a label into index in the matrix
        labels_indexes = dict([(x,i) for i,x in enumerate(labels)])

        labels_rev = []
        if labels_map_rev is not None:
            labels_rev = [','.join([str(x) for x in labels_map_rev[l]])
                                   for l in labels]
            labels_map_full = dict(zip(labels_rev, labels))

        if labels_order is not None:
            labels_order_filtered = filter(lambda x:x is not None, labels_order)
            labels_order_filtered_set = set(labels_order_filtered)
            # Verify if all labels provided in labels
            if set(labels) == labels_order_filtered_set:
                # We were provided numerical (most probably) set
                labels_plot = labels_order
            elif len(labels_rev) \
                     and set(labels_rev) == labels_order_filtered_set:
                # not clear if right whenever there were multiple labels
                # mapped into the same
                labels_plot = []
                for l in labels_order:
                    v = None
                    if l is not None: v = labels_map_full[l]
                    labels_plot += [v]
            else:
                raise ValueError, \
                      "Provided labels %s do not match set of known " \
                      "original labels (%s) or mapped labels (%s)" % \
                      (labels_order, labels, labels_rev)
        else:
            labels_plot = labels

        # where we have Nones?
        isempty = np.array([l is None for l in labels_plot])
        non_empty = np.where(np.logical_not(isempty))[0]
        # numbers of different entries
        NlabelsNN = len(non_empty)
        Nlabels = len(labels_plot)

        if matrix.shape != (NlabelsNN, NlabelsNN):
            raise ValueError, \
                  "Number of labels %d doesn't correspond the size" + \
                  " of a confusion matrix %s" % (NlabelsNN, matrix.shape)

        confusionmatrix = np.zeros((Nlabels, Nlabels))
        mask = confusionmatrix.copy()
        ticks = []
        tick_labels = []
        # populate in a silly way
        reordered_indexes = [labels_indexes[i] for i in labels_plot
                             if i is not None]
        for i, l in enumerate(labels_plot):
            if l is not None:
                j = labels_indexes[l]
                confusionmatrix[i, non_empty] = matrix[j, reordered_indexes]
                confusionmatrix[non_empty, i] = matrix[reordered_indexes, j]
                ticks += [i + 0.5]
                if labels_map_rev is not None:
                    tick_labels += ['/'.join(labels_map_rev[l])]
                else:
                    tick_labels += [str(l)]
            else:
                mask[i, :] = mask[:, i] = 1

        confusionmatrix = np.ma.MaskedArray(confusionmatrix, mask=mask)

        # turn off automatic update if interactive
        if pl.matplotlib.get_backend() == 'TkAgg':
            pl.ioff()

        fig = pl.gcf()
        ax = pl.gca()
        ax.axis('off')

        # some customization depending on the origin
        xticks_position, yticks, ybottom = {
            'upper': ('top', [Nlabels-x for x in ticks], 0.1),
            'lower': ('bottom', ticks, 0.2)
            }[origin]


        # Plot
        axi = fig.add_axes([0.15, ybottom, 0.7, 0.7])
        im = axi.imshow(confusionmatrix, interpolation="nearest", origin=origin,
                        aspect='equal', extent=(0, Nlabels, 0, Nlabels),
                        **kwargs)

        # plot numbers
        if numbers:
            numbers_kwargs_ = {'fontsize': 10,
                               'horizontalalignment': 'center',
                               'verticalalignment': 'center'}
            maxv = float(np.max(confusionmatrix))
            colors = [im.to_rgba(0), im.to_rgba(maxv)]
            for i,j in zip(*np.logical_not(mask).nonzero()):
                v = confusionmatrix[j, i]
                # scale alpha non-linearly
                if numbers_alpha is None:
                    alpha = 1.0
                else:
                    # scale according to value
                    alpha = 1 - np.array(1 - v / maxv) ** numbers_alpha
                y = {'lower':j, 'upper':Nlabels-j-1}[origin]
                numbers_kwargs_['color'] = colors[int(v<maxv/2)]
                numbers_kwargs_.update(numbers_kwargs)
                pl.text(i+0.5, y+0.5, '%d' % v, alpha=alpha, **numbers_kwargs_)

        maxv = np.max(confusionmatrix)
        boundaries = np.linspace(0, maxv, np.min((maxv, 10)), True)

        # Label axes
        pl.xlabel("targets")
        pl.ylabel("predictions")

        pl.setp(axi, xticks=ticks, yticks=yticks,
               xticklabels=tick_labels, yticklabels=tick_labels)

        axi.xaxis.set_ticks_position(xticks_position)
        axi.xaxis.set_label_position(xticks_position)

        if xlabels_vertical:
            pl.setp(pl.getp(axi, 'xticklabels'), rotation='vertical')

        axcb = fig.add_axes([0.8, ybottom, 0.02, 0.7])
        cb = pl.colorbar(im, cax=axcb, format='%d', ticks = boundaries)

        if pl.matplotlib.get_backend() == 'TkAgg':
            pl.ion()
        pl.draw()
        # Store it primarily for testing
        self._plotted_confusionmatrix = confusionmatrix
        return fig, im, cb


    @property
    def error(self):
        self.compute()
        return 1.0-self.__Ncorrect*1.0/sum(self.__Nsamples)


    @property
    def labels(self):
        self.compute()
        return self.__labels


    ##REF: Name was automagically refactored
    def get_labels_map(self):
        return self.__labels_map


    ##REF: Name was automagically refactored
    def set_labels_map(self, val):
        if val is None or isinstance(val, dict):
            self.__labels_map = val
        else:
            raise ValueError, "Cannot set labels_map to %s" % val
        # reset it just in case
        self.__labels_map_rev = None
        self._computed = False


    @property
    def matrix(self):
        self.compute()
        return self.__matrix


    @property
    ##REF: Name was automagically refactored
    def percent_correct(self):
        self.compute()
        return 100.0*self.__Ncorrect/sum(self.__Nsamples)

    labels_map = property(fget=get_labels_map, fset=set_labels_map)


class ConfusionMatrixError(object):
    """Compute confusion matrix as an "error function"

    This class can be used to compute confusion matrices from classifier
    output inside cross-validation fold without the ``stats`` conditional
    attribute. Simply pass an instance of this class to the ``errorfx``
    argument of ``CrossValidation``.
    """
    def __init__(self, labels=None):
        """
        Parameters
        ----------
        labels : list
          Class labels for confusion matrix columns/rows
        """
        self.labels = labels

    def __call__(self, predictions, targets):
        cm = ConfusionMatrix(labels=list(self.labels),
                             targets=targets, predictions=predictions)
        #print cm.matrix
        # We have to add a degenerate leading dimension
        # so we could separate them into separate 'samples'
        return cm.matrix[None, :]


class Confusion(Node):
    """Compute a confusion matrix from predictions and targets (Node interface)

    This class is very similar to ``ConfusionMatrix`` and
    ``ConfusionMatrixError``.  However, in contrast to these this class can be
    used in any place that accepts ``Nodes`` -- most importantly others node's
    ``postproc`` functionality. This makes it very straightforward to compute
    confusion matrices from classifier output as an intermediate result and
    continue processing with other nodes. A sketch of a cross-validation setup
    using this functionality looks like this::

      CrossValidation(some_classifier,
                      some_partitioner,
                      errorfx=None,
                      postproc=Confusion())

    It is vital to set ``errorfx`` to ``None`` to preserve raw classifier
    prediction values in the output dataset to allow for proper data aggregation
    in a confusion matrix.
    """
    def __init__(self, attr='targets', labels=None, add_confusion_obj=False,
                 **kwargs):
        """
        Parameters
        ----------
        attr : str
          Sample attribute name where classification target values are stored
          for each prediction.
        labels : list or None
          Optional list of labels to compute a confusion matrix for. This can be
          useful if a particular  prediction dataset doesn't have all
          theoretically possible labels as targets.
        add_confusion_obj : bool
          If True, the ConfusionMatrix object will be added to the output
          dataset as attribute 'confusion_obj', i.e. ds.a.confusion_obj
        **kwargs
          All remaining argments will be passed on to the Node base-class.
        """
        Node.__init__(self, **kwargs)
        self._labels = labels
        self._target_attr = attr
        self._add_confusion_obj = add_confusion_obj

    def _call(self, ds):
        if not len(ds.shape) == 2 and ds.shape[1] == 1:
            raise ValueError("Confusion cannot deal with multi-dimensional "
                             "predictions, got shape %s." % ds.shape[1:])
        # MH: we could also make it iterate over individual chunks and create
        # matrix sets -- but not sure if there is a use case for the Node
        # interface
        # compute the confusion matrix
        cm = ConfusionMatrix(labels=list(self._labels),
                             predictions=ds.samples[:,0],
                             targets=ds.sa[self._target_attr].value)
        # figure out where to store the labels
        # by default the confusion matrix in the Dataset will look just like
        # a printed ConfusionMatrix
        if self.get_space() is None:
            fa_attr = 'targets'
            sa_attr = 'predictions'
        else:
            sa_attr = fa_attr = self.get_space()
        out = Dataset(cm.matrix,
                      sa={sa_attr: cm.labels},
                      fa={fa_attr: cm.labels})
        if self._add_confusion_obj:
            out.a['confusion_obj'] = cm

        return out


class BayesConfusionHypothesis(Node):
    """Bayesian hypothesis testing on confusion matrices.

    For multi-class classification a single accuracy value is often not a
    meaningful performance measure -- or at least hard to interpret. This class
    allows for convenient Bayesian hypothesis testing of confusion matrices.
    It computes the likelihood of discriminibility of any partitions of
    classes given a confusion matrix.

    The returned dataset contains at least one feature (the log likelihood of
    a hypothesis) and as many samples as (possible) partitions of classes.
    The actual partition configurations are stored in a sample attribute
    of nested lists. The top-level list contains discriminable groups of
    classes, whereas the second level lists contain groups of classes that
    cannot be discriminated under a given hypothesis. For example::

      [[0, 1], [2], [3, 4, 5]]

    This hypothesis represent the state where class 0 and 1 cannot be
    distinguish from each other, but both 0 and 1 together can be distinguished
    from class 2 and the group of 3, 4, and 5 -- where classes from the later
    group cannot be distinguished from one another.

    This algorithms is based on

        Olivetti, E., Greiner, S. and Avesani, P. (2012). Testing for
        Information with Brain Decoding. In: Pattern Recognition in NeuroImaging
        (PRNI), International Workshop on.
    """
    def __init__(self, alpha=None, labels_attr='predictions',
                 space='hypothesis', prior_Hs=None, log=True,
                 postprob=True, hypotheses=None, **kwargs):
        """
        Parameters
        ----------
        alpha : array
          Bayesian hyper-prior alpha (in a multivariate-Dirichlet sense)
        labels_attr : str
          Name of the sample attribute in the input dataset that contains
          the class labels corresponding to the confusion matrix rows. If an
          attribute with this name is not found, hypotheses will be reported
          based on confusion table row/column numbers, instead of their
          corresponding labels. If such an attribute is found in the input
          dataset, any ``hypotheses`` specification has to be specified
          using literal labels also.
        space : str
          Name of the sample attribute in the output dataset where the
          hypothesis partition configurations will be stored.
        prior_Hs : array
          Vector of priors for each hypotheses. Typically used in conjuction
          with an explicit set of possible hypotheses (see ``hypotheses``).
          If ``None`` a flat prior is assumed.
        log : bool
          Whether to return values (likelihood or posterior probabilities) in
          log scale to mitigate numerical precision problems with near-zero
          probabilities.
        postprob : bool
          Whether to return posterior probabilities p(hypothesis|confusion)
          instead of likelihood(confusion|hypothesis).
        hypotheses : list
          List of possible hypotheses. XXX needs work on how to specify them.
        **kwargs
          All remaining argments will be passed on to the Node base-class.
        """
        Node.__init__(self, space=space, **kwargs)
        self._alpha = alpha
        self._prior_Hs = prior_Hs
        self._labels_attr = labels_attr
        self._log = log
        self._postprob = postprob
        self._hypotheses = hypotheses

    def _call(self, ds):
        from mvpa2.support.bayes.partitioner import Partition
        from mvpa2.support.bayes.partial_independence import compute_logp_H

        hypotheses = self._hypotheses
        if hypotheses is None:
            # generate all possible hypotheses if none are given
            partitions = Partition(range(len(ds)))
        else:
            if self._labels_attr in ds.sa:
                # literal labels are given -> recode into digits to match
                # underlying API
                recode = dict([(e, i)
                    for i, e in enumerate(ds.sa[self._labels_attr].value)])
                partitions = [[[recode[label] for label in class_]
                                for class_ in hyp]
                                    for hyp in hypotheses]
            else:
                # use hypotheses as is -- all bets are off
                partitions = hypotheses

        if self._prior_Hs is None:
            # default: uniform prior on hypotheses: p(H_i)
            prior_Hs = np.ones(len(partitions)) / len(partitions)
        else:
            prior_Hs = self._prior_Hs

        # p(X|H_i) for all H
        logp_X_given_Hs = np.zeros(len(partitions))
        for i, psi in enumerate(partitions):
            # use Emanuele's toolbox
            logp_X_given_Hs[i] = compute_logp_H(ds.samples, psi, self._alpha)
        out = logp_X_given_Hs
        statfa = ['log(p(C|H))']

        if self._postprob:
            # convert into posterior probabilities: p(H|X)
            # normalization constant: p(X)
            logp_X = reduce(np.logaddexp, logp_X_given_Hs + np.log(prior_Hs))

            # p(H|X) from Bayes rule:
            log_posterior_Hs_given_X = logp_X_given_Hs + np.log(prior_Hs) - logp_X

            out = np.vstack((out, log_posterior_Hs_given_X)).T
            statfa.append('log(p(H|C))')

        if not self._log:
            # convert from log scale
            out = np.exp(out)
            # remove the log() from the stat label
            statfa = [s[4:-1] for s in statfa]

        if hypotheses is None:
            if self._labels_attr in ds.sa:
                # recode partition IDs into actual labels, if the necessary attr
                # is available
                hypotheses = Partition(ds.sa[self._labels_attr].value)
            else:
                hypotheses = partitions
            hypotheses = list(hypotheses)

        out = Dataset(out,
                      sa={self.get_space(): hypotheses,
                          'prior': prior_Hs},
                      fa={'stat': statfa})
        return out


class RegressionStatistics(SummaryStatistics):
    """Class to contain information and display on regression results.

    """

    _STATS_DESCRIPTION = (
        ('CCe', 'Error based on correlation coefficient',
         '1 - corr_coef'),
        ('CCp', 'Correlation coefficient (p-value)', None),
        ('RMSE', 'Root mean squared error', None),
        ('STD', 'Standard deviation', None),
        ('RMP', 'Root mean power (compare to RMSE of results)',
         'sqrt(mean( data**2 ))'),
        ) + SummaryStatistics._STATS_DESCRIPTION


    def __init__(self, **kwargs):
        """Initialize RegressionStatistics

        Parameters
        ----------
        targets
         Optional set of targets
        predictions
         Optional set of predictions
         """

        SummaryStatistics.__init__(self, **kwargs)


    def _compute(self):
        """Actually compute the confusion matrix based on all the sets"""

        super(RegressionStatistics, self)._compute()
        sets = self.sets
        Nsets = len(sets)

        stats = {}

        funcs = {
            'RMP_t': lambda p,t:root_mean_power_fx(t),
            'STD_t': lambda p,t:np.std(t),
            'RMP_p': lambda p,t:root_mean_power_fx(p),
            'STD_p': lambda p,t:np.std(p),
            'RMSE': rms_error,
            'RMSE/RMP_t': relative_rms_error
            }
        if externals.exists('scipy'):
            funcs['CCe'] = corr_error
            funcs['CCp'] = corr_error_prob

        for funcname, func in funcs.iteritems():
            funcname_all = funcname + '_all'
            stats[funcname_all] = []
            for i, (targets, predictions, estimates) in enumerate(sets):
                stats[funcname_all] += [func(predictions, targets)]
            stats[funcname_all] = np.array(stats[funcname_all])
            stats[funcname] = np.mean(stats[funcname_all])
            stats[funcname+'_std'] = np.std(stats[funcname_all])
            stats[funcname+'_max'] = np.max(stats[funcname_all])
            stats[funcname+'_min'] = np.min(stats[funcname_all])

        # create ``summary`` statistics, since some per-set statistics
        # might be uncomputable if a set contains just a single number
        # (like in the case of correlation coefficient)
        targets, predictions = [], []
        for i, (targets_, predictions_, estimates_) in enumerate(sets):
            targets += list(targets_)
            predictions += list(predictions_)

        for funcname, func in funcs.iteritems():
            funcname_all = 'Summary ' + funcname
            stats[funcname_all] = func(predictions, targets)

        self._stats.update(stats)


    def plot(self,
             plot=True, plot_stats=True,
             splot=True
             #labels=None, numbers=False, origin='upper',
             #numbers_alpha=None, xlabels_vertical=True,
             #numbers_kwargs={},
             #**kwargs
             ):
        """Provide presentation of regression performance in image

        Parameters
        ----------
        plot : bool
          Plot regular plot of values (targets/predictions)
        plot_stats : bool
          Print basic statistics in the title
        splot : bool
          Plot scatter plot

        Returns
        -------
         (fig, im, cb) -- figure, imshow, colorbar
        """
        externals.exists("pylab", raise_=True)
        import pylab as pl

        self.compute()
        # total number of plots
        nplots = plot + splot

        # turn off automatic update if interactive
        if pl.matplotlib.get_backend() == 'TkAgg':
            pl.ioff()

        fig = pl.gcf()
        pl.clf()
        sps = []                        # subplots

        nplot = 0
        if plot:
            nplot += 1
            sps.append(pl.subplot(nplots, 1, nplot))
            xstart = 0
            lines = []
            for s in self.sets:
                nsamples = len(s[0])
                xend = xstart+nsamples
                xs = xrange(xstart, xend)
                lines += [pl.plot(xs, s[0], 'b')]
                lines += [pl.plot(xs, s[1], 'r')]
                # vertical line
                pl.plot([xend, xend], [np.min(s[0]), np.max(s[0])], 'k--')
                xstart = xend
            if len(lines)>1:
                pl.legend(lines[:2], ('Target', 'Prediction'))
            if plot_stats:
                pl.title(self.as_string(short='very'))

        if splot:
            nplot += 1
            sps.append(pl.subplot(nplots, 1, nplot))
            for s in self.sets:
                pl.plot(s[0], s[1], 'o',
                       markeredgewidth=0.2,
                       markersize=2)
            pl.gca().set_aspect('equal')

        if pl.matplotlib.get_backend() == 'TkAgg':
            pl.ion()
        pl.draw()

        return fig, sps

    ##REF: Name was automagically refactored
    def as_string(self, short=False, header=True,  summary=True,
                 description=False):
        """'Pretty print' the statistics"""

        if len(self.sets) == 0:
            return "Empty"

        self.compute()

        stats = self.stats

        if short:
            if short == 'very':
                # " RMSE/RMP_t:%(RMSE/RMP_t).2f" \
                return "%(# of sets)d sets CCe=%(CCe).2f p=%(CCp).2g" \
                       " RMSE:%(RMSE).2f" \
                       " Summary (stacked data): " \
                       "CCe=%(Summary CCe).2f p=%(Summary CCp).2g" \
                       % stats
            else:
                return "%(# of sets)d sets CCe=%(CCe).2f+-%(CCe_std).3f" \
                       " RMSE=%(RMSE).2f+-%(RMSE_std).3f" \
                       " RMSE/RMP_t=%(RMSE/RMP_t).2f+-%(RMSE/RMP_t_std).3f" \
                       % stats

        stats_data = ['RMP_t', 'STD_t', 'RMP_p', 'STD_p']
        # CCp needs tune up of format so excluded
        stats_ = ['CCe', 'RMSE', 'RMSE/RMP_t']
        stats_summary = ['# of sets']

        out = StringIO()

        printed = []
        if header:
            # labels
            printed.append(['Statistics', 'Mean', 'Std', 'Min', 'Max'])
            # underscores
            printed.append(['----------', '-----', '-----', '-----', '-----'])

        def print_stats(printed, stats_):
            # Statistics itself
            for stat in stats_:
                s = [stat]
                for suffix in ['', '_std', '_min', '_max']:
                    s += [ _p2(stats[stat+suffix], 3) ]
                printed.append(s)

        printed.append(["Data:     "])
        print_stats(printed, stats_data)
        printed.append(["Results:  "])
        print_stats(printed, stats_)
        printed.append(["Summary:  "])
        printed.append(["CCe", _p2(stats['Summary CCe']), "", "p=", '%g' % stats['Summary CCp']])
        printed.append(["RMSE", _p2(stats['Summary RMSE'])])
        printed.append(["RMSE/RMP_t", _p2(stats['Summary RMSE/RMP_t'])])

        if summary:
            for stat in stats_summary:
                printed.append([stat] + [_p2(stats[stat])])

        table2string(printed, out)

        if description:
            out.write("\nDescription of printed statistics.\n"
                      " Suffixes: _t - targets, _p - predictions\n")

            for d, val, eq in self._STATS_DESCRIPTION:
                out.write(" %-3s: %s\n" % (d, val))
                if eq is not None:
                    out.write("      " + eq + "\n")

        result = out.getvalue()
        out.close()
        return result


    @property
    def error(self):
        self.compute()
        return self.stats['RMSE']



class ClassifierError(ClassWithCollections):
    """Compute (or return) some error of a (trained) classifier on a dataset.
    """

    confusion = ConditionalAttribute(enabled=False)
    """TODO Think that labels might be also symbolic thus can't directly
       be indicies of the array
    """

    training_stats = ConditionalAttribute(enabled=False,
        doc="Proxy training_stats from underlying classifier.")


    def __init__(self, clf, labels=None, train=True, **kwargs):
        """Initialization.

        Parameters
        ----------
        clf : Classifier
          Either trained or untrained classifier
        labels : list
          if provided, should be a set of labels to add on top of the
          ones present in testdata
        train : bool
          unless train=False, classifier gets trained if
          trainingdata provided to __call__
        """
        ClassWithCollections.__init__(self, **kwargs)
        self.__clf = clf

        self._labels = labels
        """Labels to add on top to existing in testing data"""

        self.__train = train
        """Either to train classifier if trainingdata is provided"""


    __doc__ = enhanced_doc_string('ClassifierError', locals(), ClassWithCollections)


    def __copy__(self):
        """TODO: think... may be we need to copy self.clf"""
        out = ClassifierError.__new__(TransferError)
        ClassifierError.__init__(out, self.clf)
        return out


    def _precall(self, testdataset, trainingdataset=None):
        """Generic part which trains the classifier if necessary
        """
        if not trainingdataset is None:
            if self.__train:
                # XXX can be pretty annoying if triggered inside an algorithm
                # where it cannot be switched of, but retraining might be
                # intended or at least not avoidable.
                # Additonally is_trained docs say:
                #   MUST BE USED WITH CARE IF EVER
                #
                # switching it off for now
                #if self.__clf.is_trained(trainingdataset):
                #    warning('It seems that classifier %s was already trained' %
                #            self.__clf + ' on dataset %s. Please inspect' \
                #                % trainingdataset)
                if self.ca.is_enabled('training_stats'):
                    self.__clf.ca.change_temporarily(
                        enable_ca=['training_stats'])
                self.__clf.train(trainingdataset)
                if self.ca.is_enabled('training_stats'):
                    self.ca.training_stats = \
                        self.__clf.ca.training_stats
                    self.__clf.ca.reset_changed_temporarily()

        if self.__clf.ca.is_enabled('trained_targets') \
               and not self.__clf.__is_regression__ \
               and not testdataset is None:
            newlabels = set(testdataset.sa[self.clf.get_space()].unique) \
                        - set(self.__clf.ca.trained_targets)
            if len(newlabels)>0:
                warning("Classifier %s wasn't trained to classify labels %s" %
                        (self.__clf, newlabels) +
                        " present in testing dataset. Make sure that you have" +
                        " not mixed order/names of the arguments anywhere")

        ### Here checking for if it was trained... might be a cause of trouble
        # XXX disabled since it is unreliable.. just rely on explicit
        # self.__train
        #    if  not self.__clf.is_trained(trainingdataset):
        #        self.__clf.train(trainingdataset)
        #    elif __debug__:
        #        debug('CERR',
        #              'Not training classifier %s since it was ' % `self.__clf`
        #              + ' already trained on dataset %s' % `trainingdataset`)


    def _call(self, testdataset, trainingdataset=None):
        raise NotImplementedError


    def _postcall(self, testdataset, trainingdataset=None, error=None):
        pass


    def __call__(self, testdataset, trainingdataset=None):
        """Compute the transfer error for a certain test dataset.

        If `trainingdataset` is not `None` the classifier is trained using the
        provided dataset before computing the transfer error. Otherwise the
        classifier is used in it's current state to make the predictions on
        the test dataset.

        Returns a scalar value of the transfer error.
        """
        self._precall(testdataset, trainingdataset)
        error = self._call(testdataset, trainingdataset)
        self._postcall(testdataset, trainingdataset, error)
        if __debug__:
            debug('CERR', 'Classifier error on %s: %.2f'
                  % (testdataset, error))
        return error


    def untrain(self):
        """Untrain the `*Error` which relies on the classifier
        """
        self.clf.untrain()


    @property
    def clf(self):
        return self.__clf


    @property
    def labels(self):
        return self._labels



class ConfusionBasedError(ClassifierError):
    """For a given classifier report an error based on internally
    computed error measure (given by some `ConfusionMatrix` stored in
    some conditional attribute of `Classifier`).

    This way we can perform feature selection taking as the error
    criterion either learning error, or transfer to splits error in
    the case of SplitClassifier
    """

    def __init__(self, clf, labels=None, confusion_state="training_stats",
                 **kwargs):
        """Initialization.

        Parameters
        ----------
        clf : Classifier
          Either trained or untrained classifier
        confusion_state
          Id of the conditional attribute which stores `ConfusionMatrix`
        labels : list
          if provided, should be a set of labels to add on top of the
          ones present in testdata
        """
        ClassifierError.__init__(self, clf, labels, **kwargs)

        self.__confusion_state = confusion_state
        """What state to extract from"""

        if not clf.ca.has_key(confusion_state):
            raise ValueError, \
                  "Conditional attribute %s is not defined for classifier %r" % \
                  (confusion_state, clf)
        if not clf.ca.is_enabled(confusion_state):
            if __debug__:
                debug('CERR', "Forcing state %s to be enabled for %r" %
                      (confusion_state, clf))
            clf.ca.enable(confusion_state)


    __doc__ = enhanced_doc_string('ConfusionBasedError', locals(),
                                ClassifierError)


    def _call(self, testdata, trainingdata=None):
        """Extract transfer error. Nor testdata, neither trainingdata is used
        """
        confusion = self.clf.ca[self.__confusion_state].value
        self.ca.confusion = confusion
        return confusion.error

########NEW FILE########
__FILENAME__ = warehouse
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Collection of classifiers to ease the exploration.
"""

__docformat__ = 'restructuredtext'

from mvpa2.base.types import is_sequence_type

# Define sets of classifiers
from mvpa2.clfs.meta import FeatureSelectionClassifier, SplitClassifier, \
     MulticlassClassifier, RegressionAsClassifier
from mvpa2.clfs.smlr import SMLR
from mvpa2.clfs.knn import kNN
from mvpa2.clfs.gda import LDA, QDA
from mvpa2.clfs.gnb import GNB
from mvpa2.kernels.np import LinearKernel, SquaredExponentialKernel, \
     GeneralizedLinearKernel
from mvpa2.featsel.rfe import SplitRFE
from mvpa2.clfs.dummies import RandomClassifier

# Helpers
from mvpa2.base import externals, cfg
from mvpa2.measures.anova import OneWayAnova
from mvpa2.mappers.fx import absolute_features, maxofabs_sample
from mvpa2.clfs.smlr import SMLRWeights
from mvpa2.featsel.helpers import FractionTailSelector, \
    FixedNElementTailSelector, RangeElementSelector
from mvpa2.generators.partition import OddEvenPartitioner
from mvpa2.featsel.base import SensitivityBasedFeatureSelection

# Kernels
from mvpa2.kernels.libsvm import LinearLSKernel, RbfLSKernel, \
     PolyLSKernel, SigmoidLSKernel

_KNOWN_INTERNALS = [ 'knn', 'binary', 'svm', 'linear',
        'smlr', 'does_feature_selection', 'has_sensitivity',
        'multiclass', 'non-linear', 'kernel-based', 'lars',
        'regression', 'regression_based', 'random_tie_breaking',
        'non-deterministic', 'needs_population',
        'libsvm', 'sg', 'meta', 'retrainable', 'gpr',
        'notrain2predict', 'ridge', 'blr', 'gnpp', 'enet', 'glmnet',
        'gnb', 'plr', 'rpy2', 'swig', 'skl', 'lda', 'qda',
        'random-forest', 'extra-trees', 'random']

class Warehouse(object):
    """Class to keep known instantiated classifiers

    Should provide easy ways to select classifiers of needed kind:
    clfswh['linear', 'svm'] should return all linear SVMs
    clfswh['linear', 'multiclass'] should return all linear classifiers
    capable of doing multiclass classification
    """

    def __init__(self, known_tags=None, matches=None):
        """Initialize warehouse

        Parameters
        ----------
        known_tags : list of str
          List of known tags
        matches : dict
          Optional dictionary of additional matches. E.g. since any
          regression can be used as a binary classifier,
          matches={'binary':['regression']}, would allow to provide
          regressions also if 'binary' was requested
          """
        self._known_tags = set(known_tags)
        self.__items = []
        self.__keys = set()
        if matches is None:
            matches = {}
        self.__matches = matches
        self.__descriptions = {}

    def __getitem__(self, *args):
        if isinstance(args[0], tuple):
            args = args[0]

        # so we explicitely handle [:]
        if args == (slice(None),):
            args = []

        # lets remove optional modifier '!'
        dargs = set([str(x).lstrip('!') for x in args]).difference(
            self._known_tags)

        if len(dargs)>0:
            raise ValueError, "Unknown internals %s requested. Known are %s" % \
                  (list(dargs), list(self._known_tags))

        # dummy implementation for now
        result = []
        # check every known item
        for item in self.__items:
            good = True
            # by default each one counts
            for arg in args:
                # check for rejection first
                if arg.startswith('!'):
                    if (arg[1:] in item.__tags__):
                        good = False
                        break
                    else:
                        continue
                # check for inclusion
                found = False
                for arg in [arg] + self.__matches.get(arg, []):
                    if (arg in item.__tags__):
                        found = True
                        break
                good = found
                if not good:
                    break
            if good:
                result.append(item)
        return result

    def __iadd__(self, item):
        if is_sequence_type(item):
            for item_ in item:
                self.__iadd__(item_)
        else:
            if not hasattr(item, '__tags__'):
                raise ValueError, "Cannot register %s " % item + \
                      "which has no __tags__ defined"
            if item.descr in self.__descriptions:
                raise ValueError("Cannot register %s, " % item + \
                      "an item with descriptions '%s' already exists" \
                      % item.descr)
            if len(item.__tags__) == 0:
                raise ValueError, "Cannot register %s " % item + \
                      "which has empty __tags__"
            clf_internals = set(item.__tags__)
            if clf_internals.issubset(self._known_tags):
                self.__items.append(item)
                self.__keys |= clf_internals
            else:
                raise ValueError, 'Unknown clf internal(s) %s' % \
                      clf_internals.difference(self._known_tags)
            # access by descr
            self.__descriptions[item.descr] = item
        return self

    def get_by_descr(self, descr):
        return self.__descriptions[descr]

    @property
    def internals(self):
        """Known internal tags of the classifiers
        """
        return self.__keys

    def listing(self):
        """Listing (description + internals) of registered items
        """
        return [(x.descr, x.__tags__) for x in self.__items]

    def print_registered(self, *args):
        if not len(args):
            args = (slice(None))
        import numpy as np
        import textwrap
        # sort by description
        for lrn in sorted(self.__getitem__(args), key=lambda x: x.descr.lower()):
            print '%s\n%s' % (
                    lrn.descr,
                    textwrap.fill(', '.join(np.unique(lrn.__tags__)), 70,
                                  initial_indent=' ' * 4,
                                  subsequent_indent=' ' * 4)
                )

    @property
    def items(self):
        """Registered items
        """
        return self.__items

    @property
    def descriptions(self):
        """Descriptions of registered items"""
        return self.__descriptions.keys()


clfswh = Warehouse(known_tags=_KNOWN_INTERNALS) # classifiers
regrswh = Warehouse(known_tags=_KNOWN_INTERNALS) # regressions

# NB:
#  - Nu-classifiers are turned off since for haxby DS default nu
#    is an 'infisible' one
#  - Python's SMLR is turned off for the duration of development
#    since it is slow and results should be the same as of C version
#
clfswh += [ SMLR(lm=0.1, implementation="C", descr="SMLR(lm=0.1)"),
          SMLR(lm=1.0, implementation="C", descr="SMLR(lm=1.0)"),
          #SMLR(lm=10.0, implementation="C", descr="SMLR(lm=10.0)"),
          #SMLR(lm=100.0, implementation="C", descr="SMLR(lm=100.0)"),
          #SMLR(implementation="Python", descr="SMLR(Python)")
          ]

clfswh += \
     [ MulticlassClassifier(SMLR(lm=0.1),
                            descr='Pairs+maxvote multiclass on SMLR(lm=0.1)') ]

clfswh += [ RandomClassifier(descr="Random"),
            RandomClassifier(same=True, descr="RandomSame"),
            ]

if externals.exists('libsvm'):
    from mvpa2.clfs.libsvmc import svm as libsvm
    clfswh._known_tags.update(libsvm.SVM._KNOWN_IMPLEMENTATIONS.keys())
    clfswh += [libsvm.SVM(descr="libsvm.LinSVM(C=def)", probability=1),
             libsvm.SVM(
                 C=-10.0, descr="libsvm.LinSVM(C=10*def)", probability=1),
             libsvm.SVM(
                 C=1.0, descr="libsvm.LinSVM(C=1)", probability=1),
             libsvm.SVM(svm_impl='NU_SVC',
                        descr="libsvm.LinNuSVM(nu=def)", probability=1)
             ]
    clfswh += [libsvm.SVM(kernel=RbfLSKernel(), descr="libsvm.RbfSVM()"),
             libsvm.SVM(kernel=RbfLSKernel(), svm_impl='NU_SVC',
                        descr="libsvm.RbfNuSVM(nu=def)"),
             libsvm.SVM(kernel=PolyLSKernel(),
                        descr='libsvm.PolySVM()', probability=1),
             #libsvm.svm.SVM(kernel=SigmoidLSKernel(),
             #               svm_impl='C_SVC',
             #               descr='libsvm.SigmoidSVM()'),
             ]

    # regressions
    regrswh._known_tags.update(['EPSILON_SVR', 'NU_SVR'])
    regrswh += [libsvm.SVM(svm_impl='EPSILON_SVR', descr='libsvm epsilon-SVR'),
                libsvm.SVM(svm_impl='NU_SVR', descr='libsvm nu-SVR')]

if externals.exists('shogun'):
    from mvpa2.clfs import sg
    
    from mvpa2.kernels.sg import LinearSGKernel, PolySGKernel, RbfSGKernel
    clfswh._known_tags.update(sg.SVM._KNOWN_IMPLEMENTATIONS)

    # TODO: some classifiers are not yet ready to be used out-of-the-box in
    # PyMVPA, thus we don't populate warehouse with their instances
    bad_classifiers = [
        'mpd',  # was segfault, now non-training on testcases, and XOR.
                # and was described as "for educational purposes", thus
                # shouldn't be used for real data ;-)
        # Should be a drop-in replacement for lightsvm
        'gpbt', # fails to train for testAnalyzerWithSplitClassifier
                # also 'retraining' doesn't work -- fails to generalize
        'gmnp', # would fail with 'assertion Cache_Size > 2'
                # if shogun < 0.6.3, also refuses to train
        'svrlight', # fails to 'generalize' as a binary classifier
                    # after 'binning'
        'krr', # fails to generalize
        'svmocas', # fails to generalize
        'libsvr'                        # XXXregr removing regressions as classifiers
        ]
    if not externals.exists('sg_fixedcachesize'):
        # would fail with 'assertion Cache_Size > 2' if shogun < 0.6.3
        bad_classifiers.append('gnpp')

    for impl in sg.SVM._KNOWN_IMPLEMENTATIONS:
        # Uncomment the ones to disable
        if impl in bad_classifiers:
            continue
        clfswh += [
            sg.SVM(
                descr="sg.LinSVM(C=def)/%s" % impl, svm_impl=impl),
            sg.SVM(
                C=-10.0, descr="sg.LinSVM(C=10*def)/%s" % impl, svm_impl=impl),
            sg.SVM(
                C=1.0, descr="sg.LinSVM(C=1)/%s" % impl, svm_impl=impl),
            ]
        if not impl in ['svmocas']:     # inherently linear only
            clfswh += [
                sg.SVM(kernel=RbfSGKernel(),
                       descr="sg.RbfSVM()/%s" % impl, svm_impl=impl),
    #            sg.SVM(kernel=RbfSGKernel(),
    #                   descr="sg.RbfSVM(gamma=0.1)/%s"
    #                    % impl, svm_impl=impl, gamma=0.1),
    #           sg.SVM(descr="sg.SigmoidSVM()/%s"
    #                   % impl, svm_impl=impl, kernel=SigmoidSGKernel(),),
                ]

    _optional_regressions = []
    if externals.exists('shogun.krr') and externals.versions['shogun'] >= '0.9':
        _optional_regressions += ['krr']
    for impl in ['libsvr'] + _optional_regressions:# \
        # XXX svrlight sucks in SG -- dont' have time to figure it out
        #+ ([], ['svrlight'])['svrlight' in sg.SVM._KNOWN_IMPLEMENTATIONS]:
        regrswh._known_tags.update([impl])
        regrswh += [ sg.SVM(svm_impl=impl, descr='sg.LinSVMR()/%s' % impl),
                   #sg.SVM(svm_impl=impl, kernel_type='RBF',
                   #       descr='sg.RBFSVMR()/%s' % impl),
                   ]

if len(clfswh['svm', 'linear']) > 0:
    # if any SVM implementation is known, import default ones
    from mvpa2.clfs.svm import *

# lars from R via RPy
if externals.exists('lars'):
    import mvpa2.clfs.lars as lars
    from mvpa2.clfs.lars import LARS
    for model in lars.known_models:
        # XXX create proper repository of classifiers!
        lars_clf = RegressionAsClassifier(
            LARS(descr="LARS(%s)" % model,
                 model_type=model),
            descr='LARS(model_type=%r) classifier' % model)
        clfswh += lars_clf

        # is a regression, too
        lars_regr = LARS(descr="_LARS(%s)" % model,
                         model_type=model)
        regrswh += lars_regr
        # clfswh += MulticlassClassifier(lars,
        #             descr='Multiclass %s' % lars.descr)

## Still fails unittests battery although overhauled otherwise.
## # enet from R via RPy2
## if externals.exists('elasticnet'):
##     from mvpa2.clfs.enet import ENET
##     clfswh += RegressionAsClassifier(ENET(),
##                                      descr="RegressionAsClassifier(ENET())")
##     regrswh += ENET(descr="ENET()")

# glmnet from R via RPy
if externals.exists('glmnet'):
    from mvpa2.clfs.glmnet import GLMNET_C, GLMNET_R
    clfswh += GLMNET_C(descr="GLMNET_C()")
    regrswh += GLMNET_R(descr="GLMNET_R()")

# LDA/QDA
clfswh += LDA(descr='LDA()')
clfswh += QDA(descr='QDA()')

if externals.exists('skl'):
    _skl_version = externals.versions['skl']
    _skl_api09 = _skl_version >= '0.9'
    def _skl_import(submod, class_):
        if _skl_api09:
            submod_ = __import__('sklearn.%s' % submod, fromlist=[submod])
        else:
            submod_ = __import__('scikits.learn.%s' % submod, fromlist=[submod])
        return getattr(submod_, class_)

    sklLDA = _skl_import('lda', 'LDA')
    from mvpa2.clfs.skl.base import SKLLearnerAdapter
    clfswh += SKLLearnerAdapter(sklLDA(),
                                tags=['lda', 'linear', 'multiclass', 'binary'],
                                descr='skl.LDA()')

    if _skl_version >= '0.10':
        # Out of Bag Estimates
        sklRandomForestClassifier = _skl_import('ensemble', 'RandomForestClassifier')
        clfswh += SKLLearnerAdapter(sklRandomForestClassifier(),
                                     tags=['random-forest', 'linear', 'non-linear',
                                           'binary', 'multiclass',
                                           'non-deterministic', 'needs_population',],
                                     descr='skl.RandomForestClassifier()')

        sklRandomForestRegression = _skl_import('ensemble', 'RandomForestRegressor')
        regrswh += SKLLearnerAdapter(sklRandomForestRegression(),
                                     tags=['random-forest', 'linear', 'non-linear',
                                           'regression',
                                           'non-deterministic', 'needs_population',],
                                     descr='skl.RandomForestRegression()')


        sklExtraTreesClassifier = _skl_import('ensemble', 'ExtraTreesClassifier')
        clfswh += SKLLearnerAdapter(sklExtraTreesClassifier(),
                                     tags=['extra-trees', 'linear', 'non-linear',
                                           'binary', 'multiclass',
                                           'non-deterministic', 'needs_population',],
                                     descr='skl.ExtraTreesClassifier()')

        sklExtraTreesRegression = _skl_import('ensemble', 'ExtraTreesRegressor')
        regrswh += SKLLearnerAdapter(sklExtraTreesRegression(),
                                     tags=['extra-trees', 'linear', 'non-linear',
                                           'regression',
                                           'non-deterministic', 'needs_population',],
                                     descr='skl.ExtraTreesRegression()')


    if _skl_version >= '0.8':
        if _skl_version >= '0.14':
            sklPLSRegression = _skl_import('cross_decomposition', 'PLSRegression')
        else:
            sklPLSRegression = _skl_import('pls', 'PLSRegression')
        # somewhat silly use of PLS, but oh well
        regrswh += SKLLearnerAdapter(sklPLSRegression(n_components=1),
                                     tags=['linear', 'regression'],
                                     enforce_dim=1,
                                     descr='skl.PLSRegression_1d()')

    if externals.versions['skl'] >= '0.6.0':
        sklLars = _skl_import('linear_model',
                              _skl_api09 and 'Lars' or 'LARS')
        sklLassoLars = _skl_import('linear_model',
                                   _skl_api09 and 'LassoLars' or 'LassoLARS')
        sklElasticNet = _skl_import('linear_model', 'ElasticNet')
        _lars_tags = ['lars', 'linear', 'regression', 'does_feature_selection']

        _lars = SKLLearnerAdapter(sklLars(),
                                  tags=_lars_tags,
                                  descr='skl.Lars()')

        _lasso_lars = SKLLearnerAdapter(sklLassoLars(alpha=0.01),
                                        tags=_lars_tags,
                                        descr='skl.LassoLars()')

        _elastic_net = SKLLearnerAdapter(
            sklElasticNet(alpha=.01,
                          **{'l1_ratio' if externals.versions['skl'] >= '0.13'
                                        else 'rho': .3}),
            tags=['enet', 'regression', 'linear', # 'has_sensitivity',
                 'does_feature_selection'],
            descr='skl.ElasticNet()')

        regrswh += [_lars, _lasso_lars, _elastic_net]
        clfswh += [RegressionAsClassifier(_lars, descr="skl.Lars_C()"),
                   RegressionAsClassifier(_lasso_lars, descr="skl.LassoLars_C()"),
                   RegressionAsClassifier(_elastic_net, descr="skl.ElasticNet_C()"),
                   ]

    if _skl_version >= '0.10':
        sklLassoLarsIC = _skl_import('linear_model', 'LassoLarsIC')
        _lasso_lars_ic = SKLLearnerAdapter(sklLassoLarsIC(),
                                           tags=_lars_tags,
                                           descr='skl.LassoLarsIC()')
        regrswh += [_lasso_lars_ic]
        clfswh += [RegressionAsClassifier(_lasso_lars_ic,
                                          descr='skl.LassoLarsIC_C()')]

# kNN
clfswh += kNN(k=5, descr="kNN(k=5)")
clfswh += kNN(k=5, voting='majority', descr="kNN(k=5, voting='majority')")

clfswh += \
    FeatureSelectionClassifier(
        kNN(),
        SensitivityBasedFeatureSelection(
           SMLRWeights(SMLR(lm=1.0, implementation="C"),
                       postproc=maxofabs_sample()),
           RangeElementSelector(mode='select')),
        descr="kNN on SMLR(lm=1) non-0")

clfswh += \
    FeatureSelectionClassifier(
        kNN(),
        SensitivityBasedFeatureSelection(
           OneWayAnova(),
           FractionTailSelector(0.05, mode='select', tail='upper')),
        descr="kNN on 5%(ANOVA)")

clfswh += \
    FeatureSelectionClassifier(
        kNN(),
        SensitivityBasedFeatureSelection(
           OneWayAnova(),
           FixedNElementTailSelector(50, mode='select', tail='upper')),
        descr="kNN on 50(ANOVA)")


# GNB
clfswh += GNB(descr="GNB()")
clfswh += GNB(common_variance=True, descr="GNB(common_variance=True)")
clfswh += GNB(prior='uniform', descr="GNB(prior='uniform')")
clfswh += \
    FeatureSelectionClassifier(
        GNB(),
        SensitivityBasedFeatureSelection(
           OneWayAnova(),
           FractionTailSelector(0.05, mode='select', tail='upper')),
        descr="GNB on 5%(ANOVA)")

# GPR
if externals.exists('scipy'):
    from mvpa2.clfs.gpr import GPR

    regrswh += GPR(kernel=LinearKernel(), descr="GPR(kernel='linear')")
    regrswh += GPR(kernel=SquaredExponentialKernel(),
                   descr="GPR(kernel='sqexp')")

    # Add wrapped GPR as a classifier
    gprcb = RegressionAsClassifier(
        GPR(kernel=GeneralizedLinearKernel()), descr="GPRC(kernel='linear')")
    # lets remove multiclass label from it
    gprcb.__tags__.pop(gprcb.__tags__.index('multiclass'))
    clfswh += gprcb

    # and create a proper multiclass one
    clfswh += MulticlassClassifier(
        RegressionAsClassifier(
            GPR(kernel=GeneralizedLinearKernel())),
        descr="GPRCM(kernel='linear')")

# BLR
from mvpa2.clfs.blr import BLR
clfswh += RegressionAsClassifier(BLR(descr="BLR()"),
                                 descr="BLR Classifier")

#PLR
from mvpa2.clfs.plr import PLR
clfswh += PLR(descr="PLR()")
if externals.exists('scipy'):
    clfswh += PLR(reduced=0.05, descr="PLR(reduced=0.01)")

# SVM stuff

if len(clfswh['linear', 'svm']) > 0:

    linearSVMC = clfswh['linear', 'svm',
                             cfg.get('svm', 'backend', default='libsvm').lower()
                             ][0]

    # "Interesting" classifiers
    clfswh += \
         FeatureSelectionClassifier(
             linearSVMC.clone(),
             SensitivityBasedFeatureSelection(
                SMLRWeights(SMLR(lm=0.1, implementation="C"),
                            postproc=maxofabs_sample()),
                RangeElementSelector(mode='select')),
             descr="LinSVM on SMLR(lm=0.1) non-0")

    _rfeclf = linearSVMC.clone()
    clfswh += \
         FeatureSelectionClassifier(
             _rfeclf,
             SplitRFE(
                 _rfeclf,
                 OddEvenPartitioner(),
                 fselector=FractionTailSelector(
                     0.2, mode='discard', tail='lower')),
             descr="LinSVM with nested-CV RFE")

    clfswh += \
        FeatureSelectionClassifier(
            linearSVMC.clone(),
            SensitivityBasedFeatureSelection(
                SMLRWeights(SMLR(lm=1.0, implementation="C"),
                            postproc=maxofabs_sample()),
                RangeElementSelector(mode='select')),
            descr="LinSVM on SMLR(lm=1) non-0")


    # "Interesting" classifiers
    clfswh += \
        FeatureSelectionClassifier(
            RbfCSVMC(),
            SensitivityBasedFeatureSelection(
               SMLRWeights(SMLR(lm=1.0, implementation="C"),
                           postproc=maxofabs_sample()),
               RangeElementSelector(mode='select')),
            descr="RbfSVM on SMLR(lm=1) non-0")

    clfswh += \
        FeatureSelectionClassifier(
            linearSVMC.clone(),
            SensitivityBasedFeatureSelection(
               OneWayAnova(),
               FractionTailSelector(0.05, mode='select', tail='upper')),
            descr="LinSVM on 5%(ANOVA)")

    clfswh += \
        FeatureSelectionClassifier(
            linearSVMC.clone(),
            SensitivityBasedFeatureSelection(
               OneWayAnova(),
               FixedNElementTailSelector(50, mode='select', tail='upper')),
            descr="LinSVM on 50(ANOVA)")

    clfswh += \
        FeatureSelectionClassifier(
            linearSVMC.clone(),
            SensitivityBasedFeatureSelection(
               linearSVMC.get_sensitivity_analyzer(postproc=maxofabs_sample()),
               FractionTailSelector(0.05, mode='select', tail='upper')),
            descr="LinSVM on 5%(SVM)")

    clfswh += \
        FeatureSelectionClassifier(
            linearSVMC.clone(),
            SensitivityBasedFeatureSelection(
               linearSVMC.get_sensitivity_analyzer(postproc=maxofabs_sample()),
               FixedNElementTailSelector(50, mode='select', tail='upper')),
            descr="LinSVM on 50(SVM)")


    ### Imports which are specific to RFEs
    # from mvpa2.datasets.splitters import OddEvenSplitter
    # from mvpa2.clfs.transerror import TransferError
    # from mvpa2.featsel.rfe import RFE
    # from mvpa2.featsel.helpers import FixedErrorThresholdStopCrit
    # from mvpa2.clfs.transerror import ConfusionBasedError

    # SVM with unbiased RFE -- transfer-error to another splits, or in
    # other terms leave-1-out error on the same dataset
    # Has to be bound outside of the RFE definition since both analyzer and
    # error should use the same instance.
    rfesvm_split = SplitClassifier(linearSVMC)#clfswh['LinearSVMC'][0])

    # "Almost" classical RFE. If this works it would differ only that
    # our transfer_error is based on internal splitting and classifier used
    # within RFE is a split classifier and its sensitivities per split will get
    # averaged
    #

    #clfswh += \
    #  FeatureSelectionClassifier(
    #    clf = LinearCSVMC(), #clfswh['LinearSVMC'][0],         # we train LinearSVM
    #    feature_selection = RFE(             # on features selected via RFE
    #        # based on sensitivity of a clf which does splitting internally
    #        sensitivity_analyzer=rfesvm_split.get_sensitivity_analyzer(),
    #        transfer_error=ConfusionBasedError(
    #           rfesvm_split,
    #           confusion_state="confusion"),
    #           # and whose internal error we use
    #        feature_selector=FractionTailSelector(
    #                           0.2, mode='discard', tail='lower'),
    #                           # remove 20% of features at each step
    #        update_sensitivity=True),
    #        # update sensitivity at each step
    #    descr='LinSVM+RFE(splits_avg)' )
    #
    #clfswh += \
    #  FeatureSelectionClassifier(
    #    clf = LinearCSVMC(),                 # we train LinearSVM
    #    feature_selection = RFE(             # on features selected via RFE
    #        # based on sensitivity of a clf which does splitting internally
    #        sensitivity_analyzer=rfesvm_split.get_sensitivity_analyzer(),
    #        transfer_error=ConfusionBasedError(
    #           rfesvm_split,
    #           confusion_state="confusion"),
    #           # and whose internal error we use
    #        feature_selector=FractionTailSelector(
    #                           0.2, mode='discard', tail='lower'),
    #                           # remove 20% of features at each step
    #        update_sensitivity=False),
    #        # update sensitivity at each step
    #    descr='LinSVM+RFE(splits_avg,static)' )

    rfesvm = LinearCSVMC()

    # This classifier will do RFE while taking transfer error to testing
    # set of that split. Resultant classifier is voted classifier on top
    # of all splits, let see what that would do ;-)
    #clfswh += \
    #  SplitClassifier(                      # which does splitting internally
    #   FeatureSelectionClassifier(
    #    clf = LinearCSVMC(),
    #    feature_selection = RFE(             # on features selected via RFE
    #        sensitivity_analyzer=\
    #            rfesvm.get_sensitivity_analyzer(postproc=absolute_features()),
    #        transfer_error=TransferError(rfesvm),
    #        stopping_criterion=FixedErrorThresholdStopCrit(0.05),
    #        feature_selector=FractionTailSelector(
    #                           0.2, mode='discard', tail='lower'),
    #                           # remove 20% of features at each step
    #        update_sensitivity=True)),
    #        # update sensitivity at each step
    #    descr='LinSVM+RFE(N-Fold)')
    #
    #
    #clfswh += \
    #  SplitClassifier(                      # which does splitting internally
    #   FeatureSelectionClassifier(
    #    clf = LinearCSVMC(),
    #    feature_selection = RFE(             # on features selected via RFE
    #        sensitivity_analyzer=\
    #            rfesvm.get_sensitivity_analyzer(postproc=absolute_features()),
    #        transfer_error=TransferError(rfesvm),
    #        stopping_criterion=FixedErrorThresholdStopCrit(0.05),
    #        feature_selector=FractionTailSelector(
    #                           0.2, mode='discard', tail='lower'),
    #                           # remove 20% of features at each step
    #        update_sensitivity=True)),
    #        # update sensitivity at each step
    #   splitter = OddEvenSplitter(),
    #   descr='LinSVM+RFE(OddEven)')


########NEW FILE########
__FILENAME__ = _svmbase
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Common to all SVM implementations functionality. For internal use only"""

__docformat__ = 'restructuredtext'

import numpy as np
import textwrap

from mvpa2.support.copy import deepcopy

from mvpa2.base import warning
from mvpa2.base.types import is_sequence_type

from mvpa2.kernels.base import Kernel
from mvpa2.base.dochelpers import handle_docstring, _rst, _rst_section, \
     _rst_indentstr

from mvpa2.clfs.base import Classifier
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import EnsureListOf

if __debug__:
    from mvpa2.base import debug


class _SVM(Classifier):
    """Support Vector Machine Classifier.

    Base class for all external SVM implementations.
    """

    """
    Derived classes should define:

    * _KERNELS: map(dict) should define assignment to a tuple containing
      implementation kernel type, list of parameters adherent to the
      kernel, and sensitivity analyzer e.g.::

        _KERNELS = {
             'linear': (shogun.Kernel.LinearKernel, (), LinearSVMWeights),
             'rbf' :   (shogun.Kernel.GaussianKernel, ('gamma',), None),
             ...
             }

    * _KNOWN_IMPLEMENTATIONS: map(dict) should define assignment to a
      tuple containing implementation of the SVM, list of parameters
      adherent to the implementation, additional internals, and
      description e.g.::

        _KNOWN_IMPLEMENTATIONS = {
          'C_SVC' : (svm.svmc.C_SVC, ('C',),
                   ('binary', 'multiclass'), 'C-SVM classification'),
          ...
          }

    """

    
    _ATTRIBUTE_COLLECTIONS = ['params'] # enforce presence of params collections

    # Placeholder: map kernel names to sensitivity classes, ie
    # 'linear':LinearSVMWeights, for each backend
    _KNOWN_SENSITIVITIES={}
    kernel = Parameter(None,
                       # XXX: Currently, can't be ensured using constraints
                       # allowedtype=Kernel,
                       doc='Kernel object', index=-1)

    _SVM_PARAMS = {
        'C' : Parameter(-1.0,
                  doc='Trade-off parameter between width of the '
                      'margin and number of support vectors. Higher C -- '
                      'more rigid margin SVM. In linear kernel, negative '
                      'values provide automatic scaling of their value '
                      'according to the norm of the data'),
        'nu' : Parameter(0.5, min=0.0, max=1.0,
                  doc='Fraction of datapoints within the margin'),
        'cache_size': Parameter(100,
                  doc='Size of the kernel cache, specified in megabytes'),
        'tube_epsilon': Parameter(0.01,
                  doc='Epsilon in epsilon-insensitive loss function of '
                      'epsilon-SVM regression (SVR)'),
        'tau': Parameter(1e-6, doc='TAU parameter of KRR regression in shogun'),
        'probability': Parameter(0,
                  doc='Flag to signal either probability estimate is obtained '
                      'within LIBSVM'),
        'shrinking': Parameter(1, doc='Either shrinking is to be conducted'),
        'weight_label': Parameter([], constraints=EnsureListOf(int),
                  doc='To be used in conjunction with weight for custom '
                      'per-label weight'),
        # TODO : merge them into a single dictionary
        'weight': Parameter([], constraints=EnsureListOf(float),
                  doc='Custom weights per label'),
        # For some reason setting up epsilon to 1e-5 slowed things down a bit
        # in comparison to how it was before (in yoh/master) by up to 20%... not clear why
        # may be related to 1e-3 default within _svm.py?
        'epsilon': Parameter(5e-5, min=1e-10,
                  doc='Tolerance of termination criteria. (For nu-SVM default is 0.001)')
        }

    _KNOWN_PARAMS = ()                  # just a placeholder to please lintian
    """Parameters which are specific to a given instantiation of SVM
    """

    __tags__ = [ 'svm', 'kernel-based', 'swig' ]

    def __init__(self, **kwargs):
        """Init base class of SVMs. *Not to be publicly used*

        TODO: handling of parameters might migrate to be generic for
        all classifiers. SVMs are chosen to be testbase for that
        functionality to see how well it would fit.
        """

        # Check if requested implementation is known
        svm_impl = kwargs.get('svm_impl', None)
        if not svm_impl in self._KNOWN_IMPLEMENTATIONS:
            raise ValueError, \
                  "Unknown SVM implementation '%s' is requested for %s." \
                  "Known are: %s" % (svm_impl, self.__class__,
                                     self._KNOWN_IMPLEMENTATIONS.keys())
        self._svm_impl = svm_impl

        impl, add_params, add_internals, descr = \
              self._KNOWN_IMPLEMENTATIONS[svm_impl]

        # Add corresponding parameters to 'known' depending on the
        # implementation chosen
        if add_params is not None:
            self._KNOWN_PARAMS = \
                 self._KNOWN_PARAMS[:] + list(add_params)


        # Assign per-instance __tags__
        self.__tags__ = self.__tags__[:] + [svm_impl]

        # Add corresponding internals
        if add_internals is not None:
            self.__tags__ += list(add_internals)
        self.__tags__.append(svm_impl)

        k = kwargs.get('kernel', None)
        if k is None:
            kwargs['kernel'] = self.__default_kernel_class__()
        if 'linear' in ('%s'%kwargs['kernel']).lower(): # XXX not necessarily best
            self.__tags__ += [ 'linear', 'has_sensitivity' ]
        else:
            self.__tags__ += [ 'non-linear' ]

        # pop out all args from **kwargs which are known to be SVM parameters
        _args = {}
        for param in self._KNOWN_PARAMS + ['svm_impl']: # Update to remove kp's?
            if param in kwargs:
                _args[param] = kwargs.pop(param)

        try:
            Classifier.__init__(self, **kwargs)
            
        except TypeError, e:
            if "__init__() got an unexpected keyword argument " in e.args[0]:
                # TODO: make it even more specific -- if that argument is listed
                # within _SVM_PARAMS
                e.args = tuple( [e.args[0] +
                                 "\n Given SVM instance of class %s knows following parameters: %s" %
                                 (self.__class__, self._KNOWN_PARAMS) + \
                                 list(e.args)[1:]])
            raise e

        # populate collections and add values from arguments
        for paramfamily, paramset in ( (self._KNOWN_PARAMS, self.params),):
            for paramname in paramfamily:
                if not (paramname in self._SVM_PARAMS):
                    raise ValueError, "Unknown parameter %s" % paramname + \
                          ". Known SVM params are: %s" % self._SVM_PARAMS.keys()
                param = deepcopy(self._SVM_PARAMS[paramname])
                if paramname in _args:
                    param.value = _args[paramname]
                    # XXX might want to set default to it -- not just value

                paramset[paramname] = param

        # TODO: Below commented out because kernel_type has been removed.  
        # Find way to set default C as necessary
        
        # tune up C if it has one and non-linear classifier is used
        #if self.params.has_key('C') and kernel_type != "linear" \
               #and self.params['C'].is_default:
            #if __debug__:
                #debug("SVM_", "Assigning default C value to be 1.0 for SVM "
                      #"%s with non-linear kernel" % self)
            #self.params['C'].default = 1.0

        # Some postchecks
        if self.params.has_key('weight') and self.params.has_key('weight_label'):
            if not len(self.params.weight_label) == len(self.params.weight):
                raise ValueError, "Lenghts of 'weight' and 'weight_label' lists " \
                      "must be equal."

            
        if __debug__:
            debug("SVM", "Initialized %s with kernel %s" % 
                  (self, self.params.kernel))


    # XXX RF
    @property
    def kernel_params(self):
        if self.params.kernel:
            return self.params.kernel.params
        return None
    
    def __repr__(self):
        """Definition of the object summary over the object
        """
        res = "%s(svm_impl=%r" % \
              (self.__class__.__name__, self._svm_impl)
        sep = ", "
        # XXX TODO: we should have no kernel_params any longer
        for col in [self.params]:#, self.kernel_params]:
            for k in col.keys():
                # list only params with not default values
                if col[k].is_default: continue
                res += "%s%s=%r" % (sep, k, col[k].value)
                #sep = ', '
        ca = self.ca
        for name, invert in ( ('enable', False), ('disable', True) ):
            ca_chosen = ca._get_enabled(nondefault=False, invert=invert)
            if len(ca_chosen):
                res += sep + "%s_ca=%r" % (name, ca_chosen)

        res += ")"
        return res

    ##REF: Name was automagically refactored
    def _get_cvec(self, data):
        """Estimate default and return scaled by it negative user's C values
        """
        if not self.params.has_key('C'):#svm_type in [_svm.svmc.C_SVC]:
            raise RuntimeError, \
                  "Requested estimation of default C whenever C was not set"

        C = self.params.C
        if not is_sequence_type(C):
            # we were not given a tuple for balancing between classes
            C = [C]

        Cs = list(C[:])               # copy
        for i in xrange(len(Cs)):
            if Cs[i] < 0:
                Cs[i] = self._get_default_c(data.samples)*abs(Cs[i])
                if __debug__:
                    debug("SVM", "Default C for %s was computed to be %s" %
                          (C[i], Cs[i]))

        return Cs

    ##REF: Name was automagically refactored
    def _get_default_c(self, data):
        """Compute default C

        TODO: for non-linear SVMs
        """

        if self.params.kernel.__kernel_name__ == 'linear':
            # TODO: move into a function wrapper for
            #       np.linalg.norm
            if np.issubdtype(data.dtype, np.integer):
                # we are dealing with integers and overflows are
                # possible, so assure working with floats
                def sq_func(x):
                    y = x.astype(float) # copy as float
                    y *= y              # in-place square
                    return y
            else:
                sq_func = np.square
            # perform it per each sample so we do not double memory
            # with calling sq_func on full data
            # Having a list of norms here automagically resolves issue
            # with memmapped operations on which return
            # in turn another memmap
            datasetnorm = np.mean([np.sqrt(np.sum(sq_func(s)))
                                   for s in data])
            if datasetnorm == 0:
                warning("Obtained degenerate data with zero norm for training "
                        "of %s.  Scaling of C cannot be done." % self)
                return 1.0
            value = 1.0/(datasetnorm**2)
            if __debug__:
                debug("SVM", "Default C computed to be %f" % value)
        else:
            warning("TODO: Computation of default C is not yet implemented" +
                    " for non-linear SVMs. Assigning 1.0")
            value = 1.0

        return value


    # TODO: make part of kernel object
    #def _getDefaultGamma(self, dataset):
        #"""Compute default Gamma

        #TODO: unify bloody libsvm interface so it makes use of this function.
        #Now it is computed within SVMModel.__init__
        #"""

        ## TODO: Check validity of this w/ new kernels (ie sg.Rbf has sigma)
        #if self.kernel_params.has_key('gamma'):
            #value = 1.0 / len(dataset.uniquetargets)
            #if __debug__:
                #debug("SVM", "Default Gamma is computed to be %f" % value)
        #else:
            #raise RuntimeError, "Shouldn't ask for default Gamma here"

        #return value

    ##REF: Name was automagically refactored
    def get_sensitivity_analyzer(self, **kwargs):
        """Returns an appropriate SensitivityAnalyzer."""

        sana = self._KNOWN_SENSITIVITIES.get(self.params.kernel.__kernel_name__,
                                             None)
        if sana:
            return sana(self, **kwargs)
        else:
            raise NotImplementedError, \
                  "Sensitivity analyzers for kernel %s is unknown" % \
                  self.params.kernel


    @classmethod
    ##REF: Name was automagically refactored
    def _customize_doc(cls):
        #cdoc_old = cls.__doc__
        # Need to append documentation to __init__ method
        idoc_old = cls.__init__.__doc__

        idoc = """
SVM/SVR definition is dependent on specifying kernel, implementation
type, and parameters for each of them which vary depending on the
choices made.

Desired implementation is specified in ``svm_impl`` argument. Here
is the list if implementations known to this class, along with
specific to them parameters (described below among the rest of
parameters), and what tasks it is capable to deal with
(e.g. regression, binary and/or multiclass classification):

"""
        # XXX Deprecate
        # To not confuse sphinx -- lets avoid Implementations section
        # %s""" % (_rst_section('Implementations'),)


        class NOSClass(object):
            """Helper -- NothingOrSomething ;)
            If list is not empty -- return its entries within string s
            """
            def __init__(self):
                self.seen = []
            def __call__(self, l, s, empty=''):
                if l is None or not len(l):
                    return empty
                else:
                    lsorted = list(l)
                    lsorted.sort()
                    self.seen += lsorted
                    return s % (', '.join(lsorted))
        NOS = NOSClass()

        # Describe implementations
        idoc += ''.join(
            ['\n%s%s : %s' % (_rst_indentstr, k, v[3])
             + NOS(v[1], "\n" + _rst_indentstr + "  Parameters: %s")
             + NOS(v[2], "%s" % _rst(('','\n')[int(len(v[1])>0)], '')
                   + _rst_indentstr + "  Capabilities: %s")
             for k,v in cls._KNOWN_IMPLEMENTATIONS.iteritems()])

        # Describe kernels
        idoc += """

Kernel choice is specified as a kernel instance with kwargument ``kernel``.
Some kernels (e.g. Linear) might allow computation of per feature
sensitivity.

"""
        # XXX Deprecate
        # %s""" % (_rst_section('Kernels'),)

        #idoc += ''.join(
        #    ['\n%s%s' % (_rst_indentstr, k)
        #     + ('', ' : provides sensitivity')[int(v[2] is not None)]
        #     + '\n    ' + NOS(v[1], '%s', 'No parameters')
        #     for k,v in cls._KERNELS.iteritems()])

        # Finally parameters
        NOS.seen += cls._KNOWN_PARAMS# + cls._KNOWN_KERNEL_PARAMS

        idoc += '\n' + _rst_section('Parameters') + '\n' + '\n'.join(
            [v._paramdoc()
             for k,v in cls._SVM_PARAMS.iteritems()
             if k in NOS.seen])

        cls.__dict__['__init__'].__doc__ = handle_docstring(idoc_old) + idoc


# populate names in parameters
for k, v in _SVM._SVM_PARAMS.iteritems():
    v._set_name(k)


########NEW FILE########
__FILENAME__ = cmd_atlaslabeler
#!/usr/bin/python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Query atlases for anatomical labels of voxel coordinates, or their statistics

Examples:

> pymvpa2 atlaslabeler -s -A talairach-dist -d 10 -R Closest\ Gray -l Structure,Brodmann\ area  -cC mask.nii.gz

produces a summary per each structure and brodmann area, for each voxel
looking within 10mm radius for the closest gray matter voxel.

Simpler, more reliable, and faster usage is by providing a
corresponding atlas volume image registered to the volume at hands,
e.g.:

> pymvpa2 atlaslabeler -f MNI-prob-bold.nii.gz -A MNI -s mask_vt.nii.gz
> pymvpa2 atlaslabeler -f HarvardOxford-cort-prob-bold.nii.gz -A HarvardOxford-Cortical -s mask_vt.nii.gz

would provide summary over the MNI (or HarvardOxford-Cortical) atlas,
.nii.gz of which were previously flirted (or fnirted) into the space of
mask_vt.nii.gz and provided in '-f' argument.
"""
# magic line for manpage summary
# man: -*- % query stereotaxic atlases

__docformat__ = 'restructuredtext'

import re, sys, os
import argparse

import mvpa2
from mvpa2.base import verbose, warning, externals

if externals.exists('nibabel', raise_=True):
    import nibabel as nb

if __debug__:
    from mvpa2.base import debug

from mvpa2.atlases.transformation import *

if externals.exists('lxml', raise_=True):
    from mvpa2.atlases import Atlas, ReferencesAtlas, FSLProbabilisticAtlas, \
         KNOWN_ATLASES, KNOWN_ATLAS_FAMILIES, XMLAtlasException

import numpy as np
#import numpy.linalg as la
# to read in transformation matrix


def select_from_volume_iterator(volFileName, lt=None, ut=None):
    """
    Generator which returns value + coordinates with values of non-0 entries
    from the `volFileName`

    Returns
    -------
    tuple with 0th entry value, the others are voxel coordinates
    More effective than previous loopy iteration since uses numpy's where
    function, but for now is limited only to non-0 voxels selection
    """
    try:
        volFile = nb.load(volFileName)
    except:
        raise IOError("Cannot open image file %s" % volFileName)

    volData = volFile.get_data()
    voxdim = volFile.get_header().get_zooms()[:3]
    if lt is None and ut is None:
        mask = volData != 0.0
    elif lt is None and ut is not None:
        mask = volData <= ut
    elif lt is not None and ut is None:
        mask = volData >= lt
    else:
        mask = np.logical_and(volData >= lt, volData <= ut)

    matchingVoxels = np.where(mask)
    for e in zip(volData[matchingVoxels], *matchingVoxels):
        e_ = tuple(e)
        if len(e_) < 5:
            e_ = e_ + (0,) # add time=0
        yield  e_


def parsed_coordinates_iterator(
    parseString="^\s*(?P<x>\S+)[ \t,](?P<y>\S+)[ \t,](?P<z>\S+)\s*$",
    inputStream=sys.stdin,
    ctype=float,
    dtype=float):
    """Iterator to provide coordinates/values parsed from the string stream,
    most often from the stdin
    """
    parser = re.compile(parseString)
    for line in inputStream.readlines():
        line = line.strip()
        match = parser.match(line)
        if not match:
            if __debug__:
                debug('ATL', "Line '%s' did not match '%s'"
                      % (line, parseString))
        else:
            r = match.groupdict()
            if r.has_key('v'): v = dtype(r['v'])
            else:              v = 0.0
            if r.has_key('t'): t = dtype(r['t'])
            else:              t = 0.0
            yield (v, ctype(r['x']), ctype(r['y']), ctype(r['z']), t)


# XXX helper to process labels... move me
##REF: Name was automagically refactored
def present_labels(args, labels):
    if isinstance(labels, list):
        res = []
        for label in labels:
            # XXX warning -- some inconsistencies in atlas.py
            #     need refactoring
            s = label['label'] #.text
            if label.has_key('prob') and not args.createSummary:
                s += "(%d%%%%)" % label['prob']
            res += [s]
        if res == []:
            res = ['None']
        return '/'.join(res)
    else:
        if args.abbreviatedLabels:
            return labels['label'].abbr
        else:
            return labels['label'].text

def statistics(values):
    N_ = len(values)
    if N_==0:
        return 0, None, None, None, None, ""
    mean = np.mean(values)
    std = np.std(values)
    minv = np.min(values)
    maxv = np.max(values)
    ssummary = "[%3.2f : %3.2f] %3.2f+-%3.2f" % (minv, maxv, mean, std)
    return N_, mean, std, minv, maxv, ssummary


##REF: Name was automagically refactored
def get_summary(args, summary, output):
    """Output the summary
    """
    # Sort either by the name (then ascending) or by the number of
    # elements (then descending)
    sort_keys = [(k, len(v['values']), v['maxcoord'][1])
                 for k,v in summary.iteritems()]
    sort_index, sort_reverse = {
        'name' : (0, False),
        'count': (1, True),
        'a-p': (2, True)}[args.sortSummaryBy]
    sort_keys.sort(cmp=lambda x,y: cmp(x[sort_index], y[sort_index]),
                   reverse=sort_reverse)
    # and here are the keys
    keys = [x[0] for x in sort_keys]
    maxkeylength = max (map(len, keys))

    # may be I should have simply made a counter ;-)
    total = sum(map(lambda x:len(x['values']), summary.values()))
    count_reported = 0
    for index in keys:
        if index.rstrip(' /') == 'None' and args.suppressNone:
            continue
        summary_ = summary[index]
        values = summary_['values']
        N, mean, std, minv, maxv, ssummary = statistics(values)
        Npercent = 100.0*N/total
        if N < args.countThreshold \
               or Npercent < args.countPercentThreshold:
            continue
        count_reported += N
        msg = "%%%ds:" % maxkeylength
        output.write(msg % index)
        output.write("%4d/%4.1f%% items" \
                     % (N, Npercent))

        if args.createSummary>1:
            output.write(" %s" % ssummary)

        if args.createSummary>2:
            output.write(" max at %s" % summary_['maxcoord'])
            if args.showOriginalCoordinates and volQForm:
                #import pydb
                #pydb.debugger()
                #coord = np.dot(volQForm, summary_['maxcoord']+[1])[:3]
                coord = volQForm[summary_['maxcoord']]
                output.write(" %r" % (tuple(coord),))

        if args.createSummary>3 and summary_.has_key('distances'):
            # if we got statistics over referenced voxels
            Nr, mean, std, minv, maxv, ssummary = \
                statistics(summary_['distances'])
            Nr = len(summary_['distances'])
            # print "N=", N, " Nr=", Nr
            output.write(" Referenced: %d/%d%% Distances: %s" \
                         % (Nr, int(Nr*100.0 / N), \
                            ssummary))
        output.write("\n")
        # output might fail to flush, like in the case with broken pipe
        # -- imho that is not a big deal, ie not worth scaring the user
        try:
            output.flush()
        except IOError:
            pass
    output.write("-----\n")
    output.write("TOTAL: %d items" % count_reported)
    if total != count_reported:
        output.write(" (out of %i, %i were excluded)" % (total, total-count_reported))
    output.write("\n")


parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

def setup_parser(parser):
    parser.add_argument("-a", "--atlas-file",
                      action="store", dest="atlasFile",
                      default=None,
                      help="Atlas file to use. Overrides --atlas-path and --atlas")

    parser.add_argument("--atlas-path",
                      action="store", dest="atlasPath",
                      default=None,
                      help=r"Path to the atlas files. '%(name)s' will be replaced"
                           " with the atlas name. See -A. Defaults depend on the"
                           " atlas family.")

    parser.add_argument("-A", "--atlas",
                      action="store", dest="atlasName",
                      default="talairach", choices=KNOWN_ATLASES.keys(),
                      help="Atlas to use. Choices: %s"
                           % ', '.join(KNOWN_ATLASES.keys()))

    parser.add_argument("-f", "--atlas-image-file",
                      action="store", dest="atlasImageFile",
                      default=None,
                      help=r"Path to the data image for the corresponding atlas. "
                           " Can be used to override default image if it was "
                           " already resliced into a corresponding space (e.g."
                           " subject)")

    parser.add_argument("-i", "--input-coordinates-file",
                      action="store", dest="inputCoordFile",
                      default=None,
                      help="Fetch coordinates from ASCII file")

    parser.add_argument("-v", "--input-volume-file",
                      action="store", dest="inputVolFile",
                      default=None,
                      help="Fetch coordinates from volumetric file")

    parser.add_argument("-o", "--output-file",
                      action="store", dest="outputFile",
                      default=None,
                      help="Output file. Otherwise standard output")

    parser.add_argument("-d", "--max-distance",
                      action="store", type=float, dest="maxDistance",
                      default=0,
                      help="When working with reference/distance atlases, what"
                      " maximal distance to use to look for the voxel of interest")

    parser.add_argument("-T", "--transformation-file",
                      dest="transformationFile",
                      help="First transformation to apply to the data. Usually"+
                      " should be subject -> standard(MNI) transformation")

    parser.add_argument("-s", "--summary",
                      action="count", dest="createSummary", default=0,
                      help="Either to create a summary instead of dumping voxels."
                      " Use multiple -s for greater verbose summary")


    parser.add_argument("--ss", "--sort-summary-by",
                       dest="sortSummaryBy", default="name",
                      choices=['name', 'count', 'a-p'],
                      help="How to sort summary entries. "
                      " a-p sorts anterior-posterior order")

    parser.add_argument("--dumpmap-file",
                      action="store", dest="dumpmapFile", default=None,
                      help="If original data is given as image file, dump indexes"
                      " per each treholded voxels into provided here output file")

    parser.add_argument("-l", "--levels",
                      dest="levels", default=None,
                      help="Indexes of levels which to print, or based on which "
                      "to create a summary (for a summary levels=4 is default). "
                      "To get listing of known for the atlas levels, use '-l list'")

    parser.add_argument("--mni2tal",
                      choices=["matthewbrett", "lancaster07fsl",
                               "lancaster07pooled", "meyerlindenberg98"],
                      dest="MNI2TalTransformation", default="matthewbrett",
                      help="Choose between available transformations from mni "
                      "2 talairach space")

    parser.add_argument("--thr", "--lthr", "--lower-threshold",
                      action="store", type=float, dest="lowerThreshold",
                      default=None,
                      help="Lower threshold for voxels to output")

    parser.add_argument("--uthr", "--upper-threshold",
                      action="store", type=float, dest="upperThreshold",
                      default=None,
                      help="Upper threshold for voxels to output")

    parser.add_argument("--count-thr", "--cthr",
                      action="store", type=int, dest="countThreshold",
                      default=1,
                      help="Lowest number of voxels for area to be reported in summary")

    parser.add_argument("--count-pthr", "--pthr",
                      action="store", type=float, dest="countPercentThreshold",
                      default=0.00001,
                      help="Lowest percentage of voxels within an area to be reported in summary")

    parser.add_argument("--suppress-none", "--sn",
                      action="store_true", dest="suppressNone",
                      help="Suppress reporting of voxels which found no labels (reported as None)")

    parser.add_argument("--abbr", "--abbreviated-labels",
                      action="store_true", dest="abbreviatedLabels",
                      help="Manipulate with abbreviations for labels instead of"
                      " full names, if the atlas has such")

    # Parameters to be inline with older talairachlabel

    parser.add_argument("-c", "--tc", "--show-target-coord",
                      action="store_true", dest="showTargetCoordinates",
                      help="Show target coordinates")


    parser.add_argument("--tv", "--show-target-voxel",
                      action="store_true", dest="showTargetVoxel",
                      help="Show target coordinates")

    parser.add_argument("--rc", "--show-referenced-coord",
                      action="store_true", dest="showReferencedCoordinates",
                      help="Show referenced coordinates/distance in case if we are"
                      " working with reference atlas")

    parser.add_argument("-C", "--oc", "--show-orig-coord",
                      action="store_true", dest="showOriginalCoordinates",
                      help="Show original coordinates")

    parser.add_argument("-V", "--show-values",
                      action="store_true", dest="showValues",
                      help="Show values")


    parser.add_argument("-I", "--input-space",
                      action="store", dest="inputSpace",
                      default="MNI",
                      help="Space in which input volume/coordinates provided in. For instance Talairach/MNI")

    parser.add_argument("-F", "--forbid-direct-mapping",
                      action="store_true", dest="forbidDirectMapping",
                      default=False,
                      help="If volume is provided it first tries to do direct "
                      "mapping voxel-2-voxel if there is no transformation file "
                      "given. This option forbids such behavior and does "
                      "coordinates mapping anyway.")

    parser.add_argument("-t", "--talairach",
                      action="store_true", dest="coordInTalairachSpace",
                      default=False,
                      help="Coordinates are in talairach space (1x1x1mm)," +
                      " otherwise assumes in mni space (2x2x2mm)."
                      " Shortcut for '-I Talairach'")

    parser.add_argument("-H", "--half-voxel-correction",
                      action="store_true", dest="halfVoxelCorrection",
                      default=False,
                      help="Adjust coord by 0.5mm after transformation to "
                      "Tal space.")

    parser.add_argument("-r", "--relative-to-origin",
                      action="store_true", dest="coordRelativeToOrigin",
                      help="Coords are relative to the origin standard form" +
                      " ie in spatial units (mm), otherwise the default assumes" +
                      " raw voxel dimensions")

    parser.add_argument("--input-line-format",
                      action="store", dest="inputLineFormat",
                      default=r"^\s*(?P<x>\S+)[ \t,]+(?P<y>\S+)[ \t,]+(?P<z>\S+)\s*$",
                      help="Format of the input lines (if ASCII input is provided)")

    parser.add_argument("--iv", "--input-voxels",
                      action="store_true", dest="input_voxels",
                      default=False,
                      help="Input lines carry voxel indices (int), not coordinates.")

    # Specific atlas options
    # TODO : group into options groups

    # Reference atlas
    parser.add_argument("-R", "--reference",
                      action="store", dest="referenceLevel",
                      default="Closest Gray",
                      help="Which level to reference in the case of reference"
                      " atlas")

    # Probabilistic atlases
    parser.add_argument("--prob-thr",
                      action="store", type=float, dest="probThr",
                      default=25.0,
                      help="At what probability (in %) to threshold in "
                      "probabilistic atlases (e.g. FSL)")

    parser.add_argument("--prob-strategy",
                      action="store", dest="probStrategy",
                      choices=['all', 'max'], default='max',
                      help="What strategy to use for reporting. 'max' would report"
                      " single area (above threshold) with maximal probabilitity")


def run(args):
    #atlas.relativeToOrigin = args.coordRelativeToOrigin

    fileIn = None
    coordT = None
    niftiInput = None
    # define data type for coordinates
    if args.input_voxels:
        ctype = int
        query_voxel = True
    else:
        ctype = float
        query_voxel = False

    # Setup coordinates read-in
    volQForm = None

    #
    # compatibility with older talairachlabel
    if args.inputCoordFile:
        fileIn = open(args.inputCoordFile)
        coordsIterator = parsed_coordinates_iterator(
            args.inputLineFormat, fileIn, ctype=ctype)
    if args.inputVolFile:
        infile = args.inputVolFile
        # got a volume/file to process
        if __debug__:
            debug('ATL', "Testing if 0th element in the list a volume")
        niftiInput = None
        try:
            niftiInput = nb.load(infile)
            if __debug__:
                debug('ATL', "Yes it is")
        except Exception, e:
            if __debug__:
                debug('ATL', "No it is not due to %s. Trying to parse the file" % e)

        if niftiInput:
            # if we got here -- it is a proper volume
            # XXX ask Michael to remove nasty warning message
            coordsIterator = select_from_volume_iterator(
                infile, args.lowerThreshold, args.upperThreshold)
            assert(coordT is None)
            coordT = Linear(niftiInput.get_header().get_qform())
            # lets store volumeQForm for possible conversion of voxels into coordinates
            volQForm = coordT
            # previous iterator returns space coordinates
            args.coordRelativeToOrigin = True
        else:
            raise ValueError('could not open volumetric input file')
    # input is stdin
    else:
        coordsIterator = parsed_coordinates_iterator(
            args.inputLineFormat, ctype=ctype)

    # Open and initialize atlas lookup
    if args.atlasFile is None:
        if args.atlasPath is None:
            args.atlasPath = KNOWN_ATLASES[args.atlasName]
        args.atlasFile = args.atlasPath % ( {'name': args.atlasName} )

    akwargs_common = {}
    if args.atlasImageFile:
        akwargs_common['image_file'] = args.atlasImageFile

    if not args.forbidDirectMapping \
           and niftiInput is not None and not args.transformationFile:
        akwargs = {'resolution': niftiInput.get_header().get_zooms()[0]}
        query_voxel = True   # if we can query directly by voxel, do so

        akwargs.update(akwargs_common)
        verbose(1, "Will attempt direct mapping from input voxels into atlas "
                   "voxels at resolution %.2f" % akwargs['resolution'])

        atlas = Atlas(args.atlasFile, **akwargs)

        # verify that we got the same qforms in atlas and in the data file
        if atlas.space != args.inputSpace:
            verbose(0,
                "Cannot do direct mapping between input image in %s space and"
                " atlas in %s space. Use -I switch to override input space if"
                " it misspecified, or use -T to provide transformation. Trying"
                " to proceed" %(args.inputSpace, atlas.space))
            query_voxel = False
        elif not (niftiInput.get_header().get_qform() == atlas._image.get_header().get_qform()).all():
            if args.atlasImageFile is None:
                warning(
                    "Cannot do direct mapping between files with different qforms."
                    " Please provide original transformation (-T)."
                    "\n Input qform:\n%s\n Atlas qform: \n%s"
                    %(niftiInput.get_header().get_qform(), atlas._image.get_header().get_qform), 1)
                # reset ability to query by voxels
                query_voxel = False
            else:
                warning(
                    "QForms are different between input image and "
                    "provided atlas image."
                    "\n Input qform of %s:\n%s\n Atlas qform of %s:\n%s"
                    %(infile, niftiInput.get_header().get_qform(),
                      args.atlasImageFile, atlas._image.get_header().get_qform()), 1)
        else:
            coordT = None
    else:
        atlas = Atlas(args.atlasFile, **akwargs_common)


    if isinstance(atlas, ReferencesAtlas):
        args.referenceLevel = args.referenceLevel.replace('/', ' ')
        atlas.set_reference_level(args.referenceLevel)
        atlas.distance = args.maxDistance
    else:
        args.showReferencedCoordinates = False

    if isinstance(atlas, FSLProbabilisticAtlas):
        atlas.strategy = args.probStrategy
        atlas.thr = args.probThr

    ## If not in Talairach -- in MNI with voxel size 2x2x2
    # Original talairachlabel assumed that if respective to origin -- voxels were
    # scaled already.
    #if args.coordInTalairachSpace:
    #   voxelSizeOriginal = np.array([1, 1, 1])
    #else:
    #   voxelSizeOriginal = np.array([2, 2, 2])

    if args.coordInTalairachSpace:
            args.inputSpace = "Talairach"

    if not (args.inputSpace == atlas.space or
            (args.inputSpace in ["MNI", "Talairach"] and
             atlas.space == "Talairach")):
        raise XMLAtlasException("Unknown space '%s' which is not the same as atlas "
                                "space '%s' either" % ( args.inputSpace, atlas.space ))

    if query_voxel:
        # we do direct mapping
        coordT = None
    else:
        verbose(2, "Chaining needed transformations")
        # by default -- no transformation
        if args.transformationFile:
            #externals.exists('scipy', raise_=True)
            # scipy.io.read_array was deprecated a while back (around 0.8.0)
            from numpy import loadtxt

            transfMatrix = loadtxt(args.transformationFile)
            coordT = Linear(transfMatrix, previous=coordT)
            verbose(2, "coordT got linear transformation from file %s" %
                       args.transformationFile)

        voxelOriginOriginal = None
        voxelSizeOriginal = None

        if not args.coordRelativeToOrigin:
            if args.inputSpace == "Talairach":
                # assume that atlas is in Talairach space already
                voxelOriginOriginal = atlas.origin
                voxelSizeOriginal = np.array([1, 1, 1])
            elif args.inputSpace == "MNI":
                # need to adjust for MNI origin as it was thought to be at
                # in terms of voxels
                #voxelOriginOriginal = np.array([46, 64, 37])
                voxelOriginOriginal = np.array([45, 63, 36])
                voxelSizeOriginal = np.array([2.0, 2.0, 2.0])
                warning("Assuming elderly sizes for MNI volumes with"
                           " origin %s and sizes %s" %\
                           ( `voxelOriginOriginal`, `voxelSizeOriginal`))


        if not (voxelOriginOriginal is None and voxelSizeOriginal is None):
            verbose(2, "Assigning origin adjusting transformation with"+\
                    " origin=%s and voxelSize=%s" %\
                    ( `voxelOriginOriginal`, `voxelSizeOriginal`))

            coordT = SpaceTransformation(origin=voxelOriginOriginal,
                                         voxelSize=voxelSizeOriginal,
                                         to_real_space=True, previous=coordT)

        # besides adjusting for different origin we need to transform into
        # Talairach space
        if args.inputSpace == "MNI" and atlas.space == "Talairach":
            verbose(2, "Assigning transformation %s" %
                       args.MNI2TalTransformation)
            # What transformation to use
            coordT = {"matthewbrett": MNI2Tal_MatthewBrett,
                      "lancaster07fsl":  mni_to_tal_lancaster07_fsl,
                      "lancaster07pooled":  mni_to_tal_lancaster07pooled,
                      "meyerlindenberg98":  mni_to_tal_meyer_lindenberg98,
                      "yohflirt": mni_to_tal_yohflirt
                      }\
                      [args.MNI2TalTransformation](previous=coordT)

        if args.inputSpace == "MNI" and args.halfVoxelCorrection:
            originCorrection = np.array([0.5, 0.5, 0.5])
        else:
            # perform transformation any way to convert to voxel space (integers)
            originCorrection = None

        # To be closer to what original talairachlabel did -- add 0.5 to each coord
        coordT = SpaceTransformation(origin=originCorrection, voxelSize=None,
                                         to_real_space=False, previous = coordT)

    if args.createSummary:
        summary = {}
        if args.levels is None:
            args.levels = str(min(4, atlas.nlevels-1))
    if args.levels is None:
        args.levels = range(atlas.nlevels)
    elif isinstance(args.levels, basestring):
        if args.levels == 'list':
            print "Known levels and their indicies:\n" + atlas.levels_listing()
            sys.exit(0)
        slevels = args.levels.split(',')
        args.levels = []
        for level in slevels:
            try:
                int_level = int(level)
            except:
                if atlas.levels.has_key(level):
                    int_level = atlas.levels[level].index
                else:
                    raise RuntimeError(
                        "Unknown level '%s'. " % level +
                        "Known levels and their indicies:\n"
                        + atlas.levels_listing())
            args.levels += [int_level]
    else:
        raise ValueError("Don't know how to handle list of levels %s."
                         "Example is '1,2,3'" % (args.levels,))

    verbose(3, "Operating on following levels: %s" % args.levels)
    # assign levels to the atlas
    atlas.default_levels = args.levels

    if args.outputFile:
        output = open(args.outputFile, 'w')
    else:
        output = sys.stdout

    # validity check
    if args.dumpmapFile:
        if niftiInput is None:
            raise RuntimeError, "You asked to dump indexes into the volume, " \
                  "but input wasn't a volume"
            sys.exit(1)
        ni_dump = nb.load(infile)
        ni_dump_data = np.zeros(ni_dump.get_header().get_data_shape()[:3] + (len(args.levels),))

    # Also check if we have provided voxels but not querying by voxels
    if args.input_voxels:
        if coordT is not None:
            raise NotImplementedError, \
                  "Cannot perform voxels querying having coordT defined"
        if not query_voxel:
            raise NotImplementedError, \
                  "query_voxel was reset to False, can't do queries by voxel"

    # Read coordinates
    numVoxels = 0
    for c in coordsIterator:

        value, coord_orig, t = c[0], c[1:4], c[4]
        if __debug__:
            debug('ATL', "Obtained coord_orig=%s with value %s"
                  % (repr(coord_orig), value))

        lt, ut = args.lowerThreshold, args.upperThreshold
        if lt is not None and value < lt:
            verbose(5, "Value %s is less than lower threshold %s, thus voxel "
                    "is skipped" % (value, args.lowerThreshold))
            continue
        if ut is not None and value > ut:
            verbose(5, "Value %s is greater than upper threshold %s, thus voxel "
                    "is skipped" % (value, args.upperThreshold))
            continue

        numVoxels += 1

        # Apply necessary transformations
        coord = coord_orig = np.array(coord_orig)

        if coordT:
            coord = coordT[ coord_orig ]

        # Query label
        if query_voxel:
            voxel = atlas[coord]
        else:
            voxel = atlas(coord)
        voxel['coord_orig'] = coord_orig
        voxel['value'] = value
        voxel['t'] = t
        if args.createSummary:
            summaryIndex = ""
            voxel_labels = voxel["labels"]
            for i,ind in enumerate(args.levels):
                voxel_label = voxel_labels[i]
                text = present_labels(args, voxel_label)
                #if len(voxel_label):
                #   assert(voxel_label['index'] == ind)
                summaryIndex += text + " / "
            if not summary.has_key(summaryIndex):
                summary[summaryIndex] = {'values':[], 'max':value,
                                         'maxcoord':coord_orig}
                if voxel.has_key('voxel_referenced'):
                    summary[summaryIndex]['distances'] = []
            summary_ = summary[summaryIndex]
            summary_['values'].append(value)
            if summary_['max'] < value:
                summary_['max'] = value
                summary_['maxcoord'] = coord_orig
            if voxel.has_key('voxel_referenced'):
                if voxel['voxel_referenced'] and voxel['distance']>=1e-3:
                    verbose(5, 'Appending distance %e for voxel at %s'
                            % (voxel['distance'], voxel['coord_orig']))
                    summary_['distances'].append(voxel['distance'])
        else:
            # Display while reading/processing
            first, out = True, ""

            if args.showValues:
                out += "%(value)5.2f "
            if args.showOriginalCoordinates:
                out += "%(coord_orig)s ->"
            if args.showReferencedCoordinates:
                out += " %(voxel_referenced)s=>%(distance).2f=>%(voxel_queried)s ->"
            if args.showTargetCoordinates:
                out += " %(coord_queried)s: "
                #out += "(%d,%d,%d): " % tuple(map(lambda x:int(round(x)),coord))
            if args.showTargetVoxel:
                out += " %(voxel_queried)s ->"

            if args.levels is None:
                args.levels = range(len(voxel['labels']))

            labels = [present_labels(args, voxel['labels'][i]) for i in args.levels]
            out += ','.join(labels)
            #if args.abbreviatedLabels:
            #   out += ','.join([l.abbr for l in labels])
            #else:
            #   out += ','.join([l.text for l in labels])
            #try:
            output.write(out % voxel + "\n")
            #except:
            #    import pydb
            #    pydb.debugger()

        if args.dumpmapFile:
            try:
                ni_dump_data[coord_orig[0], coord_orig[1], coord_orig[2]] = \
                  [voxel['labels'][i]['label'].index
                   for i,ind in enumerate(args.levels)]
            except Exception, e:
                import pydb
                pydb.debugger()

    # if we opened any file -- close it
    if fileIn:
        fileIn.close()

    if args.dumpmapFile:
        ni_dump = nb.Nifti1Image(ni_dump_data, None, ni_dump.get_header())
        ni_dump.to_filename(args.dumpmapFile)

    if args.createSummary:
        if numVoxels == 0:
            verbose(1, "No matching voxels were found.")
        else:
            get_summary(args, summary, output)

    if args.outputFile:
        output.close()

########NEW FILE########
__FILENAME__ = cmd_crossval
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Cross-validation of a learner's performance

A learner is repeatedly trained and tested on partitions of an input
dataset that are generated by a configurable partitioning scheme.
Partition usually constitute training and testing portions.  The learner
is trained on training portion of the dataset and then learner's
generalization is tested by comparing its predictions on the testing portion.

A summary of a learner performance is written to STDOUT. Depending on the
particular setup of the cross-validation analysis, either the learner's raw
predictions or summary statistics are returned in an output dataset.

If Monte-Carlo permutation testing is enabled (see --permutations) a second
output dataset with the corresponding p-values is stored as well (filename
suffix '_nullprob').

"""

# magic line for manpage summary
# man: -*- % cross-validation of a learner's performance

__docformat__ = 'restructuredtext'

import numpy as np
import copy
import sys
import argparse
from mvpa2.base import verbose, warning, error
from mvpa2.datasets import vstack
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
    import parser_add_common_opt, \
           ds2hdf5, hdf2ds, learner_opt, partitioner_opt, \
           learner_space_opt, arg2errorfx, get_crossvalidation_instance, \
           crossvalidation_opts_grp


parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

def setup_parser(parser):
    from .helpers import parser_add_optgroup_from_def, \
        parser_add_common_attr_opts, single_required_hdf5output
    parser_add_common_opt(parser, 'multidata', required=True)
    # make learner and partitioner options required
    cv_opts_grp = copy.deepcopy(crossvalidation_opts_grp)
    for i in (0,2):
        cv_opts_grp[1][i][1]['required'] = True
    parser_add_optgroup_from_def(parser, cv_opts_grp)
    parser_add_optgroup_from_def(parser, single_required_hdf5output)

def run(args):
    dss = hdf2ds(args.data)
    verbose(3, 'Loaded %i dataset(s)' % len(dss))
    ds = vstack(dss)
    verbose(3, 'Concatenation yielded %i samples with %i features' % ds.shape)
    # get CV instance
    cv = get_crossvalidation_instance(
            args.learner, args.partitioner, args.errorfx, args.sampling_repetitions,
            args.learner_space, args.balance_training, args.permutations,
            args.avg_datafold_results, args.prob_tail)
    res = cv(ds)
    # some meaningful output
    # XXX make condition on classification analysis only?
    print cv.ca.stats
    print 'Results\n-------'
    if args.permutations > 0:
        nprob =  cv.ca.null_prob.samples
    if res.shape[1] == 1:
        # simple result structure
        if args.permutations > 0:
            p=', p-value (%s tail)' % args.prob_tail
        else:
            p=''
        print 'Fold, Result%s' % p
        for i in xrange(len(res)):
            if args.permutations > 0:
                p = ', %f' % nprob[i, 0]
            else:
                p = ''
            print '%s, %f%s' % (res.sa.cvfolds[i], res.samples[i, 0], p)
    # and store
    ds2hdf5(res, args.output, compression=args.hdf5_compression)
    if args.permutations > 0:
        if args.output.endswith('.hdf5'):
            args.output = args.output[:-5]
        ds2hdf5(cv.ca.null_prob, '%s_nullprob' % args.output,
                compression=args.hdf5_compression)
    return res

########NEW FILE########
__FILENAME__ = cmd_describe
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Describe a dataset's content

This command can produce a number of reports for datasets. Currently
supported are summary statistics in text format, as well as basic plots.
See the --report option for more information.

"""

# magic line for manpage summary
# man: -*- % describe a dataset's content

__docformat__ = 'restructuredtext'

import numpy as np
import sys
import argparse
from mvpa2.base import verbose, warning, error
from mvpa2.datasets import vstack
from mvpa2.base.dochelpers import _indent
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
    import arg2ds, parser_add_optgroup_from_def, parser_add_common_opt

parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

def arg2transform(args):
    args = args.split(':')
    if not len(args) == 2:
        raise ValueError("--numpy-xfm needs exactly two arguments")
    if not args[0] in ('samples', 'features'):
        raise ValueError("transformation axis must be 'samples' or 'features' (was: %s)"
                         % args[0])
    axis = args[0]
    if not hasattr(np, args[1]):
        raise ValueError("the NumPy package does not have a '%s' function" % args[1])
    fx = getattr(np, args[1])
    return fx, axis

def _limit_lines(string, maxlines):
    lines = string.split('\n')
    truncated = '\n'.join(lines[:maxlines])
    if len(lines) > maxlines:
        truncated += ' ...'
    return truncated

def _describe_samples(samp, style):
    if style == 'terse':
        return "%s@%s\n" % (samp.shape, samp.dtype)
    else:
        return 'IMPLEMENT ME\n'

def _describe_array_attr(attr, style):
    if len(attr.value.shape) == 1:
        shape = attr.value.shape[0]
    else:
        shape = attr.value.shape
    if style == 'terse':
        return '%s %s@%s' % (attr.name, shape, attr.value.dtype)
    else:
        return 'IMPLEMENT ME\n'

def _describe_attr(attr, style):
    if style == 'terse':
        return '%s %s' % (attr.name,
                          _limit_lines(str(attr.value), 1))
    else:
        return 'IMPLEMENT ME\n'

def txt_content_summary_terse(ds, args):
    print ds.summary(targets_attr=args.target_attr)
    info = '\n\nDetails on dataset attributes:\n'
    for cdesc, col, describer in \
            (('sample', ds.sa, _describe_array_attr),
             ('feature', ds.fa, _describe_array_attr),
             ('dataset', ds.a, _describe_attr)):
        info += ' %s attributes:\n' % cdesc
        for attr in sorted(col.values(),
                           cmp=lambda x, y: cmp(x.name, y.name)):
            info += '  %s\n' % describer(attr, 'terse')
    print info

def sample_histogram(ds, args):
    import pylab as pl
    pl.figure()
    pl.hist(np.ravel(ds.samples), bins=args.histogram_bins)
    if not args.xlim is None:
        pl.xlim(*args.xlim)
    if not args.ylim is None:
        pl.ylim(*args.ylim)
    for opt, fx in ((args.x_marker, pl.axvline),
                    (args.y_marker, pl.axhline)):
        if not opt is None:
            for val in opt:
                fx(val, linestyle='--')
    if not args.figure_title is None:
        pl.title(args.figure_title)
    pl.show()

info_fx = {
        'txtsummary' : txt_content_summary_terse,
        'sample_histogram' : sample_histogram,
}

xfm_grp = ('options for transforming dataset content before plotting', [
    (('--numpy-xfm',), dict(type=arg2transform, metavar='SPEC',
        help="""apply a Numpy function along a given axis of the samples before
        generating the dataset info summary. For example, 'samples:std' will
        apply the 'std' function along the samples axis, i.e. compute a vector
        of standard deviations for all features in a dataset""")),
])

output_grp = ('options for plot formatting', [
    (('--figure-title',), dict(type=str,
        help="""title for a plot""")),
    (('--histogram-bins',), dict(type=int, default=20,
        metavar='VALUE',
        help="""number of bin for histograms""")),
    (('--xlim',), dict(type=float, nargs=2,
        help="""minimum and maximum value of the x-axis extent in a figure""")),
    (('--ylim',), dict(type=float, nargs=2,
        help="""minimum and maximum value of the y-axis extent in a figure""")),
    (('--x-marker',), dict(type=float, nargs='+',
        help="""list of x-value to draw markers on in a figure""")),
    (('--y-marker',), dict(type=float, nargs='+',
        help="""list of y-value to draw markers on in a figure""")),
])


ds_descr_grp = ('options for dataset description', [
    (('--target-attr',), dict(default='targets', metavar='NAME',
        help="""name of a samples attributes defining 'target'. This
        information is used to define groups of samples when
        generating information on the within and between category
        data structure in a dataset.""")),
])

def setup_parser(parser):
    parser_add_common_opt(parser, 'multidata', required=True)
    parser.add_argument('-r', '--report',
            **dict(type=str, choices=info_fx.keys(),
                 default='txtsummary',
                 help="""choose a type of report. Default: terse summary in
                 text format."""))
    parser_add_optgroup_from_def(parser, xfm_grp)
    parser_add_optgroup_from_def(parser, output_grp)
    parser_add_optgroup_from_def(parser, ds_descr_grp)


def run(args):
    ds = arg2ds(args.data)
    verbose(3, 'Concatenation yielded %i samples with %i features' % ds.shape)
    if not args.numpy_xfm is None:
        from mvpa2.mappers.fx import FxMapper
        fx, axis = args.numpy_xfm
        mapper = FxMapper(axis, fx)
        ds = ds.get_mapped(mapper)
    info_fx[args.report](ds, args)

########NEW FILE########
__FILENAME__ = cmd_dump
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Dump dataset components in various formats

A single arbitrary dataset component (sample data, sample attribute, feature
attribute, dataset attribute) can be selected and exported into another
format. A list of supported formats and their respective capabilities is below.

PLAIN TEXT OUTPUT

1D and 2D numerical data can be export as plain text. In addition lists of
strings are supported as well. Typically data is exported with one element per
line is, except for 2D numerical matrices, where an entire row is written on
a single line (space-separated).

For all unsupported data a warning is issued and a truncated textual description
of the dataset component is given.

HDF5 STORAGE

Arbitrary data (types) can be stored in HDF5 containers. For simple data that is
natively supported by HDF5 a top-level HDF5 dataset is created and contains all
data. Complex, not natively supported data types are serialized before stored
in HDF5.

NUMPY'S NPY BINARY FILES

This data format is for storing numerical data (with arbitrary number of
dimensions in binary format).

NIFTI FILES

This data format is for (multi-dimensional) spatial images. Input datasets
should have a mapper that can reverse-map the corresponding dataset component
back into the image space.

Examples:

Print a sample attribute

  $ pymvpa2 dump -i mydata.hdf5 --sa subj

Export the sample data array into NumPy's .npy format

  $ pymvpa2 dump -i mydata.hdf5 -s -f npy -d mysamples.npy

"""

# magic line for manpage summary
# man: -*- % export dataset components into other (file) formats

__docformat__ = 'restructuredtext'

import numpy as np
import sys
import argparse
from mvpa2.base import verbose, warning, error

if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
    import arg2ds, parser_add_common_opt, hdf5compression

def _check_output(args):
    if args.output is None:
        raise ValueError("no output filename given (missing --output)")

def to_nifti(dumpy, ds, args):
    from mvpa2.datasets.mri import map2nifti
    # TODO allow overriding the nifti header
    nimg = map2nifti(ds, dumpy)
    nimg.to_filename(args.output)

parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

component_grp = ('options for selecting dataset components', [
    (('-s', '--samples'), dict(action='store_true',
        help="""dump the dataset samples.""")),
    (('--sa',), dict(
        help="""name of the sample attribute to be dumped.""")),
    (('--fa',), dict(
        help="""name of the feature attribute to be dumped.""")),
    (('--da',), dict(
        help="""name of the dataset attribute to be dumped.""")),
])

hdf5_grp =('option for HDF5 output', [
    hdf5compression[1:]
])

def setup_parser(parser):
    from .helpers import parser_add_optgroup_from_def, \
        parser_add_common_attr_opts
    parser_add_common_opt(parser, 'multidata', required=True)
    parser_add_optgroup_from_def(parser, component_grp, exclusive=True)
    parser.add_argument('-o', '--output',
                        help="""output filename. If no output file name
                        is given output will be directed to stdout, if permitted by
                        the data format""")
    parser.add_argument('-f', '--format', default='txt',
                        choices=('hdf5', 'nifti', 'npy', 'txt'),
                        help="""output format""")
    parser_add_common_opt(
            parser, 'multidata', names=('--mapper-dataset',), dest='mapperds',
            help="""path to a PyMVPA dataset whose mapper should be used for
            reverse mapping features into volumetric space for NIfTI export.
            By default the mapper in the input dataset is used.""")
    parser_add_optgroup_from_def(parser, hdf5_grp)


def run(args):
    ds = arg2ds(args.data)
    # What?
    if args.samples:
        dumpy = ds.samples
    elif not ((args.sa is None) and (args.fa is None) and (args.da is None)):
        for attr, col in ((args.sa, ds.sa), (args.fa, ds.fa), (args.da, ds.a)):
            if attr is None:
                continue
            try:
                dumpy = col[attr].value
            except KeyError:
                raise ValueError("unknown attribute '%s', known are %s)"
                                 % (attr, col.keys()))
    else:
        raise ValueError('no dataset component chosen')
    # How?
    if args.format == 'txt':
        if args.output:
            out = open(args.output, 'w')
        else:
            out = sys.stdout
        try:
            # trying to write numerical data
            fmt=None
            if np.issubdtype(dumpy.dtype, int):
                fmt='%i'
            elif np.issubdtype(dumpy.dtype, float):
                fmt='%G'
            if fmt is None:
                np.savetxt(out, dumpy)
            else:
                np.savetxt(out, dumpy, fmt=fmt)
        except:
            # it could be something 1d that we can try to print
            if hasattr(dumpy, 'shape') and len(dumpy.shape) == 1:
                for v in dumpy:
                    print v
            else:
                warning("conversion to plain text is not supported for "
                        "this data type")
                # who knows what it is
                out.write(repr(dumpy))
        if not out is sys.stdout:
            out.close()
    elif args.format == 'hdf5':
        from mvpa2.base.hdf5 import h5save
        _check_output(args)
        if not args.output.endswith('.hdf5'):
            args.output += '.hdf5'
        h5save(args.output, dumpy)
    elif args.format == 'npy':
        _check_output(args)
        np.save(args.output, dumpy)
    elif args.format == 'nifti':
        _check_output(args)
        if args.fa:
            # need to wrap into a length-1 sequence to survive rev-mapping
            # properly
            # TODO maybe we should allow more complex transformations, e.g.
            # 2d features may just need a transpose() to fit into a NIfTI
            dumpy = dumpy[None]
        if args.mapperds is None:
            mapperds = ds
        else:
            mapperds = arg2ds(args.mapperds)
        to_nifti(dumpy, mapperds, args)
    return ds

########NEW FILE########
__FILENAME__ = cmd_exec
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Execute arbitrary Python expressions (on datasets)

This command can be used to execute arbitrary Python scripts while avoiding
unnecessary boilerplate code to load datasets and store results. This command
is also useful for testing functionality and results via the commandline
interface and for asserting arbitrary conditions in scripts.

First, optional dataset(s) are loaded from one or more sources. Afterwards any
number of given expressions (see --exec) are executed. An expression can be
given as an argument on the command line, read from a file, or from STDIN. The
return value of any given expression is ignored (not evaluated anyhow), only
exceptions are treated as errors and cause the command to exit with a non-zero
return value.  To implement tests and assertions it is best to utilize a Python
unittest framework such as 'nose'.

In the namespace in which all expressions are evaluated the NumPy module is
available via the alias 'np', and the nose.tools under the alias 'nt' (if
installed). Any loaded datasets are available as a list named ``dss``. The
first dataset in that list (if any) is available under the name ``ds``.

Examples:

Assert some condition

  $ pymvpa2 exec -e 'assert(4==4)'

Check for the presence of a particular sample attribute in a dataset

  $ pymvpa2 exec -e 'dss[0].sa.subj3' -i mydata.hdf5

Extract and store results

  $ pymvpa2 exec -e 'a=5' -e 'print a' --store a -o mylittlea.hdf5
"""

# magic line for manpage summary
# man: -*- % evaluate arbitrary Python expressions for tests and assertions

__docformat__ = 'restructuredtext'

import os
import sys
import numpy as np
import argparse
from mvpa2.base import verbose, warning, error
from mvpa2.datasets import vstack
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers import arg2ds, ds2hdf5, parser_add_common_opt, \
        hdf5compression, parser_add_optgroup_from_def

hdf5output = ('output options', [
    (('-s', '--store'), dict(type=str, nargs='+', metavar='NAME', help="""\
        One or more names of variables or objects to extract from the local
        name space after all expressions have been executed. They will be
        stored in a dictionary in HDF5 format (requires --output).""")),
    (('-o', '--output'), dict(type=str,
         help="""output filename ('.hdf5' extension is added automatically if
         necessary). NOTE: The output format is suitable for data exchange between
         PyMVPA commands, but is not recommended for long-term storage or exchange
         as its specific content may vary depending on the actual software
         environment. For long-term storage consider conversion into other data
         formats (see 'dump' command).""")),
    hdf5compression[1:],
])



parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

def setup_parser(parser):
    parser_add_common_opt(parser, 'multidata', nargs='*', action='append')
    parser.add_argument('-e', '--exec', type=str, required=True,
            metavar='EXPR', action='append', dest='eval',
            help="""Python expression, or filename of a Python script,
            or '-' to read expressions from STDIN.""")
    parser_add_optgroup_from_def(parser, hdf5output)

def run(args):
    if not args.store is None and args.output is None:
        raise ValueError("--output is require for result storage")
    if not args.data is None:
        dss = [arg2ds(d) for d in args.data]
        if len(dss):
            # convenience short-cut
            ds = dss[0]
    try:
        import nose.tools as nt
    except ImportError:
        pass
    for expr in args.eval:
        if expr == '-':
            exec sys.stdin
        elif os.path.isfile(expr):
            execfile(expr, globals(), locals())
        else:
            exec expr
    if not args.store is None:
        out = {}
        for var in args.store:
            try:
                out[var] = locals()[var]
            except KeyError:
                warning("'%s' not found in local name space -- skipped." % var)
        if len(out):
            ds2hdf5(out, args.output, compression=args.hdf5_compression)



########NEW FILE########
__FILENAME__ = cmd_hyperalign
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
""""""

# magic line for manpage summary
# man: -*- % align the features across multiple datasets into a common space

__docformat__ = 'restructuredtext'

import numpy as np
import argparse
from mvpa2.base.hdf5 import h5save
from mvpa2.base import verbose
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
        import strip_from_docstring, parser_add_common_opt, \
               param2arg, ca2arg, arg2ds

from mvpa2.algorithms.hyperalignment import Hyperalignment
from mvpa2.mappers.procrustean import ProcrusteanMapper

parser_args = {
    'description': strip_from_docstring(Hyperalignment.__doc__,
                                        paragraphs=(4,),
                                        sections=(('Examples', 'Notes'))),
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

_supported_cas = {
    'residual_errors': {
        'output_suffix': '_resid_errors.txt',
        },
    'training_residual_errors': {
        'output_suffix': '_resid_errors_train.txt',
        },
}

_output_specs = {
    'commonspace': {
        'output_suffix': '_commonspace',
        'help': 'Store the final common space dataset after completion of level two.'
        },
    'store-transformation': {
        'help': 'Store common space transformation mappers for each training dataset.',
        },
}

_supported_parameters = (
    'alpha', 'level2_niter', 'ref_ds', 'zscore_all', 'zscore_common',
)

def _transform_dss(srcs, mappers, args):
    if __debug__:
        debug('CMDLINE', "loading to-be-transformed data from %s" % srcs)
    dss = [arg2ds(d) for d in srcs]
    verbose(1, "Loaded %i to-be-transformed datasets" % len(dss))
    if __debug__:
        debug('CMDLINE', "transform datasets")
    tdss = [ mappers[i].forward(td) for i, td in enumerate(dss)]
    return tdss, dss


def setup_parser(parser):
    # order of calls is relevant!
    inputargs = parser.add_argument_group('input data arguments')
    parser_add_common_opt(inputargs, 'multidata', action='append',
            required=True)
    parser_add_common_opt(
            inputargs, 'multidata',
            names=('-t', '--transform'), dest='transform',
             help="""\
Additional datasets for transformation into the common space. The number and
order of these datasets have to match those of the training dataset arguments
as the correspond mapper will be used to transform each individual dataset.""")
    algoparms = parser.add_argument_group('algorithm parameters')
    for param in _supported_parameters:
        param2arg(algoparms, Hyperalignment, param)
    outopts = parser.add_argument_group('output options')
    parser_add_common_opt(outopts, 'output_prefix', required=True)
    parser_add_common_opt(outopts, 'hdf5compression')
    for oopt in sorted(_output_specs):
        outopts.add_argument('--%s' % oopt, action='store_true',
            help=_output_specs[oopt]['help'])
    for ca in sorted(_supported_cas):
        ca2arg(outopts, Hyperalignment, ca,
               help="\nOutput will be stored into '<PREFIX>%s'"
                    % _supported_cas[ca]['output_suffix'])

def run(args):
    print args.data
    dss = [arg2ds(d)[:,:100] for d in args.data]
    verbose(1, "Loaded %i input datasets" % len(dss))
    if __debug__:
        for i, ds in enumerate(dss):
            debug('CMDLINE', "dataset %i: %s" % (i, str(ds)))
    # TODO at this point more check could be done, e.g. ref_ds > len(dss)
    # assemble parameters
    params = dict([(param, getattr(args, param)) for param in _supported_parameters])
    if __debug__:
        debug('CMDLINE', "configured parameters: '%s'" % params)
    # assemble CAs
    enabled_ca = [ca for ca in _supported_cas if getattr(args, ca)]
    if __debug__:
        debug('CMDLINE', "enabled conditional attributes: '%s'" % enabled_ca)
    hyper = Hyperalignment(enable_ca=enabled_ca,
                           alignment=ProcrusteanMapper(svd='dgesvd',
                                                       space='commonspace'),
                           **params)
    verbose(1, "Running hyperalignment")
    promappers = hyper(dss)
    verbose(2, "Alignment reference is dataset %i" % hyper.ca.chosen_ref_ds)
    verbose(1, "Writing output")
    # save on memory and remove the training data
    del dss
    if args.commonspace:
        if __debug__:
            debug('CMDLINE', "write commonspace as hdf5")
        h5save('%s%s.hdf5' % (args.output_prefix,
                              _output_specs['commonspace']['output_suffix']),
               hyper.commonspace,
               compression=args.hdf5_compression)
    for ca in _supported_cas:
        if __debug__:
            debug('CMDLINE', "check conditional attribute: '%s'" % ca)
        if getattr(args, ca):
            if __debug__:
                debug('CMDLINE', "store conditional attribute: '%s'" % ca)
            np.savetxt('%s%s' % (args.output_prefix,
                                 _supported_cas[ca]['output_suffix']),
                       hyper.ca[ca].value.samples)
    if args.store_transformation:
        for i, pm in enumerate(promappers):
            if __debug__:
                debug('CMDLINE', "store mapper %i: %s" % (i, str(pm)))
            h5save('%s%s.hdf5' % (args.output_prefix, '_map%.3i' % i),
                   pm, compression=args.hdf5_compression)
    if args.transform:
        tdss, dss = _transform_dss(args.transform, promappers, args)
        del dss
        verbose(1, "Store transformed datasets")
        for i, td in enumerate(tdss):
            if __debug__:
                debug('CMDLINE', "store transformed data %i: %s" % (i, str(td)))
            h5save('%s%s.hdf5' % (args.output_prefix, '_transformed%.3i' % i),
                   td, compression=args.hdf5_compression)

########NEW FILE########
__FILENAME__ = cmd_info
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Query various information about a PyMVPA installation.

If no option is given, a  useful subset of the available information is printed.
"""

# magic line for manpage summary
# man: -*- % query various information about a PyMVPA installation

import mvpa2

__docformat__ = 'restructuredtext'

def setup_parser(parser):
    excl = parser.add_mutually_exclusive_group()
    excl.add_argument('--externals', action='store_true',
                        help='list status of external dependencies')
    if __debug__:
        excl.add_argument('--debug', action='store_true',
                          help='list available debug channels')
    excl.add_argument(
            '--learner-warehouse', nargs='*', default=False, metavar='TAG',
            help="""list available algorithms in the learner warehouse.
            Optionally, an arbitrary number of tags can be specified to
            constrain the listing to learners with matching tags.""")
    return parser

def run(args):
    if args.externals:
        print mvpa2.wtf(include=['externals'])
    elif args.debug:
        mvpa2.debug.print_registered()
    elif not args.learner_warehouse is False:
        from mvpa2.clfs.warehouse import clfswh
        clfswh.print_registered(*args.learner_warehouse)
    else:
        print mvpa2.wtf()

########NEW FILE########
__FILENAME__ = cmd_mkds
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Create a PyMVPA dataset from various sources.

This command converts data from various sources, such as text files, NumPy's
NPY files, and MR (magnetic resonance) images into a PyMVPA dataset that gets
stored in HDF5 format. An arbitrary number of sample and feature attributes can
be added to a dataset, and individual attributes can be read from
heterogeneous sources (e.g. they do not have to be all from text files).

For datasets from MR images this command also supports automatic conversion
of additional images into (volumetric) feature attributes. This can be useful
for describing features with, for example, atlas labels.

COMPOSE ATTRIBUTES ON THE COMMAND LINE

Options --add-sa and --add-fa  can be used to compose dataset attributes directly on
the command line. The syntax is:

... --add-sa <attribute name> <comma-separated values> [DTYPE]

where the optional 'DTYPE' is any identifier of a NumPy data type (e.g. 'int',
or 'float32'). If no data type is specified the attribute values will be
strings.

If only one attribute value is given, it will copied and assigned to all
entries in the dataset.

LOAD DATA FROM TEXT FILES

All options for loading data from text files support optional parameters to
tweak the conversion:

... --add-sa-txt <mandatory values> [DELIMITER [DTYPE [SKIPROWS [COMMENTS]]]]

where 'DELIMITER' is the string that is used to separate values in the input
file, 'DTYPE' is any identifier of a NumPy data type (e.g. 'int', or 'float32'),
'SKIPROWS' is an integer indicating how many lines at the beginning of the
respective file shall be ignored, and 'COMMENTS' is a string indicating how
to-be-ignored comment lines are prefixed in the file.

LOAD DATA FROM NUMPY NPY FILES

All options for loading data from NumPy NPY files support an optional parameter:

... --add-fa-npy <mandatory values> [MEMMAP]

where 'MEMMAP' is a flag  that triggers whether the respective file shall be
read by memory-mapping, i.e. not read (immediately) into memory. Enable by
with on of: yes|1|true|enable|on'.

Examples:

Load 4D MRI image, assign atlas labels to a feature attribute, and attach class
labels from a text file. The resulting dataset is stored as 'ds.hdf5' in
the current directory.

  $ pymvpa2 mkds -o ds --mri-data bold.nii.gz --vol-attr area harvox.nii.gz --add-sa-txt targets labels.txt

"""

# magic line for manpage summary
# man: -*- % create a PyMVPA dataset from various sources

__docformat__ = 'restructuredtext'

import numpy as np
import argparse

from mvpa2.base import verbose, warning, error
from mvpa2.datasets import Dataset
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers import process_common_dsattr_opts, \
        hdf2ds, parser_add_common_opt
# necessary to enable dataset.summary()
import mvpa2.datasets.miscfx


parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

datasrc_args = ('input data sources', [
    (('--txt-data',), dict(type=str, nargs='+', metavar='VALUE',
        help="""load samples from a text file. The first value
                is the filename the data will be loaded from. Additional
                values modifying the way the data is loaded are described in the
                section "Load data from text files".""")),
    (('--npy-data',), dict(type=str, nargs='+', metavar='VALUE',
        help="""load samples from a Numpy .npy file. Compressed files (i.e.
             .npy.gz) are supported as well. The first value is the filename
             the data will be loaded from. Additional values modifying the way
             the data is loaded are described in the section "Load data from
             Numpy NPY files".""")),
    (('--mri-data',), {
        'type': str,
        'nargs': '+',
        'metavar': 'IMAGE',
        'help': """load data from an MR image, such as a NIfTI file. This can
                either be a single 4D image, or a list of 3D images, or a
                combination of both."""}),
])

mri_args = ('options for input from MR images', [
    (('--mask',), {
        'type': str,
        'metavar': 'IMAGE',
        'help': """mask image file with the same dimensions as an input data
                sample. All voxels corresponding to non-zero mask elements will
                be permitted into the dataset."""}),
    (('--add-vol-attr',), {
        'type': str,
        'nargs': 2,
        'action': 'append',
        'metavar': 'ARG',
        'help': """attribute name (1st argument) and image file with the same
                dimensions as an input data sample (2nd argument). The image
                data will be added as a feature attribute under the specified
                name."""}),
    (('--add-fsl-mcpar',), dict(type=str, metavar='FILENAME', help=
                """6-column motion parameter file in FSL's McFlirt format. Six
                additional sample attributes will be created: mc_{x,y,z} and
                mc_rot{1-3}, for translation and rotation estimates
                respectively.""")),
])


def setup_parser(parser):
    from .helpers import parser_add_optgroup_from_def, \
        parser_add_common_attr_opts, single_required_hdf5output
    # order of calls is relevant!
    parser_add_common_opt(parser, 'multidata', metavar='dataset', nargs='*',
            default=None)
    parser_add_optgroup_from_def(parser, datasrc_args, exclusive=True)
    parser_add_common_attr_opts(parser)
    parser_add_optgroup_from_def(parser, mri_args)
    parser_add_optgroup_from_def(parser, single_required_hdf5output)

def run(args):
    from mvpa2.base.hdf5 import h5save
    ds = None
    if not args.txt_data is None:
        verbose(1, "Load data from TXT file '%s'" % args.txt_data)
        samples = _load_from_txt(args.txt_data)
        ds = Dataset(samples)
    elif not args.npy_data is None:
        verbose(1, "Load data from NPY file '%s'" % args.npy_data)
        samples = _load_from_npy(args.npy_data)
        ds = Dataset(samples)
    elif not args.mri_data is None:
        verbose(1, "Load data from MRI image(s) %s" % args.mri_data)
        from mvpa2.datasets.mri import fmri_dataset
        vol_attr = dict()
        if not args.add_vol_attr is None:
            # XXX add a way to use the mapper of an existing dataset to
            # add a volume attribute without having to load the entire
            # mri data again
            vol_attr = dict(args.add_vol_attr)
            if not len(args.add_vol_attr) == len(vol_attr):
                warning("--vol-attr option with duplicate attribute name: "
                        "check arguments!")
            verbose(2, "Add volumetric feature attributes: %s" % vol_attr)
        ds = fmri_dataset(args.mri_data, mask=args.mask, add_fa=vol_attr)

    if ds is None:
        if args.data is None:
            raise RuntimeError('no data source specific')
        else:
            ds = hdf2ds(args.data)[0]
    else:
        if args.data is not None:
            verbose(1, 'ignoring dataset input in favor of other data source -- remove either one to disambiguate')

    # act on all attribute options
    ds = process_common_dsattr_opts(ds, args)

    if not args.add_fsl_mcpar is None:
        from mvpa2.misc.fsl.base import McFlirtParams
        mc_par = McFlirtParams(args.add_fsl_mcpar)
        for param in mc_par:
            verbose(2, "Add motion regressor as sample attribute '%s'"
                       % ('mc_' + param))
            ds.sa['mc_' + param] = mc_par[param]

    verbose(3, "Dataset summary %s" % (ds.summary()))
    # and store
    outfilename = args.output
    if not outfilename.endswith('.hdf5'):
        outfilename += '.hdf5'
    verbose(1, "Save dataset to '%s'" % outfilename)
    h5save(outfilename, ds, mkdir=True, compression=args.hdf5_compression)

########NEW FILE########
__FILENAME__ = cmd_mkevds
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Extract (multi-sample) events from a dataset

An arbitrary number of input datasets is loaded from HDF5 storage. All loaded
datasets are concatenated along the samples axis. Based on information about
onset and duration of a sequence of events corresponding samples are extracted
from the input datasets and converted into event samples. It is possible for an
event sample to consist of multiple input samples (i.e. temporal windows).

Events are defined by onset sample ID and number of consecutive samples that
comprise an event. However, events can also be defined as temporal onsets and
durations, which will be translated into sample IDs using time stamp information
in the input datasets.

Analogous to the 'mkds' command the event-related dataset can be extended with
arbitrary feature and sample attributes (one value per event for the latter).

The finished event-related dataset is written to an HDF5 file.

Examples:

Extract two events comprising of four consecutive samples from a dataset.

  $ pymvpa2 mkevds --onsets 3 9 --duration 4 -o evds.hdf5 -i 'mydata*.hdf5'

"""

# magic line for manpage summary
# man: -*- % extract (multi-sample) events from a dataset

__docformat__ = 'restructuredtext'

import numpy as np
import sys
import argparse
from mvpa2.base import verbose, warning, error
from mvpa2.datasets import Dataset, vstack
from mvpa2.mappers.fx import FxMapper, merge2first
from mvpa2.datasets.eventrelated import eventrelated_dataset, find_events
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
    import parser_add_common_opt, \
           ds2hdf5, arg2ds, process_common_dsattr_opts, _load_csv_table

parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

define_events_grp = ('options for defining events (choose one)', [
    (('--event-attrs',), dict(type=str, nargs='+', metavar='ATTR',
        help="""define events as a unique combinations of values from a set of
        sample attributes. Going through all samples in the order in which they
        appear in the input dataset, onset of events are determined by changes
        in the combination of attribute values. The length of an event is
        determined by the number of identical consecutive value combinations."""
        )),
    (('--onsets',), dict(type=float, nargs='*', metavar='TIME',
        help="""reads a list of event onsets (float) from the command line
        (space-separated). If this option is given, but no arguments are
        provided, onsets will be read from STDIN (one per line). If --time-attr
        is also given, onsets will be interpreted as time stamps, otherwise
        they are treated a integer ID of samples."""
        )),
    (('--csv-events',), dict(type=str, metavar='FILENAME',
        help="""read event information from a CSV table. A variety of dialects
        are supported. A CSV file must contain a header line with field names
        as a first row. The table must include an 'onset' column, and can
        optionally include an arbitrary number of additional columns
        (e.g. duration, target). All values are passed on to the event-related
        samples. If '-' is given as a value the CSV table is read from STDIN.
        """)),
    (('--fsl-ev3',), dict(type=str, nargs='+', metavar='FILENAME',
        help="""read event information from a text file in FSL's EV3 format
        (one event per line, three columns: onset, duration, intensity). One
        of more filenames can be given.""")),
])

mod_events_grp = ('options for modifying or converting events', [
    (('--time-attr',), dict(type=str, metavar='ATTR',
        help="""dataset attribute with time stamps for input samples. Onset and
        duration for all events will be converted using this information. All
        values are assumed to be of the same units.""")),
    (('--onset-column',), dict(type=str, metavar='ATTR',
        help="""name of the column in the CSV event table that indicates event
        onsets""")),
    (('--offset',), dict(type=float, metavar='VALUE',
        help="""fixed uniform event offset for all events. If no --time-attr
        option is given, this value indicates the number of input samples all
        event onsets shall be shifted. If --time-attr is given, this is treated
        as a temporal offset that needs to be given in the same unit as the time
        stamp attribute (see --time-attr).""")),
    (('--duration',), dict(type=float, metavar='VALUE',
        help="""fixed uniform duration for all events. If no --time-attr option
        is given, this value indicates the number of consecutive input samples
        following an onset that belong to an event. If --time-attr is given,
        this is treated as a temporal duration that needs to be given in the
        same unit as the time stamp attribute (see --time-attr).""")),
    (('--match-strategy',), dict(type=str, choices=('prev', 'next', 'closest'),
        default='prev',
        help="""strategy used to match time-based onsets to sample indices.
        'prev' chooses the closes preceding samples, 'next' the closest
        following sample and 'closest' to absolute closest sample. Default:
        'prev'""")),
    (('--event-compression',), dict(choices=('mean', 'median', 'min', 'max'),
        help="""specify whether and how events spanning multiple input samples
        shall be compressed. A number of methods can be chosen. Selecting, for
        example, 'mean' will yield the mean of all relevant input samples for
        an event. By default (when this option is not given) an event will
        comprise of all concatenated input samples.""")),
])

def setup_parser(parser):
    from .helpers import parser_add_optgroup_from_def, \
        parser_add_common_attr_opts, single_required_hdf5output
    parser_add_common_opt(parser, 'multidata', required=True)
    parser_add_optgroup_from_def(parser, define_events_grp, exclusive=True)
    parser_add_optgroup_from_def(parser, mod_events_grp)
    parser_add_common_attr_opts(parser)
    parser_add_optgroup_from_def(parser, single_required_hdf5output)


def run(args):
    ds = arg2ds(args.data)
    verbose(3, 'Concatenation yielded %i samples with %i features' % ds.shape)
    # build list of events
    events = []
    timebased_events = False
    if not args.event_attrs is None:
        def_attrs = dict([(k, ds.sa[k].value) for k in args.event_attrs])
        events = find_events(**def_attrs)
    elif not args.csv_events is None:
        if args.csv_events == '-':
            csv = sys.stdin.read()
            import cStringIO
            csv = cStringIO.StringIO(csv)
        else:
            csv = open(args.csv_events, 'rU')
        csvt = _load_csv_table(csv)
        if not len(csvt):
            raise ValueError("no CSV columns found")
        if args.onset_column:
            csvt['onset'] = csvt[args.onset_column]
        nevents = len(csvt[csvt.keys()[0]])
        events = []
        for ev in xrange(nevents):
            events.append(dict([(k, v[ev]) for k, v in csvt.iteritems()]))
    elif not args.onsets is None:
        if not len(args.onsets):
            args.onsets = [i for i in sys.stdin]
        # time or sample-based?
        if args.time_attr is None:
            oconv = int
        else:
            oconv = float
        events = [{'onset': oconv(o)} for o in args.onsets]
    elif not args.fsl_ev3 is None:
        timebased_events = True
        from mvpa2.misc.fsl import FslEV3
        events = []
        for evsrc in args.fsl_ev3:
            events.extend(FslEV3(evsrc).to_events())
    if not len(events):
        raise ValueError("no events defined")
    verbose(2, 'Extracting %i events' % len(events))
    if args.offset:
        # shift events
        for ev in events:
            ev['onset'] += args.offset
    if args.duration:
        # overwrite duration
        for ev in events:
            ev['duration'] = args.duration
    if args.event_compression is None:
        evmap = None
    elif args.event_compression == 'mean':
        evmap = FxMapper('features', np.mean, attrfx=merge2first)
    elif args.event_compression == 'median':
        evmap = FxMapper('features', np.median, attrfx=merge2first)
    elif args.event_compression == 'min':
        evmap = FxMapper('features', np.min, attrfx=merge2first)
    elif args.event_compression == 'max':
        evmap = FxMapper('features', np.max, attrfx=merge2first)
    # convert to event-related ds
    evds = eventrelated_dataset(ds, events, time_attr=args.time_attr,
                                match=args.match_strategy,
                                event_mapper=evmap)
    # act on all attribute options
    evds = process_common_dsattr_opts(evds, args)
    # and store
    ds2hdf5(evds, args.output, compression=args.hdf5_compression)
    return evds

########NEW FILE########
__FILENAME__ = cmd_preproc
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Preprocess a PyMVPA dataset.

This command can apply a number of preprocessing steps to a dataset. Currently
supported are

1. Polynomial de-trending

2. Spectral filtering

3. Feature-wise Z-scoring

All preprocessing steps are applied in the above order. If a different order is
required, preprocessing has to be split into two separate command calls.


POLYNOMIAL DE-TRENDING

This type of de-trending can be used to regress out arbitrary signals. In
addition to polynomials of any degree arbitrary timecourses stored as sample
attributes in a dataset can be used as confound regressors. This detrending
functionality is, in contrast to the implementation of spectral filtering,
also applicable to sparse-sampled data with potentially irregular inter-sample
intervals.


SPECTRAL FILTERING

Several option are provided that are used to construct a Butterworth low-,
high-, or band-pass filter. It is advised to inspect the filtered data
carefully as inappropriate filter settings can lead to unintented side-effect.
Only dataset with a fixed sampling rate are supported. The sampling rate
must be provided.


Examples:

Normalize all features in a dataset by Z-scoring

  $ pymvpa2 preproc --zscore -o ds_preprocessed -i dataset.hdf5

Perform Z-scoring and quadratic detrending of all features, but process all
samples sharing a unique value of the "chunks" sample attribute individually

  $ pymvpa2 preproc --chunks "chunks" --poly-detrend 2 --zscore -o ds_pp2 -i ds.hdf5

"""

# magic line for manpage summary
# man: -*- % apply preprocessing steps to a PyMVPA dataset

__docformat__ = 'restructuredtext'

import numpy as np
import argparse

from mvpa2.base import verbose, warning, error
from mvpa2.datasets import Dataset
from mvpa2.mappers.detrend import PolyDetrendMapper
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
        import parser_add_common_opt, ds2hdf5, \
               arg2ds, parser_add_optgroup_from_def, \
               single_required_hdf5output

parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

detrend_args = ('options for data detrending', [
    (('--poly-detrend',), (PolyDetrendMapper, 'polyord'), dict(metavar='DEG')),
    (('--detrend-chunks',), (PolyDetrendMapper, 'chunks_attr'),
     dict(metavar='CHUNKS_ATTR')),
    (('--detrend-coords',),
     dict(type=str, metavar='COORDS_ATTR',
         help="""name of a samples attribute that is added to the
              preprocessed dataset storing the coordinates of each sample in the
              space spanned by the polynomials. If an attribute of such name
              is already present in the dataset its values are interpreted
              as sample coordinates in the space spanned by the polynomials.
              This can be used to detrend datasets with irregular sample
              spacing.""")),
    (('--detrend-regrs',), (PolyDetrendMapper, 'opt_regs'),
     dict(nargs='+', metavar='ATTR', type=str))
])

normalize_args = ('options for data normalization', [
    (('--zscore',),
     dict(action='store_true',
          help="""perform feature normalization by Z-scoring.""")),
    (('--zscore-chunks',),
     dict(metavar='CHUNKS_ATTR',
          help="""name of a dataset sample attribute defining chunks of
               samples that shall be Z-scored independently. By default
               no chunk-wise normalization is done.""")),
    (('--zscore-params',),
     dict(metavar='PARAM', nargs=2, type=float,
          help="""define a fixed parameter set (mean, std) for Z-scoring,
               instead of computing from actual data.""")),
])

bandpassfilter_args = ('options for spectral filtering', [
    (('--filter-passband',),
     dict(metavar='FREQ', nargs='+', type=float,
          help="""critical frequencies of a Butterworth filter's pass band.
          Critical frequencies need to match the unit of the specified sampling
          rate (see: --sampling-rate). In case of a band pass filter low and
          high frequency cutoffs need to be specified (in this order). For
          low and high-pass filters is single cutoff frequency must be
          provided. The type of filter (low/high-pass) is determined from the
          relation to the stop band frequency (--filter-stopband).""")),
    (('--filter-stopband',),
     dict(metavar='FREQ', nargs='+', type=float,
          help="""Analog setting to --filter-passband for specifying the
          filter's stop band.""")),
    (('--sampling-rate',),
     dict(metavar='FREQ', type=float,
          help="""sampling rate of the dataset. All frequency specifications
          need to match the unit of the sampling rate.""")),
    (('--filter-passloss',),
     dict(metavar='dB', type=float, default=1.0,
          help="""maximum loss in the passband (dB). Default: 1 dB""")),
    (('--filter-stopattenuation',),
     dict(metavar='dB', type=float, default=30.0,
         help="""minimum attenuation in the stopband (dB). Default: 30 dB""")),
])

common_args = ('common options for all preprocessing', [
    (('--chunks',),
     dict(metavar='CHUNKS_ATTR',
          help="""shortcut option to enabled uniform chunkwise processing for
               all relevant preprocessing steps (see --zscore-chunks,
               --detrend-chunks). This global setting can be overwritten by
               additionally specifying the corresponding individual "chunk"
               options.""")),
    (('--strip-invariant-features',),
     dict(action='store_true',
          help="""After all pre-processing steps are done, strip all invariant
          features from the dataset.""")),

])

def setup_parser(parser):
    parser_add_common_opt(parser, 'multidata', required=True)
    # order of calls is relevant!
    for src in (common_args, detrend_args, bandpassfilter_args,
                normalize_args):
        parser_add_optgroup_from_def(parser, src)
    parser_add_optgroup_from_def(parser, single_required_hdf5output)


def run(args):
    if not args.chunks is None:
        # apply global "chunks" setting
        for cattr in ('detrend_chunks', 'zscore_chunks'):
            if getattr(args, cattr) is None:
                # only overwrite if individual option is not given
                args.__setattr__(cattr, args.chunks)
    ds = arg2ds(args.data)
    if not args.poly_detrend is None:
        if not args.detrend_chunks is None \
           and not args.detrend_chunks in ds.sa:
            raise ValueError(
                "--detrend-chunks attribute '%s' not found in dataset"
                % args.detrend_chunks)
        from mvpa2.mappers.detrend import poly_detrend
        verbose(1, "Detrend")
        poly_detrend(ds, polyord=args.poly_detrend,
                     chunks_attr=args.detrend_chunks,
                     opt_regs=args.detrend_regrs,
                     space=args.detrend_coords)
    if args.filter_passband is not None:
        from mvpa2.mappers.filters import iir_filter
        from scipy.signal import butter, buttord
        if args.sampling_rate is None or args.filter_stopband is None:
            raise ValueError(
                "spectral filtering requires specification of "
                "--filter-stopband and --sampling-rate")
        # determine filter type
        nyquist = args.sampling_rate / 2.0
        if len(args.filter_passband) > 1:
            btype = 'bandpass'
            if not len(args.filter_passband) == len(args.filter_stopband):
                raise ValueError("passband and stopband specifications have to "
                        "match in size")
            wp = [v / nyquist for v in args.filter_passband]
            ws = [v / nyquist for v in args.filter_stopband]
        elif args.filter_passband[0] < args.filter_stopband[0]:
            btype = 'lowpass'
            wp = args.filter_passband[0] / nyquist
            ws = args.filter_stopband[0] / nyquist
        elif args.filter_passband[0] > args.filter_stopband[0]:
            btype = 'highpass'
            wp = args.filter_passband[0] / nyquist
            ws = args.filter_stopband[0] / nyquist
        else:
            raise ValueError("invalid specification of Butterworth filter")
        # create filter
        verbose(1, "Spectral filtering (%s)" % (btype,))
        try:
            ord, wn = buttord(wp, ws,
                              args.filter_passloss,
                              args.filter_stopattenuation,
                              analog=False)
            b, a = butter(ord, wn, btype=btype)
        except OverflowError:
            raise ValueError("cannot contruct Butterworth filter for the given "
                             "specification")
        ds = iir_filter(ds, b, a)

    if args.zscore:
        from mvpa2.mappers.zscore import zscore
        verbose(1, "Z-score")
        zscore(ds, chunks_attr=args.zscore_chunks,
               params=args.zscore_params)
        verbose(3, "Dataset summary %s" % (ds.summary()))
    # invariants?
    if not args.strip_invariant_features is None:
        from mvpa2.datasets.miscfx import remove_invariant_features
        ds = remove_invariant_features(ds)
    # and store
    ds2hdf5(ds, args.output, compression=args.hdf5_compression)
    return ds

########NEW FILE########
__FILENAME__ = cmd_searchlight
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Traveling ROI analysis

"""

# magic line for manpage summary
# man: -*- % traveling ROI analysis

__docformat__ = 'restructuredtext'

import numpy as np
import sys
import os
import argparse
from mvpa2.base import verbose, warning, error
from mvpa2.datasets import vstack
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
    import parser_add_common_opt, ds2hdf5, hdf2ds, \
           get_crossvalidation_instance, crossvalidation_opts_grp, \
           arg2neighbor, script2obj

parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

searchlight_opts_grp = ('options for searchlight setup', [
    (('--payload',), dict(required=True,
        help="""switch to select a particular analysis type to be run in a
        searchlight fashion on a dataset. Depending on the choice the
        corresponding analysis setup options are evaluated. 'cv' computes
        a cross-validation analysis. Alternatively, the argument to this option
        can also be a script filename in which a custom measure is built that
        is then ran as a searchlight.""")),
    (('--neighbors',), dict(type=arg2neighbor, metavar='SPEC', action='append',
        required=True,
        help="""define the size and shape of an ROI with respect to a
        center/seed location. If a single integer number is given, it is
        interpreted as the radius (in number of grid elements) around a seed
        location. By default grid coordinates for features are taken from
        a 'voxel_indices' feature attribute in the input dataset. If coordinates
        shall be taken from a different attribute, the radius value can be
        prefixed with the attribute name, i.e. 'altcoords:2'. For ROI shapes
        other than spheres (with potentially additional parameters), the shape
        name can be specified as well, i.e. 'voxel_indices:HollowSphere:3:2'.
        All neighborhood objects from the mvpa2.misc.neighborhood module are
        supported. For custom ROI shapes it is also possible to pass a script
        filename, or an attribute name plus script filename combination, i.e.
        'voxel_indices:myownshape.py' (advanced). It is possible to specify
        this option multiple times to define multi-space ROI shapes for, e.g.,
        spatio-temporal searchlights.""")),
    (('--nproc',), dict(type=int, default=1,
        help="""Use the specific number or worker processes for computing.""")),
    (('--multiproc-backend',), dict(choices=('native', 'hdf5'),
        default='native',
        help="""Specifies the way results are provided back from a processing
        block in case of --nproc > 1. 'native' is pickling/unpickling of
        results, while 'hdf5' uses HDF5 based file storage. 'hdf5' might be more
        time and memory efficient in some cases.""")),
    (('--aggregate-fx',), dict(type=script2obj,
        help="""use a custom result aggregation function for the searchlight
             """)),
])

searchlight_constraints_opts_grp = ('options for constraining the searchlight', [
    (('--scatter-rois',), dict(type=arg2neighbor, metavar='SPEC',
        help="""scatter ROI locations across the available space. The arguments
        supported by this option are identical to those of --neighbors. ROI
        locations are randomly picked from all possible locations with the
        constraint that the center coordinates of any ROI is NOT within
        the neighborhood (as defined by this option's argument) of a second
        ROI. Increasing the size of the neighborhood therefore increases the
        scarceness of the sampling.""")),
    (('--roi-attr',), dict(metavar='ATTR/EXPR', nargs='+',
        help="""name of a feature attribute whose non-zero values define
        possible ROI seeds/centers. Alternatively, this can also be an
        expression like: parcellation_roi eq 16 (see the 'select' command
        on information what expressions are supported).""")),
])


# XXX this should eventually move into the main code base, once
# sufficiently generalized
def _fill_in_scattered_results(sl, dataset, roi_ids, results):
    """this requires the searchlight conditional attribute 'roi_feature_ids'
    to be enabled"""
    import numpy as np
    from mvpa2.datasets import Dataset

    resmap = None
    probmap = None
    for resblock in results:
        for res in resblock:
            if resmap is None:
                # prepare the result container
                resmap = np.zeros((len(res), dataset.nfeatures),
                                  dtype=res.samples.dtype)
                if 'null_prob' in res.fa:
                    # initialize the prob map also with zeroes, as p=0 can never
                    # happen as an empirical result
                    probmap = np.zeros((dataset.nfeatures,) + res.fa.null_prob.shape[1:],
                                      dtype=res.samples.dtype)
                observ_counter = np.zeros(dataset.nfeatures, dtype=int)
            #project the result onto all features -- love broadcasting!
            resmap[:, res.a.roi_feature_ids] += res.samples
            if not probmap is None:
                probmap[res.a.roi_feature_ids] += res.fa.null_prob
            # increment observation counter for all relevant features
            observ_counter[res.a.roi_feature_ids] += 1
    # when all results have been added up average them according to the number
    # of observations
    observ_mask = observ_counter > 0
    resmap[:, observ_mask] /= observ_counter[observ_mask]
    result_ds = Dataset(resmap,
                        fa={'observations': observ_counter})
    if not probmap is None:
        # transpose to make broadcasting work -- creates a view, so in-place
        # modification still does the job
        probmap.T[:,observ_mask] /= observ_counter[observ_mask]
        result_ds.fa['null_prob'] = probmap.squeeze()
    if 'mapper' in dataset.a:
        import copy
        result_ds.a['mapper'] = copy.copy(dataset.a.mapper)
    return result_ds


def setup_parser(parser):
    from .helpers import parser_add_optgroup_from_def, \
        parser_add_common_attr_opts, single_required_hdf5output, ca_opts_grp
    parser_add_common_opt(parser, 'multidata', required=True)
    parser_add_optgroup_from_def(parser, searchlight_opts_grp)
    parser_add_optgroup_from_def(parser, ca_opts_grp)
    parser_add_optgroup_from_def(parser, searchlight_constraints_opts_grp)
    parser_add_optgroup_from_def(parser, crossvalidation_opts_grp,
                                 prefix='--cv-')
    parser_add_optgroup_from_def(parser, single_required_hdf5output)

def run(args):
    if os.path.isfile(args.payload) and args.payload.endswith('.py'):
        measure = script2obj(args.payload)
    elif args.payload == 'cv':
        if args.cv_learner is None or args.cv_partitioner is None:
            raise ValueError('cross-validation payload requires --learner and --partitioner')
        # get CV instance
        measure = get_crossvalidation_instance(
                    args.cv_learner, args.cv_partitioner, args.cv_errorfx,
                    args.cv_sampling_repetitions, args.cv_learner_space,
                    args.cv_balance_training, args.cv_permutations,
                    args.cv_avg_datafold_results, args.cv_prob_tail)
    else:
        raise RuntimeError("this should not happen")
    dss = hdf2ds(args.data)
    verbose(3, 'Loaded %i dataset(s)' % len(dss))
    ds = vstack(dss)
    verbose(3, 'Concatenation yielded %i samples with %i features' % ds.shape)
    # setup neighborhood
    # XXX add big switch to allow for setting up surface-based neighborhoods
    from mvpa2.misc.neighborhood import IndexQueryEngine
    qe = IndexQueryEngine(**dict(args.neighbors))
    # determine ROIs
    rids = None     # all by default
    aggregate_fx = args.aggregate_fx
    if args.roi_attr is not None:
        # first figure out which roi features should be processed
        if len(args.roi_attr) == 1 and args.roi_attr[0] in ds.fa.keys():
            # name of an attribute -> pull non-zeroes
            rids = ds.fa[args.roi_attr[0]].value.nonzero()[0]
        else:
            # an expression?
            from .cmd_select import _eval_attr_expr
            rids = _eval_attr_expr(args.roi_attr, ds.fa).nonzero()[0]

    seed_ids = None
    if args.scatter_rois is not None:
        # scatter_neighborhoods among available ids if was requested
        from mvpa2.misc.neighborhood import scatter_neighborhoods
        attr, nb = args.scatter_rois
        coords = ds.fa[attr].value
        if rids is not None:
            # select only those which were chosen by ROI
            coords = coords[rids]
        _, seed_ids = scatter_neighborhoods(nb, coords)
        if aggregate_fx is None:
            # no custom one given -> use default "fill in" function
            aggregate_fx = _fill_in_scattered_results
            if args.enable_ca is None:
                args.enable_ca = ['roi_feature_ids']
            elif 'roi_feature_ids' not in args.enable_ca:
                args.enable_ca += ['roi_feature_ids']

    if seed_ids is None:
        roi_ids = rids
    else:
        if rids is not None:
            # we had to sub-select by scatterring among available rids
            # so we would need to get original ids
            roi_ids = rids[seed_ids]
        else:
            # scattering happened on entire feature-set
            roi_ids = seed_ids

    verbose(3, 'Attempting %i ROI analyses'
               % ((roi_ids is None) and ds.nfeatures or len(roi_ids)))

    from mvpa2.measures.searchlight import Searchlight

    sl = Searchlight(measure,
                     queryengine=qe,
                     roi_ids=roi_ids,
                     nproc=args.nproc,
                     results_backend=args.multiproc_backend,
                     results_fx=aggregate_fx,
                     enable_ca=args.enable_ca,
                     disable_ca=args.disable_ca)
    # XXX support me too!
    #                 add_center_fa
    #                 tmp_prefix
    #                 nblocks
    #                 null_dist
    # run
    res = sl(ds)
    if (seed_ids is not None) and ('mapper' in res.a):
        # strip the last mapper link in the chain, which would be the seed ID selection
        res.a['mapper'] = res.a.mapper[:-1]
    # XXX create more output
    # and store
    ds2hdf5(res, args.output, compression=args.hdf5_compression)
    return res

########NEW FILE########
__FILENAME__ = cmd_select
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Select a subset of samples and/or features from a dataset

A subset of samples and/or feature can be selected by their indices (see
--samples/features-by-index) or via simple expressions evaluating attribute
value (see --samples/features-by-attr). It is possible to specify options
for selecting samples and features simultaneously. It is also possible to strip
arbitrary attributes from the output dataset (see --strip-...).

SELECTION BY INDEX

All --...-by-index options accept a sequence of integer indices. Alternatively
it is possible to specify regular sequences of indices using a START:STOP:STEP
notation (zero-based). For example, ':5' selects the first five elements, '2:4'
selects the third and fourth element, and ':20:2' selects all even numbered
elements from the first 20.

SELECTION BY ATTRIBUTE

All --...by-attr options support a simple expression language that allows for
creating filters/masks from attribute values. Such selection expressions are
made up of ATTRIBUTE OPERATOR VALUE triplets that can be combined via 'and' or
'or' keywords. For example:

... --samples-by-attr subj eq 5 and run lt 5 or run gt 10

selects all samples where attribute 'subj' equals 5 and the run attribute is
either less than 5 or greater than 10. 'and' and 'or' operations are done in
strictly serial order (no nested conditions).

Supported operators are:

eq (equal)
ne (not equal)
ge (greater or equal)
le (less or equal)
gt (greater than)
lt (less than)

"""

# magic line for manpage summary
# man: -*- % select a subset of samples and/or features from a dataset

__docformat__ = 'restructuredtext'

import numpy as np
import sys
import argparse
from mvpa2.base import verbose, warning, error
from mvpa2.datasets import Dataset, vstack
from mvpa2.mappers.fx import FxMapper
from mvpa2.datasets.eventrelated import eventrelated_dataset, find_events
if __debug__:
    from mvpa2.base import debug
from mvpa2.cmdline.helpers \
    import parser_add_common_opt, \
           ds2hdf5, hdf2ds

# map operator symbols to method names
attr_operators = {
    'eq': '__eq__',
    'ne': '__ne__',
    'ge': '__ge__',
    'le': '__le__',
    'gt': '__gt__',
    'lt': '__lt__',
}

filter_joiner = {
    'and': np.logical_and,
    'or': np.logical_or,
}

def _eval_attr_expr(expr, col):
    #read from the front
    rev_expr = expr[::-1]
    # current filter -- take all
    actfilter = np.ones(len(col[col.keys()[0]]), dtype='bool')
    joiner = None
    while len(rev_expr):
        ex = rev_expr.pop()
        if not ex in ('and', 'or'):
            # eval triplet (attr, operator, value)
            attr = ex
            try:
                attr = col[attr].value
            except KeyError:
                raise ValueError("unknown attribute '%s' in expression [%s]. Valid attributes: %s"
                                 % (attr, ' '.join(expr), col.keys())) 
            op = rev_expr.pop()
            try:
                op = attr_operators[op]
            except KeyError:
                raise ValueError("unknown operator '%s' in expression [%s]. Valid operators: %s"
                                 % (op, ' '.join(expr), attr_operators.keys())) 
            val = rev_expr.pop()
            # convert value into attr dtype
            try:
                val = attr.dtype.type(val)
            except ValueError:
                raise ValueError("can't convert '%s' into attribute data type '%s' in expression [%s]"
                                 % (val, attr.dtype, ' '.join(expr))) 
            # evaluate expression
            newfilter = getattr(attr, op)(val)
            # merge with existing filter
            if joiner is None:
                # replace
                actfilter = newfilter
            else:
                # join
                actfilter = joiner(actfilter, newfilter)
        else:
            joiner = filter_joiner[ex]
    return actfilter

parser_args = {
    'formatter_class': argparse.RawDescriptionHelpFormatter,
}

samples_grp = ('options for selecting samples', [
    (('--samples-by-index',), dict(type=str, nargs='+', metavar='IDX',
        help="""select a subset of samples by index. See section 'SELECTION BY
        INDEX' for more details."""
        )),
    (('--samples-by-attr',), dict(type=str, nargs='+', metavar='EXPR',
        help="""select a subset of samples by attribute evaluation. See section
        'SELECTION BY ATTRIBUTE' for more details."""
        )),
])

features_grp = ('options for selecting features', [
    (('--features-by-index',), dict(type=str, nargs='+', metavar='IDX',
        help="""select a subset of features by index. See section 'SELECTION BY
        INDEX' for more details."""
        )),
    (('--features-by-attr',), dict(type=str, nargs='+', metavar='EXPR',
        help="""select a subset of features by attribute evaluation. See section
        'SELECTION BY ATTRIBUTE' for more details."""
        )),
])


strip_grp = ('options for removing attributes', [
    (('--strip-sa',), dict(type=str, nargs='+', metavar='ATTR',
        help="""strip one or more samples attributes given by their name from
        a dataset."""
        )),
    (('--strip-fa',), dict(type=str, nargs='+', metavar='ATTR',
        help="""strip one or more feature attributes given by their name from
        a dataset."""
        )),
    (('--strip-da',), dict(type=str, nargs='+', metavar='ATTR',
        help="""strip one or more dataset attributes given by their name from
        a dataset."""
        )),
])

def setup_parser(parser):
    from .helpers import parser_add_optgroup_from_def, \
        parser_add_common_attr_opts, single_required_hdf5output
    parser_add_common_opt(parser, 'multidata', required=True)
    parser_add_optgroup_from_def(parser, samples_grp, exclusive=True)
    parser_add_optgroup_from_def(parser, features_grp, exclusive=True)
    parser_add_optgroup_from_def(parser, strip_grp)
    parser_add_optgroup_from_def(parser, single_required_hdf5output)

def run(args):
    dss = hdf2ds(args.data)
    verbose(3, 'Loaded %i dataset(s)' % len(dss))
    ds = vstack(dss)
    verbose(3, 'Concatenation yielded %i samples with %i features' % ds.shape)
    # slicing
    sliceme = {'samples': slice(None), 'features': slice(None)}
    # indices
    for opt, col, which in ((args.samples_by_index, ds.sa, 'samples'),
                     (args.features_by_index, ds.fa, 'features')):
        if opt is None:
            continue
        if len(opt) == 1 and opt[0].count(':'):
            # slice spec
            arg = opt[0].split(':')
            spec = []
            for a in arg:
                if not len(a):
                    spec.append(None)
                else:
                    spec.append(int(a))
            sliceme[which] = slice(*spec)
        else:
            # actual indices
            sliceme[which] = [int(o) for o in opt]
    # attribute evaluation
    for opt, col, which in ((args.samples_by_attr, ds.sa, 'samples'),
                     (args.features_by_attr, ds.fa, 'features')):
        if opt is None:
            continue
        sliceme[which] = _eval_attr_expr(opt, col)

    # apply selection
    ds = ds.__getitem__((sliceme['samples'], sliceme['features']))
    verbose(1, 'Selected %i samples with %i features' % ds.shape)

    # strip attributes
    for attrarg, col, descr in ((args.strip_sa, ds.sa, 'sample '),
                                (args.strip_fa, ds.fa, 'feature '),
                                (args.strip_da, ds.a, '')):
        if not attrarg is None:
            for attr in attrarg:
                try:
                    del col[attr]
                except KeyError:
                    warning("dataset has no %sattribute '%s' to remove"
                            % (descr, attr))
    # and store
    ds2hdf5(ds, args.output, compression=args.hdf5_compression)
    return ds

########NEW FILE########
__FILENAME__ = helpers
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
""""""

__docformat__ = 'restructuredtext'

import argparse
import re
import sys
import copy
import os

import numpy as np

from mvpa2.base import verbose
if __debug__:
    from mvpa2.base import debug
from mvpa2.base.types import is_datasetlike
from mvpa2.base.state import ClassWithCollections

class HelpAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        if option_string == '--help':
            # lets use the manpage on mature systems ...
            try:
                import subprocess
                subprocess.check_call(
                        'man %s 2> /dev/null' % parser.prog.replace(' ', '-'),
                        shell=True)
                sys.exit(0)
            except (subprocess.CalledProcessError, OSError):
                # ...but silently fall back if it doesn't work
                pass
        if option_string == '-h':
            helpstr = "%s\n%s" \
                    % (parser.format_usage(),
                       "Use '--help' to get more comprehensive information.")
        else:
            helpstr = parser.format_help()
        # better for help2man
        helpstr = re.sub(r'optional arguments:', 'options:', helpstr)
        helpstr = re.sub(r'positional arguments:\n.*\n', '', helpstr)
        # convert all heading to have the first character uppercase
        headpat = re.compile(r'^([a-z])(.*):$',  re.MULTILINE)
        helpstr = re.subn(headpat,
               lambda match: r'{0}{1}:'.format(match.group(1).upper(),
                                             match.group(2)),
               helpstr)[0]
        # usage is on the same line
        helpstr = re.sub(r'^usage:', 'Usage:', helpstr)
        if option_string == '--help-np':
            usagestr = re.split(r'\n\n[A-Z]+', helpstr, maxsplit=1)[0]
            usage_length = len(usagestr)
            usagestr = re.subn(r'\s+', ' ', usagestr.replace('\n', ' '))[0]
            helpstr = '%s\n%s' % (usagestr, helpstr[usage_length:])
        print helpstr
        sys.exit(0)


def parser_add_common_opt(parser, opt, names=None, **kwargs):
    """Add a named option to a cmdline arg parser.

    Parameters
    ----------
    opt: str
      name of the option
    names: tuple or None
      sequence of names under which the option should be available.
      If None, the default will be used.
    """
    opt_tmpl = globals()[opt]
    opt_kwargs = opt_tmpl[2].copy()
    opt_kwargs.update(kwargs)
    if names is None:
        parser.add_argument(*opt_tmpl[1], **opt_kwargs)
    else:
        parser.add_argument(*names, **opt_kwargs)

def strip_from_docstring(doc, paragraphs=None, sections=None):
    if paragraphs is None:
        paragraphs = []
    if sections is None:
        sections = []
    out = []
    # split into paragraphs
    doc = doc.split('\n\n')
    section = ''
    for par_i, par in enumerate(doc):
        lines = par.split('\n')
        if len(lines) > 1 \
           and len(lines[0]) == len(lines[1]) \
           and lines[1] == '-' * len(lines[0]):
               section = lines[0]
        if (par_i in paragraphs) or (section in sections):
            continue
        out.append(par)
    return '\n\n'.join(out)

def param2arg(parser, param, arg_names=None, **kwargs):
    """Convert a Node parameter into a parser argument.

    Parameters
    ----------
    parser : instance
      argparse parser instance (could be option group)
    param : tuple or parameter instance
      A length-2 tuple with the Node class (no instance) as first, and the
      name of the parameter as second element.
    arg_names : tuple(str), optional
      Argument name(s) to overwrite the default parameter name.
    **kwargs
      Any addtional options are passed on to the `add_argument()` function
      call.
    """
    if kwargs is None:
        kwargs = {}
    if isinstance(param, tuple):
        # get param instance
        param = param[0]._collections_template['params'][param[1]]
    if arg_names is None:
        # use parameter name as default argument name
        arg_names = ('--%s' % param.name.replace('_', '-'),)
    help = param.__doc__
    if param.constraints is not None:
        # allow for parameter setting overwrite via kwargs
        if not 'default' in kwargs:
            kwargs['default'] = param.default
        if not 'type' in kwargs:
            kwargs['type'] = param.constraints
        # include value contraint description and default
        # into the help string
        cdoc = param.constraints.long_description()
        if cdoc[0] == '(' and cdoc[-1] == ')':
            cdoc = cdoc[1:-1]
        help += ' Constraints: %s.' % cdoc
    try:
        help += " [Default: %r]" % (kwargs['default'],)
    except:
        pass
    # create the parameter, using the constraint instance for type
    # conversion
    parser.add_argument(*arg_names, help=help,
                        **kwargs)

def ca2arg(parser, klass, ca, arg_names=None, help=None):
    ca = klass._collections_template['ca'][ca]
    if arg_names is None:
        arg_names = ('--%s' % ca.name.replace('_', '-'),)
    help_ = ca.__doc__
    if help:
        help_ = help_ + help
    parser.add_argument(*arg_names, help=help_, default=False,
                        action='store_true')


def arg2bool(arg):
    arg = arg.lower()
    if arg in ['0', 'no', 'off', 'disable', 'false']:
        return False
    elif arg in ['1', 'yes', 'on', 'enable', 'true']:
        return True
    else:
        raise argparse.ArgumentTypeError(
                "'%s' cannot be converted into a boolean" % (arg,))

def arg2none(arg):
    arg = arg.lower()
    if arg == 'none':
        return None
    else:
        raise argparse.ArgumentTypeError(
                "'%s' cannot be converted into `None`" % (arg,))

def arg2learner(arg, index=0):
    from mvpa2.clfs.warehouse import clfswh
    if arg in clfswh.descriptions:
        # arg is a description
        return clfswh.get_by_descr(arg)
    elif os.path.isfile(arg) and arg.endswith('.py'):
        # arg is a script filepath
        return script2obj(arg)
    else:
        # warehouse tag collection?
        try:
            learner = clfswh.__getitem__(*arg.split(':'))
            if not len(learner):
                raise argparse.ArgumentTypeError(
                    "not match for given learner capabilities %s in the warehouse" % (arg,))
            return learner[index]
        except ValueError:
            # unknown tag
            raise argparse.ArgumentTypeError(
                "'%s' is neither a known classifier description, nor a script, "
                "nor a sequence of valid learner capabilities" % (arg,))

def script2obj(filepath):
    locals = {}
    execfile(filepath, locals, locals)
    if not len(locals):
        raise argparse.ArgumentTypeError(
            "executing script '%s' did not create at least one object" % filepath)
    elif len(locals) > 1 and not ('obj' in locals or 'fx' in locals):
        raise argparse.ArgumentTypeError(
            "executing script '%s' " % filepath
            + "did create multiple objects %s " % locals.keys()
            + "but none is named 'obj' or 'fx'")
    if len(locals) == 1:
        return locals.values()[0]
    else:
        if 'obj' in locals:
            return locals['obj']
        else:
            return locals['fx']

def arg2partitioner(arg):
    # check for an optional 'attr' argument
    args = arg.split(':')
    arg = args[0]
    if len(args) == 1:
        chunk_attr = 'chunks'
    else:
        chunk_attr = ':'.join(args[1:])
    arglower = arg.lower()
    import mvpa2.generators.partition as part
    if arglower == 'oddeven':
        return part.OddEvenPartitioner(attr=chunk_attr)
    elif arglower == 'half':
        return part.HalfPartitioner(attr=chunk_attr)
    elif arglower.startswith('group-'):
        ngroups = int(arglower[6:])
        return part.NGroupPartitioner(ngroups, attr=chunk_attr)
    elif arglower.startswith('n-'):
        nfolds = int(arglower[2:])
        return part.NFoldPartitioner(nfolds, attr=chunk_attr)
    elif os.path.isfile(arg) and arg.endswith('.py'):
        # arg is a script filepath
        return script2obj(arg)
    else:
        raise argparse.ArgumentTypeError(
            "'%s' does not describe a supported partitioner type" % arg)

def arg2errorfx(arg):
    import mvpa2.misc.errorfx as efx
    if hasattr(efx, arg):
        return getattr(efx, arg)
    elif os.path.isfile(arg) and arg.endswith('.py'):
        # arg is a script filepath
        return script2obj(arg)
    else:
        raise argparse.ArgumentTypeError(
            "'%s' does not describe a supported error function" % arg)

def arg2hdf5compression(arg):
    try:
        return int(arg)
    except:
        return arg

def arg2neighbor(arg):
    # [[shape:]shape:]params
    comp = arg.split(':')
    if not len(comp):
        # need at least a radius
        raise ValueError("incomplete neighborhood specification")
    if len(comp) == 1:
        # [file|sphere radius]
        attr = 'voxel_indices'
        arg = comp[0]
        if os.path.isfile(arg) and arg.endswith('.py'):
            neighbor = script2obj(arg)
        else:
            from mvpa2.misc.neighborhood import Sphere
            neighbor = Sphere(int(arg))
    elif len(comp) == 2:
        # attr:[file|sphere radius]
        attr = comp[0]
        arg = comp[1]
        if os.path.isfile(arg) and arg.endswith('.py'):
            neighbor = script2obj(arg)
        else:
            from mvpa2.misc.neighborhood import Sphere
            neighbor = Sphere(int(arg))
    elif len(comp) > 2:
        attr = comp[0]
        shape = comp[1]
        params = [float(c) for c in comp[2:]]
        import mvpa2.misc.neighborhood as neighb
        neighbor = getattr(neighb, shape)(*params)
    return attr, neighbor

def ds2hdf5(ds, fname, compression=None):
    """Save one or more datasets into an HDF5 file.

    Parameters
    ----------
    ds : Dataset or list(Dataset)
      One or more datasets to store
    fname : str
      Filename of the output file. If it doesn't end with '.hdf5', such an
      extension will be appended.
    compression : {'gzip','lzf','szip'} or 1-9
      compression type for HDF5 storage. Available values depend on the specific
      HDF5 installation.
    """
    # this one doesn't actually check what it stores
    from mvpa2.base.hdf5 import h5save
    if not fname.endswith('.hdf5'):
        fname = '%s.hdf5' % fname
    verbose(1, "Save dataset to '%s'" % fname)
    h5save(fname, ds, mkdir=True, compression=compression)


def hdf2ds(fnames):
    """Load dataset(s) from an HDF5 file

    Parameters
    ----------
    fname : list(str)
      Names of the input HDF5 files

    Returns
    -------
    list(Dataset)
      All datasets-like elements in all given HDF5 files (in order of
      appearance). If any given HDF5 file contains non-Dataset elements
      they are silently ignored. If no given HDF5 file contains any
      dataset, an empty list is returned.
    """
    from mvpa2.base.hdf5 import h5load
    dss = []
    for fname in fnames:
        content = h5load(fname)
        if is_datasetlike(content):
            dss.append(content)
        else:
            for c in content:
                if is_datasetlike(c):
                    dss.append(c)
    return dss

def arg2ds(sources):
    """Convert a sequence of dataset sources into a dataset.

    This function would be used to used to convert a single --input
    multidata specification into a dataset. For multiple --input
    arguments execute this function in a loop.
    """
    from mvpa2.base.dataset import vstack
    return vstack(hdf2ds(sources))

def parser_add_common_attr_opts(parser):
    """Set up common parser options for adding dataset attributes"""
    for args in (attr_from_cmdline, attr_from_txt, attr_from_npy):
        parser_add_optgroup_from_def(parser, args)

def parser_add_optgroup_from_def(parser, defn, exclusive=False, prefix=None):
    """Add an entire option group from a definition in a custom format

    Parameters
    ----------
    parser : argparser instance
    defn : tuple
      Option group spec. Complicated beast. Grep source code for syntax examples.
    exclusive : bool
      Flag to make all options in the group mutually exclusive.
    prefix : str
      Prefix all option names with this string.

    Returns
    -------
    parser argument group
    """
    optgrp = parser.add_argument_group(defn[0])
    if exclusive:
        rgrp = optgrp.add_mutually_exclusive_group()
    else:
        rgrp = optgrp
    for opt in defn[1]:
        namespec = opt[0]
        param = None
        if len(namespec) == 2 and not isinstance(namespec[0], basestring) \
          and issubclass(namespec[0], ClassWithCollections):
            # parameter spec -> use its name
            param = namespec[0]._collections_template['params'][namespec[1]]
            optnames = ('--%s' % param.name.replace('_', '-'),)
        else:
            # take the literal names
            optnames = namespec
        if not prefix is None:
            # overwrite all option names with a common prefix
            optnames = ['%s%s' % (prefix, on.lstrip('-')) for on in optnames]
        if param is None and len(opt) > 1 and not isinstance(opt[1], dict):
            # parameter spec is given at 2nd position
            param = opt[1]
        if isinstance(opt[-1], dict):
            # last element has kwags for add_argument
            add_kwargs = opt[-1]
        else:
            # nothing to add
            add_kwargs = {}
        if not param is None:
            param2arg(rgrp, param, arg_names=optnames, **add_kwargs)
        else:
            rgrp.add_argument(*optnames, **add_kwargs)
    return optgrp

def process_common_dsattr_opts(ds, args):
    """Goes through an argument namespace and processes attribute options"""
    # legacy support
    if not args.add_sa_attr is None:
        from mvpa2.misc.io.base import SampleAttributes
        smpl_attrs = SampleAttributes(args.add_sa_attr)
        for a in ('targets', 'chunks'):
            verbose(2, "Add sample attribute '%s' from sample attributes file"
                       % a)
            ds.sa[a] = getattr(smpl_attrs, a)
    # loop over all attribute configurations that we know
    attr_cfgs = (# var, dst_collection, loader
            ('--add-sa', args.add_sa, ds.sa, _load_from_cmdline),
            ('--add-fa', args.add_fa, ds.fa, _load_from_cmdline),
            ('--add-sa-txt', args.add_sa_txt, ds.sa, _load_from_txt),
            ('--add-fa-txt', args.add_fa_txt, ds.fa, _load_from_txt),
            ('--add-sa-npy', args.add_sa_npy, ds.sa, _load_from_npy),
            ('--add-fa-npy', args.add_fa_npy, ds.fa, _load_from_npy),
        )
    for varid, srcvar, dst_collection, loader in attr_cfgs:
        if not srcvar is None:
            for spec in srcvar:
                attr_name = spec[0]
                if not len(spec) > 1:
                    raise argparse.ArgumentTypeError(
                        "%s option need at least two values " % varid +
                        "(attribute name and source filename (got: %s)" % spec)
                if dst_collection is ds.sa:
                    verbose(2, "Add sample attribute '%s' from '%s'"
                               % (attr_name, spec[1]))
                else:
                    verbose(2, "Add feature attribute '%s' from '%s'"
                               % (attr_name, spec[1]))
                attr = loader(spec[1:])
                try:
                    dst_collection[attr_name] = attr
                except ValueError, e:
                    # try making the exception more readable
                    e_str = str(e)
                    if e_str.startswith('Collectable'):
                        raise ValueError('attribute %s' % e_str[12:])
                    else:
                        raise e
    return ds

def _load_from_txt(args):
    defaults = dict(dtype=None, delimiter=None, skiprows=0, comments=None)
    if len(args) > 1:
        defaults['delimiter'] = args[1]
    if len(args) > 2:
        defaults['dtype'] = args[2]
    if len(args) > 3:
        defaults['skiprows'] = int(args[3])
    if len(args) > 4:
        defaults['comments'] = args[4]
    data = np.loadtxt(args[0], **defaults)
    return data

def _load_from_cmdline(args):
    defaults = dict(dtype='str', sep=',')
    if len(args) > 1:
        defaults['dtype'] = args[1]
    if defaults['dtype'] == 'str':
        data = [s.strip() for s in args[0].split(defaults['sep'])]
    else:
        import numpy as np
        data = np.fromstring(args[0], **defaults)
    return data

def _load_from_npy(args):
    defaults = dict(mmap_mode=None)
    if len(args) > 1 and arg2bool(args[1]):
        defaults['mmap_mode'] = 'r'
    data = np.load(args[0], **defaults)
    return data

def _load_csv_table(f):
    import csv
    import numpy as np
    sniffer = csv.Sniffer()
    try:
        dialect = sniffer.sniff(f.read(1024))
    except:
        # maybe a sloppy header with a trailing delimiter?
        f.seek(0)
        sample = [f.readline() for s in range(3)]
        sample[0] = sample[0].strip()
        dialect = sniffer.sniff('\n'.join(sample))
    f.seek(0)
    reader = csv.DictReader(f, dialect=dialect)
    table = dict(zip(reader.fieldnames,
                       [list() for i in xrange(len(reader.fieldnames))]))
    for row in reader:
        for k, v in row.iteritems():
            table[k].append(v)
    del_me = []
    for k, v in table.iteritems():
        if not len(k) and len(v) and v[0] is None:
            # this is an artifact of a trailing delimiter
            del_me.append(k)
        try:
            table[k] = np.array(v, dtype=int)
        except ValueError:
            try:
                table[k] = np.array(v, dtype=float)
            except ValueError:
                # we tried ...
                pass
        except TypeError:
            # tolerate any unexpected types and keep them as is
            pass
    for d in del_me:
        # delete artifacts
        del table[d]
    return table

def get_crossvalidation_instance(learner, partitioner, errorfx,
                                 sampling_repetitions=1,
                                 learner_space='targets',
                                 balance_training=None,
                                 permutations=0,
                                 avg_datafold_results=True,
                                 prob_tail='left'):
    from mvpa2.base.node import ChainNode
    from mvpa2.measures.base import CrossValidation
    if not balance_training is None:
        # balance training data
        try:
            amount = int(balance_training)
        except ValueError:
            try:
                amount = float(balance_training)
            except ValueError:
                amount = balance_training
        from mvpa2.generators.resampling import Balancer
        balancer = Balancer(amount=amount, attr=learner_space,
                            count=sampling_repetitions,
                            limit={partitioner.get_space(): 1},
                            apply_selection=True,
                            include_offlimit=True)
    else:
        balancer = None
    # set learner space
    learner.set_space(learner_space)
    # setup generator for data folding -- put in a chain node for easy
    # amending
    gennode = ChainNode([partitioner], space=partitioner.get_space())
    if avg_datafold_results:
        from mvpa2.mappers.fx import mean_sample
        postproc = mean_sample()
    else:
        postproc = None
    if not balancer is None:
        # enable balancing step for each partitioning step
        gennode.append(balancer)
    if permutations > 0:
        from mvpa2.generators.base import Repeater
        from mvpa2.generators.permutation import AttributePermutator
        from mvpa2.clfs.stats import MCNullDist
        # how often do we want to shuffle the data
        repeater = Repeater(count=permutations)
        # permute the training part of a dataset exactly ONCE
        permutator = AttributePermutator(
                        learner_space,
                        limit={partitioner.get_space(): 1},
                        count=1)
        # CV with null-distribution estimation that permutes the training data for
        # each fold independently
        perm_gen_node = copy.deepcopy(gennode)
        perm_gen_node.append(permutator)
        null_cv = CrossValidation(learner,
                                  perm_gen_node,
                                  postproc=postproc,
                                  errorfx=errorfx)
        # Monte Carlo distribution estimator
        distr_est = MCNullDist(repeater,
                               tail=prob_tail,
                               measure=null_cv,
                               enable_ca=['dist_samples'])
        # pass the p-values as feature attributes on to the results
        pass_attr = [('ca.null_prob', 'fa', 1)]
    else:
        distr_est = None
        pass_attr = None
    # final CV node
    cv = CrossValidation(learner,
                         gennode,
                         errorfx=errorfx,
                         null_dist=distr_est,
                         postproc=postproc,
                         enable_ca=['stats', 'null_prob'],
                         pass_attr=pass_attr)
    return cv



########################
#
# common arguments
#
########################
# argument spec template
#<name> = (
#    <id_as_positional>, <id_as_option>
#    {<ArgusmentParser.add_arguments_kwargs>}
#)

help = (
    'help', ('-h', '--help', '--help-np'),
    dict(nargs=0, action=HelpAction,
         help="""show this help message and exit. --help-np forcefully disables
                 the use of a pager for displaying the help.""")
)

version = (
    'version', ('--version',),
    dict(action='version',
         help="show program's version and license information and exit")
)

multidata = (
    'data', ('-i', '--input'),
    {'nargs': '+',
     'dest': 'data',
     'metavar': 'DATASET',
     'help': """path(s) to one or more PyMVPA dataset files. All datasets
             will be merged into a single dataset (vstack'ed) in order of
             specification. In some cases this option may need to be specified
             more than once if multiple, but separate, input datasets are
             required."""
    }
)

multimask = (
    'masks', ('-m', '--masks'),
    {'nargs': '+'}
)

mask = (
    'mask', ('-m', '--mask'),
    {'help': 'single mask item'}
)

output_file = (
    'output', ('-o', '--output'),
    dict(type=str,
         help="""output filename ('.hdf5' extension is added automatically
        if necessary).""")
)

output_prefix = (
    'outprefix', ('-o', '--output-prefix'),
    {'type': str,
     'metavar': 'PREFIX',
     'help': 'prefix for all output file'
    }
)

learner_opt = (
    'learner', ('--learner',),
    {'type': arg2learner,
     'help': """select a learner (trainable node) via its description in the
             learner warehouse (see 'info' command for a listing), a
             colon-separated list of capabilities, or by a file path to a Python
             script that creates a classifier instance (advanced)."""
    }
)

learner_space_opt = (
    'learnerspace', ('--learner-space',),
    {'type': str, 'default': 'targets',
     'help': """name of a sample attribute that defines the model to be
             learned by a learner. By default this is an attribute named
             'targets'."""
    }
)

partitioner_opt = (
    'partitioner', ('--partitioner',),
    {'type': arg2partitioner,
     'help': """select a data folding scheme. Supported arguments are: 'half'
             for split-half partitioning, 'oddeven' for partitioning into odd
             and even chunks, 'group-X' where X can be any positive integer for
             partitioning in X groups, 'n-X' where X can be any positive
             integer for leave-X-chunks out partitioning. By default
             partitioners operate on dataset chunks that are defined by a
             'chunks' sample attribute. The name of the "chunking" attribute
             can be changed by appending a colon and the name of the attribute
             (e.g. 'oddeven:run'). optionally an argument to this option can
             also be a file path to a Python script that creates a custom
             partitioner instance (advanced)."""
    }
)

enable_ca_opt = (
    'enable_ca', ('--enable-ca',),
    {'nargs': '+', 'metavar': 'NAME',
     'help': """list of conditional attributes to be enabled"""
    }
)

disable_ca_opt = (
    'disable_ca', ('--disable-ca',),
    {'nargs': '+', 'metavar': 'NAME',
     'help': """list of conditional attributes to be disabled"""
    }
)

ca_opts_grp = ('options for conditional attributes',
        [enable_ca_opt[1:], disable_ca_opt[1:]])

hdf5compression = (
    'compression', ('--hdf5-compression',),
    dict(type=arg2hdf5compression, default=None, metavar='TYPE', help="""\
compression type for HDF5 storage. Available values depend on the specific HDF5
installation. Typical values are: 'gzip', 'lzf', 'szip', or integers from 1 to
9 indicating gzip compression levels."""))


attr_from_cmdline = ('options for attributes from the command line', [
    (('--add-sa',), dict(type=str, nargs='+', action='append', metavar='VALUE',
        help="""compose a sample attribute from the command line input.
                The first value is the desired attribute name, the second value
                is a comma-separated list (appropriately quoted) of actual
                attribute values. An optional third value can be given to
                specify a data type.
                Additional information on defining dataset attributes on the
                command line are given in the section "Compose attributes
                on the command line.""")),
    (('--add-fa',), dict(type=str, nargs='+', action='append', metavar='VALUE',
        help="""compose a feature attribute from the command line input.
                The first value is the desired attribute name, the second value
                is a comma-separated list (appropriately quoted) of actual
                attribute values. An optional third value can be given to
                specify a data type.
                Additional information on defining dataset attributes on the
                command line are given in the section "Compose attributes
                on the command line.""")),
])

attr_from_txt = ('options for attributes from text files', [
    (('--add-sa-txt',), dict(type=str, nargs='+', action='append', metavar='VALUE',
        help="""load sample attribute from a text file. The first value
                is the desired attribute name, the second value is the filename
                the attribute will be loaded from. Additional values modifying
                the way the data is loaded are described in the section
                "Load data from text files".""")),
    (('--add-fa-txt',), dict(type=str, nargs='+', action='append', metavar='VALUE',
        help="""load feature attribute from a text file. The first value
                is the desired attribute name, the second value is the filename
                the attribute will be loaded from. Additional values modifying
                the way the data is loaded are described in the section
                "Load data from text files".""")),
    (('--add-sa-attr',), dict(type=str, metavar='FILENAME',
        help="""load sample attribute values from an legacy 'attributes file'.
                Column data is read as "literal". Only two column files
                ('targets' + 'chunks') without headers are supported. This
                option allows for reading attributes files from early PyMVPA
                versions.""")),
])

attr_from_npy = ('options for attributes from stored Numpy arrays', [
    (('--add-sa-npy',), dict(type=str, nargs='+', metavar='VALUE', action='append',
        help="""load sample attribute from a Numpy .npy file. Compressed files
             (i.e. .npy.gz) are supported as well. The first value is the
             desired attribute name, the second value is the filename
             the data will be loaded from. Additional values modifying the way
             the data is loaded are described in the section "Load data from
             Numpy NPY files".""")),
    (('--add-fa-npy',), dict(type=str, nargs='+', metavar='VALUE', action='append',
        help="""load feature attribute from a Numpy .npy file. Compressed files
             (i.e. .npy.gz) are supported as well. The first value is the
             desired attribute name, the second value is the filename
             the data will be loaded from. Additional values modifying the way
             the data is loaded are described in the section "Load data from
             Numpy NPY files".""")),
])

single_required_hdf5output = ('output options', [
    (('-o', '--output'), dict(type=str, required=True,
         help="""output filename ('.hdf5' extension is added automatically if
         necessary). NOTE: The output format is suitable for data exchange between
         PyMVPA commands, but is not recommended for long-term storage or exchange
         as its specific content may vary depending on the actual software
         environment. For long-term storage consider conversion into other data
         formats (see 'dump' command).""")),
    hdf5compression[1:],
])

crossvalidation_opts_grp = ('options for cross-validation setup', [
    learner_opt[1:], learner_space_opt[1:], partitioner_opt[1:],
    (('--errorfx',), dict(type=arg2errorfx,
        help="""error function to be applied to the targets and predictions
        of each cross-validation data fold. This can either be a name of
        any error function in PyMVPA's mvpa2.misc.errorfx module, or a file
        path to a Python script that creates a custom error function
        (advanced).""")),
    (('--avg-datafold-results',), dict(action='store_true',
        help="""average result values across data folds generated by the
        partitioner. For example to compute a mean prediction error across
        all folds of a cross-validation procedure.""")),
    (('--balance-training',), dict(type=str,
        help="""If enabled, training samples are balanced within each data fold.
        If the keyword 'equal' is given as argument an equal number of random
        samples for each unique target value is chosen. The number of samples
        per category is determined by the category with the least number of
        samples in the respective training set. An integer argument will cause
        the a corresponding number of samples per category to be randomly
        selected. A floating point number argument (interval [0,1]) indicates
        what fraction of the available samples shall be selected.""")),
    (('--sampling-repetitions',), dict(type=int, default=1,
        help="""If training set balancing is enabled, how often should random
        sample selection be performed for each data fold. Default: 1""")),
    (('--permutations',), dict(type=int, default=0,
        help="""Number of Monte-Carlo permutation runs to be computed for
        estimating an H0 distribution for all cross-validation results. Enabling
        this option will make reports of corresponding p-values available in
        the result summary and output.""")),
    (('--prob-tail',), dict(choices=('left', 'right'), default='left',
        help="""which tail of the probability distribution to report p-values
        from when evaluating permutation test results. For example, a
        cross-validation computing mean prediction error could report left-tail
        p-value for a single-sided test.""")),
])

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""PyMVPA's common Dataset container."""

__docformat__ = 'restructuredtext'

import numpy as np
import copy

from mvpa2.base import warning
from mvpa2.base.collections import SampleAttributesCollection, \
        FeatureAttributesCollection, DatasetAttributesCollection, \
        SampleAttribute, FeatureAttribute, DatasetAttribute
from mvpa2.base.dataset import AttrDataset
from mvpa2.base.dataset import _expand_attribute
from mvpa2.misc.support import idhash as idhash_
from mvpa2.mappers.base import ChainMapper
from mvpa2.featsel.base import StaticFeatureSelection
from mvpa2.mappers.flatten import mask_mapper, FlattenMapper

if __debug__:
    from mvpa2.base import debug


class Dataset(AttrDataset):
    __doc__ = AttrDataset.__doc__

    def get_mapped(self, mapper):
        """Feed this dataset through a trained mapper (forward).

        Parameters
        ----------
        mapper : Mapper
          This mapper instance has to be trained.

        Returns
        -------
        Dataset
          The forward-mapped dataset.
        """
        # if we use .forward, no postcall is called... is that
        #  desired?  doesn't seem to have major impact on unittests
        #  BUT since postcall might change dimensionality/meaning of
        #  data, it would not be any longer reversible; more over
        #  since chain of .forwards do not invoke postcalls, also
        #  forward would lead to different behavior
        #mds = mapper(self)
        mds = mapper.forward(self)
        mds._append_mapper(mapper)
        return mds


    def _append_mapper(self, mapper):
        if not 'mapper' in self.a:
            self.a['mapper'] = mapper
            return

        pmapper = self.a.mapper
        # otherwise we have a mapper already, but is it a chain?
        if not isinstance(pmapper, ChainMapper):
            self.a.mapper = ChainMapper([pmapper])

        # is a chain mapper
        # merge slicer?
        lastmapper = self.a.mapper[-1]
        if isinstance(lastmapper, StaticFeatureSelection):
            try:
                # try whether mappers can be merged
                lastmapper += mapper
            except TypeError:
                # append new one if not
                self.a.mapper.append(mapper)
        else:
            self.a.mapper.append(mapper)


    def __getitem__(self, args):
        # uniformize for checks below; it is not a tuple if just single slicing
        # spec is passed
        if not isinstance(args, tuple):
            args = (args,)

        # if we get an slicing array for feature selection and it is *not* 1D
        # try feeding it through the mapper (if there is any)
        if len(args) > 1 and isinstance(args[1], np.ndarray) \
           and len(args[1].shape) > 1 \
           and self.a.has_key('mapper'):
            args = list(args)
            args[1] = self.a.mapper.forward1(args[1])
            args = tuple(args)

        # let the base do the work
        ds = super(Dataset, self).__getitem__(args)

        # and adjusting the mapper (if any)
        if len(args) > 1 and 'mapper' in ds.a:
            # create matching mapper
            # the mapper is just appended to the dataset. It could also be
            # actually used to perform the slicing and prevent duplication of
            # functionality between the Dataset.__getitem__ and the mapper.
            # However, __getitem__ is sometimes more efficient, since it can
            # slice samples and feature axis at the same time. Moreover, the
            # mvpa2.base.dataset.Dataset has no clue about mappers and should
            # be fully functional without them.
            subsetmapper = StaticFeatureSelection(args[1],
                                              dshape=self.samples.shape[1:])
            # do not-act forward mapping to charge the output shape of the
            # slice mapper without having it to train on a full dataset (which
            # is most likely more expensive)
            subsetmapper.forward(np.zeros((1,) + self.shape[1:], dtype='bool'))
            # mapper is ready to use -- simply store
            ds._append_mapper(subsetmapper)

        return ds


    def find_collection(self, attr):
        """Lookup collection that contains an attribute of a given name.

        Collections are search in the following order: sample attributes,
        feature attributes, dataset attributes. The first collection
        containing a matching attribute is returned.

        Parameters
        ----------
        attr : str
          Attribute name to be looked up.

        Returns
        -------
        Collection
          If not matching collection is found a LookupError exception is raised.
        """
        if attr in self.sa:
            col = self.sa
            if __debug__ and (attr in self.fa or attr in self.a):
                warning("An attribute with name '%s' is also present "
                        "in another attribute collection (fa=%s, a=%s) -- make "
                        "sure that you got the right one (see ``col`` "
                        "argument)." % (attr, attr in self.fa, attr in self.a))
        elif attr in self.fa:
            col = self.fa
            if __debug__ and attr in self.a:
                warning("An attribute with name '%s' is also present "
                        "in the dataset attribute collection -- make sure "
                        "that you got the right one (see ``col`` argument)."
                        % (attr,))
        elif attr in self.a:
            col = self.a
            # we don't need to warn here, since it wouldn't happen
        else:
            raise LookupError("Cannot find '%s' attribute in any dataset "
                              "collection." % attr)
        return col


    def _collection_id2obj(self, col):
        if col == 'sa':
            col = self.sa
        elif col == 'fa':
            col = self.fa
        elif col == 'a':
            col = self.a
        else:
            raise LookupError("Unknown collection '%s'. Possible values "
                              "are: 'sa', 'fa', 'a'." % col)
        return col


    def set_attr(self, name, value):
        """Set an attribute in a collection.

        Parameters
        ----------
        name : str
          Collection and attribute name. This has to be in the same format as
          for ``get_attr()``.
        value : array
          Value of the attribute.
        """
        if '.' in name:
            col, name = name.split('.')[0:2]
            # translate collection names into collection
            col = self._collection_id2obj(col)
        else:
            # auto-detect collection
            col = self.find_collection(name)

        col[name] = value


    def get_attr(self, name):
        """Return an attribute from a collection.

        A collection can be specified, but can also be auto-detected.

        Parameters
        ----------
        name : str
          Attribute name. The attribute name can also be prefixed with any valid
          collection name ('sa', 'fa', or 'a') separated with a '.', e.g.
          'sa.targets'. If no collection prefix is found auto-detection of the
          collection is attempted.

        Returns
        -------
        (attr, collection)
          2-tuple: First element is the requested attribute and the second
          element is the collection that contains the attribute. If no matching
          attribute can be found a LookupError exception is raised.
        """
        if '.' in name:
            col, name = name.split('.')[0:2]
            # translate collection names into collection
            col = self._collection_id2obj(col)
        else:
            # auto-detect collection
            col = self.find_collection(name)

        return (col[name], col)


    def item(self):
        """Provide the first element of samples array.

        Notes
        -----
        Introduced to provide compatibility with `numpy.asscalar`.
        See `numpy.ndarray.item` for more information.
        """
        return self.samples.item()


    @property
    def idhash(self):
        """To verify if dataset is in the same state as when smth else was done

        Like if classifier was trained on the same dataset as in question
        """

        res = 'self@%s samples@%s' % (idhash_(self), idhash_(self.samples))

        for col in (self.a, self.sa, self.fa):
            # We cannot count on the order the values in the dict will show up
            # with `self._data.value()` and since idhash will be order-dependent
            # we have to make it deterministic
            keys = col.keys()
            keys.sort()
            for k in keys:
                res += ' %s@%s' % (k, idhash_(col[k].value))
        return res


    @classmethod
    def from_wizard(cls, samples, targets=None, chunks=None, mask=None,
                    mapper=None, flatten=None, space=None):
        """Convenience method to create dataset.

        Datasets can be created from N-dimensional samples. Data arrays with
        more than two dimensions are going to be flattened, while preserving
        the first axis (separating the samples) and concatenating all other as
        the second axis. Optionally, it is possible to specify targets and
        chunk attributes for all samples, and masking of the input data (only
        selecting elements corresponding to non-zero mask elements

        Parameters
        ----------
        samples : ndarray
          N-dimensional samples array. The first axis separates individual
          samples.
        targets : scalar or ndarray, optional
          Labels for all samples. If a scalar is provided its values is assigned
          as label to all samples.
        chunks : scalar or ndarray, optional
          Chunks definition for all samples. If a scalar is provided its values
          is assigned as chunk of all samples.
        mask : ndarray, optional
          The shape of the array has to correspond to the shape of a single
          sample (shape(samples)[1:] == shape(mask)). Its non-zero elements
          are used to mask the input data.
        mapper : Mapper instance, optional
          A trained mapper instance that is used to forward-map
          possibly already flattened (see flatten) and masked samples
          upon construction of the dataset. The mapper must have a
          simple feature space (samples x features) as output. Use a
          `ChainMapper` to achieve that, if necessary.
        flatten : None or bool, optional
          If None (default) and no mapper provided, data would get flattened.
          Bool value would instruct explicitly either to flatten before
          possibly passing into the mapper if no mask is given.
        space : str, optional
          If provided it is assigned to the mapper instance that performs the
          initial flattening of the data.

        Returns
        -------
        instance : Dataset
        """
        # for all non-ndarray samples you need to go with the constructor
        samples = np.asanyarray(samples)

        # compile the necessary samples attributes collection
        sa_items = {}

        if not targets is None:
            sa_items['targets'] = _expand_attribute(targets,
                                                   samples.shape[0],
                                                  'targets')

        if not chunks is None:
            # unlike previous implementation, we do not do magic to do chunks
            # if there are none, there are none
            sa_items['chunks'] = _expand_attribute(chunks,
                                                   samples.shape[0],
                                                   'chunks')

        # common checks should go into __init__
        ds = cls(samples, sa=sa_items)
        # apply mask through mapper
        if mask is None:
            # if we have multi-dim data
            if len(samples.shape) > 2 and \
                   ((flatten is None and mapper is None) # auto case
                    or flatten):                         # bool case
                fm = FlattenMapper(shape=samples.shape[1:], space=space)
                ds = ds.get_mapped(fm)
        else:
            mm = mask_mapper(mask, space=space)
            mm.train(ds)
            ds = ds.get_mapped(mm)

        # apply generic mapper
        if not mapper is None:
            ds = ds.get_mapped(mapper)
        return ds


    @classmethod
    def from_channeltimeseries(cls, samples, targets=None, chunks=None,
                               t0=None, dt=None, channelids=None):
        """Create a dataset from segmented, per-channel timeseries.

        Channels are assumes to contain multiple, equally spaced acquisition
        timepoints. The dataset will contain additional feature attributes
        associating each feature with a specific `channel` and `timepoint`.

        Parameters
        ----------
        samples : ndarray
          Three-dimensional array: (samples x channels x timepoints).
        t0 : float
          Reference time of the first timepoint. Can be used to preserve
          information about the onset of some stimulation. Preferably in
          seconds.
        dt : float
          Temporal distance between two timepoints. Preferably in seconds.
        channelids : list
          List of channel names.
        targets, chunks
          See `Dataset.from_wizard` for documentation about these arguments.
        """
        # check samples
        if len(samples.shape) != 3:
            raise ValueError(
                "Input data should be (samples x channels x timepoints. Got: %s"
                % samples.shape)

        if not t0 is None and not dt is None:
            timepoints = np.arange(t0, t0 + samples.shape[2] * dt, dt)
            # broadcast over all channels
            timepoints = np.vstack([timepoints] * samples.shape[1])
        else:
            timepoints = None

        if not channelids is None:
            if len(channelids) != samples.shape[1]:
                raise ValueError(
                    "Number of channel ids does not match channels in the "
                    "sample data. Expected %i, but got %i"
                    % (samples.shape[1], len(channelids)))
            # broadcast over all timepoints
            channelids = np.dstack([channelids] * samples.shape[2])[0]

        ds = cls.from_wizard(samples, targets=targets, chunks=chunks)

        # add additional attributes
        if not timepoints is None:
            ds.fa['timepoints'] = ds.a.mapper.forward1(timepoints)
        if not channelids is None:
            ds.fa['channels'] = ds.a.mapper.forward1(channelids)

        return ds


    # shortcut properties
    S = property(fget=lambda self:self.samples)
    targets = property(fget=lambda self:self.sa.targets,
                      fset=lambda self, v:self.sa.__setattr__('targets', v))
    uniquetargets = property(fget=lambda self:self.sa['targets'].unique)

    T = targets
    UT = property(fget=lambda self:self.sa['targets'].unique)
    chunks = property(fget=lambda self:self.sa.chunks,
                      fset=lambda self, v:self.sa.__setattr__('chunks', v))
    uniquechunks = property(fget=lambda self:self.sa['chunks'].unique)
    C = chunks
    UC = property(fget=lambda self:self.sa['chunks'].unique)
    mapper = property(fget=lambda self:self.a.mapper)
    O = property(fget=lambda self:self.a.mapper.reverse(self.samples))


# convenience alias
dataset_wizard = Dataset.from_wizard


class HollowSamples(object):
    """Samples container that doesn't store samples.

    The purpose of this class is to provide an object that can be used as
    ``samples`` in a Dataset, without having actual samples. Instead of storing
    multiple samples it only maintains a IDs for samples and features it
    pretends to contain.

    Using this class in a dataset in conjuction will actual attributes, will
    yield a lightweight dataset that is compatible with the majority of all
    mappers and can be used to 'simulate' processing by mappers. The class
    offers acces to the sample and feature IDs via its ``sid`` and ``fid``
    members.
    """
    def __init__(self, shape=None, sid=None, fid=None, dtype=np.float):
        """
        Parameters
        ----------
        shape : 2-tuple or None
          Shape of the pretend-sample array (nsamples x nfeatures). Can be
          left out if both ``sid`` and ``fid`` are provided.
        sid : 1d-array or None
          Vector of sample IDs. Can be left out if ``shape`` is provided.
        fid : 1d-array or None
          Vector of feature IDs. Can be left out if ``shape`` is provided.
        dtype : type or str
          Pretend-datatype of the non-existing samples.
        """
        if shape is None and sid is None and fid is None:
            raise ValueError("Either shape or ID vectors have to be given")
        if not shape is None and not len(shape) == 2:
            raise ValueError("Only two-dimensional shapes are supported")
        if sid is None:
            self.sid = np.arange(shape[0], dtype='uint')
        else:
            self.sid = sid
        if fid is None:
            self.fid = np.arange(shape[1], dtype='uint')
        else:
            self.fid = fid
        self.dtype = dtype
        # sanity check
        if not shape is None and not len(self.sid) == shape[0] \
                and not len(self.fid) == shape[1]:
            raise ValueError("Provided ID vectors do not match given `shape`")


    def __reduce__(self):
        return (self.__class__,
                ((len(self.sid), len(self.fid)),
                 self.sid,
                 self.fid,
                 self.dtype))

    def copy(self, deep=True):
        return deep and copy.deepcopy(self) or copy.copy(self)

    @property
    def shape(self):
        return (len(self.sid), len(self.fid))


    @property
    def samples(self):
        return np.zeros((len(self.sid), len(self.fid)), dtype=self.dtype)


    def __array__(self, dtype=None):
        # come up with a fake array of proper dtype
        return np.zeros((len(self.sid), len(self.fid)), dtype=self.dtype)


    def __getitem__(self, args):
        if not isinstance(args, tuple):
            args = (args,)

        if len(args) > 2:
            raise ValueError("Too many arguments (%i). At most there can be "
                             "two arguments, one for samples selection and one "
                             "for features selection" % len(args))

        if len(args) == 1:
            args = [args[0], slice(None)]
        else:
            args = [a for a in args]
        # ints need to become lists to prevent silent dimensionality changes
        # of the arrays when slicing
        for i, a in enumerate(args):
            if isinstance(a, int):
                args[i] = [a]
        # apply to vectors
        sid = self.sid[args[0]]
        fid = self.fid[args[1]]

        return HollowSamples((len(sid), len(fid)), sid=sid, fid=fid,
                             dtype=self.dtype)

    def view(self):
        """Return itself"""
        return self

########NEW FILE########
__FILENAME__ = channel
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Dataset handling data structured in channels."""

__docformat__ = 'restructuredtext'

#
#
# THIS CODE IS OBSOLETE!
#
# PLEASE PORT substract_baseline() AND resample() TO WORK WITH ANY DATASET.
#

from mvpa2.base import warning

warning("Deprecated: ChannelDataset has vanished already")

if False:           # just to please Python so it could parse the file
    ##REF: Name was automagically refactored
    def substract_baseline(self, t=None):
        """Substract mean baseline signal from the each timepoint.

        The baseline is determined by computing the mean over all timepoints
        specified by `t`.

        The samples of the dataset are modified in-place and nothing is
        returned.

        Parameters
        ----------
        t : int or float or None
          If an integer, `t` denotes the number of timepoints in the from the
          start of each sample to be used to compute the baseline signal.
          If a floating point value, `t` is the duration of the baseline
          window from the start of each sample in whatever unit
          corresponding to the datasets `samplingrate`. Finally, if `None`
          the `t0` property of the dataset is used to determine `t` as it
          would have been specified as duration.
        """
        # if no baseline length is given, use t0
        if t is None:
            t = np.abs(self.t0)

        # determine length of baseline in samples
        if isinstance(t, float):
            t = np.round(t * self.samplingrate)

        # get original data
        data = self.O

        # compute baseline
        # XXX: shouldn't this be done per chunk?
        baseline = np.mean(data[:, :, :t], axis=2)
        # remove baseline
        data -= baseline[..., np.newaxis]

        # put data back into dataset
        self.samples[:] = self.mapForward(data)

########NEW FILE########
__FILENAME__ = eeglab
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support for EEGLAB's electrode-time series text file format.

This module offers functions to import data from EEGLAB_ text files.

.. _EEGLAB: http://sccn.ucsd.edu/eeglab/
"""

__docformat__ = 'restructuredtext'

import numpy as np
import os

from mvpa2.datasets.base import Dataset
from mvpa2.mappers.flatten import FlattenMapper

# restrict public interface to not misguide sphinx
__all__ = [ 'eeglab_dataset' ]

def _looks_like_filename(s):
    if os.path.exists(s):
        return True
    return len(s) <= 256 and not '\n' in s

def eeglab_dataset(samples):
    '''Make a Dataset instance from EEGLAB input data

    Parameters
    ----------
    samples: str
        Filename of EEGLAB text file

    Returns
    -------
    ds: mvpa2.base.dataset.Dataset
        Dataset with the contents of the input file
    '''
    if not isinstance(samples, basestring):
        raise ValueError("Samples should be a string")

    if _looks_like_filename(samples):
        if not os.path.exists(samples):
            raise ValueError("Input looks like a filename, but file"
                                " %s does not exist" % samples)
        with open(samples) as f:
            samples = f.read()

    lines = samples.split('\n')
    samples = []
    cur_sample = None

    for i, line in enumerate(lines):
        if not line:
            continue
        if i == 0:
            # first line contains the channel names
            channel_labels = line.split()
            n_channels = len(channel_labels)
        else:
            # first value is the time point, the remainders the value 
            # for each channel
            values = map(float, line.split())
            t = values[0]  # time 
            eeg = values[1:] # values for each electrode

            if len(eeg) != n_channels:
                raise ValueError("Line %d: expected %d values but found %d" %
                                    (n_channels, len(eeg)))

            if cur_sample is None or t < prev_t:
                # new sample
                cur_sample = []
                samples.append(cur_sample)

            cur_sample.append((t, eeg))
            prev_t = t

    # get and verify number of elements in each dimension
    n_samples = len(samples)
    n_timepoints_all = map(len, samples)

    n_timepoints_unique = set(n_timepoints_all)
    if len(n_timepoints_unique) != 1:
        raise ValueError("Different number of time points in different"
                            "samples: found %d different lengths" %
                            len(n_timepoints_unique))

    n_timepoints = n_timepoints_all[0]

    shape = (n_samples, n_timepoints, n_channels)

    # allocate space for data
    data = np.zeros(shape)

    # make a list of all channels and timepoints
    channel_array = np.asarray(channel_labels)
    timepoint_array = np.asarray([samples[0][i][0]
                                  for i in xrange(n_timepoints)])

    dts = timepoint_array[1:] - timepoint_array[:-1]
    if not np.all(dts == dts[0]):
        raise ValueError("Delta time points are different")

    # put the values in the data array
    for i, sample in enumerate(samples):
        for j, (t, values) in enumerate(sample):
            # check that the time is the same
            if i > 0 and timepoint_array[j] != t:
                raise ValueError("Sample %d, time point %s is different "
                                 "than the first sample (%s)" %
                                 (i, t, timepoint_array[j]))

            for k, value in enumerate(values):
                data[i, j, k] = value

    samples = None # and let gc do it's job

    # make a Dataset instance with the data
    ds = Dataset(data)

    # append a flatten_mapper to go from 3D (sample X time X channel)
    # to 2D (sample X (time X channel))
    flatten_mapper = FlattenMapper(shape=shape[1:], space='time_channel_indices')
    ds = ds.get_mapped(flatten_mapper)

    # make this a 3D array of the proper size
    channel_array_3D = np.tile(channel_array, (1, n_timepoints, 1))
    timepoint_array_3D = np.tile(np.reshape(timepoint_array, (-1, 1)),
                                            (1, 1, n_channels))

    # for consistency use the flattan_mapper defined above to 
    # flatten channel and timepoint names as well
    ds.fa['channelids'] = flatten_mapper.forward(channel_array_3D).ravel()
    ds.fa['timepoints'] = flatten_mapper.forward(timepoint_array_3D).ravel()

    # make some dynamic properties
    # XXX at the moment we don't have propert 'protection' in case
    # the feature space is sliced in a way so that some channels and/or
    # timepoints occur more often than others 
    _eeglab_set_attributes(ds)

    return ds

def _eeglab_set_attributes(ds):
    setattr(ds.__class__, 'nchannels', property(
            fget=lambda self: len(set(self.fa['time_channel_indices'][:, 1]))))
    setattr(ds.__class__, 'ntimepoints', property(
            fget=lambda self: len(set(self.fa['time_channel_indices'][:, 0]))))

    setattr(ds.__class__, 'channelids', property(
            fget=lambda self: np.unique(self.fa['channelids'].value)))
    setattr(ds.__class__, 'timepoints', property(
            fget=lambda self: np.unique(self.fa['timepoints'].value)))


    setattr(ds.__class__, 't0', property(
                    fget=lambda self: np.min(self.fa['timepoints'].value)))

    def _get_dt(ds):
        ts = np.unique(ds.fa['timepoints'].value)
        if len(ts) >= 1:
            delta = ts[1:] - ts[:-1]
            if len(np.unique(delta)) == 1:
                return delta[0]
        return float(numpy.nan)

    setattr(ds.__class__, 'dt', property(fget=lambda self: _get_dt(self)))

    def selector(f, xs):
        if type(f) in (list, tuple):
            flist = f
            f = lambda x:x in flist

        return np.nonzero(map(f, xs))[0]

    # attributes for getting certain time points or channels ids
    # the argument f should be either be a function, or a list or tuple
    setattr(ds.__class__, 'get_features_by_timepoints', property(
                            fget=lambda self: lambda f: selector(f,
                                            self.fa['timepoints']),
                            doc='Given a filter function f returns the '
                                'indices of features for which f(x) holds '
                                ' for each x in timepoints'))
    setattr(ds.__class__, 'get_features_by_channelids', property(
                            fget=lambda self: lambda f: selector(f,
                                            self.fa['channelids']),
                            doc='Given a filter function f returns the '
                                'indices of features for which f(x) holds '
                                ' for each x in channelids'))

########NEW FILE########
__FILENAME__ = eep
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support for the binary EEP file format for EEG data"""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.datasets import Dataset
from mvpa2.misc.io import DataReader

def eep_dataset(samples, targets=None, chunks=None):
    """Create a dataset using an EEP binary file as source.

    EEP files are used by *eeprobe* a software for analysing even-related
    potentials (ERP), which was developed at the Max-Planck Institute for
    Cognitive Neuroscience in Leipzig, Germany.

      http://www.ant-neuro.com/products/eeprobe

    Parameters
    ----------
    samples : str or EEPBin instance
      This is either a filename of an EEP file, or an EEPBin instance, providing
      the samples data in EEP format.
    targets, chunks : sequence or scalar or None
      Values are pass through to `Dataset.from_wizard()`. See its documentation
      for more information.

    Returns
    -------
    Dataset
      Besides is usual attributes (e.g. targets, chunks, and a mapper). The
      returned dataset also includes feature attributes associating each same
      with a channel (by id), and a specific timepoint -- based on information
      read from the EEP data.
    """
    if isinstance(samples, str):
        # open the eep file
        eb = EEPBin(samples)
    elif isinstance(samples, EEPBin):
        # nothing special
        eb = samples
    else:
        raise ValueError("eep_dataset takes the filename of an "
              "EEP file or a EEPBin object as 'samples' argument.")

    # init dataset
    ds = Dataset.from_channeltimeseries(
            eb.data, targets=targets, chunks=chunks, t0=eb.t0, dt=eb.dt,
            channelids=eb.channels)
    return ds



class EEPBin(DataReader):
    """Read-access to binary EEP files.

    EEP files are used by *eeprobe* a software for analysing even-related
    potentials (ERP), which was developed at the Max-Planck Institute for
    Cognitive Neuroscience in Leipzig, Germany.

      http://www.ant-neuro.com/products/eeprobe

    EEP files consist of a plain text header and a binary data block in a
    single file. The header starts with a line of the form

    ';%d %d %d %g %g' % (Nchannels, Nsamples, Ntrials, t0, dt)

    where Nchannels, Nsamples, Ntrials are the numbers of channels, samples
    per trial and trials respectively. t0 is the time of the first sample
    of a trial relative to the stimulus onset and dt is the sampling interval.

    The binary data block consists of single precision floats arranged in the
    following way::

        <trial1,channel1,sample1>,<trial1,channel1,sample2>,...
        <trial1,channel2,sample1>,<trial1,channel2,sample2>,...
        .
        <trial2,channel1,sample1>,<trial2,channel1,sample2>,...
        <trial2,channel2,sample1>,<trial2,channel2,sample2>,...
    """
    def __init__(self, source):
        """Read EEP file and store header and data.

        Parameters
        ----------
        source : str
          Filename.
        """
        # init base class
        DataReader.__init__(self)
        # temp storage of number of samples
        nsamples = None
        # non-critical header components stored in temp dict
        hdr = {}

        infile = open(source, "rb")

        # read file the end of header of EOF
        while True:
            # one line at a time
            try:
                line = infile.readline().decode('ascii')
            except UnicodeDecodeError:
                break

            # stop if EOH or EOF
            if not line or line.startswith(';EOH;'):
                break

            # no crap!
            line = line.strip()

            # all but first line as colon
            if not line.count(':'):
                # top header
                l = line.split()
                # extract critical information
                self._props['nchannels'] = int(l[0][1:])
                self._props['ntimepoints'] = int(l[1])
                self._props['t0'] = float(l[3])
                self._props['dt'] = float(l[4])
                nsamples = int(l[2])
            else:
                # simply store non-critical extras
                l = line.split(':')
                key = l[0].lstrip(';')
                value = ':'.join(l[1:])
                hdr[key] = value

        # post process channel name info -> list
        if hdr.has_key('channels'):
            self._props['channels'] = hdr['channels'].split()

        self._data = \
            np.reshape(np.fromfile(infile, dtype='f'), \
                (nsamples,
                 self._props['nchannels'],
                 self._props['ntimepoints']))

        # cleanup
        infile.close()


    nchannels = property(fget=lambda self: self._props['nchannels'],
                         doc="Number of channels")
    ntimepoints  = property(fget=lambda self: self._props['ntimepoints'],
                         doc="Number of data timepoints")
    nsamples   = property(fget=lambda self: self._data.shape[0],
                         doc="Number of trials/samples")
    t0        = property(fget=lambda self: self._props['t0'],
                         doc="Relative start time of sampling interval")
    dt        = property(fget=lambda self: self._props['dt'],
                         doc="Time difference between two adjacent samples")
    channels  = property(fget=lambda self: self._props['channels'],
                         doc="List of channel names")

########NEW FILE########
__FILENAME__ = eventrelated
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Functions for event segmentation or modeling of dataset."""

__docformat__ = 'restructuredtext'

import copy
import numpy as np
from mvpa2.misc.support import Event, value2idx
from mvpa2.datasets import Dataset
from mvpa2.base.dataset import _expand_attribute
from mvpa2.mappers.fx import _uniquemerge2literal
from mvpa2.mappers.flatten import FlattenMapper
from mvpa2.mappers.boxcar import BoxcarMapper
from mvpa2.base import warning, externals


def find_events(**kwargs):
    """Detect changes in multiple synchronous sequences.

    Multiple sequence arguments are scanned for changes in the unique value
    combination at corresponding locations. Each change in the combination is
    taken as a new event onset.  The length of an event is determined by the
    number of identical consecutive combinations.

    Parameters
    ----------
    **kwargs : sequences
      Arbitrary number of sequences that shall be scanned.

    Returns
    -------
    list
      Detected events, where each event is a dictionary with the unique
      combination of values stored under their original name. In addition, the
      dictionary also contains the ``onset`` of the event (as index in the
      sequence), as well as the ``duration`` (as number of identical
      consecutive items).

    See Also
    --------
    eventrelated_dataset : event-related segmentation of a dataset

    Examples
    --------
    >>> seq1 = ['one', 'one', 'two', 'two']
    >>> seq2 = [1, 1, 1, 2]
    >>> events = find_events(targets=seq1, chunks=seq2)
    >>> for e in events:
    ...     print e
    {'chunks': 1, 'duration': 2, 'onset': 0, 'targets': 'one'}
    {'chunks': 1, 'duration': 1, 'onset': 2, 'targets': 'two'}
    {'chunks': 2, 'duration': 1, 'onset': 3, 'targets': 'two'}
    """
    def _build_event(onset, duration, combo):
        ev = Event(onset=onset, duration=duration, **combo)
        return ev

    events = []
    prev_onset = 0
    old_combo = None
    duration = 1
    # over all samples
    for r in xrange(len(kwargs.values()[0])):
        # current attribute combination
        combo = dict([(k, v[r]) for k, v in kwargs.iteritems()])

        # check if things changed
        if not combo == old_combo:
            # did we ever had an event
            if not old_combo is None:
                events.append(_build_event(prev_onset, duration, old_combo))
                # reset duration for next event
                duration = 1
                # store the current samples as onset for the next event
                prev_onset = r

            # update the reference combination
            old_combo = combo
        else:
            # current event is lasting
            duration += 1

    # push the last event in the pipeline
    if not old_combo is None:
        events.append(_build_event(prev_onset, duration, old_combo))

    return events

def _events2dict(events):
    evvars = {}
    for k in events[0]:
        try:
            evvars[k] = [e[k] for e in events]
        except KeyError:
            raise ValueError("Each event property must be present for all "
                             "events (could not find '%s')" % k)
    return evvars

def _evvars2ds(ds, evvars, eprefix):
    for a in evvars:
        if not eprefix is None and a in ds.sa:
            # if there is already a samples attribute like this, it got mapped
            # previously (e.g. by BoxcarMapper and is multi-dimensional).
            # We move it aside under new `eprefix` name
            ds.sa[eprefix + '_' + a] = ds.sa[a]
        ds.sa[a] = evvars[a]
    return ds


def _extract_boxcar_events(
        ds, events=None, time_attr=None, match='prev',
        eprefix='event', event_mapper=None):
    """see eventrelated_dataset() for docs"""
    # relabel argument
    conv_strategy = {'prev': 'floor',
                     'next': 'ceil',
                     'closest': 'round'}[match]

    if not time_attr is None:
        tvec = ds.sa[time_attr].value
        # we are asked to convert onset time into sample ids
        descr_events = []
        for ev in events:
            # do not mess with the input data
            ev = copy.deepcopy(ev)
            # best matching sample
            idx = value2idx(ev['onset'], tvec, conv_strategy)
            # store offset of sample time and real onset
            ev['orig_offset'] = ev['onset'] - tvec[idx]
            # rescue the real onset into a new attribute
            ev['orig_onset'] = ev['onset']
            ev['orig_duration'] = ev['duration']
            # figure out how many samples we need
            ev['duration'] = \
                    len(tvec[idx:][tvec[idx:] < ev['onset'] + ev['duration']])
            # new onset is sample index
            ev['onset'] = idx
            descr_events.append(ev)
    else:
        descr_events = events
    # convert the event specs into the format expected by BoxcarMapper
    # take the first event as an example of contained keys
    evvars = _events2dict(descr_events)
    # checks
    for p in ['onset', 'duration']:
        if not p in evvars:
            raise ValueError("'%s' is a required property for all events."
                             % p)
    boxlength = max(evvars['duration'])
    if __debug__:
        if not max(evvars['duration']) == min(evvars['duration']):
            warning('Boxcar mapper will use maximum boxlength (%i) of all '
                    'provided Events.'% boxlength)

    # finally create, train und use the boxcar mapper
    bcm = BoxcarMapper(evvars['onset'], boxlength, space=eprefix)
    bcm.train(ds)
    ds = ds.get_mapped(bcm)
    if event_mapper is None:
        # at last reflatten the dataset
        # could we add some meaningful attribute during this mapping, i.e. would
        # assigning 'inspace' do something good?
        ds = ds.get_mapped(FlattenMapper(shape=ds.samples.shape[1:]))
    else:
        ds = ds.get_mapped(event_mapper)
    # add samples attributes for the events, simply dump everything as a samples
    # attribute
    # special case onset and duration in case of conversion into descrete time
    if not time_attr is None:
        for attr in ('onset', 'duration'):
            evvars[attr] = [e[attr] for e in events]
    ds = _evvars2ds(ds, evvars, eprefix)

    return ds


def _fit_hrf_event_model(
        ds, events, time_attr, condition_attr='targets', design_kwargs=None,
        glmfit_kwargs=None, regr_attrs=None):
    if externals.exists('nipy', raise_=True):
        from nipy.modalities.fmri.design_matrix import make_dmtx
        from mvpa2.mappers.glm import NiPyGLMMapper

    # Decide/device condition attribute on which GLM will actually be done
    if isinstance(condition_attr, basestring):
        # must be a list/tuple/array for the logic below
        condition_attr = [condition_attr]

    glm_condition_attr = 'regressor_names' # actual regressors
    glm_condition_attr_map = dict([(con, dict()) for con in condition_attr])    #
    # to map back to original conditions
    events = copy.deepcopy(events)  # since we are modifying in place
    for event in events:
        if glm_condition_attr in event:
            raise ValueError("Event %s already has %s defined.  Should not "
                             "happen.  Choose another name if defined it"
                             % (event, glm_condition_attr))
        compound_label = event[glm_condition_attr] = \
            'glm_label_' + '+'.join(
                str(event[con]) for con in condition_attr)
        # and mapping back to original values, without str()
        # for each condition:
        for con in condition_attr:
            glm_condition_attr_map[con][compound_label] = event[con]

    evvars = _events2dict(events)
    add_paradigm_kwargs = {}
    if 'amplitude' in evvars:
        add_paradigm_kwargs['amplitude'] = evvars['amplitude']
    # create paradigm
    if 'duration' in evvars:
        from nipy.modalities.fmri.experimental_paradigm import BlockParadigm
        # NiPy considers everything with a duration as a block paradigm
        paradigm = BlockParadigm(
                        con_id=evvars[glm_condition_attr],
                        onset=evvars['onset'],
                        duration=evvars['duration'],
                        **add_paradigm_kwargs)
    else:
        from nipy.modalities.fmri.experimental_paradigm \
                import EventRelatedParadigm
        paradigm = EventRelatedParadigm(
                        con_id=evvars[glm_condition_attr],
                        onset=evvars['onset'],
                        **add_paradigm_kwargs)
    # create design matrix -- all kinds of fancy additional regr can be
    # auto-generated
    if design_kwargs is None:
        design_kwargs = {}
    if not regr_attrs is None:
        names = []
        regrs = []
        for attr in regr_attrs:
            names.append(attr)
            regrs.append(ds.sa[attr].value)
        if len(regrs) < 2:
            regrs = [regrs]
        regrs = np.hstack(regrs).T
        if 'add_regs' in design_kwargs:
            design_kwargs['add_regs'] = np.hstack((design_kwargs['add_regs'],
                                                   regrs))
        else:
            design_kwargs['add_regs'] = regrs
        if 'add_reg_names' in design_kwargs:
            design_kwargs['add_reg_names'].extend(names)
        else:
            design_kwargs['add_reg_names'] = names
    design_matrix = make_dmtx(ds.sa[time_attr].value,
                              paradigm,
                              **design_kwargs)

    # push design into source dataset
    glm_regs = [
        (reg, design_matrix.matrix[:, i])
        for i, reg in enumerate(design_matrix.names)]

    # GLM
    glm = NiPyGLMMapper([], glmfit_kwargs=glmfit_kwargs,
            add_regs=glm_regs,
            return_design=True, return_model=True, space=glm_condition_attr)

    model_params = glm(ds)

    # some regressors might be corresponding not to original condition_attr
    # so let's separate them out
    regressor_names = model_params.sa[glm_condition_attr].value
    condition_regressors = np.array([v in glm_condition_attr_map.values()[0]
                                     for v in regressor_names])
    assert(condition_regressors.dtype == np.bool)
    if not np.all(condition_regressors):
        # some regressors do not correspond to conditions and would need
        # to be taken into a separate dataset
        model_params.a['add_regs'] = model_params[~condition_regressors]
        # then we process the rest
        model_params = model_params[condition_regressors]
        regressor_names = model_params.sa[glm_condition_attr].value

    # now define proper condition sa's
    for con, con_map in glm_condition_attr_map.iteritems():
        model_params.sa[con] = [con_map[v] for v in regressor_names]
    model_params.sa.pop(glm_condition_attr) # remove generated one
    return model_params


def eventrelated_dataset(ds, events, time_attr=None, match='prev',
                         eprefix='event', event_mapper=None,
                         condition_attr='targets', design_kwargs=None,
                         glmfit_kwargs=None, regr_attrs=None, model='boxcar'):
    """Segment a dataset by modeling events.

    This function can be used to extract event-related samples from any
    (time-series) based dataset. The principal event modeling approaches are
    available (see ``model`` argument):

    1. Boxcar model: (multiple) consecutive samples are extracted for each
                     event, and are either returned in a flattened shape,
                     or subject to further processing.
    2. HRF model:    a univariate GLM is fitted for each feature and model
                     parameters are returned as samples. Model parameters
                     returned for each regressor in the design matrix. Using
                     NiPy design matrices can be generated with a variety of
                     customizations (HRF model, confound regressors, ...).

    Events are specified as a list of dictionaries
    (see:class:`~mvpa2.misc.support.Event`) for a helper class. Each dictionary
    contains all relevant attributes to describe an event. This is at least the
    ``onset`` time of an event, but can also comprise of ``duration``,
    ``amplitude``, and arbitrary other attributes -- depending on the selected
    event model.

    Boxcar event model details
    --------------------------

    For each event all samples covering that particular event are used to form
    a corresponding sample. One sample for each event is returned. Event
    specification dictionaries must contain an ``onset`` attribute (as sample
    index in the input dataset), ``duration`` (as number of consecutive samples
    after the onset). Any number of additional attributes can be present in an
    event specification. Those attributes are included as sample attributes in
    the returned dataset.

    Alternatively, ``onset`` and ``duration`` may also be given in a
    non-discrete time specification. In this case a dataset attribute needs to
    be specified that contains time-stamps for each input data sample, and is
    used to convert times into discrete sample indices (see ``match``
    argument).

    A mapper instance can be provided (see ``event_mapper``) to implement
    futher processing of each event sample, for example in order to yield
    average samples.

    HRF event model details
    -----------------------

    The event specifications are used to generate a design matrix for all
    present conditions. In addition to the mandatory ``onset`` information
    each event definition needs to include a label in order to associate
    individual events to conditions (the design matrix will contain at least
    one regressor for each condition). The name of this label attribute must
    be specified too (see ``condition_attr`` argument).

    NiPy is used to generate the actual design matrix.  It is required to
    specify a dataset sample attribute that contains time-stamps for all input
    data samples (see ``time_attr``).  NiPy operation could be customized (see
    ``design_kwargs`` argument). Additional regressors from sample attributes
    of the input dataset can be included in the design matrix (see ``regr_attrs``).

    The actual GLM fit is also performed by NiPy and can be fully customized
    (see ``glmfit_kwargs``).

    Parameters
    ----------
    ds : Dataset
      The samples of this input dataset have to be in whatever ascending order.
    events : list
      Each event definition has to specify ``onset`` and ``duration``. All
      other attributes will be passed on to the sample attributes collection of
      the returned dataset.
    model : {'boxcar', 'hrf'}
      Event model selection -- see documentation for details.
    time_attr : str or None
      Attribute with dataset sample time-stamps.
      For boxcar modeling, if not None, the ``onset`` and ``duration`` specs
      from the event list will be converted using information from this sample
      attribute. Its values will be treated as in-the-same-unit and are used to
      determine corresponding samples from real-value onset and duration
      definitions.
      For HRF modeling this argument is mandatory.
    match : {'prev', 'next', 'closest'}
      For boxcar modeling: strategy used to match real-value onsets to sample
      indices. 'prev' chooses the closes preceding samples, 'next' the closest
      following sample and 'closest' to absolute closest sample.
    eprefix : str or None
      For boxcar modeling: if not None, this prefix is used to name additional
      attributes generated by the underlying
      `~mvpa2.mappers.boxcar.BoxcarMapper`. If it is set to None, no additional
      attributes will be created.
    event_mapper : Mapper
      This mapper is used to forward-map the dataset containing the boxcar event
      samples. If None (default) a FlattenMapper is employed to convert
      multi-dimensional sample matrices into simple one-dimensional sample
      vectors. This option can be used to implement temporal compression, by
      e.g. averaging samples within an event boxcar using an FxMapper. Any
      mapper needs to keep the sample axis unchanged, i.e. number and order of
      samples remain the same.
    condition_attr : str
      For HRF modeling: name of the event attribute with the condition labels.
      Can be a list of those (e.g. ['targets', 'chunks'] combination of which
      would constitute a condition.
    design_kwargs : dict
      Arbitrary keyword arguments for NiPy's make_dmtx() used for design matrix
      generation. Choose HRF model, confound regressors, etc.
    glmfit_kwargs : dict
      Arbitrary keyword arguments for NiPy's GeneralLinearModel.fit() used for
      estimating model parameter. Choose fitting algorithm: OLS or AR1.
    regr_attrs : list
      List of dataset sample attribute names that shall be extracted from the
      input dataset and used as additional regressors in the design matrix.

    Returns
    -------
    Dataset
      In case of a boxcar model, the returned dataset has one sample per each
      event definition that has been passed to the function. Additional
      event attributes are included as sample attributes.

      In case of an HRF model, one sample for each regressor/condition in the
      design matrix is returned. The condition names are included as a sample
      attribute with the name specified by the ``condition_attr`` argument.
      The actual design regressors are included as ``regressors`` sample
      attribute. An instance with the fitted NiPy GLM results is included as
      a dataset attribute ``glmfit``, and can be used for computing contrasts
      subsequently.

    Examples
    --------
    The documentation also contains an :ref:`example script
    <example_eventrelated>` showing a spatio-temporal analysis of fMRI data
    that involves this function.

    >>> from mvpa2.datasets import Dataset
    >>> ds = Dataset(np.random.randn(10, 25))
    >>> events = [{'onset': 2, 'duration': 4},
    ...           {'onset': 4, 'duration': 4}]
    >>> eds = eventrelated_dataset(ds, events)
    >>> len(eds)
    2
    >>> eds.nfeatures == ds.nfeatures * 4
    True
    >>> 'mapper' in ds.a
    False
    >>> print eds.a.mapper
    <Chain: <Boxcar: bl=4>-<Flatten>>

    And now the same conversion, but with events specified as real time. This is
    on possible if the input dataset contains a sample attribute with the
    necessary information about the input samples.

    >>> ds.sa['record_time'] = np.linspace(0, 5, len(ds))
    >>> rt_events = [{'onset': 1.05, 'duration': 2.2},
    ...              {'onset': 2.3, 'duration': 2.12}]
    >>> rt_eds = eventrelated_dataset(ds, rt_events, time_attr='record_time',
    ...                               match='closest')
    >>> np.all(eds.samples == rt_eds.samples)
    True
    >>> # returned dataset e.g. has info from original samples
    >>> rt_eds.sa.record_time
    array([[ 1.11111111,  1.66666667,  2.22222222,  2.77777778],
           [ 2.22222222,  2.77777778,  3.33333333,  3.88888889]])

    And finally some simplistic HRF modeling:

    >>> ds.sa['time_coords'] = np.linspace(0, 50, len(ds))
    >>> events = [{'onset': 2, 'duration': 4, 'condition': 'one'},
    ...           {'onset': 4, 'duration': 4, 'condition': 'two'}]
    >>> hrf_estimates = eventrelated_dataset(
    ...                   ds, events,
    ...                   time_attr='time_coords',
    ...                   condition_attr='condition',
    ...                   design_kwargs=dict(drift_model='blank'),
    ...                   glmfit_kwargs=dict(model='ols'),
    ...                   model='hrf')
    >>> print hrf_estimates.sa.condition
    ['one' 'two']
    >>> print hrf_estimates.shape
    (2, 25)
    >>> len(hrf_estimates.a.model.get_mse())
    25

    Additional regressors used in GLM modeling are also available in a
    dataset attribute:

    >>> print hrf_estimates.a.add_regs.sa.regressor_names
    ['constant']
    """
    if not len(events):
        raise ValueError("no events specified")

    if model == 'boxcar':
        return _extract_boxcar_events(
                    ds, events=events, time_attr=time_attr, match=match,
                    eprefix=eprefix, event_mapper=event_mapper)
    elif model == 'hrf':
        if condition_attr is None:
            raise ValueError(
                    "missing name of event attribute with condition names")
        if time_attr is None:
            raise ValueError(
                    "missing name of attribute with sample timing information")
        return _fit_hrf_event_model(
                    ds, events=events, time_attr=time_attr,
                    condition_attr=condition_attr,
                    design_kwargs=design_kwargs, glmfit_kwargs=glmfit_kwargs,
                    regr_attrs=regr_attrs)
    else:
        raise ValueError("unknown event model '%s'" % model)

########NEW FILE########
__FILENAME__ = formats
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support for commonly used data source formats.

"""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals

import sys
import numpy as np

if __debug__:
    from mvpa2.base import debug

from mvpa2.datasets.base import Dataset
from mvpa2.base import warning
from mvpa2.misc.attrmap import AttributeMap

# LightSVM :

def to_lightsvm_format(dataset, out, targets_attr='targets',
                       domain=None, am=None):
    """Export dataset into LightSVM format

    Parameters
    ----------
    dataset : Dataset
    out
      Anything understanding .write(string), such as `File`
    targets_attr : string, optional
      Name of the samples attribute to be output
    domain : {None, 'regression', 'binary', 'multiclass'}, optional
      What domain dataset belongs to.  If `None`, it would be deduced
      depending on the datatype ('regression' if float, classification
      in case of int or string, with 'binary'/'multiclass' depending on
      the number of unique targets)
    am : `AttributeMap` or None, optional
      Which mapping to use for storing the non-conformant targets. If
      None was provided, new one would be automagically generated
      depending on the given/deduced domain.

    Returns
    -------
    am

    LightSVM format is an ASCII representation with a single sample per
    each line::

      output featureIndex:featureValue ... featureIndex:featureValue

    where ``output`` is specific for a given domain:

    regression
      float number
    binary
      integer labels from {-1, 1}
    multiclass
      integer labels from {1..ds.targets_attr.nunique}

    """
    targets_a = dataset.sa[targets_attr]
    targets = targets_a.value

    # XXX this all below
    #  * might become cleaner
    #  * might be RF to become more generic to be used may be elsewhere as well

    if domain is None:
        if targets.dtype.kind in ['S', 'U', 'i']:
            if len(targets_a.unique) == 2:
                domain = 'binary'
            else:
                domain = 'multiclass'
        else:
            domain = 'regression'

    if domain in ['multiclass', 'binary']:
        # check if labels are appropriate and provide mapping if necessary
        utargets = targets_a.unique
        if domain == 'binary' and set(utargets) != set([-1, 1]):
            # need mapping
            if len(utargets) != 2:
                raise ValueError, \
                      "We need 2 unique targets in %s of %s. Got targets " \
                      "from set %s" % (targets_attr, dataset, utargets)
            if am is None:
                am = AttributeMap(dict(zip(utargets, [-1, 1])))
            elif set(am.keys()) != set([-1, 1]):
                raise ValueError, \
                      "Provided %s doesn't map into binary " \
                      "labels -1,+1" % (am,)
        elif domain == 'multiclass' \
                 and set(utargets) != set(range(1, len(utargets)+1)):
            if am is None:
                am = AttributeMap(dict(zip(utargets,
                                           range(1, len(utargets) + 1))))
            elif set(am.keys()) != set([-1, 1]):
                raise ValueError, \
                      "Provided %s doesn't map into multiclass " \
                      "range 1..N" % (am, )

    if am is not None:
        # map the targets
        targets = am.to_numeric(targets)

    for t, s in zip(targets, dataset.samples):
        out.write(('%g %s\n'
                   % (t,
                      ' '.join(
                          '%i:%.8g' % (i, v)
                          for i,v in zip(range(1, dataset.nfeatures+1), s)))).encode('ascii'))

    out.flush()                # push it out
    return am


def from_lightsvm_format(in_, targets_attr='targets', am=None):
    """Loads dataset from a file in lightsvm format

    Parameters
    ----------
    in_
      Anything we could iterate over and obtain strings, such as `File`
    targets_attr : string, optional
      Name of the samples attribute to be used to store targets/labels
    am : `AttributeMap` or None, optional
      Which mapping to use for mapping labels back into possibly a
      literal representation.

    Returns
    -------
    dataset

    See Also
    --------
    to_lightsvm_format : conversion to lightsvm format
    """
    targets = []
    samples = []
    for l in in_:
        # we need to parse the line
        entries = l.split()
        targets += entries[:1]
        id_features = [e.split(':') for e in entries[1:]]
        f_ids = np.array([x[0] for x in id_features], dtype=int)
        f_vals = [float(x[1]) for x in id_features]
        if np.any(f_ids != np.arange(1, len(f_ids)+1)):
            raise NotImplementedError, \
                  "For now supporting only input of non-sparse " \
                  "lightsvm-formatted files. got line with feature " \
                  "ids %s " % f_ids
        samples.append(f_vals)

    # lets try to make targets of int, float, string until first
    # matching type ;)
    for t in (int, float, None):
        try:
            targets = np.array(targets, dtype=t)
            break
        except ValueError:
            pass

    if am is not None:
        targets = am.to_literal(targets)
    samples = np.array(samples)
    sa = {}
    sa[targets_attr] = targets
    ds = Dataset(samples, sa=sa)

    return ds

# CRF++ : http://crfpp.sourceforge.net/




########NEW FILE########
__FILENAME__ = miscfx
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Miscellaneous functions to perform operations on datasets.

All the functions defined in this module must accept dataset as the
first argument since they are bound to Dataset class in the trailer.
"""

__docformat__ = 'restructuredtext'

import random

import numpy as np

from mvpa2.base.dataset import datasetmethod
from mvpa2.datasets.base import Dataset
from mvpa2.base.dochelpers import table2string
from mvpa2.misc.support import get_nelements_per_value

from mvpa2.base import externals, warning
from mvpa2.base.types import is_sequence_type

if __debug__:
    from mvpa2.base import debug


@datasetmethod
##REF: Name was automagically refactored
def aggregate_features(dataset, fx=np.mean):
    """Apply a function to each row of the samples matrix of a dataset.

    The functor given as `fx` has to honour an `axis` keyword argument in the
    way that NumPy used it (e.g. NumPy.mean, var).

    Returns
    -------
     a new `Dataset` object with the aggregated feature(s).
    """
    agg = fx(dataset.samples, axis=1)

    return Dataset(samples=np.array(agg, ndmin=2).T, sa=dataset.sa)


@datasetmethod
##REF: Name was automagically refactored
def remove_invariant_features(dataset):
    """Returns a new dataset with all invariant features removed.
    """
    return dataset[:, dataset.samples.std(axis=0).nonzero()[0]]


@datasetmethod
##REF: Name was automagically refactored
def coarsen_chunks(source, nchunks=4):
    """Change chunking of the dataset

    Group chunks into groups to match desired number of chunks. Makes
    sense if originally there were no strong groupping into chunks or
    each sample was independent, thus belonged to its own chunk

    Parameters
    ----------
    source : Dataset or list of chunk ids
      dataset or list of chunk ids to operate on. If Dataset, then its chunks
      get modified
    nchunks : int
      desired number of chunks
    """

    if isinstance(source, Dataset):
        chunks = source.chunks
    else:
        chunks = source
    chunks_unique = np.unique(chunks)
    nchunks_orig = len(chunks_unique)

    if nchunks_orig < nchunks:
        raise ValueError, \
              "Original number of chunks is %d. Cannot coarse them " \
              "to get %d chunks" % (nchunks_orig, nchunks)

    # figure out number of samples per each chunk
    counts = dict(zip(chunks_unique, [ 0 ] * len(chunks_unique)))
    for c in chunks:
        counts[c] += 1

    # now we need to group chunks to get more or less equalized number
    # of samples per chunk. No sophistication is done -- just
    # consecutively group to get close to desired number of samples
    # per chunk
    avg_chunk_size = np.sum(counts.values())*1.0/nchunks
    chunks_groups = []
    cur_chunk = []
    nchunks = 0
    cur_chunk_nsamples = 0
    samples_counted = 0
    for i, c in enumerate(chunks_unique):
        cc = counts[c]

        cur_chunk += [c]
        cur_chunk_nsamples += cc

        # time to get a new chunk?
        if (samples_counted + cur_chunk_nsamples
            >= (nchunks+1)*avg_chunk_size) or i==nchunks_orig-1:
            chunks_groups.append(cur_chunk)
            samples_counted += cur_chunk_nsamples
            cur_chunk_nsamples = 0
            cur_chunk = []
            nchunks += 1

    if len(chunks_groups) != nchunks:
        warning("Apparently logic in coarseChunks is wrong. "
                "It was desired to get %d chunks, got %d"
                % (nchunks, len(chunks_groups)))

    # remap using groups
    # create dictionary
    chunks_map = {}
    for i, group in enumerate(chunks_groups):
        for c in group:
            chunks_map[c] = i

    # we always want an array!
    chunks_new = np.array([chunks_map[x] for x in chunks])

    if __debug__:
        debug("DS_", "Using dictionary %s to remap old chunks %s into new %s"
              % (chunks_map, chunks, chunks_new))

    if isinstance(source, Dataset):
        if __debug__:
            debug("DS", "Coarsing %d chunks into %d chunks for %s"
                  %(nchunks_orig, len(chunks_new), source))
        source.sa['chunks'].value = chunks_new
        return
    else:
        return chunks_new


@datasetmethod
## TODO: make more efficient and more generic (accept >=1 attrs to
##       operate on)
def get_samples_per_chunk_target(dataset,
                                 targets_attr='targets', chunks_attr='chunks'):
    """Returns an array with the number of samples per target in each chunk.

    Array shape is (chunks x targets).

    Parameters
    ----------
    dataset : Dataset
      Source dataset.
    """
    # shortcuts/local bindings
    ta = dataset.sa[targets_attr]
    ca = dataset.sa[chunks_attr]

    # unique
    ut = ta.unique
    uc = ca.unique

    # all
    ts = ta.value
    cs = ca.value

    count = np.zeros((len(uc), len(ut)), dtype='uint')

    for ic, c in enumerate(uc):
        for it, t in enumerate(ut):
            count[ic, it] = np.sum(np.logical_and(ts==t, cs==c))

    return count


@datasetmethod
def random_samples(dataset, npertarget, targets_attr='targets'):
    """Create a dataset with a random subset of samples.

    Parameters
    ----------
    dataset : Dataset
    npertarget : int or list
      If an `int` is given, the specified number of samples is randomly
      chosen from the group of samples sharing a unique target value. Total
      number of selected samples: npertarget x len(uniquetargets).
      If a `list` is given of length matching the unique target values, it
      specifies the number of samples chosen for each particular unique
      target.
    targets_attr : str, optional

    Returns
    -------
    Dataset
      A dataset instance for the chosen samples. All feature attributes and
      dataset attribute share there data with the source dataset.
    """
    satargets = dataset.sa[targets_attr]
    utargets = satargets.unique
    # if interger is given take this value for all classes
    if isinstance(npertarget, int):
        npertarget = [npertarget for i in utargets]

    sample = []
    # for each available class
    targets = satargets.value
    for i, r in enumerate(utargets):
        # get the list of pattern ids for this class
        sample += random.sample(list((targets == r).nonzero()[0]), npertarget[i] )

    return dataset[sample]


@datasetmethod
def get_nsamples_per_attr(dataset, attr):
    """Returns the number of samples per unique value of a sample attribute.

    Parameters
    ----------
    attr : str
      Name of the sample attribute

    Returns
    -------
    dict with the number of samples (value) per unique attribute (key).
    """
    return get_nelements_per_value(dataset.sa[attr])


@datasetmethod
def get_samples_by_attr(dataset, attr, values, sort=True):
    """Return indices of samples given a list of attributes
    """

    if not is_sequence_type(values) \
           or isinstance(values, basestring):
        values = [ values ]

    # TODO: compare to plain for loop through the targets
    #       on a real data example
    sel = np.array([], dtype=np.int16)
    sa = dataset.sa
    for value in values:
        sel = np.concatenate((
            sel, np.where(sa[attr].value == value)[0]))

    if sort:
        # place samples in the right order
        sel.sort()

    return sel

@datasetmethod
def summary(dataset, stats=True, lstats='auto', sstats='auto', idhash=False,
            targets_attr='targets', chunks_attr='chunks',
            maxc=30, maxt=20):
    """String summary over the object

    Parameters
    ----------
    stats : bool
      Include some basic statistics (mean, std, var) over dataset samples
    lstats : 'auto' or bool
      Include statistics on chunks/targets.  If 'auto', includes only if both
      targets_attr and chunks_attr are present.
    sstats : 'auto' or bool
      Sequence (order) statistics. If 'auto', includes only if
      targets_attr is present.
    idhash : bool
      Include idhash value for dataset and samples
    targets_attr : str, optional
      Name of sample attributes of targets
    chunks_attr : str, optional
      Name of sample attributes of chunks -- independent groups of samples
    maxt : int
      Maximal number of targets when provide details on targets/chunks
    maxc : int
      Maximal number of chunks when provide details on targets/chunks
    """
    # local bindings
    samples = dataset.samples
    sa = dataset.sa
    s = str(dataset)[1:-1]

    if idhash:
        s += '\nID-Hashes: %s' % dataset.idhash

    # Deduce if necessary lstats and sstats
    if lstats == 'auto':
        lstats = (targets_attr in sa) and (chunks_attr in sa)
    if sstats == 'auto':
        sstats = (targets_attr in sa)

    ssep = (' ', '\n')[lstats]

    ## Possibly summarize attributes listed as having unique
    if stats:
        if np.issctype(samples.dtype):
            # TODO -- avg per chunk?
            # XXX We might like to use scipy.stats.describe to get
            # quick summary statistics (mean/range/skewness/kurtosis)
            if dataset.nfeatures:
                s += "%sstats: mean=%g std=%g var=%g min=%g max=%g\n" % \
                     (ssep, np.mean(samples), np.std(samples),
                      np.var(samples), np.min(samples), np.max(samples))
            else:
                s += "%sstats: dataset has no features\n" % ssep
        else:
            s += "%sstats: no stats for dataset of '%s' dtype" % (ssep, samples.dtype)
    if lstats:
        try:
            s += dataset.summary_targets(
                targets_attr=targets_attr, chunks_attr=chunks_attr,
                maxc=maxc, maxt=maxt)
        except KeyError, e:
            s += 'No per %s/%s due to %r' % (targets_attr, chunks_attr, e)

    if sstats and not targets_attr is None:
        if len(dataset.sa[targets_attr].unique) < maxt:
            ss = SequenceStats(dataset.sa[targets_attr].value)
            s += str(ss)
        else:
            s += "Number of unique %s > %d thus no sequence statistics" % \
                 (targets_attr, maxt)
    return s

@datasetmethod
def summary_targets(dataset, targets_attr='targets', chunks_attr='chunks',
                    maxc=30, maxt=20):
    """Provide summary statistics over the targets and chunks

    Parameters
    ----------
    dataset : `Dataset`
      Dataset to operate on
    targets_attr : str, optional
      Name of sample attributes of targets
    chunks_attr : str, optional
      Name of sample attributes of chunks -- independent groups of samples
    maxc : int
      Maximal number of chunks when provide details
    maxt : int
      Maximal number of targets when provide details
    """
    # We better avoid bound function since if people only
    # imported Dataset without miscfx it would fail
    spcl = get_samples_per_chunk_target(
        dataset, targets_attr=targets_attr, chunks_attr=chunks_attr)
    # XXX couldn't they be unordered?
    ul = dataset.sa[targets_attr].unique.tolist()
    uc = dataset.sa[chunks_attr].unique.tolist()
    s = ""
    if len(ul) < maxt and len(uc) < maxc:
        s += "\nCounts of targets in each chunk:"
        # only in a reasonable case do printing
        table = [['  %s\\%s' % (chunks_attr, targets_attr)] + ul]
        table += [[''] + ['---'] * len(ul)]
        for c, counts in zip(uc, spcl):
            table.append([ str(c) ] + counts.tolist())
        s += '\n' + table2string(table)
    else:
        s += "No details due to large number of targets or chunks. " \
             "Increase maxc and maxt if desired"


    def cl_stats(axis, u, name1, name2):
        """Compute statistics per target
        """
        stats = {'min': np.min(spcl, axis=axis),
                 'max': np.max(spcl, axis=axis),
                 'mean': np.mean(spcl, axis=axis),
                 'std': np.std(spcl, axis=axis),
                 '#%s' % name2: np.sum(spcl>0, axis=axis)}
        entries = ['  ' + name1, 'mean', 'std', 'min', 'max', '#%s' % name2]
        table = [ entries ]
        for i, l in enumerate(u):
            d = {'  ' + name1 : l}
            d.update(dict([ (k, stats[k][i]) for k in stats.keys()]))
            table.append( [ ('%.3g', '%s')[isinstance(d[e], basestring)]
                            % d[e] for e in entries] )
        return '\nSummary for %s across %s\n' % (name1, name2) \
               + table2string(table)

    if len(ul) < maxt:
        s += cl_stats(0, ul, targets_attr, chunks_attr)
    if len(uc) < maxc:
        s += cl_stats(1, uc, chunks_attr, targets_attr)
    return s


class SequenceStats(dict):
    """Simple helper to provide representation of sequence statistics

    Matlab analog:
    http://cfn.upenn.edu/aguirre/code/matlablib/mseq/mtest.m

    WARNING: Experimental -- API might change without warning!
    Current implementation is ugly!
    """

    # TODO: operate given some "chunks" so it could report also
    #       counter-balance for the borders, mean across chunks, etc
    def __init__(self, seq, order=2):#, chunks=None, chunks_attr=None):
        """Initialize SequenceStats

        Parameters
        ----------
        seq : list or ndarray
          Actual sequence of targets
        order : int
          Maximal order of counter-balancing check. For perfect
          counterbalancing all matrices should be identical
        """

        """
          chunks : None or list or ndarray
            Chunks to use if `perchunk`=True
          """
        dict.__init__(self)
        self.order = order
        self._seq = seq
        self.stats = None
        self._str_stats = None
        self._compute()


    def __repr__(self):
        """Representation of SequenceStats
        """
        return "SequenceStats(%s, order=%d)" % (repr(self._seq), self.order)

    def __str__(self):
        return self._str_stats

    def _compute(self):
        """Compute stats and string representation
        """
        # Do actual computation
        order = self.order
        seq = list(self._seq)               # assure list
        nsamples = len(seq)                 # # of samples/targets
        utargets = sorted(list(set(seq)))    # unique targets
        ntargets = len(utargets)              # # of targets

        # mapping for targets
        targets_map = dict([(l, i) for i, l in enumerate(utargets)])

        # map sequence first
        seqm = [targets_map[i] for i in seq]
        # npertarget = np.bincount(seqm)

        res = dict(utargets=utargets)
        # Estimate counter-balance
        cbcounts = np.zeros((order, ntargets, ntargets), dtype=int)
        for cb in xrange(order):
            for i, j in zip(seqm[:-(cb+1)], seqm[cb+1:]):
                cbcounts[cb, i, j] += 1
        res['cbcounts'] = cbcounts

        """
        Lets compute relative counter-balancing
        Ideally, npertarget[i]/ntargets should precede each target
        """
        # Autocorrelation
        corr = []
        # for all possible shifts:
        for shift in xrange(1, nsamples):
            shifted = seqm[shift:] + seqm[:shift]
            # ??? User pearsonsr with p may be?
            corr += [np.corrcoef(seqm, shifted)[0, 1]]
            # ??? report high (anti)correlations?
        res['corrcoef'] = corr = np.array(corr)
        res['sumabscorr'] = sumabscorr = np.sum(np.abs(corr))
        self.update(res)

        # Assign textual summary
        # XXX move into a helper function and do on demand
        t = [ [""] * (1 + self.order*(ntargets+1)) for i in xrange(ntargets+1) ]
        t[0][0] = "Targets/Order"
        for i, l  in enumerate(utargets):
            t[i+1][0] = '%s:' % l
        for cb in xrange(order):
            t[0][1+cb*(ntargets+1)] = "O%d" % (cb+1)
            for i  in xrange(ntargets+1):
                t[i][(cb+1)*(ntargets+1)] = " | "
            m = cbcounts[cb]
            # ??? there should be better way to get indexes
            ind = np.where(~np.isnan(m))
            for i, j in zip(*ind):
                t[1+i][1+cb*(ntargets+1)+j] = '%d' % m[i, j]

        sout = "Sequence statistics for %d entries" \
               " from set %s\n" % (len(seq), utargets) + \
               "Counter-balance table for orders up to %d:\n" % order \
               + table2string(t)
        if len(corr):
            sout += "Correlations: min=%.2g max=%.2g mean=%.2g sum(abs)=%.2g" \
                    % (min(corr), max(corr), np.mean(corr), sumabscorr)
        self._str_stats = sout


    def plot(self):
        """Plot correlation coefficients
        """
        externals.exists('pylab', raise_=True)
        import pylab as pl
        pl.plot(self['corrcoef'])
        pl.title('Auto-correlation of the sequence')
        pl.xlabel('Offset')
        pl.ylabel('Correlation Coefficient')
        pl.show()

########NEW FILE########
__FILENAME__ = mri
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support for magnetic resonance imaging (MRI) data IO.

This module offers functions to import into PyMVPA MRI data from files
in any format supported by NiBabel_ (e.g. NIfTI, MINC, Analyze), and
export PyMVPA datasets back into data formats supported by NiBabel_.

.. _NiBabel: http://nipy.sourceforge.net/nibabel
"""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals
externals.exists('nibabel', raise_=True)

import sys
import numpy as np
from mvpa2.support.copy import deepcopy
from mvpa2.misc.support import Event
from mvpa2.base.collections import DatasetAttribute
from mvpa2.base.dataset import _expand_attribute

if __debug__:
    from mvpa2.base import debug

from mvpa2.datasets.base import Dataset
from mvpa2.mappers.fx import _uniquemerge2literal
from mvpa2.mappers.flatten import FlattenMapper
from mvpa2.mappers.boxcar import BoxcarMapper
from mvpa2.base import warning


def _data2img(data, hdr=None, imgtype=None):
    # input data is t,x,y,z
    # let's try whether we can get it done with nibabel
    import nibabel
    if imgtype is None:
        # default is NIfTI1
        imgtype = nibabel.Nifti1Image
    else:
        itype = imgtype

def _img2data(src):
    # break early of nothing has been given
    # XXX feels a little strange to handle this so deep inside, but well...
    if src is None:
        return None

    # let's try whether we can get it done with nibabel
    import nibabel
    if isinstance(src, basestring):
        # filename
        img = nibabel.load(src)
    else:
        # assume this is an image already
        img = src
    if isinstance(img, nibabel.spatialimages.SpatialImage):

        data, header = img.get_data(), img.get_header()

        if len(img.shape) == 5 and img.shape[3] == 1:
            # hack to allow loading NIFTI files generated by AFNI
            # these have time in the fifth dimension while the fourth
            # dimension is singleton
            warning('dataset with 5th dimension found but 4th is empty (AFNI '
                    ' NIFTI conversion syndrome) - squeezing data to 4D')

            s = img.shape
            newshape = (s[0], s[1], s[2], s[4])

            header.set_data_shape(newshape)
            data = np.reshape(data, newshape)

        # nibabel image, dissect and return pieces
        return _get_txyz_shaped(data), header, img.__class__
    else:
        # no clue what it is
        return None


def map2nifti(dataset, data=None, imghdr=None, imgtype=None):
    """Maps data(sets) into the original dataspace and wraps it into an Image.

    Parameters
    ----------
    dataset : Dataset
      The mapper of this dataset is used to perform the reverse-mapping.
    data : ndarray or Dataset, optional
      The data to be wrapped into NiftiImage. If None (default), it
      would wrap samples of the provided dataset. If it is a Dataset
      instance -- takes its samples for mapping.
    imghdr : None or dict, optional
      Image header data. If None, the header is taken from `dataset.a.imghdr`.
    imgtype : None or class, optional
      Image class to be used for the instance. If None, the type is taken
      from `dataset.a.imgtype`.

    Returns
    -------
    Image
      Instance of a class derived from :class:`nibabel.spatialimages.SpatialImage`,
      such as Nifti1Image
    """
    import nibabel
    if data is None:
        data = dataset.samples
    elif isinstance(data, Dataset):
        # ease users life
        data = data.samples

    # call the appropriate function to map single samples or multiples
    if len(data.shape) > 1:
        dsarray = dataset.a.mapper.reverse(data)
    else:
        dsarray = dataset.a.mapper.reverse1(data)

    if imghdr is None:
        if 'imghdr' in dataset.a:
            imghdr = dataset.a.imghdr
        elif __debug__:
            debug('DS_NIFTI', 'No image header found. Using defaults.')

    if imgtype is None:
        if 'imgtype' in dataset.a:
            imgtype = dataset.a.imgtype
        else:
            imgtype = nibabel.Nifti1Image
            if __debug__:
                debug('DS_NIFTI',
                      'No image type found in %s. Using default Nifti1Image.'
                      % (dataset.a))

    # set meaningful range
    try:
        if not imghdr is None:
            if 'cal_max' in imghdr:
                imghdr['cal_max'] = dsarray.max()
                imghdr['cal_min'] = dsarray.min()
    except:
        # probably not a NIfTI header
        pass

    # Augment header if data dsarray dtype could not be represented
    # with imghdr.get_data_dtype()

    if issubclass(imgtype, nibabel.spatialimages.SpatialImage) \
       and (imghdr is None or hasattr(imghdr, 'get_data_dtype')):
        # we can handle the desired image type and hdr with nibabel
        # use of `None` for the affine should cause to pull it from
        # the header
        return imgtype(_get_xyzt_shaped(dsarray), None, imghdr)
    else:
        raise ValueError(
            "Got imgtype=%s and imghdr=%s -- cannot generate an Image"
            % (imgtype, imghdr))
    return RuntimeError("Should have never got here -- check your Python")


def fmri_dataset(samples, targets=None, chunks=None, mask=None,
                 sprefix='voxel', tprefix='time', add_fa=None,):
    """Create a dataset from an fMRI timeseries image.

    The timeseries image serves as the samples data, with each volume becoming
    a sample. All 3D volume samples are flattened into one-dimensional feature
    vectors, optionally being masked (i.e. subset of voxels corresponding to
    non-zero elements in a mask image).

    In addition to (optional) samples attributes for targets and chunks the
    returned dataset contains a number of additional attributes:

    Samples attributes (per each volume):

      * volume index (time_indices)
      * volume acquisition time (time_coord)

    Feature attributes (per each voxel):

      * voxel indices (voxel_indices), sometimes referred to as ijk

    Dataset attributes:

      * dump of the image (e.g. NIfTI) header data (imghdr)
      * class of the image (e.g. Nifti1Image) (imgtype)
      * volume extent (voxel_dim)
      * voxel extent (voxel_eldim)

    The default attribute name is listed in parenthesis, but may be altered by
    the corresponding prefix arguments. The validity of the attribute values
    relies on correct settings in the NIfTI image header.

    Parameters
    ----------
    samples : str or NiftiImage or list
      fMRI timeseries, specified either as a filename (single file 4D image),
      an image instance (4D image), or a list of filenames or image instances
      (each list item corresponding to a 3D volume).
    targets : scalar or sequence
      Label attribute for each volume in the timeseries, or a scalar value that
      is assigned to all samples.
    chunks : scalar or sequence
      Chunk attribute for each volume in the timeseries, or a scalar value that
      is assigned to all samples.
    mask : str or NiftiImage
      Filename or image instance of a 3D volume mask. Voxels corresponding to
      non-zero elements in the mask will be selected. The mask has to be in the
      same space (orientation and dimensions) as the timeseries image
    sprefix : str or None
      Prefix for attribute names describing spatial properties of the
      timeseries. If None, no such attributes are stored in the dataset.
    tprefix : str or None
      Prefix for attribute names describing temporal properties of the
      timeseries. If None, no such attributes are stored in the dataset.
    add_fa : dict or None
      Optional dictionary with additional volumetric data that shall be stored
      as feature attributes in the dataset. The dictionary key serves as the
      feature attribute name. Each value might be of any type supported by the
      'mask' argument of this function.

    Returns
    -------
    Dataset
    """
    # load the samples
    imgdata, imghdr, imgtype = _load_anyimg(samples, ensure=True, enforce_dim=4)

    # figure out what the mask is, but only handle known cases, the rest
    # goes directly into the mapper which maybe knows more
    maskimg = _load_anyimg(mask)
    if maskimg is None:
        pass
    else:
        # take just data and ignore the header
        mask = maskimg[0]

    # compile the samples attributes
    sa = {}
    if not targets is None:
        sa['targets'] = _expand_attribute(targets, imgdata.shape[0], 'targets')
    if not chunks is None:
        sa['chunks'] = _expand_attribute(chunks, imgdata.shape[0], 'chunks')

    # create a dataset
    ds = Dataset(imgdata, sa=sa)
    if sprefix is None:
        space = None
    else:
        space = sprefix + '_indices'
    ds = ds.get_mapped(FlattenMapper(shape=imgdata.shape[1:], space=space))

    # now apply the mask if any
    if not mask is None:
        flatmask = ds.a.mapper.forward1(mask)
        # direct slicing is possible, and it is potentially more efficient,
        # so let's use it
        #mapper = StaticFeatureSelection(flatmask)
        #ds = ds.get_mapped(StaticFeatureSelection(flatmask))
        ds = ds[:, flatmask != 0]

    # load and store additional feature attributes
    if not add_fa is None:
        for fattr in add_fa:
            value = _load_anyimg(add_fa[fattr], ensure=True)[0]
            ds.fa[fattr] = ds.a.mapper.forward1(value)

    # store interesting props in the dataset
    ds.a['imghdr'] = imghdr
    ds.a['imgtype'] = imgtype
    # If there is a space assigned , store the extent of that space
    if sprefix is not None:
        ds.a[sprefix + '_dim'] = imgdata.shape[1:]
        # 'voxdim' is (x,y,z) while 'samples' are (t,z,y,x)
        ds.a[sprefix + '_eldim'] = _get_voxdim(imghdr)
        # TODO extend with the unit
    if tprefix is not None:
        ds.sa[tprefix + '_indices'] = np.arange(len(ds), dtype='int')
        ds.sa[tprefix + '_coords'] = np.arange(len(ds), dtype='float') \
                                     * _get_dt(imghdr)
        # TODO extend with the unit

    return ds


def _get_voxdim(hdr):
    """Get the size of a voxel from some image header format."""
    return hdr.get_zooms()[:-1]


def _get_dt(hdr):
    """Get the TR of a fMRI timeseries from some image header format."""
    return hdr.get_zooms()[-1]


def _get_txyz_shaped(arr):
    # we get the data as x,y,z[,t] but we want to have the time axis first
    # if any
    if len(arr.shape) == 4:
        arr = np.rollaxis(arr, -1)
    return arr


def _get_xyzt_shaped(arr):
    # we get the data as [t,]x,y,z but we want to have the time axis last
    # if any
    if len(arr.shape) == 4:
        arr = np.rollaxis(arr, 0, 4)
    return arr


def _load_anyimg(src, ensure=False, enforce_dim=None):
    """Load/access NIfTI data from files or instances.

    Parameters
    ----------
    src : str or NiftiImage
      Filename of a NIfTI image or a `NiftiImage` instance.
    ensure : bool, optional
      If True, throw ValueError exception if cannot be loaded.
    enforce_dim : int or None
      If not None, it is the dimensionality of the data to be enforced,
      commonly 4D for the data, and 3D for the mask in case of fMRI.

    Returns
    -------
    tuple or None
      If the source is not supported None is returned.  Otherwise a
      tuple of (imgdata, imghdr, imgtype)

    Raises
    ------
    ValueError
      If there is a problem with data (variable dimensionality) or
      failed to load data and ensure=True.
    """
    imgdata = imghdr = imgtype = None

    # figure out whether we have a list of things to load and handle that
    # first
    if (isinstance(src, list) or isinstance(src, tuple)) \
            and len(src) > 0:
        # load from a list of given entries
        srcs = [_load_anyimg(s, ensure=ensure, enforce_dim=enforce_dim)
                for s in src]
        if __debug__:
            # lets check if they all have the same dimensionality
            # besides the leading one
            shapes = [s[0].shape[1:] for s in srcs]
            if not np.all([s == shapes[0] for s in shapes]):
                raise ValueError(
                      "Input volumes vary in their shapes: %s" % (shapes,))
        # Combine them all into a single beast
        # will be t,x,y,z
        imgdata = np.vstack([s[0] for s in srcs])
        imghdr, imgtype = srcs[0][1:3]
    else:
        # try opening the beast; this might yield none in case of an unsupported
        # argument and is handled accordingly below
        data = _img2data(src)
        if not data is None:
            imgdata, imghdr, imgtype = data

    if imgdata is not None and enforce_dim is not None:
        shape, new_shape = imgdata.shape, None
        lshape = len(shape)

        # check if we need to tune up shape
        if lshape < enforce_dim:
            # if we are missing required dimension(s)
            new_shape = (1,) * (enforce_dim - lshape) + shape
        elif lshape > enforce_dim:
            # if there are bogus dimensions at the beginning
            bogus_dims = lshape - enforce_dim
            if shape[:bogus_dims] != (1,) * bogus_dims:
                raise ValueError, \
                      "Cannot enforce %dD on data with shape %s" \
                      % (enforce_dim, shape)
            new_shape = shape[bogus_dims:]

        # tune up shape if needed
        if new_shape is not None:
            if __debug__:
                debug('DS_NIFTI', 'Enforcing shape %s for %s data from %s' %
                      (new_shape, shape, src))
            imgdata.shape = new_shape

    if imgdata is None:
        return None
    else:
        return imgdata, imghdr, imgtype

########NEW FILE########
__FILENAME__ = niml
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support for storage using the NeuroImaging Markup Language (NIML).

Supports storing most typical values (samples, feature attributes, sample
attributes, dataset attributes) that are in a dataset in NIML format, as
long as these values are array-like.

Notes
-----
No support for 'sophisticated' values such as Mappers

.. versionadded:: 2.3.0

"""

__docformat__ = 'restructuredtext'

import numpy as np

import os

from mvpa2.support.nibabel import afni_niml_dset as niml_dset

from mvpa2.base.collections import SampleAttributesCollection, \
        FeatureAttributesCollection, DatasetAttributesCollection, \
        ArrayCollectable


from mvpa2.base import warning, debug, externals
from mvpa2.datasets.base import Dataset
from mvpa2.base import dataset

if externals.exists('h5py'):
    from mvpa2.base.hdf5 import h5save, h5load

_PYMVPA_PREFIX = 'PYMVPA'
_PYMVPA_SEP = '_'

def from_niml(dset, fa_labels=[], sa_labels=[], a_labels=[]):
    '''Convert a NIML dataset to a Dataset

    Parameters
    ----------
    dset: dict
        Dictionary with NIML key-value pairs, such as obtained from
        mvpa2.support.nibabel.afni_niml_dset.read()
    fa_labels: list
        Keys in dset that are enforced to be feature attributes
    sa_labels: list
        Keys in dset that are enforced to be sample attributes
    a_labels: list
        Keys in dset that are enforced to be dataset attributes

    Returns
    -------
    dataset: mvpa2.base.Dataset
        a PyMVPA Dataset
    '''

    # check for singleton element
    if type(dset) is list and len(dset) == 1:
        # recursive call
        return from_niml(dset[0])

    if not type(dset) is dict:
        raise ValueError("Expected a dict")

    if not 'data' in dset:
        raise ValueError("dset with no data?")

    data = dset['data']
    if len(data.shape) == 1:
        nfeatures = data.shape[0]
        nsamples = 1
    else:
        nfeatures, nsamples = data.shape

    # some labels have predefined destinations
    sa_labels_ = ['labels', 'stats', 'chunks', 'targets'] + sa_labels
    fa_labels_ = ['node_indices', 'center_ids'] + fa_labels
    a_labels_ = ['history'] + a_labels
    ignore_labels = ('data', 'dset_type')

    sa = SampleAttributesCollection(length=nsamples)
    fa = FeatureAttributesCollection(length=nfeatures)
    a = DatasetAttributesCollection()

    labels_collections = [(sa_labels_, sa),
                          (fa_labels_, fa),
                          (a_labels_, a)]

    infix2collection = {'sa':sa,
                      'fa':fa,
                      'a':a}

    infix2length = {'sa':nsamples, 'fa':nfeatures}

    for k, v in dset.iteritems():
        if k in ignore_labels:
            continue

        if k.startswith(_PYMVPA_PREFIX + _PYMVPA_SEP):
            # special PYVMPA field - do the proper conversion
            k_split = k.split(_PYMVPA_SEP)
            if len(k_split) > 2:
                infix = k_split[1].lower()
                collection = infix2collection.get(infix, None)
                if not collection is None:
                    short_k = _PYMVPA_SEP.join(k_split[2:])
                    expected_length = infix2length.get(infix, None)
                    if expected_length:
                        if isinstance(v, np.ndarray) and np.dtype == np.str_:
                            v = str(v)

                        while isinstance(v, basestring):
                            # strings are seperated by ';'
                            # XXX what if this is part of the value
                            # intended by the user?
                            v = v.split(';')

                        if expected_length != len(v):
                            raise ValueError("Unexpected length: %d != %d" %
                                                (expected_length, len(v)))

                        v = ArrayCollectable(v, length=expected_length)

                    collection[short_k] = v
                    continue

        found_label = False

        for label, collection in labels_collections:
            if k in label:
                collection[k] = v
                found_label = True
                break

        if found_label:
            continue

        # try to be smart and deduce this from dimensions.
        # this only works if nfeatures!=nsamples otherwise it would be
        # ambiguous
        # XXX is this ugly?
        if nfeatures != nsamples:
            try:
                n = len(v)
                if n == nfeatures:
                    fa[k] = v
                    continue
                elif n == nsamples:
                    sa[k] = v
                    continue
            except:
                pass

        # don't know what this is - make it a general attribute
        a[k] = v

    ds = Dataset(np.transpose(data), sa=sa, fa=fa, a=a)

    return ds

def to_niml(ds):
    '''Convert a Dataset to a NIML dataset

    Parameters
    ----------
    dataset: mvpa2.base.Dataset
        A PyMVPA Dataset

    Returns
    -------
    dset: dict
        Dictionary with NIML key-value pairs, such as obtained from
        mvpa2.support.nibabel.afni_niml_dset.read()
     '''
    if isinstance(ds, np.ndarray):
        ds = Dataset(ds)

    dset = dict(data=np.transpose(ds.samples))

    node_indices_labels = ('node_indices', 'center_ids', 'ids', 'roi_ids')
    node_indices = _find_node_indices(ds, node_indices_labels)
    if not node_indices is None:
        dset['node_indices'] = node_indices

    sample_labels = ('labels', 'targets')
    labels = _find_sample_labels(ds, sample_labels)
    if not labels is None:
        dset['labels'] = labels

    attr_labels = ('a', 'fa', 'sa')

    # a few labels are directly used in NIML dsets
    # without prefixing it with a pyMVPA string
    # for (dataset, feature, sample) attributes
    # here we define two for sample attributes
    attr_special_labels = ([], [], ['labels', 'stats'])

    for i, attr_label in enumerate(attr_labels):
        attr = getattr(ds, attr_label)
        special_labels = attr_special_labels[i]
        for k in attr.keys():
            v = attr[k]
            if hasattr(v, 'value'):
                v = v.value

            if k in special_labels:
                long_key = k
            else:
                long_key = _PYMVPA_SEP.join((_PYMVPA_PREFIX,
                                             attr_label.upper(), k))

            dset[long_key] = v

    return dset

def hstack(dsets, pad_to_feature_index=None, hstack_method='drop_nonunique',
                set_empty_value=0.):
    '''Stacks NIML datasets while considering node indices

    Parameters
    ----------
    dsets: list
        datasets to be stacked
    pad_to_feature_index: list or int or None
        If a list then it should be of the same length as dsets and indicates
        to which node index the input should be padded. A single int means
        that the same value is used for all dset in dsets. None means
        no padding, and is only allowed for non-sparse datasets.
    hstack_method: str:
        How datasets are stacked; see dataset.hstack.
    set_empty_value: float
        Value to which empty (padded) dataset values are set.

    Returns
    dset: Dataset
        Data combined from all dset in dsets.
    '''

    n = len(dsets)

    # make sure pad_to_feature_index has n values
    if pad_to_feature_index is None or type(pad_to_feature_index) is int:
        pad_to_feature_index = [pad_to_feature_index] * n
    elif len(pad_to_feature_index) != n:
        raise ValueError("illegal pad_to_feature_index: expected list or int")

    # labels that can contain node indices
    node_indices_labels = ('node_indices', 'center_ids', 'ids', 'roi_ids')
    node_indices = []

    # allocate space for output
    padded_dsets = []
    hstack_indices = []
    first_node_index = 0
    for i, (dset, pad_to) in enumerate(zip(dsets, pad_to_feature_index)):
        # get node indices in this dataset
        node_index = _find_node_indices(dset, node_indices_labels)
        if node_index is None:
            node_index = np.arange(dset.nfeatures)
        max_node_index = np.max(node_index)

        # make a stripped version - without node index labels
        stripped_dset = dset.copy()
        for label in node_indices_labels:
            if label in stripped_dset.fa:
                stripped_dset.fa.pop(label)

        # see if padding is needed
        if pad_to is None or pad_to == max_node_index + 1:
            if not np.array_equal(np.arange(max_node_index + 1), np.sort(node_index)):
                raise ValueError("Sparse input %d: need pad_to input" % (i + 1))
            padded_dset = stripped_dset
            other_index = np.arange(0)
        else:
            # have to use empty values
            nfeatures_empty = pad_to - dset.nfeatures
            if nfeatures_empty < 0:
                raise ValueError("Dataset has %d features, cannot pad "
                                    "to %d" % (dset.nfeatures, pad_to))

            # make empty array
            empty_arr = np.zeros((dset.nsamples, nfeatures_empty),
                                    dtype=dset.samples.dtype) + set_empty_value
            empty_dset = Dataset(empty_arr, sa=stripped_dset.sa.copy(deep=True))

            # combine current dset and empty array
            padded_dset = dataset.hstack((stripped_dset, empty_dset), hstack_method)

            # set the proper node indices
            other_index = np.setdiff1d(np.arange(pad_to), node_index)

        # sanity check to make sure that indices are ok
        # XXX could be more informative
        if len(np.setdiff1d(node_index, np.arange(pad_to or max_node_index + 1))):
            raise ValueError("Illegal indices")

        hstack_index = node_index + first_node_index
        hstack_other_index = other_index + first_node_index
        first_node_index += pad_to or (max_node_index + 1) # prepare for next iteration

        padded_dsets.append(padded_dset)
        hstack_indices.append(hstack_index)
        if len(other_index):
            hstack_indices.append(hstack_other_index)

    hstack_dset = dataset.hstack(padded_dsets, hstack_method)
    hstack_indices = np.hstack(hstack_indices)

    hstack_dset.fa[node_indices_labels[0]] = hstack_indices

    return hstack_dset


def _find_sample_labels(dset, sample_labels):
    '''Helper function to find labels in this dataset.
    Looks for any in sample_labels and returns the first one
    that matches '''
    use_label = None

    dset_keys = dset.sa.keys()
    for label in sample_labels:
        if label in dset_keys:
            sample_label = dset.sa[label].value
            if isinstance(sample_label, basestring):
                # split using
                sample_label = sample_label.split(';')

            # they can be of any type so ensure they are strings
            sample_label_list = [str(i) for i in sample_label]
            if len(sample_label_list) != dset.nsamples:
                # unlike node indices here we are more lenient
                # so not throw an exception but just continue
                continue

            use_label = label

            # do not look for any other labels
            break

    return None if use_label is None else sample_label_list


def _find_node_indices(dset, node_indices_labels):
    '''Helper function to find node indices in this dataset
    Sees if any of the node_indices_labels is a feature attribute
    in the dataset and returns it. If they are multiple matches
    ensure they are identical, otherwise raise an error.
    A use case is searchlight results that assignes center_ids as
    a feature attributes, but it should be named node_indices
    before conversion to NIML format'''

    use_label = None

    dset_keys = dset.fa.keys()
    for label in node_indices_labels:
        if label in dset_keys:
            if use_label is None:
                # make vector and ensure all integer values
                node_indices = dset.fa[label].value
                node_indices = np.asarray(node_indices).ravel()
                if len(node_indices) != dset.nfeatures:
                    raise ValueError("Node indices mismatch: found %d values "
                                     " but dataset has %d features" %
                                     (len(node_indices, dset.nfeatures)))
                node_indices_int = np.asarray(node_indices, dtype=np.int)
                if not np.array_equal(node_indices_int, node_indices):
                    raise ValueError("Node indices should have integer values")
                use_label = label

            else:
                if not np.array_equal(dset.fa[label].value, node_indices_int):
                    raise ValueError("Different indices for feature attributes"
                                     " %s and %s" % (use_key, label))

    return None if use_label is None else node_indices_int

def write(fn, ds, form='binary'):
    '''Write a Dataset to a file in NIML format

    Parameters
    ----------
    fn: str
        Filename
    ds: mvpa2.base.Dataset
        Dataset to be stored
    form: str
        Data format: 'binary' or 'text' or 'base64'
    '''
    niml_ds = to_niml(ds)
    niml_dset.write(fn, niml_ds, form=form)

def read(fn):
    '''Read a Dataset from a file in NIML format

    Parameters
    ----------
    fn: str
        Filename
    '''

    readers_converters = [(niml_dset.read, from_niml)]
    if externals.exists('h5py'):
        readers_converters.append((h5load, None))

    for reader, converter in readers_converters:
        try:
            r = reader(fn)
            if converter:
                r = converter(r)
            return r

        except:
            pass

    raise ValueError("Unable to read %s" % fn)


def from_any(x):
    '''Get a Dataset from the input

    Parameters
    ----------
    x: str or dict or Dataset
        Filename, or NIML-dictionary, or a Dataset itself

    Returns
    -------
    ds: mvpa2.base.Dataset
        Dataset instance
    '''
    if isinstance(x, basestring):
        return read(x)
    elif isinstance(x, dict):
        return from_niml(x)
    elif isinstance(x, Dataset):
        return x

    raise ValueError("Not supported: %r" % (x,))


########NEW FILE########
__FILENAME__ = skl_data
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wrapper for sklearn datasets/data generators."""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals

if externals.exists('skl', raise_=True):
    if externals.versions['skl'] >= '0.9':
        from sklearn import datasets as sklds
    else:
        from scikits.learn import datasets as sklds

    import inspect

    __all__ = []

    for fx in sklds.__dict__:
        if not (fx.startswith('make_') or fx.startswith('load_')) \
                or fx in ['load_filenames', 'load_files',
                          'load_sample_image', 'load_sample_images',
                          'load_svmlight_files', 'load_svmlight_file']:
            continue
        fx = getattr(sklds, fx)
        argnames, varargs, varkw, defaults = inspect.getargspec(fx)
        if not defaults is None:
            kwargs = list(zip(argnames[::-1], defaults[::-1]))[::-1]
        else:
            kwargs = tuple()
        args = argnames[:len(argnames) - len(kwargs)]
        sig = ''
        if len(args):
            sig += ', '.join(args)
        if len(kwargs):
            if len(sig):
                sig += ', '
            sig += ', '.join(['%s=%s' % (kw[0], repr(kw[1])) for kw in kwargs])
        if varargs:
            if len(sig):
                sig += ', '
            sig += '*%s' % varargs
        if varkw:
            if len(sig):
                sig += ', '
            sig += '**%s' % varkw

        pymvpa_fxname = 'skl_%s' % fx.__name__[5:]
        fxdef = '''
def %s(%s):
    """%s

    Notes
    -----
    This function has been auto-generated by wrapping %s() from the
    `sklearn <http://scikit-learn.org>`_ package. The documentation of
    this function has been kept verbatim. Consequently, the actual return
    value is not as described in the documentation, but the data is returned
    as a PyMVPA dataset.
    """
    from sklearn import datasets as sklds
    from mvpa2.datasets import Dataset
    data = sklds.%s(%s)
    if isinstance(data, tuple):
        ds = Dataset(data[0])
        if len(data) > 1:
            ds.sa['targets'] = data[1]
        if len(data) > 2:
            raise RuntimeError("sklearn function returned unexpected amount of data")
    else:
        ds = Dataset(data['data'])
        if 'DESCR' in data:
            ds.a['descr'] = data['DESCR']
        if 'feature_names' in data:
            ds.fa['names'] = data['feature_names']
        if 'target' in data:
            if 'target_names' in data:
                names = data['target_names']
                ds.sa['targets'] = [names[t] for t in data['target']]
            else:
                ds.sa['targets'] = data['target']
    return ds
''' % (pymvpa_fxname,
               sig,
               fx.__doc__,
               fx.__name__,
               fx.__name__,
               ', '.join(argnames))
        exec fxdef
        __all__.append(pymvpa_fxname)


########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Feature selection base class and related stuff base classes and helpers."""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.featsel.helpers import FractionTailSelector, \
                                 NBackHistoryStopCrit, \
                                 BestDetector
from mvpa2.mappers.slicing import SliceMapper
from mvpa2.mappers.base import accepts_dataset_as_samples
from mvpa2.base.dochelpers import _repr_attrs
from mvpa2.base.state import ConditionalAttribute
from mvpa2.generators.splitters import mask2slice
from mvpa2.base.dataset import split_by_sample_attribute, vstack
from mvpa2.base import externals

if __debug__:
    from mvpa2.base import debug


class FeatureSelection(SliceMapper):
    """Mapper to select a subset of features.

    Depending on the actual slicing two FeatureSelections can be merged in a
    number of ways: incremental selection (+=), union (&=) and intersection
    (|=).  Were the former assumes that two feature selections are applied
    subsequently, and the latter two assume that both slicings operate on the
    set of input features.

    Examples
    --------
    >>> from mvpa2.datasets import *
    >>> ds = Dataset([[1,2,3,4,5]])
    >>> fs0 = StaticFeatureSelection([0,1,2,3])
    >>> fs0(ds).samples
    array([[1, 2, 3, 4]])

    Merge two incremental selections: the resulting mapper performs a selection
    that is equivalent to first applying one slicing and subsequently the next
    slicing. In this scenario the slicing argument of the second mapper is
    relative to the output feature space of the first mapper.

    >>> fs1 = StaticFeatureSelection([0,2])
    >>> fs0 += fs1
    >>> fs0(ds).samples
    array([[1, 3]])
    """

    __init__doc__exclude__ = ['slicearg']

    def __init__(self, filler=0, **kwargs):
        """
        Parameters
        ----------
        filler : optional
          Value to fill empty entries upon reverse operation
        """
        # init slicearg with None
        SliceMapper.__init__(self, None, **kwargs)
        self._dshape = None
        self._oshape = None
        self.filler = filler


    def __repr__(self, prefixes=[]):
        return super(FeatureSelection, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['filler'], default=0))


    def _forward_data(self, data):
        """Map data from the original dataspace into featurespace.

        Parameters
        ----------
        data : array-like
          Either one-dimensional sample or two-dimensional samples matrix.
        """
        mdata = data[:, self._slicearg]
        # store the output shape if not set yet
        if self._oshape is None:
            self._oshape = mdata.shape[1:]
        return mdata


    def _forward_dataset(self, dataset):
        # XXX this should probably not affect the source dataset, but right now
        # init_origid is not flexible enough
        if not self.get_space() is None:
            # TODO need to do a copy first!!!
            dataset.init_origids('features', attr=self.get_space())
        # invoke super class _forward_dataset, this calls, _forward_dataset
        # and this calles _forward_data in this class
        mds = super(FeatureSelection, self)._forward_dataset(dataset)
        # attribute collection needs to have a new length check
        mds.fa.set_length_check(mds.nfeatures)
        # now slice all feature attributes
        for k in mds.fa:
            mds.fa[k] = self.forward1(mds.fa[k].value)
        return mds


    def reverse1(self, data):
        # we need to reject inappropriate "single" samples to allow
        # chainmapper to properly switch to reverse() for multiple samples
        # use the fact that a single sample needs to conform to the known
        # data shape -- but may have additional appended dimensions
        if not data.shape[:len(self._oshape)] == self._oshape:
            raise ValueError("Data shape does not match training "
                             "(trained: %s; got: %s)"
                             % (self._dshape, data.shape))
        return super(FeatureSelection, self).reverse1(data)


    def _reverse_data(self, data):
        """Reverse map data from featurespace into the original dataspace.

        Parameters
        ----------
        data : array-like
          Either one-dimensional sample or two-dimensional samples matrix.
        """
        if self._dshape is None:
            raise RuntimeError(
                "Cannot reverse-map data since the original data shape is "
                "unknown. Either set `dshape` in the constructor, or call "
                "train().")
        # this wouldn't preserve ndarray subclasses
        #mapped = np.zeros(data.shape[:1] + self._dshape,
        #                 dtype=data.dtype)
        # let's do it a little awkward but pass subclasses through
        # suggestions for improvements welcome
        mapped = data.copy() # make sure we own the array data
        # "guess" the shape of the final array, the following only supports
        # changes in the second axis -- the feature axis
        # this madness is necessary to support mapping of multi-dimensional
        # features
        mapped.resize(data.shape[:1] + self._dshape + data.shape[2:],
                      refcheck=False)
        mapped.fill(self.filler)
        mapped[:, self._slicearg] = data
        return mapped


    def _reverse_dataset(self, dataset):
        # invoke super class _reverse_dataset, this calls, _reverse_dataset
        # and this calles _reverse_data in this class
        mds = super(FeatureSelection, self)._reverse_dataset(dataset)
        # attribute collection needs to have a new length check
        mds.fa.set_length_check(mds.nfeatures)
        # now reverse all feature attributes
        for k in mds.fa:
            mds.fa[k] = self.reverse1(mds.fa[k].value)
        return mds


    @accepts_dataset_as_samples
    def _train(self, data):
        if self._dshape is None:
            # XXX what about arrays of generic objects???
            # MH: in this case the shape will be (), which is just
            # fine since feature slicing is meaningless without features
            # the only thing we can do is kill the whole samples matrix
            self._dshape = data.shape[1:]
            # we also need to know what the output shape looks like
            # otherwise we cannot reliably say what is appropriate input
            # for reverse*()
            self._oshape = data[:, self._slicearg].shape[1:]


    def _untrain(self):
        if __debug__:
            debug("FS_", "Untraining FS: %s" % self)
        self._dshape = None
        self._oshape = None
        super(SliceMapper, self)._untrain()



class StaticFeatureSelection(FeatureSelection):
    """Feature selection by static slicing argument.
    """

    __init__doc__exclude__ = []           # slicearg is relevant again
    def __init__(self, slicearg, dshape=None, oshape=None, **kwargs):
        """
        Parameters
        ----------
        slicearg : int, list(int), array(int), array(bool)
          Any slicing argument that is compatible with numpy arrays. Depending
          on the argument the mapper will perform basic slicing or
          advanced indexing (with all consequences on speed and memory
          consumption).
        dshape : tuple
          Preseed the mappers input data shape (single sample shape).
        oshape: tuple
          Preseed the mappers output data shape (single sample shape).
        """
        FeatureSelection.__init__(self, **kwargs)
        # store it here, might be modified later
        self._dshape = self.__orig_dshape = dshape
        self._oshape = self.__orig_oshape = oshape
        # we also want to store the original slicearg to be able to reset to it
        # during training. Derived classes will override this default
        # implementation of _train()
        self.__orig_slicearg = slicearg
        self._safe_assign_slicearg(slicearg)

    def __repr__(self, prefixes=[]):
        return super(FeatureSelection, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['dshape', 'oshape']))

    @accepts_dataset_as_samples
    def _train(self, ds):
        # first thing is to reset the slicearg to the original value passed to
        # the constructor
        self._safe_assign_slicearg(self.__orig_slicearg)
        # not resetting {d,o}shape here as they will be handled upstream
        # and perform base training
        super(StaticFeatureSelection, self)._train(ds)


    def _untrain(self):
        # make trained again immediately
        self._safe_assign_slicearg(self.__orig_slicearg)
        self._dshape = self.__orig_dshape
        self._oshape = self.__orig_oshape
        super(FeatureSelection, self)._untrain()


    dshape = property(fget=lambda self: self.__orig_dshape)
    oshape = property(fget=lambda self: self.__orig_oshape)

class SensitivityBasedFeatureSelection(FeatureSelection):
    """Feature elimination.

    A `FeaturewiseMeasure` is used to compute sensitivity maps given a certain
    dataset. These sensitivity maps are in turn used to discard unimportant
    features.
    """

    sensitivity = ConditionalAttribute(enabled=False)

    def __init__(self,
                 sensitivity_analyzer,
                 feature_selector=FractionTailSelector(0.05),
                 train_analyzer=True,
                 **kwargs
                 ):
        """Initialize feature selection

        Parameters
        ----------
        sensitivity_analyzer : FeaturewiseMeasure
          sensitivity analyzer to come up with sensitivity
        feature_selector : Functor
          Given a sensitivity map it has to return the ids of those
          features that should be kept.
        train_analyzer : bool
          Flag whether to train the sensitivity analyzer on the input dataset
          during train(). If False, the employed sensitivity measure has to be
          already trained before.
        """

        # base init first
        FeatureSelection.__init__(self, **kwargs)

        self.__sensitivity_analyzer = sensitivity_analyzer
        """Sensitivity analyzer to use once"""

        self.__feature_selector = feature_selector
        """Functor which takes care about removing some features."""

        self.__train_analyzer = train_analyzer

    def _get_selected_ids(self, dataset):
        """Given a dataset actually select the features

        Returns
        -------
        indexes of the selected features
        """
        # optionally train the analyzer first
        if self.__train_analyzer:
            self.__sensitivity_analyzer.train(dataset)

        sensitivity = self.__sensitivity_analyzer(dataset)
        """Compute the sensitivity map."""
        self.ca.sensitivity = sensitivity

        # Select features to preserve
        selected_ids = self.__feature_selector(sensitivity)

        if __debug__:
            debug("FS_", "Sensitivity: %s Selected ids: %s" %
                  (sensitivity, selected_ids))

        # XXX not sure if it really has to be sorted
        selected_ids.sort()
        return selected_ids

    def _train(self, dataset):
        """Select the most important features

        Parameters
        ----------
        dataset : Dataset
          used to compute sensitivity maps
        """
        # Get selected feature ids
        selected_ids = self._get_selected_ids(dataset)
        # announce desired features to the underlying slice mapper
        self._safe_assign_slicearg(selected_ids)
        # and perform its own training
        super(SensitivityBasedFeatureSelection, self)._train(dataset)


    def _untrain(self):
        if __debug__:
            debug("FS_", "Untraining sensitivity-based FS: %s" % self)
        self.__sensitivity_analyzer.untrain()
        # ask base class to do its untrain
        super(SensitivityBasedFeatureSelection, self)._untrain()

    # make it accessible from outside
    sensitivity_analyzer = property(fget=lambda self:self.__sensitivity_analyzer,
                                    doc="Measure which was used to do selection")



class IterativeFeatureSelection(FeatureSelection):
    """
    """
    errors = ConditionalAttribute(
        doc="History of errors")
    nfeatures = ConditionalAttribute(
        doc="History of # of features left")

    def __init__(self,
                 fmeasure,
                 pmeasure,
                 splitter,
                 fselector,
                 stopping_criterion=NBackHistoryStopCrit(BestDetector()),
                 bestdetector=BestDetector(),
                 train_pmeasure=True,
                 # XXX should we may be guard splitter so we do not end up
                 # with inappropriate one for the use, i.e. which
                 # generates more than 2 splits
                 # guard_splitter=True,
                 **kwargs
                 ):
        """
        Parameters
        ----------
        fmeasure : Measure
          Computed for each candidate feature selection. The measure has
          to compute a scalar value.
        pmeasure : Measure
          Compute against a test dataset for each incremental feature
          set.
        splitter: Splitter
          This splitter instance has to generate at least one dataset split
          when called with the input dataset that is used to compute the
          per-feature criterion for feature selection.
        bestdetector : Functor
          Given a list of error values it has to return a boolean that
          signals whether the latest error value is the total minimum.
        stopping_criterion : Functor
          Given a list of error values it has to return whether the
          criterion is fulfilled.
        fselector : Functor
        train_pmeasure : bool
          Flag whether the `pmeasure` should be trained before
          computing the error. In general this is required, but if the
          `fmeasure` and `pmeasure` share and make use of the same
          classifier AND `pmeasure` does not really need training, it
          can be switched off to save CPU cycles.
        """
        # bases init first
        FeatureSelection.__init__(self, **kwargs)

        self._fmeasure = fmeasure
        self._pmeasure = pmeasure
        self._splitter = splitter
        self._fselector = fselector
        self._stopping_criterion = stopping_criterion
        self._bestdetector = bestdetector
        self._train_pmeasure = train_pmeasure


    def _untrain(self):
        if __debug__:
            debug("FS_", "Untraining Iterative FS: %s" % self)
        self._fmeasure.untrain()
        if self._pmeasure is not None:
            self._pmeasure.untrain()
        # ask base class to do its untrain
        super(IterativeFeatureSelection, self)._untrain()


    def _evaluate_pmeasure(self, train, test):
        # local binding
        pmeasure = self._pmeasure
        # might safe some cycles to prevent training the measure, but only
        # the user can know whether this is sensible or possible
        if self._train_pmeasure:
            pmeasure.train(train)
        # actually run the performance measure to estimate "quality" of
        # selection
        return pmeasure(test)


    def _get_traintest_ds(self, ds):
        # activate the dataset splitter
        dsgen = self._splitter.generate(ds)
        # and derived the dataset part that is used for computing the selection
        # criterion
        trainds = dsgen.next()
        testds = dsgen.next()
        return trainds, testds

    # access properties
    fmeasure = property(fget=lambda self: self._fmeasure)
    pmeasure = property(fget=lambda self: self._pmeasure)
    splitter = property(fget=lambda self: self._splitter)
    fselector = property(fget=lambda self: self._fselector)
    stopping_criterion = property(fget=lambda self: self._stopping_criterion)
    bestdetector = property(fget=lambda self: self._bestdetector)
    train_pmeasure = property(fget=lambda self: self._train_pmeasure)


class CombinedFeatureSelection(FeatureSelection):
    """Meta feature selection utilizing several embedded selection methods.

    During training each embedded feature selection method is computed
    individually. Afterwards all feature sets are combined by either taking the
    union or intersection of all sets.
    """
    def __init__(self, selectors, method, **kwargs):
        """
        Parameters
        ----------
        selectors : list
          FeatureSelection instances to run. Order is not important.
        method : {'union', 'intersection'}
          which method to be used to combine the feature selection set of
          all computed methods.
        """
        # by default -- auto_train
        kwargs['auto_train'] = kwargs.get('auto_train', True)
        FeatureSelection.__init__(self, **kwargs)

        self.__selectors = selectors
        self.__method = method


    def _untrain(self):
        if __debug__:
            debug("FS_", "Untraining combined FS: %s" % self)
        for fs in self.__selectors:
            fs.untrain()
        # ask base class to do its untrain
        super(CombinedFeatureSelection, self)._untrain()


    def _train(self, ds):
        # local binding
        method = self.__method

        # two major modes
        if method == 'union':
            # slice mask default: take none
            mask = np.zeros(ds.shape[1], dtype=np.bool)
            # method: OR
            cfunc = np.logical_or
        elif method == 'intersection':
            # slice mask default: take all
            mask = np.ones(ds.shape[1], dtype=np.bool)
            # method: AND
            cfunc = np.logical_and
        else:
            raise ValueError("Unknown combining method '%s'" % method)

        for fs in self.__selectors:
            # first: train all embedded selections
            fs.train(ds)
            # now get boolean mask of selections
            fsmask = np.zeros(mask.shape, dtype=np.bool)
            # use slicearg to select features
            fsmask[fs._slicearg] = True
            # merge with current global mask
            mask = cfunc(mask, fsmask)

        # turn the derived boolean mask into a slice if possible
        slicearg = mask2slice(mask)
        # and assign to baseclass, done
        self._safe_assign_slicearg(slicearg)

    method = property(fget=lambda self: self.__method)
    selectors = property(fget=lambda self: self.__selectors)


class SplitSamplesProbabilityMapper(SliceMapper):
    '''
    Mapper to select features & samples  based on some sensitivity value.

    A use case is feature selection across participants,
    where either the same features are selected in all
    participants or not (see select_common_features parameter).

    Examples
    --------
    >>> nf = 10
    >>> ns = 100
    >>> nsubj = 5
    >>> nchunks = 5
    >>> data = np.random.normal(size=(ns, nf))
    >>> from mvpa2.base.dataset import AttrDataset
    >>> from mvpa2.measures.anova import OneWayAnova
    >>> ds = AttrDataset(data,
    ...                sa=dict(sidx=np.arange(ns),
    ...                        targets=np.arange(ns) % nchunks,
    ...                        chunks=np.floor(np.arange(ns) * nchunks / ns),
    ...                        subjects=np.arange(ns) / (ns / nsubj / nchunks) % nsubj),
    ...                fa=dict(fidx=np.arange(nf)))
    >>> analyzer=OneWayAnova()
    >>> element_selector=FractionTailSelector(.4, mode='select', tail='upper')
    >>> common=True
    >>> m=SplitSamplesProbabilityMapper(analyzer, 'subjects',
    ...                                 probability_label='fprob',
    ...                                 select_common_features=common,
    ...                                 selector=element_selector)
    >>> m.train(ds)
    >>> y=m(ds)
    >>> z=m(ds.samples)
    >>> np.all(np.equal(z, y.samples))
    True
    >>> y.shape
    (100, 4)

    '''
    def __init__(self,
                 sensitivity_analyzer,
                 split_by_labels,
                 select_common_features=True,
                 probability_label=None,
                 probability_combiner=None,
                 selector=FractionTailSelector(0.05),
                 **kwargs):
        '''
        Parameters
        ----------
        sensitivity_analyzer: FeaturewiseMeasure
            Sensitivity analyzer to come up with sensitivity.
        split_by_labels: str or list of str
            Sample labels on which input datasets are split before
            data is selected.
        select_common_features: bool
            True means that the same features are selected after the split.
        probablity_label: None or str
            If None, then the output dataset ds from the
            sensitivity_analyzer is taken to select the samples.
            If not None it takes ds.sa['probablity_label'].
            For example if sensitivity_analyzer=OneWayAnova then
            probablity_label='fprob' is a sensible value.
        probability_combiner: function
            If select_common_features is True, then this function is
            applied to the feature scores across splits. If None,
            it uses lambda x:np.sum(-np.log(x)) which is sensible if
            the scores are probability values
        selector: Selector
            function that returns the indices to keep.
        '''

        SliceMapper.__init__(self, None, **kwargs)

        if probability_combiner is None:
            def f(x):
                y = -np.log(x.ravel())

                # address potential NaNs
                # set to max value in y
                m = np.isnan(y)
                if np.all(m):
                    return 0 # p=1

                y[m] = np.max(y[np.logical_not(m)])
                return np.sum(y)
            probability_combiner = f # avoid lambda as h5py doesn't like it

        self._sensitivity_analyzer = sensitivity_analyzer
        self._split_by_labels = split_by_labels
        self._select_common_features = select_common_features
        self._probability_label = probability_label
        self._probability_combiner = probability_combiner
        self._selector = selector


    def _train(self, ds):
        # add a sample attribute indicating the sample indices
        # so that we can recover where each part came from
        ds_copy = ds.copy(deep=False)
        ds_copy.sa['orig_fidxs_'] = np.arange(ds.nsamples)

        splits = split_by_sample_attribute(ds_copy,
                                         self._split_by_labels)

        scores_ds = map(self._sensitivity_analyzer, splits)

        if self._probability_label is None:
            scores = [ds.samples for ds in scores_ds]
        else:
            scores = [ds.fa[self._probability_label].value for ds in scores_ds]

        selector = self._selector

        if self._select_common_features:
            # must have the same number of features
            stacked = np.vstack(scores)
            f = self._probability_combiner

            n = stacked.shape[-1] # number of features
            common_all = np.asarray([f(stacked[:, i]) for i in xrange(n)])

            # combine the scores
            common_feature_ids = selector(common_all)

            # same feature ids for each element in split
            feature_ids = [common_feature_ids for _ in splits]
        else:
            # do the selection split=wise
            feature_ids = [selector(score) for score in scores]

        self._slice_feature_ids = feature_ids
        self._slice_sample_ids = [ds.sa.orig_fidxs_ for ds in splits]
        super(SplitSamplesProbabilityMapper, self)._train(ds)

    def _untrain(self):
        self._slice_feature_ids = None
        self._slice_sample_ids = None
        super(SplitSamplesProbabilityMapper, self)._untrain()


    def _forward_dataset(self, ds):
        sliced_ds = [ds[sample_ids, feature_ids]
                            for sample_ids, feature_ids in
                                    zip(*(self._slice_sample_ids,
                                    self._slice_feature_ids))]

        return vstack(sliced_ds, True)


    def _forward_data(self, data):
        sliced_data = [np.vstack(data[sample_id, feature_ids]
                         for sample_id in sample_ids)
                                for sample_ids, feature_ids in
                                    zip(*(self._slice_sample_ids,
                                    self._slice_feature_ids))]

        return vstack(sliced_data)

    sensitivity_analyzer = property(fget=lambda self:self._sensitivity_analyzer,
                                    doc="Measure which was used to do selection")
    selector = property(fget=lambda self:self._selector,
                                    doc="Function used to do selection")

########NEW FILE########
__FILENAME__ = helpers
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helpers for feature selection (scoring, selection strategies)"""

__docformat__ = 'restructuredtext'

from math import floor
import numpy as np

from mvpa2.base.dataset import AttrDataset
from mvpa2.base.state import ClassWithCollections, ConditionalAttribute

if __debug__:
    from mvpa2.base import debug

#
# Functors to be used for FeatureSelection
#

class BestDetector(object):
    """Determine whether the last value in a sequence is the best one given
    some criterion.
    """
    def __init__(self, func=min, lastminimum=False):
        """Initialize with number of steps

        Parameters
        ----------
        fun : functor
          Functor to select the best results. Defaults to min
        lastminimum : bool
          Toggle whether the latest or the earliest minimum is used as
          optimal value to determine the stopping criterion.
        """
        self.__func = func
        self.__lastminimum = lastminimum
        self.__bestindex = None
        """Stores the index of the last detected best value."""


    def __call__(self, errors):
        """Returns True if the last value in `errors` is the best or False
        otherwise.
        """
        isbest = False

        # just to prevent ValueError
        if len(errors)==0:
            return isbest

        minerror = self.__func(errors)

        if self.__lastminimum:
            # make sure it is an array
            errors = np.array(errors)
            # to find out the location of the minimum but starting from the
            # end!
            minindex = np.array((errors == minerror).nonzero()).max()
        else:
            minindex = errors.index(minerror)

        self.__bestindex = minindex

        # if minimal is the last one reported -- it is the best
        if minindex == len(errors)-1:
            isbest = True

        return isbest

    bestindex = property(fget=lambda self:self.__bestindex)



class StoppingCriterion(object):
    """Base class for all functors to decide when to stop RFE (or may
    be general optimization... so it probably will be moved out into
    some other module
    """

    def __call__(self, errors):
        """Instruct when to stop.

        Every implementation should return `False` when an empty list is
        passed as argument.

        Returns tuple `stop`.
        """
        raise NotImplementedError



class MultiStopCrit(StoppingCriterion):
    """Stop computation if the latest error drops below a certain threshold.
    """
    def __init__(self, crits, mode='or'):
        """
        Parameters
        ----------
        crits : list of StoppingCriterion instances
          For each call to MultiStopCrit all of these criterions will
          be evaluated.
        mode : {'and', 'or'}
          Logical function to determine the multi criterion from the set
          of base criteria.
        """
        if not mode in ('and', 'or'):
            raise ValueError, \
                  "A mode %r is not supported." % (mode, )

        self.__mode = mode
        self.__crits = crits


    def __call__(self, errors):
        """Evaluate all criteria to determine the value of the multi criterion.
        """
        # evaluate all crits
        crits = [ c(errors) for c in self.__crits ]

        if self.__mode == 'and':
            return np.all(crits)
        else:
            return np.any(crits)



class FixedErrorThresholdStopCrit(StoppingCriterion):
    """Stop computation if the latest error drops below a certain threshold.
    """
    def __init__(self, threshold):
        """Initialize with threshold.

        Parameters
        ----------
          threshold : float [0,1]
              Error threshold.
        """
        StoppingCriterion.__init__(self)
        if threshold > 1.0 or threshold < 0.0:
            raise ValueError, \
                  "Threshold %f is out of a reasonable range [0,1]." \
                    % threshold
        self.__threshold = threshold


    def __call__(self, errors):
        """Nothing special."""
        if len(errors)==0:
            return False
        if errors[-1] < self.__threshold:
            return True
        else:
            return False


    threshold = property(fget=lambda x:x.__threshold)



class NStepsStopCrit(StoppingCriterion):
    """Stop computation after a certain number of steps.
    """
    def __init__(self, steps):
        """Initialize with number of steps.

        Parameters
        ----------
        steps : int
          Number of steps after which to stop.
        """
        StoppingCriterion.__init__(self)
        if steps < 0:
            raise ValueError, \
                  "Number of steps %i is out of a reasonable range." \
                    % steps
        self.__steps = steps


    def __call__(self, errors):
        """Nothing special."""
        if len(errors) >= self.__steps:
            return True
        else:
            return False


    steps = property(fget=lambda x:x.__steps)



class NBackHistoryStopCrit(StoppingCriterion):
    """Stop computation if for a number of steps error was increasing
    """

    def __init__(self, bestdetector=BestDetector(), steps=10):
        """Initialize with number of steps

        Parameters
        ----------
        bestdetector : BestDetector
          used to determine where the best error is located.
        steps : int
          How many steps to check after optimal value.
        """
        StoppingCriterion.__init__(self)
        if steps < 0:
            raise ValueError, \
                  "Number of steps (got %d) should be non-negative" % steps
        self.__bestdetector = bestdetector
        self.__steps = steps


    def __call__(self, errors):
        stop = False

        # just to prevent ValueError
        if len(errors)==0:
            return stop

        # charge best detector
        self.__bestdetector(errors)

        # if number of elements after the min >= len -- stop
        if len(errors) - self.__bestdetector.bestindex > self.__steps:
            stop = True

        return stop

    steps = property(fget=lambda x:x.__steps)



class ElementSelector(ClassWithCollections):
    """Base class to implement functors to select some elements based on a
    sequence of values.
    """

    ndiscarded = ConditionalAttribute(enabled=True,
        doc="Store number of discarded elements.")

    def __init__(self, mode='discard', **kwargs):
        """
        Parameters
        ----------
         mode : {'discard', 'select'}
            Decides whether to `select` or to `discard` features.
        """
        ClassWithCollections.__init__(self, **kwargs)

        self._set_mode(mode)
        """Flag whether to select or to discard elements."""


    ##REF: Name was automagically refactored
    def _set_mode(self, mode):
        """Choose `select` or `discard` mode."""

        if not mode in ['discard', 'select']:
            raise ValueError, "Unkown selection mode [%s]. Can only be one " \
                              "of 'select' or 'discard'." % mode

        self.__mode = mode


    def __call__(self, seq):
        """
        Parameters
        ----------
        seq
           Sequence based on values of which to perform the selection.
           If `Dataset`, then only 1st sample is taken.
        """
        if isinstance(seq, AttrDataset):
            if len(seq)>1:
                raise ValueError(
                    "Feature selectors cannot handle multiple "
                    "sequences in a Dataset at once.  We got dataset %s "
                    "as input."
                    % (seq,))
            seq = seq.samples[0]
        elif hasattr(seq, 'shape'):
            shape = seq.shape
            if len(shape) > 1:
                raise ValueError(
                    "Feature selectors cannot handle multidimensional "
                    "inputs (such as ndarrays with more than a single "
                    "dimension.  We got %s with shape %s "
                    "as input." % (seq.__class__, shape))
        return self._call(seq)

    def _call(self, seq):
        """Implementations in derived classed have to return a list of selected
        element IDs based on the given sequence.
        """
        raise NotImplementedError

    mode = property(fget=lambda self:self.__mode, fset=_set_mode)


class RangeElementSelector(ElementSelector):
    """Select elements based on specified range of values"""

    def __init__(self, lower=None, upper=None, inclusive=False,
                 mode='select', **kwargs):
        """Initialization `RangeElementSelector`

        Parameters
        ----------
         lower
           If not None -- select elements which are above of
           specified value
         upper
           If not None -- select elements which are lower of
           specified value
         inclusive
           Either to include end points
         mode
           overrides parent's default to be 'select' since it is more
           native for RangeElementSelector
           XXX TODO -- unify??

        `upper` could be lower than `lower` -- then selection is done
        on values <= lower or >=upper (ie tails). This would produce
        the same result if called with flipped values for mode and
        inclusive.

        If no upper no lower is set, assuming upper,lower=0, thus
        outputing non-0 elements
        """

        if lower is None and upper is None:
            lower, upper = 0, 0
            """Lets better return non-0 values if none of bounds is set"""

        # init State before registering anything
        ElementSelector.__init__(self, mode=mode, **kwargs)

        self.__range = (lower, upper)
        """Values on which to base selection"""

        self.__inclusive = inclusive

    def _call(self, seq):
        """Returns selected IDs.
        """
        lower, upper = self.__range
        len_seq = len(seq)
        if not lower is None:
            if self.__inclusive:
                selected = seq >= lower
            else:
                selected = seq > lower
        else:
            selected = np.ones( (len_seq), dtype=np.bool )

        if not upper is None:
            if self.__inclusive:
                selected_upper = seq <= upper
            else:
                selected_upper = seq < upper
            if not lower is None:
                if lower < upper:
                    # regular range
                    selected = np.logical_and(selected, selected_upper)
                else:
                    # outside, though that would be similar to exclude
                    selected = np.logical_or(selected, selected_upper)
            else:
                selected = selected_upper

        if self.mode == 'discard':
            selected = np.logical_not(selected)

        result = np.where(selected)[0]

        if __debug__:
            debug("ES", "Selected %d out of %d elements" %
                  (len(result), len_seq))
        return result

    lower = property(lambda self: self.__range[0])
    upper = property(lambda self: self.__range[1])

class TailSelector(ElementSelector):
    """Select elements from a tail of a distribution.

    The default behaviour is to discard the lower tail of a given distribution.
    """

    # TODO: 'both' to select from both tails
    def __init__(self, tail='lower', sort=True, **kwargs):
        """Initialize TailSelector

        Parameters
        ----------
         tail : ['lower', 'upper']
            Choose the tail to be processed.
         sort : bool
            Flag whether selected IDs will be sorted. Disable if not
            necessary to save some CPU cycles.

        """
        # init State before registering anything
        ElementSelector.__init__(self, **kwargs)

        self._set_tail(tail)
        """Know which tail to select."""

        self.__sort = sort


    ##REF: Name was automagically refactored
    def _set_tail(self, tail):
        """Set the tail to be processed."""
        if not tail in ['lower', 'upper']:
            raise ValueError, "Unkown tail argument [%s]. Can only be one " \
                              "of 'lower' or 'upper'." % tail

        self.__tail = tail


    ##REF: Name was automagically refactored
    def _get_n_elements(self, seq):
        """In derived classes has to return the number of elements to be
        processed given a sequence values forming the distribution.
        """
        raise NotImplementedError


    def _call(self, seq):
        """Returns selected IDs.
        """
        # TODO: Think about selecting features which have equal values but
        #       some are selected and some are not
        len_seq = len(seq)
        # how many to select (cannot select more than available)
        nelements = min(self._get_n_elements(seq), len_seq)

        # make sure that data is ndarray and compute a sequence rank matrix
        # lowest value is first
        seqrank = np.array(seq).argsort()

        if self.mode == 'discard' and self.__tail == 'upper':
            good_ids = seqrank[:-1*nelements]
            self.ca.ndiscarded = nelements
        elif self.mode == 'discard' and self.__tail == 'lower':
            good_ids = seqrank[nelements:]
            self.ca.ndiscarded = nelements
        elif self.mode == 'select' and self.__tail == 'upper':
            good_ids = seqrank[-1*nelements:]
            self.ca.ndiscarded = len_seq - nelements
        else: # select lower tail
            good_ids = seqrank[:nelements]
            self.ca.ndiscarded = len_seq - nelements

        # sort ids to keep order
        # XXX should we do here are leave to other place
        if self.__sort:
            good_ids.sort()

        # only return proper slice args: this is a list of int ids, hence return
        # a list not an array
        return list(good_ids)



class FixedNElementTailSelector(TailSelector):
    """Given a sequence, provide set of IDs for a fixed number of to be selected
    elements.
    """

    def __init__(self, nelements, **kwargs):
        """
        Parameters
        ----------
        nelements : int
          Number of elements to select/discard.
        """
        TailSelector.__init__(self, **kwargs)
        self.__nelements = None
        self._set_n_elements(nelements)


    def __repr__(self):
        return "%s number=%f" % (
            TailSelector.__repr__(self), self.nelements)


    ##REF: Name was automagically refactored
    def _get_n_elements(self, seq):
        return self.__nelements


    ##REF: Name was automagically refactored
    def _set_n_elements(self, nelements):
        if __debug__:
            if nelements <= 0:
                raise ValueError, "Number of elements less or equal to zero " \
                                  "does not make sense."

        self.__nelements = nelements


    nelements = property(fget=lambda x:x.__nelements,
                         fset=_set_n_elements)



class FractionTailSelector(TailSelector):
    """Given a sequence, provide Ids for a fraction of elements
    """

    def __init__(self, felements, **kwargs):
        """
        Parameters
        ----------
         felements : float (0,1.0]
            Fraction of elements to select/discard. Note: Even when 0.0 is
            specified at least one element will be selected.
        """
        TailSelector.__init__(self, **kwargs)
        self._set_f_elements(felements)


    def __repr__(self):
        return "%s fraction=%f" % (
            TailSelector.__repr__(self), self.__felements)


    ##REF: Name was automagically refactored
    def _get_n_elements(self, seq):
        num = int(floor(self.__felements * len(seq)))
        num = max(1, num)               # remove at least 1
        # no need for checks as base class will do anyway
        #return min(num, nselect)
        return num


    ##REF: Name was automagically refactored
    def _set_f_elements(self, felements):
        """What fraction to discard"""
        if felements > 1.0 or felements < 0.0:
            raise ValueError, \
                  "Fraction (%f) cannot be outside of [0.0,1.0]" \
                  % felements

        self.__felements = felements


    felements = property(fget=lambda x:x.__felements,
                         fset=_set_f_elements)




########NEW FILE########
__FILENAME__ = ifs
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Incremental feature search (IFS).

Very similar to Recursive feature elimination (RFE), but instead of begining
with all features and stripping some sequentially, start with an empty feature
set and include important features successively.
"""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.support.copy import copy
from mvpa2.featsel.base import StaticFeatureSelection, IterativeFeatureSelection
from mvpa2.featsel.helpers import NBackHistoryStopCrit, \
                                 FixedNElementTailSelector, \
                                 BestDetector

from mvpa2.base.state import ConditionalAttribute

if __debug__:
    from mvpa2.base import debug


class IFS(IterativeFeatureSelection):
    """Incremental feature search.

    A scalar `Measure` is computed multiple times on variations of a
    certain dataset. These measures are in turn used to incrementally select
    important features. Starting with an empty feature set the dataset measure
    is first computed for each single feature. A number of features is selected
    based on the resulting data measure map (using an `ElementSelector`).

    Next the dataset measure is computed again using each feature in addition
    to the already selected feature set. Again the `ElementSelector` is used to
    select more features.

    For each feature selection the transfer error on some testdatset is
    computed. This procedure is repeated until a given `StoppingCriterion`
    is reached.
    """
    def __init__(self,
                 fmeasure,
                 pmeasure,
                 splitter,
                 fselector=FixedNElementTailSelector(1, tail='upper',
                                                     mode='select'),
                 **kwargs):
        """Initialize incremental feature search

        Parameters
        ----------
        feature_measure : Measure
          Computed for each candidate feature selection. The measure has
          to compute a scalar value.
        performance_measure : Measure
          Compute against a test dataset for each incremental feature
          set.
        splitter: Splitter
          This splitter instance has to generate at least two dataset splits
          when called with the input dataset. The first split serves as the
          training dataset and the second as the evaluation dataset.
        """
        # bases init first
        IterativeFeatureSelection.__init__(self, fmeasure, pmeasure, splitter,
                                           fselector, **kwargs)


    def _train(self, ds):
        # local binding
        fmeasure = self._fmeasure
        fselector = self._fselector
        scriterion = self._stopping_criterion
        bestdetector = self._bestdetector

        # init
        # Computed error for each tested features set.
        errors = []
        # feature candidate are all features in the pattern object
        candidates = range(ds.nfeatures)
        # initially empty list of selected features
        selected = []
        # results in here please
        results = None

        # as long as there are candidates left
        # the loop will most likely get broken earlier if the stopping
        # criterion is reached
        while len(candidates):
            # measures for all candidates
            measures = []

            # for all possible candidates
            for i, candidate in enumerate(candidates):
                if __debug__:
                    debug('IFSC', "Tested %i" % i, cr=True)

                # take the new candidate and all already selected features
                # select a new temporay feature subset from the dataset
                # slice the full dataset, because for the initial iteration
                # steps this will be much mure effecient than splitting the
                # full ds into train and test at first
                fslm = StaticFeatureSelection(selected + [candidate])
                fslm.train(ds)
                candidate_ds = fslm(ds)
                # activate the dataset splitter
                dsgen = self._splitter.generate(candidate_ds)
                # and derived the dataset part that is used for computing the selection
                # criterion
                trainds = dsgen.next()
                # compute data measure on the training part of this feature set
                measures.append(fmeasure(trainds))

            # relies on ds.item() to work properly
            measures = [np.asscalar(m) for m in measures]

            # Select promissing feature candidates (staging)
            # IDs are only applicable to the current set of feature candidates
            tmp_staging_ids = fselector(measures)

            # translate into real candidate ids
            staging_ids = [candidates[i] for i in tmp_staging_ids]

            # mark them as selected and remove from candidates
            selected += staging_ids
            for i in staging_ids:
                candidates.remove(i)

            # actually run the performance measure to estimate "quality" of
            # selection
            fslm = StaticFeatureSelection(selected)
            fslm.train(ds)
            selectedds = fslm(ds)
            # split into train and test part
            trainds, testds = self._get_traintest_ds(selectedds)
            # evaluate and store
            error = self._evaluate_pmeasure(trainds, testds)
            errors.append(np.asscalar(error))
            # intermediate cleanup, so the datasets do not hand around while
            # the next candidate evaluation is computed
            del trainds
            del testds

            # Check if it is time to stop and if we got
            # the best result
            stop = scriterion(errors)
            isthebest = bestdetector(errors)

            if __debug__:
                debug('IFSC',
                      "nselected %i; error: %.4f " \
                      "best/stop=%d/%d\n" \
                      % (len(selected), errors[-1], isthebest, stop),
                      cr=True, lf=True)

            if isthebest:
                # announce desired features to the underlying slice mapper
                # do copy to survive later selections
                self._safe_assign_slicearg(copy(selected))

            # leave the loop when the criterion is reached
            if stop:
                break

        # charge state
        self.ca.errors = errors

########NEW FILE########
__FILENAME__ = rfe
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Recursive feature elimination."""

__docformat__ = 'restructuredtext'

from mvpa2.base.dochelpers import _repr_attrs
from mvpa2.support.copy import copy
from mvpa2.clfs.transerror import ClassifierError
from mvpa2.measures.base import Sensitivity
from mvpa2.featsel.base import IterativeFeatureSelection
from mvpa2.featsel.helpers import BestDetector, \
                                 NBackHistoryStopCrit, \
                                 FractionTailSelector

# For RFELearner
from mvpa2.clfs.meta import ProxyClassifier, FeatureSelectionClassifier
from mvpa2.misc.errorfx import mean_mismatch_error
from mvpa2.measures.base import ProxyMeasure
from mvpa2.generators.splitters import Splitter
from mvpa2.mappers.fx import maxofabs_sample, BinaryFxNode
from mvpa2.base.dochelpers import _str
from mvpa2.generators.base import Repeater


import numpy as np
from mvpa2.base.state import ConditionalAttribute

if __debug__:
    from mvpa2.base import debug

# TODO: Abs value of sensitivity should be able to rule RFE
# Often it is what abs value of the sensitivity is what matters.
# So we should either provide a simple decorator around arbitrary
# FeatureSelector to convert sensitivities to abs values before calling
# actual selector, or a decorator around SensitivityEstimators


class RFE(IterativeFeatureSelection):
    """Recursive feature elimination.

    A `FeaturewiseMeasure` is used to compute sensitivity maps given a
    certain dataset. These sensitivity maps are in turn used to discard
    unimportant features. For each feature selection the transfer error on some
    testdatset is computed. This procedure is repeated until a given
    `StoppingCriterion` is reached.

    References
    ----------
    Such strategy after
      Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene
      selection for cancer classification using support vector
      machines. Mach. Learn., 46(1-3), 389--422.
    was applied to SVM-based analysis of fMRI data in
      Hanson, S. J. & Halchenko, Y. O. (2008). Brain reading using
      full brain support vector machines for object recognition:
      there is no "face identification area". Neural Computation, 20,
      486--503.

    Examples
    --------

    There are multiple possible ways to design an RFE.  Here is one
    example which would rely on a SplitClassifier to extract
    sensitivities and provide estimate of performance (error)

    >>> # Lazy import
    >>> from mvpa2.suite import *
    >>> rfesvm_split = SplitClassifier(LinearCSVMC(), OddEvenPartitioner())
    >>> # design an RFE feature selection to be used with a classifier
    >>> rfe = RFE(rfesvm_split.get_sensitivity_analyzer(
    ...              # take sensitivities per each split, L2 norm, mean, abs them
    ...              postproc=ChainMapper([ FxMapper('features', l2_normed),
    ...                                     FxMapper('samples', np.mean),
    ...                                     FxMapper('samples', np.abs)])),
    ...           # use the error stored in the confusion matrix of split classifier
    ...           ConfusionBasedError(rfesvm_split, confusion_state='stats'),
    ...           # we just extract error from confusion, so need to split dataset
    ...           Repeater(2),
    ...           # select 50% of the best on each step
    ...           fselector=FractionTailSelector(
    ...               0.50,
    ...               mode='select', tail='upper'),
    ...           # and stop whenever error didn't improve for up to 10 steps
    ...           stopping_criterion=NBackHistoryStopCrit(BestDetector(), 10),
    ...           # we just extract it from existing confusion
    ...           train_pmeasure=False,
    ...           # but we do want to update sensitivities on each step
    ...           update_sensitivity=True)
    >>> clf = FeatureSelectionClassifier(
    ...           LinearCSVMC(),
    ...           # on features selected via RFE
    ...           rfe,
    ...           # custom description
    ...           descr='LinSVM+RFE(splits_avg)' )
    
    Note: If you rely on cross-validation for the StoppingCriterion, make sure
    that you have at least 3 chunks so that SplitClassifier could have at least
    2 chunks to split. Otherwise it can not split more (one chunk could not be
    splitted).

    """

    history = ConditionalAttribute(
        doc="Last step # when each feature was still present")
    sensitivities = ConditionalAttribute(enabled=False,
        doc="History of sensitivities (might consume too much memory")

    def __init__(self,
                 fmeasure,
                 pmeasure,
                 splitter,
                 fselector=FractionTailSelector(0.05),
                 update_sensitivity=True,
                 nfeatures_min=0,
                 **kwargs):
        # XXX Allow for multiple stopping criterions, e.g. error not decreasing
        # anymore OR number of features less than threshold
        """Initialize recursive feature elimination

        Parameters
        ----------
        fmeasure : FeaturewiseMeasure
        pmeasure : Measure
          used to compute the transfer error of a classifier based on a
          certain feature set on the test dataset.
          NOTE: If sensitivity analyzer is based on the same
          classifier as transfer_error is using, make sure you
          initialize transfer_error with train=False, otherwise
          it would train classifier twice without any necessity.
        splitter: Splitter
          This splitter instance has to generate at least two dataset splits
          when called with the input dataset. The first split serves as the
          training dataset and the second as the evaluation dataset.
        fselector : Functor
          Given a sensitivity map it has to return the ids of those
          features that should be kept.
        update_sensitivity : bool
          If False the sensitivity map is only computed once and reused
          for each iteration. Otherwise the senstitivities are
          recomputed at each selection step.
        nfeatures_min : int
          Number of features for RFE to stop if reached.
        """
        # bases init first
        IterativeFeatureSelection.__init__(self, fmeasure, pmeasure, splitter,
                                           fselector, **kwargs)

        self.__update_sensitivity = update_sensitivity
        """Flag whether sensitivity map is recomputed for each step."""

        self._nfeatures_min = nfeatures_min


    def __repr__(self, prefixes=[]):
        return super(RFE, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['update_sensitivity'], default=True))


    def _train(self, ds):
        """Proceed and select the features recursively eliminating less
        important ones.

        Parameters
        ----------
        dataset : Dataset
          used to compute sensitivity maps and train a classifier
          to determine the transfer error
        testdataset : Dataset
          used to test the trained classifer to determine the
          transfer error

        Returns a tuple of two new datasets with the feature subset of
        `dataset` that had the lowest transfer error of all tested
        sets until the stopping criterion was reached. The first
        dataset is the feature subset of the training data and the
        second the selection of the test dataset.
        """
        # get the initial split into train and test
        dataset, testdataset = self._get_traintest_ds(ds)

        if __debug__:
            debug('RFEC',
                  "Initiating RFE with training on %s and testing using %s",
                  (dataset, testdataset))
        errors = []
        """Computed error for each tested features set."""

        ca = self.ca
        ca.nfeatures = []
        """Number of features at each step. Since it is not used by the
        algorithm it is stored directly in the conditional attribute"""

        ca.history = np.arange(dataset.nfeatures)
        """Store the last step # when the feature was still present
        """

        ca.sensitivities = []

        stop = False
        """Flag when RFE should be stopped."""

        results = None
        """Will hold the best feature set ever."""

        wdataset = dataset
        """Operate on working dataset initially identical."""

        wtestdataset = testdataset
        """Same feature selection has to be performs on test dataset as well.
        This will hold the current testdataset."""

        step = 0
        """Counter how many selection step where done."""

        orig_feature_ids = np.arange(dataset.nfeatures)
        """List of feature Ids as per original dataset remaining at any given
        step"""

        sensitivity = None
        """Contains the latest sensitivity map."""

        result_selected_ids = orig_feature_ids
        """Resultant ids of selected features. Since the best is not
        necessarily is the last - we better keep this one around. By
        default -- all features are there"""
        selected_ids = result_selected_ids

        isthebest = True
        """By default (e.g. no errors even estimated) every step is the best one
        """

        while wdataset.nfeatures > 0:

            if __debug__:
                debug('RFEC',
                      "Step %d: nfeatures=%d" % (step, wdataset.nfeatures))

            # mark the features which are present at this step
            # if it brings anyb mentionable computational burden in the future,
            # only mark on removed features at each step
            ca.history[orig_feature_ids] = step

            # Compute sensitivity map
            if self.__update_sensitivity or sensitivity == None:
                sensitivity = self._fmeasure(wdataset)
                if len(sensitivity) > 1:
                    raise ValueError(
                            "RFE cannot handle multiple sensitivities at once. "
                            "'%s' returned %i sensitivities."
                            % (self._fmeasure.__class__.__name__,
                               len(sensitivity)))

            if ca.is_enabled("sensitivities"):
                ca.sensitivities.append(sensitivity)

            if self._pmeasure:
                # get error for current feature set (handles optional retraining)
                error = np.asscalar(self._evaluate_pmeasure(wdataset, wtestdataset))
                # Record the error
                errors.append(error)

                # Check if it is time to stop and if we got
                # the best result
                if self._stopping_criterion is not None:
                    stop = self._stopping_criterion(errors)
                if self._bestdetector is not None:
                    isthebest = self._bestdetector(errors)
            else:
                error = None

            nfeatures = wdataset.nfeatures

            if ca.is_enabled("nfeatures"):
                ca.nfeatures.append(wdataset.nfeatures)

            # store result
            if isthebest:
                result_selected_ids = orig_feature_ids

            if __debug__:
                debug('RFEC',
                      "Step %d: nfeatures=%d error=%s best/stop=%d/%d " %
                      (step, nfeatures, error, isthebest, stop))

            # stop if it is time to finish
            if nfeatures == 1 or nfeatures <= self.nfeatures_min or stop:
                break

            # Select features to preserve
            selected_ids = self._fselector(sensitivity)

            if __debug__:
                debug('RFEC_',
                      "Sensitivity: %s, nfeatures_selected=%d, selected_ids: %s" %
                      (sensitivity, len(selected_ids), selected_ids))


            # Create a dataset only with selected features
            wdataset = wdataset[:, selected_ids]

            # select corresponding sensitivity values if they are not
            # recomputed
            if not self.__update_sensitivity:
                sensitivity = sensitivity[selected_ids]

            # need to update the test dataset as well
            # XXX why should it ever become None?
            # yoh: because we can have __transfer_error computed
            #      using wdataset. See xia-generalization estimate
            #      in lightsvm. Or for god's sake leave-one-out
            #      on a wdataset
            # TODO: document these cases in this class
            if not testdataset is None:
                wtestdataset = wtestdataset[:, selected_ids]

            step += 1

            # WARNING: THIS MUST BE THE LAST THING TO DO ON selected_ids
            selected_ids.sort()
            if self.ca.is_enabled("history") \
                   or self.ca.is_enabled('selected_ids'):
                orig_feature_ids = orig_feature_ids[selected_ids]

            # we already have the initial sensitivities, so even for a shared
            # classifier we can cleanup here
            if self._pmeasure:
                self._pmeasure.untrain()

        # charge conditional attributes
        self.ca.errors = errors
        self.ca.selected_ids = result_selected_ids
        if __debug__:
            debug('RFEC',
                  "Selected %d features: %s",
                  (len(result_selected_ids), result_selected_ids))

        # announce desired features to the underlying slice mapper
        # do copy to survive later selections
        self._safe_assign_slicearg(copy(result_selected_ids))
        # call super to set _Xshape etc
        super(RFE, self)._train(dataset)

    def _get_nfeatures_min(self):
        return self._nfeatures_min

    def _set_nfeatures_min(self, v):
        if self.is_trained:
            self.untrain()
        if v < 0:
            raise ValueError("nfeatures_min must not be negative. Got %s" % v)
        self._nfeatures_min = v

    nfeatures_min = property(fget=_get_nfeatures_min, fset=_set_nfeatures_min)
    update_sensitivity = property(fget=lambda self: self.__update_sensitivity)


class SplitRFE(RFE):
    """RFE with the nested cross-validation to estimate optimal number of features.

    Given a learner (classifier) with a sensitivity analyzer and a
    partitioner, during training SplitRFE first performs a
    cross-validation with RFE to later estimate optimal number of
    features which should survive in RFE.  Optimal number is chosen as
    the mid-point among all minimums of the average errors across
    splits.  After deducing optimal number of features, SplitRFE
    applies regular RFE again on the full training dataset stopping at
    the estimated optimal number of features.
    """

    # exclude those since we are really an adapter here
    __init__doc__exclude__ = RFE.__init__doc__exclude__ + \
      ['fmeasure', 'pmeasure', 'splitter',
       'train_pmeasure', 'stopping_criterion',
       'bestdetector',   # now it is a diff strategy
       'nfeatures_min'   # will get 'trained'
       ]

    def __init__(self, lrn, partitioner,
                 fselector,
                 errorfx=mean_mismatch_error,
                 analyzer_postproc=maxofabs_sample(),
                 # callback?
                 **kwargs):
        """
        Parameters
        ----------
        lrn : Learner
          Learner with a sensitivity analyzer which will be used both
          for the sensitivity analysis and transfer error estimation
        partitioner : Partitioner
          Used to generate cross-validation partitions for cross-validation
          to deduce optimal number of features to maintain
        fselector : Functor
          Given a sensitivity map it has to return the ids of those
          features that should be kept.
        errorfx : func, optional
          Functor to use for estimation of cross-validation error
        analyzer_postproc : func, optional
          Function to provide to the sensitivity analyzer as postproc
        """
        # Initialize itself preparing for the 2nd invocation
        # with determined number of nfeatures_min
        fmeasure = lrn.get_sensitivity_analyzer(postproc=analyzer_postproc)

        RFE.__init__(self,
                     fmeasure,
                     None,
                     Repeater(2),
                     fselector=fselector,
                     bestdetector=None,
                     train_pmeasure=False,
                     stopping_criterion=None,
                     **kwargs)
        self._lrn = lrn                   # should not be modified, thus _
        self.partitioner = partitioner
        self.errorfx = errorfx
        self.analyzer_postproc = analyzer_postproc

    def __repr__(self, prefixes=[]):
        return super(SplitRFE, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['lrn', 'partitioner'])
            + _repr_attrs(self, ['errorfx'], default=mean_mismatch_error)
            + _repr_attrs(self, ['analyzer_postproc'], default=maxofabs_sample())
            )


    @property
    def lrn(self):
        return self._lrn

    def _train(self, dataset):
        pmeasure = ProxyMeasure(self.lrn,
                                postproc=BinaryFxNode(self.errorfx,
                                                      self.lrn.space),
                                skip_train=True   # do not train since fmeasure will
                                )

        # First we need to replicate our RFE construct but this time
        # with pmeasure for the classifier
        rfe = RFE(self.fmeasure,
                  pmeasure,
                  Splitter('partitions'),
                  fselector=self.fselector,
                  bestdetector=None,
                  train_pmeasure=False,
                  stopping_criterion=None,   # full "track"
                  update_sensitivity=self.update_sensitivity,
                  enable_ca=['errors', 'nfeatures'])

        errors, nfeatures = [], []

        if __debug__:
            debug("RFEC", "Stage 1: initial nested CV/RFE for %s", (dataset,))

        for partition in self.partitioner.generate(dataset):
            rfe.train(partition)
            errors.append(rfe.ca.errors)
            nfeatures.append(rfe.ca.nfeatures)

        # mean errors across splits and find optimal number
        errors_mean = np.mean(errors, axis=0)
        nfeatures_mean = np.mean(nfeatures, axis=0)
        # we will take the "mean location" of the min to stay
        # within the most 'stable' choice

        mins_idx = np.where(errors_mean==np.min(errors_mean))[0]
        min_idx = mins_idx[int(len(mins_idx)/2)]
        min_error = errors_mean[min_idx]
        assert(min_error == np.min(errors_mean))
        nfeatures_min = nfeatures_mean[min_idx]

        if __debug__:
            debug("RFEC",
                  "Choosing among %d choices to have %d features with "
                  "mean error=%.2g (initial mean error %.2g)",
                  (len(mins_idx), nfeatures_min, min_error, errors_mean[0]))

        self.nfeatures_min = nfeatures_min

        if __debug__:
            debug("RFEC", "Stage 2: running RFE on full training dataset to "
                  "distil best %d features" % nfeatures_min)

        super(SplitRFE, self)._train(dataset)


    def _untrain(self):
        super(SplitRFE, self)._untrain()
        self.nfeatures_min = 0            # reset the knowledge


########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Repeat and filter as sequence of dataset"""

__docformat__ = 'restructuredtext'

import random

import numpy as np

from mvpa2.base.node import Node
from mvpa2.base.collections import \
     SampleAttributesCollection, FeatureAttributesCollection
from mvpa2.base.dochelpers import _str, _repr

if __debug__:
    from mvpa2.base import debug

class Repeater(Node):
    """Node that yields the same dataset for a certain number of repetitions.

    Each yielded dataset has a dataset attribute that identifies the iteration
    (see the ``space`` setting).
    """

    def __init__(self, count, space='repetitons', **kwargs):
        """
        Parameters
        ----------
        count : int
          Positive integer that set the numbed of repetitions.
        space : str
          The name of the dataset attribute that will hold the actual repetiton
          in the yielded datasets.
        """
        Node.__init__(self, space=space, **kwargs)
        self.count = count


    def generate(self, ds):
        """Generate the desired number of repetitions."""
        space = self.get_space()
        for i in xrange(self.count):
            out = ds.copy(deep=False)
            out.a[space] = i
            yield out


    def __str__(self):
        return _str(self, str(self.count))


class Sifter(Node):
    """Exclude (do not generate) provided dataset on the values of the attributes.

    Examples
    --------

    Typical usecase: it is necessary to generate all possible
    combinations of two chunks while being interested only in the
    combinations where both targets are present.

    >>> from mvpa2.datasets import Dataset
    >>> from mvpa2.generators.partition import NFoldPartitioner
    >>> from mvpa2.base.node import ChainNode
    >>> ds = Dataset(samples=np.arange(8).reshape((4,2)),
    ...              sa={'chunks':   [ 0 ,  1 ,  2 ,  3 ],
    ...                  'targets':  ['c', 'c', 'p', 'p']})

    Plain 'NFoldPartitioner(cvtype=2)' would provide also partitions
    with only two 'c's or 'p's present, which we do not want to
    include in our cross-validation since it would break balancing
    between training and testing sets.

    >>> par = ChainNode([NFoldPartitioner(cvtype=2, attr='chunks'),
    ...                  Sifter([('partitions', 2),
    ...                          ('targets', ['c', 'p'])])
    ...                 ], space='partitions')

    We have to provide appropriate 'space' parameter for the
    'ChainNode' so possible future splitting using 'TransferMeasure'
    could operate along that attribute.  Here we just matched default
    space of NFoldPartitioner -- 'partitions'.

    >>> print par
    <ChainNode: <NFoldPartitioner>-<Sifter: partitions=2, targets=['c', 'p']>>

    Additionally, e.g. for cases with cvtype > 2, if balancing is
    needed to be guaranteed (and other generated partitions
    discarded), specification could carry a dict with 'uvalues'
    and 'balanced' keys, e.g.:

    >>> par = ChainNode([NFoldPartitioner(cvtype=2, attr='chunks'),
    ...                  Sifter([('partitions', 2),
    ...                          ('targets', dict(uvalues=['c', 'p'],
    ...                                           balanced=True))])
    ...                 ], space='partitions')

    N.B. In this example it is equivalent to the previous definition
    since things are guaranteed to be balanced with cvtype=2 and 2
    unique values requested.

    >>> for ds_ in par.generate(ds):
    ...     testing = ds[ds_.sa.partitions == 2]
    ...     print list(zip(testing.sa.chunks, testing.sa.targets))
    [(0, 'c'), (2, 'p')]
    [(0, 'c'), (3, 'p')]
    [(1, 'c'), (2, 'p')]
    [(1, 'c'), (3, 'p')]

    """
    def __init__(self, includes, *args, **kwargs):
        """
        Parameters
        ----------
        includes : list
          List of tuples rules (attribute, uvalues) where all
          listed 'uvalues' must be present in the dataset.
          Matching samples or features get selected to proceed to the
          next rule in the list.  If at some point not all listed
          values of the attribute are present, dataset does not pass
          through the 'Sifter'.
          uvalues might also be a `dict`, see example above.
        """
        Node.__init__(self, *args, **kwargs)
        self._includes = includes

    def generate(self, ds):
        """Validate obtained dataset and yield if matches
        """
        # we  start by considering all samples
        sa_mask = np.ones(ds.nsamples, dtype=bool)
        fa_mask = np.ones(ds.nfeatures, dtype=bool)
        # Check the dataset against the rules
        for attrname, crit in self._includes:
            attr, col = ds.get_attr(attrname)

            # figure out which mask and adjust accordingly
            if isinstance(col, SampleAttributesCollection):
                mask = sa_mask
            elif isinstance(col, FeatureAttributesCollection):
                mask = fa_mask
            else:
                raise ValueError(
                    "%s cannot filter based on attribute %s=%s -- "
                    "only collections from .sa or .fa are supported."
                    % (self, attrname, attr))

            uvalues_ = np.unique(attr[mask])

            if not isinstance(crit, dict):
                # so that just a list of unique values to be present specified
                crit = {'uvalues': crit}

            # now it is specified as dictionary with more restrictions
            # XXX sorted/reverse here is just to guarantee that
            #     "uvalues" goes before "balanced".  If we get more
            #     cases -- put proper order here
            for crit_k in sorted(crit.keys(), reverse=True):
                crit_v = crit[crit_k]
                if crit_k.lower() == 'uvalues':
                    # Check if all of those values are present

                    # just to assure consistency in order and type
                    uvalues = np.unique(np.atleast_1d(crit_v))

                    # do matching and reset those not matching
                    mask[np.array([not a in uvalues for a in attr.value])] = False

                    # exit if resultant attributes do no match
                    uvalues_selected = np.unique(attr[mask])

                    # use set() so we could compare results of different lengths as well
                    # and not worry about sorting etc
                    if not (set(uvalues_selected) == set(uvalues) and len(uvalues_selected)):
                        if __debug__ and 'SPL' in debug.active:
                            debug('SPL',
                                  'Skipping dataset %s because selection using %s '
                                  'attribute resulted in the set of values %s while '
                                  'needing %s'
                                  % (ds, attrname, uvalues_selected, uvalues))
                        return

                elif crit_k.lower() == 'balanced':
                    # guarantee that in the given category
                    # TODO: check once again if order of evaluation of
                    # these restrictions matters
                    values_selected = attr[mask]
                    counts = dict((k, 0) for k in np.unique(values_selected))
                    for v in values_selected:
                        counts[v] += 1

                    # bool() to guarantee booleans
                    same_counts = bool(len(np.unique(counts.values())) == 1)
                    crit_v = bool(crit_v)

                    if crit_v != same_counts:
                        if __debug__ and 'SPL' in debug.active:
                            debug('SPL',
                                  'Skipping dataset %s because selection using %s '
                                  'attribute resulted same_counts=%s while balanced=%s'
                                  % (ds, attrname, same_counts, crit_v))
                        return
                else:
                    raise ValueError("Unknown key %s in definition of %s"
                                     % (crit_k, self)) 
                    # print attrname, attr.value, uvalues, uvalues_selected, mask

        yield ds

    def __str__(self):
        return _str(self, ', '.join("%s=%s" % x for x in self._includes))

########NEW FILE########
__FILENAME__ = partition
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Dataset partitioning strategies"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.dochelpers import _repr_attrs
from mvpa2.support.utils import deprecated

from mvpa2.base.node import Node
from mvpa2.datasets.miscfx import coarsen_chunks
import mvpa2.misc.support as support

if __debug__:
    from mvpa2.base import debug


class Partitioner(Node):
    """Generator node to partition a dataset.

    Partitioning is done by adding a sample attribute that assigns samples to an
    arbitrary number of partitions. Subclasses offer a variety of partitioning
    technique that are useful in e.g. cross-validation procedures.

    it is important to note that other than adding a new sample attribute input
    datasets are not modified. In particular, there is no splitting of datasets
    into multiple pieces. If this is desired, a Partitioner can be chained to a
    `Splitter` node to achieve this.
    """

    _STRATEGIES = ('first', 'random', 'equidistant')

    def __init__(self,
                 count=None,
                 selection_strategy='equidistant',
                 attr='chunks',
                 space='partitions',
                 **kwargs):
        """
        Parameters
        ----------
        count : None or int
          Desired number of splits to be output. It is limited by the
          number of splits possible for a given splitter
          (e.g. `OddEvenSplitter` can have only up to 2 splits). If None,
          all splits are output (default).
        selection_strategy : str
          If `count` is not None, possible strategies are possible:
          'first': First `count` splits are chosen;
          'random': Random (without replacement) `count` splits are chosen;
          'equidistant': Splits which are equidistant from each other.
        attr : str
          Sample attribute used to determine splits.
        space : str
          Name of the to be created sample attribute defining the partitions.
          In addition, a dataset attribute named '``space``\_set' will be added
          to each output dataset, indicating the number of the partition set
          it corresponds to.
        """
        Node.__init__(self, space=space, **kwargs)
        # pylint happyness block
        self.__attr = attr
        # we don't check it, thus no reason to make it private.
        # someone might find it useful to change post creation
        # TODO utilize such (or similar) policy through out the code
        self.count = count
        self._set_selection_strategy(selection_strategy)


    def __repr__(self, prefixes=[]):
        # Here we are jumping over Node's __repr__ since
        # it would enforce placing space
        return super(Node, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['count'])
            + _repr_attrs(self, ['selection_strategy'], default='equidistant')
            + _repr_attrs(self, ['attr'], default='chunks')
            + _repr_attrs(self, ['space'], default='partitions')
            )


    def _set_selection_strategy(self, strategy):
        """Set strategy to select splits out from available
        """
        strategy = strategy.lower()
        if not strategy in self._STRATEGIES:
            raise ValueError, "selection_strategy is not known. Known are %s" \
                  % str(self._STRATEGIES)
        self.__selection_strategy = strategy


    def _get_partition_specs(self, uniqueattr):
        """Return list with samples of 2nd dataset in a split.

        Each subclass has to implement this method. It gets a sequence with
        the unique attribute ids of a dataset and has to return a list of lists
        containing sample ids to split into the second dataset.
        """
        raise NotImplementedError


    def generate(self, ds):
        # for each split
        cfgs = self.get_partition_specs(ds)
        n_cfgs = len(cfgs)

        for iparts, parts in enumerate(cfgs):
            # give attribute array defining the current partition set
            pattr = self.get_partitions_attr(ds, parts)
            # shallow copy of the dataset
            pds = ds.copy(deep=False)
            pds.sa[self.get_space()] = pattr
            pds.a[self.get_space() + "_set"] = iparts
            pds.a['lastpartitionset'] = iparts == (n_cfgs - 1)
            yield pds


    def get_partitions_attr(self, ds, specs):
        """Create a partition attribute array for a particular partion spec.

        Parameters
        ----------
        ds : Dataset
          This is this source dataset.
        specs : sequence of sequences
          Contains ids of a sample attribute that shall go into each partition.

        Returns
        -------
        array(ints)
          Each partition is represented by a unique integer value.
        """
        # collect the sample ids for each resulting dataset
        filters = []
        none_specs = 0
        cum_filter = None

        splitattr_data = ds.sa[self.__attr].value
        # for each partition in this set
        for spec in specs:
            if spec is None:
                filters.append(None)
                none_specs += 1
            else:
                filter_ = np.array([ i in spec \
                                    for i in splitattr_data], dtype='bool')
                filters.append(filter_)
                if cum_filter is None:
                    cum_filter = filter_
                else:
                    cum_filter = np.logical_and(cum_filter, filter_)

        # need to turn possible Nones into proper ids sequences
        if none_specs > 1:
            raise ValueError("'%s' cannot handle more than one `None` " \
                              "partition spec." % self.__class__.__name__)

        # go with ints for simplicity. By default the attr is zeros, and the
        # first configured partition starts with one.
        part_attr = np.zeros(len(ds), dtype='int')
        for i, filter_ in enumerate(filters):
            # turn the one 'all the rest' filter into a slicing arg
            if filter_ is None:
                filter_ = np.logical_not(cum_filter)
            # now filter is guaranteed to be a slicing argument that can be used
            # to assign the attribute values
            part_attr[filter_] = i + 1
        return part_attr


    def get_partition_specs(self, ds):
        """Returns the specs for all to be generated partition sets.

        Returns
        -------
        list(lists)
        """
        # list (#splits) of lists (#partitions)
        cfgs = self._get_partition_specs(ds.sa[self.__attr].unique)

        # Select just some splits if desired
        count, n_cfgs = self.count, len(cfgs)

        # further makes sense only if count < n_cfgs,
        # otherwise all strategies are equivalent
        if count is not None and count < n_cfgs:
            if count < 1:
                # we can only wish a good luck
                return []
            strategy = self.selection_strategy
            if strategy == 'first':
                cfgs = cfgs[:count]
            elif strategy in ['equidistant', 'random']:
                if strategy == 'equidistant':
                    # figure out what step is needed to
                    # accommodate the `count` number
                    step = float(n_cfgs) / count
                    assert(step >= 1.0)
                    indexes = [int(round(step * i)) for i in xrange(count)]
                elif strategy == 'random':
                    indexes = np.random.permutation(range(n_cfgs))[:count]
                    # doesn't matter much but lets keep them in the original
                    # order at least
                    indexes.sort()
                else:
                    # who said that I am paranoid?
                    raise RuntimeError, "Really should not happen"
                if __debug__:
                    debug("SPL", "For %s selection strategy selected %s "
                          "partition specs from %d total"
                          % (strategy, indexes, n_cfgs))
                cfgs = [cfgs[i] for i in indexes]

        return cfgs

    @property
    @deprecated("to be removed in PyMVPA 2.1; use .attr instead")
    def splitattr(self):
        return self.attr

    selection_strategy = property(fget=lambda self:self.__selection_strategy,
                        fset=_set_selection_strategy)
    attr = property(fget=lambda self: self.__attr)



class OddEvenPartitioner(Partitioner):
    """Create odd and even partitions based on a sample attribute.

    The partitioner yields two datasets. In the first set all odd chunks are
    labeled '1' and all even runs are labeled '2'. In the second set the
    assignment is reversed (odd: '2', even: '1').
    """
    def __init__(self, usevalues=False, **kwargs):
        """
        Parameters
        ----------
        usevalues : bool
          If True the values of the attribute used for partitioning will be
          used to determine odd and even samples. If False odd and even
          chunks are defined by the order of attribute values, i.e. first
          unique attribute is odd, second is even, despite the
          corresponding values might indicate the opposite (e.g. in case
          of [2,3].
        """
        Partitioner.__init__(self, **(kwargs))
        self.__usevalues = usevalues

    def __repr__(self, prefixes=[]):
        return super(OddEvenPartitioner, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['usevalues'], default=False))

    def _get_partition_specs(self, uniqueattrs):
        """
        Returns
        -------
        list of tuples (None, list of int)
          2 items: odd samples into 1st split
        """
        if self.__usevalues:
            return [(None, uniqueattrs[(uniqueattrs % 2) == True]),
                    (None, uniqueattrs[(uniqueattrs % 2) == False])]
        else:
            return [(None, uniqueattrs[np.arange(len(uniqueattrs)) %2 == True]),
                    (None, uniqueattrs[np.arange(len(uniqueattrs)) %2 == False])]


    usevalues = property(fget=lambda self: self.__usevalues)


class HalfPartitioner(Partitioner):
    """Partition a dataset into two halves of the sample attribute.

    The partitioner yields two datasets. In the first set second half of
    chunks are labeled '1' and the first half labeled '2'. In the second set the
    assignment is reversed (1st half: '1', 2nd half: '2').
    """
    def _get_partition_specs(self, uniqueattrs):
        """
        Returns
        -------
        list of tuples (None, list of int)
          2 items: first half of samples into 1st split
        """
        return [(None, uniqueattrs[:len(uniqueattrs)/2]),
                (None, uniqueattrs[len(uniqueattrs)/2:])]



class NGroupPartitioner(Partitioner):
    """Partition a dataset into N-groups of the sample attribute.

    For example, NGroupPartitioner(2) is the same as the HalfPartitioner and
    yields exactly the same partitions and labeling patterns.
    """
    def __init__(self, ngroups=4, **kwargs):
        """
        Parameters
        ----------
        ngroups : int
          Number of groups to split the attribute into.
        """
        Partitioner.__init__(self, **(kwargs))
        self.__ngroups = ngroups


    def __repr__(self, prefixes=[]):
        return super(NGroupPartitioner, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['ngroups'], default=4))

    def _get_partition_specs(self, uniqueattrs):
        """
        Returns
        -------
        list of tuples (None, list of int)
          Indices for splitting
        """

        # make sure there are more of attributes than desired groups
        if len(uniqueattrs) < self.__ngroups:
            raise ValueError("Number of groups (%d) " % (self.__ngroups) + \
                  "must be less than " + \
                  "or equal to the number of unique attributes (%d)" % \
                  (len(uniqueattrs)))

        # use coarsen_chunks to get the split indices
        split_ind = coarsen_chunks(uniqueattrs, nchunks=self.__ngroups)
        split_ind = np.asarray(split_ind)

        # loop and create splits
        split_list = [(None, uniqueattrs[split_ind==i])
                       for i in range(self.__ngroups)]
        return split_list

    ngroups = property(fget=lambda self: self.__ngroups)


class CustomPartitioner(Partitioner):
    """Partition a dataset using an arbitrary custom rule.

    The partitioner is configured by passing a custom rule (``splitrule``) to its
    constructor. Such a rule is basically a sequence of partition definitions.
    Every single element in this sequence results in exactly one partition set.
    Each element is another sequence of attribute values whose corresponding
    samples shall go into a particular partition.

    Examples
    --------
    Generate two sets. In the first set the *second* partition
    contains all samples with sample attributes corresponding to
    either 0, 1 or 2. The *first* partition of the first set contains
    all samples which are not part of the second partition.

    The second set yields three partitions. The first with all samples
    corresponding to sample attributes 1 and 2, the second contains only
    samples with attribute 3 and the last contains the samples with attribute 5
    and 6.

    >>> ptr = CustomPartitioner([(None, [0, 1, 2]), ([1,2], [3], [5, 6])])

    The numeric labels of all partitions correspond to their position in the
    ``splitrule`` of a particular set. Note that the actual labels start with
    '1' as all unselected elements are labeled '0'.
    """
    def __init__(self, splitrule, **kwargs):
        """
        Parameters
        ----------
        splitrule : list of tuple
          Custom partition set specs.
        """
        Partitioner.__init__(self, **(kwargs))
        self.splitrule = splitrule


    def __repr__(self, prefixes=[]):
        return super(CustomPartitioner, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['splitrule']))


    def _get_partition_specs(self, uniqueattrs):
        """
        Returns
        -------
        whatever was provided in splitrule argument
        """
        return self.splitrule


class NFoldPartitioner(Partitioner):
    """Generic N-fold data partitioner.

    Given a dataset with N chunks, with ``cvtype`` = 1 (which is default), it
    would generate N partition sets, where each chunk is sequentially taken out
    (with replacement) to form a second partition, while all other samples
    together form the first partition.  Example, if there are 4 chunks, partition
    sets for ``cvtype`` = 1 are::

        [[1, 2, 3], [0]]
        [[0, 2, 3], [1]]
        [[0, 1, 3], [2]]
        [[0, 1, 2], [3]]

    If ``cvtype``>1, then all possible combinations of ``cvtype`` number of
    chunks are taken out, so for ``cvtype`` = 2 in previous example yields::

        [[2, 3], [0, 1]]
        [[1, 3], [0, 2]]
        [[1, 2], [0, 3]]
        [[0, 3], [1, 2]]
        [[0, 2], [1, 3]]
        [[0, 1], [2, 3]]

    Note that the "taken-out" partition is always labeled '2' while the
    remaining elements are labeled '1'.

    If ``cvtype`` is a float in the range from 0 to 1, it specifies
    the ratio of present unique values to be taken.

    If ``cvtype`` is large enough generating prohibitively large
    number of combinations, provide ``count`` to limit number of
    combinations and provide ``selection_strategy`` = 'random'.
    """

    _DEV__doc__ = """
    Might want to make it smarter and implement generate() generator?
    Especially for the cases which use xrandom_unique_combinations

    All needed machinery is there
    """

    def __init__(self, cvtype=1, **kwargs):
        """
        Parameters
        ----------
        cvtype : int, float
          Type of leave-one-out scheme: N-(cvtype).  float value
          (0..1) specifies ratio of samples to be taken into the
          combination (e.g. 0.5 for 50%) given a dataset
        """
        Partitioner.__init__(self, **kwargs)
        if isinstance(cvtype, float):
            # some checks
            if not (0 < cvtype < 1):
                raise ValueError("Float value for cvtype must be within range "
                                 "(0, 1), excluding boundaries. Got %r."
                                 % cvtype)
        self.cvtype = cvtype

    def __repr__(self, prefixes=[]): #pylint: disable-msg=W0102
        return super(NFoldPartitioner, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['cvtype'], default=1))


    def _get_partition_specs(self, uniqueattrs):
        if isinstance(self.cvtype, float):
            n = int(self.cvtype * len(uniqueattrs))
        else:
            n = self.cvtype
        if self.count is None \
           or self.selection_strategy != 'random' \
           or self.count >= support.ncombinations(len(uniqueattrs), n):
            # all combinations were requested so no need for
            # randomization
            combs = support.xunique_combinations(uniqueattrs, n)
        else:
            # due to selection_strategy=random they would be also
            # reshuffled by super class later on but that should be ok
            combs = support.xrandom_unique_combinations(uniqueattrs, n,
                                                        self.count)

        if self.count is None or self.selection_strategy != 'random':
            # we are doomed to return all of them
            return [(None, i) for i in combs]
        else:
            # It makes sense to limit number of returned combinations
            # right away
            return [(None, i) for ind, i in enumerate(combs)
                    if ind < self.count]


class ExcludeTargetsCombinationsPartitioner(Node):
    """Exclude combinations for a given partition from other partitions

    Given a pre-generated partitioning this generates new partitions
    by selecting all possible combinations of k-targets from all
    targets and excluding samples with the selected k-targets from
    training partition for each combination.

    A simple example would be:

    Examples
    --------

    For a dataset with 3 classes with one sample per class, k=2 gives
    3 combination partitions with 2 samples for testing and one sample
    for training (since it excludes the 2 selected target samples) per
    partition.

    >>> from mvpa2.base.node import ChainNode
    >>> partitioner = ChainNode([NFoldPartitioner(),
    ...                          ExcludeTargetsCombinationsPartitioner(
    ...                             k=2,
    ...                             targets_attr='targets',
    ...                             space='partitions')],
    ...                         space='partitions')


    While cross-validating across subjects (e.g. working with
    hyperaligned data), to avoid significant bias due to matching
    trial-order effects instead of categorical boundaries, it is
    important to exclude from training chunks with the order matching
    the ones in testing.

    >>> partitioner = ChainNode([NFoldPartitioner(attr='subject'),
    ...                          ExcludeTargetsCombinationsPartitioner(
    ...                             k=1,
    ...                             targets_attr='chunks',
    ...                             space='partitions')],
    ...                         space='partitions')

    """
    def __init__(self, k,
                 targets_attr,
                 partitions_attr='partitions',
                 partitions_keep=2,  # default for testing partition
                 partition_assign=3, # assign one which Splitter doesn't even get to
                 **kwargs):
        Node.__init__(self, **kwargs)
        self.k = k
        self.targets_attr = targets_attr
        self.partitions_attr = partitions_attr
        self.partitions_keep = partitions_keep
        self.partition_assign = partition_assign

    def __repr__(self, prefixes=[]):
        # Here we are jumping over Node's __repr__ since
        # it would enforce placing space
        return super(ExcludeTargetsCombinationsPartitioner, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['k', 'targets_attr'])
            + _repr_attrs(self, ['partitions_attr'], default='partitions')
            + _repr_attrs(self, ['partitions_keep'], default=2)
            + _repr_attrs(self, ['partition_assign'], default=3)
            )

    def generate(self, ds):
        orig_partitioning = ds.sa[self.partitions_attr].value.copy()
        targets = ds.sa[self.targets_attr].value

        testing_part = orig_partitioning == self.partitions_keep
        nontesting_part = np.logical_not(testing_part)

        utargets = np.unique(targets[testing_part])
        for combination in support.xunique_combinations(utargets, self.k):
            partitioning = orig_partitioning.copy()
            combination_matches = [ t in combination for t in targets ]
            combination_nonmatches = np.logical_not(combination_matches)

            partitioning[np.logical_and(testing_part,
                                        combination_nonmatches)] \
                        = self.partition_assign
            partitioning[np.logical_and(nontesting_part,
                                        combination_matches)] \
                        = self.partition_assign
            pds = ds.copy(deep=False)
            pds.sa[self.space] = partitioning
            yield pds

########NEW FILE########
__FILENAME__ = permutation
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Generator nodes to permute datasets.
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.dochelpers import _repr_attrs

from mvpa2.base.node import Node
from mvpa2.base.dochelpers import _str, _repr
from mvpa2.misc.support import get_limit_filter

from mvpa2.support.utils import deprecated
from mvpa2.mappers.fx import _product

if __debug__:
    from mvpa2.base import debug

class AttributePermutator(Node):
    """Node to permute one a more attributes in a dataset.

    This node can permute arbitrary sample or feature attributes in a dataset.
    Moreover, it supports limiting the permutation to a subset of samples or
    features (see ``limit`` argument). The node can simply be called with a
    dataset for a one time permutation, or used as a generator to produce
    multiple permutations.

    This node only permutes dataset attributes, dataset samples are no affected.
    The permuted output dataset shares the samples container with the input
    dataset.
    """
    def __init__(self, attr, count=1, limit=None, assure=False,
                 strategy='simple', rng=np.random, **kwargs):
        """
        Parameters
        ----------
        attr : str or list(str)
          Name of the to-be-permuted attribute. This can also be a list of
          attribute names, in which case the *identical* shuffling is applied to
          all listed attributes.
        count : int
          Number of permutations to be yielded by .generate()
        limit : None or str or dict
          If ``None`` all attribute values will be permuted. If an single
          attribute name is given, its unique values will be used to define
          chunks of data that are permuted individually (i.e. no attributed
          values will be replaced across chunks). Finally, if a dictionary is
          provided, its keys define attribute names and its values (single value
          or sequence thereof) attribute value, where all key-value combinations
          across all given items define a "selection" of to-be-permuted samples
          or features.
        strategy : 'simple', 'uattrs'
          'simple' strategy is the straightfoward permutation of attributes (given
          the limit).  In some sense it assumes independence of those samples.
          'uattrs' strategy looks at unique values of attr (or their unique
          combinations in case of `attr` being a list), and "permutes" those
          unique combinations values thus breaking their assignment to the samples
          but preserving any dependencies between samples within the same unique
          combination.
        assure : bool
          If set, by-chance non-permutations will be prevented, i.e. it is
          checked that at least two items change their position. Since this
          check adds a runtime penalty it is off by default.
        """
        Node.__init__(self, **kwargs)
        self._pattr = attr

        self.count = count
        self._limit = limit
        self._pcfg = None
        self._assure_permute = assure
        self.strategy = strategy
        self.rng = rng


    def _get_pcfg(self, ds):
        # determine to be permuted attribute to find the collection
        pattr = self._pattr
        if isinstance(pattr, str):
            pattr, collection = ds.get_attr(pattr)
        else:
            # must be sequence of attrs, take first since we only need the shape
            pattr, collection = ds.get_attr(pattr[0])

        return get_limit_filter(self._limit, collection)


    def _call(self, ds):
        # local binding
        pattr = self._pattr
        assure_permute = self._assure_permute

        # get permutation setup if not set already (maybe from generate())
        if self._pcfg is None:
            pcfg = self._get_pcfg(ds)
        else:
            pcfg = self._pcfg

        if isinstance(pattr, str):
            # wrap single attr name into tuple to simplify the code
            pattr = (pattr,)

        # get actual attributes
        in_pattrs = [ds.get_attr(pa)[0] for pa in pattr]

        # Method to use for permutations
        try:
            permute_fx = getattr(self, "_permute_%s" % self.strategy)
        except AttributeError:
            raise ValueError("Unknown permutation strategy %r" % self.strategy)

        for i in xrange(10):  # for the case of assure_permute
            # shallow copy of the dataset for output
            out = ds.copy(deep=False)

            out_pattrs = [out.get_attr(pa)[0] for pa in pattr]
            # replace .values with copies in out_pattrs so we do
            # not override original values
            for pa in out_pattrs:
                pa.value = pa.value.copy()

            for limit_value in np.unique(pcfg):
                if pcfg.dtype == np.bool:
                    # simple boolean filter -> do nothing on False
                    if not limit_value:
                        continue
                    # otherwise get indices of "selected ones"
                    limit_idx = pcfg.nonzero()[0]
                else:
                    # non-boolean limiter -> determine "chunk" and permute within
                    limit_idx = (pcfg == limit_value).nonzero()[0]

                # need list to index properly
                limit_idx = list(limit_idx)

                permute_fx(limit_idx, in_pattrs, out_pattrs)

            if not assure_permute:
                break

            # otherwise check if we differ from original, and if so -- break
            differ = False
            for in_pattr, out_pattr in zip(in_pattrs, out_pattrs):
                differ = differ or np.any(in_pattr.value != out_pattr.value)
                if differ: break                 # leave check loop if differ
            if differ: break                     # leave 10 loop, otherwise go to the next round

        if assure_permute and not differ:
            raise RuntimeError(
                "Cannot assure permutation of %s with limit %r for "
                "some reason (dataset %s). Should not happen"
                % (pattr, self._limit, ds))            

        return out


    def _permute_simple(self, limit_idx, in_pattrs, out_pattrs):
        """The simplest permutation
        """
        perm_idx = self.rng.permutation(limit_idx)

        if __debug__:
            debug('APERM', "Obtained permutation %s", (perm_idx, ))

        # for all to be permuted attrs
        for in_pattr, out_pattr in zip(in_pattrs, out_pattrs):
            # replace all values in current limit with permutations
            # of the original ds's attributes
            out_pattr.value[limit_idx] = in_pattr.value[perm_idx]


    def _permute_uattrs(self, limit_idx, in_pattrs, out_pattrs):
        """Provide a permutation given a specified strategy
        """
        # Select given limit_idx
        pattrs_lim = [p.value[limit_idx] for p in in_pattrs]
        # convert to list of tuples
        pattrs_lim_zip = zip(*pattrs_lim)
        # find unique groups
        unique_groups = list(set(pattrs_lim_zip))
        # now we need to permute the groups to generate remapping
        # get permutation indexes first
        perm_idx = self.rng.permutation(np.arange(len(unique_groups)))
        # generate remapping
        remapping = dict([(t, unique_groups[i])
                          for t, i in zip(unique_groups, perm_idx)])
        if __debug__:
            debug('APERM', "Using remapping %s", (remapping,))

        for i, in_group in zip(limit_idx, pattrs_lim_zip):
            out_group = remapping[in_group]
            # now we need to assign them ot out_pattrs
            for pa, out_v in zip(out_pattrs, out_group):
                pa.value[i] = out_v


    def generate(self, ds):
        """Generate the desired number of permuted datasets."""
        # figure out permutation setup once for all runs
        self._pcfg = self._get_pcfg(ds)
        # permute as often as requested
        for i in xrange(self.count):
            ## if __debug__:
            ##     debug('APERM', "%s generating %i-th permutation", (self, i))
            yield self(ds)

        # reset permutation setup to do the right thing upon next call to object
        self._pcfg = None


    def __str__(self):
        return _str(self, self._pattr, n=self.count, limit=self._limit,
                    assure=self._assure_permute)

    def __repr__(self, prefixes=[]):
        return super(AttributePermutator, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['attr'])
            + _repr_attrs(self, ['count'], default=1)
            + _repr_attrs(self, ['limit'])
            + _repr_attrs(self, ['assure'], default=False)
            + _repr_attrs(self, ['strategy'], default='permute')
            + _repr_attrs(self, ['rng'], default=np.random)
            )

    @property
    @deprecated("to be removed in 2.1 -- use .count instead")
    def nruns(self):
        return self.count

    attr = property(fget=lambda self: self._pattr)
    limit = property(fget=lambda self: self._limit)
    assure = property(fget=lambda self: self._assure_permute)

########NEW FILE########
__FILENAME__ = resampling
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Dataset content resampling (e.g. balance number of samples per condition)"""

__docformat__ = 'restructuredtext'

import random

import numpy as np

from mvpa2.base.node import Node
from mvpa2.base.dochelpers import _str, _repr
from mvpa2.misc.support import get_limit_filter, get_nelements_per_value


class Balancer(Node):
    """Generator to (repeatedly) select subsets of a dataset.

    The Balancer can equalize the number of samples/features in a dataset, or
    select an absolute number or fraction of all available data. Selection is
    performed given a particular attribute and additionally can be limited to
    a subset of the dataset defined by more complex criteria (see ``limit``
    argument). The node can either "mark" elements as selected by adding a
    corresponding attribute to the output dataset, or actually apply the
    selection by returning a new dataset with only selected elements.
    """
    def __init__(self,
                 amount='equal',
                 attr='targets',
                 count=1,
                 limit='chunks',
                 apply_selection=False,
                 include_offlimit=False,
                 space='balanced_set',
                 **kwargs):
        """
        Parameters
        ----------
        amount : {'equal'} or int or float
          Specify the amount of elements to be selected (within the current
          ``limit``). The amount can be given as an integer value corresponding
          to the absolute number of elements per unique attribute (see ``attr``)
          value, as a float corresponding to the fraction of elements, or with
          the keyword 'equal'. In the latter case the number of to be selected
          elements is determined by the least number of available elements for
          any given unique attribute value within the current limit.
        attr : str
          Dataset attribute whose unique values define element classes that are
          to be balanced in number.
        count : int
          How many iterations to perform on ``generate()``.
        limit : None or str or dict
          If ``None`` the whole dataset is considered as one. If a single
          attribute name is given, its unique values will be used to define
          chunks of data that are balanced individually. Finally, if a
          dictionary is provided, its keys define attribute names and its values
          (single value or sequence thereof) attribute value, where all
          key-value combinations across all given items define a "selection" of
          to-be-balanced samples or features.
        apply_selection : bool
          Flag whether the balanced selection shall be applied, i.e. the output
          dataset only contains selected elements. If False, the selection is
          instead added as an attribute that merely marks selected elements (see
          ``space`` argument).
        include_offlimit : bool
          If True, all samples that were off limit (i.e. not included in the
          balancing input are included in the balanced selection. If False
          (default) they are excluded.
        space : str
          Name of the selection marker attribute in the output dataset that is
          created if the balanced selection is not applied to the output dataset
          (see ``apply_selection`` argument).
        """
        Node.__init__(self, space=space, **kwargs)
        self._amount = amount
        self._attr = attr
        self.count = count
        self._limit = limit
        self._limit_filter = None
        self._include_offlimit = include_offlimit
        self._apply_selection = apply_selection


    def _call(self, ds):
        # local binding
        amount = self._amount
        attr, collection = ds.get_attr(self._attr)

        # get filter if not set already (maybe from generate())
        if self._limit_filter is None:
            limit_filter = get_limit_filter(self._limit, collection)
        else:
            limit_filter = self._limit_filter

        # ids of elements that are part of the balanced set
        balanced_set = []
        full_limit_set = []
        # for each chunk in the filter (might be just the selected ones)
        for limit_value in np.unique(limit_filter):
            if limit_filter.dtype == np.bool:
                # simple boolean filter -> do nothing on False
                if not limit_value:
                    continue
                # otherwise get indices of "selected ones"
                limit_idx = limit_filter.nonzero()[0]
            else:
                # non-boolean limiter -> determine "chunk" and balance within
                limit_idx = (limit_filter == limit_value).nonzero()[0]
            full_limit_set += list(limit_idx)

            # apply the current limit to the target attribute
            # need list to index properly
            attr_limited = attr[list(limit_idx)]
            uattr_limited = np.unique(attr_limited)

            # handle all types of supported arguments
            if amount == 'equal':
                # go for maximum possible number of samples provided
                # by each label in this dataset
                # determine the min number of samples per class
                epa = get_nelements_per_value(attr_limited)
                min_epa = min(epa.values())
                for k in epa:
                    epa[k] = min_epa
            elif isinstance(amount, float):
                epa = get_nelements_per_value(attr_limited)
                for k in epa:
                    epa[k] = int(round(epa[k] * amount))
            elif isinstance(amount, int):
                epa = dict(zip(uattr_limited, [amount] * len(uattr_limited)))
            else:
                raise ValueError("Unknown type of amount argument '%s'" % amount)

            # select determined number of elements per unique attribute value
            selected = []
            for ua in uattr_limited:
                selected += random.sample(list((attr_limited == ua).nonzero()[0]),
                                          epa[ua])

            # determine the final indices of selected elements and store
            # as part of the balanced set
            balanced_set += list(limit_idx[selected])

        # make full-sized boolean selection attribute and put it into
        # the right collection of the output dataset
        if self._include_offlimit:
            # start with all-in
            battr = np.ones(len(attr), dtype=np.bool)
            # throw out all samples that could have been limited
            battr[full_limit_set] = False
            # put back the ones that got into the balanced set
            battr[balanced_set] = True
        else:
            # start with nothing
            battr = np.zeros(len(attr), dtype=np.bool)
            # only keep the balanced set
            battr[balanced_set] = True

        if self._apply_selection:
            if collection is ds.sa:
                return ds[battr]
            elif collection is ds.fa:
                return ds[:, battr]
            else:
                # paranoid
                raise RuntimeError(
                        "Don't know where this collection comes from. "
                        "This should never happen!")
        else:
            # shallow copy of the dataset for output
            out = ds.copy(deep=False)
            if collection is ds.sa:
                out.sa[self.get_space()] = battr
            elif collection is ds.fa:
                out.fa[self.get_space()] = battr
            else:
                # paranoid
                raise RuntimeError(
                        "Don't know where this collection comes from. "
                        "This should never happen!")
            return out


    def generate(self, ds):
        """Generate the desired number of balanced datasets datasets."""
        # figure out filter for all runs at once
        attr, collection = ds.get_attr(self._attr)
        self._limit_filter = get_limit_filter(self._limit, collection)
        # permute as often as requested
        for i in xrange(self.count):
            yield self(ds)

        # reset filter to do the right thing upon next call to object
        self._limit_filter = None


    def __str__(self):
        return _str(self, str(self._amount), n=self._attr, count=self.count,
                    apply_selection=self._apply_selection)

########NEW FILE########
__FILENAME__ = splitters
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Split a single input dataset into multiple parts"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.node import Node
from mvpa2.base import warning
from mvpa2.misc.support import mask2slice

if __debug__:
    from mvpa2.base import debug

class Splitter(Node):
    """Generator node for dataset splitting.

    The splitter is configured with the name of an attribute. When its
    ``generate()`` methods is called with a dataset, it subsequently yields
    all possible subsets of this dataset, by selecting all dataset
    samples/features corresponding to a particular attribute value, for all
    unique attribute values.

    Dataset splitting is possible by sample attribute, or by feature attribute.
    The maximum number of splits can be limited, and custom attribute values
    may be provided.
    """
    def __init__(self, attr, attr_values=None, count=None, noslicing=False,
                 reverse=False, ignore_values=None, **kwargs):
        """
        Parameters
        ----------
        attr : str
          Typically the sample or feature attribute used to determine splits.
        attr_values : tuple
          If not None, this is a list of value of the ``attr`` used to determine
          the splits. The order of values in this list defines the order of the
          resulting splits. It is possible to specify a particular value
          multiple times. All dataset samples with values that are not listed
          are going to be ignored.
        count : None or int
          Desired number of generated splits. If None, all splits are output
          (default), otherwise the number of splits is limited to the given
          ``count`` or the maximum number of possible split (whatever is less).
        noslicing : bool
          If True, dataset splitting is not done by slicing (causing
          shared data between source and split datasets) even if it would
          be possible. By default slicing is performed whenever possible
          to reduce the memory footprint.
        reverse : bool
          If True, the order of datasets in the split is reversed, e.g.
          instead of (training, testing), (training, testing) will be spit
          out.
        ignore_values : tuple
          If not None, this is a list of value of the ``attr`` the shall be
          ignored when determining the splits. This settings also affects
          any specified ``attr_values``.
        """
        Node.__init__(self, space=attr, **kwargs)
        self.__splitattr_values = attr_values
        self.__splitattr_ignore = ignore_values
        self.__count = count
        self.__noslicing = noslicing
        self.__reverse = reverse


    def generate(self, ds):
        """Yield dataset splits.

        Parameters
        ----------
        ds: Dataset
          Input dataset

        Returns
        -------
        generator
          The generator yields every possible split according to the splitter
          configuration. All generated dataset have a boolean 'lastsplit'
          attribute in their dataset attribute collection indicating whether
          this particular dataset is the last one.
        """
        # localbinding
        noslicing = self.__noslicing
        count = self.__count
        splattr = self.get_space()
        ignore = self.__splitattr_ignore

        # get attribute and source collection from dataset
        splattr, collection = ds.get_attr(splattr)
        splattr_data = splattr.value
        cfgs = self.__splitattr_values
        if cfgs is None:
            cfgs = splattr.unique
        if __debug__:
            debug('SPL', 'Determined %i split specifications' % len(cfgs))
        if not ignore is None:
            # remove to be ignored bits
            cfgs = [c for c in cfgs if not c in ignore]
            if __debug__:
                debug('SPL',
                      '%i split specifications left after removing ignored ones'
                      % len(cfgs))
        n_cfgs = len(cfgs)

        if self.__reverse:
            if __debug__:
                debug('SPL', 'Reversing split order')
            cfgs = cfgs[::-1]

        # split the data
        for isplit, split in enumerate(cfgs):
            if not count is None and isplit >= count:
                # number of max splits is reached
                if __debug__:
                    debug('SPL',
                          'Discard remaining splits as maximum of %i is reached'
                          % count)
                break
            # safeguard against 'split' being `None` -- in which case a single
            # boolean would be the result of the comparision below, and not
            # a boolean vector from element-wise comparision
            if split is None:
                split = [None]
            # boolean mask is 'selected' samples for this split
            filter_ = splattr_data == split

            if not noslicing:
                # check whether we can do slicing instead of advanced
                # indexing -- if we can split the dataset without causing
                # the data to be copied, its is quicker and leaner.
                # However, it only works if we have a contiguous chunk or
                # regular step sizes for the samples to be split
                filter_ = mask2slice(filter_)

            if collection is ds.sa:
                if __debug__:
                    debug('SPL', 'Split along samples axis')
                split_ds = ds[filter_]
            elif collection is ds.fa:
                if __debug__:
                    debug('SPL', 'Split along feature axis')
                split_ds = ds[:, filter_]
            else:
                RuntimeError("This should never happen.")

            # is this the last split
            if count is None:
                lastsplit = (isplit == n_cfgs - 1)
            else:
                lastsplit = (isplit == count - 1)

            if not split_ds.a.has_key('lastsplit'):
                # if not yet known -- add one
                split_ds.a['lastsplit'] = lastsplit
            else:
                # otherwise just assign a new value
                split_ds.a.lastsplit = lastsplit

            yield split_ds

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Base Kernel classes

"""

_DEV_DOC_ = """
Concerns:

- Assure proper type of _k assigned
- The same issue "Dataset vs data" in input arguments

"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.types import is_datasetlike
from mvpa2.base.state import ClassWithCollections
from mvpa2.base.param import Parameter
from mvpa2.misc.sampleslookup import SamplesLookup # required for CachedKernel

if __debug__:
    from mvpa2.base import debug

__all__ = ['Kernel', 'NumpyKernel', 'CustomKernel', 'PrecomputedKernel',
           'CachedKernel']

class Kernel(ClassWithCollections):
    """Abstract class which calculates a kernel function between datasets

    Each instance has an internal representation self._k which might be of
    a different form depending on the intended use.  Some kernel types should
    be translatable to other representations where possible, e.g., between
    Numpy and Shogun-based kernels.

    This class should not be used directly, but rather use a subclass which
    enforces a consistent internal representation, such as a NumpyKernel.

    Notes
    -----
    Conversion mechanisms: Each kernel type should implement methods
    as necessary for the following two methods to work:

    :meth:`~mvpa2.kernels.Kernel.as_np`
      *Return a new NumpyKernel object with internal Numpy kernel*.
      This method can be generally inherited from the base Kernel class by
      creating a PrecomputedKernel from the raw numpy matrix, as implemented
      here.

    :meth:`~mvpa2.kernels.Kernel.as_raw_np`
      *Return a raw Numpy array from this kernel*.
      This method should behave identically to numpy.array(kernel), and in fact,
      defining either method (via defining Kernel.__array__) will be sufficient
      for both method calls to work.  See this source code for more details.

    Other kernel types should implement similar mechanisms to convert numpy
    arrays to their own internal representations.  See `add_conversion` for a
    helper method, and examples in mvpa2.kernels.sg

    Assuming such `Kernel.as_*` methods exist, all kernel types should be
    seamlessly convertable amongst each other.

    Note that kernels are not meant to be 'functionally translateable' in the
    sense that one kernel can be created, translated, then used to compute
    results in a new framework.  Rather, the results are meant to be
    exchangeable, hence the standard practice of using a precomputed kernel
    object to store the results in the new kernel type.

    For example:

    ::

      k = SomeShogunKernel()
      k.compute(data1, data2)

      # Incorrect and unsupported use
      k2 = k.as_cuda()
      k2.compute(data3, data4) # Would require 'functional translation' to the new
                               # backend, which is impossible

      # Correct use
      someOtherAlgorithm(k.as_raw_cuda()) # Simply uses kernel results in CUDA
    """

    _ATTRIBUTE_COLLECTIONS = ['params'] # enforce presence of params collections

    # Define this per class: standard string describing kernel type, ie
    # 'linear', or 'rbf', to help coordinate kernel types across backends
    __kernel_name__ = None

    def __init__(self, *args, **kwargs):
        """Base Kernel class has no parameters
        """
        ClassWithCollections.__init__(self, *args, **kwargs)
        self._k = None
        """Implementation specific version of the kernel"""

    def compute(self, ds1, ds2=None):
        """Generic computation of any kernel

        Assumptions:

         - ds1, ds2 are either datasets or arrays,
         - presumably 2D (not checked neither enforced here
         - _compute takes ndarrays. If your kernel needs datasets,
           override compute
        """
        if is_datasetlike(ds1):
            ds1 = ds1.samples
        if ds2 is None:
            ds2 = ds1
        elif is_datasetlike(ds2):
            ds2 = ds2.samples
        # TODO: assure 2D shape
        self._compute(ds1, ds2)

    def _compute(self, d1, d2):
        """Specific implementation to be overridden
        """
        raise NotImplemented, "Abstract method"

    def computed(self, *args, **kwargs):
        """Compute kernel and return self
        """
        self.compute(*args, **kwargs)
        return self

    ############################################################################
    # The following methods are circularly defined.  Child kernel types can
    # override either one or both to allow conversion to Numpy
    def __array__(self):
        return self.as_raw_np()

    def as_raw_np(self):
        """Directly return this kernel as a numpy array"""
        return np.array(self)

    ############################################################################

    def as_np(self):
        """Converts this kernel to a Numpy-based representation"""
        p = PrecomputedKernel(matrix=self.as_raw_np())
        p.compute()
        return p

    def cleanup(self):
        """Wipe out internal representation

        XXX unify: we have reset in other places to accomplish similar
        thing
        """
        self._k = None

    @classmethod
    def add_conversion(cls, typename, methodfull, methodraw):
        """Adds methods to the Kernel class for new conversions

        Parameters
        ----------
        typename : string
          Describes kernel type
        methodfull : function
          Method which converts to the new kernel object class
        methodraw : function
          Method which returns a raw kernel

        Examples
        --------
        Kernel.add_conversion('np', fullmethod, rawmethod)
        binds kernel.as_np() to fullmethod()
        binds kernel.as_raw_np() to rawmethod()

        Can also be used on subclasses to override the default conversions
        """
        setattr(cls, 'as_%s'%typename, methodfull)
        setattr(cls, 'as_raw_%s'%typename, methodraw)

class NumpyKernel(Kernel):
    """A Kernel object with internal representation as a 2d numpy array"""

    _ATTRIBUTE_COLLECTIONS = Kernel._ATTRIBUTE_COLLECTIONS + ['ca']
    # enforce presence of params AND ca collections for gradients etc

    def __array__(self):
        # By definintion, a NumpyKernel's internal representation is an array
        return self._k

    def as_np(self):
        """Converts this kernel to a Numpy-based representation"""
        # Already numpy!!
        return self

    def as_raw_np(self):
        """Directly return this kernel as a numpy array.

        For Numpy-based kernels - simply returns stored matrix."""

        return self._k
    # wasn't that easy?


class CustomKernel(NumpyKernel):
    """Custom Kernel defined by an arbitrary function

    Examples
    --------

    Basic linear kernel
    >>> k = CustomKernel(kernelfunc=lambda a,b: numpy.dot(a,b.T))
    """

    __TODO__ = """
    - repr/doc sicne now kernelfunc is not a Parameter
    """

    def __init__(self, kernelfunc=None, *args, **kwargs):
        """Initialize CustomKernel with an arbitrary function.

        Parameters
        ----------
        kernelfunc : function
          Any callable function which takes two numpy arrays and
          calculates a kernel function, treating the rows as samples and the
          columns as features. It is called from compute(d1, d2) -> func(d1,d2)
          and should return a numpy matrix K(i,j) which holds the kernel
          evaluated from d1 sample i and d2 sample j
        """
        NumpyKernel.__init__(self, *args, **kwargs)
        self._kernelfunc = kernelfunc

    def _compute(self, d1, d2):
        self._k = self._kernelfunc(d1, d2)



class PrecomputedKernel(NumpyKernel):
    """Precomputed matrix
    """

    __TODO__ = """
    - repr/doc sicne now matrix is not a Parameter
    """

    # NB: to avoid storing matrix twice, after compute
    # self.params.matrix = self._k
    def __init__(self, matrix=None, *args, **kwargs):
        """
        Parameters
        ----------
        matrix : Numpy array or convertable kernel, or other object type
        """
        NumpyKernel.__init__(self, *args, **kwargs)

        self._k = np.array(matrix)

    def compute(self, *args, **kwargs):
        pass


class CachedKernel(NumpyKernel):
    """Kernel which caches all data to avoid duplicate computation

    This kernel is very useful for any analysis which will retrain or
    repredict the same data multiple times, as this kernel will avoid
    recalculating the kernel function.  Examples of such analyses include cross
    validation, bootstrapping, and model selection (assuming the kernel function
    itself does not change, e.g. when selecting for C in an SVM).

    The kernel will automatically cache any new data sent through compute, and
    will be able to use this cache whenever a subset of this data is sent
    through compute again.  If new (uncached) data is sent through compute, then
    the cache is recreated from scratch.  Therefore, you should compute the
    kernel on the entire superset of your data before using this kernel
    normally (computing a new cache invalidates any previous cached data).

    The cache is asymmetric for lhs and rhs, so compute(d1, d2) does not create
    a cache usable for compute(d2, d1).
    """

    # TODO: Figure out how to design objects like CrossValidation etc to
    # precompute this kernel automatically, making it transparent to the user

    @property
    def __kernel_name__(self):
        """Allows checking name of subkernel"""
        return self._kernel.__kernel_name__

    def __init__(self, kernel=None, *args, **kwargs):
        """Initialize `CachedKernel`

        Parameters
        ----------
        kernel : Kernel
          Base kernel to cache.  Any kernel which can be converted to a
          `NumpyKernel` is allowed
        """
        super(CachedKernel, self).__init__(*args, **kwargs)
        self._kernel = kernel
        self.params.update(self._kernel.params)
        self._rhsids = self._lhsids = self._kfull = None
        self._recomputed = None

    def _cache(self, ds1, ds2=None):
        """Initializes internal lookups + _kfull via caching the kernel matrix
        """
        if __debug__ and 'KRN' in debug.active:
            debug('KRN', "Caching %(inst)s for ds1=%(ds1)s, ds2=%(ds1)s"
                  % dict(inst=self, ds1=ds1, ds2=ds2))

        self._lhsids = SamplesLookup(ds1)
        if (ds2 is None) or (ds2 is ds1):
            self._rhsids = self._lhsids
        else:
            self._rhsids = SamplesLookup(ds2)

        ckernel = self._kernel
        ckernel.compute(ds1, ds2)
        self._kfull = ckernel.as_raw_np()
        ckernel.cleanup()
        self._k = self._kfull

        self._recomputed = True
        self.params.reset()
        # TODO: store params representation for later comparison

    def compute(self, ds1, ds2=None, force=False):
        """Automatically computes and caches the kernel or extracts the
        relevant part of a precached kernel into self._k

        Parameters
        ----------
        force : bool
          If True it forces re-caching of the kernel.  It is advised
          to be used whenever explicitly pre-caching the kernel and
          it is known that data was changed.
        """
        if __debug__ and 'KRN' in debug.active:
            debug('KRN', "Computing kernel %(inst)s on ds1=%(ds1)s, ds2=%(ds1)s"
                  % dict(inst=self, ds1=ds1, ds2=ds2))

        # Flag lets us know whether cache was recomputed
        self._recomputed = False

        #if self._ds_cached_info is not None:
        # Check either those ds1, ds2 are coming from the same
        # dataset as before

        # TODO: figure out if data were modified...
        # params_modified = True
        changedData = False or force
        if len(self.params.which_set()) or changedData \
           or self._lhsids is None:
            self._cache(ds1, ds2)# hopefully this will never reset values, just
            # changed status
        else:
            # figure d1, d2
            try:
                lhsids = self._lhsids(ds1) #
                if ds2 is None:
                    rhsids = lhsids
                else:
                    rhsids = self._rhsids(ds2)
                self._k = self._kfull[np.ix_(lhsids, rhsids)]
            except KeyError:
                self._cache(ds1, ds2)

        if __debug__ and self._recomputed:
            debug('KRN',
                  "Kernel %(inst)s was recomputed on ds1=%(ds1)s, ds2=%(ds1)s"
                  % dict(inst=self, ds1=ds1, ds2=ds2))


__BOGUS_NOTES__ = """
if ds1 is the "derived" dataset as it was computed on:
    * ds2 is None
      ds2 bound to ds1
      -
    * ds1 and ds2 present
      - ds1 and ds2 come from the same dataset
        - whatever CachedKernel was computed on is a superset
        - not a superset -- puke?
      - ds2 comes from different than ds1
        - puke?
else:
    compute (ds1, ds2)
      - different data ids


ckernel = PrecomputedKernel(matrix=np.array([1,2,3]))
ck = CachedKernel(kernel=ckernel)

"""


########NEW FILE########
__FILENAME__ = libsvm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""PyMVPA LibSVM-based kernels

These kernels do not currently have the ability to run the calculations, so
they are not translateable to other kernel types.  They are implemented solely
to standardize the interface between other kernel machines.
"""

__docformat__ = 'restructuredtext'

from mvpa2.kernels.base import Kernel
from mvpa2.base.param import Parameter

#from mvpa2.clfs.libsvmc import _svmc
class _svmc(object):
    """Locally defining constants for LibSVM to avoid circular import.
    """
    LINEAR = 0
    POLY = 1
    RBF = 2
    SIGMOID = 3


class LSKernel(Kernel):
    """A Kernel object which dictates how LibSVM will calculate the kernel"""

    def __init__(self, *args, **kwargs):
        """Base class for LIBSVM Kernels has no parameters
        """
        Kernel.__init__(self, *args, **kwargs)
        self.compute()

    def compute(self, *args, **kwargs):
        self._k = self.__kernel_type__ # Nothing to compute

    def as_raw_ls(self):
        return self._k

    def as_ls(self):
        return self

    def as_raw_np(self):
        raise ValueError, 'LibSVM calculates kernels internally; they ' +\
              'cannot be converted to Numpy'

# Conversion methods
def _as_ls(kernel):
    raise NotImplemented, 'LibSVM calculates kernels internally; they ' +\
          'cannot be converted from Numpy'
def _as_raw_ls(kernel):
    raise NotImplemented, 'LibSVM calculates kernels internally; they ' +\
          'cannot be converted from Numpy'
Kernel.add_conversion('ls', _as_ls, _as_raw_ls)

class LinearLSKernel(LSKernel):
    """A simple Linear kernel: K(a,b) = a*b.T"""
    __kernel_type__ = _svmc.LINEAR
    __kernel_name__ = 'linear'


class RbfLSKernel(LSKernel):
    """Radial Basis Function kernel (aka Gaussian):
    K(a,b) = exp(-gamma*||a-b||**2)
    """
    __kernel_type__ = _svmc.RBF
    __kernel_name__ = 'rbf'
    gamma = Parameter(1, doc='Gamma multiplying paramater for Rbf')

    def __init__(self, **kwargs):
        # Necessary for proper docstring construction
        LSKernel.__init__(self, **kwargs)


class PolyLSKernel(LSKernel):
    """Polynomial kernel: K(a,b) = (gamma*a*b.T + coef0)**degree"""
    __kernel_type__ = _svmc.POLY
    __kernel_name__ = 'poly'
    gamma = Parameter(1, doc='Gamma multiplying parameter for Polynomial')
    degree = Parameter(2, doc='Degree of polynomial')
    coef0 = Parameter(1, doc='Offset inside polynomial') # aka coef0

    def __init__(self, **kwargs):
        # Necessary for proper docstring construction
        LSKernel.__init__(self, **kwargs)


class SigmoidLSKernel(LSKernel):
    """Sigmoid kernel: K(a,b) = tanh(gamma*a*b.T + coef0)"""
    __kernel_type__ = _svmc.SIGMOID
    __kernel_name__ = 'sigmoid'
    gamma = Parameter(1, doc='Gamma multiplying parameter for SigmoidKernel')
    coef0 = Parameter(1, doc='Offset inside tanh')

    def __init__(self, **kwargs):
        # Necessary for proper docstring construction
        LSKernel.__init__(self, **kwargs)


########NEW FILE########
__FILENAME__ = np
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com> and
#   PyMVPA Team. See COPYING file distributed along with the PyMVPA
#   package for complete list of copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Kernels for Gaussian Process Regression and Classification."""


_DEV__DOC__ = """
Make use of Parameter Collections to keep parameters of the
kernels. Then we would get a uniform .reset() functionality. Now reset
is provided just for parts which are failing in the unittests, but
there is many more places where they are not reset properly if
classifier gets trained on some new data of different dimensionality
"""

__docformat__ = 'restructuredtext'


import numpy as np

from mvpa2.base.state import ConditionalAttribute
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import EnsureFloat, EnsureListOf
from mvpa2.misc.exceptions import InvalidHyperparameterError
from mvpa2.clfs.distance import squared_euclidean_distance
from mvpa2.kernels.base import NumpyKernel
if __debug__:
    from mvpa2.base import debug, warning

# Simple stuff

class LinearKernel(NumpyKernel):
    """Simple linear kernel: K(a,b) = a*b.T"""
    def _compute(self, d1, d2):
        self._k = np.dot(d1, d2.T)


class PolyKernel(NumpyKernel):
    """Polynomial kernel: K(a,b) = (gamma*a*b.T+coef0)**degree"""
    gamma = Parameter(1, doc='Gamma scaling coefficient')
    degree = Parameter(2, doc="Polynomial degree")
    coef0 = Parameter(1, doc="Offset added to dot product before exponent")
    
    def _compute(self, d1, d2):
        self._k = np.power(self.params.gamma*np.dot(d1, d2.T)+self.params.coef0,
                          self.params.degree)


class RbfKernel(NumpyKernel):
    """Radial basis function (aka Gausian, aka ) kernel
    K(a,b) = exp(-||a-b||**2/sigma)
    """
    sigma = Parameter(1.0, constraints='float', doc="Width parameter sigma")
    
    def _compute(self, d1, d2):
        # Do the Rbf
        self._k = np.exp(-squared_euclidean_distance(d1,d2) / self.params.sigma)
        
# More complex
class ConstantKernel(NumpyKernel):
    """The constant kernel class.
    """

    sigma_0 = Parameter(1.0,
                        doc="""
       A simple constant squared value of which is broadcasted across
       kernel. In the case of GPR -- standard deviation of the Gaussian
       prior probability N(0,sigma_0**2) of the intercept of the
       constant regression.""")

    def _compute(self, data1, data2):
        """Compute kernel matrix.

        Parameters
        ----------
        data1 : numpy.ndarray
          lhs data
        data2 : numpy.ndarray
          rhs data
        """
        self._k = \
            (self.params.sigma_0 ** 2) * np.ones((data1.shape[0], data2.shape[0]))

    ## def set_hyperparameters(self, hyperparameter):
    ##     if hyperparameter < 0:
    ##         raise InvalidHyperparameterError()
    ##     self.sigma_0 = hyperparameter
    ##     return

    def compute_lml_gradient(self, alphaalphaT_Kinv, data):
        K_grad_sigma_0 = 2*self.params.sigma_0
        # self.lml_gradient = 0.5*(np.trace(np.dot(alphaalphaT_Kinv,K_grad_sigma_0*np.ones(alphaalphaT_Kinv.shape)))
        # Faster formula: np.trace(np.dot(A,B)) = (A*(B.T)).sum()
        # Fastest when B is a constant: B*A.sum()
        self.lml_gradient = 0.5*np.array(K_grad_sigma_0*alphaalphaT_Kinv.sum())
        #return self.lml_gradient

    def compute_lml_gradient_logscale(self, alphaalphaT_Kinv, data):
        K_grad_sigma_0 = 2*self.params.sigma_0**2
        self.lml_gradient = 0.5*np.array(K_grad_sigma_0*alphaalphaT_Kinv.sum())
        #return self.lml_gradient
    pass


class GeneralizedLinearKernel(NumpyKernel):
    """The linear kernel class.
    """

    sigma_0 = Parameter(1.0,
                        doc="""
       A simple constant squared value which is broadcasted across
       kernel. In the case of GPR -- standard deviation of the Gaussian
       prior probability Normal(0, sigma_0**2) of the intercept of the
       linear regression.""")

    Sigma_p = Parameter(1.0,
                        doc=r"""
       A generic scalar or vector, or diagonal matrix to scale all
       dimensions or associate different scaling to each dimensions
       while computing te kernel matrix:
       :math:`k(x_A,x_B) = x_A^\top \Sigma_p x_B + \sigma_0^2`.
       In the case of GPR -- a scalar or a diagonal of covariance matrix
       of the Gaussian prior probability Normal(0, Sigma_p) on the weights
       of the linear regression.""")

    gradients = ConditionalAttribute(enabled=False,
        doc="Dictionary of gradients per a parameter")

    gradientslog = ConditionalAttribute(enabled=False,
        doc="Dictionary of gradients per a parameter in logspace")

    def __init__(self, *args, **kwargs):
        # for docstring holder
        NumpyKernel.__init__(self, *args, **kwargs)

    ## def __init__(self, Sigma_p=None, sigma_0=1.0, **kwargs):
    ##     """Initialize the linear kernel instance.

    ##     Parameters
    ##     ----------
    ##     Sigma_p : numpy.ndarray
    ##       Covariance matrix of the Gaussian prior probability N(0,Sigma_p)
    ##       on the weights of the linear regression.
    ##       (Defaults to None)
    ##     sigma_0 : float
    ##       the standard deviation of the Gaussian prior N(0,sigma_0**2)
    ##       of the intercept of the linear regression.
    ##       (Deafults to 1.0)
    ##     """
    ##     # init base class first
    ##     NumpyKernel.__init__(self, **kwargs)

    ##     # TODO: figure out cleaner way... probably by using KernelParameters ;-)
    ##     self.Sigma_p = Sigma_p
    ##     self.sigma_0 = sigma_0


    ## def __repr__(self):
    ##     return "%s(Sigma_p=%s, sigma_0=%s)" \
    ##         % (self.__class__.__name__, str(self.Sigma_p), str(self.sigma_0))

    # XXX ??? would we reset correctly to the original value... model selection
    #     currently depends on this I believe
    def reset(self):
        super(GeneralizedLinearKernel, self).reset()
        self._Sigma_p = self._Sigma_p_orig


    def _compute(self, data1, data2):
        """Compute kernel matrix.
        """
        # it is better to use separate lines of computation, to don't
        # incure computation cost without need (otherwise
        # np.dot(self.Sigma_p, data2.T) can take forever for relatively
        # large number of features)

        Sigma_p = self.params.Sigma_p          # local binding
        sigma_0 = self.params.sigma_0

        #if scalar - scale second term appropriately
        if np.isscalar(Sigma_p):
            if Sigma_p == 1.0:
                data2_sc = data2.T
            else:
                data2_sc = Sigma_p * data2.T

        # if vector use it as diagonal matrix -- ie scale each row by
        # the given value
        elif len(Sigma_p.shape) == 1 and \
                 Sigma_p.shape[0] == data2.shape[1]:
            # which due to numpy broadcasting is the same as product
            # with scalar above
            data2_sc = (Sigma_p * data2).T
        # If (diagonal) or full-matrix -- full-featured and lengthy matrix
        # product
        elif len(Sigma_p.shape) == 2 and \
                 Sigma_p.shape[0] == Sigma_p.shape[1] == data2.shape[1]:
            # which due to numpy broadcasting is the same as product
            # with scalar above
            data2_sc = np.dot(Sigma_p, data2.T)
        else:
            raise ValueError, "Please provide Sigma_p as a scalar, vector, " \
                  "or square (diagonal) matrix."

        # XXX if Sigma_p is changed a warning should be issued!
        # XXX other cases of incorrect Sigma_p could be catched
        self._k = k = np.dot(data1, data2_sc) + sigma_0 ** 2

        # Compute gradients if any was requested
        do_g  = self.ca.is_enabled('gradients')
        do_gl = self.ca.is_enabled('gradientslog')
        if do_g or do_gl:
            if np.isscalar(Sigma_p):
                g_Sigma_p = np.dot(data1, data2.T)
                gl_Sigma_p = Sigma_p * g_Sigma_p
            else:
                nfeat = len(Sigma_p)
                gsize = (len(data1), len(data2), nfeat)
                if do_g:  g_Sigma_p = np.empty(gsize)
                if do_gl: gl_Sigma_p = np.empty(gsize)
                for i in xrange(nfeat):
                    outer = np.multiply.outer(data1[:, i], data2[:, i])
                    if do_g:  g_Sigma_p[:, :, i] = outer
                    if do_gl: gl_Sigma_p = Sigma_p[i] * outer
            if do_g:
                self.ca.gradients = dict(
                    sigma_0=2*sigma_0,
                    Sigma_p=g_Sigma_p)
            if do_gl:
                self.ca.gradientslog = dict(
                    sigma_0=2*sigma_0**2,
                    Sigma_p=gl_Sigma_p)
    pass


class ExponentialKernel(NumpyKernel):
    """The Exponential kernel class.

    Note that it can handle a length scale for each dimension for
    Automtic Relevance Determination.

    """

    length_scale = Parameter(1.0, constraints=EnsureFloat() | EnsureListOf(float), doc="""
        The characteristic length-scale (or length-scales) of the phenomenon
        under investigation.""")

    sigma_f = Parameter(1.0, constraints='float',
        doc="""Signal standard deviation.""")

    def __init__(self, *args, **kwargs):
        # for docstring holder
        NumpyKernel.__init__(self, *args, **kwargs)

    ## def __init__(self, length_scale=1.0, sigma_f = 1.0, **kwargs):
    ##     """Initialize an Exponential kernel instance.

    ##     Parameters
    ##     ----------
    ##     length_scale : float or numpy.ndarray
    ##       the characteristic length-scale (or length-scales) of the
    ##       phenomenon under investigation.
    ##       (Defaults to 1.0)
    ##     sigma_f : float
    ##       Signal standard deviation.
    ##       (Defaults to 1.0)
    ##     """
    ##     # init base class first
    ##     NumpyKernel.__init__(self, **kwargs)

    ##     self.length_scale = length_scale
    ##     self.sigma_f = sigma_f
    ##     self._k = None


    ## def __repr__(self):
    ##     return "%s(length_scale=%s, sigma_f=%s)" \
    ##       % (self.__class__.__name__, str(self.length_scale), str(self.sigma_f))

    def _compute(self, data1, data2):
        """Compute kernel matrix.

        Parameters
        ----------
        data1 : numpy.ndarray
          lhs data
        data2 : numpy.ndarray
          rhs data
        """
        params = self.params
        # XXX the following computation can be (maybe) made more
        # efficient since length_scale is squared and then
        # square-rooted uselessly.
        # Weighted euclidean distance matrix:
        self.wdm = np.sqrt(squared_euclidean_distance(
            data1, data2, weight=(params.length_scale**-2)))
        self._k = \
            params.sigma_f**2 * np.exp(-self.wdm)

    def gradient(self, data1, data2):
        """Compute gradient of the kernel matrix. A must for fast
        model selection with high-dimensional data.
        """
        raise NotImplementedError

    ## def set_hyperparameters(self, hyperparameter):
    ##     """Set hyperaparmeters from a vector.

    ##     Used by model selection.
    ##     """
    ##     if np.any(hyperparameter < 0):
    ##         raise InvalidHyperparameterError()
    ##     self.sigma_f = hyperparameter[0]
    ##     self.length_scale = hyperparameter[1:]
    ##     return

    def compute_lml_gradient(self,alphaalphaT_Kinv,data):
        """Compute grandient of the kernel and return the portion of
        log marginal likelihood gradient due to the kernel.
        Shorter formula. Allows vector of lengthscales (ARD)
        BUT THIS LAST OPTION SEEMS NOT TO WORK FOR (CURRENTLY)
        UNKNOWN REASONS.
        """
        self.lml_gradient = []
        def lml_grad(K_grad_i):
            # return np.trace(np.dot(alphaalphaT_Kinv,K_grad_i))
            # Faster formula: np.trace(np.dot(A,B)) = (A*(B.T)).sum()
            return (alphaalphaT_Kinv*(K_grad_i.T)).sum()
        grad_sigma_f = 2.0/self.sigma_f*self.kernel_matrix
        self.lml_gradient.append(lml_grad(grad_sigma_f))
        if np.isscalar(self.length_scale) or self.length_scale.size==1:
            # use the same length_scale for all dimensions:
            K_grad_l = self.wdm*self.kernel_matrix*(self.length_scale**-1)
            self.lml_gradient.append(lml_grad(K_grad_l))
        else:
            # use one length_scale for each dimension:
            for i in range(self.length_scale.size):
                K_grad_i = (self.length_scale[i]**-3)*(self.wdm**-1)*self.kernel_matrix*np.subtract.outer(data[:,i],data[:,i])**2
                self.lml_gradient.append(lml_grad(K_grad_i))
                pass
            pass
        self.lml_gradient = 0.5*np.array(self.lml_gradient)
        return self.lml_gradient

    def compute_lml_gradient_logscale(self,alphaalphaT_Kinv,data):
        """Compute grandient of the kernel and return the portion of
        log marginal likelihood gradient due to the kernel.
        Shorter formula. Allows vector of lengthscales (ARD).
        BUT THIS LAST OPTION SEEMS NOT TO WORK FOR (CURRENTLY)
        UNKNOWN REASONS.
        """
        self.lml_gradient = []
        def lml_grad(K_grad_i):
            # return np.trace(np.dot(alphaalphaT_Kinv,K_grad_i))
            # Faster formula: np.trace(np.dot(A,B)) = (A*(B.T)).sum()
            return (alphaalphaT_Kinv*(K_grad_i.T)).sum()
        grad_log_sigma_f = 2.0*self.kernel_matrix
        self.lml_gradient.append(lml_grad(grad_log_sigma_f))
        if np.isscalar(self.length_scale) or self.length_scale.size==1:
            # use the same length_scale for all dimensions:
            K_grad_l = self.wdm*self.kernel_matrix
            self.lml_gradient.append(lml_grad(K_grad_l))
        else:
            # use one length_scale for each dimension:
            for i in range(self.length_scale.size):
                K_grad_i = (self.length_scale[i]**-2)*(self.wdm**-1)*self.kernel_matrix*np.subtract.outer(data[:,i],data[:,i])**2
                self.lml_gradient.append(lml_grad(K_grad_i))
                pass
            pass
        self.lml_gradient = 0.5*np.array(self.lml_gradient)
        return self.lml_gradient

    pass


class SquaredExponentialKernel(NumpyKernel):
    """The Squared Exponential kernel class.

    Note that it can handle a length scale for each dimension for
    Automtic Relevance Determination.

    """
    def __init__(self, length_scale=1.0, sigma_f=1.0, **kwargs):
        """Initialize a Squared Exponential kernel instance.

        Parameters
        ----------
        length_scale : float or numpy.ndarray, optional
          the characteristic length-scale (or length-scales) of the
          phenomenon under investigation.
          (Defaults to 1.0)
        sigma_f : float, optional
          Signal standard deviation.
          (Defaults to 1.0)
        """
        # init base class first
        NumpyKernel.__init__(self, **kwargs)

        self.length_scale = length_scale
        self.sigma_f = sigma_f

    # XXX ??? 
    def reset(self):
        super(SquaredExponentialKernel, self).reset()
        self._length_scale = self._length_scale_orig


    def __repr__(self):
        return "%s(length_scale=%s, sigma_f=%s)" \
          % (self.__class__.__name__, str(self.length_scale), str(self.sigma_f))

    def _compute(self, data1, data2):
        """Compute kernel matrix.

        Parameters
        ----------
        data1 : numpy.ndarray
          data
        data2 : numpy.ndarray
          data
          (Defaults to None)
        """
        # weighted squared euclidean distance matrix:
        self.wdm2 = squared_euclidean_distance(data1, data2, weight=(self.length_scale**-2))
        self._k = self.sigma_f**2 * np.exp(-0.5*self.wdm2)
        # XXX EO: old implementation:
        # self.kernel_matrix = \
        #     self.sigma_f * np.exp(-squared_euclidean_distance(
        #         data1, data2, weight=0.5 / (self.length_scale ** 2)))

    def set_hyperparameters(self, hyperparameter):
        """Set hyperaparmeters from a vector.

        Used by model selection.
        """
        if np.any(hyperparameter < 0):
            raise InvalidHyperparameterError()
        self.sigma_f = hyperparameter[0]
        self._length_scale = hyperparameter[1:]
        return

    def compute_lml_gradient(self,alphaalphaT_Kinv,data):
        """Compute grandient of the kernel and return the portion of
        log marginal likelihood gradient due to the kernel.
        Shorter formula. Allows vector of lengthscales (ARD).
        """
        self.lml_gradient = []
        def lml_grad(K_grad_i):
            # return np.trace(np.dot(alphaalphaT_Kinv,K_grad_i))
            # Faster formula: np.trace(np.dot(A,B)) = (A*(B.T)).sum()
            return (alphaalphaT_Kinv*(K_grad_i.T)).sum()
        grad_sigma_f = 2.0/self.sigma_f*self.kernel_matrix
        self.lml_gradient.append(lml_grad(grad_sigma_f))
        if np.isscalar(self.length_scale) or self.length_scale.size==1:
            # use the same length_scale for all dimensions:
            K_grad_l = self.wdm2*self.kernel_matrix*(1.0/self.length_scale)
            self.lml_gradient.append(lml_grad(K_grad_l))
        else:
            # use one length_scale for each dimension:
            for i in range(self.length_scale.size):
                K_grad_i = 1.0/(self.length_scale[i]**3)*self.kernel_matrix*np.subtract.outer(data[:,i],data[:,i])**2
                self.lml_gradient.append(lml_grad(K_grad_i))
                pass
            pass
        self.lml_gradient = 0.5*np.array(self.lml_gradient)
        return self.lml_gradient

    def compute_lml_gradient_logscale(self,alphaalphaT_Kinv,data):
        """Compute grandient of the kernel and return the portion of
        log marginal likelihood gradient due to the kernel.
        Hyperparameters are in log scale which is sometimes more
        stable. Shorter formula. Allows vector of lengthscales (ARD).
        """
        self.lml_gradient = []
        def lml_grad(K_grad_i):
            # return np.trace(np.dot(alphaalphaT_Kinv,K_grad_i))
            # Faster formula: np.trace(np.dot(A,B)) = (A*(B.T)).sum()
            return (alphaalphaT_Kinv*(K_grad_i.T)).sum()
        K_grad_log_sigma_f = 2.0*self.kernel_matrix
        self.lml_gradient.append(lml_grad(K_grad_log_sigma_f))
        if np.isscalar(self.length_scale) or self.length_scale.size==1:
            # use the same length_scale for all dimensions:
            K_grad_log_l = self.wdm2*self.kernel_matrix
            self.lml_gradient.append(lml_grad(K_grad_log_l))
        else:
            # use one length_scale for each dimension:
            for i in range(self.length_scale.size):
                K_grad_log_l_i = 1.0/(self.length_scale[i]**2)*self.kernel_matrix*np.subtract.outer(data[:,i],data[:,i])**2
                self.lml_gradient.append(lml_grad(K_grad_log_l_i))
                pass
            pass
        self.lml_gradient = 0.5*np.array(self.lml_gradient)
        return self.lml_gradient

    def _setlength_scale(self, v):
        """Set value of length_scale and its _orig
        """
        self._length_scale = self._length_scale_orig = v

    length_scale = property(fget=lambda x:x._length_scale,
                            fset=_setlength_scale)
    pass

class Matern_3_2Kernel(NumpyKernel):
    """The Matern kernel class for the case ni=3/2 or ni=5/2.

    Note that it can handle a length scale for each dimension for
    Automtic Relevance Determination.

    """
    def __init__(self, length_scale=1.0, sigma_f=1.0, numerator=3.0, **kwargs):
        """Initialize a Squared Exponential kernel instance.

        Parameters
        ----------
        length_scale : float or numpy.ndarray, optional
          the characteristic length-scale (or length-scales) of the
          phenomenon under investigation.
          (Defaults to 1.0)
        sigma_f : float, optional
          Signal standard deviation.
          (Defaults to 1.0)
        numerator : float, optional
          the numerator of parameter ni of Matern covariance functions.
          Currently only numerator=3.0 and numerator=5.0 are implemented.
          (Defaults to 3.0)
        """
        # init base class first
        NumpyKernel.__init__(self, **kwargs)

        self.length_scale = length_scale
        self.sigma_f = sigma_f
        if numerator == 3.0 or numerator == 5.0:
            self.numerator = numerator
        else:
            raise NotImplementedError

    def __repr__(self):
        return "%s(length_scale=%s, ni=%d/2)" \
            % (self.__class__.__name__, str(self.length_scale), self.numerator)

    def _compute(self, data1, data2):
        """Compute kernel matrix.

        Parameters
        ----------
        data1 : numpy.ndarray
          lhs data
        data2 : numpy.ndarray
          rhs data
        """
        tmp = squared_euclidean_distance(
                data1, data2, weight=0.5 / (self.length_scale ** 2))
        if self.numerator == 3.0:
            tmp = np.sqrt(tmp)
            self._k = \
                self.sigma_f**2 * (1.0 + np.sqrt(3.0) * tmp) \
                * np.exp(-np.sqrt(3.0) * tmp)
        elif self.numerator == 5.0:
            tmp2 = np.sqrt(tmp)
            self._k = \
                self.sigma_f**2 * (1.0 + np.sqrt(5.0) * tmp2 + 5.0 / 3.0 * tmp) \
                * np.exp(-np.sqrt(5.0) * tmp2)


    def gradient(self, data1, data2):
        """Compute gradient of the kernel matrix. A must for fast
        model selection with high-dimensional data.
        """
        # TODO SOON
        # grad = ...
        # return grad
        raise NotImplementedError

    def set_hyperparameters(self, hyperparameter):
        """Set hyperaparmeters from a vector.

        Used by model selection.
        Note: 'numerator' is not considered as an hyperparameter.
        """
        if np.any(hyperparameter < 0):
            raise InvalidHyperparameterError()
        self.sigma_f = hyperparameter[0]
        self.length_scale = hyperparameter[1:]
        return

    pass


class Matern_5_2Kernel(Matern_3_2Kernel):
    """The Matern kernel class for the case ni=5/2.

    This kernel is just Matern_3_2Kernel(numerator=5.0).
    """
    def __init__(self, **kwargs):
        """Initialize a Squared Exponential kernel instance.

        Parameters
        ----------
        length_scale : float or numpy.ndarray
          the characteristic length-scale (or length-scales) of the
          phenomenon under investigation.
          (Defaults to 1.0)
        """
        Matern_3_2Kernel.__init__(self, numerator=5.0, **kwargs)
        pass


class RationalQuadraticKernel(NumpyKernel):
    """The Rational Quadratic (RQ) kernel class.

    Note that it can handle a length scale for each dimension for
    Automtic Relevance Determination.

    """
    def __init__(self, length_scale=1.0, sigma_f=1.0, alpha=0.5, **kwargs):
        """Initialize a Squared Exponential kernel instance.

        Parameters
        ----------
        length_scale : float or numpy.ndarray
          the characteristic length-scale (or length-scales) of the
          phenomenon under investigation.
          (Defaults to 1.0)
        sigma_f : float
          Signal standard deviation.
          (Defaults to 1.0)
        alpha : float
          The parameter of the RQ functions family.
          (Defaults to 2.0)
        """
        # init base class first
        NumpyKernel.__init__(self, **kwargs)

        self.length_scale = length_scale
        self.sigma_f = sigma_f
        self.alpha = alpha

    def __repr__(self):
        return "%s(length_scale=%s, alpha=%f)" \
            % (self.__class__.__name__, str(self.length_scale), self.alpha)

    def _compute(self, data1, data2):
        """Compute kernel matrix.

        Parameters
        ----------
        data1 : numpy.ndarray
          lhs data
        data2 : numpy.ndarray
          rhs data
        """
        tmp = squared_euclidean_distance(
                data1, data2, weight=1.0 / (self.length_scale ** 2))
        self._k = \
            self.sigma_f**2 * (1.0 + tmp / (2.0 * self.alpha)) ** -self.alpha

    def gradient(self, data1, data2):
        """Compute gradient of the kernel matrix. A must for fast
        model selection with high-dimensional data.
        """
        # TODO SOON
        # grad = ...
        # return grad
        raise NotImplementedError

    def set_hyperparameters(self, hyperparameter):
        """Set hyperaparmeters from a vector.

        Used by model selection.
        Note: 'alpha' is not considered as an hyperparameter.
        """
        if np.any(hyperparameter < 0):
            raise InvalidHyperparameterError()
        self.sigma_f = hyperparameter[0]
        self.length_scale = hyperparameter[1:]
        return

    pass


# dictionary of avalable kernels with names as keys:
kernel_dictionary = {'constant': ConstantKernel,
                     'linear': LinearKernel, #GeneralizedLinearKernel,
                     'genlinear': GeneralizedLinearKernel,
                     'poly': PolyKernel,
                     'rbf': RbfKernel,
                     'exponential': ExponentialKernel,
                     'squared exponential': SquaredExponentialKernel,
                     'Matern ni=3/2': Matern_3_2Kernel,
                     'Matern ni=5/2': Matern_5_2Kernel,
                     'rational quadratic': RationalQuadraticKernel}


########NEW FILE########
__FILENAME__ = sg
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""PyMVPA shogun-based kernels

Provides interface to kernels defined in shogun toolbox.  Commonly
used kernels are provided with convenience classes: `LinearSGKernel`,
`RbfSGKernel`, `PolySGKernel`.  If you need to use some other shogun
kernel, use `CustomSGKernel` to define one.
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.externals import exists, versions
from mvpa2.kernels.base import Kernel
from mvpa2.base.param import Parameter

if exists('shogun', raise_=True):
    import shogun.Kernel as sgk
    from shogun.Features import RealFeatures
else:
    # Just to please sphinx documentation
    class Bogus(object):
        pass
    sgk = Bogus()
    sgk.LinearKernel = None
    sgk.GaussianKernel = None
    sgk.PolyKernel = None

if __debug__:
    from mvpa2.base import debug

class SGKernel(Kernel):
    """A Kernel object with internal representation in Shogun"""

    def as_sg(self):
        return self

    def as_raw_sg(self):
        return self._k

    def __array__(self):
        return self._k.get_kernel_matrix()

    @staticmethod
    def _data2features(data):
        """Converts data to shogun features"""
        if __debug__:
            debug('KRN_SG',
                  'Converting data of shape %s into shogun RealFeatures'
                  % (data.shape,))
        res = RealFeatures(data.astype(float).T)
        if __debug__:
            debug('KRN_SG', 'Done converting data')

        return res

# Conversion methods
def _as_raw_sg(kernel):
    """Converts directly to a Shogun kernel"""
    return sgk.CustomKernel(kernel.as_raw_np())
def _as_sg(kernel):
    """Converts this kernel to a Shogun-based representation"""
    return PrecomputedSGKernel(matrix=kernel.as_raw_np())
Kernel.add_conversion('sg', _as_sg, _as_raw_sg)


class _BasicSGKernel(SGKernel):
    """Abstract class which can handle most shogun kernel types

    Subclasses can specify new kernels using the following declarations:

      - __kernel_cls__ = Shogun kernel class
      - __kp_order__ = Tuple which specifies the order of kernel params.
        If there is only one kernel param, this is not necessary
    """

    __TODO__ = """
    - Think either normalizer_* should not become proper Parameter.
    """

    def __init__(self, normalizer_cls=None, normalizer_args=None, **kwargs):
        """
        Parameters
        ----------
        normalizer_cls : sg.Kernel.CKernelNormalizer
          Class to use as a normalizer for the kernel.  Will be instantiated
          upon compute().  Only supported for shogun >= 0.6.5.
          By default (if left None) assigns IdentityKernelNormalizer to assure no
          normalization.
        normalizer_args : None or list
          If necessary, provide a list of arguments for the normalizer.
        """
        SGKernel.__init__(self, **kwargs)
        if (normalizer_cls is not None) and (versions['shogun:rev'] < 3377):
            raise ValueError, \
               "Normalizer specification is supported only for sg >= 0.6.5. " \
               "Please upgrade shogun python modular bindings."

        if normalizer_cls is None and exists('sg ge 0.6.5'):
            normalizer_cls = sgk.IdentityKernelNormalizer
        self._normalizer_cls = normalizer_cls

        if normalizer_args is None:
            normalizer_args = []
        self._normalizer_args = normalizer_args

    def _compute(self, d1, d2):
        d1 = SGKernel._data2features(d1)
        d2 = SGKernel._data2features(d2)
        try:
            order = self.__kp_order__
        except AttributeError:
            # XXX may be we could use param.index to have them sorted?
            order = self.params.keys()
        kvals = [self.params[kp].value for kp in order]
        self._k = self.__kernel_cls__(d1, d2, *kvals)

        if self._normalizer_cls:
            self._k.set_normalizer(
                self._normalizer_cls(*self._normalizer_args))


class CustomSGKernel(_BasicSGKernel):
    """Class which can wrap any Shogun kernel and it's kernel parameters
    """
    # TODO: rename args here for convenience?
    def __init__(self, kernel_cls, kernel_params=[], **kwargs):
        """Initialize CustomSGKernel.

        Parameters
        ----------
        kernel_cls : Shogun.Kernel
          Class of a Kernel from Shogun
        kernel_params : list
          Each item in this list should be a tuple of (kernelparamname, value),
          and the order is the explicit order required by the Shogun constructor
        """
        self.__kernel_cls__ = kernel_cls # These are normally static

        _BasicSGKernel.__init__(self, **kwargs)
        order = []
        for k, v in kernel_params:
            self.params[k] = Parameter(default=v)
            order.append(k)
        self.__kp_order__ = tuple(order)

class LinearSGKernel(_BasicSGKernel):
    """A basic linear kernel computed via Shogun: K(a,b) = a*b.T"""
    __kernel_cls__ = sgk.LinearKernel
    __kernel_name__ = 'linear'


class RbfSGKernel(_BasicSGKernel):
    """Radial basis function: K(a,b) = exp(-||a-b||**2/sigma)"""
    __kernel_cls__ = sgk.GaussianKernel
    __kernel_name__ = 'rbf'
    sigma = Parameter(1, doc="Width/division parameter for gaussian kernel")

    def __init__(self, **kwargs):
        # Necessary for proper docstring construction
        _BasicSGKernel.__init__(self, **kwargs)


class PolySGKernel(_BasicSGKernel):
    """Polynomial kernel: K(a,b) = (a*b.T + c)**degree
    c is 1 if and only if 'inhomogenous' is True
    """
    __kernel_cls__ = sgk.PolyKernel
    __kernel_name__ = 'poly'
    __kp_order__ = ('degree', 'inhomogenous')
    degree = Parameter(2, constraints='int', doc="Polynomial order of the kernel")
    inhomogenous = Parameter(True, constraints='bool',
                             doc="Whether +1 is added within the expression")

    if not exists('sg ge 0.6.5'):

        use_normalization = Parameter(False, constraints='bool',
                                      doc="Optional normalization")
        __kp_order__ = __kp_order__ + ('use_normalization',)

    def __init__(self, **kwargs):
        # Necessary for proper docstring construction
        _BasicSGKernel.__init__(self, **kwargs)

class PrecomputedSGKernel(SGKernel):
    """A kernel which is precomputed from a numpy array or a Shogun kernel"""
    # This class can't be handled directly by BasicSGKernel because it never
    # should take data, and never has compute called, etc

    # NB: To avoid storing kernel twice, self.params.matrix = self._k once the
    # kernel is 'computed'

    def __init__(self, matrix=None, **kwargs):
        """Initialize PrecomputedSGKernel

        Parameters
        ----------
        matrix : SGKernel or Kernel or ndarray
          Kernel matrix to be used
        """
        # Convert to appropriate kernel for input
        if isinstance(matrix, SGKernel):
            k = matrix._k # Take internal shogun
        elif isinstance(matrix, Kernel):
            k = matrix.as_raw_np() # Convert to NP otherwise
        else:
            # Otherwise SG would segfault ;-)
            k = np.array(matrix)

        SGKernel.__init__(self, **kwargs)

        if versions['shogun:rev'] >= 4455:
            self._k = sgk.CustomKernel(k)
        else:
            raise RuntimeError, \
                  "Cannot create PrecomputedSGKernel using current version" \
                  " of shogun -- please upgrade"
            # Following lines are not effective since we should have
            # also provided data for CK in those earlier versions
            #self._k = sgk.CustomKernel()
            #self._k.set_full_kernel_matrix_from_full(k)

    def compute(self, *args, **kwargs):
        """'Compute' `PrecomputedSGKernel` -- no actual "computation" is done
        """
        pass

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Basic, general purpose and meta mappers."""

__docformat__ = 'restructuredtext'

import numpy as np
import copy

from mvpa2.base.learner import Learner
from mvpa2.base.node import ChainNode
from mvpa2.base.types import is_datasetlike, accepts_dataset_as_samples
from mvpa2.base.dochelpers import _str, _repr_attrs
from mvpa2.base.dochelpers import borrowdoc

if __debug__:
    from mvpa2.base import debug

def _assure_consistent_a(ds, oshape):
    """If ds.shape differs from oshape, invoke set_length_check
       for the corresponding collection
    """
    shape = ds.shape
    if oshape[0] != shape[0]:
        ds.sa.set_length_check(shape[0])
    if oshape[1] != shape[1]:
        ds.fa.set_length_check(shape[1])


class Mapper(Learner):
    """Basic mapper interface definition.

    ::

              forward
             --------->
         IN              OUT
             <--------/
               reverse

    """
    def __init__(self, **kwargs):
        """
        Parameters
        ----------
        **kwargs
          All additional arguments are passed to the baseclass.
        """
        Learner.__init__(self, **kwargs)
        # internal settings that influence what should be done to the dataset
        # attributes in the default forward() and reverse() implementations.
        # they are passed to the Dataset.copy() method
        self._sa_filter = None
        self._fa_filter = None
        self._a_filter = None


    #
    # The following methods are abstract and merely define the intended
    # interface of a mapper and have to be implemented in derived classes. See
    # the docstrings of the respective methods for details about what they
    # should do.
    #
    def _forward_data(self, data):
        """Forward-map some data.

        This is a private method that has to be implemented in derived
        classes.

        Parameters
        ----------
        data : anything (supported the derived class)
        """
        raise NotImplementedError


    def _reverse_data(self, data):
        """Reverse-map some data.

        This is a private method that has to be implemented in derived
        classes.

        Parameters
        ----------
        data : anything (supported the derived class)
        """
        raise NotImplementedError


    #
    # The following methods are candidates for reimplementation in derived
    # classes, in cases where the provided default behavior is not appropriate.
    #
    def _forward_dataset(self, dataset):
        """Forward-map a dataset.

        This is a private method that can be reimplemented in derived
        classes. The default implementation forward-maps the dataset samples
        and returns a new dataset that is a shallow copy of the input with
        the mapped samples.

        Parameters
        ----------
        dataset : Dataset-like
        """
        if __debug__:
            debug('MAP_', "Forward-map %s-shaped samples in dataset with '%s'."
                        % (dataset.samples.shape, self))
        msamples = self._forward_data(dataset.samples)
        if __debug__:
            debug('MAP_', "Make shallow copy of to-be-forward-mapped dataset "
                    "and assigned forward-mapped samples ({sf}a_filters: "
                    "%s, %s, %s)." % (self._sa_filter, self._fa_filter,
                                      self._a_filter))
        mds = dataset.copy(deep=False,
                           sa=self._sa_filter,
                           fa=self._fa_filter,
                           a=self._a_filter)
        mds.samples = msamples
        _assure_consistent_a(mds, dataset.shape)

        if __debug__:
            debug('MAP_', "Return forward-mapped dataset.")
        return mds


    def _reverse_dataset(self, dataset):
        """Reverse-map a dataset.

        This is a private method that can be reimplemented in derived
        classes. The default implementation reverse-maps the dataset samples
        and returns a new dataset that is a shallow copy of the input with
        the mapped samples.

        Parameters
        ----------
        dataset : Dataset-like
        """
        if __debug__:
            debug('MAP_', "Reverse-map %s-shaped samples in dataset with '%s'."
                        % (dataset.samples.shape, self))
        msamples = self._reverse_data(dataset.samples)
        if __debug__:
            debug('MAP_', "Make shallow copy of to-be-reverse-mapped dataset "
                    "and assigned reverse-mapped samples ({sf}a_filters: "
                    "%s, %s, %s)." % (self._sa_filter, self._fa_filter,
                                      self._a_filter))
        mds = dataset.copy(deep=False,
                           sa=self._sa_filter,
                           fa=self._fa_filter,
                           a=self._a_filter)
        mds.samples = msamples
        _assure_consistent_a(mds, dataset.shape)

        return mds


    #
    # The following methods provide common functionality for all mappers
    # and there should be no immediate need to reimplement them
    #
    def forward(self, data):
        """Map data from input to output space.

        Parameters
        ----------
        data : Dataset-like, (at least 2D)-array-like
          Typically this is a `Dataset`, but it might also be a plain data
          array, or even something completely different(TM) that is supported
          by a subclass' implementation. If such an object is Dataset-like it
          is handled by a dedicated method that also transforms dataset
          attributes if necessary. If an array-like is passed, it has to be
          at least two-dimensional, with the first axis separating samples
          or observations. For single samples `forward1()` might be more
          appropriate.
        """
        if is_datasetlike(data):
            if __debug__:
                debug('MAP', "Forward-map %s-shaped dataset through '%s'."
                        % (data.shape, self))
            return self._forward_dataset(data)
        else:
            if hasattr(data, 'ndim') and data.ndim < 2:
                raise ValueError(
                    'Mapper.forward() only support mapping of data with '
                    'at least two dimensions, where the first axis '
                    'separates samples/observations. Consider using '
                    'Mapper.forward1() instead.')
            if __debug__:
                debug('MAP', "Forward-map data through '%s'." % (self))
            return self._forward_data(data)


    def forward1(self, data):
        """Wrapper method to map single samples.

        It is basically identical to `forward()`, but also accepts
        one-dimensional arguments. The map whole dataset this method cannot
        be used. but `forward()` handles them.
        """
        if isinstance(data, np.ndarray):
            data = data[np.newaxis]
        else:
            data = np.array([data])
        if __debug__:
            debug('MAP', "Forward-map single %s-shaped sample through '%s'."
                    % (data.shape[1:], self))
        return self.forward(data)[0]



    def reverse(self, data):
        """Reverse-map data from output back into input space.

        Parameters
        ----------
        data : Dataset-like, anything
          Typically this is a `Dataset`, but it might also be a plain data
          array, or even something completely different(TM) that is supported
          by a subclass' implementation. If such an object is Dataset-like it
          is handled by a dedicated method that also transforms dataset
          attributes if necessary.
        """
        if is_datasetlike(data):
            if __debug__:
                debug('MAP', "Reverse-map %s-shaped dataset through '%s'."
                        % (data.shape, self))
            return self._reverse_dataset(data)
        else:
            if __debug__:
                debug('MAP', "Reverse-map data through '%s'." % (self))
            return self._reverse_data(data)


    def reverse1(self, data):
        """Wrapper method to map single samples.

        It is basically identical to `reverse()`, but accepts one-dimensional
        arguments. To map whole dataset this method cannot be used. but
        `reverse()` handles them.
        """
        if isinstance(data, np.ndarray):
            data = data[np.newaxis]
        else:
            data = np.array([data])
        if __debug__:
            debug('MAP', "Reverse-map single %s-shaped sample through '%s'."
                    % (data.shape[1:], self))
        mapped = self.reverse(data)[0]
        if __debug__:
            debug('MAP', "Mapped single %s-shaped sample to %s."
                    % (data.shape[1:], mapped.shape))
        return mapped


    def _call(self, ds):
        return self.forward(ds)



class ChainMapper(ChainNode):
    """Class that amends ChainNode with a mapper-like interface.

    ChainMapper supports sequential training of a mapper chain, as well as
    reverse-mapping and mapping of single samples.
    """
    def forward(self, ds):
        return self(ds)


    def forward1(self, data):
        """Forward data or datasets through the chain.

        See `Mapper` for more information.
        """
        mp = data
        for m in self:
            if __debug__:
                debug('MAP', "Forwarding single input (%s) though '%s'."
                        % (mp.shape, str(m)))
            mp = m.forward1(mp)
        return mp


    def reverse(self, data):
        """Reverse-maps data or datasets through the chain (backwards).

        See `Mapper` for more information.
        """
        mp = data
        for m in reversed(self):
            # we ignore mapper that do not have reverse mapping implemented
            # (e.g. detrending). That might cause problems if ignoring the
            # mapper make the data incompatible input for the next mapper in
            # the chain. If that pops up, we have to think about a proper
            # solution.
            try:
                if __debug__:
                    debug('MAP',
                          "Reversing %s-shaped input though '%s'."
                           % (mp.shape, str(m)))
                mp = m.reverse(mp)
            except NotImplementedError:
                if __debug__:
                    debug('MAP', "Ignoring %s on reverse mapping." % m)
        return mp


    def reverse1(self, data):
        """Reverse-maps data or datasets through the chain (backwards).

        See `Mapper` for more information.
        """
        mp = data
        for i, m in enumerate(reversed(self)):
            # we ignore mapper that do not have reverse mapping implemented
            # (e.g. detrending). That might cause problems if ignoring the
            # mapper make the data incompatible input for the next mapper in
            # the chain. If that pops up, we have to think about a proper
            # solution.
            try:
                if __debug__:
                    debug('MAP',
                          "Reversing single %s-shaped input though chain node '%s'."
                           % (mp.shape, str(m)))
                mp = m.reverse1(mp)
            except NotImplementedError:
                if __debug__:
                    debug('MAP', "Ignoring %s on reverse mapping." % m)
            except ValueError:
                if __debug__:
                    debug('MAP',
                          "Failed to reverse-map through chain at '%s'. Maybe "
                          "previous mapper return multiple samples. Trying to "
                          "switch to reverse() for the remainder of the chain."
                          % str(m))
                mp = self[:-1 * i].reverse(mp)
                return mp
        return mp


    def train(self, dataset):
        """Train the mapper chain sequentially.

        The training dataset is used to train the first mapper. Afterwards it is
        forward-mapped by this (now trained) mapper and the transformed dataset
        and then used to train the next mapper. This procedure is done till all
        mappers are trained.

        Parameters
        ----------
        dataset: `Dataset`
        """
        nmappers = len(self) - 1
        tdata = dataset
        for i, mapper in enumerate(self):
            if __debug__:
                debug('MAP',
                      "Training child mapper (%i/%i) %s with %s-shaped input."
                      % (i + 1, nmappers + 1, str(mapper), tdata.shape))
            mapper.train(tdata)
            # forward through all but the last mapper
            if i < nmappers:
                tdata = mapper.forward(tdata)


    def untrain(self):
        """Untrain all embedded mappers."""
        for m in self:
            m.untrain()


    def __str__(self):
        return super(ChainMapper, self).__str__().replace('Mapper', '')



class CombinedMapper(Mapper):
    """Mapper to pass a dataset on to a set of mappers and combine there output.

    Output combination or aggregation is currently done by hstacking or
    vstacking the resulting datasets.
    """

    def __init__(self, mappers, combine_axis, a=None, **kwargs):
        """
        Parameters
        ----------
        mappers : list
        combine_axis : ['h', 'v']
        a: {'unique','drop_nonunique','uniques','all'} or True or False or None (default: None)
            Indicates which dataset attributes from datasets are stored 
            in merged_dataset. If an int k, then the dataset attributes from 
            datasets[k] are taken. If 'unique' then it is assumed that any
            attribute common to more than one dataset in datasets is unique;
            if not an exception is raised. If 'drop_nonunique' then as 'unique',
            except that exceptions are not raised. If 'uniques' then, for each 
            attribute,  any unique value across the datasets is stored in a tuple 
            in merged_datasets. If 'all' then each attribute present in any 
            dataset across datasets is stored as a tuple in merged_datasets; 
            missing values are replaced by None. If None (the default) then no 
            attributes are stored in merged_dataset. True is equivalent to
            'drop_nonunique'. False is equivalent to None.

        Examples
        --------
        >>> import numpy as np
        >>> from mvpa2.mappers.base import CombinedMapper
        >>> from mvpa2.featsel.base import StaticFeatureSelection
        >>> from mvpa2.datasets import Dataset
        >>> mp = CombinedMapper([StaticFeatureSelection([1,2]),
        ...                      StaticFeatureSelection([2,3])],
        ...                     combine_axis='h')
        >>> mp.is_trained = True
        >>> ds = Dataset(np.arange(12).reshape(3,4))
        >>> out = mp(ds)
        >>> out.samples
        array([[ 1,  2,  2,  3],
               [ 5,  6,  6,  7],
               [ 9, 10, 10, 11]])
        """
        Mapper.__init__(self, **kwargs)
        self._mappers = mappers
        self._combine_axis = combine_axis
        self._a = a

    @borrowdoc(Mapper)
    def __repr__(self, prefixes=[]):
        return super(CombinedMapper, self).__repr__(
                prefixes=prefixes
                    + _repr_attrs(self, ['mappers', 'combine_axis', 'a']))

    def __str__(self):
        return _str(self)

    def _train(self, ds):
        for mapper in self._mappers:
            mapper.train(ds)

    def _untrain(self):
        for mapper in self._mappers:
            mapper.untrain()

    @borrowdoc(Mapper)
    def _forward_dataset(self, ds):
        from mvpa2.datasets import hstack, vstack
        mapped_ds = [mapper.forward(ds) for mapper in self._mappers]
        stacker = {'h': hstack, 'v': vstack}
        out = stacker[self._combine_axis](mapped_ds, self._a)
        return out

    mappers = property(fget=lambda self:self._mappers)


########NEW FILE########
__FILENAME__ = boxcar
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Transform consecutive samples into individual multi-dimensional samples"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.mappers.base import Mapper
from mvpa2.clfs.base import accepts_dataset_as_samples
from mvpa2.base.dochelpers import _str

if __debug__:
    from mvpa2.base import debug

class BoxcarMapper(Mapper):
    """Mapper to combine multiple samples into a single sample.

    Notes
    -----

    This mapper is somewhat unconventional since it doesn't preserve number
    of samples (ie the size of 0-th dimension).
    """
    # TODO: extend with the possibility to provide real onset vectors and a
    #       samples attribute that is used to determine the actual sample that
    #       is matching a particular onset. The difference between target onset
    #       and sample could be stored as an additional sample attribute. Some
    #       utility functionality (outside BoxcarMapper) could be used to merge
    #       arbitrary sample attributes into the samples matrix (with
    #       appropriate mapper adjustment, e.g. CombinedMapper).
    def __init__(self, startpoints, boxlength, offset=0, **kwargs):
        """
        Parameters
        ----------
        startpoints : sequence
          Index values along the first axis of 'data'.
        boxlength : int
          The number of elements after 'startpoint' along the first axis of
          'data' to be considered for the boxcar.
        offset : int
          The offset between the provided starting point and the actual start
          of the boxcar.
        """
        Mapper.__init__(self, **kwargs)
        self._outshape = None

        startpoints = np.asanyarray(startpoints)
        if np.issubdtype(startpoints.dtype, 'i'):
            self.startpoints = startpoints
        else:
            if __debug__:
                debug('MAP', "Boxcar: obtained startpoints are not of int type."
                      " Rounding and changing dtype")
            self.startpoints = np.asanyarray(np.round(startpoints), dtype='i')

        # Sanity checks
        if boxlength < 1:
            raise ValueError, "Boxlength lower than 1 makes no sense."
        if boxlength - int(boxlength) != 0:
            raise ValueError, "boxlength must be an integer value."

        self.boxlength = int(boxlength)
        self.offset = offset
        self.__selectors = None

        # build a list of list where each sublist contains the indexes of to be
        # averaged data elements
        self.__selectors = [ slice(i + offset, i + offset + boxlength) \
                             for i in startpoints ]


    def __reduce__(self):
        # python < 2.6 cannot copy slices, we will use the constructor the get
        # them back and additionally reapply the stae of the object (except for
        # the bad bad slices)
        state = self.__dict__.copy()
        badguy = '_%s__selectors' % self.__class__.__name__
        if badguy in state:
            del state[badguy]
        return (self.__class__,
                    (self.startpoints, self.boxlength, self.offset),
                    state)


    @accepts_dataset_as_samples
    def _train(self, data):
        startpoints = self.startpoints
        boxlength = self.boxlength
        if __debug__:
            offset = self.offset
            for sp in startpoints:
                if ( sp + offset + boxlength - 1 > len(data)-1 ) \
                   or ( sp + offset < 0 ):
                    raise ValueError('Illegal box (start: %i, offset: %i, '
                          'length: %i) with total input sample being %i.' \
                          % (sp, offset, boxlength, len(data)))
        self._outshape = (len(startpoints), boxlength) + data.shape[1:]


    def __repr__(self):
        s = super(BoxcarMapper, self).__repr__()
        return s.replace("(", "(boxlength=%d, offset=%d, startpoints=%s, " %
                         (self.boxlength, self.offset, str(self.startpoints)),
                         1)


    def __str__(self):
        return _str(self, bl=self.boxlength)


    def forward1(self, data):
        # if we have a single 'raw' sample (not a boxcar)
        # extend it to cover the full box -- useful if one
        # wants to forward map a mask in raw dataspace (e.g.
        # fMRI ROI or channel map) into an appropriate mask vector
        if not self._outshape:
            raise RuntimeError("BoxcarMapper needs to be trained before "
                               ".forward1() can be used.")
        # first axes need to match
        if not data.shape[0] == self._outshape[2]:
            raise ValueError("Data shape %s does not match sample shape %s."
                             % (data.shape[0], self._outshape[2]))

        return np.vstack([data[np.newaxis]] * self.boxlength)


    def _forward_data(self, data):
        """Project an ND matrix into N+1D matrix

        This method also handles the special of forward mapping a single 'raw'
        sample. Such a sample is extended (by concatenating clones of itself) to
        cover a full boxcar. This functionality is only availably after a full
        data array has been forward mapped once.

        Returns
        -------
        array: (#startpoint, ...)
        """
        # NOTE: _forward_dataset() relies on the assumption that the following
        # also works with 1D arrays and still yields sane results
        return np.vstack([data[box][np.newaxis] for box in self.__selectors])


    def _forward_dataset(self, dataset):
        msamp = self._forward_data(dataset.samples)
        # make a shallow copy of the dataset, but excluding all sample
        # and feature attributes, since they need to be transformed anyway
        mds = dataset.copy(deep=False, sa=[], fa=[])
        # assign the new samples and adjust the length check of the collections
        mds.samples = msamp
        mds.sa.set_length_check(len(mds))
        mds.fa.set_length_check(mds.nfeatures)
        # map old feature attributes -- which simply get broadcasted along the
        # boxcar
        for k in dataset.fa:
            mds.fa[k] = self.forward1(dataset.fa[k].value)
        # map old sample attributes -- which simply get stacked into one for all
        # boxcar elements/samples
        for k in dataset.sa:
            # using _forward_data() instead of forward(), since we know that
            # this implementation can actually deal with 1D-arrays
            mds.sa[k] = self._forward_data(dataset.sa[k].value)
        # create the box offset attribute if space name is given
        if self.get_space():
            mds.fa[self.get_space() + '_offsetidx'] = np.arange(self.boxlength,
                                                                dtype='int')
            mds.sa[self.get_space() + '_onsetidx'] = self.startpoints.copy()
        return mds


    def reverse1(self, data):
        if __debug__:
            if not data.shape == self._outshape[1:]:
                raise ValueError("BoxcarMapper has not been train to "
                                 "reverse-map %s-shaped data, but %s."
                                 % (data.shape, self._outshape[1:]))

        # reimplemented since it is really only that
        return data


    def _reverse_data(self, data):
        if len(data.shape) < 2:
            # this is not something that this mapper created -- let's broadcast
            # its elements and hope that it would work
            return np.repeat(data, self.boxlength)

        # stack them all together -- this will cause overlapping boxcars to
        # result in multiple identical samples
        if not data.shape[1] == self.boxlength:
            # stacking doesn't make sense, since we got something strange
            raise ValueError("%s cannot reverse-map, since the number of "
                             "elements along the second axis (%i) does not "
                             "match the boxcar-length (%i)."
                             % (self.__class__.__name__,
                                data.shape[1],
                                self.boxlength))

        return np.concatenate(data)


    def _reverse_dataset(self, dataset):
        msamp = self._reverse_data(dataset.samples)
        # make a shallow copy of the dataset, but excluding all sample
        # and feature attributes, since they need to be transformed anyway
        mds = dataset.copy(deep=False, sa=[], fa=[])
        mds.samples = msamp
        mds.sa.set_length_check(len(mds))
        mds.fa.set_length_check(mds.nfeatures)
        # map old feature attributes -- which simply is taken the first one
        # and kill the inspace attribute, since it 
        inspace = self.get_space()
        for k in dataset.fa:
            if inspace is None or k != (inspace + '_offsetidx'):
                mds.fa[k] = dataset.fa[k].value[0]
        # reverse-map old sample attributes
        for k in dataset.sa:
            if inspace is None or k != (inspace + '_onsetidx'):
                mds.sa[k] = self._reverse_data(dataset.sa[k].value)
        return mds

########NEW FILE########
__FILENAME__ = detrend
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Polynomial de-trending and regression."""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.base.types import is_sequence_type

from mvpa2.base import externals
if externals.exists('scipy', raise_=True):
    # if we construct the polynomials ourselves, we wouldn't need scipy here
    from scipy.special import legendre

    def legendre_(n, x):
        """Helper to avoid problems with scipy 0.8.0 returning inf for -1

        Scipy 0.8.0 (and possibly later) has regression of reporting
        'inf's for negative boundary. Lets guard against it for now
        """
        leg = legendre(n)
        r = leg(x)
        infs = np.isinf(r)
        if np.any(infs):
            r[infs] = leg(x[infs] + 1e-10) # offset to try to overcome problems
        return r

from mvpa2.base.dochelpers import _str, borrowkwargs
from mvpa2.mappers.base import Mapper
from ..base.param import Parameter
from ..base import constraints as cts

class PolyDetrendMapper(Mapper):
    """Mapper for regression-based removal of polynomial trends.

    Noteworthy features are the possibility for chunk-wise detrending, optional
    regressors, and the ability to use positional information about the samples
    from the dataset.

    Any sample attribute from the to be mapped dataset can be used to define
    `chunks` that shall be detrended separately. The number of chunks is
    determined from the number of unique values of the attribute and all samples
    with the same value are considered to be in the same chunk.

    It is possible to provide a list of additional sample attribute names that
    will be used as confound regressors during detrending. This, for example,
    allows to use fMRI motion correction parameters to be considered.

    Finally, it is possible to use positional information about the dataset
    samples for the detrending. This is useful, for example, if the samples in
    the dataset are not equally spaced out over the acquisition time-window.
    In that case an actually linear trend in the data would be distorted and
    not properly removed. By setting the `inspace` argument to the name of a
    samples attribute that carries this information, the mapper will take this
    into account and shift the polynomials accordingly. If `inspace` is given,
    but the dataset doesn't contain such an attribute evenly spaced coordinates
    are generated and this information is stored in the mapped dataset.

    Notes
    -----
    The mapper only support mapping of datasets, not plain data. Moreover,
    reverse mapping, or subsequent forward-mapping of partial datasets are
    currently not implemented.

    Examples
    --------
    >>> from mvpa2.datasets import dataset_wizard
    >>> from mvpa2.mappers.detrend import PolyDetrendMapper
    >>> samples = np.array([[1.0, 2, 3, 3, 2, 1],
    ...                    [-2.0, -4, -6, -6, -4, -2]]).T
    >>> chunks = [0, 0, 0, 1, 1, 1]
    >>> ds = dataset_wizard(samples, chunks=chunks)
    >>> dm = PolyDetrendMapper(chunks_attr='chunks', polyord=1)

    >>> # the mapper will be auto-trained upon first use
    >>> mds = dm.forward(ds)

    >>> # total removal all all (chunk-wise) linear trends
    >>> np.sum(np.abs(mds)) < 0.00001
    True
    """
    polyord = Parameter(1, doc=\
          """Order of the Legendre polynomial to remove from the data.  This
          will remove every polynomial up to and including the provided
          value.  For example, 3 will remove 0th, 1st, 2nd, and 3rd order
          polynomials from the data.  np.B.: The 0th polynomial is the
          baseline shift, the 1st is the linear trend.
          If you specify a single int and the `chunks_attr` parameter is not None, then this value
          is used for each chunk.  You can also specify a different polyord
          value for each chunk by providing a list or ndarray of polyord
          values with the length equal to the number of chunks.""",
          constraints=cts.AltConstraints(cts.EnsureInt()))

    chunks_attr = Parameter(None, doc=\
          """If None, the whole dataset is detrended at once. Otherwise, the given
          samples attribute (given by its name) is used to define chunks of the
          dataset that are processed individually. In that case, all the samples
          within a chunk should be in contiguous order and the chunks should be
          sorted in order from low to high -- unless the dataset provides
          information about the coordinate of each sample in the space that
          should be spanned be the polynomials (see `space` argument).""",
          constraints=cts.AltConstraints(None, cts.EnsureStr()))

    opt_regs = Parameter(None, doc=\
          """List of sample attribute names that should be used as
          additional regressors.  An example use would be to regress out motion
          parameters.""",
          constraints=cts.AltConstraints(None, cts.EnsureListOf(str)))

    def __init__(self, polyord=1, chunks_attr=None, opt_regs=None, **kwargs):
        """
        Parameters
        ----------
        space : str or None
          If not None, a samples attribute of the same name is added to the
          mapped dataset that stores the coordinates of each sample in the
          space that is spanned by the polynomials. If an attribute of that
          name is already present in the input dataset its values are interpreted
          as sample coordinates in the space that should be spanned by the
          polynomials.
        """
        # keep param init for historical reasons
        self.params.chunks_attr = chunks_attr
        self.params.polyord = polyord
        self.params.opt_regs = opt_regs

        # things that come from train()
        self._polycoords = None
        self._regs = None

        # secret switch to perform in-place detrending
        self._secret_inplace_detrend = False

        # need to init last to prevent base class puking
        Mapper.__init__(self, **kwargs)


    def __repr__(self):
        s = super(PolyDetrendMapper, self).__repr__()
        return s.replace("(",
                         "(polyord=%i, chunks_attr=%s, opt_regs=%s, "
                          % (self.params.polyord,
                             repr(self.params.chunks_attr),
                             repr(self.params.opt_regs)),
                         1)

    def __str__(self):
        return _str(self, ord=self.params.polyord)


    def _scale_array(self, a):
        # scale an array into the interval [-1,1] using its min and max values
        # as input range
        return a / ((a.max() - a.min()) / 2) - 1


    def _get_polycoords(self, ds, chunk_slicer):
        """Compute human-readable and scaled polycoords.

        Parameters
        ----------
        ds : dataset
        chunk_slicer : boolean array
          True elements indicate samples selected for detrending.

        Returns
        -------
        tuple
          (real-world polycoords, scaled polycoords into [-1,1])
        """
        inspace = self.get_space()
        if chunk_slicer is None:
            nsamples = len(ds)
        else:
            nsamples = chunk_slicer.sum()

        # if we don't have to take care of an inspace thing are easy
        if inspace is None:
            polycoords_scaled = np.linspace(-1, 1, nsamples)
            return None, polycoords_scaled
        else:
            # there is interest in the inspace, but do we have information, or
            # just want to store it later on
            if inspace in ds.sa:
                # we have info
                if chunk_slicer is None:
                    chunk_slicer = slice(None)
                polycoords = ds.sa[inspace].value[chunk_slicer]
            else:
                # no info in the dataset, just be nice and generate some
                # meaningful polycoords
                polycoords = np.arange(nsamples)
            return polycoords, self._scale_array(polycoords.astype('float'))


    def _train(self, ds):
        # local binding
        chunks_attr = self.params.chunks_attr
        polyord = self.params.polyord
        opt_reg = self.params.opt_regs
        inspace = self.get_space()
        self._polycoords = None

        # global detrending is desired
        if chunks_attr is None:
            # consider the entire dataset
            reg = []
            # create the timespan
            self._polycoords, polycoords_scaled = self._get_polycoords(ds, None)
            for n in range(polyord + 1):
                reg.append(legendre_(n, polycoords_scaled)[:, np.newaxis])
        # chunk-wise detrending is desired
        else:
             # get the unique chunks
            uchunks = ds.sa[chunks_attr].unique

            # Process the polyord to be a list with length of the number of
            # chunks
            if not is_sequence_type(polyord):
                # repeat to be proper length
                polyord = [polyord] * len(uchunks)
            elif not chunks_attr is None and len(polyord) != len(uchunks):
                raise ValueError("If you specify a sequence of polyord values "
                                 "they sequence length must match the "
                                 "number of unique chunks in the dataset.")

            # loop over each chunk
            reg = []
            update_polycoords = True
            # if the dataset know about the inspace we can store the
            # polycoords right away
            if not inspace is None and inspace in ds.sa:
                self._polycoords = ds.sa[inspace].value
                update_polycoords = False
            else:
                # otherwise we prepare and empty array that is going to be
                # filled below -- we know that those polycoords are going to
                # be ints
                self._polycoords = np.empty(len(ds), dtype='int')
            for n, chunk in enumerate(uchunks):
                # get the indices for that chunk
                cinds = ds.sa[chunks_attr].value == chunk

                # create the timespan
                polycoords, polycoords_scaled = self._get_polycoords(ds, cinds)
                if update_polycoords and not polycoords is None:
                    self._polycoords[cinds] = polycoords
                # create each polyord with the value for that chunk
                for n in range(polyord[n] + 1):
                    newreg = np.zeros((len(ds), 1))
                    newreg[cinds, 0] = legendre_(n, polycoords_scaled)
                    reg.append(newreg)

        # if we don't handle in inspace, there is no need to store polycoords
        if inspace is None:
            self._polycoords = None

        # see if add in optional regs
        if not opt_reg is None:
            # add in the optional regressors, too
            for oreg in opt_reg:
                reg.append(ds.sa[oreg].value[np.newaxis].T)

        # combine the regs (time x reg)
        self._regs = np.hstack(reg)


    def _forward_dataset(self, ds):
        # auto-train the mapper if not yet done
        if self._regs is None:
            self.train(ds)

        if self._secret_inplace_detrend:
            mds = ds
        else:
            # shallow copy to put the new stuff in
            mds = ds.copy(deep=False)

        # local binding
        regs = self._regs
        inspace = self.get_space()
        polycoords = self._polycoords

        # is it possible to map that dataset?
        if inspace is None and len(regs) != len(ds):
            raise ValueError("Cannot detrend the dataset, since it neither "
                             "provides location information of its samples "
                             "in the space spanned by the polynomials, "
                             "nor does it match the number of samples this "
                             "this mapper has been trained on. (got: %i "
                             " and was trained on %i)."
                             % (len(ds), len(regs)))
        # do we have to handle the polynomial space somehow?
        if not inspace is None:
            if inspace in ds.sa:
                space_coords = ds.sa[inspace].value
                # this dataset has some notion about our polyspace
                # we need to put the right regressor values for its samples
                # let's first see whether the coords are identical to the
                # trained ones (that should be the common case and nothing needs
                # to be done
                # otherwise look whether we can find the right regressors
                if not np.all(space_coords == polycoords):
                    # to make the stuff below work, we'd need to store chunk
                    # info too, otherwise we cannot determine the correct
                    # regressor rows
                    raise NotImplementedError
                    # determine the regressor rows that match the samples
                    reg_idx = [np.argwhere(polycoords == c).flatten()[0]
                                    for c in space_coords]
                    # slice the regressors accordingly
                    regs = regs[reg_idx]
            else:
                # the input dataset knows nothing about the polyspace
                # let's put that information into the output dataset
                mds.sa[inspace] = self._polycoords

        # regression for each feature
        fit = np.linalg.lstsq(regs, ds.samples)
        # actually we are only interested in the solution
        # res[0] is (nregr x nfeatures)
        y = fit[0]
        # remove all and keep only the residuals
        if self._secret_inplace_detrend:
            # if we are in evil mode do evil

            # cast the data to float, since in-place operations below do not
            # upcast!
            if np.issubdtype(mds.samples.dtype, np.integer):
                mds.samples = mds.samples.astype('float')

            mds.samples -= np.dot(regs, y)
        else:
            # important to assign to ensure COW behavior
            mds.samples = ds.samples - np.dot(regs, y)

        return mds



    def _forward_data(self, data):
        raise RuntimeError("%s cannot map plain data."
                           % self.__class__.__name__)



@borrowkwargs(PolyDetrendMapper, '__init__')
def poly_detrend(ds, **kwargs):
    """In-place polynomial detrending.

    This function behaves identical to the `PolyDetrendMapper`. The only
    difference is that the actual detrending is done in-place -- potentially
    causing a significant reduction of the memory demands.

    Parameters
    ----------
    ds : Dataset
      The dataset that will be detrended in-place.
    **kwargs
      For all other arguments, please see the documentation of
      PolyDetrendMapper.
    """
    dm = PolyDetrendMapper(**kwargs)
    dm._secret_inplace_detrend = True
    # map
    mapped = dm.forward(ds)
    # and append the mapper to the dataset
    mapped._append_mapper(dm)

########NEW FILE########
__FILENAME__ = filters
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Spectral filtering and FFT-based resampling."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import externals

if externals.exists('scipy', raise_=True):
    from scipy.signal import resample
    from mvpa2.support.scipy.signal import filtfilt

from mvpa2.base import warning
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import EnsureChoice, EnsureInt, EnsureNone
from mvpa2.base.dochelpers import _str, borrowkwargs
from mvpa2.mappers.base import Mapper
from mvpa2.datasets import Dataset
from mvpa2.base.dataset import vstack
from mvpa2.generators.splitters import Splitter

class FFTResampleMapper(Mapper):
    """Mapper for FFT-based resampling.

    Can do per-chunk.

    Supports positional information of samples and outputs them as sample
    attribute. however, only meaningful for data with equally spaced sampling
    points.

    Pretty much Mapper frontend for scipy.signal.resample

    """
    def __init__(self, num, window=None, chunks_attr=None, position_attr=None,
                 attr_strategy='remove', **kwargs):
        """
        Parameters
        ----------
        num : int
          Number of output samples. If operating on chunks, this is the number
          of samples per chunk.
        window : str or float or tuple
          Passed to scipy.signal.resample
        chunks_attr : str or None
          If not None, this samples attribute defines chunks that will be
          resampled individually.
        position_attr : str
          A samples attribute with positional information that is passed
          to scipy.signal.resample. If not None, the output dataset will
          also contain a sample attribute of this name, with updated
          positional information (this is, however, only meaningful for
          equally spaced samples).
        attr_strategy : {'remove', 'sample', 'resample'}
          Strategy to process sample attributes during mapping. 'remove' will
          cause all sample attributes to be removed. 'sample' will pick orginal
          attribute values matching the new resampling frequency (e.g. every
          10th), and 'resample' will also apply the actual data resampling
          procedure to the attributes as well (which might not be possible, e.g.
          for literal attributes).
        """
        Mapper.__init__(self, **kwargs)

        self.__num = num
        self.__window_args = window
        self.__chunks_attr = chunks_attr
        self.__position_attr = position_attr
        self.__attr_strategy = attr_strategy


    def __repr__(self):
        s = super(FFTResamplemapper, self).__repr__()
        return s.replace("(",
                         "(chunks_attr=%s, "
                          % (repr(self.__chunks_attr),),
                         1)


    def __str__(self):
        return _str(self, chunks_attr=self.__chunks_attr)


    def _forward_data(self, data):
        # we cannot have position information without a dataset
        return resample(data, self.__num, t=None, window=self.__window_args)


    def _forward_dataset(self, ds):
        if self.__chunks_attr is None:
            return self._forward_dataset_helper(ds)
        else:
            # strip down dataset to speedup local processing
            if self.__attr_strategy == 'remove':
                keep_sa = []
            else:
                keep_sa = None
            proc_ds = ds.copy(deep=False, sa=keep_sa, fa=[], a=[])
            # process all chunks individually
            # use a customsplitter to speed-up splitting
            spl = Splitter(self.__chunks_attr)
            dses = [self._forward_dataset_helper(d)
                        for d in spl.generate(proc_ds)]
            # and merge them again
            mds = vstack(dses)
            # put back attributes
            mds.fa.update(ds.fa)
            mds.a.update(ds.a)
            return mds


    def _forward_dataset_helper(self, ds):
        # local binding
        num = self.__num

        pos = None
        if not self.__position_attr is None:
            # we know something about sample position
            pos = ds.sa[self.__position_attr].value
            rsamples, pos = resample(ds.samples, self.__num, t=pos,
                                     window=self.__window_args)
        else:
            # we know nothing about samples position
            rsamples = resample(ds.samples, self.__num, t=None,
                                window=self.__window_args)
        # new dataset that reuses that feature and dataset attributes of the
        # source
        mds = Dataset(rsamples, fa=ds.fa, a=ds.a)

        # the tricky part is what to do with the samples attributes, since their
        # number has changes
        if self.__attr_strategy == 'remove':
            # nothing to be done
            pass
        elif self.__attr_strategy == 'sample':
            step = int(len(ds) / num)
            sa = dict([(k, ds.sa[k].value[0::step][:num]) for k in ds.sa])
            mds.sa.update(sa)
        elif self.__attr_strategy == 'resample':
            # resample the attributes themselves
            sa = {}
            for k in ds.sa:
                v = ds.sa[k].value
                if pos is None:
                    sa[k] = resample(v, self.__num, t=None,
                                     window=self.__window_args)
                else:
                    if k == self.__position_attr:
                        # position attr will be handled separately at the end
                        continue
                    sa[k] = resample(v, self.__num, t=pos,
                                     window=self.__window_args)[0]
            # inject them all
            mds.sa.update(sa)
        else:
            raise ValueError("Unkown attribute handling strategy '%s'."
                             % self.__attr_strategy)

        if not pos is None:
            # we got the new sample positions and can store them
            mds.sa[self.__position_attr] = pos
        return mds


@borrowkwargs(FFTResampleMapper, '__init__')
def fft_resample(ds, num, **kwargs):
    """FFT-based resampling.

    Parameters
    ----------
    ds : Dataset
    **kwargs
      For all other arguments, please see the documentation of
      FFTResampleMapper.
    """
    dm = FFTResampleMapper(num, **kwargs)
    return dm.forward(ds)


class IIRFilterMapper(Mapper):
    """Mapper using IIR filters for data transformation.

    This mapper is able to perform any IIR-based low-pass, high-pass, or
    band-pass frequency filtering. This is a front-end for SciPy's filtfilt(),
    hence its usage looks almost exactly identical, and any of SciPy's IIR
    filters can be used with this mapper:

    >>> from scipy import signal
    >>> b, a = signal.butter(8, 0.125)
    >>> mapper = IIRFilterMapper(b, a, padlen=150)

    """

    axis = Parameter(0, constraints='int',
            doc="""The axis of `x` to which the filter is applied. By default
            the filter is applied to all features along the samples axis""")

    padtype = Parameter('odd', 
            constraints=EnsureChoice('odd', 'even', 'constant') | EnsureNone(),
            doc="""Must be 'odd', 'even', 'constant', or None.  This determines
            the type of extension to use for the padded signal to which the
            filter is applied.  If `padtype` is None, no padding is used.  The
            default is 'odd'""")

    padlen = Parameter(None, constraints=EnsureInt() | EnsureNone(),
            doc="""The number of elements by which to extend `x` at both ends
            of `axis` before applying the filter. This value must be less than
            `x.shape[axis]-1`.  `padlen=0` implies no padding. The default
            value is 3*max(len(a),len(b))""")

    def __init__(self, b, a, **kwargs):
        """
        All constructor parameters are analogs of filtfilt() or are passed
        on to the Mapper base class.

        Parameters
        ----------
        b : (N,) array_like
            The numerator coefficient vector of the filter.
        a : (N,) array_like
            The denominator coefficient vector of the filter.  If a[0]
            is not 1, then both a and b are normalized by a[0].
        """
        Mapper.__init__(self, auto_train=True, **kwargs)
        self.__iir_num = b
        self.__iir_denom = a

    def _forward_data(self, data):
        params = self.params
        try:
            mapped = filtfilt(self.__iir_num,
                              self.__iir_denom,
                              data,
                              axis=params.axis,
                              padtype=params.padtype,
                              padlen=params.padlen)
        except TypeError:
            # we have an ancient scipy, do manually
            # but is will only support 2d arrays
            if params.axis == 0:
                data = data.T
            if params.axis > 1:
                raise ValueError("this version of scipy does not "
                                 "support nd-arrays for filtfilt()")
            if not (params['padlen'].is_default and params['padtype'].is_default):
                warning("this version of scipy.signal.filtfilt() does not "
                        "support `padlen` and `padtype` arguments -- ignoring "
                        "them")
            mapped = [filtfilt(self.__iir_num,
                               self.__iir_denom,
                               x)
                    for x in data]
            mapped = np.array(mapped)
            if params.axis == 0:
                mapped = mapped.T
        return mapped


@borrowkwargs(IIRFilterMapper, '__init__')
def iir_filter(ds, *args, **kwargs):
    """IIR-based frequency filtering.

    Parameters
    ----------
    ds : Dataset
    **kwargs
      For all other arguments, please see the documentation of
      IIRFilterMapper.
    """
    dm = IIRFilterMapper(*args, **kwargs)
    return dm.forward(ds)

########NEW FILE########
__FILENAME__ = flatten
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Flatten multi-dimensional samples"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.dochelpers import _str, _repr_attrs
from mvpa2.mappers.base import Mapper, accepts_dataset_as_samples, \
        ChainMapper
from mvpa2.featsel.base import StaticFeatureSelection
from mvpa2.misc.support import is_in_volume

if __debug__:
    from mvpa2.base import debug

class FlattenMapper(Mapper):
    """Reshaping mapper that flattens multidimensional arrays into 1D vectors.

    This mapper performs relatively cheap reshaping of arrays from ND into 1D
    and back upon reverse-mapping. The mapper has to be trained with a data
    array or dataset that has the first axis as the samples-separating
    dimension. Mapper training will set the particular multidimensional shape
    the mapper is transforming into 1D vector samples. The setting remains in
    place until the mapper is retrained.

    Notes
    -----
    At present this mapper is only designed (and tested) to work with C-ordered
    arrays.
    """
    def __init__(self, shape=None, maxdims=None, **kwargs):
        """
        Parameters
        ----------
        shape : tuple
          The shape of a single sample. If this argument is given the mapper
          is going to be fully configured and no training is necessary anymore.
        maxdims : int or None
          The maximum number of dimensions to flatten (starting with the first).
          If None, all axes will be flattened.
        """
        # by default auto train
        kwargs['auto_train'] = kwargs.get('auto_train', True)
        Mapper.__init__(self, **kwargs)
        self.__origshape = None         # pylint pacifier
        self.__maxdims = maxdims
        if not shape is None:
            self._train_with_shape(shape)

    def __repr__(self, prefixes=[]):
        return super(FlattenMapper, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['shape', 'maxdims']))

    def __str__(self):
        return _str(self)


    @accepts_dataset_as_samples
    def _train(self, samples):
        """Train the mapper.

        Parameters
        ----------
        samples : array-like
          The first axis has to represent the samples-separating dimension. In
          case of a 1D-array each element is considered to be an individual
          element and *not* the whole array as a single sample!
        """
        self._train_with_shape(samples.shape[1:])


    def _train_with_shape(self, shape):
        """Configure the mapper with a particular sample shape.
        """
        # infer the sample shape from the data under the assumption that the
        # first axis is the samples-separating dimension
        self.__origshape = shape
        # flag the mapper as trained
        self._set_trained()


    def _forward_data(self, data):
        # this method always gets data where the first axis is the samples axis!
        # local binding
        nsamples = data.shape[0]
        sshape = data.shape[1:]
        oshape = self.__origshape

        if oshape is None:
            raise RuntimeError("FlattenMapper needs to be trained before it "
                               "can be used.")
        # at least the first feature axis has to match match
        if oshape[0] != sshape[0]:
            raise ValueError("FlattenMapper has not been trained for data "
                             "shape '%s' (known only '%s')."
                             % (str(sshape), str(oshape)))
        ## input matches the shape of a single sample
        #if sshape == oshape:
        #    return data.reshape(nsamples, -1)
        ## the first part of the shape matches (e.g. some additional axes present)
        #elif sshape[:len(oshape)] == oshape:
        if not self.__maxdims is None:
            maxdim = min(len(oshape), self.__maxdims)
        else:
            maxdim = len(oshape)
        # flatten the pieces the mapper knows about and preserve the rest
        return data.reshape((nsamples, -1) + sshape[maxdim:])



    def _forward_dataset(self, dataset):
        # invoke super class _forward_dataset, this calls, _forward_dataset
        # and this calls _forward_data in this class
        mds = super(FlattenMapper, self)._forward_dataset(dataset)
        # attribute collection needs to have a new length check
        mds.fa.set_length_check(mds.nfeatures)
        # we need to duplicate all existing feature attribute, as each original
        # feature is now spread across the new feature axis
        # take all "additional" axes after the actual feature axis and count
        # elements a sample -- if not axis exists this will be 1
        for k in dataset.fa:
            if __debug__:
                debug('MAP_', "Forward-mapping fa '%s'." % k)
            attr = dataset.fa[k].value
            # the maximmum number of axis to flatten in the attr
            if not self.__maxdims is None:
                maxdim = min(len(self.__origshape), self.__maxdims)
            else:
                maxdim = len(self.__origshape)
            multiplier = mds.nfeatures \
                    / np.prod(attr.shape[:maxdim])
            if __debug__:
                debug('MAP_', "Broadcasting fa '%s' %s %d times"
                        % (k, attr.shape, multiplier))
            # broadcast as many times as necessary to get 'matching dimensions'
            bced = np.repeat(attr, multiplier, axis=0)
            # now reshape as many dimensions as the mapper knows about
            mds.fa[k] = bced.reshape((-1,) + bced.shape[maxdim:])

        # if there is no inspace return immediately
        if self.get_space() is None:
            return mds
        # otherwise create the coordinates as feature attributes
        else:
            mds.fa[self.get_space()] = \
                list(np.ndindex(dataset.samples[0].shape))
            return mds


    def _reverse_data(self, data):
        # this method always gets data where the first axis is the samples axis!
        # local binding
        nsamples = data.shape[0]
        sshape = data.shape[1:]
        oshape = self.__origshape
        return data.reshape((nsamples,) + oshape + sshape[1:])


    def _reverse_dataset(self, dataset):
        # invoke super class _reverse_dataset, this calls, _reverse_dataset
        # and this calles _reverse_data in this class
        mds = super(FlattenMapper, self)._reverse_dataset(dataset)
        # attribute collection needs to have a new length check
        mds.fa.set_length_check(mds.nfeatures)
        # now unflatten all feature attributes
        inspace = self.get_space()
        for k in mds.fa:
            # reverse map all attributes, but not the inspace indices, since the
            # did not come through this mapper and make not sense in inspace
            if k != inspace:
                mds.fa[k] = self.reverse1(mds.fa[k].value)
        # wipe out the inspace attribute -- needs to be done after the loop to
        # not change the size of the dict
        if inspace and inspace in mds.fa:
            del mds.fa[inspace]
        return mds

    shape = property(fget=lambda self:self.__origshape)
    maxdims = property(fget=lambda self:self.__maxdims)

class ProductFlattenMapper(FlattenMapper):
    """Reshaping mapper that flattens multidimensional arrays and
    preserves information for each dimension in feature attributes

    Notes
    -----
    This class' name contains 'product' because it maps feature
    attributes in a cartesian-product way."""

    def __init__(self, factor_names, factor_values=None, **kwargs):
        '''
        Parameters
        ----------
        factor_names: iterable
            The names for each dimension. If the dataset to
            be flattened is shaped ns X nf1 x nf2 x ... x nfN, then
            factor_names should have a length of N. Furthermore
            when applied to a dataset ds, it should have each
            of the factor names factor_names[K] as an attribute and the value
            of this attribute should have nfK values.
            Applying this mapper to such a dataset yields a new dataset
            with size ns X (nf1 * nf2 * ... * nfN) with
            feature attributes nameK and nameKindices for each nameK
            in the factor names.
        factor_values: iterable or None
            Optionally the factor values for each dimension. If
            not provided or set to None, then it will be inferred
            upon training on a dataset. Setting this parameter
            explicitly means this instance does not have to be trained.
        '''
        kwargs['auto_train'] = kwargs.get('auto_train', True)

        # make sure the factor names and values are properly set
        factor_names = list(factor_names)

        # override default value for space argument
        space = kwargs.get('space', None)
        if kwargs.get('space', None) is None:
            kwargs['space'] = '_'.join(factor_names) + '_indices'

        super(ProductFlattenMapper, self).__init__(**kwargs)

        self._factor_names = factor_names

        if factor_values is not None:
            if len(factor_values) != len(factor_names):
                raise ValueError('factor_values must have %d elements, '
                                 'found %d' % (len(factor_names),
                                               len(factor_names)))
        self._factor_values = factor_values

    def __repr__(self, prefixes=[]):
        return super(ProductFlattenMapper, self).__repr__(
                        prefixes=prefixes
                        + _repr_attrs(self, ['factor_names',
                                             'factor_values']))

    @property
    def factor_names(self):
        return self._factor_names

    @property
    def factor_values(self):
        return self._factor_values

    def _train(self, ds):
        super(ProductFlattenMapper, self)._train(ds)
        self._factor_values = []
        for nm in self._factor_names:
            if not nm in ds.a.keys():
                raise KeyError("Missing attribute: %s" % nm)
            self._factor_values.append(ds.a[nm].value)


    def _untrain(self):
        self._factor_values = None

    def _check_factor_name_values(self, ds):
        ### currently unished...
        self._check_is_trained()
        for nm, value in zip(*(self._factor_names, self._factor_values)):
            if any(ds.a[nm].value != value):
                raise ValueError("Mismatch for attribute %s: %s != %s" %
                                        (nm, value, ds.a[nm].value))


    def _forward_dataset(self, dataset):
        self._train(dataset)

        mds = super(ProductFlattenMapper, self)._forward_dataset(dataset)

        oshape = self.shape

        factor_names_values = zip(*(self._factor_names, self._factor_values))
        # now map all the factor names and values to feature attributes
        for i, (name, value) in enumerate(factor_names_values):
            # keep track of both the value itself and the indices
            for repr, postfix in ((value, None),
                                  (np.arange(len(value)), '_indices')):

                nshape = [1] + list(oshape) # full shape with one sample
                nshape[i + 1] = 1 # dimension of current factor

                # shape for repr with 1 value at all dimensions except the
                # current one. In other words nshapa and ushape complement
                # each other.
                ushape = [1] * len(nshape)
                ushape[i + 1] = len(value)

                # reshape and tile
                repr_rs = np.reshape(np.asarray(repr), ushape)
                repr_arr = np.tile(repr_rs, nshape)

                # ensure that values have the proper shape
                if repr_arr.shape[1:] != oshape:
                    raise ValueError("Shape mismatch: %s != %s - this should"
                                    " not happen" % ((repr_arr.shape,), (oshape,)))

                # flatten the attributes
                repr_flat = self.forward(repr_arr)
                # assigne as feature attribute
                fa_label = name if postfix is None else name + postfix

                mds.fa[fa_label] = repr_flat.ravel()

            del mds.a[name]

        return mds

    def _check_is_trained(self):
        if self._factor_values is None or self.shape is None:
            raise ValueError("Dataset is not trained, and factor_values not "
                             "given in constructor of %s" %
                                            self.__class__.__name__)



    def _reverse_dataset(self, dataset):
        self._check_is_trained()

        factor_names_values = zip(*(self._factor_names, self._factor_values))

        mds = super(ProductFlattenMapper, self)._reverse_dataset(dataset)
        postfix = '_indices'
        for name, _ in factor_names_values:
            label = name + postfix
            if label in mds.fa:
                del mds.fa[label]

        for nm, values in factor_names_values: #:self._get_reversed_factor_name_values(mds):
            if nm in mds.a.keys() and any(mds.a[nm].value != values):
                raise ValueError("name clash for %s" % nm)
            del mds.fa[nm]
            mds.a[nm] = values

        return mds


    def _get_reversed_factor_name_values(self, reversed_dataset):
        '''Helper function to trace back the original names and values
        after the mapper reversed a dataset.

        Parameters
        ----------
        reversed_dataset: Dataset
            The instance that was reversed using this instance

        Returns
        -------
        factor_names_values: iterable
            The names and values for each dimension. If the reversed_dataset
            is shaped ns X nf1 x nf2 x ... x nfN, then
            factor_names_values will have a length of N. Furthermore
            the K-th element in factor_names_values is a tuple
            (nameK, valueK) where nameK is a string and valueK has
            length nfK.

        Notes
        -----
        It is not guaranteed that the order of valueK matches the
        corresponding element in self.factor_names_values that was
        supplied to the __init__ method
        '''

        output_factor_names = []
        factor_names_values = zip(*(self._factor_names, self._factor_values))
        for dim, (name, values) in enumerate(factor_names_values):
            vs = reversed_dataset.fa[name].value

            if dim > 0:
                vs = vs.swapaxes(0, dim)

            n = vs.shape[0]
            assert(n == len(values))

            unq_vs = []
            for i in xrange(n):
                v = vs[i, ...]
                unq_v = np.unique(v)
                assert(unq_v.size == 1)
                assert(unq_v in values)
                unq_vs.append(unq_v)

            output_factor_names.append((name, np.asarray(unq_vs).ravel()))

        return output_factor_names


def mask_mapper(mask=None, shape=None, space=None):
    """Factory method to create a chain of Flatten+StaticFeatureSelection Mappers

    Parameters
    ----------
    mask : None or array
      an array in the original dataspace and its nonzero elements are
      used to define the features included in the dataset. Alternatively,
      the `shape` argument can be used to define the array dimensions.
    shape : None or tuple
      The shape of the array to be mapped. If `shape` is provided instead
      of `mask`, a full mask (all True) of the desired shape is
      constructed. If `shape` is specified in addition to `mask`, the
      provided mask is extended to have the same number of dimensions.
    inspace
      Provided to `FlattenMapper`
    """
    if mask is None:
        if shape is None:
            raise ValueError, \
                  "Either `shape` or `mask` have to be specified."
        else:
            # make full dataspace mask if nothing else is provided
            mask = np.ones(shape, dtype='bool')
    else:
        if not shape is None:
            # expand mask to span all dimensions but first one
            # necessary e.g. if only one slice from timeseries of volumes is
            # requested.
            mask = np.array(mask, copy=False, subok=True, ndmin=len(shape))
            # check for compatibility
            if not shape == mask.shape:
                raise ValueError, \
                    "The mask dataspace shape %s is not " \
                    "compatible with the provided shape %s." \
                    % (mask.shape, shape)

    fm = FlattenMapper(shape=mask.shape, space=space)
    flatmask = fm.forward1(mask)
    mapper = ChainMapper([fm,
                          StaticFeatureSelection(
                              flatmask,
                              dshape=flatmask.shape,
                              oshape=(len(flatmask.nonzero()[0]),))])
    return mapper

########NEW FILE########
__FILENAME__ = fx
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Transform data by applying a function along samples or feature axis."""

__docformat__ = 'restructuredtext'

import numpy as np
import inspect

from mvpa2.base import warning
from mvpa2.base.node import Node
from mvpa2.datasets import Dataset
from mvpa2.base.dochelpers import _str, _repr_attrs
from mvpa2.mappers.base import Mapper
from mvpa2.misc.support import array_whereequal
from mvpa2.base.dochelpers import borrowdoc

from mvpa2.misc.transformers import sum_of_abs, max_of_abs

if __debug__:
    from mvpa2.base import debug

class FxMapper(Mapper):
    """Apply a custom transformation to (groups of) samples or features.
    """

    is_trained = True
    """Indicate that this mapper is always trained."""

    def __init__(self, axis, fx, fxargs=None, uattrs=None,
                 attrfx='merge', order='uattrs'):
        """
        Parameters
        ----------
        axis : {'samples', 'features'}
        fx : callable
        fxargs : tuple
        uattrs : list
          List of attribute names to consider. All possible combinations
          of unique elements of these attributes are used to determine the
          sample groups to operate on.
        attrfx : callable
          Functor that is called with each sample attribute elements matching
          the respective samples group. By default the unique value is
          determined. If the content of the attribute is not uniform for a
          samples group a unique string representation is created.
          If `None`, attributes are not altered.
        order : {'uattrs', 'occurrence', None}
          If which order groups should be merged together.  If `None` (default
          before 2.3.1), the order is imposed only by the order of
          `uattrs` as keys in the dictionary, thus can vary from run to run.
          If `'occurrence'`, groups will be ordered by the first occurrence
          of group samples in original dataset. If `'uattrs'`, groups will be
          sorted by the values of uattrs with follow-up attr having higher
          importance for ordering (e .g. `uattrs=['targets', 'chunks']` would
          order groups first by `chunks` and then by `targets` within each
          chunk).
        """
        Mapper.__init__(self)

        if not axis in ['samples', 'features']:
            raise ValueError("%s `axis` arguments can only be 'samples' or "
                             "'features' (got: '%s')." % repr(axis))
        self.__axis = axis
        self.__uattrs = uattrs
        self.__fx = fx
        if not fxargs is None:
            self.__fxargs = fxargs
        else:
            self.__fxargs = ()
        if attrfx == 'merge':
            self.__attrfx = _uniquemerge2literal
        else:
            self.__attrfx = attrfx
        assert(order in (None, 'uattrs', 'occurrence'))
        self.__order = order


    @borrowdoc(Mapper)
    def __repr__(self, prefixes=[]):
        return super(FxMapper, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['axis', 'fx', 'uattrs'])
            + _repr_attrs(self, ['fxargs'], default=())
            + _repr_attrs(self, ['attrfx'], default='merge')
            + _repr_attrs(self, ['order'], default='uattrs')
            )


    def __str__(self):
        return _str(self, fx=self.__fx.__name__)


    def _train(self, ds):
        # right now it needs no training, if anything is added here make sure to
        # remove is_trained class attribute
        pass

    def __smart_apply_along_axis(self, data):
        # because apply_along_axis could be very much slower than a
        # direct invocation of native functions capable of operating
        # along specific axis, let's make it smarter for those we know
        # could do that.
        fx = None
        naxis = {'samples': 0, 'features': 1}[self.__axis]
        try:
            # if first argument is 'axis' -- just proceed with a native call
            if inspect.getargs(self.__fx.__code__).args[1] == 'axis':
                fx = self.__fx
            elif __debug__:
                debug('FX', "Will apply %s via apply_along_axis",
                          (self.__fx))
        except Exception, e:
            if __debug__:
                debug('FX',
                      "Failed to deduce either %s has 'axis' argument: %s",
                      (self.__fx, repr(e)))
            pass

        if fx is not None:
            if __debug__:
                debug('FX', "Applying %s directly to data giving axis=%d",
                      (self.__fx, naxis))
            mdata = fx(data, naxis, *self.__fxargs)
        else:
            # either failed to deduce signature or just didn't
            # have 'axis' second
            # apply fx along naxis for each sample/feature
            mdata = np.apply_along_axis(self.__fx, naxis, data, *self.__fxargs)
        assert(mdata.ndim in (data.ndim, data.ndim-1))
        return mdata

    @borrowdoc(Mapper)
    def _forward_data(self, data):
        if not self.__uattrs is None:
            raise RuntimeError("%s does not support forward-mapping of plain "
                               "data when data grouping based on attributes "
                               "is requested"
                               % self.__class__.__name__)

        mdata = self.__smart_apply_along_axis(data)

        if self.__axis == 'features':
            if len(mdata.shape) == 1:
                # in case we only have a scalar per sample we need to transpose
                # it properly, to keep the length of the samples axis intact
                mdata = np.atleast_2d(mdata).T
        return np.atleast_2d(mdata)

    @borrowdoc(Mapper)
    def _forward_dataset(self, ds):
        if self.__uattrs is None:
            mdata, sattrs = self._forward_dataset_full(ds)
        else:
            mdata, sattrs = self._forward_dataset_grouped(ds)

        samples = np.atleast_2d(mdata)

        # return early if there is no attribute treatment desired
        if self.__attrfx is None:
            out = ds.copy(deep=False)
            out.samples = samples
            return out

        # not copying the samples attributes, since they have to be modified
        # anyway
        if self.__axis == 'samples':
            out = ds.copy(deep=False, sa=[])
            col = out.sa
            incol = ds.sa
            col.set_length_check(samples.shape[0])
        else:
            out = ds.copy(deep=False, fa=[])
            col = out.fa
            incol = ds.fa
            col.set_length_check(samples.shape[1])
        # assign samples to do COW
        out.samples = samples

        for attr in sattrs:
            a = sattrs[attr]
            # TODO -- here might puke if e.g it is a list where some items
            # are empty lists... I guess just wrap in try/except and
            # do dtype=object if catch
            a = np.atleast_1d(a)
            # make sure we do not inflate the number of dimensions for no reason
            # this could happen if there was only one unique value for an
            # attribute and the default 'uniquemerge2literal' attrfx was given
            if len(a.shape) > 1 and a.shape[-1] == 1 and attr in incol \
                    and len(a.shape) > len(incol[attr].value.shape):
                a.shape = a.shape[:-1]
            col[attr] = a

        return out


    def _forward_dataset_grouped(self, ds):
        mdata = [] # list of samples array pieces
        if self.__axis == 'samples':
            col = ds.sa
            axis = 0
        elif self.__axis == 'features':
            col = ds.fa
            axis = 1
        else:
            raise RuntimeError("This should not have happened!")

        attrs = dict(zip(col.keys(), [[] for i in col]))

        # create a dictionary for all unique elements in all attribute this
        # mapper should operate on
        self.__attrcombs = dict(zip(self.__uattrs,
                                [col[attr].unique for attr in self.__uattrs]))
        # let it generate all combinations of unique elements in any attr
        order = self.order
        order_keys = []
        for comb in _orthogonal_permutations(self.__attrcombs):
            selector = reduce(np.multiply,
                                [array_whereequal(col[attr].value, value)
                                 for attr, value in comb.iteritems()])

            # process the samples
            if axis == 0:
                samples = ds.samples[selector]
            else:
                samples = ds.samples[:, selector]

            # check if there were any samples for such a combination,
            # if not -- warning and skip the rest of the loop body
            if not len(samples):
                warning('There were no samples for combination %s. It might be '
                        'a sign of a disbalanced dataset %s.' % (comb, ds))
                continue

            fxed_samples = self.__smart_apply_along_axis(samples)
            mdata.append(fxed_samples)
            if not self.__attrfx is None:
                # and now all samples attributes
                fxed_attrs = [self.__attrfx(col[attr].value[selector])
                                    for attr in col]
                for i, attr in enumerate(col):
                    attrs[attr].append(fxed_attrs[i])
            # possibly take care about collecting information to have groups ordered
            if order == 'uattrs':
                # reverse order as per docstring -- most of the time we have
                # used uattrs=['targets', 'chunks'] and did expect chunks being
                # groupped together.
                order_keys.append([comb[a] for a in self.__uattrs[::-1]])
            elif order == 'occurrence':
                # First index should be sufficient since we are dealing
                # with unique non-overlapping groups here (AFAIK ;) )
                order_keys.append(np.where(selector)[0][0])

        if order:
            # reorder our groups using collected "order_keys"
            # data
            order_idxs = argsort(order_keys)
            mdata = [mdata[i] for i in order_idxs]
            # and attributes
            attrs = dict((k, [v[i] for i in order_idxs])
                         for k,v in attrs.iteritems())

        if axis == 0:
            mdata = np.vstack(mdata)
        else:
            mdata = np.vstack(np.transpose(mdata))
        return mdata, attrs


    def _forward_dataset_full(self, ds):
        # simply map the all of the data
        mdata = self._forward_data(ds.samples)

        # if the attributes should not be handled, don't handle them
        if self.__attrfx is None:
            return mdata, None

        # and now all attributes
        if self.__axis == 'samples':
            attrs = dict(zip(ds.sa.keys(),
                              [self.__attrfx(ds.sa[attr].value)
                                    for attr in ds.sa]))
        if self.__axis == 'features':
            attrs = dict(zip(ds.fa.keys(),
                              [self.__attrfx(ds.fa[attr].value)
                                    for attr in ds.fa]))
        return mdata, attrs

    axis = property(fget=lambda self:self.__axis)
    fx = property(fget=lambda self:self.__fx)
    fxargs = property(fget=lambda self:self.__fxargs)
    uattrs = property(fget=lambda self:self.__uattrs)
    attrfx = property(fget=lambda self:self.__attrfx)
    order = property(fget=lambda self:self.__order)

#
# Convenience functions to create some useful mapper with less complexity
#

def mean_sample(attrfx='merge'):
    """Returns a mapper that computes the mean sample of a dataset.

    Parameters
    ----------
    attrfx : 'merge' or callable, optional
      Callable that is used to determine the sample attributes of the computed
      mean samples. By default this will be a string representation of all
      unique value of a particular attribute in any sample group. If there is
      only a single value in a group it will be used as the new attribute value.

    Returns
    -------
    FxMapper instance.
    """
    return FxMapper('samples', np.mean, attrfx=attrfx)


def mean_group_sample(attrs, attrfx='merge', **kwargs):
    """Returns a mapper that computes the mean samples of unique sample groups.

    The sample groups are identified by the unique combination of all
    values of a set of provided sample attributes.  Order of output
    samples might differ from original and correspond to sorted order
    of corresponding `attrs`  by default.  Use `order='occurrence'` if you would
    like to maintain the order.

    Parameters
    ----------
    attrs : list
      List of sample attributes whose unique values will be used to identify the
      samples groups.
    attrfx : 'merge' or callable, optional
      Callable that is used to determine the sample attributes of the computed
      mean samples. By default this will be a string representation of all
      unique value of a particular attribute in any sample group. If there is
      only a single value in a group it will be used as the new attribute value.

    Returns
    -------
    FxMapper instance.
    """
    return FxMapper('samples', np.mean, uattrs=attrs, attrfx=attrfx, **kwargs)


def sum_sample(attrfx='merge'):
    """Returns a mapper that computes the sum sample of a dataset.

    Parameters
    ----------
    attrfx : 'merge' or callable, optional
      Callable that is used to determine the sample attributes of the computed
      sum samples. By default this will be a string representation of all
      unique value of a particular attribute in any sample group. If there is
      only a single value in a group it will be used as the new attribute value.

    Returns
    -------
    FxMapper instance.
    """
    return FxMapper('samples', np.sum, attrfx=attrfx)


def mean_feature(attrfx='merge'):
    """Returns a mapper that computes the mean feature of a dataset.

    Parameters
    ----------
    attrfx : 'merge' or callable, optional
      Callable that is used to determine the feature attributes of the computed
      mean features. By default this will be a string representation of all
      unique value of a particular attribute in any feature group. If there is
      only a single value in a group it will be used as the new attribute value.

    Returns
    -------
    FxMapper instance.
    """
    return FxMapper('features', np.mean, attrfx=attrfx)


def mean_group_feature(attrs, attrfx='merge', **kwargs):
    """Returns a mapper that computes the mean features of unique feature groups.

    The feature groups are identified by the unique combination of all values of
    a set of provided feature attributes.  Order of output
    features might differ from original and correspond to sorted order
    of corresponding `attrs` by default.  Use `order='occurrence'` if you would
    like to maintain the order.

    Parameters
    ----------
    attrs : list
      List of feature attributes whos unique values will be used to identify the
      feature groups.
    attrfx : 'merge' or callable, optional
      Callable that is used to determine the feature attributes of the computed
      mean features. By default this will be a string representation of all
      unique value of a particular attribute in any feature group. If there is
      only a single value in a group it will be used as the new attribute value.

    Returns
    -------
    FxMapper instance.
    """
    return FxMapper('features', np.mean, uattrs=attrs, attrfx=attrfx, **kwargs)


def absolute_features():
    """Returns a mapper that converts features into absolute values.

    This mapper does not alter any attributes.

    Returns
    -------
    FxMapper instance.
    """
    return FxMapper('features', np.absolute, attrfx=None)


def sumofabs_sample():
    """Returns a mapper that returns the sum of absolute values of all samples.
    """
    return FxMapper('samples', sum_of_abs)

def maxofabs_sample():
    """Returns a mapper that finds max of absolute values of all samples.
    """
    return FxMapper('samples', max_of_abs)
#
# Utility functions
#

def _uniquemerge2literal(attrs):
    """Compress a sequence into its unique elements (with string merge).

    Whenever there is more then one unique element in `attrs`, these
    are converted to a string and join with a '+' character inbetween.

    Parameters
    ----------
    attrs : sequence, arbitrary

    Returns
    -------
    Non-sequence arguments are passed as is, otherwise a sequences of unique
    items is. None is returned in case of an empty sequence.
    """
    try:
        if isinstance(attrs[0], basestring):
            # do not try to disassemble sequences of strings
            raise TypeError
        unq = [np.array(u) for u in set([tuple(p) for p in attrs])]
    except TypeError:
        # either no 2d-iterable...
        try:
            unq = np.unique(attrs)
        except TypeError:
            # or no iterable at all -- return the original
            return attrs

    lunq = len(unq)
    if lunq > 1:
        return ['+'.join([str(l) for l in unq])]
    elif lunq:
        return unq
    else:
        return None

def merge2first(attrs):
    """Compress a sequence by discard all but the first element

    This function can be useful as 'attrfx' argument for an FxMapper.

    Parameters
    ----------
    attrs : sequence, arbitrary

    Returns
    -------
    First element of the input sequence.
    """
    return attrs[0]

def argsort(seq, reverse=False):
    """Return indices to get sequence sorted
    """
    # Based on construct from
    # http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python
    # Thanks!

    # cmp was not passed through since seems to be absent in python3
    return sorted(range(len(seq)), key=seq.__getitem__, reverse=reverse)

def _orthogonal_permutations(a_dict):
    """
    Takes a dictionary with lists as values and returns all permutations
    of these list elements in new dicts.

    This function is useful, when a method with several arguments
    shall be tested and all of the arguments can take several values.

    The order is not defined, therefore the elements should be
    orthogonal to each other.

    >>> for i in _orthogonal_permutations({'a': [1,2,3], 'b': [4,5]}):
    ...     print i
    {'a': 1, 'b': 4}
    {'a': 1, 'b': 5}
    {'a': 2, 'b': 4}
    {'a': 2, 'b': 5}
    {'a': 3, 'b': 4}
    {'a': 3, 'b': 5}
    """
    # Taken from MDP (LGPL)
    pool = dict(a_dict)
    args = []
    for func, all_args in pool.items():
        # check the size of the list in the second item of the tuple
        args_with_fun = [(func, arg) for arg in all_args]
        args.append(args_with_fun)
    for i in _product(args):
        yield dict(i)


def _product(iterable):
    # MDP took it and adapted it from itertools 2.6 (Python license)
    # PyMVPA took it from MDP (LGPL)
    pools = tuple(iterable)
    result = [[]]
    for pool in pools:
        result = [x+[y] for x in result for y in pool]
    for prod in result:
        yield tuple(prod)



class BinaryFxNode(Node):
    """Extract a dataset attribute and call a function with it and the samples.

    This node takes a dataset's samples and a configurable attribute and passes
    them to a custom callable. This node can be used to implement comparisons,
    or error quantifications.

    When called with a dataset the node returns a new dataset with the return
    value of the callable as samples.
    """
    # TODO: Allow using feature attributes too
    def __init__(self, fx, space, **kwargs):
        """
        Parameters
        ----------
        fx : callable
          Callable that is passed with the dataset samples as first and
          attribute values as second argument.
        space : str
          name of the sample attribute that contains the target values.
        """
        Node.__init__(self, space=space, **kwargs)
        self.fx = fx


    def _call(self, ds):
        # extract samples and targets and pass them to the errorfx
        targets = ds.sa[self.get_space()].value
        # squeeze to remove bogus dimensions and prevent problems during
        # comparision later on
        values = np.atleast_1d(ds.samples.squeeze())
        if not values.shape == targets.shape:
            # if they have different shape numpy's broadcasting might introduce
            # pointless stuff (compare individual features or yield a single
            # boolean
            raise ValueError("Trying to compute an error between data of "
                             "different shape (%s vs. %s)."
                             % (values.shape, targets.shape))
        err = self.fx(values, targets)
        if np.isscalar(err):
            err = np.array(err, ndmin=2)
        return Dataset(err)

########NEW FILE########
__FILENAME__ = fxy
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Evaluate functions on pairs of datasets"""

__docformat__ = 'restructuredtext'

from mvpa2.base.dochelpers import _str, _repr_attrs
from mvpa2.datasets import Dataset
from mvpa2.mappers.base import Mapper
from mvpa2.base.dochelpers import borrowdoc

class FxyMapper(Mapper):
    """Mapper to execute a callable with two datasets as arguments.

    The first dataset is passed to the mapper during training, the second
    dataset is passed to forward/call(). This mapper is useful to, for example,
    compare two datasets regarding particular aspects, merge them, or perform
    other operations that require the presence of two datasets.
    """

    def __init__(self, fx, train_as_1st=True, **kwargs):
        """
        Parameters
        ----------
        fx : callable
          Functor that is called with the two datasets upon forward-mapping.
        train_as_1st : bool
          If True, the training dataset is passed to the target callable as
          the first argument and the other dataset as the second argument.
          If False, it is vice versa.

        Examples
        --------
        >>> from mvpa2.mappers.fxy import FxyMapper
        >>> from mvpa2.datasets import Dataset
        >>> callable = lambda x,y: len(x) > len(y)
        >>> ds1 = Dataset(range(5))
        >>> ds2 = Dataset(range(3))
        >>> fxy = FxyMapper(callable)
        >>> fxy.train(ds1)
        >>> fxy(ds2).item()
        True
        >>> fxy = FxyMapper(callable, train_as_1st=False)
        >>> fxy.train(ds1)
        >>> fxy(ds2).item()
        False
        """
        Mapper.__init__(self, **kwargs)
        self._fx = fx
        self._train_as_1st = train_as_1st
        self._ds_train = None

    @borrowdoc(Mapper)
    def __repr__(self, prefixes=[]):
        return super(FxyMapper, self).__repr__(
                prefixes=prefixes + _repr_attrs(self, ['fx']))

    def __str__(self):
        return _str(self, fx=self._fx.__name__)

    def _train(self, ds):
        self._ds_train = ds

    def _untrain(self):
        self._ds_train = None

    @borrowdoc(Mapper)
    def _forward_dataset(self, ds):
        # apply function
        if self._train_as_1st:
            out = self._fx(self._ds_train, ds)
        else:
            out = self._fx(ds, self._ds_train)
        # wrap output in a dataset if necessary
        if not isinstance(out, Dataset):
            try:
                out = Dataset(out)
            except ValueError:
                # not a sequence?
                out = Dataset([out])
        return out

    fx = property(fget=lambda self:self.__fx)


########NEW FILE########
__FILENAME__ = nipy_glm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""GLMMapper implementation based on the NiPy package."""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals
if externals.exists('nipy', raise_=True):
    from nipy.modalities.fmri.glm import GeneralLinearModel

import numpy as np

from mvpa2.datasets import Dataset
from mvpa2.mappers.glm import GLMMapper

class NiPyGLMMapper(GLMMapper):
    """NiPy-based GLMMapper implementation

    This is basically a front-end for
    :class:`~ nipy.modalities.fmri.glm.GeneralLinearModel`.
    In particular, it supports all keyword arguments of its
    ``fit()`` method.
    """

    def __init__(self, regs, glmfit_kwargs=None, **kwargs):
        """
        Parameters
        ----------
        regs : list
          Names of sample attributes to be extracted from an input dataset and
          used as design matrix columns.
        glmfit_kwargs : dict, optional
          Keyword arguments to be passed to GeneralLinearModel.fit().
          By default an AR1 model is used.
        """
        GLMMapper.__init__(self, regs, **kwargs)
        if glmfit_kwargs is None:
            glmfit_kwargs = {}
        self.glmfit_kwargs = glmfit_kwargs

    def _fit_model(self, ds, X, reg_names):
        glm = GeneralLinearModel(X)
        glm.fit(ds.samples, **self.glmfit_kwargs)
        out = Dataset(glm.get_beta(),
                      sa={self.get_space(): reg_names})
        return glm, out

########NEW FILE########
__FILENAME__ = statsmodels_glm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""GLMMapper implementation based on the statsmodels package."""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals
if externals.exists('statsmodels', raise_=True):
    from mvpa2.measures.statsmodels_adaptor import UnivariateStatsModels
    import statsmodels.api as sm

import numpy as np

from mvpa2.datasets import Dataset
from mvpa2.mappers.glm import GLMMapper

class StatsmodelsGLMMapper(GLMMapper):
    """Statsmodels-based GLMMapper implementation

    This is basically a front-end for
    :class:`~mvpa2.measures.statsmodels_adaptor.UnivariateStatsModels`.
    In particular, it supports all ``model_gen`` and ``results`` arguments
    as described in the documentation for this class.
    """
    def __init__(self, regs, model_gen=None, results='params',
                 **kwargs):
        """
        Parameters
        ----------
        regs : list
          Names of sample attributes to be extracted from an input dataset and
          used as design matrix columns.
        model_gen : callable, optional
          See UnivariateStatsModels documentation for details on the
          specification of the model fitting procedure. By default an
          OLS model is used.
        results : str or array, optional
          See UnivariateStatsModels documentation for details on the
          specification of model fit results. By default parameter
          estimates are returned.
        """
        GLMMapper.__init__(self, regs, **kwargs)
        self.result_expr = results
        if model_gen is None:
            model_gen=lambda y, x: sm.OLS(y, x)
        self.model_gen = model_gen

    def _fit_model(self, ds, X, reg_names):
        mod = UnivariateStatsModels(
                X,
                res=self.result_expr,
                add_constant=False,
                model_gen=self.model_gen)
        res = mod(ds)
        res.sa[self.get_space()] = reg_names
        return mod, res

########NEW FILE########
__FILENAME__ = lle
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Local Linear Embedding.

This is a wrapper class around the corresponding MDP nodes LLE and HLLE
(since MDP 2.4).
"""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals

import numpy as np

from mvpa2.mappers.mdp_adaptor import MDPNodeMapper

if externals.exists('mdp ge 2.4', raise_=True):
    import mdp


class LLEMapper(MDPNodeMapper):
    """Locally linear embbeding Mapper.

    This mapper performs dimensionality reduction. It wraps two algorithms
    provided by the Modular Data Processing (MDP) framework.

    Locally linear embedding (LLE) approximates the input data with a
    low-dimensional surface and reduces its dimensionality by learning a
    mapping to the surface.

    This wrapper class provides access to two different LLE algorithms (i.e.
    the corresponding MDP processing nodes). 1) An algorithm outlined in *An
    Introduction to Locally Linear Embedding* by L. Saul and S. Roweis, using
    improvements suggested in *Locally Linear Embedding for Classification* by
    D. deRidder and R.pl.W. Duin (aka `LLENode`) and 2) Hessian Locally Linear
    Embedding analysis based on algorithm outlined in *Hessian Eigenmaps: new
    locally linear embedding techniques for high-dimensional data* by C. Grimes
    and D. Donoho, 2003.

    For more information see the MDP website at
    http://mdp-toolkit.sourceforge.net

    Notes
    -----
    This mapper only provides forward-mapping functionality -- no reverse
    mapping is available.
    """
    def __init__(self, k, alg='LLE', nodeargs=None, **kwargs):
        """
        Parameters
        ----------
        k : int
          Number of nearest neighbors to be used by the algorithm.
        algorithm : {'LLE', 'HLLE'}
          Either use the standard LLE algorithm or Hessian Linear Local
          Embedding (HLLE).
        nodeargs : None or dict
          Arguments passed to the MDP node in various stages of its lifetime.
          See the baseclass for more details.
        **kwargs
          Additional constructor arguments for the MDP node.
        """
        if alg == 'LLE':
            node = mdp.nodes.LLENode(k, **kwargs)
        elif alg == 'HLLE':
            node = mdp.nodes.HLLENode(k, **kwargs)
        else:
            raise ValueError("Unkown algorithm '%s' for LLEMapper.")

        MDPNodeMapper.__init__(self, node, nodeargs=nodeargs)

########NEW FILE########
__FILENAME__ = mdp_adaptor
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wrapper to use MDP nodes and flows as PyMVPA mappers.

This module provides to mapper that allow embedding MDP nodes, or flows
into PyMVPA.
"""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals
if externals.exists('mdp', raise_=True):
    import mdp

import numpy as np

from mvpa2.base.dataset import DatasetAttributeExtractor
from mvpa2.mappers.base import Mapper, accepts_dataset_as_samples
from mvpa2.misc.support import is_in_volume


class MDPNodeMapper(Mapper):
    """Mapper encapsulating an arbitray MDP node.

    This mapper wraps an MDP node and uses it for forward and reverse data
    mapping (reverse is only available if the underlying MDP node supports
    it).  It is possible to specify arbitrary arguments for all processing
    steps of an MDP node (training, training stop, execution, and
    inverse).

    Because MDP does not allow to 'reset' a node and (re)train it from
    scratch the mapper uses a copy of the initially wrapped node for the
    actual processing. Upon subsequent training attempts a new copy of the
    original node is made and replaces the previous one.

    Notes
    -----
    MDP nodes requiring multiple training phases are not supported. Use a
    MDPFlowWrapper for that. Moreover, it is not possible to perform
    incremental training of a node.
    """
    def __init__(self, node, nodeargs=None, **kwargs):
        """
        Parameters
        ----------
        node : mdp.Node instance
          This node instance is taken as the pristine source of which a
          copy is made for actual processing upon each training attempt.
        nodeargs : dict
          Dictionary for additional arguments for all calls to the MDP
          node. The dictionary key's meaning is as follows:
          
          'train'
            Arguments for calls to `Node.train()`
          'stoptrain'
            Arguments for calls to `Node.stop_training()`
          'exec'
            Arguments for calls to `Node.execute()`
          'inv'
            Arguments for calls to `Node.inverse()`
          
          The value for each item is always a 2-tuple, consisting of a
          tuple (for the arguments), and a dictionary (for keyword
          arguments), i.e.  ((), {}). Both, tuple and dictionary have to be
          provided even if they are empty.
        space : see base class
        """
        # NOTE: trailing spaces in above docstring must not be pruned
        # for correct parsing

        if (externals.versions['mdp'] >= (2, 5) \
                and node.has_multiple_training_phases()) \
            or not len(node._train_seq) == 1:
            raise ValueError("MDPNodeMapper does not support MDP nodes with "
                             "multiple training phases.")
        Mapper.__init__(self, **kwargs)
        self.__pristine_node = None
        self.node = node
        self.nodeargs = nodeargs


    def __repr__(self):
        s = super(MDPNodeMapper, self).__repr__()
        return s.replace("(", "(node=%s, nodeargs=%s, "
                              % (repr(self.node),
                                 repr(self.nodeargs)), 1)


    def _expand_args(self, phase, ds=None):
        args = []
        kwargs = {}
        if not self.nodeargs is None and phase in self.nodeargs:
            sargs, skwargs = self.nodeargs[phase]
            for a in sargs:
                if isinstance(a, DatasetAttributeExtractor):
                    if ds is None:
                        raise RuntimeError('MDPNodeMapper does not (yet) '
                                           'support argument extraction from dataset on '
                                           'forward()')
                    args.append(a(ds))
                else:
                    args.append(a)
            for k in skwargs:
                if isinstance(skwargs[k], DatasetAttributeExtractor):
                    if ds is None:
                        raise RuntimeError('MDPNodeMapper does not (yet) '
                                           'support argument extraction from dataset on '
                                           'forward()')
                    kwargs[k] = skwargs[k](ds)
                else:
                    kwargs[k] = skwargs[k]
        return args, kwargs


    def _train(self, ds):
        if not self.node.is_trainable():
            return

        # whenever we have no cannonical node source, we assign the current
        # node -- this can only happen prior training and allows modifying
        # the node of having the MDPNodeMapper instance
        if self.__pristine_node is None:
            self.__pristine_node = self.node

        # training is done on a copy of the pristine node, because nodes cannot
        # be reset, but PyMVPA's mapper need to be able to be retrained from
        # scratch
        self.node = self.__pristine_node.copy()
        # train
        args, kwargs = self._expand_args('train', ds)
        self.node.train(ds.samples, *args, **kwargs)
        # stop train
        args, kwargs = self._expand_args('stoptrain', ds)
        self.node.stop_training(*args, **kwargs)


    def _forward_data(self, data):
        args, kwargs = self._expand_args('exec', data)
        return self.node.execute(np.atleast_2d(data), *args, **kwargs).squeeze()


    def _reverse_data(self, data):
        args, kwargs = self._expand_args('inv', data)
        return self.node.inverse(np.atleast_2d(data), *args, **kwargs).squeeze()



class PCAMapper(MDPNodeMapper):
    """Convenience wrapper to perform PCA using MDP's Mapper
    """

    def __init__(self, alg='PCA', nodeargs=None, **kwargs):
        """
        Parameters
        ----------
        alg : {'PCA', 'NIPALS'}
          Which MDP implementation of a PCA to use.
        nodeargs : None or dict
          Arguments passed to the MDP node in various stages of its lifetime.
          See the :class:`MDPNodeMapper` for more details.
        **kwargs
          Additional constructor arguments for the MDP node.
        """
        if alg == 'PCA':
            node = mdp.nodes.PCANode(**kwargs)
        elif alg == 'NIPALS':
            node = mdp.nodes.NIPALSNode(**kwargs)
        else:
            raise ValueError("Unkown algorithm '%s' for PCAMapper."
                             % alg)
        MDPNodeMapper.__init__(self, node, nodeargs=nodeargs)


    proj = property(fget=lambda self: self.node.get_projmatrix(),
                    doc="Projection matrix (as an array)")
    recon = property(fget=lambda self: self.node.get_projmatrix(),
                     doc="Backprojection matrix (as an array)")
    var = property(fget=lambda self: self.node.d, doc="Variances per component")
    centroid = property(fget=lambda self: self.node.avg,
                        doc="Mean of the traiing data")


class ICAMapper(MDPNodeMapper):
    """Convenience wrapper to perform ICA using MDP nodes.
    """
    def __init__(self, alg='FastICA', nodeargs=None, **kwargs):
        """
        Parameters
        ----------
        alg : {'FastICA', 'CuBICA'}
          Which MDP implementation of an ICA to use.
        nodeargs : None or dict
          Arguments passed to the MDP node in various stages of its lifetime.
          See the baseclass for more details.
        **kwargs
          Additional constructor arguments for the MDP node.
        """
        if alg == 'FastICA':
            node = mdp.nodes.FastICANode(**kwargs)
        elif alg == 'CuBICA':
            node = mdp.nodes.CuBICANode(*kwargs)
        else:
            raise ValueError("Unkown algorithm '%s' for ICAMapper."
                             % alg)
        MDPNodeMapper.__init__(self, node, nodeargs=nodeargs)


    proj = property(fget=lambda self: self.node.get_projmatrix(),
                    doc="Projection matrix (as an array)")
    recon = property(fget=lambda self: self.node.get_projmatrix(),
                     doc="Backprojection matrix (as an array)")



class MDPFlowMapper(Mapper):
    """Mapper encapsulating an arbitray MDP flow.

    This mapper wraps an MDP flow and uses it for forward and reverse data
    mapping (reverse is only available if the underlying MDP flow supports
    it).  It is possible to specify arbitrary arguments for the training of
    the MDP flow.

    Because MDP does not allow to 'reset' a flow and (re)train it from
    scratch the mapper uses a copy of the initially wrapped flow for the
    actual processing. Upon subsequent training attempts a new copy of the
    original flow is made and replaces the previous one.

    Examples
    --------
    >>> import mdp
    >>> from mvpa2.mappers.mdp_adaptor import MDPFlowMapper
    >>> from mvpa2.base.dataset import DAE
    >>> flow = (mdp.nodes.PCANode() + mdp.nodes.IdentityNode() +
    ...         mdp.nodes.FDANode())
    >>> mapper = MDPFlowMapper(flow,
    ...                        node_arguments=(None, None,
    ...                        [DAE('sa', 'targets')]))

    Notes
    -----
    It is not possible to perform incremental training of the MDP flow. 
    """
    def __init__(self, flow, node_arguments=None, **kwargs):
        """
        Parameters
        ----------
        flow : mdp.Flow instance
          This flow instance is taken as the pristine source of which a
          copy is made for actual processing upon each training attempt.
        node_arguments : tuple, list
          A tuple or a list the same length as the flow. Each item is a
          list of arguments for the training of the corresponding node in
          the flow. If a node does not require additional arguments, None
          can be provided instead. Keyword arguments are currently not
          supported by mdp.Flow.
        """
        if not node_arguments is None and len(node_arguments) != len(flow):
            raise ValueError("Length of node_arguments (%i) does not match the "
                             "number of nodes in the flow (%i)."
                             % (len(node_arguments), len(flow)))
        Mapper.__init__(self, **kwargs)
        self.__pristine_flow = None
        self.flow = flow
        self.node_arguments = node_arguments


    def __repr__(self):
        s = super(MDPFlowMapper, self).__repr__()
        return s.replace("(", "(flow=%s, node_arguments=%s, "
                              % (repr(self.flow),
                                 repr(self.node_arguments)), 1)


    def _expand_nodeargs(self, ds, args):
        enal = []
        for a in args:
            if isinstance(a, DatasetAttributeExtractor):
                enal.append(a(ds))
            else:
                enal.append(a)
        return enal


    def _build_node_arguments(self, ds):
        if self.node_arguments is not None:
            node_arguments = []
            for ndi in self.node_arguments:
                l = [ds.samples]
                if ndi is not None:
                    l = [ds.samples]
                    l.extend(self._expand_nodeargs(ds, ndi))
                node_arguments.append([l])
        else:
            node_arguments = ds.samples
        return node_arguments


    def _train(self, ds):
        # whenever we have no cannonical flow source, we assign the current
        # flow -- this can only happen prior training and allow modifying
        # the flow of having the MDPNodeMapper instance
        if self.__pristine_flow is None:
            self.__pristine_flow = self.flow

        # training is done on a copy of the pristine flow, because flows cannot
        # be reset, but PyMVPA's mapper need to be able to be retrained from
        # scratch
        self.flow = self.__pristine_flow.copy()
        self.flow.train(self._build_node_arguments(ds))


    def _forward_data(self, data):
        return self.flow.execute(np.atleast_2d(data)).squeeze()


    def _reverse_data(self, data):
        return self.flow.inverse(np.atleast_2d(data)).squeeze()

########NEW FILE########
__FILENAME__ = procrustean
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Procrustean rotation mapper"""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.base import externals
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import EnsureChoice
from mvpa2.base.types import is_datasetlike
from mvpa2.mappers.projection import ProjectionMapper

from mvpa2.base import warning
if __debug__:
    from mvpa2.base import debug



class ProcrusteanMapper(ProjectionMapper):
    """Mapper to project from one space to another using Procrustean
    transformation (shift + scaling + rotation).

    Training this mapper requires data for both source and target space to be
    present in the training dataset. The source space data is taken from the
    training dataset's ``samples``, while the target space is taken from a
    sample attribute corresponding to the ``space`` setting of the
    ProcrusteanMapper.

    See: http://en.wikipedia.org/wiki/Procrustes_transformation
    """
    scaling = Parameter(True, constraints='bool',
                doc="""Estimate a global scaling factor for the transformation
                       (no longer rigid body)""")
    reflection = Parameter(True, constraints='bool',
                 doc="""Allow for the data to be reflected (so it might not be
                     a rotation. Effective only for non-oblique transformations.
                     """)
    reduction = Parameter(True, constraints='bool',
                 doc="""If true, it is allowed to map into lower-dimensional
                     space. Forward transformation might be suboptimal then and
                     reverse transformation might not recover all original
                     variance.""")
    oblique = Parameter(False, constraints='bool',
                 doc="""Either to allow non-orthogonal transformation -- might
                     heavily overfit the data if there is less samples than
                     dimensions. Use `oblique_rcond`.""")
    oblique_rcond = Parameter(-1, constraints='float',
                 doc="""Cutoff for 'small' singular values to regularize the
                     inverse. See :class:`~numpy.linalg.lstsq` for more
                     information.""")
    svd = Parameter('numpy', constraints=EnsureChoice('numpy', 'scipy', 'dgesvd'),
                 doc="""Implementation of SVD to use. dgesvd requires ctypes to
                 be available.""")
    def __init__(self, space='targets', **kwargs):
        ProjectionMapper.__init__(self, space=space, **kwargs)

        self._scale = None
        """Estimated scale"""
        if self.params.svd == 'dgesvd' and not externals.exists('liblapack.so'):
            warning("Reverting choice of svd for ProcrusteanMapper to be default "
                    "'numpy' since liblapack.so seems not to be available for "
                    "'dgesvd'")
            self.params.svd = 'numpy'


    def _train(self, source):
        params = self.params
        # Since it is unsupervised, we don't care about labels
        datas = ()
        odatas = ()
        means = ()
        shapes = ()

        assess_residuals = __debug__ and 'MAP_' in debug.active

        target = source.sa[self.get_space()].value

        for i, ds in enumerate((source, target)):
            if is_datasetlike(ds):
                data = np.asarray(ds.samples)
            else:
                data = ds
            if assess_residuals:
                odatas += (data,)
            if i == 0:
                mean = self._offset_in
            else:
                mean = data.mean(axis=0)
            data = data - mean
            means += (mean,)
            datas += (data,)
            shapes += (data.shape,)

        # shortcuts for sizes
        sn, sm = shapes[0]
        tn, tm = shapes[1]

        # Check the sizes
        if sn != tn:
            raise ValueError, "Data for both spaces should have the same " \
                  "number of samples. Got %d in source and %d in target space" \
                  % (sn, tn)

        # Sums of squares
        ssqs = [np.sum(d**2, axis=0) for d in datas]

        # XXX check for being invariant?
        #     needs to be tuned up properly and not raise but handle
        for i in xrange(2):
            if np.all(ssqs[i] <= np.abs((np.finfo(datas[i].dtype).eps
                                       * sn * means[i] )**2)):
                raise ValueError, "For now do not handle invariant in time datasets"

        norms = [ np.sqrt(np.sum(ssq)) for ssq in ssqs ]
        normed = [ data/norm for (data, norm) in zip(datas, norms) ]

        # add new blank dimensions to source space if needed
        if sm < tm:
            normed[0] = np.hstack( (normed[0], np.zeros((sn, tm-sm))) )

        if sm > tm:
            if params.reduction:
                normed[1] = np.hstack( (normed[1], np.zeros((sn, sm-tm))) )
            else:
                raise ValueError, "reduction=False, so mapping from " \
                      "higher dimensionality " \
                      "source space is not supported. Source space had %d " \
                      "while target %d dimensions (features)" % (sm, tm)

        source, target = normed
        if params.oblique:
            # Just do silly linear system of equations ;) or naive
            # inverse problem
            if sn == sm and tm == 1:
                T = np.linalg.solve(source, target)
            else:
                T = np.linalg.lstsq(source, target, rcond=params.oblique_rcond)[0]
            ss = 1.0
        else:
            # Orthogonal transformation
            # figure out optimal rotation
            if params.svd == 'numpy':
                U, s, Vh = np.linalg.svd(np.dot(target.T, source),
                               full_matrices=False)
            elif params.svd == 'scipy':
                # would raise exception if not present
                externals.exists('scipy', raise_=True)
                import scipy
                U, s, Vh = scipy.linalg.svd(np.dot(target.T, source),
                               full_matrices=False)
            elif params.svd == 'dgesvd':
                from mvpa2.support.lapack_svd import svd as dgesvd
                U, s, Vh = dgesvd(np.dot(target.T, source),
                                    full_matrices=True, algo='svd')
            else:
                raise ValueError('Unknown type of svd %r'%(params.svd))
            T = np.dot(Vh.T, U.T)

            if not params.reflection:
                # then we need to assure that it is only rotation
                # "recipe" from
                # http://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem
                # for more and info and original references, see
                # http://dx.doi.org/10.1007%2FBF02289451
                nsv = len(s)
                s[:-1] = 1
                s[-1] = np.linalg.det(T)
                T = np.dot(U[:, :nsv] * s, Vh)

            # figure out scale and final translation
            # XXX with reflection False -- not sure if here or there or anywhere...
            ss = sum(s)

        # if we were to collect standardized distance
        # std_d = 1 - sD**2

        # select out only relevant dimensions
        if sm != tm:
            T = T[:sm, :tm]

        self._scale = scale = ss * norms[1] / norms[0]
        # Assign projection
        if self.params.scaling:
            proj = scale * T
        else:
            proj = T
        self._proj = proj

        if self._demean:
            self._offset_out = means[1]

        if __debug__ and 'MAP_' in debug.active:
            # compute the residuals
            res_f = self.forward(odatas[0])
            d_f = np.linalg.norm(odatas[1] - res_f)/np.linalg.norm(odatas[1])
            res_r = self.reverse(odatas[1])
            d_r = np.linalg.norm(odatas[0] - res_r)/np.linalg.norm(odatas[0])
            debug('MAP_', "%s, residuals are forward: %g,"
                  " reverse: %g" % (repr(self), d_f, d_r))

########NEW FILE########
__FILENAME__ = projection
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Base class for mappers doing linear transformations"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.dochelpers import enhanced_doc_string
from mvpa2.mappers.base import Mapper, accepts_dataset_as_samples


if __debug__:
    from mvpa2.base import debug


class ProjectionMapper(Mapper):
    """Linear mapping between multidimensional spaces.

    This class cannot be used directly. Sub-classes have to implement
    the `_train()` method, which has to compute the projection matrix
    `_proj` and optionally offset vectors `_offset_in` and
    `_offset_out` (if initialized with demean=True, which is default)
    given a dataset (see `_train()` docstring for more information).

    Once the projection matrix is available, this class provides
    functionality to perform forward and backwards linear mapping of
    data, the latter by default using pseudo-inverse (but could be
    altered in subclasses, like hermitian (conjugate) transpose in
    case of SVD).  Additionally, `ProjectionMapper` supports optional
    selection of arbitrary component (i.e. columns of the projection
    matrix) of the projection.

    Forward and back-projection matrices (a.k.a. *projection* and
    *reconstruction*) are available via the `proj` and `recon`
    properties.
    """

    _DEV__doc__ = """Think about renaming `demean`, may be `translation`?"""

    def __init__(self, demean=True, **kwargs):
        """Initialize the ProjectionMapper

        Parameters
        ----------
        demean : bool
          Either data should be demeaned while computing
          projections and applied back while doing reverse()
        """
        Mapper.__init__(self, **kwargs)

        # by default we want to wipe the feature attributes out during mapping
        self._fa_filter = []

        self._proj = None
        """Forward projection matrix."""
        self._recon = None
        """Reverse projection (reconstruction) matrix."""
        self._demean = demean
        """Flag whether to demean the to be projected data, prior to projection.
        """
        self._offset_in = None
        """Offset (most often just mean) in the input space"""
        self._offset_out = None
        """Offset (most often just mean) in the output space"""

    __doc__ = enhanced_doc_string('ProjectionMapper', locals(), Mapper)


    @accepts_dataset_as_samples
    def _pretrain(self, samples):
        """Determine the projection matrix.

        Parameters
        ----------
        dataset : Dataset
          Dataset to operate on
        """
        if self._demean:
            self._offset_in = samples.mean(axis=0)


    ##REF: Name was automagically refactored
    def _demean_data(self, data):
        """Helper which optionally demeans
        """
        if self._demean:
            # demean the training data
            data = data - self._offset_in

            if __debug__ and "MAP_" in debug.active:
                debug("MAP_",
                      "%s: Mean of data in input space %s was subtracted" %
                      (self.__class__.__name__, self._offset_in))
        return data


    def _forward_data(self, data):
        if self._proj is None:
            raise RuntimeError, "Mapper needs to be train before used."

        # local binding
        demean = self._demean

        d = np.asmatrix(data)

        # Remove input offset if present
        if demean and self._offset_in is not None:
            d = d - self._offset_in

        # Do forward projection
        res = (d * self._proj).A

        # Add output offset if present
        if demean and self._offset_out is not None:
            res += self._offset_out

        return res


    def _reverse_data(self, data):
        if self._proj is None:
            raise RuntimeError, "Mapper needs to be trained before used."
        d = np.asmatrix(data)
        # Remove offset if present in output space
        if self._demean and self._offset_out is not None:
            d = d - self._offset_out

        # Do reverse projection
        res = (d * self.recon).A

        # Add offset in input space
        if self._demean and self._offset_in is not None:
            res += self._offset_in

        return res


    ##REF: Name was automagically refactored
    def _compute_recon(self):
        """Given that a projection is present -- compute reconstruction matrix.
        By default -- pseudoinverse of projection matrix.  Might be overridden
        in derived classes for efficiency.
        """
        return np.linalg.pinv(self._proj)


    ##REF: Name was automagically refactored
    def _get_recon(self):
        """Compute (if necessary) and return reconstruction matrix
        """
        # (re)build reconstruction matrix
        recon = self._recon
        if recon is None:
            self._recon = recon = self._compute_recon()
        return recon


    proj  = property(fget=lambda self: self._proj, doc="Projection matrix")
    recon = property(fget=_get_recon, doc="Backprojection matrix")

########NEW FILE########
__FILENAME__ = prototype
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Project data onto a space defined by prototypes via a similarity function"""

import numpy as np

from mvpa2.mappers.base import accepts_dataset_as_samples
from mvpa2.mappers.projection import ProjectionMapper

if __debug__:
    from mvpa2.base import debug


class PrototypeMapper(ProjectionMapper):
    """Mapper to project data onto a space defined by prototypes from
    the same space via a similarity function.
    """
    def __init__(self,
                 similarities,
                 prototypes=None,
                 **kwargs):
        """
        Parameters
        ----------
        similarities : list
          A list of similarity functions.
        prototypes : Dataset or list
          A dataset or a list of instances (e.g., streamlines)?
        **kwargs:
          All keyword arguments are passed to the ProjectionMapper
          constructor
        """
        ProjectionMapper.__init__(self, **kwargs)

        self.similarities = similarities
        self.prototypes = prototypes


    @accepts_dataset_as_samples
    def _train(self, samples):
        """Train PrototypeMapper
        """

        self._proj = np.hstack([similarity.computed(samples, self.prototypes)
                               for similarity in self.similarities])
        if __debug__:
            debug("MAP", "projected data of shape %s: %s "
                  % (self._proj.shape, self._proj))

########NEW FILE########
__FILENAME__ = shape
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Basic dataset shape modifications."""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.mappers.base import Mapper
from mvpa2.datasets import Dataset

class TransposeMapper(Mapper):
    """Swap sample and feature axes.

    This mapper swaps the sample axis (first axis) and feature axis (second
    axis) of a dataset (additional axes in multi-dimensional datasets are left
    untouched). Both, sample and feature attribute collections are also
    swapped accordingly. Neither dataset samples, not attribute collections
    are copied. Reverse mapping is supported as well. This mapper does not
    require training and a single instance can be used on different datasets
    without problems.
    """
    is_trained = True

    def __init__(self,  **kwargs):
        Mapper.__init__(self, **kwargs)

    def _swap_samples_and_feature_axes(self, ds):
        out = Dataset(np.swapaxes(ds.samples, 0, 1),
                      sa=ds.fa,
                      fa=ds.sa,
                      a=ds.a)
        return out

    def _forward_dataset(self, ds):
        return self._swap_samples_and_feature_axes(ds)

    def _reverse_dataset(self, ds):
        return self._swap_samples_and_feature_axes(ds)


class AddAxisMapper(Mapper):
    """Add an axis to a dataset at an arbitrary position.

    This mapper can be useful when there is need for aggregating multiple
    datasets, where it is often necessary or at least useful to have a
    dedicated aggregation axis.  An axis can be added at any position

    When adding an axis that causes the current sample (1st) or feature axis
    (2nd) to shift the corresponding attribute collections are modified to
    accomodate the change. This typically means also adding an axis at the
    corresponding position of the attribute arrays. A special case is, however,
    prepending an axis to the dataset, i.e. shifting both sample and feature
    axis towards the back. In this case all feature attibutes are duplicated
    to match the new number of features (formaly the number of samples).

    Examples
    --------
    >>> from mvpa2.datasets.base import Dataset
    >>> from mvpa2.mappers.shape import AddAxisMapper
    >>> ds = Dataset(np.arange(24).reshape(2,3,4))
    >>> am = AddAxisMapper(pos=1)
    >>> print am(ds).shape
    (2, 1, 3, 4)
    """
    is_trained = True

    def __init__(self, pos, **kwargs):
        """
        Parameters
        ----------
        pos : int
            Axis index to which the new axis is prepended. Negative indices are
            supported as well, but the new axis will be placed behind the given
            index. For example, a position of ``-1`` will cause an axis to be
            added behind the last axis. If ``pos`` is larger than the number of
            existing axes additional new axes will be created match the value of
            ``pos``.
        """
        Mapper.__init__(self, **kwargs)
        self._pos = pos

    def _forward_dataset(self, ds):
        pos = self._pos
        if pos < 0:
            # support negative/reverse indices
            pos = len(ds.shape) + 1 + pos
        # select all prior axes, but at most all existing axes
        slicer = [slice(None)] * min(pos, len(ds.shape))
        # and as many new axes as necessary afterwards
        slicer += [None] * max(1, pos + 1 - len(ds.shape))
        # there are two special cases that require modification of feature
        # attributes
        if pos == 0:
            # prepend an axis to all sample attributes
            out_sa = dict([(attr, ds.sa[attr].value[None]) for attr in ds.sa])
            # prepend an axis to all FAs and repeat for each previous sample
            out_fa = dict([(attr,
                            np.repeat(ds.fa[attr].value[None], len(ds), axis=0))
                for attr in ds.fa])
        elif pos == 1:
            # prepend an axis to all feature attributes
            out_fa = dict([(attr, ds.fa[attr].value[None]) for attr in ds.fa])
            out_sa = ds.sa
        else:
            out_sa = ds.sa
            out_fa = ds.fa
        out = Dataset(ds.samples.__getitem__(tuple(slicer)),
                      sa=out_sa, fa=out_fa, a=ds.a)
        return out

########NEW FILE########
__FILENAME__ = skl_adaptor
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Use scikit-learn transformer as mappers.

This module provides an adaptor to use sklearn transformers as PyMVPA mappers.
"""

__docformat__ = 'restructuredtext'

from mvpa2.support.copy import deepcopy

from mvpa2.base.learner import Learner
from mvpa2.mappers.base import Mapper

class SKLTransformer(Mapper):
    """Adaptor to use arbitrary sklearn transformer as a mapper.

    This basic adaptor support forward mapping only. It is clever enough
    to call ``fit_transform()`` instead of a serial ``fit()`` and
    ``transform()`` combo when an untrained instance is called with a dataset.

    >>> from sklearn.manifold import MDS
    >>> from mvpa2.misc.data_generators import normal_feature_dataset
    >>> ds = normal_feature_dataset(perlabel=10, nlabels=2)
    >>> print ds.shape
    (20, 4)
    >>> mds = SKLTransformer(MDS())
    >>> mapped = mds(ds)
    >>> print mapped.shape
    (20, 2)
    """
    def __init__(self, transformer, **kwargs):
        """
        Parameters
        ----------
        transformer : sklearn.transformer instance
        space : str or None, optional
          If not None, a sample attribute of the given name will be extracted
          from the training dataset and passed to the sklearn transformer's
          ``fit()`` method as ``y`` argument.

        """
        # NOTE: trailing spaces in above docstring must not be pruned
        # for correct parsing

        Mapper.__init__(self, auto_train=False, **kwargs)
        self._transformer = None
        self._pristine_transformer = transformer

    def __call__(self, ds):
        # overwrite __call__ to prevent the rigorous check of the learner was
        # trained before use and auto-train, because sklearn has optimized ways
        # for doing that, i.e. fit_transform()
        return super(Learner, self).__call__(ds)

    def _untrain(self):
        self._transformer = None

    def _get_y(self, ds):
        space = self.get_space()
        if space:
            y = ds.sa[space].value
        else:
            y = None
        return y

    def _get_transformer(self):
        if self._transformer is None:
            self._transformer = deepcopy(self._pristine_transformer)
        return self._transformer

    def _train(self, ds):
        tf = self._get_transformer()
        return tf.fit(ds.samples, self._get_y(ds))

    def _forward_dataset(self, ds):
        tf = self._get_transformer()
        if not self.is_trained:
            # sklearn support fit and transform at the same time, which might
            # be a lot faster, but we only do that, if the mapper is not
            # trained already
            out = tf.fit_transform(ds.samples, self._get_y(ds))
            self._set_trained()
        else:
            out = tf.transform(ds.samples, self._get_y(ds))
        return out

########NEW FILE########
__FILENAME__ = slicing
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Collection of dataset slicing procedures."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.node import Node
from mvpa2.mappers.base import Mapper, accepts_dataset_as_samples
from mvpa2.base.dochelpers import _str, _repr_attrs
from mvpa2.generators.splitters import mask2slice


class SliceMapper(Mapper):
    """Baseclass of Mapper that slice a Dataset in various ways.
    """
    def __init__(self, slicearg, **kwargs):
        """
        Parameters
        ----------
        slicearg
          Argument for slicing
        """
        Mapper.__init__(self, **kwargs)
        self._safe_assign_slicearg(slicearg)


    def _safe_assign_slicearg(self, slicearg):
        # convert int sliceargs into lists to prevent getting scalar values when
        # slicing
        if isinstance(slicearg, int):
            slicearg = [slicearg]
        self._slicearg = slicearg
        # if we got some sort of slicearg we assume that we are ready to go
        if not slicearg is None:
            self._set_trained()

    def __repr__(self, prefixes=[]):
        return super(SliceMapper, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['slicearg']))


    def __str__(self):
        # with slicearg it can quickly get very unreadable
        #return _str(self, str(self._slicearg))
        return _str(self)


    def _untrain(self):
        self._safe_assign_slicearg(None)
        super(SliceMapper, self)._untrain()


    def __iadd__(self, other):
        # our slicearg
        this = self._slicearg
        # if another slice mapper work on its slicearg
        if isinstance(other, SliceMapper):
            other = other._slicearg
        # catch stupid arg
        if not (isinstance(other, tuple) or isinstance(other, list) \
                or isinstance(other, np.ndarray) or isinstance(other, slice)):
            return NotImplemented
        if isinstance(this, slice):
            # we can always merge if the slicing arg can be sliced itself (i.e.
            # it is not a slice-object... unless it doesn't really slice we do
            # not want to expand slices into index lists to become mergable,
            # since that would cause cheap view-based slicing to become
            # expensive copy-based slicing
            if this == slice(None):
                # this one did nothing, just use the other and be done
                self._safe_assign_slicearg(other)
                return self
            else:
                # see comment above
                return NotImplemented
        # list or tuple are alike
        if isinstance(this, list) or isinstance(this, tuple):
            # simply convert it into an array and proceed from there
            this = np.asanyarray(this)
        if this.dtype.type is np.bool_:
            # simply convert it into an index array --prevents us from copying a
            # lot and allows for sliceargs such as [3,3,4,4,5,5]
            this = this.nonzero()[0]
        if this.dtype.char in np.typecodes['AllInteger']:
            self._safe_assign_slicearg(this[other])
            return self

        # if we get here we got something the isn't supported
        return NotImplemented

    slicearg = property(fget=lambda self:self._slicearg)


class SampleSliceMapper(SliceMapper):
    """Mapper to select a subset of samples."""
    def __init__(self, slicearg, **kwargs):
        """
        Parameters
        ----------
        slicearg : int, list(int), array(int), array(bool)
          Any slicing argument that is compatible with numpy arrays. Depending
          on the argument the mapper will perform basic slicing or
          advanced indexing (with all consequences on speed and memory
          consumption).
        """
        SliceMapper.__init__(self, slicearg, **kwargs)


    def _call(self, ds):
        # it couldn't be simpler
        return ds[self._slicearg]



class StripBoundariesSamples(Node):
    """Strip samples on boundaries defines by sample attribute values.

    A sample attribute of a dataset is scanned for consecutive blocks of
    identical values. Every change in the value is treated as a boundary
    and custom number of samples is removed prior and after this boundary.
    """
    def __init__(self, space, prestrip, poststrip, **kwargs):
        """
        Parameters
        ----------
        space : str
          name of the sample attribute that shall be used to determine the
          boundaries.
        prestrip : int
          Number of samples to be stripped prior to each boundary.
        poststrip : int
          Number of samples to be stripped after each boundary (this includes
          the boundary sample itself, i.e. the first samples with a different
          sample attribute value).
        """
        Node.__init__(self, space=space, **kwargs)
        self._prestrip = prestrip
        self._poststrip = poststrip


    def _call(self, ds):
        # attribute to detect boundaries
        battr = ds.sa[self.get_space()].value
        # filter which samples to keep
        filter_ = np.ones(battr.shape, dtype='bool')
        # determine boundary indices -- shift by one to have the new value
        # as the boundary
        bindices = (battr[:-1] != battr[1:]).nonzero()[0] + 1

        # for all boundaries
        for b in bindices:
            lower = b - self._prestrip
            upper = b + self._poststrip
            filter_[lower:upper] = False

        filter_ = mask2slice(filter_)

        return ds[filter_]

########NEW FILE########
__FILENAME__ = som
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Self-organizing map (SOM)."""

__docformat__ = 'restructuredtext'


import numpy as np
from mvpa2.mappers.base import Mapper, accepts_dataset_as_samples

if __debug__:
    from mvpa2.base import debug

class SimpleSOMMapper(Mapper):
    """Mapper using a self-organizing map (SOM) for dimensionality reduction.

    This mapper provides a simple, but pretty fast implementation of a
    self-organizing map using an unsupervised training algorithm. It performs a
    ND -> 2D mapping, which can for, example, be used for visualization of
    high-dimensional data.

    This SOM implementation uses squared Euclidean distance to determine
    the best matching Kohonen unit and a Gaussian neighborhood influence
    kernel.
    """
    def __init__(self, kshape, niter, learning_rate=0.005,
                 iradius=None, distance_metric=None, initialization_func=None):
        """
        Parameters
        ----------
        kshape : (int, int)
            Shape of the internal Kohonen layer. Currently, only 2D Kohonen
            layers are supported, although the length of an axis might be set
            to 1.
        niter : int
            Number of iteration during network training.
        learning_rate : float
            Initial learning rate, which will continuously decreased during
            network training.
        iradius : float or None
            Initial radius of the Gaussian neighborhood kernel radius, which
            will continuously decreased during network training. If `None`
            (default) the radius is set equal to the longest edge of the
            Kohonen layer.
        distance_metric: callable or None
            Kernel distance metric between elements in Kohonen layer. If None
            then Euclidean distance is used. Otherwise it should be a 
            callable that accepts two input arguments x and y and returns
            the distance d through d=distance_metric(x,y)
        initialization_func: callable or None
            Initialization function to set self._K, that should take one 
            argument with training samples and return an numpy array. If None,
            then values in the returned array are taken from a standard normal 
            distribution.  
        """
        # init base class
        Mapper.__init__(self)

        self.kshape = np.array(kshape, dtype='int')

        if iradius is None:
            self.radius = self.kshape.max()
        else:
            self.radius = iradius

        if distance_metric is None:
            self.distance_metric = lambda x, y: (x ** 2 + y ** 2) ** 0.5
        else:
            self.distance_metric = distance_metric

        # learning rate
        self.lrate = learning_rate

        # number of training iterations
        self.niter = niter

        # precompute whatever can be done
        # scalar for decay of learning rate and radius across all iterations
        self.iter_scale = self.niter / np.log(self.radius)

        # the internal kohonen layer
        self._K = None
        self._dqd = None
        self._initialization_func = initialization_func

    @accepts_dataset_as_samples
    def _pretrain(self, samples):
        """Perform network pre-training.

        Parameters
        ----------
        samples : array-like
            Used for unsupervised training of the SOM
        """
        ifunc = self._initialization_func
        # XXX initialize with clever default, e.g. plain of first two PCA
        # components
        if ifunc is None:
             ifunc = lambda x:np.random.standard_normal(tuple(self.kshape) \
                                                             + (x.shape[1],))

        self._K = ifunc(samples)

         # precompute distance kernel between elements in the Kohonen layer
        # that will remain constant throughout the training
        # (just compute one quadrant, as the distances are symmetric)
        # XXX maybe do other than squared Euclidean?
        self._dqd = np.fromfunction(self.distance_metric,
                             self.kshape, dtype='float')


    @accepts_dataset_as_samples
    def _train(self, samples):
        """Perform network training.

        Parameters
        ----------
        samples : array-like
            Used for unsupervised training of the SOM.
          
        Notes
        -----
        It is assumed that prior to calling this method the _pretrain method 
        was called with the same argument.  
        """

        # ensure that dqd was set properly
        dqd = self._dqd
        if dqd is None:
            raise ValueError("This should not happen - was _pretrain called?")

        # units weight vector deltas for batch training
        # (height x width x #features)
        unit_deltas = np.zeros(self._K.shape, dtype='float')

        # for all iterations
        for it in xrange(1, self.niter + 1):
            # compute the neighborhood impact kernel for this iteration
            # has to be recomputed since kernel shrinks over time
            k = self._compute_influence_kernel(it, dqd)

            # for all training vectors
            for s in samples:
                # determine closest unit (as element coordinate)
                b = self._get_bmu(s)
                # train all units at once by unfolding the kernel (from the
                # single quadrant that is precomputed), cutting it to the
                # right shape and simply multiply it to the difference of target
                # and all unit weights....
                infl = np.vstack((
                        np.hstack((
                            # upper left
                            k[b[0]:0:-1, b[1]:0:-1],
                            # upper right
                            k[b[0]:0:-1, :self.kshape[1] - b[1]])),
                        np.hstack((
                            # lower left
                            k[:self.kshape[0] - b[0], b[1]:0:-1],
                            # lower right
                            k[:self.kshape[0] - b[0], :self.kshape[1] - b[1]]))
                               ))
                unit_deltas += infl[:, :, np.newaxis] * (s - self._K)

            # apply cumulative unit deltas
            self._K += unit_deltas

            if __debug__:
                debug("SOM", "Iteration %d/%d done: ||unit_deltas||=%g" %
                      (it, self.niter, np.sqrt(np.sum(unit_deltas ** 2))))

            # reset unit deltas
            unit_deltas.fill(0.)


    ##REF: Name was automagically refactored
    def _compute_influence_kernel(self, iter, dqd):
        """Compute the neighborhood kernel for some iteration.

        Parameters
        ----------
        iter : int
          The iteration for which to compute the kernel.
        dqd : array (nrows x ncolumns)
          This is one quadrant of Euclidean distances between Kohonen unit
          locations.
        """
        # compute radius decay for this iteration
        curr_max_radius = self.radius * np.exp(-1.0 * iter / self.iter_scale)

        # same for learning rate
        curr_lrate = self.lrate * np.exp(-1.0 * iter / self.iter_scale)

        # compute Gaussian influence kernel
        infl = np.exp((-1.0 * dqd) / (2 * curr_max_radius * iter))
        infl *= curr_lrate

        # hard-limit kernel to max radius
        # XXX is this really necessary?
        infl[dqd > curr_max_radius] = 0.

        return infl


    ##REF: Name was automagically refactored
    def _get_bmu(self, sample):
        """Returns the ID of the best matching unit.

        'best' is determined as minimal squared Euclidean distance between
        any units weight vector and some given target `sample`

        Parameters
        ----------
        sample : array
          Target sample.

        Returns
        -------
        tuple: (row, column)
        """
        # TODO expose distance function as parameter
        loc = np.argmin(((self.K - sample) ** 2).sum(axis=2))
        # assumes 2D Kohonen layer
        return (np.divide(loc, self.kshape[1]).astype('int'), loc % self.kshape[1])


    def _forward_data(self, data):
        """Map data from the IN dataspace into OUT space.

        Mapping is performs by simple determining the best matching Kohonen
        unit for each data sample.
        """
        return np.array([self._get_bmu(d) for d in data])


    def _reverse_data(self, data):
        """Reverse map data from OUT space into the IN space.
        """
        # simple transform into appropriate array slicing and
        # return the associated Kohonen unit weights
        return self.K[tuple(np.transpose(data))]


    def __repr__(self):
        s = Mapper.__repr__(self).rstrip(' )')
        # beautify
        if not s[-1] == '(':
            s += ' '
        s += 'kshape=%s, niter=%i, learning_rate=%f, iradius=%f)' \
                % (str(tuple(self.kshape)), self.niter, self.lrate,
                   self.radius)
        return s


    ##REF: Name was automagically refactored
    def _access_kohonen(self):
        """Provide access to the Kohonen layer.

        With some care.
        """
        if self._K is None:
            raise RuntimeError, \
                  'The SOM needs to be trained before access to the Kohonen ' \
                  'layer is possible.'

        return self._K


    K = property(fget=_access_kohonen)

########NEW FILE########
__FILENAME__ = staticprojection
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Transform data via static projection matrices"""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.base.dochelpers import borrowdoc
from mvpa2.mappers.projection import ProjectionMapper

if __debug__:
    from mvpa2.base import debug


class StaticProjectionMapper(ProjectionMapper):
    """Mapper to project data onto arbitrary space using transformation given as input.
       Both forward and reverse projections can be provided.
    """

    def __init__(self, proj, recon=None, **kwargs):
        """Initialize the StaticProjectionMapper

        Parameters
        ----------
        proj : 2-D array
          Projection matrix to be used for forward projection.
        recon: 2-D array
          Projection matrix to be used for reverse projection.
          If this is not given, `numpy.linalg.pinv` of proj
          will be used by default.
        **kwargs:
          All keyword arguments are passed to the ProjectionMapper
          constructor.
        """
        ProjectionMapper.__init__(self, auto_train=True, **kwargs)
        self._proj = proj
        self._recon = recon

    def _train(self, dummyds):
        """Do Nothing
        """
        if __debug__:
            debug("MAP_", "Mixing matrix has %s shape and norm=%f" %
                  (self._proj.shape, np.linalg.norm(self._proj)))




########NEW FILE########
__FILENAME__ = svd
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Singular-value decomposition"""

__docformat__ = 'restructuredtext'

import numpy as np
#import scipy.linalg as spl

from mvpa2.base.dochelpers import borrowdoc
from mvpa2.mappers.base import accepts_dataset_as_samples
from mvpa2.mappers.projection import ProjectionMapper
from mvpa2.featsel.helpers import ElementSelector

if __debug__:
    from mvpa2.base import debug


class SVDMapper(ProjectionMapper):
    """Mapper to project data onto SVD components estimated from some dataset.
    """

    @borrowdoc(ProjectionMapper)
    def __init__(self, **kwargs):
        """Initialize the SVDMapper

        Parameters
        ----------
        **kwargs:
          All keyword arguments are passed to the ProjectionMapper
          constructor.

        """
        ProjectionMapper.__init__(self, **kwargs)

        self._sv = None
        """Singular values of the training matrix."""


    @accepts_dataset_as_samples
    def _train(self, samples):
        """Determine the projection matrix onto the SVD components from
        a 2D samples x feature data matrix.
        """
        X = np.asmatrix(samples)
        X = self._demean_data(X)

        # singular value decomposition
        U, SV, Vh = np.linalg.svd(X, full_matrices=0)
        #U, SV, Vh = spl.svd(X, full_matrices=0)

        # store the final matrix with the new basis vectors to project the
        # features onto the SVD components. And store its .H right away to
        # avoid computing it in forward()
        self._proj = Vh.H

        # also store singular values of all components
        self._sv = SV

        if __debug__:
            debug("MAP", "SVD was done on %s and obtained %d SVs " %
                  (samples, len(SV)) + " (%d non-0, max=%f)" %
                  (len(SV.nonzero()), SV[0]))
            # .norm might be somewhat expensive to compute
            if "MAP_" in debug.active:
                debug("MAP_", "Mixing matrix has %s shape and norm=%f" %
                      (self._proj.shape, np.linalg.norm(self._proj)))


    ##REF: Name was automagically refactored
    def _compute_recon(self):
        """Since singular vectors are orthonormal, sufficient to take hermitian
        """
        return self._proj.H


    sv = property(fget=lambda self: self._sv, doc="Singular values")

########NEW FILE########
__FILENAME__ = wavelet
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wavelet transformation"""

from mvpa2.base import externals

if externals.exists('pywt', raise_=True):
    # import conditional to be able to import the whole module while building
    # the docs even if pywt is not installed
    import pywt

import numpy as np

from mvpa2.base import warning
from mvpa2.mappers.base import Mapper

if __debug__:
    from mvpa2.base import debug

# WaveletPacket and WaveletTransformation mappers share lots of common
# functionality at the moment

class _WaveletMapper(Mapper):
    """Generic class for Wavelet mappers (decomposition and packet)
    """

    def __init__(self, dim=1, wavelet='sym4', mode='per', maxlevel=None):
        """Initialize _WaveletMapper mapper

        Parameters
        ----------
        dim : int or tuple of int
          dimensions to work across (for now just scalar value, ie 1D
          transformation) is supported
        wavelet : str
          one from the families available withing pywt package
        mode : str
          periodization mode
        maxlevel : int or None
          number of levels to use. If None - automatically selected by pywt
        """
        Mapper.__init__(self)

        self._dim = dim
        """Dimension to work along"""

        self._maxlevel = maxlevel
        """Maximal level of decomposition. None for automatic"""

        if not wavelet in pywt.wavelist():
            raise ValueError, \
                  "Unknown family of wavelets '%s'. Please use one " \
                  "available from the list %s" % (wavelet, pywt.wavelist())
        self._wavelet = wavelet
        """Wavelet family to use"""

        if not mode in pywt.MODES.modes:
            raise ValueError, \
                  "Unknown periodization mode '%s'. Please use one " \
                  "available from the list %s" % (mode, pywt.MODES.modes)
        self._mode = mode
        """Periodization mode"""


    def _forward_data(self, data):
        data = np.asanyarray(data)
        self._inshape = data.shape
        self._intimepoints = data.shape[self._dim]
        res = self._wm_forward(data)
        self._outshape = res.shape
        return res


    def _reverse_data(self, data):
        data = np.asanyarray(data)
        return self._wm_reverse(data)


    def _wm_forward(self, *args):
        raise NotImplementedError


    def _wm_reverse(self, *args):
        raise NotImplementedError



##REF: Name was automagically refactored
def _get_indexes(shape, dim):
    """Generator for coordinate tuples providing slice for all in `dim`

    XXX Somewhat sloppy implementation... but works...
    """
    if len(shape) < dim:
        raise ValueError, "Dimension %d is incorrect for a shape %s" % \
              (dim, shape)
    n = len(shape)
    curindexes = [0] * n
    curindexes[dim] = Ellipsis#slice(None)       # all elements for dimension dim
    while True:
        yield tuple(curindexes)
        for i in xrange(n):
            if i == dim and dim == n-1:
                return                  # we reached it -- thus time to go
            if curindexes[i] == shape[i] - 1:
                if i == n-1:
                    return
                curindexes[i] = 0
            else:
                if i != dim:
                    curindexes[i] += 1
                    break


class WaveletPacketMapper(_WaveletMapper):
    """Convert signal into an overcomplete representaion using Wavelet packet
    """

    def __init__(self, level=None, **kwargs):
        """Initialize WaveletPacketMapper mapper

        Parameters
        ----------
        level : int or None
          What level to decompose at. If 'None' data for all levels
          is provided, but due to different sizes, they are placed
          in 1D row.
        """

        _WaveletMapper.__init__(self,**kwargs)

        self.__level = level


    # XXX too much of duplications between such methods -- it begs
    #     refactoring
    ##REF: Name was automagically refactored
    def __forward_single_level(self, data):
        if __debug__:
            debug('MAP', "Converting signal using DWP (single level)")

        wp = None

        level = self.__level
        wavelet = self._wavelet
        mode = self._mode
        dim = self._dim

        level_paths = None
        for indexes in _get_indexes(data.shape, self._dim):
            if __debug__:
                debug('MAP_', " %s" % (indexes,), lf=False, cr=True)
            WP = pywt.WaveletPacket(
                data[indexes], wavelet=wavelet,
                mode=mode, maxlevel=level)

            level_nodes = WP.get_level(level)
            if level_paths is None:
                # Needed for reconstruction
                self.__level_paths = np.array([node.path for node in level_nodes])
            level_datas = np.array([node.data for node in level_nodes])

            if wp is None:
                newdim = data.shape
                newdim = newdim[:dim] + level_datas.shape + newdim[dim+1:]
                if __debug__:
                    debug('MAP_', "Initializing storage of size %s for single "
                          "level (%d) mapping of data of size %s" % (newdim, level, data.shape))
                wp = np.empty( tuple(newdim) )

            wp[indexes] = level_datas

        return wp


    ##REF: Name was automagically refactored
    def __forward_multiple_levels(self, data):
        wp = None
        levels_length = None                # total length at each level
        levels_lengths = None                # list of lengths per each level
        for indexes in _get_indexes(data.shape, self._dim):
            if __debug__:
                debug('MAP_', " %s" % (indexes,), lf=False, cr=True)
            WP = pywt.WaveletPacket(
                data[indexes],
                wavelet=self._wavelet,
                mode=self._mode, maxlevel=self._maxlevel)

            if levels_length is None:
                levels_length = [None] * WP.maxlevel
                levels_lengths = [None] * WP.maxlevel

            levels_datas = []
            for level in xrange(WP.maxlevel):
                level_nodes = WP.get_level(level+1)
                level_datas = [node.data for node in level_nodes]

                level_lengths = [len(x) for x in level_datas]
                level_length = np.sum(level_lengths)

                if levels_lengths[level] is None:
                    levels_lengths[level] = level_lengths
                elif levels_lengths[level] != level_lengths:
                    raise RuntimeError, \
                          "ADs of same level of different samples should have same number of elements." \
                          " Got %s, was %s" % (level_lengths, levels_lengths[level])

                if levels_length[level] is None:
                    levels_length[level] = level_length
                elif levels_length[level] != level_length:
                    raise RuntimeError, \
                          "Levels of different samples should have same number of elements." \
                          " Got %d, was %d" % (level_length, levels_length[level])

                level_data = np.hstack(level_datas)
                levels_datas.append(level_data)

            # assert(len(data) == levels_length)
            # assert(len(data) >= Ntimepoints)
            if wp is None:
                newdim = list(data.shape)
                newdim[self._dim] = np.sum(levels_length)
                wp = np.empty( tuple(newdim) )
            wp[indexes] = np.hstack(levels_datas)

        self.levels_lengths, self.levels_length = levels_lengths, levels_length
        if __debug__:
            debug('MAP_', "")
            debug('MAP', "Done convertion into wp. Total size %s" % str(wp.shape))
        return wp


    def _wm_forward(self, data):
        if __debug__:
            debug('MAP', "Converting signal using DWP")

        if self.__level is None:
            return self.__forward_multiple_levels(data)
        else:
            return self.__forward_single_level(data)

    #
    # Reverse mapping
    #
    ##REF: Name was automagically refactored
    def __reverse_single_level(self, wp):

        # local bindings
        level_paths = self.__level_paths

        # define wavelet packet to use
        WP = pywt.WaveletPacket(
            data=None, wavelet=self._wavelet,
            mode=self._mode, maxlevel=self.__level)

        # prepare storage
        signal_shape = wp.shape[:1] + self._inshape[1:]
        signal = np.zeros(signal_shape)
        Ntime_points = self._intimepoints
        for indexes in _get_indexes(signal_shape,
                                   self._dim):
            if __debug__:
                debug('MAP_', " %s" % (indexes,), lf=False, cr=True)

            for path, level_data in zip(level_paths, wp[indexes]):
                WP[path] = level_data

            signal[indexes] = WP.reconstruct(True)[:Ntime_points]

        return signal


    def _wm_reverse(self, data):
        if __debug__:
            debug('MAP', "Converting signal back using DWP")

        if self.__level is None:
            raise NotImplementedError
        else:
            if not externals.exists('pywt wp reconstruct'):
                raise NotImplementedError, \
                      "Reconstruction for a single level for versions of " \
                      "pywt < 0.1.7 (revision 103) is not supported"
            if not externals.exists('pywt wp reconstruct fixed'):
                warning("%s: Reverse mapping with this version of 'pywt' might "
                        "result in incorrect data in the tails of the signal. "
                        "Please check for an update of 'pywt', or be careful "
                        "when interpreting the edges of the reverse mapped "
                        "data." % self.__class__.__name__)
            return self.__reverse_single_level(data)



class WaveletTransformationMapper(_WaveletMapper):
    """Convert signal into wavelet representaion
    """

    def _wm_forward(self, data):
        """Decompose signal into wavelets's coefficients via dwt
        """
        if __debug__:
            debug('MAP', "Converting signal using DWT")
        wd = None
        coeff_lengths = None
        for indexes in _get_indexes(data.shape, self._dim):
            if __debug__:
                debug('MAP_', " %s" % (indexes,), lf=False, cr=True)
            coeffs = pywt.wavedec(
                data[indexes],
                wavelet=self._wavelet,
                mode=self._mode,
                level=self._maxlevel)
            # Silly Yarik embedds extraction of statistics right in place
            #stats = []
            #for coeff in coeffs:
            #    stats_ = [np.std(coeff),
            #              np.sqrt(np.dot(coeff, coeff)),
            #              ]# + list(np.histogram(coeff, normed=True)[0]))
            #    stats__ = list(coeff) + stats_[:]
            #    stats__ += list(np.log(stats_))
            #    stats__ += list(np.sqrt(stats_))
            #    stats__ += list(np.array(stats_)**2)
            #    stats__ += [  np.median(coeff), np.mean(coeff), scipy.stats.kurtosis(coeff) ]
            #    stats.append(stats__)
            #coeffs = stats
            coeff_lengths_ = np.array([len(x) for x in coeffs])
            if coeff_lengths is None:
                coeff_lengths = coeff_lengths_
            assert((coeff_lengths == coeff_lengths_).all())
            if wd is None:
                newdim = list(data.shape)
                newdim[self._dim] = np.sum(coeff_lengths)
                wd = np.empty( tuple(newdim) )
            coeff = np.hstack(coeffs)
            wd[indexes] = coeff
        if __debug__:
            debug('MAP_', "")
            debug('MAP', "Done DWT. Total size %s" % str(wd.shape))
        self.lengths = coeff_lengths
        return wd


    def _wm_reverse(self, wd):
        if __debug__:
            debug('MAP', "Performing iDWT")
        signal = None
        wd_offsets = [0] + list(np.cumsum(self.lengths))
        nlevels = len(self.lengths)
        Ntime_points = self._intimepoints #len(time_points)
        # unfortunately sometimes due to padding iDWT would return longer
        # sequences, thus we just limit to the right ones

        for indexes in _get_indexes(wd.shape, self._dim):
            if __debug__:
                debug('MAP_', " %s" % (indexes,), lf=False, cr=True)
            wd_sample = wd[indexes]
            wd_coeffs = [wd_sample[wd_offsets[i]:wd_offsets[i+1]] for i in xrange(nlevels)]
            # need to compose original list
            time_points = pywt.waverec(
                wd_coeffs, wavelet=self._wavelet, mode=self._mode)
            if signal is None:
                newdim = list(wd.shape)
                newdim[self._dim] = Ntime_points
                signal = np.empty(newdim)
            signal[indexes] = time_points[:Ntime_points]
        if __debug__:
            debug('MAP_', "")
            debug('MAP', "Done iDWT. Total size %s" % (signal.shape, ))
        return signal

########NEW FILE########
__FILENAME__ = zscore
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Data normalization by Z-Scoring."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import warning
from mvpa2.base.dochelpers import _str, borrowkwargs, _repr_attrs
from mvpa2.mappers.base import accepts_dataset_as_samples, Mapper
from mvpa2.datasets.base import Dataset
from mvpa2.datasets.miscfx import get_nsamples_per_attr, get_samples_by_attr
from mvpa2.support import copy


class ZScoreMapper(Mapper):
    """Mapper to normalize features (Z-scoring).

    Z-scoring can be done chunk-wise (with independent mean and standard
    deviation per chunk) or on the full data. It is possible to specify
    a sample attribute, unique value of which would then be used to determine
    the chunks.

    By default, Z-scoring parameters (mean and standard deviation) are
    estimated from the data (either chunk-wise or globally). However, it is
    also possible to define fixed parameters (again a global setting or
    per-chunk definitions), or to select a specific subset of samples from
    which these parameters should be estimated.

    If necessary, data is upcasted into a configurable datatype to prevent
    information loss.

    Notes
    -----

    It should be mentioned that the mapper can be used for forward-mapping
    of datasets without prior training (it will auto-train itself
    upon first use). It is, however, not possible to map plain data arrays
    without prior training. Also, for obvious reasons, it is also not possible
    to perform chunk-wise Z-scoring of plain data arrays.

    Reverse-mapping is currently not implemented.
    """
    def __init__(self, params=None, param_est=None, chunks_attr='chunks',
                 dtype='float64', **kwargs):
        """
        Parameters
        ----------
        params : None or tuple(mean, std) or dict
          Fixed Z-Scoring parameters (mean, standard deviation). If provided,
          no parameters are estimated from the data. It is possible to specify
          individual parameters for each chunk by passing a dictionary with the
          chunk ids as keys and the parameter tuples as values. If None,
          parameters will be estimated from the training data.
        param_est : None or tuple(attrname, attrvalues)
          Limits the choice of samples used for automatic parameter estimation
          to a specific subset identified by a set of a given sample attribute
          values.  The tuple should have the name of that sample
          attribute as the first element, and a sequence of attribute values
          as the second element. If None, all samples will be used for parameter
          estimation.
        chunks_attr : str or None
          If provided, it specifies the name of a samples attribute in the
          training data, unique values of which will be used to identify chunks of
          samples, and to perform individual Z-scoring within them.
        dtype : Numpy dtype, optional
          Target dtype that is used for upcasting, in case integer data is to be
          Z-scored.
        """
        Mapper.__init__(self, **kwargs)

        self.__chunks_attr = chunks_attr
        self.__params = params
        self.__param_est = param_est
        self.__params_dict = None
        self.__dtype = dtype

        # secret switch to perform in-place z-scoring
        self._secret_inplace_zscore = False


    def __repr__(self, prefixes=[]):
        return super(ZScoreMapper, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['params', 'param_est', 'chunks_attr'])
            + _repr_attrs(self, ['dtype'], default='float64'))


    def __str__(self):
        return _str(self)


    def _train(self, ds):
        # local binding
        chunks_attr = self.__chunks_attr
        params = self.__params
        param_est = self.__param_est

        # populate a dictionary with tuples of (mean,std) for all chunks, or
        # a global value that is is used for the whole data
        if not params is None:
            # we got mean and std already
            if not isinstance(params, dict):
                # turn into dict, otherwise assume that we have parameters per
                # chunk
                params = {'__all__': params}
        else:
            # no parameters given, need to estimate
            if not param_est is None:
                est_attr, est_attr_values = param_est
                # which samples to use for estimation
                est_ids = set(get_samples_by_attr(ds, est_attr,
                                                  est_attr_values))
            else:
                est_ids = slice(None)

            # now we can either do it one for all, or per chunk
            if not chunks_attr is None:
                # per chunk estimate
                params = {}
                for c in ds.sa[chunks_attr].unique:
                    slicer = np.where(ds.sa[chunks_attr].value == c)[0]
                    if not isinstance(est_ids, slice):
                        slicer = list(est_ids.intersection(set(slicer)))
                    params[c] = self._compute_params(ds.samples[slicer])
            else:
                # global estimate
                params = {'__all__': self._compute_params(ds.samples[est_ids])}


        self.__params_dict = params


    def _forward_dataset(self, ds):
        # local binding
        chunks_attr = self.__chunks_attr
        dtype = self.__dtype

        if __debug__ and not chunks_attr is None:
            nsamples_per_chunk = get_nsamples_per_attr(ds, chunks_attr)
            min_nsamples_per_chunk = np.min(nsamples_per_chunk.values())
            if min_nsamples_per_chunk in range(3, 6):
                warning("Z-scoring chunk-wise having a chunk with only "
                        "%d samples is 'discouraged'. "
                        "You have chunks with following number of samples: %s"
                        % (min_nsamples_per_chunk, nsamples_per_chunk,))
            if min_nsamples_per_chunk <= 2:
                warning("Z-scoring chunk-wise having a chunk with less "
                        "than three samples will set features in these "
                        "samples to either zero (with 1 sample in a chunk) "
                        "or -1/+1 (with 2 samples in a chunk). "
                        "You have chunks with following number of samples: %s"
                        % (nsamples_per_chunk,))

        params = self.__params_dict
        if params is None:
            raise RuntimeError, \
                  "ZScoreMapper needs to be trained before call to forward"

        if self._secret_inplace_zscore:
            mds = ds
        else:
            # shallow copy to put the new stuff in
            mds = ds.copy(deep=False)
            # but deepcopy the samples since _zscore would modify inplace
            mds.samples = mds.samples.copy()

        # cast the data to float, since in-place operations below do not upcast!
        if np.issubdtype(mds.samples.dtype, np.integer):
            mds.samples = mds.samples.astype(dtype)

        if '__all__' in params:
            # we have a global parameter set
            mds.samples = self._zscore(mds.samples, *params['__all__'])
        else:
            # per chunk z-scoring
            for c in mds.sa[chunks_attr].unique:
                if not c in params:
                    raise RuntimeError(
                        "%s has no parameters for chunk '%s'. It probably "
                        "wasn't present in the training dataset!?"
                        % (self.__class__.__name__, c))
                slicer = np.where(mds.sa[chunks_attr].value == c)[0]
                mds.samples[slicer] = self._zscore(mds.samples[slicer],
                                                   *params[c])

        return mds


    def _forward_data(self, data):
        if self.__chunks_attr is not None:
            raise RuntimeError(
                "%s cannot do chunk-wise Z-scoring of plain data "
                "since it has to be parameterized with chunks_attr." % self)
        if self.__param_est is not None:
            raise RuntimeError("%s cannot do Z-scoring with estimating "
                               "parameters on some attributes of plain"
                               "data." % self)

        params = self.__params_dict
        if params is None:
            raise RuntimeError, \
                  "ZScoreMapper needs to be trained before call to forward"

        # mappers should not modify the input data
        # cast the data to float, since in-place operations below to not upcast!
        if np.issubdtype(data.dtype, np.integer):
            if self._secret_inplace_zscore:
                raise TypeError(
                    "Cannot perform inplace z-scoring since data is of integer "
                    "type. Please convert to float before calling zscore")
            mdata = data.astype(self.__dtype)
        elif self._secret_inplace_zscore:
            mdata = data
        else:
            # do not call .copy() directly, since it might not be an array
            mdata = copy.deepcopy(data)

        self._zscore(mdata, *params['__all__'])
        return mdata


    def _compute_params(self, samples):
        return (np.mean(samples, axis=0), np.std(samples, axis=0))


    def _zscore(self, samples, mean, std):
        # de-mean
        if np.isscalar(mean) or samples.shape[1] == len(mean):
            mean = np.asanyarray(mean)  # assure array
            samples -= mean
        else:
            raise RuntimeError("mean should be a per-feature vector. Got: %r"
                               % (mean,))

        # scale
        if np.isscalar(std):
            if std == 0:
                samples[:] = 0
            else:
                samples /= std
        else:
            std = np.asanyarray(std)
            if samples.shape[1] != len(std):
                raise RuntimeError("std should be a per-feature vector.")
            else:
                # check for invariant features
                std_nz = std != 0
                samples[:, std_nz] /= np.asanyarray(std)[std_nz]
        return samples

    params = property(fget=lambda self:self.__params)
    param_est = property(fget=lambda self:self.__param_est)
    chunks_attr = property(fget=lambda self:self.__chunks_attr)
    dtype = property(fget=lambda self:self.__dtype)


@borrowkwargs(ZScoreMapper, '__init__')
def zscore(ds, **kwargs):
    """In-place Z-scoring of a `Dataset` or `ndarray`.

    This function behaves identical to `ZScoreMapper`. The only difference is
    that the actual Z-scoring is done in-place -- potentially causing a
    significant reduction of memory demands.

    Parameters
    ----------
    ds : Dataset or ndarray
      The data that will be Z-scored in-place.
    **kwargs
      For all other arguments, please see the documentation of `ZScoreMapper`.
    """
    zm = ZScoreMapper(**kwargs)
    zm._secret_inplace_zscore = True
    # train
    if isinstance(ds, Dataset):
        zm.train(ds)
    else:
        zm.train(Dataset(ds))
    # map
    mapped = zm.forward(ds)
    # and append the mapper to the dataset
    if isinstance(mapped, Dataset):
        mapped._append_mapper(zm)

########NEW FILE########
__FILENAME__ = adhocsearchlightbase
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support functionality for GNB and M1NN searchlights"""

__docformat__ = 'restructuredtext'

import numpy as np

#from numpy import ones, zeros, sum, abs, isfinite, dot
#from mvpa2.base import warning, externals
from mvpa2.datasets.base import Dataset
from mvpa2.misc.errorfx import mean_mismatch_error
from mvpa2.measures.searchlight import BaseSearchlight
from mvpa2.base import externals, warning
from mvpa2.base.dochelpers import borrowkwargs, _repr_attrs
from mvpa2.generators.splitters import Splitter

#from mvpa2.base.param import Parameter
#from mvpa2.base.state import ConditionalAttribute
#from mvpa2.measures.base import Sensitivity

from mvpa2.misc.neighborhood import IndexQueryEngine, Sphere

if __debug__:
    from mvpa2.base import debug
    import time as time

if externals.exists('scipy'):
    import scipy.sparse as sps
    # API of scipy.sparse has changed in 0.7.0 -- lets account for this
    _coo_shape_argument = {
        True: 'shape',
        False: 'dims'} [externals.versions['scipy'] >= '0.7.0']

__all__ = [ "SimpleStatSearchlight" ]

def lastdim_columnsums_fancy_indexing(a, inds, out):
    for i, inds_ in enumerate(inds):
        out[..., i] = a[..., inds_].sum(axis=-1)

#
# Machinery for sparse matrix way
#

# silly Yarik failed to do np.r_[*neighbors] directly, so here is a
# trick
def r_helper(*args):
    return np.r_[args]

def _inds_list_to_coo(inds, shape=None):
    inds_r = r_helper(*(inds))
    inds_i = r_helper(*[[i]*len(ind)
                        for i,ind in enumerate(inds)])
    data = np.ones(len(inds_r))
    ij = np.array([inds_r, inds_i])

    spmat = sps.coo_matrix((data, ij), dtype=int, **{_coo_shape_argument:shape})
    return spmat

def _inds_array_to_coo(inds, shape=None):
    n_sums, n_cols_per_sum = inds.shape
    cps_inds = inds.ravel()
    row_inds = np.repeat(np.arange(n_sums)[None, :],
                         n_cols_per_sum, axis=0).T.ravel()
    ij = np.r_[cps_inds[None, :], row_inds[None, :]]
    data  = np.ones(ij.shape[1])

    inds_s = sps.coo_matrix((data, ij), **{_coo_shape_argument:shape})
    return inds_s

def inds_to_coo(inds, shape=None):
    """Dispatcher for conversion to coo
    """
    if isinstance(inds, np.ndarray):
        return _inds_array_to_coo(inds, shape)
    elif isinstance(inds, list):
        return _inds_list_to_coo(inds, shape)
    else:
        raise NotImplementedError, "add conversion here"

def lastdim_columnsums_spmatrix(a, inds, out):
    # inds is a 2D array or list or already a sparse matrix, with each
    # row specifying a set of columns (in fact last dimension indices)
    # to sum.  Thus there are the same number of sums as there are
    # rows in `inds`.

    n_cols = a.shape[-1]
    in_shape = a.shape[:-1]

    # first convert to sparse if necessary
    if sps.isspmatrix(inds):
        n_sums = inds.shape[1]
        inds_s = inds
    else:                               # assume regular iterable
        n_sums = len(inds)
        inds_s = inds_to_coo(inds, shape=(n_cols, n_sums))

    ar = a.reshape((-1, a.shape[-1]))
    sums = np.asarray((sps.csr_matrix(ar) * inds_s).todense())
    out[:] = sums.reshape(in_shape+(n_sums,))


class _STATS:
    """Just a dummy container to group/access stats
    """
    pass


class SimpleStatBaseSearchlight(BaseSearchlight):
    """Base class for clf searchlights based on basic univar. statistics

    Used for GNB and M1NN Searchlights

    TODO
    ----

    some stats are not needed (eg per sample X^2's) for M1NN, so we
    should make them optional depending on the derived class

    Notes
    -----

    refactored from the original GNBSearchlight

    """

    # TODO: implement parallelization (see #67) and then uncomment
    __init__doc__exclude__ = ['nproc']

    def __init__(self, generator, queryengine, errorfx=mean_mismatch_error,
                 indexsum=None,
                 reuse_neighbors=False,
                 **kwargs):
        """Initialize the base class for "naive" searchlight classifiers

        Parameters
        ----------
        generator : `Generator`
          Some `Generator` to prepare partitions for cross-validation.
          It must not change "targets", thus e.g. no AttributePermutator's
        errorfx : func, optional
          Functor that computes a scalar error value from the vectors of
          desired and predicted values (e.g. subclass of `ErrorFunction`).
        indexsum : ('sparse', 'fancy'), optional
          What use to compute sums over arbitrary columns.  'fancy'
          corresponds to regular fancy indexing over columns, whenever
          in 'sparse', product of sparse matrices is used (usually
          faster, so is default if `scipy` is available).
        reuse_neighbors : bool, optional
          Compute neighbors information only once, thus allowing for
          efficient reuse on subsequent calls where dataset's feature
          attributes remain the same (e.g. during permutation testing)
        """

        # init base class first
        BaseSearchlight.__init__(self, queryengine, **kwargs)

        self._errorfx = errorfx
        self._generator = generator

        # TODO: move into _call since resetting over default None
        #       obscures __repr__
        if indexsum is None:
            if externals.exists('scipy'):
                indexsum = 'sparse'
            else:
                indexsum = 'fancy'
        else:
            if indexsum == 'sparse' and not externals.exists('scipy'):
                warning("Scipy.sparse isn't available so taking 'fancy' as "
                        "'indexsum' method.")
                indexsum = 'fancy'
        self._indexsum = indexsum

        if not self.nproc in (None, 1):
            raise NotImplementedError, "For now only nproc=1 (or None for " \
                  "autodetection) is supported by GNBSearchlight"

        self.__pb = None            # statistics per each block/label
        self.__reuse_neighbors = reuse_neighbors

        # Storage to be used for neighborhood information
        self.__roi_fids = None

    def __repr__(self, prefixes=[]):
        return super(SimpleStatBaseSearchlight, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['generator'])
            + _repr_attrs(self, ['errorfx'], default=mean_mismatch_error)
            + _repr_attrs(self, ['indexsum'])
            + _repr_attrs(self, ['reuse_neighbors'], default=False)
            )

    def _get_space(self):
        raise NotImplementedError("Must be implemented in derived classes")

    def _untrain(self):
        super(SimpleStatBaseSearchlight, self)._untrain()
        self.__pb = None


    def _compute_pb_stats(self, labels_numeric,
                          X, shape):
        #
        # reusable containers which should stay of the same size
        #
        nblocks = shape[0]
        pb = self.__pb = _STATS()

        # sums and sums of squares per each block
        pb.sums = np.zeros(shape)
        # sums of squares
        pb.sums2 = np.zeros(shape)

        pb.nsamples = np.zeros((nblocks,))
        pb.labels = [None] * nblocks

        if np.issubdtype(X.dtype, np.int):
            # might result in overflow e.g. while taking .square which
            # would result in negative variances etc, thus to be on a
            # safe side -- convert to float
            X = X.astype(float)

        X2 = np.square(X)
        # silly way for now
        for l, s, s2, ib in zip(labels_numeric, X, X2, self.__sample2block):
            pb.sums[ib] += s
            pb.sums2[ib] += s2
            pb.nsamples[ib] += 1
            if pb.labels[ib] is None:
                pb.labels[ib] = l
            else:
                assert(pb.labels[ib] == l)

        pb.labels = np.asanyarray(pb.labels)
        # additional silly tests for paranoid
        assert(pb.labels.dtype.kind == 'i')


    def _compute_pl_stats(self, sis, pl):
        """
        Uses blocked stats to get stats across given samples' indexes
        (might be training or testing)

        Parameters
        ----------
        sis : array of int
          Indexes of samples
        *args:
          In-place containers
        """
        # local binding
        pb = self.__pb

        # convert to blocks training split
        bis = np.unique(self.__sample2block[sis])

        # Let's collect stats summaries
        nsamples = 0
        for il, l in enumerate(self._ulabels_numeric):
            bis_il = bis[pb.labels[bis] == l]
            pl.nsamples[il] = N_float = \
                                     float(np.sum(pb.nsamples[bis_il]))
            nsamples += N_float
            if N_float == 0.0:
                pl.variances[il] = pl.sums[il] \
                    = pl.means[il] = pl.sums2[il] = 0.
            else:
                pl.sums[il] = np.sum(pb.sums[bis_il], axis=0)
                pl.means[il] = pl.sums[il] / N_float
                pl.sums2[il] = np.sum(pb.sums2[bis_il], axis=0)

        ## Actually compute the non-0 pl.variances
        non0labels = (pl.nsamples.squeeze() != 0)
        if np.all(non0labels):
            # For a possible tiny speed up avoiding copying and
            # using (no) slicing
            non0labels = slice(None)

        return nsamples, non0labels


    def _sl_call(self, dataset, roi_ids, nproc):
        """Call to SimpleStatBaseSearchlight
        """
        # Local bindings
        generator = self.generator
        qe = self.queryengine
        errorfx = self.errorfx

        if __debug__:
            time_start = time.time()

        targets_sa_name = self._get_space()
        targets_sa = dataset.sa[targets_sa_name]

        if __debug__:
            debug_slc_ = 'SLC_' in debug.active

        # get the dataset information into easy vars
        X = dataset.samples
        if len(X.shape) != 2:
            raise ValueError(
                  'Unlike a classifier, %s (for now) operates on already'
                  'flattened datasets' % (self.__class__.__name__))
        labels = targets_sa.value
        ulabels = targets_sa.unique
        nlabels = len(ulabels)
        label2index = dict((l, il) for il, l in enumerate(ulabels))
        labels_numeric = np.array([label2index[l] for l in labels])
        self._ulabels_numeric = [label2index[l] for l in ulabels]
        # set the feature dimensions
        nsamples = len(X)
        nrois = len(roi_ids)
        s_shape = X.shape[1:]           # shape of a single sample
        # The shape of results
        r_shape = (nrois,) + X.shape[2:]

        #
        # Everything toward optimization ;)
        #
        # Silly Yarik thinks that it might be worth to pre-compute
        # statistics per each feature within a block of the samples
        # which always come together in splits -- most often it is a
        # (chunk, label) combination, but since we simply use a
        # generator -- who knows! Therefore lets figure out what are
        # those blocks and operate on them instead of original samples.
        #
        # After additional thinking about this -- probably it would be
        # just minor additional improvements (ie not worth it) but
        # since it is coded already -- let it be so

        # 1. Query generator for the splits we will have
        if __debug__:
            debug('SLC',
                  'Phase 1. Initializing partitions using %s on %s'
                  % (generator, dataset))

        # Lets just create a dummy ds which will store for us actual sample
        # indicies
        # XXX we could make it even more lightweight I guess...
        dataset_indicies = Dataset(np.arange(nsamples), sa=dataset.sa)
        splitter = Splitter(attr=generator.get_space())
        partitions = list(generator.generate(dataset_indicies))
        if __debug__:
            for p in partitions:
                if not (np.all(p.sa[targets_sa_name].value == labels)):
                    raise NotImplementedError(
                        "%s does not yet support partitioners altering the targets "
                        "(e.g. permutators)" % self.__class__)

        nsplits = len(partitions)
        # ATM we need to keep the splits instead since they are used
        # in two places in the code: step 2 and 5
        splits = list(tuple(splitter.generate(ds_)) for ds_ in partitions)
        del partitions                    # not used any longer

        # 2. Figure out the new 'chunks x labels' blocks of combinations
        #    of samples
        if __debug__:
            debug('SLC',
                  'Phase 2. Blocking data for %i splits and %i labels'
                  % (nsplits, nlabels))
        # array of indicies for label, split1, split2, ...
        # through which we will pass later on to figure out
        # unique combinations
        combinations = np.ones((nsamples, 1+nsplits), dtype=int)*-1
        # labels
        combinations[:, 0] = labels_numeric
        for ipartition, (split1, split2) in enumerate(splits):
            combinations[split1.samples[:, 0], 1+ipartition] = 1
            combinations[split2.samples[:, 0], 1+ipartition] = 2
            # Check for over-sampling, i.e. no same sample used twice here
            if not (len(np.unique(split1.samples[:, 0])) == len(split1) and
                    len(np.unique(split2.samples[:, 0])) == len(split2)):
                raise RuntimeError(
                    "%s needs a partitioner which does not reuse "
                    "the same the same samples more than once"
                    % self.__class__)
        # sample descriptions -- should be unique for
        # samples within the same block
        descriptions = [tuple(c) for c in combinations]
        udescriptions = sorted(list(set(descriptions)))
        nblocks = len(udescriptions)
        description2block = dict([(d, i) for i, d in enumerate(udescriptions)])
        # Indices for samples to point to their block
        self.__sample2block = sample2block = \
            np.array([description2block[d] for d in descriptions])

        # 3. Compute statistics per each block
        #
        if __debug__:
            debug('SLC',
                  'Phase 3. Computing statistics for %i blocks' % (nblocks,))

        self._compute_pb_stats(labels_numeric, X, (nblocks,) + s_shape)

        # derived classes might decide differently on what they
        # actually need, so defer reserving the space and computing
        # stats to them
        self._reserve_pl_stats_space((nlabels, ) + s_shape)

        # results
        results = np.zeros((nsplits,) + r_shape)

        # 4. Lets deduce all neighbors... might need to be RF into the
        #    parallel part later on
        # TODO: needs OPT since this is the step consuming 50% of time
        #       or more allow to cache them entirely so this would
        #       not be an unnecessary burden during permutation testing
        if not self.reuse_neighbors or self.__roi_fids is None:
            if __debug__:
                debug('SLC',
                      'Phase 4. Deducing neighbors information for %i ROIs'
                      % (nrois,))
            roi_fids = [qe.query_byid(f) for f in roi_ids]

        else:
            if __debug__:
                debug('SLC',
                      'Phase 4. Reusing neighbors information for %i ROIs'
                      % (nrois,))
            roi_fids = self.__roi_fids

        self.ca.roi_feature_ids = roi_fids

        roi_sizes = []
        if isinstance(roi_fids, list):
            nroi_fids = len(roi_fids)
            if self.ca.is_enabled('roi_sizes'):
                roi_sizes = [len(x) for x in roi_fids]
        elif externals.exists('scipy') and isinstance(roi_fids, sps.spmatrix):
            nroi_fids = roi_fids.shape[1]
            if self.ca.is_enabled('roi_sizes'):
                # very expensive operation, so better not to ask over again
                # roi_sizes = [roi_fids.getrow(r).nnz for r in range(nroi_fids)]
                warning("Since 'sparse' trick is used, extracting sizes of "
                        "roi's are expensive at this point.  Get them from the "
                        ".ca value of the original instance before "
                        "calling again and using reuse_neighbors")
        else:
            raise RuntimeError("Should not be reachable")

        # Since this is ad-hoc implementation of the searchlight, we are not passing
        # those via ds.a  but rather assign directly to self.ca
        self.ca.roi_sizes = roi_sizes

        indexsum = self._indexsum
        if indexsum == 'sparse':
            if not self.reuse_neighbors or self.__roi_fids is None:
                if __debug__:
                    debug('SLC',
                          'Phase 4b. Converting neighbors to sparse matrix '
                          'representation')
                # convert to "sparse representation" where column j contains
                # 1s only at the roi_fids[j] indices
                roi_fids = inds_to_coo(roi_fids,
                                       shape=(dataset.nfeatures, nroi_fids))
            indexsum_fx = lastdim_columnsums_spmatrix
        elif indexsum == 'fancy':
            indexsum_fx = lastdim_columnsums_fancy_indexing
        else:
            raise ValueError, \
                  "Do not know how to deal with indexsum=%s" % indexsum

        # Store roi_fids
        if self.reuse_neighbors and self.__roi_fids is None:
            self.__roi_fids = roi_fids

        # 5. Lets do actual "splitting" and "classification"
        if __debug__:
            debug('SLC', 'Phase 5. Major loop' )


        for isplit, split in enumerate(splits):
            if __debug__:
                debug('SLC', ' Split %i out of %i' % (isplit, nsplits))
            # figure out for a given splits the blocks we want to work
            # with
            # sample_indicies
            training_sis = split[0].samples[:, 0]
            testing_sis = split[1].samples[:, 0]

            # That is the GNB specificity
            targets, predictions = self._sl_call_on_a_split(
                split, X,               # X2 might light to go
                training_sis, testing_sis,
                ## training_nsamples,      # GO? == np.sum(pl.nsamples)
                ## training_non0labels,
                ## pl.sums, pl.means, pl.sums2, pl.variances,
                # passing nroi_fids as well since in 'sparse' way it has no 'length'
                nroi_fids, roi_fids,
                indexsum_fx,
                labels_numeric,
                )

            # assess the errors
            if __debug__:
                debug('SLC', "  Assessing accuracies")

            if errorfx is mean_mismatch_error:
                results[isplit, :] = \
                    (predictions != targets[:, None]).sum(axis=0) \
                    / float(len(targets))
            else:
                # somewhat silly but a way which allows to use pre-crafted
                # error functions without a chance to screw up
                for i, fpredictions in enumerate(predictions.T):
                    results[isplit, i] = errorfx(fpredictions, targets)


        if __debug__:
            debug('SLC', "%s._call() is done in %.3g sec" %
                  (self.__class__.__name__, time.time() - time_start))

        return Dataset(results)

    generator = property(fget=lambda self: self._generator)
    errorfx = property(fget=lambda self: self._errorfx)
    indexsum = property(fget=lambda self: self._indexsum)
    reuse_neighbors = property(fget=lambda self: self.__reuse_neighbors)

########NEW FILE########
__FILENAME__ = anova
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Univariate ANOVA"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import externals
from mvpa2.measures.base import FeaturewiseMeasure
from mvpa2.base.dataset import vstack
from mvpa2.datasets.base import Dataset

# TODO: Extend with access to functionality from scipy.stats?
# For binary:
#  2-sample kolmogorov-smirnof might be interesting
#   (scipy.stats.ks_2samp) to judge if two conditions are derived
#   from different distributions (take it as 'activity' vs 'rest'),
#
# For binary+multiclass:
#  kruskal-wallis H-test (scipy.stats.kruskal)
#
# and may be some others

class OneWayAnova(FeaturewiseMeasure):
    """`FeaturewiseMeasure` that performs a univariate ANOVA.

    F-scores are computed for each feature as the standard fraction of between
    and within group variances. Groups are defined by samples with unique
    labels.

    No statistical testing is performed, but raw F-scores are returned as a
    sensitivity map. As usual F-scores have a range of [0,inf] with greater
    values indicating higher sensitivity.

    The sensitivity map is returned as a single-sample dataset. If SciPy is
    available the associated p-values will also be computed and are available
    from the 'fprob' feature attribute.
    """

    def __init__(self, space='targets', **kwargs):
        """
        Parameters
        ----------
        space : str
          What samples attribute to use as targets (labels).
        """
        # set auto-train flag since we have nothing special to be done
        # so by default auto train
        kwargs['auto_train'] = kwargs.get('auto_train', True)
        FeaturewiseMeasure.__init__(self, space=space, **kwargs)


    def __repr__(self, prefixes=None):
        if prefixes is None:
            prefixes = []
        if self.get_space() != 'targets':
            prefixes = prefixes + ['targets_attr=%r' % (self.get_space())]
        return \
            super(FeaturewiseMeasure, self).__repr__(prefixes=prefixes)


    def _call(self, dataset):
        # This code is based on SciPy's stats.f_oneway()
        # Copyright (c) Gary Strangman.  All rights reserved
        # License: BSD
        #
        # However, it got tweaked and optimized to better fit into PyMVPA.

        # number of groups
        targets_sa = dataset.sa[self.get_space()]
        labels = targets_sa.value
        ul = targets_sa.unique

        na = len(ul)
        bign = float(dataset.nsamples)
        alldata = dataset.samples

        # total squares of sums
        sostot = np.sum(alldata, axis=0)
        sostot *= sostot
        sostot /= bign

        # total sum of squares
        sstot = np.sum(alldata * alldata, axis=0) - sostot

        # between group sum of squares
        ssbn = 0
        for l in ul:
            # all samples for the respective label
            d = alldata[labels == l]
            sos = np.sum(d, axis=0)
            sos *= sos
            ssbn += sos / float(len(d))

        ssbn -= sostot
        # within
        sswn = sstot - ssbn

        # degrees of freedom
        dfbn = na-1
        dfwn = bign - na

        # mean sums of squares
        msb = ssbn / float(dfbn)
        msw = sswn / float(dfwn)
        f = msb / msw
        # assure no NaNs -- otherwise it leads instead of
        # sane unittest failure (check of NaNs) to crazy
        #   File "mtrand.pyx", line 1661, in mtrand.shuffle
        #  TypeError: object of type 'numpy.int64' has no len()
        # without any sane backtrace
        f[np.isnan(f)] = 0

        if externals.exists('scipy'):
            from scipy.stats import fprob
            return Dataset(f[np.newaxis], fa={'fprob': fprob(dfbn, dfwn, f)})
        else:
            return Dataset(f[np.newaxis])


class CompoundOneWayAnova(OneWayAnova):
    """Compound comparisons via univariate ANOVA.

    This measure compute an ANOVA F-score per each feature, for each
    one-vs-rest comparision for all unique labels in a dataset. Each F-score
    vector for each comparision is included in the return datasets as a separate
    samples. Corresponding p-values are avialable in feature attributes named
    'fprob_X', where `X` is the name of the actual comparision label. Note that
    p-values are only available, if SciPy is installed. The comparison labels
    for each F-vectore are also stored as 'targets' sample attribute in the
    returned dataset.
    """

    def _call(self, dataset):
        """Computes featurewise f-scores using compound comparisons."""

        targets_sa = dataset.sa[self.get_space()]
        orig_labels = targets_sa.value
        labels = orig_labels.copy()

        # Lets create a very shallow copy of a dataset with just
        # samples and targets_attr
        dataset_mod = Dataset(dataset.samples,
                              sa={self.get_space() : labels})
        results = []
        for ul in targets_sa.unique:
            labels[orig_labels == ul] = 1
            labels[orig_labels != ul] = 2
            f_ds = OneWayAnova._call(self, dataset_mod)
            if 'fprob' in f_ds.fa:
                # rename the fprob attribute to something label specific
                # to survive final aggregation stage
                f_ds.fa['fprob_' + str(ul)] = f_ds.fa.fprob
                del f_ds.fa['fprob']
            results.append(f_ds)

        results = vstack(results)
        results.sa[self.get_space()] = targets_sa.unique
        return results

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Plumbing for measures: algorithms that quantify properties of datasets.

Besides the `Measure` base class this module also provides the
(abstract) `FeaturewiseMeasure` class. The difference between a general
measure and the output of the `FeaturewiseMeasure` is that the latter
returns a 1d map (one value per feature in the dataset). In contrast there are
no restrictions on the returned value of `Measure` except for that it
has to be in some iterable container.

"""

__docformat__ = 'restructuredtext'

import numpy as np
import mvpa2.support.copy as copy

from mvpa2.base.node import Node
from mvpa2.base.learner import Learner
from mvpa2.base.state import ConditionalAttribute
from mvpa2.misc.args import group_kwargs
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.misc.errorfx import mean_mismatch_error
from mvpa2.base.types import asobjarray

from mvpa2.base.dochelpers import enhanced_doc_string, _str, _repr_attrs
from mvpa2.base import externals, warning
from mvpa2.clfs.stats import auto_null_dist
from mvpa2.base.dataset import AttrDataset, vstack
from mvpa2.datasets import Dataset, vstack, hstack
from mvpa2.mappers.fx import BinaryFxNode
from mvpa2.generators.splitters import Splitter

if __debug__:
    from mvpa2.base import debug


class Measure(Learner):
    """A measure computed from a `Dataset`

    All dataset measures support arbitrary transformation of the measure
    after it has been computed. Transformation are done by processing the
    measure with a functor that is specified via the `transformer` keyword
    argument of the constructor. Upon request, the raw measure (before
    transformations are applied) is stored in the `raw_results` conditional attribute.

    Additionally all dataset measures support the estimation of the
    probabilit(y,ies) of a measure under some distribution. Typically this will
    be the NULL distribution (no signal), that can be estimated with
    permutation tests. If a distribution estimator instance is passed to the
    `null_dist` keyword argument of the constructor the respective
    probabilities are automatically computed and stored in the `null_prob`
    conditional attribute.

    Notes
    -----
    For developers: All subclasses shall get all necessary parameters via
    their constructor, so it is possible to get the same type of measure for
    multiple datasets by passing them to the __call__() method successively.

    """

    null_prob = ConditionalAttribute(enabled=True)
    """Stores the probability of a measure under the NULL hypothesis"""
    null_t = ConditionalAttribute(enabled=False)
    """Stores the t-score corresponding to null_prob under assumption
    of Normal distribution"""

    def __init__(self, null_dist=None, **kwargs):
        """
        Parameters
        ----------
        null_dist : instance of distribution estimator
          The estimated distribution is used to assign a probability for a
          certain value of the computed measure.
        """
        Learner.__init__(self, **kwargs)

        null_dist_ = auto_null_dist(null_dist)
        if __debug__:
            debug('SA', 'Assigning null_dist %s whenever original given was %s'
                  % (null_dist_, null_dist))
        self.__null_dist = null_dist_


    __doc__ = enhanced_doc_string('Measure', locals(),
                                  Learner)

    def __repr__(self, prefixes=[]):
        """String representation of a `Measure`

        Includes only arguments which differ from default ones
        """
        return super(Measure, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['null_dist']))


    def _precall(self, ds):
        # estimate the NULL distribution when functor is given
        if not self.__null_dist is None:
            if __debug__:
                debug("STAT", "Estimating NULL distribution using %s"
                      % self.__null_dist)

            # we need a matching measure instance, but we have to disable
            # the estimation of the null distribution in that child to prevent
            # infinite looping.
            measure = copy.copy(self)
            measure.__null_dist = None
            self.__null_dist.fit(measure, ds)


    def _postcall(self, dataset, result):
        """Some postprocessing on the result
        """
        if self.__null_dist is None:
            # do base-class postcall and be done
            result = super(Measure, self)._postcall(dataset, result)
        else:
            # don't do a full base-class postcall, only do the
            # postproc-application here, to gain result compatibility with the
            # fitted null distribution -- necessary to be able to use
            # a Node's 'pass_attr' to pick up ca.null_prob
            result = self._apply_postproc(dataset, result)

            if self.ca.is_enabled('null_t'):
                # get probability under NULL hyp, but also request
                # either it belong to the right tail
                null_prob, null_right_tail = \
                           self.__null_dist.p(result, return_tails=True)
                self.ca.null_prob = null_prob

                externals.exists('scipy', raise_=True)
                from scipy.stats import norm

                # TODO: following logic should appear in NullDist,
                #       not here
                tail = self.null_dist.tail
                if tail == 'left':
                    acdf = np.abs(null_prob.samples)
                elif tail == 'right':
                    acdf = 1.0 - np.abs(null_prob.samples)
                elif tail in ['any', 'both']:
                    acdf = 1.0 - np.clip(np.abs(null_prob.samples), 0, 0.5)
                else:
                    raise RuntimeError, 'Unhandled tail %s' % tail
                # We need to clip to avoid non-informative inf's ;-)
                # that happens due to lack of precision in mantissa
                # which is 11 bits in double. We could clip values
                # around 0 at as low as 1e-100 (correspond to z~=21),
                # but for consistency lets clip at 1e-16 which leads
                # to distinguishable value around p=1 and max z=8.2.
                # Should be sufficient range of z-values ;-)
                clip = 1e-16
                null_t = norm.ppf(np.clip(acdf, clip, 1.0 - clip))
                # assure that we deal with arrays:
                null_t = np.array(null_t, ndmin=1, copy=False)
                null_t[~null_right_tail] *= -1.0 # revert sign for negatives
                null_t_ds = null_prob.copy(deep=False)
                null_t_ds.samples = null_t
                self.ca.null_t = null_t_ds          # store as a Dataset
            else:
                # get probability of result under NULL hypothesis if available
                # and don't request tail information
                self.ca.null_prob = self.__null_dist.p(result)
            # now do the second half of postcall and invoke pass_attr
            result = self._pass_attr(dataset, result)
        return result


    @property
    def null_dist(self):
        """Return Null Distribution estimator"""
        return self.__null_dist


class ProxyMeasure(Measure):
    """Wrapper to allow for alternative post-processing of a shared measure.

    This class is useful whenever a measure (or for example a trained
    classifier) shall be utilized in multiple nodes, but each node needs to
    perform its on post-processing of results. One can simply wrap the
    measure into this class and assign arbitrary post-processing nodes to the
    wrapper, instead of the measure itself.
    """

    def __init__(self, measure, skip_train=False, **kwargs):
        """
        Parameters
        ----------
        skip_train : bool, optional
          Flag whether the measure does not need to be really trained,
          since proxied measure is guaranteed to be trained appropriately
          prior to this call.  Use with caution
        """

        # by default auto train
        kwargs['auto_train'] = kwargs.get('auto_train', True)
        Measure.__init__(self, **kwargs)
        self.__measure = measure
        self.skip_train = skip_train

    def __repr__(self, prefixes=[]):
        """String representation of a `ProxyMeasure`
        """
        return super(ProxyMeasure, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['measure'])
            + _repr_attrs(self, ['skip_train'], default=False)
            )

    def _train(self, ds):
        if not self.skip_train:
            self.measure.train(ds)
        else:
            # only flag that it was trained
            self._set_trained()


    def _call(self, ds):
        return self.measure(ds)


    @property
    def measure(self):
        """Return proxied measure"""
        return self.__measure


class RepeatedMeasure(Measure):
    """Repeatedly run a measure on generated dataset.

    A measure is ran multiple times on datasets yielded by a custom generator.
    Results of all measure runs are stacked and returned as a dataset upon call.
    """

    repetition_results = ConditionalAttribute(enabled=False, doc=
       """Store individual result datasets for each repetition""")
    stats = ConditionalAttribute(enabled=False, doc=
       """Summary statistics about the node performance across all repetitions
       """)
    datasets = ConditionalAttribute(enabled=False, doc=
       """Store generated datasets for all repetitions. Can be memory expensive
       """)

    is_trained = True
    """Indicate that this measure is always trained."""

    def __init__(self,
                 node,
                 generator,
                 callback=None,
                 concat_as='samples',
                 **kwargs):
        """
        Parameters
        ----------
        node : Node
          Node or Measure implementing the procedure that is supposed to be run
          multiple times.
        generator : Node
          Generator to yield a dataset for each measure run. The number of
          datasets returned by the node determines the number of runs.
        callback : functor
          Optional callback to extract information from inside the main loop of
          the measure. The callback is called with the input 'data', the 'node'
          instance that is evaluated repeatedly and the 'result' of a single
          evaluation -- passed as named arguments (see labels in quotes) for
          every iteration, directly after evaluating the node.
        concat_as : {'samples', 'features'}
          Along which axis to concatenate result dataset from all iterations.
          By default, results are 'vstacked' as multiple samples in the output
          dataset. Setting this argument to 'features' will change this to
          'hstacking' along the feature axis.
        """
        Measure.__init__(self, **kwargs)

        self._node = node
        self._generator = generator
        self._callback = callback
        self._concat_as = concat_as

    def __repr__(self, prefixes=[], exclude=[]):
        return super(RepeatedMeasure, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, [x for x in ['node', 'generator', 'callback']
                                 if not x in exclude])
            + _repr_attrs(self, ['concat_as'], default='samples')
            )


    def _call(self, ds):
        # local binding
        generator = self._generator
        node = self._node
        ca = self.ca
        space = self.get_space()
        concat_as = self._concat_as

        if self.ca.is_enabled("stats") and (not node.ca.has_key("stats") or
                                            not node.ca.is_enabled("stats")):
            warning("'stats' conditional attribute was enabled, but "
                    "the assigned node '%s' either doesn't support it, "
                    "or it is disabled" % node)
        # precharge conditional attributes
        ca.datasets = []

        # run the node an all generated datasets
        results = []
        for i, sds in enumerate(generator.generate(ds)):
            if __debug__:
                debug('REPM', "%d-th iteration of %s on %s",
                      (i, self, sds))
            if ca.is_enabled("datasets"):
                # store dataset in ca
                ca.datasets.append(sds)
            # run the beast
            result = node(sds)
            # callback
            if not self._callback is None:
                self._callback(data=sds, node=node, result=result)
            # subclass postprocessing
            result = self._repetition_postcall(sds, node, result)
            if space:
                # XXX maybe try to get something more informative from the
                # processing node (e.g. in 0.5 it used to be 'chunks'->'chunks'
                # to indicate what was trained and what was tested. Now it is
                # more tricky, because `node` could be anything
                result.set_attr(space, (i,))
            # store
            results.append(result)

            if ca.is_enabled("stats") and node.ca.has_key("stats") \
               and node.ca.is_enabled("stats"):
                if not ca.is_set('stats'):
                    # create empty stats container of matching type
                    ca.stats = node.ca['stats'].value.__class__()
                # harvest summary stats
                ca['stats'].value.__iadd__(node.ca['stats'].value)

        # charge condition attribute
        self.ca.repetition_results = results

        # stack all results into a single Dataset
        if concat_as == 'samples':
            results = vstack(results, True)
        elif concat_as == 'features':
            results = hstack(results, True)
        else:
            raise ValueError("Unkown concatenation mode '%s'" % concat_as)
        # no need to store the raw results, since the Measure class will
        # automatically store them in a CA
        return results


    def _repetition_postcall(self, ds, node, result):
        """Post-processing handler for each repetition.

        Maybe overwritten in subclasses to harvest additional data.

        Parameters
        ----------
        ds : Dataset
          Input dataset for the node for this repetition
        node : Node
          Node after having processed the input dataset
        result : Dataset
          Output dataset of the node for this repetition.

        Returns
        -------
        dataset
          The result dataset.
        """
        return result


    def _untrain(self):
        """Untrain this measure and the embedded node."""
        self._node.untrain()
        super(RepeatedMeasure, self)._untrain()


    node = property(fget=lambda self: self._node)
    generator = property(fget=lambda self: self._generator)
    callback = property(fget=lambda self: self._callback)
    concat_as = property(fget=lambda self: self._concat_as)


class CrossValidation(RepeatedMeasure):
    """Cross-validate a learner's transfer on datasets.

    A generator is used to resample a dataset into multiple instances (e.g.
    sets of dataset partitions for leave-one-out folding). For each dataset
    instance a transfer measure is computed by splitting the dataset into
    two parts (defined by the dataset generators output space) and train a
    custom learner on the first part and run it on the next. An arbitray error
    function can by used to determine the learner's error when prediction the
    dataset part that has been unseen during training.
    """

    training_stats = ConditionalAttribute(enabled=False, doc=
       """Summary statistics about the training status of the learner
       across all cross-validation fold.""")

    # TODO move conditional attributes from CVTE into this guy
    def __init__(self, learner, generator, errorfx=mean_mismatch_error,
                 splitter=None, **kwargs):
        """
        Parameters
        ----------
        learner : Learner
          Any trainable node that shall be run on the dataset folds.
        generator : Node
          Generator used to resample the input dataset into multiple instances
          (i.e. partitioning it). The number of datasets yielded by this
          generator determines the number of cross-validation folds.
          IMPORTANT: The ``space`` of this generator determines the attribute
          that will be used to split all generated datasets into training and
          testing sets.
        errorfx : Node or callable
          Custom implementation of an error function. The callable needs to
          accept two arguments (1. predicted values, 2. target values).  If not
          a Node, it gets wrapped into a `BinaryFxNode`.
        splitter : Splitter or None
          A Splitter instance to split the dataset into training and testing
          part. The first split will be used for training and the second for
          testing -- all other splits will be ignored. If None, a default
          splitter is auto-generated using the ``space`` setting of the
          ``generator``. The default splitter is configured to return the
          ``1``-labeled partition of the input dataset at first, and the
          ``2``-labeled partition second. This behavior corresponds to most
          Partitioners that label the taken-out portion ``2`` and the remainder
          with ``1``.
        """
        # compile the appropriate repeated measure to do cross-validation from
        # pieces
        if not errorfx is None:
            # error node -- postproc of transfer measure
            if isinstance(errorfx, Node):
                enode = errorfx
            else:
                # wrap into BinaryFxNode
                enode = BinaryFxNode(errorfx, learner.get_space())
        else:
            enode = None

        if splitter is None:
            # default splitter splits into "1" and "2" partition.
            # that will effectively ignore 'deselected' samples (e.g. by
            # Balancer). It is done this way (and not by ignoring '0' samples
            # because it is guaranteed to yield two splits) and is more likely
            # to fail in visible ways if the attribute does not have 0,1,2
            # values at all (i.e. a literal train/test/spareforlater attribute)
            splitter = Splitter(generator.get_space(), attr_values=(1, 2))
        # transfer measure to wrap the learner
        # splitter used the output space of the generator to know what to split
        tm = TransferMeasure(learner, splitter, postproc=enode)

        space = kwargs.pop('space', 'sa.cvfolds')
        # and finally the repeated measure to perform the x-val
        RepeatedMeasure.__init__(self, tm, generator, space=space,
                                 **kwargs)

        for ca in ['stats', 'training_stats']:
            if self.ca.is_enabled(ca):
                # enforce ca if requested
                tm.ca.enable(ca)
        if self.ca.is_enabled('training_stats'):
            # also enable training stats in the learner
            learner.ca.enable('training_stats')

    def __repr__(self, prefixes=[]):
        return super(CrossValidation, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['learner', 'splitter'])
            + _repr_attrs(self, ['errorfx'], default=mean_mismatch_error)
            + _repr_attrs(self, ['space'], default='sa.cvfolds'),
            # Since it is the constructor which generates and passes
            # node=TransferMeasure, it must not be present in __repr__ of CV
            # TODO: clear up hierarchy
            exclude=('node',)
            )


    def _call(self, ds):
        # always untrain to wipe out previous stats
        self.untrain()
        return super(CrossValidation, self)._call(ds)


    def _repetition_postcall(self, ds, node, result):
        # local binding
        ca = self.ca
        if ca.is_enabled("training_stats"):
            if not ca.is_set('training_stats'):
                # create empty stats container of matching type
                ca.training_stats = node.ca['training_stats'].value.__class__()
            # harvest summary stats
            training_stats = node.ca['training_stats'].value
            if isinstance(training_stats, dict):
                # if it was a dictionary of results - we should collect them per item
                for k,v in training_stats.iteritems():
                    if not len(ca['training_stats'].value) or k not in ca['training_stats'].value:
                        ca['training_stats'].value[k] = v
                    else:
                        ca['training_stats'].value[k].__iadd__(v)
            else:
                ca['training_stats'].value.__iadd__(node.ca['training_stats'].value)

        return result


    transfermeasure = property(fget=lambda self:self._node)

    # XXX Well, those properties are defined to match available
    # attributes to constructor arguments.  Unfortunately our
    # hierarchy/API is not ideal at this point
    learner = property(fget=lambda self: self.transfermeasure.measure)
    splitter = property(fget=lambda self: self.transfermeasure.splitter)
    errorfx = property(fget=lambda self: self.transfermeasure.postproc)


class TransferMeasure(Measure):
    """Train and run a measure on two different parts of a dataset.

    Upon calling a TransferMeasure instance with a dataset the input dataset
    is passed to a `Splitter` to will generate dataset subsets. The first
    generated dataset is used to train an arbitray embedded `Measure. Once
    trained, the measure is then called with the second generated dataset
    and the result is returned.
    """

    stats = ConditionalAttribute(enabled=False, doc=
       """Optional summary statistics about the transfer performance""")
    training_stats = ConditionalAttribute(enabled=False, doc=
       """Summary statistics about the training status of the learner""")

    is_trained = True
    """Indicate that this measure is always trained."""

    def __init__(self, measure, splitter, **kwargs):
        """
        Parameters
        ----------
        measure: Measure
          This measure instance is trained on the first dataset and called with
          the second.
        splitter: Splitter
          This splitter instance has to generate at least two dataset splits
          when called with the input dataset. The first split is used to train
          the measure, the second split is used to run the trained measure.
        """
        Measure.__init__(self, **kwargs)
        self.__measure = measure
        self.__splitter = splitter


    def __repr__(self, prefixes=[]):
        return super(TransferMeasure, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['measure', 'splitter'])
            )


    def _call(self, ds):
        # local binding
        measure = self.__measure
        splitter = self.__splitter
        ca = self.ca
        space = self.get_space()

        # generate the training and testing dataset subsequently to reduce the
        # memory footprint, i.e. the splitter might generate copies of the data
        # and no creates one at a time instead of two (for train and test) at
        # once
        # activate the dataset splitter
        dsgen = splitter.generate(ds)
        dstrain = dsgen.next()

        if not len(dstrain):
            raise ValueError(
                "Got empty training dataset from splitting in TransferMeasure. "
                "Unique values of input split attribute are: %s)" \
                % (ds.sa[splitter.get_space()].unique))

        if space:
            # get unique chunks for training set
            train_chunks = ','.join([str(i)
                    for i in dstrain.get_attr(splitter.get_space())[0].unique])
        # ask splitter for first part
        measure.train(dstrain)
        # cleanup to free memory
        del dstrain

        # TODO get training confusion/stats

        # run with second
        dstest = dsgen.next()
        if not len(dstest):
            raise ValueError(
                "Got empty testing dataset from splitting in TransferMeasure. "
                "Unique values of input split attribute are: %s)" \
                % (ds.sa[splitter.get_space()].unique))
        if space:
            # get unique chunks for testing set
            test_chunks = ','.join([str(i)
                    for i in dstest.get_attr(splitter.get_space())[0].unique])
        res = measure(dstest)
        if space:
            # will broadcast to desired length
            res.set_attr(space, ("%s->%s" % (train_chunks, test_chunks),))
        # cleanup to free memory
        del dstest

        # compute measure stats
        if ca.is_enabled('stats'):
            if not hasattr(measure, '__summary_class__'):
                warning('%s has no __summary_class__ attribute -- '
                        'necessary for computing transfer stats' % measure)
            else:
                stats = measure.__summary_class__(
                    # hmm, might be unsupervised, i.e no targets...
                    targets=res.sa[measure.get_space()].value,
                    # XXX this should really accept the full dataset
                    predictions=res.samples[:, 0],
                    estimates=measure.ca.get('estimates', None))
                ca.stats = stats
        if ca.is_enabled('training_stats'):
            if measure.ca.has_key("training_stats") \
               and measure.ca.is_enabled("training_stats"):
                ca.training_stats = measure.ca.training_stats
            else:
                warning("'training_stats' conditional attribute was enabled, "
                        "but the assigned measure '%s' either doesn't support "
                        "it, or it is disabled" % measure)
        return res

    measure = property(fget=lambda self:self.__measure)
    splitter = property(fget=lambda self:self.__splitter)



class FeaturewiseMeasure(Measure):
    """A per-feature-measure computed from a `Dataset` (base class).

    Should behave like a Measure.
    """

    def _postcall(self, dataset, result):
        """Adjusts per-feature-measure for computed `result`
        """
        # This method get the 'result' either as a 1D array, or as a Dataset
        # everything else is illegal.


        if not (len(result.shape) == 1 or isinstance(result, AttrDataset)):
            raise RuntimeError("FeaturewiseMeasures have to return "
                               "their results as 1D array, or as a Dataset "
                               "(error made by: '%s')." % repr(self))

        return Measure._postcall(self, dataset, result)


class StaticMeasure(Measure):
    """A static (assigned) sensitivity measure.

    Since implementation is generic it might be per feature or
    per whole dataset
    """

    def __init__(self, measure=None, bias=None, *args, **kwargs):
        """Initialize.

        Parameters
        ----------
        measure
           actual sensitivity to be returned
        bias
           optionally available bias
        """
        Measure.__init__(self, *args, **kwargs)
        if measure is None:
            raise ValueError, "Sensitivity measure has to be provided"
        self.__measure = measure
        self.__bias = bias

    def __repr__(self, prefixes=[]):
        return super(StaticMeasure, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['measure', 'bias'])
            )

    def _call(self, dataset):
        """Returns assigned sensitivity
        """
        return self.__measure

    #XXX Might need to move into ConditionalAttribute?
    measure = property(fget=lambda self:self.__measure)
    bias = property(fget=lambda self:self.__bias)



def _dont_force_slaves(slave_kwargs={}):
    """Helper to reset force_train in sensitivities with slaves
    """
    # We should not (or even must not in case of SplitCLF) force
    # training of slave analyzers since they would be trained
    # anyways by the Boosted analyzer's train
    # TODO: consider at least a warning whenever it is provided
    # and is True
    slave_kwargs = slave_kwargs or {}   # make new instance of default empty one
    slave_kwargs['force_train'] = slave_kwargs.get('force_train', False)
    return slave_kwargs

#
# Flavored implementations of FeaturewiseMeasures

class Sensitivity(FeaturewiseMeasure):
    """Sensitivities of features for a given Classifier.

    """

    _LEGAL_CLFS = []
    """If Sensitivity is classifier specific, classes of classifiers
    should be listed in the list
    """

    def __init__(self, clf, force_train=True, **kwargs):
        """Initialize the analyzer with the classifier it shall use.

        Parameters
        ----------
        clf : `Classifier`
          classifier to use.
        force_train : bool
          Flag whether the learner will enforce training on the input dataset
          upon every call.
        """

        """Does nothing special."""
        # by default auto train
        kwargs['auto_train'] = kwargs.get('auto_train', True)
        FeaturewiseMeasure.__init__(self, force_train=force_train, **kwargs)

        _LEGAL_CLFS = self._LEGAL_CLFS
        if len(_LEGAL_CLFS) > 0:
            found = False
            for clf_class in _LEGAL_CLFS:
                if isinstance(clf, clf_class):
                    found = True
                    break
            if not found:
                raise ValueError, \
                  "Classifier %s has to be of allowed class (%s), but is %r" \
                  % (clf, _LEGAL_CLFS, type(clf))

        self.__clf = clf
        """Classifier used to computed sensitivity"""


    def __repr__(self, prefixes=[]):
        return super(Sensitivity, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['clf'])
            + _repr_attrs(self, ['force_train'], default=True)
            )


    @property
    def is_trained(self):
        return self.__clf.trained

    #    """Train classifier on `dataset` and then compute actual sensitivity.

    #    If the classifier is already trained it is possible to extract the
    #    sensitivities without passing a dataset.
    #    """
    #    # local bindings
    #    clf = self.__clf
    #    if clf.trained:
    #        self._set_trained()
    #    elif self._force_training:
    #        if dataset is None:
    #            raise ValueError, \
    #                  "Training classifier to compute sensitivities requires " \
    #                  "a dataset."
    #        self.train(dataset)

    #    return FeaturewiseMeasure.__call__(self, dataset)


    def _set_classifier(self, clf):
        self.__clf = clf


    def _train(self, dataset):
        clf = self.__clf
        if __debug__:
            debug("SA", "Training classifier %s on %s %s",
                  (clf,
                   dataset,
                   {False: "since it wasn't yet trained",
                    True:  "although it was trained previously"}
                   [clf.trained]))
        return clf.train(dataset)


    def _untrain(self):
        """Untrain corresponding classifier for Sensitivity
        """
        if self.__clf is not None:
            self.__clf.untrain()
        super(Sensitivity, self)._untrain()


    @property
    def feature_ids(self):
        """Return feature_ids used by the underlying classifier
        """
        return self.__clf._get_feature_ids()


    clf = property(fget=lambda self:self.__clf,
                   fset=_set_classifier)



class CombinedFeaturewiseMeasure(FeaturewiseMeasure):
    """Set sensitivity analyzers to be merged into a single output"""

    sensitivities = ConditionalAttribute(enabled=False,
        doc="Sensitivities produced by each analyzer")

    # XXX think again about combiners... now we have it in here and as
    #     well as in the parent -- FeaturewiseMeasure
    # YYY because we don't use parent's _call. Needs RF
    def __init__(self, analyzers=None, # XXX should become actually 'measures'
                 sa_attr='combinations',
                 **kwargs):
        """Initialize CombinedFeaturewiseMeasure

        Parameters
        ----------
        analyzers : list or None
          List of analyzers to be used. There is no logic to populate
          such a list in __call__, so it must be either provided to
          the constructor or assigned to .analyzers prior calling
        sa_attr : str
          Name of the sa to be populated with the indexes of combinations
        """
        if analyzers is None:
            analyzers = []
        self._sa_attr = sa_attr
        FeaturewiseMeasure.__init__(self, **kwargs)
        self.__analyzers = analyzers
        """List of analyzers to use"""


    def __repr__(self, prefixes=[]):
        return super(CombinedFeaturewiseMeasure, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['analyzers'])
            + _repr_attrs(self, ['sa_attr'], default='combinations')
            )

    def _call(self, dataset):
        sensitivities = []
        for ind, analyzer in enumerate(self.__analyzers):
            if __debug__:
                debug("SA", "Computing sensitivity for SA#%d:%s" %
                      (ind, analyzer))
            sensitivity = analyzer(dataset)
            sensitivities.append(sensitivity)

        if __debug__:
            debug("SA",
                  "Returning %d sensitivities from %s" %
                  (len(sensitivities), self.__class__.__name__))

        sa_attr = self._sa_attr
        if isinstance(sensitivities[0], AttrDataset):
            smerged = []
            for i, s in enumerate(sensitivities):
                s.sa[sa_attr] = np.repeat(i, len(s))
                smerged.append(s)
            sensitivities = vstack(smerged)
        else:
            sensitivities = \
                Dataset(sensitivities,
                        sa={sa_attr: np.arange(len(sensitivities))})

        self.ca.sensitivities = sensitivities

        return sensitivities


    def _untrain(self):
        """Untrain CombinedFDM
        """
        if self.__analyzers is not None:
            for anal in self.__analyzers:
                anal.untrain()
        super(CombinedFeaturewiseMeasure, self)._untrain()


    ##REF: Name was automagically refactored
    def _set_analyzers(self, analyzers):
        """Set the analyzers
        """
        self.__analyzers = analyzers
        """Analyzers to use"""

    analyzers = property(fget=lambda x:x.__analyzers,
                         fset=_set_analyzers,
                         doc="Used analyzers")



class BoostedClassifierSensitivityAnalyzer(Sensitivity):
    """Set sensitivity analyzers to be merged into a single output"""


    # XXX we might like to pass parameters also for combined_analyzer
    @group_kwargs(prefixes=['slave_'], assign=True)
    def __init__(self,
                 clf,
                 analyzer=None,
                 combined_analyzer=None,
                 sa_attr='lrn_index',
                 **kwargs):
        """Initialize Sensitivity Analyzer for `BoostedClassifier`

        Parameters
        ----------
        clf : `BoostedClassifier`
          Classifier to be used
        analyzer : analyzer
          Is used to populate combined_analyzer
        sa_attr : str
          Name of the sa to be populated with the indexes of learners
          (passed to CombinedFeaturewiseMeasure is None is
          given in `combined_analyzer`)
        slave_*
          Arguments to pass to created analyzer if analyzer is None
        """
        Sensitivity.__init__(self, clf, **kwargs)

        if analyzer is not None and len(self._slave_kwargs):
            raise ValueError, \
                  "Provide either analyzer of slave_* arguments, not both"

        # Do not force_train slave sensitivity since the dataset might
        # be inappropriate -- rely on the classifier being trained by
        # the extraction by the meta classifier itself
        self._slave_kwargs = _dont_force_slaves(self._slave_kwargs)

        if combined_analyzer is None:
            # sanitarize kwargs
            kwargs.pop('force_train', None)
            combined_analyzer = CombinedFeaturewiseMeasure(sa_attr=sa_attr,
                                                                  **kwargs)
        self.__combined_analyzer = combined_analyzer
        """Combined analyzer to use"""

        self.__analyzer = analyzer
        """Analyzer to use for basic classifiers within boosted classifier"""


    ## def __repr__(self, prefixes=[]):
    ##     return super(BoostedClassifierSensitivityAnalyzer, self).__repr__(
    ##         prefixes=prefixes
    ##         + _repr_attrs(self, ['clf', 'analyzer', 'combined_analyzer'])
    ##         + _repr_attrs(self, ['sa_attr'], default='combinations')
    ##         )


    def _untrain(self):
        """Untrain BoostedClassifierSensitivityAnalyzer
        """
        if self.__analyzer is not None:
            self.__analyzer.untrain()
        self.__combined_analyzer.untrain()
        super(BoostedClassifierSensitivityAnalyzer, self)._untrain()


    def _call(self, dataset):
        analyzers = []
        # create analyzers
        for clf in self.clf.clfs:
            if self.__analyzer is None:
                analyzer = clf.get_sensitivity_analyzer(**(self._slave_kwargs))
                if analyzer is None:
                    raise ValueError, \
                          "Wasn't able to figure basic analyzer for clf %r" % \
                          (clf,)
                if __debug__:
                    debug("SA", "Selected analyzer %r for clf %r" % \
                          (analyzer, clf))
            else:
                # XXX shallow copy should be enough...
                analyzer = copy.copy(self.__analyzer)

            # assign corresponding classifier
            analyzer.clf = clf
            # if clf was trained already - don't train again
            if clf.trained:
                analyzer._force_train = False
            analyzers.append(analyzer)

        self.__combined_analyzer.analyzers = analyzers

        # XXX not sure if we don't want to call directly ._call(dataset) to avoid
        # double application of transformers/combiners, after all we are just
        # 'proxying' here to combined_analyzer...
        # YOH: decided -- lets call ._call
        return self.__combined_analyzer._call(dataset)

    combined_analyzer = property(fget=lambda x:x.__combined_analyzer)


class ProxyClassifierSensitivityAnalyzer(Sensitivity):
    """Set sensitivity analyzer output just to pass through"""

    clf_sensitivities = ConditionalAttribute(enabled=False,
        doc="Stores sensitivities of the proxied classifier")


    @group_kwargs(prefixes=['slave_'], assign=True)
    def __init__(self,
                 clf,
                 analyzer=None,
                 **kwargs):
        """Initialize Sensitivity Analyzer for `BoostedClassifier`
        """
        Sensitivity.__init__(self, clf, **kwargs)
        # _slave_kwargs is assigned due to assign=True in @group_kwargs
        if analyzer is not None and len(self._slave_kwargs):
            raise ValueError, \
                  "Provide either analyzer of slave_* arguments, not both"

        # Do not force_train slave sensitivity since the dataset might
        # be inappropriate -- rely on the classifier being trained by
        # the extraction by the meta classifier itself
        self._slave_kwargs = _dont_force_slaves(self._slave_kwargs)

        self.__analyzer = analyzer
        """Analyzer to use for basic classifiers within boosted classifier"""


    def _untrain(self):
        super(ProxyClassifierSensitivityAnalyzer, self)._untrain()
        if self.__analyzer is not None:
            self.__analyzer.untrain()


    def _call(self, dataset):
        # OPT: local bindings
        clfclf = self.clf.clf
        analyzer = self.__analyzer

        if analyzer is None:
            analyzer = clfclf.get_sensitivity_analyzer(
                **(self._slave_kwargs))
            if analyzer is None:
                raise ValueError, \
                      "Wasn't able to figure basic analyzer for clf %s" % \
                      `clfclf`
            if __debug__:
                debug("SA", "Selected analyzer %s for clf %s" % \
                      (analyzer, clfclf))
            # bind to the instance finally
            self.__analyzer = analyzer

        # TODO "remove" unnecessary things below on each call...
        # assign corresponding classifier
        analyzer.clf = clfclf

        # if clf was trained already - don't train again
        if clfclf.trained:
            analyzer._force_train = False

        result = analyzer._call(dataset)
        self.ca.clf_sensitivities = result

        return result

    analyzer = property(fget=lambda x:x.__analyzer)


class BinaryClassifierSensitivityAnalyzer(ProxyClassifierSensitivityAnalyzer):
    """Set sensitivity analyzer output to have proper labels"""

    def _call(self, dataset):
        sens = super(self.__class__, self)._call(dataset)
        clf = self.clf
        targets_attr = clf.get_space()
        if targets_attr in sens.sa:
            # if labels are present -- transform them into meaningful tuples
            # (or not if just a single beast)
            am = AttributeMap(dict([(l, -1) for l in clf.neglabels] +
                                   [(l, +1) for l in clf.poslabels]))

            # XXX here we still can get a sensitivity per each label
            # (e.g. with SMLR as the slave clf), so I guess we should
            # tune up Multiclass...Analyzer to add an additional sa
            # And here we might need to check if asobjarray call is necessary
            # and should be actually done
            #asobjarray(
            sens.sa[targets_attr] = \
                am.to_literal(sens.sa[targets_attr].value, recurse=True)
        return sens


class RegressionAsClassifierSensitivityAnalyzer(ProxyClassifierSensitivityAnalyzer):
    """Set sensitivity analyzer output to have proper labels"""

    def _call(self, dataset):
        sens = super(RegressionAsClassifierSensitivityAnalyzer,
                     self)._call(dataset)
        # We can have only a single sensitivity out of regression
        assert(sens.shape[0] == 1)
        clf = self.clf
        targets_attr = clf.get_space()
        if targets_attr not in sens.sa:
            # We just assign a tuple of all labels sorted
            labels = tuple(sorted(clf._trained_attrmap.values()))
            if len(clf._trained_attrmap):
                labels = clf._trained_attrmap.to_literal(labels, recurse=True)
            sens.sa[targets_attr] = asobjarray([labels])
        return sens


class FeatureSelectionClassifierSensitivityAnalyzer(ProxyClassifierSensitivityAnalyzer):
    pass

class MappedClassifierSensitivityAnalyzer(ProxyClassifierSensitivityAnalyzer):
    """Set sensitivity analyzer output be reverse mapped using mapper of the
    slave classifier"""

    def _call(self, dataset):
        # incoming dataset need to be forward mapped
        dataset_mapped = self.clf.mapper(dataset)
        if __debug__:
            debug('SA', 'Mapped incoming dataset %s to %s'
                        % (dataset_mapped, dataset))
        sens = super(MappedClassifierSensitivityAnalyzer,
                     self)._call(dataset_mapped)
        return self.clf.mapper.reverse(sens)


    def __str__(self):
        return _str(self, str(self.clf))



########NEW FILE########
__FILENAME__ = corrcoef
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""FeaturewiseMeasure of correlation with the labels."""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals

import numpy as np

from mvpa2.measures.base import FeaturewiseMeasure
from mvpa2.datasets.base import Dataset

class CorrCoef(FeaturewiseMeasure):
    """`FeaturewiseMeasure` that performs correlation with labels

    XXX: Explain me!
    """
    is_trained = True
    """Indicate that this measure is always trained."""

    def __init__(self, pvalue=False, attr='targets',
                        corr_backend=None, **kwargs):
        """Initialize

        Parameters
        ----------
        pvalue : bool
          Either to report p-value of the Pearson's correlation coefficient
          instead of pure correlation coefficient
        attr : str
          What attribut to correlate with
        corr_backend: None or 'builtin' or 'scipy' (default: None)
          Which function to use to compute correlations.
          None means 'scipy' if pvalue else 'builtin'.
        """
        # init base classes first

        FeaturewiseMeasure.__init__(self, **kwargs)

        self.__pvalue = int(pvalue)
        self.__attr = attr
        self.__corr_backend = corr_backend


    def _call(self, dataset):
        """Computes featurewise scores."""
        backend = self.__corr_backend

        if backend is None:
            # if p values needed, use scipy
            # otherwise
            backend = ['builtin', 'scipy'][self.__pvalue]

        if backend == 'builtin':
            if self.__pvalue:
                raise ValueError("Not supported: 'builtin' and pvalue=True")
            pearsonr = lambda x, y:(pearson_correlation(x, y),)
        elif self.__corr_backend == 'scipy':
            if externals.exists('scipy', raise_=True):
            # TODO: implement corrcoef optionally without scipy, e.g. np.corrcoef
                from scipy.stats import pearsonr

        attrdata = dataset.sa[self.__attr].value
        if (np.issubdtype(attrdata.dtype, 'c') or
             np.issubdtype(attrdata.dtype, 'U')):
            raise ValueError("Correlation coefficent measure is not meaningful "
                             "for datasets with literal labels.")

        samples = dataset.samples
        pvalue_index = self.__pvalue
        result = np.empty((dataset.nfeatures,), dtype=float)

        for ifeature in xrange(dataset.nfeatures):
            samples_ = samples[:, ifeature]
            corr = pearsonr(samples_, attrdata)
            corrv = corr[pvalue_index]
            # Should be safe to assume 0 corr_coef (or 1 pvalue) if value
            # is actually NaN, although it might not be the case (covar of
            # 2 constants would be NaN although should be 1)
            if np.isnan(corrv):
                if np.var(samples_) == 0.0 and np.var(attrdata) == 0.0 \
                   and len(samples_):
                    # constant terms
                    corrv = 1.0 - pvalue_index
                else:
                    corrv = pvalue_index
            result[ifeature] = corrv

        return Dataset(result[np.newaxis])

def pearson_correlation(x, y=None):
    '''Computes pearson correlations on matrices

    Parameters
    ----------
    x: np.ndarray or Dataset
        PxM array
    y: np.ndarray or Dataset or None (the default).
        PxN array. If None, then y=x.

    Returns
    -------
    c: np.ndarray
        MxN array with c[i,j]=r(x[:,i],y[:,j])

    Notes
    -----
    Unlike numpy. this function behaves like matlab's 'corr' function.
    Its numerical precision is slightly lower than numpy's correlate function.
    Unlike scipy's 'pearsonr' function it does not return p values.
    TODO integrate with CorrCoef

    '''

    if y is None:
        y = x

    def _get_data(ds):
        # support for dataset
        try:
            return ds.samples
        except:
            return ds

    x = _get_data(x)
    y = _get_data(y)


    xd = x - np.mean(x, axis=0)
    yd = y - np.mean(y, axis=0)

    if xd.shape[0] != yd.shape[0]:
        raise ValueError("Shape mismatch: %s != %s" % (xd.shape, yd.shape))

    # normalize
    n = 1. / (x.shape[0] - 1) # normalize

    # standard deviation
    xs = (n * np.sum(xd * xd, axis=0)) ** -.5
    ys = (n * np.sum(yd * yd, axis=0)) ** -.5

    return n * np.dot(xd.T, yd) * np.tensordot(xs, ys, 0)


########NEW FILE########
__FILENAME__ = corrstability
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Stability of labels across chunks based on correlation."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.measures.base import FeaturewiseMeasure

class CorrStability(FeaturewiseMeasure):
    """Correlation of a feature values per each target across chunks.

    It will assesses feature stability across runs for each unique label by
    correlating feature values across all labels for pairwise combinations of
    the chunks.

    If there are multiple samples with the same label in a single
    chunk (as is typically the case) this algorithm will take the
    featurewise average of the sample activations to get a single
    value per label, per chunk.

    """

    is_trained = True

    def __init__(self, space='targets', **kwargs):
        """Initialize

        Parameters
        ----------
        space : str
          What samples attribute to use as targets (labels).
        """
        # init base classes first
        FeaturewiseMeasure.__init__(self, space=space, **kwargs)

    def _call(self, dataset):
        """Computes feature-wise scores."""

        # get the attributes (usually the labels) and the samples
        attrdata = dataset.sa[self.space].value
        samples = dataset.samples

        # take mean within chunks
        dat = []
        labels = []
        chunks = []
        for c in dataset.uniquechunks:
            for l in np.unique(attrdata):
                ind = (dataset.chunks==c)&(attrdata==l)
                if ind.sum() == 0:
                    # no instances, so skip
                    continue
                # append the mean, and the label/chunk info
                dat.append(np.mean(samples[ind, :], axis=0))
                labels.append(l)
                chunks.append(c)

        # convert to arrays
        dat = np.asarray(dat)
        labels = np.asarray(labels)
        chunks = np.asarray(chunks)

        # get indices for correlation (all pairwise values across
        # chunks)
        ind1 = []
        ind2 = []
        for i,c1 in enumerate(np.unique(chunks)[:-1]):
            for c2 in np.unique(chunks)[i+1:]:
                for l in np.unique(labels):
                    v1 = np.where((chunks==c1)&(labels==l))[0]
                    v2 = np.where((chunks==c2)&(labels==l))[0]
                    if labels[v1] == labels[v2]:
                        # the labels match, so add them
                        ind1.extend(v1)
                        ind2.extend(v2)
        
        # convert the indices to arrays
        ind1 = np.asarray(ind1)
        ind2 = np.asarray(ind2)

        # remove the mean from the datasets
        dat1 = dat[ind1] - dat[ind1].mean(0)[np.newaxis]
        dat2 = dat[ind2] - dat[ind2].mean(0)[np.newaxis]

        # calculate the correlation from the covariance and std
        covar = (dat1*dat2).mean(0) / (dat1.std(0) * dat2.std(0))
        covar[np.isnan(covar)] = 0.0 # reset nan's to 0s
        return covar

########NEW FILE########
__FILENAME__ = gnbsearchlight
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""An efficient implementation of searchlight for GNB.
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.dochelpers import borrowkwargs, _repr_attrs
from mvpa2.misc.neighborhood import IndexQueryEngine, Sphere

from mvpa2.measures.adhocsearchlightbase import \
     SimpleStatBaseSearchlight, _STATS

if __debug__:
    from mvpa2.base import debug
    import time as time

__all__ = [ "GNBSearchlight", 'sphere_gnbsearchlight' ]

class GNBSearchlight(SimpleStatBaseSearchlight):
    """Efficient implementation of Gaussian Naive Bayes `Searchlight`.

    This implementation takes advantage that :class:`~mvpa2.clfs.gnb.GNB` is
    "naive" in its reliance on massive univariate conditional
    probabilities of each feature given a target class.  Plain
    :class:`~mvpa2.measures.searchlight.Searchlight` analysis approach
    asks for the same information over again and over again for
    the same feature in multiple "lights".  So it becomes possible to
    drastically cut running time of a Searchlight by pre-computing basic
    statistics necessary used by GNB beforehand and then doing their
    subselection for a given split/feature set.

    Kudos for the idea and showing that it indeed might be beneficial
    over generic Searchlight with GNB go to Francisco Pereira.
    """

    @borrowkwargs(SimpleStatBaseSearchlight, '__init__')
    def __init__(self, gnb, generator, qe, **kwargs):
        """Initialize a GNBSearchlight

        Parameters
        ----------
        gnb : `GNB`
          `GNB` classifier as the specification of what GNB parameters
          to use. Instance itself isn't used.
        """

        # init base class first
        SimpleStatBaseSearchlight.__init__(self, generator, qe, **kwargs)

        self._gnb = gnb
        self.__pl_train = None


    def __repr__(self, prefixes=[]):
        return super(GNBSearchlight, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['gnb'])
            )


    def _get_space(self):
        return self.gnb.get_space()

    def _untrain(self):
        super(GNBSearchlight, self)._untrain()
        self.__pl_train = None

    def _reserve_pl_stats_space(self, shape):
        # per each label: to be (re)computed within each loop split
        # Let's try to reuse the memory though
        pl = self.__pl_train = _STATS()
        pl.sums = np.zeros(shape)
        pl.means = np.zeros(shape)
        # means of squares for stddev computation
        pl.sums2 = np.zeros(shape)
        pl.variances = np.zeros(shape)
        # degenerate dimension are added for easy broadcasting later on
        pl.nsamples = np.zeros(shape[:1] + (1,)*(len(shape)-1))


    def _sl_call_on_a_split(self,
                            split, X,
                            training_sis, testing_sis,
                            nroi_fids, roi_fids,
                            indexsum_fx,
                            labels_numeric,
                            ):
        """Call to GNBSearchlight
        """
        # Local bindings
        gnb = self.gnb
        params = gnb.params

        pl = self.__pl_train # we want to reuse the same storage across
                             # splits

        training_nsamples, non0labels = \
            self._compute_pl_stats(training_sis, pl)

        nlabels = len(pl.nsamples)

        if params.common_variance:
            pl.variances[:] = \
                np.sum(pl.sums2 - pl.sums * pl.means, axis=0) \
                / training_nsamples
        else:
            pl.variances[non0labels] = \
                (pl.sums2 - pl.sums * pl.means)[non0labels] \
                / pl.nsamples[non0labels]

        # assign priors
        priors = gnb._get_priors(
            nlabels, training_nsamples, pl.nsamples)

        # proceed in a way we have in GNB code with logprob=True,
        # i.e. operating within the exponents -- should lead to some
        # performance advantage
        norm_weight = -0.5 * np.log(2*np.pi*pl.variances)
        # last added dimension would be for ROIs
        logpriors = np.log(priors[:, np.newaxis, np.newaxis])

        if __debug__:
            debug('SLC', "  'Training' is done")

        # Now it is time to "classify" our samples.
        # and for that we first need to compute corresponding
        # probabilities (or may be un
        data = X[split[1].samples[:, 0]]

        # argument of exponentiation
        scaled_distances = \
             -0.5 * (((data - pl.means[:, np.newaxis, ...])**2) \
                     / pl.variances[:, np.newaxis, ...])

        # incorporate the normalization from normals
        lprob_csfs = norm_weight[:, np.newaxis, ...] + scaled_distances

        ## First we need to reshape to get class x samples x features
        lprob_csf = lprob_csfs.reshape(lprob_csfs.shape[:2] + (-1,))

        ## Now we come to naive part which requires looping
        ## through all spheres
        if __debug__:
            debug('SLC', "  Doing 'Searchlight'")
        # resultant logprobs for each class x sample x roi
        lprob_cs_sl = np.zeros(lprob_csfs.shape[:2] + (nroi_fids,))
        indexsum_fx(lprob_csf, roi_fids, out=lprob_cs_sl)

        lprob_cs_sl += logpriors
        lprob_cs_cp_sl = lprob_cs_sl
        # for each of the ROIs take the class with maximal (log)probability
        predictions = lprob_cs_cp_sl.argmax(axis=0)
        # no need to map back [self.ulabels[c] for c in winners]
        #predictions = winners

        targets = labels_numeric[testing_sis]

        return targets, predictions

    gnb = property(fget=lambda self: self._gnb)

@borrowkwargs(GNBSearchlight, '__init__', exclude=['roi_ids', 'queryengine'])
def sphere_gnbsearchlight(gnb, generator, radius=1, center_ids=None,
                          space='voxel_indices', *args, **kwargs):
    """Creates a `GNBSearchlight` to assess :term:`cross-validation`
    classification performance of GNB on all possible spheres of a
    certain size within a dataset.

    The idea of taking advantage of naiveness of GNB for the sake of
    quick searchlight-ing stems from Francisco Pereira (paper under
    review).

    Parameters
    ----------
    radius : float
      All features within this radius around the center will be part
      of a sphere.
    center_ids : list of int
      List of feature ids (not coordinates) the shall serve as sphere
      centers. By default all features will be used (it is passed
      roi_ids argument for Searchlight).
    space : str
      Name of a feature attribute of the input dataset that defines the spatial
      coordinates of all features.
    **kwargs
      In addition this class supports all keyword arguments of
      :class:`~mvpa2.measures.gnbsearchlight.GNBSearchlight`.

    Notes
    -----
    If any `BaseSearchlight` is used as `SensitivityAnalyzer` one has to make
    sure that the specified scalar `Measure` returns large
    (absolute) values for high sensitivities and small (absolute) values
    for low sensitivities. Especially when using error functions usually
    low values imply high performance and therefore high sensitivity.
    This would in turn result in sensitivity maps that have low
    (absolute) values indicating high sensitivities and this conflicts
    with the intended behavior of a `SensitivityAnalyzer`.
    """
    # build a matching query engine from the arguments
    kwa = {space: Sphere(radius)}
    qe = IndexQueryEngine(**kwa)
    # init the searchlight with the queryengine
    return GNBSearchlight(gnb, generator, qe,
                          roi_ids=center_ids, *args, **kwargs)

########NEW FILE########
__FILENAME__ = irelief
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   Copyright (c) 2008 Emanuele Olivetti <emanuele@relativita.com>
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Multivariate Iterative RELIEF

See : Y. Sun, Iterative RELIEF for Feature Weighting: Algorithms, Theories,
and Applications, IEEE Trans. on Pattern Analysis and Machine Intelligence
(TPAMI), vol. 29, no. 6, pp. 1035-1051, June 2007."""


__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.datasets import Dataset
from mvpa2.measures.base import FeaturewiseMeasure
from mvpa2.kernels.np import ExponentialKernel
from mvpa2.clfs.distance import pnorm_w

if __debug__:
    from mvpa2.base import debug


class IterativeRelief_Devel(FeaturewiseMeasure):
    """`FeaturewiseMeasure` that performs multivariate I-RELIEF
    algorithm. Batch version allowing various kernels.

    UNDER DEVELOPMENT.

    Batch I-RELIEF-2 feature weighting algorithm. Works for binary or
    multiclass class-labels. Batch version with complexity O(T*N^2*I),
    where T is the number of iterations, N the number of instances, I
    the number of features.

    See: Y. Sun, Iterative RELIEF for Feature Weighting: Algorithms,
    Theories, and Applications, IEEE Trans. on Pattern Analysis and
    Machine Intelligence (TPAMI), vol. 29, no. 6, pp. 1035-1051, June
    2007. http://plaza.ufl.edu/sunyijun/Paper/PAMI_1.pdf

    Note that current implementation allows to use only
    exponential-like kernels. Support for linear kernel will be
    added later.
    """
    is_trained = True
    """Indicate that this measure doesn't have to be trained"""

    def __init__(self, threshold = 1.0e-2, kernel = None, kernel_width = 1.0,
                 w_guess = None, **kwargs):
        """Constructor of the IRELIEF class.

        """
        # init base classes first
        FeaturewiseMeasure.__init__(self, **kwargs)

        # Threshold in W changes (stopping criterion for irelief)
        self.threshold = threshold
        if kernel == None:
            self.kernel = ExponentialKernel
        else:
            self.kernel = kernel

        self.w_guess = w_guess
        self.w = None
        self.kernel_width = kernel_width


    def compute_M_H(self, label):
        """Compute hit/miss dictionaries.

        For each instance compute the set of indices having the same
        class label and different class label.

        Note that this computation is independent of the number of
        features.
        """

        M = {}
        H = {}
        for i in range(label.size):
            M[i] = np.where(label != label[i])[0]
            tmp = (np.where(label == label[i])[0]).tolist()
            tmp.remove(i)
            # There must be at least two exampls for class label[i]
            assert(tmp != [])
            H[i] = np.array(tmp)

        return M, H


    def _call(self, dataset):
        """Computes featurewise I-RELIEF weights."""
        samples = dataset.samples
        NS, NF = samples.shape[:2]
        if self.w_guess == None:
            self.w = np.ones(NF, 'd')
        # do normalization in all cases to be safe :)
        self.w = self.w/(self.w**2).sum()

        M, H = self.compute_M_H(dataset.targets)

        while True:
            self.k = self.kernel(length_scale = self.kernel_width/self.w)
            d_w_k = self.k.computed(samples).as_raw_np()
            # set d_w_k to zero where distance=0 (i.e. kernel ==
            # 1.0), otherwise I-RELIEF could not converge.
            # XXX Note that kernel==1 for distance=0 only for
            # exponential kernels!!  IMPROVE
            d_w_k[np.abs(d_w_k-1.0) < 1.0e-15] = 0.0
            ni = np.zeros(NF, 'd')
            for n in range(NS):
                # d_w_k[n,n] could be omitted since == 0.0
                gamma_n = 1.0 - np.nan_to_num(d_w_k[n, M[n]].sum() \
                                / (d_w_k[n, :].sum()-d_w_k[n, n]))
                alpha_n = np.nan_to_num(d_w_k[n, M[n]]/(d_w_k[n, M[n]].sum()))
                beta_n = np.nan_to_num(d_w_k[n, H[n]]/(d_w_k[n, H[n]].sum()))

                m_n = (np.abs(samples[n, :] - samples[M[n], :]) \
                        * alpha_n[:, None]).sum(0)
                h_n = (np.abs(samples[n, :] - samples[H[n], :]) \
                        * beta_n[:, None]).sum(0)
                ni += gamma_n*(m_n-h_n)
            ni = ni/NS

            ni_plus = np.clip(ni, 0.0, np.inf) # set all negative elements to zero
            w_new = np.nan_to_num(ni_plus/(np.sqrt((ni_plus**2).sum())))
            change = np.abs(w_new-self.w).sum()
            if __debug__ and 'IRELIEF' in debug.active:
                debug('IRELIEF',
                      "change=%.4f max=%f min=%.4f mean=%.4f std=%.4f #nan=%d"
                      % (change, w_new.max(), w_new.min(), w_new.mean(),
                         w_new.std(), np.isnan(w_new).sum()))

            # update weights:
            self.w = w_new
            if change < self.threshold:
                break

        return Dataset(self.w[np.newaxis])


class IterativeReliefOnline_Devel(IterativeRelief_Devel):
    """`FeaturewiseMeasure` that performs multivariate I-RELIEF
    algorithm. Online version.

    UNDER DEVELOPMENT

    Online version with complexity O(T*N*I),
    where N is the number of instances and I the number of features.

    See: Y. Sun, Iterative RELIEF for Feature Weighting: Algorithms,
    Theories, and Applications, IEEE Trans. on Pattern Analysis and
    Machine Intelligence (TPAMI), vol. 29, no. 6, pp. 1035-1051, June
    2007. http://plaza.ufl.edu/sunyijun/Paper/PAMI_1.pdf

    Note that this implementation is not fully online, since hit and
    miss dictionaries (H,M) are computed once at the beginning using
    full access to all labels. This can be easily corrected to a full
    online implementation. But this is not mandatory now since the
    major goal of this current online implementation is reduction of
    computational complexity.
    """
    is_trained = True
    """Indicate that this measure doesn't have to be trained"""


    def __init__(self, a=5.0, permute=True, max_iter=3, **kwargs):
        """Constructor of the IRELIEF class.

        """
        # init base classes first
        IterativeRelief_Devel.__init__(self, **kwargs)

        self.a = a # parameter of the learning rate
        self.permute = permute # shuffle data when running I-RELIEF
        self.max_iter = max_iter # maximum number of iterations


    def _call(self, dataset):
        """Computes featurewise I-RELIEF-2 weights. Online version."""
        NS = dataset.samples.shape[0]
        NF = dataset.samples.shape[1]
        if self.w_guess == None:
            self.w = np.ones(NF, 'd')
        # do normalization in all cases to be safe :)
        self.w = self.w/(self.w**2).sum()

        M, H = self.compute_M_H(dataset.targets)

        ni = np.zeros(NF, 'd')
        pi = np.zeros(NF, 'd')

        if self.permute:
            # indices to go through samples in random order
            random_sequence = np.random.permutation(NS)
        else:
            random_sequence = np.arange(NS)

        change = self.threshold + 1.0
        iteration = 0
        counter = 0.0
        while change > self.threshold and iteration < self.max_iter:
            if __debug__:
                debug('IRELIEF', "Iteration %d" % iteration)

            for t in range(NS):
                counter += 1.0
                n = random_sequence[t]

                self.k = self.kernel(length_scale = self.kernel_width/self.w)
                d_w_k_xn_Mn = self.k.computed(dataset.samples[None, n, :],
                                dataset.samples[M[n], :]).as_raw_np().squeeze()
                d_w_k_xn_Mn_sum = d_w_k_xn_Mn.sum()
                d_w_k_xn_x = self.k.computed(dataset.samples[None, n, :],
                                dataset.samples).as_raw_np().squeeze()
                gamma_n = 1.0 - d_w_k_xn_Mn_sum / d_w_k_xn_x.sum()
                alpha_n = d_w_k_xn_Mn / d_w_k_xn_Mn_sum

                d_w_k_xn_Hn = self.k.computed(dataset.samples[None, n, :],
                                dataset.samples[H[n], :]).as_raw_np().squeeze()
                beta_n = d_w_k_xn_Hn / d_w_k_xn_Hn.sum()

                m_n = (np.abs(dataset.samples[n, :] - dataset.samples[M[n], :]) \
                        * alpha_n[:, np.newaxis]).sum(0)
                h_n = (np.abs(dataset.samples[n, :] - dataset.samples[H[n], :]) \
                        * beta_n[:, np.newaxis]).sum(0)
                pi = gamma_n * (m_n-h_n)
                learning_rate = 1.0 / (counter * self.a + 1.0)
                ni_new = ni + learning_rate * (pi - ni)
                ni = ni_new

                # set all negative elements to zero
                ni_plus = np.clip(ni, 0.0, np.inf)
                w_new = np.nan_to_num(ni_plus / (np.sqrt((ni_plus ** 2).sum())))
                change = np.abs(w_new - self.w).sum()
                if t % 10 == 0 and __debug__ and 'IRELIEF' in debug.active:
                    debug('IRELIEF',
                          "t=%d change=%.4f max=%f min=%.4f mean=%.4f std=%.4f"
                          " #nan=%d" %
                          (t, change, w_new.max(), w_new.min(), w_new.mean(),
                           w_new.std(), np.isnan(w_new).sum()))

                self.w = w_new

                if change < self.threshold and iteration > 0:
                    break

            iteration += 1

        return Dataset(self.w[np.newaxis])



class IterativeRelief(FeaturewiseMeasure):
    """`FeaturewiseMeasure` that performs multivariate I-RELIEF
    algorithm. Batch version.

    Batch I-RELIEF-2 feature weighting algorithm. Works for binary or
    multiclass class-labels. Batch version with complexity O(T*N^2*I),
    where T is the number of iterations, N the number of instances, I
    the number of features.

    References
    ----------
    Y. Sun, Iterative RELIEF for Feature Weighting: Algorithms,
    Theories, and Applications, IEEE Trans. on Pattern Analysis and
    Machine Intelligence (TPAMI), vol. 29, no. 6, pp. 1035-1051, June
    2007. http://plaza.ufl.edu/sunyijun/Paper/PAMI_1.pdf

    Note that current implementation allows to use only
    exponential-like kernels. Support for linear kernel will be
    added later.
    """
    is_trained = True
    """Indicate that this measure doesn't have to be trained"""

    def __init__(self, threshold=1.0e-2, kernel_width=1.0,
                 w_guess=None, **kwargs):
        """Constructor of the IRELIEF class.

        """
        # init base classes first
        FeaturewiseMeasure.__init__(self, **kwargs)

        # Threshold in W changes (stopping criterion for irelief).
        self.threshold = threshold
        self.w_guess = w_guess
        self.w = None
        self.kernel_width = kernel_width


    def compute_M_H(self, label):
        """Compute hit/miss dictionaries.

        For each instance compute the set of indices having the same
        class label and different class label.

        Note that this computation is independent of the number of
        features.

        XXX should it be some generic function since it doesn't use self
        """

        M = {}
        H = {}
        for i in range(label.size):
            M[i] = np.where(label != label[i])[0]
            tmp = (np.where(label == label[i])[0]).tolist()
            tmp.remove(i)
            # There must be least two exampls for class label[i]
            assert(tmp != [])
            H[i] = np.array(tmp)

        return M, H


    def k(self, distances):
        """Exponential kernel."""
        kd = np.exp(-distances/self.kernel_width)
        # set kd to zero where distance=0 otherwise I-RELIEF could not converge.
        kd[np.abs(distances) < 1.0e-15] = 0.0
        return kd


    def _call(self, dataset):
        """Computes featurewise I-RELIEF weights."""
        samples = dataset.samples
        NS, NF = samples.shape[:2]

        if self.w_guess == None:
            w = np.ones(NF, 'd')

        w /= (w ** 2).sum() # do normalization in all cases to be safe :)

        M, H = self.compute_M_H(dataset.targets)

        while True:
            d_w_k = self.k(pnorm_w(data1=samples, weight=w, p=1))
            ni = np.zeros(NF, 'd')
            for n in range(NS):
                 # d_w_k[n, n] could be omitted since == 0.0
                gamma_n = 1.0 - np.nan_to_num(d_w_k[n, M[n]].sum() \
                                / (d_w_k[n, :].sum() - d_w_k[n, n]))
                alpha_n = np.nan_to_num(d_w_k[n, M[n]] / (d_w_k[n, M[n]].sum()))
                beta_n = np.nan_to_num(d_w_k[n, H[n]] / (d_w_k[n, H[n]].sum()))

                m_n = (np.abs(samples[n, :] - samples[M[n], :]) \
                       * alpha_n[:, None]).sum(0)
                h_n = (np.abs(samples[n, :] - samples[H[n], :]) \
                       * beta_n[:, None]).sum(0)
                ni += gamma_n*(m_n - h_n)

            ni = ni / NS

            ni_plus = np.clip(ni, 0.0, np.inf) # set all negative elements to zero
            w_new = np.nan_to_num(ni_plus / (np.sqrt((ni_plus**2).sum())))
            change = np.abs(w_new - w).sum()
            if __debug__ and 'IRELIEF' in debug.active:
                debug('IRELIEF',
                      "change=%.4f max=%f min=%.4f mean=%.4f std=%.4f #nan=%d" \
                      % (change, w_new.max(), w_new.min(), w_new.mean(),
                         w_new.std(), np.isnan(w_new).sum()))

            # update weights:
            w = w_new
            if change < self.threshold:
                break

        self.w = w
        return Dataset(self.w[np.newaxis])


class IterativeReliefOnline(IterativeRelief):
    """`FeaturewiseMeasure` that performs multivariate I-RELIEF
    algorithm. Online version.

    This algorithm is exactly the one in the referenced paper
    (I-RELIEF-2 online), using weighted 1-norm and Exponential
    Kernel.
    """
    is_trained = True
    """Indicate that this measure doesn't have to be trained"""


    def __init__(self, a=10.0, permute=True, max_iter=3, **kwargs):
        """Constructor of the IRELIEF class.

        """
        # init base classes first
        IterativeRelief.__init__(self, **kwargs)

        self.a = a # parameter of the learning rate
        self.permute = permute # shuffle data when running I-RELIEF
        self.max_iter = max_iter # maximum number of iterations


    def _call(self, dataset):
        """Computes featurewise I-RELIEF-2 weights. Online version."""
        # local bindings
        samples = dataset.samples
        NS, NF = samples.shape[:2]
        threshold = self.threshold
        a = self.a

        if self.w_guess == None:
            w = np.ones(NF, 'd')

        # do normalization in all cases to be safe :)
        w /= (w ** 2).sum()

        M, H = self.compute_M_H(dataset.targets)

        ni = np.zeros(NF, 'd')
        pi = np.zeros(NF, 'd')

        if self.permute:
            # indices to go through x in random order
            random_sequence = np.random.permutation(NS)
        else:
            random_sequence = np.arange(NS)

        change = threshold + 1.0
        iteration = 0
        counter = 0.0
        while change > threshold and iteration < self.max_iter:
            if __debug__:
                debug('IRELIEF', "Iteration %d" % iteration)
            for t in range(NS):
                counter += 1.0
                n = random_sequence[t]

                d_xn_x = np.abs(samples[n, :] - samples)
                d_w_k_xn_x = self.k((d_xn_x * w).sum(1))

                d_w_k_xn_Mn = d_w_k_xn_x[M[n]]
                d_w_k_xn_Mn_sum = d_w_k_xn_Mn.sum()

                gamma_n = 1.0 - d_w_k_xn_Mn_sum / d_w_k_xn_x.sum()
                alpha_n = d_w_k_xn_Mn / d_w_k_xn_Mn_sum

                d_w_k_xn_Hn = d_w_k_xn_x[H[n]]
                beta_n = d_w_k_xn_Hn / d_w_k_xn_Hn.sum()

                m_n = (d_xn_x[M[n], :] * alpha_n[:, None]).sum(0)
                h_n = (d_xn_x[H[n], :] * beta_n[:, None]).sum(0)
                pi = gamma_n * (m_n - h_n)
                learning_rate = 1.0 / (counter * a + 1.0)
                ni_new = ni + learning_rate * (pi - ni)
                ni = ni_new

                # set all negative elements to zero
                ni_plus = np.clip(ni, 0.0, np.inf)
                w_new = np.nan_to_num(ni_plus / (np.sqrt((ni_plus ** 2).sum())))
                change = np.abs(w_new - w).sum()
                if t % 10 == 0 and __debug__ and 'IRELIEF' in debug.active:
                    debug('IRELIEF',
                          "t=%d change=%.4f max=%f min=%.4f mean=%.4f std=%.4f"
                          " #nan=%d" %
                          (t, change, w_new.max(), w_new.min(), w_new.mean(),
                           w_new.std(), np.isnan(w_new).sum()))

                w = w_new

                if change < threshold and iteration > 0:
                    break

            iteration += 1

        self.w = w
        return Dataset(self.w[np.newaxis])


########NEW FILE########
__FILENAME__ = nnsearchlight
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""An efficient implementation of searchlight for M1NN.
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.dochelpers import borrowkwargs, _repr_attrs
from mvpa2.misc.neighborhood import IndexQueryEngine, Sphere

from mvpa2.clfs.distance import squared_euclidean_distance

from mvpa2.measures.adhocsearchlightbase import SimpleStatBaseSearchlight, \
     _STATS

if __debug__:
    from mvpa2.base import debug
    import time as time

__all__ = [ "M1NNSearchlight", 'sphere_m1nnsearchlight' ]

class M1NNSearchlight(SimpleStatBaseSearchlight):
    """Efficient implementation of Mean-Nearest-Neighbor `Searchlight`.

    """

    @borrowkwargs(SimpleStatBaseSearchlight, '__init__')
    def __init__(self, knn, generator, qe, **kwargs):
        """Initialize a M1NNSearchlight
        TODO -- example? or just kill altogether
                rethink providing knn sample vs specifying all parameters
                explicitly
        Parameters
        ----------
        knn : `kNN`
          Used to fetch space and dfx settings. TODO
        """
        # verify that desired features are supported
        if knn.dfx != squared_euclidean_distance:
            raise ValueError(
                "%s distance function is not yet supported by M1NNSearchlight"
                % (knn.dfx,))

        # init base class first
        SimpleStatBaseSearchlight.__init__(self, generator, qe, **kwargs)

        self._knn = knn
        self.__pl_train = self.__pl_test = None

    def __repr__(self, prefixes=[]):
        return super(M1NNSearchlight, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['knn'])
            )


    def _get_space(self):
        return self.knn.get_space()

    def _untrain(self):
        super(M1NNSearchlight, self)._untrain()
        self.__pl_train = self.__pl_test = None

    def _reserve_pl_stats_space(self, shape):
        # per each label: to be (re)computed within each loop split
        # Let's try to reuse the memory though
        self.__pl_train = _STATS()
        self.__pl_test = _STATS()
        for pl in (self.__pl_train, self.__pl_test):
            pl.sums = np.zeros(shape)
            pl.means = np.zeros(shape)
            # means of squares for stddev computation
            pl.sums2 = np.zeros(shape)
            pl.variances = np.zeros(shape)
            # degenerate dimension are added for easy broadcasting later on
            pl.nsamples = np.zeros(shape[:1] + (1,)*(len(shape)-1))


    def _sl_call_on_a_split(self,
                            split, X,
                            training_sis, testing_sis,
                            nroi_fids, roi_fids,
                            indexsum_fx,
                            labels_numeric,
                            ):
        """Call to M1NNSearchlight
        """
        # Local bindings
        knn = self.knn
        params = knn.params

        pl_train = self.__pl_train
        pl_test  = self.__pl_test

        training_nsamples, training_non0labels = \
            self._compute_pl_stats(training_sis, pl_train)

        testing_nsamples, testing_non0labels = \
            self._compute_pl_stats(testing_sis, pl_test)

        nlabels = len(pl_train.nsamples)

        assert(len(np.unique(labels_numeric)) == nlabels)
        assert(training_non0labels == slice(None)) # not sure/tested if we can handle this one
        assert(testing_non0labels == slice(None)) # not sure/tested if we can handle this one

        # squared distances between the means...

        # hm, but we need for each combination of labels
        # so we keep 0th dimension corresponding to test "samples/labels"
        diff_pl_pl = pl_test.means[:, None] - pl_train.means[None,:]
        diff_pl_pl2 = np.square(diff_pl_pl)

        # XXX OPT: is it worth may be reserving the space beforehand?
        diff_pl_pl2_sl = np.zeros(diff_pl_pl2.shape[:-1] + (nroi_fids,))
        indexsum_fx(diff_pl_pl2, roi_fids, out=diff_pl_pl2_sl)

        # predictions are just the labels with minimal distance
        predictions = np.argmin(diff_pl_pl2_sl, axis=1)

        return np.asanyarray(self._ulabels_numeric), predictions

    knn = property(fget=lambda self: self._knn)

@borrowkwargs(M1NNSearchlight, '__init__', exclude=['roi_ids', 'queryengine'])
def sphere_m1nnsearchlight(knn, generator, radius=1, center_ids=None,
                          space='voxel_indices', *args, **kwargs):
    """Creates a `M1NNSearchlight` to assess :term:`cross-validation`
    classification performance of M1NN on all possible spheres of a
    certain size within a dataset.

    The idea of taking advantage of naiveness of M1NN for the sake of
    quick searchlight-ing stems from Francisco Pereira (paper under
    review).

    Parameters
    ----------
    radius : float
      All features within this radius around the center will be part
      of a sphere.
    center_ids : list of int
      List of feature ids (not coordinates) the shall serve as sphere
      centers. By default all features will be used (it is passed
      roi_ids argument for Searchlight).
    space : str
      Name of a feature attribute of the input dataset that defines the spatial
      coordinates of all features.
    **kwargs
      In addition this class supports all keyword arguments of
      :class:`~mvpa2.measures.nnsearchlight.M1NNSearchlight`.

    Notes
    -----
    If any `BaseSearchlight` is used as `SensitivityAnalyzer` one has to make
    sure that the specified scalar `Measure` returns large
    (absolute) values for high sensitivities and small (absolute) values
    for low sensitivities. Especially when using error functions usually
    low values imply high performance and therefore high sensitivity.
    This would in turn result in sensitivity maps that have low
    (absolute) values indicating high sensitivities and this conflicts
    with the intended behavior of a `SensitivityAnalyzer`.
    """
    # build a matching query engine from the arguments
    kwa = {space: Sphere(radius)}
    qe = IndexQueryEngine(**kwa)
    # init the searchlight with the queryengine
    return M1NNSearchlight(knn, generator, qe,
                          roi_ids=center_ids, *args, **kwargs)

########NEW FILE########
__FILENAME__ = noiseperturbation
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Derive sensitivity maps for a metric by selective noise perturbation"""

__docformat__ = 'restructuredtext'

if __debug__:
    from mvpa2.base import debug

from mvpa2.support.copy import deepcopy

import numpy as np

from mvpa2.measures.base import FeaturewiseMeasure
from mvpa2.datasets.base import Dataset


class NoisePerturbationSensitivity(FeaturewiseMeasure):
    """Sensitivity based on the effect of noise perturbation on a measure.

    This is a `FeaturewiseMeasure` that uses a scalar `Measure`
    and selective noise perturbation to compute a sensitivity map.

    First the scalar `Measure` computed using the original dataset. Next
    the data measure is computed multiple times each with a single feature in
    the dataset perturbed by noise. The resulting difference in the
    scalar `Measure` is used as the sensitivity for the respective
    perturbed feature. Large differences are treated as an indicator of a
    feature having great impact on the scalar `Measure`.

    Notes
    -----
    The computed sensitivity map might have positive and negative values!
    """
    is_trained = True
    """Indicate that this measure is always trained."""

    def __init__(self, datameasure,
                 noise=np.random.normal):
        """
        Parameters
        ----------
        datameasure : `Measure`
          Used to quantify the effect of noise perturbation.
        noise: Callable
          Used to generate noise. The noise generator has to return an 1d array
          of n values when called the `size=n` keyword argument. This is the
          default interface of the random number generators in NumPy's
          `random` module.
        """
        # init base classes first
        FeaturewiseMeasure.__init__(self)

        self.__datameasure = datameasure
        self.__noise = noise


    def _call(self, dataset):
        # first cast to floating point dtype, because noise is most likely
        # floating point as well and '+=' on int would not do the right thing
        if not np.issubdtype(dataset.samples.dtype, np.float):
            ds = dataset.copy(deep=False)
            ds.samples = dataset.samples.astype('float32')
            dataset = ds

        if __debug__:
            nfeatures = dataset.nfeatures

        # using a list here, to be able to handle output of unknown
        # dimensionality
        sens_map = []

        # compute the datameasure on the original dataset
        # this is used as a baseline
        orig_measure = self.__datameasure(dataset)

        # do for every _single_ feature in the dataset
        for feature in xrange(dataset.nfeatures):
            if __debug__:
                debug('PSA', "Analyzing %i features: %i [%i%%]" \
                    % (nfeatures,
                       feature+1,
                       float(feature+1)/nfeatures*100,), cr=True)

            # store current feature to restore it later on
            current_feature = dataset.samples[:, feature].copy()

            # add noise to current feature
            dataset.samples[:, feature] += self.__noise(size=len(dataset))

            # compute the datameasure on the perturbed dataset
            perturbed_measure = self.__datameasure(dataset)

            # restore the current feature
            dataset.samples[:, feature] = current_feature

            # difference from original datameasure is sensitivity
            sens_map.append(perturbed_measure.samples - orig_measure.samples)

        if __debug__:
            debug('PSA', '')

        # turn into an array and get rid of unnecessary axes -- ideally yielding
        # 2D array
        sens_map = np.array(sens_map).squeeze()
        # swap first to axis: we have nfeatures on first but want it as second
        # in a dataset
        sens_map = np.swapaxes(sens_map, 0, 1)
        return Dataset(sens_map)

########NEW FILE########
__FILENAME__ = rsa
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Representational (dis)similarity analysis"""

__docformat__ = 'restructuredtext'

from itertools import combinations
import numpy as np
from mvpa2.measures.base import Measure
from mvpa2.datasets.base import Dataset
from mvpa2.base import externals
from mvpa2.base.param import Parameter
from mvpa2.base.constraints import EnsureChoice

if externals.exists('scipy', raise_=True):
    from scipy.spatial.distance import pdist, squareform
    from scipy.stats import rankdata, pearsonr

class PDist(Measure):
    """Compute dissimiliarity matrix for samples in a dataset

    This `Measure` returns the upper triangle of the n x n disimilarity matrix
    defined as the pairwise distances between samples in the dataset, and where
    n is the number of samples.
    """

    is_trained = True # Indicate that this measure is always trained.

    pairwise_metric = Parameter('correlation', constraints='str', doc="""\
          Distance metric to use for calculating pairwise vector distances for
          dissimilarity matrix (DSM).  See scipy.spatial.distance.pdist for
          all possible metrics.""")

    center_data = Parameter(False, constraints='bool', doc="""\
          If True then center each column of the data matrix by subtracing the
          column mean from each element. This is recommended especially when
          using pairwise_metric='correlation'.""")

    square = Parameter(False, constraints='bool', doc="""\
          If True return the square distance matrix, if False, returns the
          flattened upper triangle.""")

    def __init__(self, **kwargs):
        """
        Returns
        -------
        Dataset
          If square is False, contains a column vector of length = n(n-1)/2 of
          pairwise distances between all samples. A sample attribute ``pairs``
          identifies the indices of input samples for each individual pair.
          If square is True, the dataset contains a square dissimilarty matrix
          and the entire sample attributes collection of the input dataset.
        """

        Measure.__init__(self, **kwargs)

    def _call(self,ds):

        data = ds.samples
        # center data if specified
        if self.params.center_data:
            data = data - np.mean(data,0)

        # get dsm
        dsm = pdist(data,metric=self.params.pairwise_metric)

        # if square return value make dsm square
        if self.params.square:
            # re-add the sample attributes -- should still be valid
            out = Dataset(squareform(dsm),
                          sa=ds.sa)
        else:
            # add some attributes
            out = Dataset(dsm,
                          sa=dict(pairs=list(combinations(range(len(ds)), 2))))
        return out


class PDistConsistency(Measure):
    """Calculate the correlations of PDist measures across chunks

    This measures the consistency in similarity structure across runs
    within individuals, or across individuals if the target dataset is made from
    several subjects in some common space and where the sample attribute
    specified as the chunks_attr codes for subject identity.

    @author: ACC Aug 2013
    """
    is_trained = True
    """Indicate that this measure is always trained."""

    chunks_attr = Parameter('chunks', constraints='str', doc="""\
          Chunks attribute to use for chunking dataset. Can be any samples
          attribute.""")

    pairwise_metric = Parameter('correlation', constraints='str', doc="""\
          Distance metric to use for calculating dissimilarity matrices from
          the set of samples in each chunk specified. See
          spatial.distance.pdist for all possible metrics.""")

    consistency_metric = Parameter('pearson',
                                   constraints=EnsureChoice('pearson',
                                                            'spearman'),
                                   doc="""\
          Correlation measure to use for the correlation between dissimilarity
          matrices.""")

    center_data = Parameter(False, constraints='bool', doc="""\
          If True then center each column of the data matrix by subtracing the
          column mean from each element. This is recommended especially when
          using pairwise_metric='correlation'.""")

    square = Parameter(False, constraints='bool', doc="""\
          If True return the square distance matrix, if False, returns the
          flattened upper triangle.""")


    def __init__(self, **kwargs):
        """
        Returns
        -------
        Dataset
          Contains the pairwise correlations between the DSMs
          computed from each chunk of the input dataset. If square is False,
          this is a column vector of length N(N-1)/2 for N chunks. If square
          is True, this is a square matrix of size NxN for N chunks.
        """
        # TODO: Another metric for consistency metric could be the "Rv"
        # coefficient...  (ac)
        # init base classes first
        Measure.__init__(self, **kwargs)

    def _call(self, dataset):
        """Computes the average correlation in similarity structure across chunks."""

        chunks_attr = self.params.chunks_attr
        nchunks = len(dataset.sa[chunks_attr].unique)
        if nchunks < 2:
            raise StandardError("This measure calculates similarity consistency across "
                                "chunks and is not meaningful for datasets with only "
                                "one chunk:")
        dsms = []
        chunks = []
        for chunk in dataset.sa[chunks_attr].unique:
            data = np.atleast_2d(
                    dataset.samples[dataset.sa[chunks_attr].value == chunk,:])
            if self.params.center_data:
                data = data - np.mean(data,0)
            dsm = pdist(data, self.params.pairwise_metric)
            dsms.append(dsm)
            chunks.append(chunk)
        dsms = np.vstack(dsms)

        if self.params.consistency_metric=='spearman':
            dsms = np.apply_along_axis(rankdata, 1, dsms)
        corrmat = np.corrcoef(dsms)
        if self.params.square:
            ds = Dataset(corrmat, sa={self.params.chunks_attr: chunks})
        else:
            ds = Dataset(squareform(corrmat,checks=False),
                         sa=dict(pairs=list(combinations(chunks, 2))))
        return ds

class PDistTargetSimilarity(Measure):
    """Calculate the correlations of PDist measures with a target

    Target dissimilarity correlation `Measure`. Computes the correlation between
    the dissimilarity matrix defined over the pairwise distances between the
    samples of dataset and the target dissimilarity matrix.
    """

    is_trained = True
    """Indicate that this measure is always trained."""

    pairwise_metric = Parameter('correlation', constraints='str', doc="""\
          Distance metric to use for calculating pairwise vector distances for
          dissimilarity matrix (DSM).  See scipy.spatial.distance.pdist for
          all possible metrics.""")

    comparison_metric = Parameter('pearson',
                                   constraints=EnsureChoice('pearson',
                                                            'spearman'),
                                   doc="""\
          Similarity measure to be used for comparing dataset DSM with the
          target DSM.""")

    center_data = Parameter(False, constraints='bool', doc="""\
          If True then center each column of the data matrix by subtracing the
          column mean from each element. This is recommended especially when
          using pairwise_metric='correlation'.""")

    corrcoef_only = Parameter(False, constraints='bool', doc="""\
          If True, return only the correlation coefficient (rho), otherwise
          return rho and probability, p.""")

    def __init__(self, target_dsm, **kwargs):
        """
        Parameters
        ----------
        target_dsm : array (length N*(N-1)/2)
          Target dissimilarity matrix

        Returns
        -------
        Dataset
          If ``corrcoef_only`` is True, contains one feature: the correlation
          coefficient (rho); or otherwise two-fetaures: rho plus p.
        """
        # init base classes first
        Measure.__init__(self, **kwargs)
        self.target_dsm = target_dsm
        if self.params.comparison_metric == 'spearman':
            self.target_dsm = rankdata(target_dsm)

    def _call(self,dataset):
        data = dataset.samples
        if self.params.center_data:
            data = data - np.mean(data,0)
        dsm = pdist(data,self.params.pairwise_metric)
        if self.params.comparison_metric=='spearman':
            dsm = rankdata(dsm)
        rho, p = pearsonr(dsm,self.target_dsm)
        if self.params.corrcoef_only:
            return Dataset([rho], fa={'metrics': ['rho']})
        else:
            return Dataset([[rho,p]], fa={'metrics': ['rho', 'p']})

########NEW FILE########
__FILENAME__ = searchlight
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Searchlight implementation for arbitrary measures and spaces"""

__docformat__ = 'restructuredtext'

if __debug__:
    from mvpa2.base import debug

import numpy as np
import tempfile, os
import time

import mvpa2
from mvpa2.base import externals, warning
from mvpa2.base.types import is_datasetlike
from mvpa2.base.dochelpers import borrowkwargs, _repr_attrs
from mvpa2.base.types import is_datasetlike
from mvpa2.base.progress import ProgressBar
if externals.exists('h5py'):
    # Is optionally required for passing searchlight
    # results via storing/reloading hdf5 files
    from mvpa2.base.hdf5 import h5save, h5load

from mvpa2.datasets import hstack, Dataset
from mvpa2.support import copy
from mvpa2.featsel.base import StaticFeatureSelection
from mvpa2.measures.base import Measure
from mvpa2.base.state import ConditionalAttribute
from mvpa2.misc.neighborhood import IndexQueryEngine, Sphere
from mvpa2.mappers.base import ChainMapper

class BaseSearchlight(Measure):
    """Base class for searchlights.

    The idea for a searchlight algorithm stems from a paper by
    :ref:`Kriegeskorte et al. (2006) <KGB06>`.
    """

    roi_sizes = ConditionalAttribute(enabled=False,
        doc="Number of features in each ROI.")

    roi_feature_ids = ConditionalAttribute(enabled=False,
        doc="Feature IDs for all generated ROIs.")

    roi_center_ids = ConditionalAttribute(enabled=True,
        doc="Center ID for all generated ROIs.")

    is_trained = True
    """Indicate that this measure is always trained."""


    def __init__(self, queryengine, roi_ids=None, nproc=None,
                 **kwargs):
        """
        Parameters
        ----------
        queryengine : QueryEngine
          Engine to use to discover the "neighborhood" of each feature.
          See :class:`~mvpa2.misc.neighborhood.QueryEngine`.
        roi_ids : None or list(int) or str
          List of feature ids (not coordinates) the shall serve as ROI seeds
          (e.g. sphere centers). Alternatively, this can be the name of a
          feature attribute of the input dataset, whose non-zero values
          determine the feature ids. By default all features will be used.
        nproc : None or int
          How many processes to use for computation.  Requires `pprocess`
          external module.  If None -- all available cores will be used.
        **kwargs
          In addition this class supports all keyword arguments of its
          base-class :class:`~mvpa2.measures.base.Measure`.
      """
        Measure.__init__(self, **kwargs)

        if nproc is not None and nproc > 1 and not externals.exists('pprocess'):
            raise RuntimeError("The 'pprocess' module is required for "
                               "multiprocess searchlights. Please either "
                               "install python-pprocess, or reduce `nproc` "
                               "to 1 (got nproc=%i) or set to default None"
                               % nproc)

        self._queryengine = queryengine
        if roi_ids is not None and not isinstance(roi_ids, str) \
                and not len(roi_ids):
            raise ValueError, \
                  "Cannot run searchlight on an empty list of roi_ids"
        self.__roi_ids = roi_ids
        self.nproc = nproc


    def __repr__(self, prefixes=[]):
        """String representation of a `Measure`

        Includes only arguments which differ from default ones
        """
        return super(BaseSearchlight, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['queryengine', 'roi_ids', 'nproc']))


    def _call(self, dataset):
        """Perform the ROI search.
        """
        # local binding
        nproc = self.nproc

        if nproc is None and externals.exists('pprocess'):
            import pprocess
            try:
                nproc = pprocess.get_number_of_cores() or 1
            except AttributeError:
                warning("pprocess version %s has no API to figure out maximal "
                        "number of cores. Using 1"
                        % externals.versions['pprocess'])
                nproc = 1
        # train the queryengine
        self._queryengine.train(dataset)

        # decide whether to run on all possible center coords or just a provided
        # subset
        if isinstance(self.__roi_ids, str):
            roi_ids = dataset.fa[self.__roi_ids].value.nonzero()[0]
        elif self.__roi_ids is not None:
            roi_ids = self.__roi_ids
            # safeguard against stupidity
            if __debug__:
                qe_ids = self._queryengine.ids # known to qe
                if not set(qe_ids).issuperset(roi_ids):
                    raise IndexError(
                          "Some roi_ids are not known to the query engine %s: %s"
                          % (self._queryengine,
                             set(roi_ids).difference(qe_ids)))
        else:
            roi_ids = self._queryengine.ids

        # pass to subclass
        results = self._sl_call(dataset, roi_ids, nproc)

        if 'mapper' in dataset.a:
            # since we know the space we can stick the original mapper into the
            # results as well
            if self.__roi_ids is None:
                results.a['mapper'] = copy.copy(dataset.a.mapper)
            else:
                # there is an additional selection step that needs to be
                # expressed by another mapper
                mapper = copy.copy(dataset.a.mapper)

                # NNO if the orignal mapper has no append (because it's not a
                # chainmapper, for example), we make our own chainmapper.
                #
                # THe original code was:
                # mapper.append(StaticFeatureSelection(roi_ids,
                #                                     dshape=dataset.shape[1:]))
                feat_sel_mapper = StaticFeatureSelection(roi_ids,
                                                     dshape=dataset.shape[1:])
                if 'append' in dir(mapper):
                    mapper.append(feat_sel_mapper)
                else:
                    mapper = ChainMapper([dataset.a.mapper,
                                          feat_sel_mapper])

                results.a['mapper'] = mapper

        # charge state
        self.ca.raw_results = results
        # return raw results, base-class will take care of transformations
        return results


    def _sl_call(self, dataset, roi_ids, nproc):
        """Classical generic searchlight implementation
        """
        raise NotImplementedError("Must be implemented in the derived classes")

    queryengine = property(fget=lambda self: self._queryengine)
    roi_ids = property(fget=lambda self: self.__roi_ids)


class Searchlight(BaseSearchlight):
    """The implementation of a generic searchlight measure.

    The idea for a searchlight algorithm stems from a paper by
    :ref:`Kriegeskorte et al. (2006) <KGB06>`.  As a result it
    produces a map of measures given a `datameasure` instance of
    interest, which is ran at each spatial location.
    """

    @staticmethod
    def _concat_results(sl=None, dataset=None, roi_ids=None, results=None):
        """The simplest implementation for collecting the results --
        just put them into a list

        This this implementation simply collects them into a list and
        uses only sl. for assigning conditional attributes.  But
        custom implementation might make use of more/less of them.
        Implemented as @staticmethod just to emphasize that in
        principle it is independent of the actual searchlight instance
        """
        # collect results
        results = sum(results, [])

        if __debug__ and 'SLC' in debug.active:
            debug('SLC', '')            # just newline
            resshape = len(results) and np.asanyarray(results[0]).shape or 'N/A'
            debug('SLC', ' hstacking %d results of shape %s'
                  % (len(results), resshape))

        # but be careful: this call also serves as conversion from parallel maps
        # to regular lists!
        # this uses the Dataset-hstack
        result_ds = hstack(results)

        if __debug__:
            debug('SLC', " hstacked shape %s" % (result_ds.shape,))

        if sl.ca.is_enabled('roi_feature_ids'):
            sl.ca.roi_feature_ids = [r.a.roi_feature_ids for r in results]
        if sl.ca.is_enabled('roi_sizes'):
            sl.ca.roi_sizes = [r.a.roi_sizes for r in results]
        if sl.ca.is_enabled('roi_center_ids'):
            sl.ca.roi_center_ids = [r.a.roi_center_ids for r in results]

        if 'mapper' in dataset.a:
            # since we know the space we can stick the original mapper into the
            # results as well
            if roi_ids is None:
                result_ds.a['mapper'] = copy.copy(dataset.a.mapper)
            else:
                # there is an additional selection step that needs to be
                # expressed by another mapper
                mapper = copy.copy(dataset.a.mapper)

                # NNO if the orignal mapper has no append (because it's not a
                # chainmapper, for example), we make our own chainmapper.
                feat_sel_mapper = StaticFeatureSelection(
                                    roi_ids, dshape=dataset.shape[1:])
                if hasattr(mapper, 'append'):
                    mapper.append(feat_sel_mapper)
                else:
                    mapper = ChainMapper([dataset.a.mapper,
                                          feat_sel_mapper])

                result_ds.a['mapper'] = mapper

        # store the center ids as a feature attribute
        result_ds.fa['center_ids'] = roi_ids

        return result_ds

    def __init__(self, datameasure, queryengine, add_center_fa=False,
                 results_postproc_fx=None,
                 results_backend='native',
                 results_fx=None,
                 tmp_prefix='tmpsl',
                 nblocks=None,
                 **kwargs):
        """
        Parameters
        ----------
        datameasure : callable
          Any object that takes a :class:`~mvpa2.datasets.base.Dataset`
          and returns some measure when called.
        add_center_fa : bool or str
          If True or a string, each searchlight ROI dataset will have a boolean
          vector as a feature attribute that indicates the feature that is the
          seed (e.g. sphere center) for the respective ROI. If True, the
          attribute is named 'roi_seed', the provided string is used as the name
          otherwise.
        results_postproc_fx : callable
          Called with all the results computed in a block for possible
          post-processing which needs to be done in parallel instead of serial
          aggregation in results_fx.
        results_backend : ('native', 'hdf5'), optional
          Specifies the way results are provided back from a processing block
          in case of nproc > 1. 'native' is pickling/unpickling of results by
          pprocess, while 'hdf5' would use h5save/h5load functionality.
          'hdf5' might be more time and memory efficient in some cases.
        results_fx : callable, optional
          Function to process/combine results of each searchlight
          block run.  By default it would simply append them all into
          the list.  It receives as keyword arguments sl, dataset,
          roi_ids, and results (iterable of lists).  It is the one to take
          care of assigning roi_* ca's
        tmp_prefix : str, optional
          If specified -- serves as a prefix for temporary files storage
          if results_backend == 'hdf5'.  Thus can specify the directory to use
          (trailing file path separator is not added automagically).
        nblocks : None or int
          Into how many blocks to split the computation (could be larger than
          nproc).  If None -- nproc is used.
        **kwargs
          In addition this class supports all keyword arguments of its
          base-class :class:`~mvpa2.measures.searchlight.BaseSearchlight`.
        """
        BaseSearchlight.__init__(self, queryengine, **kwargs)
        self.datameasure = datameasure
        self.results_postproc_fx = results_postproc_fx
        self.results_backend = results_backend.lower()
        if self.results_backend == 'hdf5':
            # Assure having hdf5
            externals.exists('h5py', raise_=True)
        self.results_fx = Searchlight._concat_results \
                          if results_fx is None else results_fx
        self.tmp_prefix = tmp_prefix
        self.nblocks = nblocks
        if isinstance(add_center_fa, str):
            self.__add_center_fa = add_center_fa
        elif add_center_fa:
            self.__add_center_fa = 'roi_seed'
        else:
            self.__add_center_fa = False

    def __repr__(self, prefixes=[]):
        return super(Searchlight, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['datameasure'])
            + _repr_attrs(self, ['add_center_fa'], default=False)
            + _repr_attrs(self, ['results_postproc_fx'])
            + _repr_attrs(self, ['results_backend'], default='native')
            + _repr_attrs(self, ['results_fx', 'nblocks'])
            )


    def _sl_call(self, dataset, roi_ids, nproc):
        """Classical generic searchlight implementation
        """
        assert(self.results_backend in ('native', 'hdf5'))
        # compute
        if nproc is not None and nproc > 1:
            # split all target ROIs centers into `nproc` equally sized blocks
            nproc_needed = min(len(roi_ids), nproc)
            nblocks = nproc_needed \
                      if self.nblocks is None else self.nblocks
            roi_blocks = np.array_split(roi_ids, nblocks)

            # the next block sets up the infrastructure for parallel computing
            # this can easily be changed into a ParallelPython loop, if we
            # decide to have a PP job server in PyMVPA
            import pprocess
            p_results = pprocess.Map(limit=nproc_needed)
            if __debug__:
                debug('SLC', "Starting off %s child processes for nblocks=%i"
                      % (nproc_needed, nblocks))
            compute = p_results.manage(
                        pprocess.MakeParallel(self._proc_block))
            for iblock, block in enumerate(roi_blocks):
                # should we maybe deepcopy the measure to have a unique and
                # independent one per process?
                seed = mvpa2.get_random_seed()
                compute(block, dataset, copy.copy(self.__datameasure),
                        seed=seed, iblock=iblock)
        else:
            # otherwise collect the results in an 1-item list
            p_results = [
                    self._proc_block(roi_ids, dataset, self.__datameasure)]

        # Finally collect and possibly process results
        # p_results here is either a generator from pprocess.Map or a list.
        # In case of a generator it allows to process results as they become
        # available
        result_ds = self.results_fx(sl=self,
                                    dataset=dataset,
                                    roi_ids=roi_ids,
                                    results=self.__handle_all_results(p_results))

        # Assure having a dataset (for paranoid ones)
        if not is_datasetlike(result_ds):
            try:
                result_a = np.atleast_1d(result_ds)
            except ValueError, e:
                if 'setting an array element with a sequence' in str(e):
                    # try forcing object array.  Happens with
                    # test_custom_results_fx_logic on numpy 1.4.1 on Debian
                    # squeeze
                    result_a = np.array(result_ds, dtype=object)
                else:
                    raise
            result_ds = Dataset(result_a)

        return result_ds


    def _proc_block(self, block, ds, measure, seed=None, iblock='main'):
        """Little helper to capture the parts of the computation that can be
        parallelized

        Parameters
        ----------
        seed
          RNG seed.  Should be provided e.g. in child process invocations
          to guarantee that they all seed differently to not keep generating
          the same sequencies due to reusing the same copy of numpy's RNG
        block
          Critical for generating non-colliding temp filenames in case
          of hdf5 backend.  Otherwise RNGs of different processes might
          collide in their temporary file names leading to problems.
        """
        if seed is not None:
            mvpa2.seed(seed)
        if __debug__:
            debug_slc_ = 'SLC_' in debug.active
            debug('SLC',
                  "Starting computing block for %i elements" % len(block))
            start_time = time.time()
        results = []
        store_roi_feature_ids = self.ca.is_enabled('roi_feature_ids')
        store_roi_sizes = self.ca.is_enabled('roi_sizes')
        store_roi_center_ids = self.ca.is_enabled('roi_center_ids')

        assure_dataset = any([store_roi_feature_ids,
                              store_roi_sizes,
                              store_roi_center_ids])

        # put rois around all features in the dataset and compute the
        # measure within them
        bar = ProgressBar()

        for i, f in enumerate(block):
            # retrieve the feature ids of all features in the ROI from the query
            # engine
            roi_specs = self._queryengine[f]

            if __debug__ and  debug_slc_:
                debug('SLC_', 'For %r query returned roi_specs %r'
                      % (f, roi_specs))

            if is_datasetlike(roi_specs):
                # TODO: unittest
                assert(len(roi_specs) == 1)
                roi_fids = roi_specs.samples[0]
            else:
                roi_fids = roi_specs

            # slice the dataset
            roi = ds[:, roi_fids]

            if is_datasetlike(roi_specs):
                for n, v in roi_specs.fa.iteritems():
                    roi.fa[n] = v

            if self.__add_center_fa:
                # add fa to indicate ROI seed if requested
                roi_seed = np.zeros(roi.nfeatures, dtype='bool')
                if f in roi_fids:
                    roi_seed[roi_fids.index(f)] = True
                else:
                    warning("Center feature attribute id %s not found" % f)
                roi.fa[self.__add_center_fa] = roi_seed

            # compute the datameasure and store in results
            res = measure(roi)

            if assure_dataset and not is_datasetlike(res):
                res = Dataset(np.atleast_1d(res))
            if store_roi_feature_ids:
                # add roi feature ids to intermediate result dataset for later
                # aggregation
                res.a['roi_feature_ids'] = roi_fids
            if store_roi_sizes:
                res.a['roi_sizes'] = roi.nfeatures
            if store_roi_center_ids:
                res.a['roi_center_ids'] = f
            results.append(res)

            if __debug__:
                msg = 'ROI %i (%i/%i), %i features' % \
                            (f + 1, i + 1, len(block), roi.nfeatures)
                debug('SLC', bar(float(i + 1) / len(block), msg), cr=True)

        if __debug__:
            # just to get to new line
            debug('SLC', '')

        if self.results_postproc_fx:
            if __debug__:
                debug('SLC', "Post-processing %d results in proc_block using %s"
                      % (len(results), self.results_postproc_fx))
            results = self.results_postproc_fx(results)
        if self.results_backend == 'native':
            pass                        # nothing special
        elif self.results_backend == 'hdf5':
            # store results in a temporary file and return a filename
            results_file = tempfile.mktemp(prefix=self.tmp_prefix,
                                           suffix='-%s.hdf5' % iblock)
            if __debug__:
                debug('SLC', "Storing results into %s" % results_file)
            h5save(results_file, results)
            if __debug__:
                debug('SLC_', "Results stored")
            results = results_file
        else:
            raise RuntimeError("Must not reach this point")
        return results


    def __set_datameasure(self, datameasure):
        """Set the datameasure"""
        self.untrain()
        self.__datameasure = datameasure

    def __handle_results(self, results):
        if self.results_backend == 'hdf5':
            # 'results' must be just a filename
            assert(isinstance(results, str))
            if __debug__:
                debug('SLC', "Loading results from %s" % results)
            results_data = h5load(results)
            os.unlink(results)
            if __debug__:
                debug('SLC_', "Loaded results of len=%d from"
                      % len(results_data))
            return results_data
        else:
            return results

    def __handle_all_results(self, results):
        """Helper generator to decorate passing the results out to
        results_fx
        """
        for r in results:
            yield self.__handle_results(r)


    datameasure = property(fget=lambda self: self.__datameasure,
                           fset=__set_datameasure)
    add_center_fa = property(fget=lambda self: self.__add_center_fa)


@borrowkwargs(Searchlight, '__init__', exclude=['roi_ids', 'queryengine'])
def sphere_searchlight(datameasure, radius=1, center_ids=None,
                       space='voxel_indices', **kwargs):
    """Creates a `Searchlight` to run a scalar `Measure` on
    all possible spheres of a certain size within a dataset.

    The idea for a searchlight algorithm stems from a paper by
    :ref:`Kriegeskorte et al. (2006) <KGB06>`.

    Parameters
    ----------
    datameasure : callable
      Any object that takes a :class:`~mvpa2.datasets.base.Dataset`
      and returns some measure when called.
    radius : int
      All features within this radius around the center will be part
      of a sphere. Radius is in grid-indices, i.e. ``1`` corresponds
      to all immediate neighbors, regardless of the physical distance.
    center_ids : list of int
      List of feature ids (not coordinates) the shall serve as sphere
      centers. Alternatively, this can be the name of a feature attribute
      of the input dataset, whose non-zero values determine the feature
      ids.  By default all features will be used (it is passed as ``roi_ids``
      argument of Searchlight).
    space : str
      Name of a feature attribute of the input dataset that defines the spatial
      coordinates of all features.
    **kwargs
      In addition this class supports all keyword arguments of its
      base-class :class:`~mvpa2.measures.base.Measure`.

    Notes
    -----
    If `Searchlight` is used as `SensitivityAnalyzer` one has to make
    sure that the specified scalar `Measure` returns large
    (absolute) values for high sensitivities and small (absolute) values
    for low sensitivities. Especially when using error functions usually
    low values imply high performance and therefore high sensitivity.
    This would in turn result in sensitivity maps that have low
    (absolute) values indicating high sensitivities and this conflicts
    with the intended behavior of a `SensitivityAnalyzer`.
    """
    # build a matching query engine from the arguments
    kwa = {space: Sphere(radius)}
    qe = IndexQueryEngine(**kwa)
    # init the searchlight with the queryengine
    return Searchlight(datameasure, queryengine=qe, roi_ids=center_ids,
                       **kwargs)


#class OptimalSearchlight( object ):
#    def __init__( self,
#                  searchlight,
#                  test_radii,
#                  verbose=False,
#                  **kwargs ):
#        """
#        """
#        # results will end up here
#        self.__perfmeans = []
#        self.__perfvars = []
#        self.__chisquares = []
#        self.__chanceprobs = []
#        self.__spheresizes = []
#
#        # run searchligh for all radii in the list
#        for radius in test_radii:
#            if verbose:
#                print 'Using searchlight with radius:', radius
#            # compute the results
#            searchlight( radius, **(kwargs) )
#
#            self.__perfmeans.append( searchlight.perfmean )
#            self.__perfvars.append( searchlight.perfvar )
#            self.__chisquares.append( searchlight.chisquare )
#            self.__chanceprobs.append( searchlight.chanceprob )
#            self.__spheresizes.append( searchlight.spheresize )
#
#
#        # now determine the best classification accuracy
#        best = np.array(self.__perfmeans).argmax( axis=0 )
#
#        # select the corresponding values of the best classification
#        # in all data tables
#        self.perfmean   = best.choose(*(self.__perfmeans))
#        self.perfvar    = best.choose(*(self.__perfvars))
#        self.chisquare  = best.choose(*(self.__chisquares))
#        self.chanceprob = best.choose(*(self.__chanceprobs))
#        self.spheresize = best.choose(*(self.__spheresizes))
#
#        # store the best performing radius
#        self.bestradius = np.zeros( self.perfmean.shape, dtype='uint' )
#        self.bestradius[searchlight.mask==True] = \
#            best.choose( test_radii )[searchlight.mask==True]
#
#
#
#def makeSphericalROIMask( mask, radius, elementsize=None ):
#    """
#    """
#    # use default elementsize if none is supplied
#    if not elementsize:
#        elementsize = [ 1 for i in range( len(mask.shape) ) ]
#    else:
#        if len( elementsize ) != len( mask.shape ):
#            raise ValueError, 'elementsize does not match mask dimensions.'
#
#    # rois will be drawn into this mask
#    roi_mask = np.zeros( mask.shape, dtype='int32' )
#
#    # while increase with every ROI
#    roi_id_counter = 1
#
#    # build spheres around every non-zero value in the mask
#    for center, spheremask in \
#        algorithms.SpheresInMask( mask,
#                                  radius,
#                                  elementsize,
#                                  forcesphere = True ):
#
#        # set all elements that match the current spheremask to the
#        # current ROI index value
#        roi_mask[spheremask] = roi_id_counter
#
#        # increase ROI counter
#        roi_id_counter += 1
#
#    return roi_mask

########NEW FILE########
__FILENAME__ = statsmodels_adaptor
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wrap models of the StatsModels package into a FeaturewiseMeasure."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import externals

# do conditional to be able to build module reference
if externals.exists('scipy', raise_=True):
    from mvpa2.support.scipy.stats import scipy
    import scipy.stats as stats

if externals.exists('statsmodels', raise_=True):
    import statsmodels.api as sm

from mvpa2.measures.base import FeaturewiseMeasure
from mvpa2.datasets.base import Dataset

__all__ = [ 'UnivariateStatsModels', 'GLM' ]

class UnivariateStatsModels(FeaturewiseMeasure):
    """Adaptor for some models from the StatsModels package

    This adaptor allows for fitting several statistical models to univariate
    (in StatsModels terminology "endogeneous") data. A model, based on
    "exogeneous" data (i.e. a design matrix) and optional parameters, is fitted
    to each feature vector in a given dataset individually. The adaptor
    supports a variety of models provided by the StatsModels package, including
    simple ordinary least squares (OLS), generalized least squares (GLS) and
    others. This feature-wise measure can extract a variety of properties from
    the model fit results, and aggregate them into a result dataset. This
    includes, for example, all attributes of a StatsModels ``RegressionResult``
    class, such as model parameters and their error estimates, Aikake's
    information criteria, and a number of statistical properties. Moreover,
    it is possible to perform t-contrasts/t-tests of parameter estimates, as
    well as F-tests for contrast matrices.

    Examples
    --------
    Some example data: two features, seven samples

    >>> endog = Dataset(np.transpose([[1, 2, 3, 4, 5, 6, 8],
    ...                               [1, 2, 1, 2, 1, 2, 1]]))
    >>> exog = range(7)

    Set up a model generator -- it yields an instance of an OLS model for
    a particular design and feature vector. The generator will be called
    internally for each feature in the dataset.

    >>> model_gen = lambda y, x: sm.OLS(y, x)

    Configure the adaptor with the model generator and a common design for all
    feature model fits. Tell the adaptor to auto-add a constant to the design.
 
    >>> usm = UnivariateStatsModels(exog, model_gen, add_constant=True)

    Run the measure. By default it extracts the parameter estimates from the
    models (two per feature/model: regressor + constant).

    >>> res = usm(endog)
    >>> print res
    <Dataset: 2x2@float64, <sa: descr>>
    >>> print res.sa.descr
    ['params' 'params']

    Alternatively, extract t-values for a test of all parameter estimates
    against zero.

    >>> usm = UnivariateStatsModels(exog, model_gen, res='tvalues',
    ...                             add_constant=True)
    >>> res = usm(endog)
    >>> print res
    <Dataset: 2x2@float64, <sa: descr>>
    >>> print res.sa.descr
    ['tvalues' 'tvalues']

    Compute a t-contrast: first parameter is non-zero. This returns additional
    test statistics, such as p-value and effect size in the result dataset. The
    contrast vector is pass on to the ``t_test()`` function (``r_matrix``
    argument) of the StatsModels result class.

    >>> usm = UnivariateStatsModels(exog, model_gen, res=[1,0],
    ...                             add_constant=True)
    >>> res = usm(endog)
    >>> print res
    <Dataset: 6x2@float64, <sa: descr>>
    >>> print res.sa.descr
    ['tvalue' 'pvalue' 'effect' 'sd' 'df' 'zvalue']

    F-test for a contrast matrix, again with additional test statistics in the
    result dataset. The contrast vector is pass on to the ``f_test()`` function
    (``r_matrix`` argument) of the StatsModels result class.

    >>> usm = UnivariateStatsModels(exog, model_gen, res=[[1,0],[0,1]],
    ...                             add_constant=True)
    >>> res = usm(endog)
    >>> print res
    <Dataset: 4x2@float64, <sa: descr>>
    >>> print res.sa.descr
    ['fvalue' 'pvalue' 'df_num' 'df_denom']

    For any custom result extraction, a callable can be passed to the ``res``
    argument. This object will be called with the result of each model fit. Its
    return value(s) will be aggregated into a result dataset.

    >>> def extractor(res):
    ...     return [res.aic, res.bic]
    >>>
    >>> usm = UnivariateStatsModels(exog, model_gen, res=extractor,
    ...                             add_constant=True)
    >>> res = usm(endog)
    >>> print res
    <Dataset: 2x2@float64>

    """

    is_trained = True

    def __init__(self, exog, model_gen, res='params', add_constant=True,
                 **kwargs):
        """
        Parameters
        ----------
        exog : array-like
          Column ordered (observations in rows) design matrix.
        model_gen : callable
          Callable that returns a StatsModels model when called like
          ``model_gen(endog, exog)``.
        res : {'params', 'tvalues', ...} or 1d array or 2d array or callable
          Variable of interest that should be reported as feature-wise
          measure. If a str, the corresponding attribute of the model fit result
          class is returned (e.g. 'tvalues'). If a 1d-array, it is passed
          to the fit result class' ``t_test()`` function as a t-contrast vector.
          If a 2d-array, it is passed to the ``f_test()`` function as a
          contrast matrix.  In both latter cases a number of common test
          statistics are returned in the rows of the result dataset. A description
          is available in the 'descr' sample attribute. Any other datatype
          passed to this argument will be treated as a callable, the model
          fit result is passed to it, and its return value(s) is aggregated
          in the result dataset.
        add_constant : bool, optional
          If True, a constant will be added to the design matrix that is
          passed to ``exog``.
        """
        FeaturewiseMeasure.__init__(self, **kwargs)
        self._exog = exog
        if add_constant:
            self._exog = sm.add_constant(exog)
        self._res = res
        if isinstance(res, np.ndarray) or isinstance(res, (list, tuple)):
            self._res = np.atleast_1d(res)
        self._model_gen = model_gen


    def __fitmodel1d(self, Y):
        """Helper for apply_along_axis()"""
        res = self._res
        results = self._model_gen(Y, self._exog).fit()
        t_to_z = lambda t, df: stats.norm.ppf(stats.t.cdf(t, df))
        if isinstance(res, np.ndarray):
            if len(res.shape) == 1:
                tstats = results.t_test(self._res)
                return [np.asscalar(i) for i in [tstats.tvalue,
                                                 tstats.pvalue,
                                                 tstats.effect,
                                                 tstats.sd,
                                                 np.array(tstats.df_denom),
                                                 t_to_z(tstats.tvalue, tstats.df_denom)]]

            elif len(res.shape) == 2:
                fstats = results.f_test(self._res)
                return [np.asscalar(i) for i in
                            [fstats.fvalue,
                             fstats.pvalue]] + [fstats.df_num,
                                                fstats.df_denom]
            else:
                raise ValueError("Test specification (via `res`) has to be 1d or 2d array")
        elif isinstance(res, str):
            return results.__getattribute__(res)
        else:
            return res(results)


    def _call(self, dataset):
        # compute the regression once per feature
        results = np.apply_along_axis(self.__fitmodel1d, 0, dataset.samples)
        # figure out potential description of the results
        sa = None
        res = self._res
        if isinstance(res, np.ndarray):
            if len(res.shape) == 1:
                sa = ['tvalue', 'pvalue', 'effect', 'sd', 'df', 'zvalue']
            elif len(res.shape) == 2:
                sa = ['fvalue', 'pvalue', 'df_num', 'df_denom']
        elif isinstance(res, str):
            sa = [res] * len(results)
        if not sa is None:
            sa = {'descr': sa}
        # reassign the input feature attributes to the results
        return Dataset(results, sa=sa, fa=dataset.fa)



class GLM(UnivariateStatsModels):
    """Adaptor to the statsmodels-based UnivariateStatsModels

    This class is deprecated and only here to ease the transition of user code
    to the new classes. For all new code, please use the UnivariateStatsModels
    class.
    """
    def __init__(self, design, voi='pe', **kwargs):
        if isinstance(voi, str):
            # Possibly remap to adjusted interface
            voi = {'pe': 'params', 'zstat': 'zvalue'}.get(voi, voi)
        UnivariateStatsModels.__init__(
                              self,
                              design,
                              res=voi,
                              add_constant=False,
                              model_gen=lambda y, x: sm.OLS(y, x),
                              **kwargs)

########NEW FILE########
__FILENAME__ = winner
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Data aggregation procedures"""

__docformat__ = 'restructuredtext'

import numpy as np
from functools import partial

from mvpa2.base import externals
from mvpa2.base.learner import ChainLearner
from mvpa2.measures.base import Measure
from mvpa2.base.dataset import vstack
from mvpa2.datasets.base import Dataset
from mvpa2.mappers.fx import mean_group_sample
from mvpa2.base.node import ChainNode
import copy
from mvpa2.base.dochelpers import _repr_attrs

class WinnerMeasure(Measure):
    '''Select a "winning" element along samples or features.

    Given the specification would return a Dataset with a single sample
    (or feature).
    '''
    is_trained = True
    def __init__(self, axis, fx, other_axis_prefix=None, **kwargs):
        '''
        Parameters
        ----------
        axis: str or int
            'samples' (or 0) or 'features' (or 1).
        fx: callable
            function to determine the winner. When called with a dataset ds,
            it should return a vector with ds.nsamples values 
            (if axis=='features') or ds.nfeatures values (if axis=='samples').  
        other_axis_prefix: str
            prefix used for feature or sample attributes set on the other axis.
        '''
        Measure.__init__(self, **kwargs)
        if type(axis) is str:
            str2num = dict(samples=0, features=1)
            if not axis in str2num:
                raise ValueError("Illegal axis: should be %s" %
                                        ' or '.join(str2num))
            axis = str2num[axis]

        elif not axis in (0, 1):
            raise ValueError("Illegal axis: should be 0 or 1")

        self.__axis = axis
        self.__fx = fx
        self.__other_axis_prefix = other_axis_prefix

    def __repr__(self, prefixes=[]):
        prefixes_ = ['axis=%r,fx=%r,other_axis_prefix=%r' % (
                        self.__axis, self.__fx, self.__other_axis_prefix)]
        return "%s(%s)" % (self.__class__.__name__, ','.join(prefixes_))

    def _call(self, ds):
        '''
        Parameters
        ----------
        ds: Dataset
            input dataset 
        
        Returns
        -------
        wds: Dataset
            Result with one sample (if axis=='feature') or one feature (if 
            axis=='samples') and an equal number of features (or samples,
            respectively) as the input dataset.
        '''

        axis = self.__axis
        fx = self.__fx

        # ensure it's a dataset
        if not isinstance(ds, Dataset):
            ds = Dataset(ds)

        samples = ds.samples

        # apply the function
        winners = fx(ds)

        # set the new shape
        new_shape = list(ds.shape)
        new_shape[axis] = 1

        # the output dataset
        wta = Dataset(np.reshape(winners, new_shape))

        # copy dataset attributes
        wta.a = ds.a.copy()

        # copy feature attribute and set sample attributes, or vice versa
        fas = [ds.fa, wta.fa]
        sas = [ds.sa, wta.sa]
        fas_sas = [fas, sas]
        to_copy, to_leave = [fas_sas[(i + axis) % 2] for i in xrange(2)]

        # copy each attribute
        for k, v in to_copy[0].iteritems():
            to_copy[1][k] = copy.copy(v)

        # set source and target. feature attributes become
        # sample attributes; or vice versa
        src, _ = to_leave
        trg = to_copy[1]
        prefix = self.__other_axis_prefix
        for k, v in src.iteritems():
            # set the prefix
            prek = ('' if prefix is None else prefix) + k

            if prek in trg:
                raise KeyError("Key clash: %s already in %s"
                                    % (prek, to_copy[1]))
            trg[prek] = v.value[winners]

        return wta

def feature_winner_measure():
    '''takes winner over features'''
    return WinnerMeasure('features', partial(np.argmax, axis=1), 'wta_')

def feature_loser_measure():
    '''takes loser over features'''
    return WinnerMeasure('features', partial(np.argmin, axis=1), 'lta_')

def sample_winner_measure():
    '''takes winner over samples'''
    return WinnerMeasure('samples', partial(np.argmax, axis=0), 'wta_')

def sample_loser_measure():
    '''takes loser over samples'''
    return WinnerMeasure('samples', partial(np.argmin, axis=0), 'lta_')

def group_sample_winner_measure(attrs=('targets',)):
    '''takes winner after meaning over attrs'''
    return ChainNode((mean_group_sample(attrs), sample_winner_measure()))

def group_sample_loser_measure(attrs=('targets',)):
    '''takes loser after meaning over attrs'''
    return ChainNode((mean_group_sample(attrs), sample_loser_measure()))



if __name__ == '__main__':
    ns = 4
    nf = 3
    n = ns * nf
    ds = Dataset(np.reshape(np.mod(np.arange(0, n * 5, 5) + .5 * n, n), (ns, nf)),
                 sa=dict(targets=[0, 0, 1, 1], x=[3, 2, 1, 0]),
                 fa=dict(v=[3, 2, 1], w=['a', 'b', 'c']))

    measures2out = {feature_winner_measure : [1, 0, 2, 1],
                    feature_loser_measure: [2, 1, 0, 2],
                    sample_winner_measure: [1, 0, 2],
                    sample_loser_measure:[2, 1, 3],
                    group_sample_winner_measure:[0, 0, 0],
                    group_sample_loser_measure: [1, 0, 0]}

    for m, out in measures2out.iteritems():
        print np.all(m()(ds).samples.ravel() == np.asarray(out))


########NEW FILE########
__FILENAME__ = args
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helpers for arguments handling."""

__docformat__ = 'restructuredtext'

def split_kwargs(kwargs, prefixes=[]):
    """Helper to separate kwargs into multiple groups

    Parameters
    ----------
    prefixes : list of strs
      Each entry sets a prefix which puts entry with key starting
      with it into a separate group.
      Group '' corresponds to 'leftovers'

    :Output:
      dictionary with keys == `prefixes`
    """
    if not ('' in prefixes):
        prefixes = prefixes + ['']
    result = [ [] for i in prefixes ]
    for k,v in kwargs.iteritems():
        for i,p in enumerate(prefixes):
            if k.startswith(p):
                result[i].append((k.replace(p,'',1), v))
                break
    resultd = dict((p,dict(x)) for p,x in zip(prefixes, result))
    return resultd


def group_kwargs(prefixes, assign=False, passthrough=False):
    """Decorator function to join parts of kwargs together

    Parameters
    ----------
    prefixes : list of strs
      Prefixes to split based on. See `split_kwargs`
    assign : bool
      Flag to assign the obtained arguments to self._<prefix>_kwargs
    passthrough : bool
      Flag to pass joined arguments as <prefix>_kwargs argument.
      Usually it is sufficient to have either assign or passthrough.
      If none of those is True, decorator simply filters out mentioned
      groups from being passed to the method

    Example: if needed to join all args which start with 'slave<underscore>'
    together under slave_kwargs parameter
    """
    def decorated_method(method):
        def do_group_kwargs(self, *args_, **kwargs_):
            if '' in prefixes:
                raise ValueError, \
                      "Please don't put empty string ('') into prefixes"
            # group as needed
            splits = split_kwargs(kwargs_, prefixes)
            # adjust resultant kwargs__
            kwargs__ = splits['']
            for prefix in prefixes:
                skwargs = splits[prefix]
                k = '%skwargs' % prefix
                if k in kwargs__:
                    # is unprobable but can happen
                    raise ValueError, '%s is already given in the arguments' % k
                if passthrough:   kwargs__[k] = skwargs
                if assign: setattr(self, '_%s' % k, skwargs)
            return method(self, *args_, **kwargs__)
        do_group_kwargs.func_name = method.func_name
        return do_group_kwargs

    return decorated_method


########NEW FILE########
__FILENAME__ = attrmap
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helper to map literal attribute to numerical ones (and back)"""


import numpy as np
from mvpa2.base.types import is_sequence_type

class AttributeMap(object):
    # might be derived from dict, but do not see advantages right now,
    # since this one has forward and reverse map
    # however, it might be desirable to implement more of the dict interface
    """Map to translate literal values to numeric ones (and back).

    A translation map is derived automatically from the argument of the first
    call to to_numeric(). The default mapping is to map unique value
    (in sorted order) to increasing integer values starting from zero.

    In case the default mapping is undesired a custom map can be specified to
    the constructor call.

    Regardless of how the mapping has been specified or derived, it remains
    constant (i.e. it is not influenced by subsequent calls to meth:`to_numeric`
    or meth:`to_literal`. However, the translation map can be removed with
    meth:`clear`.

    Both conversion methods take sequence-like input and return arrays.

    Examples
    --------

    Default mapping procedure using an automatically derived translation map:

    >>> am = AttributeMap()
    >>> am.to_numeric(['eins', 'zwei', 'drei'])
    array([1, 2, 0])

    >>> print am.to_literal([1, 2, 0])
    ['eins', 'zwei', 'drei']

    Custom mapping:

    >>> am = AttributeMap(map={'eins': 11, 'zwei': 22, 'drei': 33})
    >>> am.to_numeric(['eins', 'zwei', 'drei'])
    array([11, 22, 33])
    """
    def __init__(self, map=None, mapnumeric=False,
                 collisions_resolution=None):
        """
        Parameters
        ----------
        map : dict
          Custom dict with literal keys mapping to numerical values.
        mapnumeric : bool
          In some cases it is necessary to map numeric labels too, for
          instance when target labels should be from a specific set,
          e.g. (-1, +1).
        collisions_resolution : None or {'tuple', 'lucky'}
          How to resolve collisions on to_literal if multiple entries
          map to the same value when custom map was provided.  If None
          -- exception would get raise, if 'tuple' -- collided entries
          are grouped into a tuple, if 'lucky' -- some last
          encountered literal wins (i.e. arbitrary resolution).  This
          parameter is in effect only when calling :meth:`to_literal`.

        Please see the class documentation for more information.
        """
        self.clear()
        self.mapnumeric = mapnumeric
        self.collisions_resolution = collisions_resolution

        if not map is None:
            if not isinstance(map, dict):
                raise ValueError("Custom map need to be a dict.")
            self._nmap = map
        self._lmap = None               # pylint happiness

    def __repr__(self):
        """String representation of AttributeMap
        """
        args = []
        if self._nmap:
            args.append(repr(self._nmap)),
        if self.mapnumeric:
            args.append('mapnumeric=True')
        if self.collisions_resolution:
            args.append('collisions_resolution=%r'
                        % (self.collisions_resolution,))
        return "%s(%s)"  % (self.__class__.__name__, ', '.join(args))

    def __len__(self):
        if self._nmap is None:
            return 0
        else:
            return len(self._nmap)

    def __bool__(self):
        return not self._nmap is None

    def clear(self):
        """Remove previously established mappings."""
        # map from literal TO numeric
        self._nmap = None
        # map from numeric TO literal
        self._lmap = None

    def keys(self):
        """Returns the literal names of the attribute map."""
        if self._nmap is None:
            return None
        else:
            return self._nmap.keys()

    def values(self):
        """Returns the numerical values of the attribute map."""
        if self._nmap is None:
            return None
        else:
            return self._nmap.values()

    def iteritems(self):
        """Dict-like generator yielding literal/numerical pairs."""
        if self._nmap is None:
            raise StopIteration
        else:
            for k, v in self._nmap:
                yield k, v

    # Py3 Compatibility method to keep lib2to3 happy
    items = iteritems
    
    def to_numeric(self, attr):
        """Map literal attribute values to numerical ones.

        Arguments with numerical data type will be returned as is.

        Parameters
        ----------
        attr : sequence
          Literal values to be mapped.

        Please see the class documentation for more information.
        """
        attr = np.asanyarray(attr)

        # no mapping if already numeric
        if not np.issubdtype(attr.dtype, str) and not self.mapnumeric:
            return attr

        if self._nmap is None:
            # sorted list of unique attr values
            ua = np.unique(attr)
            self._nmap = dict(zip(ua, range(len(ua))))
        elif __debug__:
            ua = np.unique(attr)
            mkeys = sorted(self._nmap.keys())
            if (ua != mkeys).any():
                # maps to not match
                raise KeyError("Existing attribute map not suitable for "
                        "to be mapped attribute (i.e. unknown values. "
                        "Attribute has '%s', but map has '%s'."
                        % (str(ua), str(mkeys)))


        num = np.empty(attr.shape, dtype=np.int)
        for k, v in self._nmap.iteritems():
            num[attr == k] = v
        return num

    def _get_lmap(self):
        """Recomputes lmap from the stored _nmap
        """
        cr = self.collisions_resolution
        if cr == 'lucky':
            lmap = dict([(v, k) for k, v in self._nmap.iteritems()])
        elif cr in [None, 'tuple']:
            lmap = {}
            counts = {}                     # is used for 'tuple' resolution
            for k, v in self._nmap.iteritems():
                count = counts.get(v, 0)
                if count:               # we saw it already
                    if cr is None:
                        raise ValueError, \
                            "Numeric value %r was already reverse mapped to " \
                            "%r.  Now trying to remap into %r.  Please adjust" \
                            " your mapping or change collissions_resolution" \
                            " parameter" % (v, lmap[v], k)
                    else:
                        if count == 1:
                            lmap[v] = (lmap[v], k)
                        else:
                            lmap[v] = lmap[v] + (k, ) # create new tuple
                else:
                    lmap[v] = k
                counts[v] = count +1
        else:
            raise ValueError, \
                  "Provided parameter collisions_resolution=%r is of unknown " \
                  "value. See documentation for AttributeMapper" % (cr,)
        return lmap

    def to_literal(self, attr, recurse=False):
        """Map numerical value back to literal ones.

        Parameters
        ----------
        attr : sequence
          Numerical values to be mapped.
        recurse : bool
          Either to recursively change items within the sequence
          if those are iterable as well

        Please see the class documentation for more information.
        """
        # we need one or the other map
        if self._lmap is None and self._nmap is None:
            raise RuntimeError("AttributeMap has no mapping information. "
                               "Ever called to_numeric()?")

        if self._lmap is None:
            self._lmap = self._get_lmap()

        lmap = self._lmap

        if is_sequence_type(attr) and not isinstance(attr, str):
            # Choose lookup function
            if recurse:
                lookupfx = lambda x: self.to_literal(x, recurse=True)
            else:
                # just dictionary lookup
                lookupfx = lambda x:lmap[x]

            # To assure the preserving the container type
            target_constr = attr.__class__
            # ndarrays are special since array is just a factory, and
            # ndarray takes shape as the first argument
            isarray = issubclass(target_constr, np.ndarray)
            if isarray:
                if attr.dtype is np.dtype('object'):
                    target_constr = lambda x: np.array(x, dtype=object)
                else:
                    # Otherwise no special handling
                    target_constr = np.array

            # Perform lookup and store to the list
            resl = [lookupfx(k) for k in attr]

            # If necessary assure derived ndarray class type
            if isarray:
                if attr.dtype is np.dtype('object'):
                    # we need first to create empty one and then
                    # assign items -- god bless numpy
                    resa = np.empty(len(resl), dtype=attr.dtype)
                    resa[:] = resl
                else:
                    resa = target_constr(resl)

                if not (attr.__class__ is np.ndarray):
                    # to accommodate subclasses of ndarray
                    res = resa.view(attr.__class__)
                else:
                    res = resa
            else:
                res = target_constr(resl)

            return res
        else:
            return lmap[attr]

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Tiny snippets to interface with FSL easily."""

__docformat__ = 'restructuredtext'

from mvpa2.misc.io import ColumnData

if __debug__:
    from mvpa2.base import debug


class BrainVoyagerRTC(ColumnData):
    """IO helper to read BrainVoyager RTC files.

    This is a textfile format that is used to specify stimulation
    protocols for data analysis in BrainVoyager. It looks like

    FileVersion:     2
    Type:            DesignMatrix
    NrOfPredictors:  4
    NrOfDataPoints:  147

    "fm_l_60dB" "fm_r_60dB" "fm_l_80dB" "fm_r_80dB"
    0.000000 0.000000 0.000000 0.000000
    0.000000 0.000000 0.000000 0.000000
    0.000000 0.000000 0.000000 0.000000

    Data is always read as `float` and header is actually ignored
    """
    def __init__(self, source):
        """Read and write BrainVoyager RTC files.

        Parameters
        ----------
        source : str
          Filename of an RTC file
        """
        # init data from known format
        ColumnData.__init__(self, source, header=True,
                            sep=None, headersep='"', dtype=float, skiplines=5)


    def toarray(self):
        """Returns the data as an array
        """
        import numpy as np

        # return as array with time axis first
        return np.array([self[i] for i in self._header_order],
                       dtype='float').T


########NEW FILE########
__FILENAME__ = cmdline
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Common functions and options definitions for command line

Conventions:
Every option (instance of optparse.Option) has prefix "opt". Lists of options
has prefix opts (e.g. `opts.common`).

Option name should be camelbacked version of .dest for the option.
"""

__docformat__ = 'restructuredtext'

import mvpa2

# TODO? all options (opt*) might migrate to respective module? discuss
from optparse import OptionParser, Option, OptionGroup, OptionConflictError

# needed for verboseCallback
from mvpa2.base import verbose, externals

class Options(object):
    """Just a convenience placeholder for all available options
    """
    pass

class OptionGroups(object):
    """Group creation is delayed until instance is requested.

    This allows to overcome the problem of poluting handled cmdline options
    """

    def __init__(self, parser):
        """
        Parameters
        ----------
        parser : OptionParser
          To which parser to add groups
        """
        self._d = {}
        self._parser = parser

    def add(self, name, l, doc):
        self._d[name] = (doc, l)

    def _get_group(self, name):
        try:
            doc, l = self._d[name]
        except KeyError:
            raise ValueError, "No group with name %s" % name
        opts = OptionGroup(self._parser, doc)
        try:
            opts.add_options(l)
        except OptionConflictError:
            print "Problem addition options to the group '%s'. Most probably" \
                  " the option was independently added already." % name
            raise
        return opts

    def __getattribute__(self, index):
        if index[0] == '_':
            return object.__getattribute__(self, index)
        if self._d.has_key(index):
            return self._get_group(index)
        return object.__getattribute__(self, index)


def split_comma_semicolon_lists(s, dtype=None):
    """TODO
    Parameters
    ----------
    s
      Input string
    dtype: optional
      Data type to impose upon values
    """
    res = []
    for x in s.split(";"):
        if not ':' in x:
            raise ValueError("Each entry must be in the form key:values,"
                             " e.g. 'targets:rest'")
        key, s_values = x.split(':', 1)
        values = s_values.split(',')
        if dtype is not None:
            values = [dtype(v) for v in values]
        res.append((key, values))
    return res


# Some local helpers

def _FORMAT(s):
    """Helper to provide uniform appearance for formats in cmdline options
    """
    return ". Specified as %r" % s

def _EXAMPLE(s):
    """Helper to provide uniform appearance for examples in cmdline options
    """
    return ", e.g. %r" % s

_DEF = "\n[Default: %default]"


# TODO: try to make groups definition somewhat lazy, since now
# whenever a group is created, those parameters are already known by
# parser, although might not be listed in the list of used and not by
# --help. But their specification on cmdline doesn't lead to
# error/help msg.
#
# Conflict hanlder to resolve situation that we have the same option added
# to some group and also available 'freely'
#
# set default version string, otherwise '--version' option is not enabled
# can be overwritten later on by assigning to `parser.version`
parser = OptionParser(version=mvpa2.__version__, # "%prog"
                      add_help_option=False,
                      conflict_handler="error")


opt = Options()
opts = OptionGroups(parser)


#
# Callbacks to tune up the output or cause specific actions to be
# taken
#
def _verbose_callback(option, optstr, value, parser):
    """Callback for -v|--verbose cmdline option
    """
    if __debug__:
        debug("CMDLINE", "Setting verbose.level to %s" % str(value))
    verbose.level = value
    optstr = optstr                     # pylint shut up
    setattr(parser.values, option.dest, value)

def _split_comma_semicolon_lists_callback(option, optstr, value, parser):
    """Callback to split provided values
    """
    if value is None:
        return None
    if __debug__:
        debug("CMDLINE", "Splitting %s for %s" % (value, optstr))
    value_split = split_comma_semicolon_lists(value)
    setattr(parser.values, option.dest, value_split)


opt.help = \
    Option("-h", "--help", "--sos",
           action="help",
           help="Show this help message and exit")

opt.verbose = \
    Option("-v", "--verbose", "--verbosity",
           action="callback", callback=_verbose_callback, nargs=1,
           type="int", dest="verbose", default=0,
           help="Verbosity level of output" + _DEF)
"""Pre-cooked `optparse`'s option to specify verbose level"""

commonopts_list = [opt.verbose, opt.help]

if __debug__:
    from mvpa2.base import debug

    ##REF: Name was automagically refactored
    def _debug_callback(option, optstr, value, parser):
        """Callback for -d|--debug cmdline option
        """
        if value == "list":
            print "Registered debug IDs:"
            keys = debug.registered.keys()
            keys.sort()
            for k in keys:
                print "%-7s: %s" % (k, debug.registered[k])
            print "Use ALL: to enable all of the debug IDs listed above."
            print "Use python regular expressions to select group. CLF.* will" \
              " enable all debug entries starting with CLF (e.g. CLFBIN, CLFMC)"
            raise SystemExit, 0

        optstr = optstr                     # pylint shut up
        debug.set_active_from_string(value)

        setattr(parser.values, option.dest, value)


    optDebug = Option("-d", "--debug",
                      action="callback", callback=_debug_callback,
                      nargs=1,
                      type="string", dest="debug", default="",
                      help="Debug entries to report. "
                      "Run with '-d list' to get a list of "
                      "registered entries" + _DEF)

    commonopts_list.append(optDebug)

opts.add("common", commonopts_list, "Common generic options")

#
# Classifiers options
#
opt.clf = \
    Option("--clf",
           type="choice", dest="clf",
           choices=['gnb', 'knn', 'svm', 'ridge', 'gpr', 'smlr'], default='svm',
           help="Type of classifier to be used" + _DEF)

opt.radius = \
    Option("-r", "--radius",
           action="store", type="float", dest="radius",
           default=2.0,
           help="Radius to be used (eg for the searchlight)" + _DEF)


opt.knearestdegree = \
    Option("-k", "--k-nearest",
           action="store", type="int", dest="knearestdegree", default=3,
           help="Degree of k-nearest classifier" + _DEF)

opts.add('KNN', [opt.radius, opt.knearestdegree], "Specification of kNN")


opt.svm_C = \
    Option("-C", "--svm-C",
           action="store", type="float", dest="svm_C", default=-1.0,
           help="C parameter for soft-margin C-SVM classification" + _DEF)

opt.svm_nu = \
    Option("--nu", "--svm-nu",
           action="store", type="float", dest="svm_nu", default=0.1,
           help="nu parameter for soft-margin nu-SVM classification" + _DEF)

opt.svm_gamma = \
    Option("--gamma", "--svm-gamma",
           action="store", type="float", dest="svm_gamma", default=1.0,
           help="gamma parameter for Gaussian kernel of RBF SVM" + _DEF)

opts.add('SVM', [opt.svm_nu, opt.svm_C, opt.svm_gamma], "SVM specification")

opt.do_sweep = \
             Option("--sweep",
                    action="store_true", dest="do_sweep",
                    default=False,
                    help="Sweep through various classifiers" + _DEF)

# Cross-validation options

opt.crossfolddegree = \
    Option("-c", "--crossfold",
           action="store", type="float", dest="crossfolddegree", default=1,
           help="Degree of N-fold crossfold" + _DEF)

opts.add('general', [opt.crossfolddegree], "Generalization estimates")


# preprocess options

opt.zscore = \
    Option("--zscore",
           action="store_true", dest="zscore", default=False,
           help="zscore dataset samples" + _DEF)

opt.mean_group_sample = \
    Option("--mean-group-sample", default=False,
           action="store_true", dest="mean_group_sample",
           help="Collapse samples in each group (chunks and samples, "
           "or specify --chunks-sa, and --targets-sa)" + _DEF)

opt.baseline_conditions = \
    Option('-b', "--baseline-conditions",
           action="callback", nargs=1, type="string", default="",
           callback=_split_comma_semicolon_lists_callback,
           dest="baseline_conditions",
           help="Baseline conditions (used for zscoring)"
                + _FORMAT("sa:value1,value2,...")
                + _EXAMPLE('targets:rest') + _DEF)

opt.exclude_conditions = \
    Option('-e', "--exclude-conditions",
           action="callback", nargs=1, type="string", default="",
           callback=_split_comma_semicolon_lists_callback,
           dest="exclude_conditions",
           help="Which conditions to exclude from the analysis "
                "(but would be present during preprocessing (e.g. zscoring)"
                + _FORMAT("sa1:value1,value2,...;sa2:value1,value2,...")
                + _EXAMPLE('targets:rest;trials:bad') + _DEF)

opt.include_conditions = \
    Option('-i', "--include-conditions",
           action="callback", nargs=1, type="string", default="",
           callback=_split_comma_semicolon_lists_callback,
           dest="include_conditions",
           help="Which conditions exclusively to analyze "
                "(but all would be present during preprocessing (e.g. zscoring)"
                + _FORMAT("sa1:value1,value2,...;sa2:value1,value2,...")
                + _EXAMPLE('targets:rest;trials:bad') + _DEF)

opt.targets_sa = \
    Option('-T', "--targets-sa",
           action="store", dest="targets_sa", default="targets",
           help="Which sample attribute would be used for (classification)"
                " analysis" + _DEF)

opt.chunks_sa = \
    Option("--chunks-sa",
           action="store", dest="chunks_sa", default="chunks",
           help="Which sample attribute would be used to describe"
                "samples grouping information for partitioning" + _DEF)


opt.tr = \
    Option("--tr",
           action="store", dest="tr", default=2.0, type='float',
           help="fMRI volume repetition time" + _DEF)

opt.detrend = \
    Option("--detrend",
           action="store_true", dest="detrend", default=0,
           help="Do linear detrending" + _DEF)

opts.add('preproc', [opt.zscore, opt.tr, opt.detrend], "Preprocessing options")


# Wavelets options
if externals.exists('pywt'):
    import pywt
    ##REF: Name was automagically refactored
    def _wavelet_family_callback(option, optstr, value, parser):
        """Callback for -w|--wavelet-family cmdline option
        """
        wl_list = pywt.wavelist()
        wl_list_str = ", ".join(
                ['-1: None'] + ['%d:%s' % w for w in enumerate(wl_list)])
        if value == "list":
            print "Available wavelet families: " + wl_list_str
            raise SystemExit, 0

        wl_family = value
        try:
            # may be int? ;-)
            wl_family_index = int(value)
            if wl_family_index >= 0:
                try:
                    wl_family = wl_list[wl_family_index]
                except IndexError:
                    print "Index is out of range. " + \
                          "Following indexes with names are known: " + \
                          wl_list_str
                    raise SystemExit, -1
            else:
                wl_family = 'None'
        except ValueError:
            pass
        # Check the value
        wl_family = wl_family.lower()
        if wl_family == 'none':
            wl_family = None
        elif not wl_family in wl_list:
            print "Uknown family '%s'. Known are %s" % (wl_family, ', '.join(wl_list))
            raise SystemExit, -1
        # Store it in the parser
        setattr(parser.values, option.dest, wl_family)


    opt.wavelet_family = \
            Option("-w", "--wavelet-family", callback=_wavelet_family_callback,
                   action="callback", type="string", dest="wavelet_family",
                   default='-1',
                   help="Wavelet family: string or index among the available. "
                   "Run with '-w list' to see available families" + _DEF)

    opt.wavelet_decomposition = \
            Option("-W", "--wavelet-decomposition",
                   action="store", type="choice", dest="wavelet_decomposition",
                   default='dwt', choices=['dwt', 'dwp'],
                   help="Wavelet decomposition: discrete wavelet transform "
                   "(dwt) or packet (dwp)" + _DEF)

    opts.add('wavelet', [opt.wavelet_family, opt.wavelet_decomposition],
             "Wavelets mappers")


# Box options

opt.boxlength = \
    Option("--boxlength",
           action="store", dest="boxlength", default=1, type='int',
           help="Length of the box in volumes (integer)" + _DEF)

opt.boxoffset = \
    Option("--boxoffset",
           action="store", dest="boxoffset", default=0, type='int',
           help="Offset of the box from the event onset in volumes" + _DEF)

opts.add('box', [opt.boxlength, opt.boxoffset], "Box options")


# sample attributes

opt.chunk = \
    Option("--chunk",
           action="store", dest="chunk", default='0',
           help="Id of the data chunk" + _DEF)

opt.chunkLimits = \
    Option("--chunklimits",
           action="store", dest="chunklimits", default=None,
           help="Limit processing to a certain chunk of data given by start " \
                "and end volume number (including lower, excluding upper " \
                "limit). Numbering starts with zero" + _DEF)

opts.add('chunk', [opt.chunk, opt.chunkLimits],
         "Chunk options AKA Sample attributes XXX")


########NEW FILE########
__FILENAME__ = data_generators
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Miscellaneous data generators for unittests and demos"""

__docformat__ = 'restructuredtext'

import os
import numpy as np

from mvpa2.base import externals
from mvpa2.datasets.base import dataset_wizard, Dataset
from mvpa2 import pymvpa_dataroot, pymvpa_datadbroot
from mvpa2.misc.fx import get_random_rotation
from mvpa2.base.dataset import vstack

if __debug__:
    from mvpa2.base import debug

##REF: Name was automagically refactored
def multiple_chunks(func, n_chunks, *args, **kwargs):
    """Replicate datasets multiple times raising different chunks

    Given some randomized (noisy) generator of a dataset with a single
    chunk call generator multiple times and place results into a
    distinct chunks.

    Returns
    -------
    ds : `mvpa2.datasets.base.Dataset`
    """
    dss = []
    for chunk in xrange(n_chunks):
        ds_ = func(*args, **kwargs)
        # might not have chunks at all
        if not ds_.sa.has_key('chunks'):
            ds_.sa['chunks'] = np.repeat(chunk + 1, ds_.nsamples)
        else:
            ds_.sa.chunks[:] = chunk + 1
        dss.append(ds_)

    return vstack(dss)


##REF: Name was automagically refactored
def dumb_feature_dataset():
    """Create a very simple dataset with 2 features and 3 labels
    """
    data = [[1, 0], [1, 1], [2, 0], [2, 1], [3, 0], [3, 1], [4, 0], [4, 1],
            [5, 0], [5, 1], [6, 0], [6, 1], [7, 0], [7, 1], [8, 0], [8, 1],
            [9, 0], [9, 1], [10, 0], [10, 1], [11, 0], [11, 1], [12, 0],
            [12, 1]]
    regs = ([1] * 8) + ([2] * 8) + ([3] * 8)

    return dataset_wizard(samples=np.array(data), targets=regs, chunks=range(len(regs)))


##REF: Name was automagically refactored
def dumb_feature_binary_dataset():
    """Very simple binary (2 labels) dataset
    """
    data = [[1, 0], [1, 1], [2, 0], [2, 1], [3, 0], [3, 1], [4, 0], [4, 1],
            [5, 0], [5, 1], [6, 0], [6, 1], [7, 0], [7, 1], [8, 0], [8, 1],
            [9, 0], [9, 1], [10, 0], [10, 1], [11, 0], [11, 1], [12, 0],
            [12, 1]]
    regs = ([0] * 12) + ([1] * 12)

    return dataset_wizard(samples=np.array(data), targets=regs, chunks=range(len(regs)))



def normal_feature_dataset(perlabel=50, nlabels=2, nfeatures=4, nchunks=5,
                         means=None, nonbogus_features=None, snr=3.0,
                         normalize=True):
    """Generate a univariate dataset with normal noise and specified means.

    Could be considered to be a generalization of
    `pure_multivariate_signal` where means=[ [0,1], [1,0] ].

    Specify either means or `nonbogus_features` so means get assigned
    accordingly.  If neither `means` nor `nonbogus_features` are
    provided, data will be pure noise and no per-label information.

    Parameters
    ----------
    perlabel : int, optional
      Number of samples per each label
    nlabels : int, optional
      Number of labels in the dataset
    nfeatures : int, optional
      Total number of features (including bogus features which carry
      no label-related signal)
    nchunks : int, optional
      Number of chunks (perlabel should be multiple of nchunks)
    means : None or ndarray of (nlabels, nfeatures) shape
      Specified means for each of features (columns) for all labels (rows).
    nonbogus_features : None or list of int
      Indexes of non-bogus features (1 per label).
    snr : float, optional
      Signal-to-noise ration assuming that signal has std 1.0 so we
      just divide random normal noise by snr
    normalize : bool, optional
      Divide by max(abs()) value to bring data into [-1, 1] range.
    """

    data = np.random.standard_normal((perlabel*nlabels, nfeatures))
    if snr != 0:
        data /= np.sqrt(snr)
    if (means is None) and (not nonbogus_features is None):
        if len(nonbogus_features) > nlabels:
            raise ValueError, "Can't assign simply a feature to a " + \
                  "class: more nonbogus_features than labels"
        means = np.zeros((len(nonbogus_features), nfeatures))
        # pure multivariate -- single bit per feature
        for i in xrange(len(nonbogus_features)):
            means[i, nonbogus_features[i]] = 1.0
    if not means is None and snr != 0:
        # add mean
        data += np.repeat(np.array(means, ndmin=2), perlabel, axis=0)
    if normalize:
        # bring it 'under 1', since otherwise some classifiers have difficulties
        # during optimization
        data = 1.0/(np.max(np.abs(data))) * data
    labels = np.concatenate([np.repeat('L%d' % i, perlabel)
                                for i in range(nlabels)])
    chunks = np.concatenate([np.repeat(range(nchunks),
                                     perlabel//nchunks) for i in range(nlabels)])
    ds = dataset_wizard(data, targets=labels, chunks=chunks)

    # If nonbogus was provided -- assign .a and .fa accordingly
    if nonbogus_features is not None:
        ds.fa['nonbogus_targets'] = np.array([None]*nfeatures)
        ds.fa.nonbogus_targets[nonbogus_features] = ['L%d' % i for i in range(nlabels)]
        ds.a['nonbogus_features'] = nonbogus_features
        ds.a['bogus_features'] = [x for x in range(nfeatures)
                                  if not x in nonbogus_features]


    return ds

##REF: Name was automagically refactored
def pure_multivariate_signal(patterns, signal2noise = 1.5, chunks=None, targets=[0, 1]):
    """ Create a 2d dataset with a clear multivariate signal, but no
    univariate information.

    ::

      %%%%%%%%%
      % O % X %
      %%%%%%%%%
      % X % O %
      %%%%%%%%%
    """

    # start with noise
    data = np.random.normal(size=(4*patterns, 2))

    # add signal
    data[:2*patterns, 1] += signal2noise

    data[2*patterns:4*patterns, 1] -= signal2noise
    data[:patterns, 0] -= signal2noise
    data[2*patterns:3*patterns, 0] -= signal2noise
    data[patterns:2*patterns, 0] += signal2noise
    data[3*patterns:4*patterns, 0] += signal2noise

    # two conditions
    regs = np.array((targets[0:1] * patterns) + (targets[1:2] * 2 * patterns) + (targets[0:1] * patterns))

    if chunks is None:
        chunks = range(len(data))
    return dataset_wizard(samples=data, targets=regs, chunks=chunks)


##REF: Name was automagically refactored
def get_mv_pattern(s2n):
    """Simple multivariate dataset"""
    return multiple_chunks(pure_multivariate_signal, 6,
                          5, s2n, 1)


def wr1996(size=200):
    """Generate '6d robot arm' dataset (Williams and Rasmussen 1996)

    Was originally created in order to test the correctness of the
    implementation of kernel ARD.  For full details see:
    http://www.gaussianprocess.org/gpml/code/matlab/doc/regression.html#ard

    x_1 picked randomly in [-1.932, -0.453]
    x_2 picked randomly in [0.534, 3.142]
    r_1 = 2.0
    r_2 = 1.3
    f(x_1,x_2) = r_1 cos (x_1) + r_2 cos(x_1 + x_2) + N(0,0.0025)
    etc.

    Expected relevances:
    ell_1      1.804377
    ell_2      1.963956
    ell_3      8.884361
    ell_4     34.417657
    ell_5   1081.610451
    ell_6    375.445823
    sigma_f    2.379139
    sigma_n    0.050835
    """
    intervals = np.array([[-1.932, -0.453], [0.534, 3.142]])
    r = np.array([2.0, 1.3])
    x = np.random.rand(size, 2)
    x *= np.array(intervals[:, 1]-intervals[:, 0])
    x += np.array(intervals[:, 0])
    if __debug__:
        for i in xrange(2):
            debug('DG', '%d columnt Min: %g Max: %g' %
                  (i, x[:, i].min(), x[:, i].max()))
    y = r[0]*np.cos(x[:, 0] + r[1]*np.cos(x.sum(1))) + \
        np.random.randn(size)*np.sqrt(0.0025)
    y -= y.mean()
    x34 = x + np.random.randn(size, 2)*0.02
    x56 = np.random.randn(size, 2)
    x = np.hstack([x, x34, x56])
    return dataset_wizard(samples=x, targets=y)


##REF: Name was automagically refactored
def sin_modulated(n_instances, n_features,
                  flat=False, noise=0.4):
    """ Generate a (quite) complex multidimensional non-linear dataset

    Used for regression testing. In the data label is a sin of a x^2 +
    uniform noise
    """
    if flat:
        data = (np.arange(0.0, 1.0, 1.0/n_instances)*np.pi)
        data.resize(n_instances, n_features)
    else:
        data = np.random.rand(n_instances, n_features)*np.pi
    label = np.sin((data**2).sum(1)).round()
    label += np.random.rand(label.size)*noise
    return dataset_wizard(samples=data, targets=label)

##REF: Name was automagically refactored
def chirp_linear(n_instances, n_features=4, n_nonbogus_features=2,
                data_noise=0.4, noise=0.1):
    """ Generates simple dataset for linear regressions

    Generates chirp signal, populates n_nonbogus_features out of
    n_features with it with different noise level and then provides
    signal itself with additional noise as labels
    """
    x = np.linspace(0, 1, n_instances)
    y = np.sin((10 * np.pi * x **2))

    data = np.random.normal(size=(n_instances, n_features ))*data_noise
    for i in xrange(n_nonbogus_features):
        data[:, i] += y[:]

    labels = y + np.random.normal(size=(n_instances,))*noise

    return dataset_wizard(samples=data, targets=labels)


def linear_awgn(size=10, intercept=0.0, slope=0.4, noise_std=0.01, flat=False):
    """Generate a dataset from a linear function with AWGN
    (Added White Gaussian Noise).

    It can be multidimensional if 'slope' is a vector. If flat is True
    (in 1 dimesion) generate equally spaces samples instead of random
    ones. This is useful for the test phase.
    """
    dimensions = 1
    if isinstance(slope, np.ndarray):
        dimensions = slope.size

    if flat and dimensions == 1:
        x = np.linspace(0, 1, size)[:, np.newaxis]
    else:
        x = np.random.rand(size, dimensions)

    y = np.dot(x, slope)[:, np.newaxis] \
        + (np.random.randn(*(x.shape[0], 1)) * noise_std) + intercept

    return dataset_wizard(samples=x, targets=y)


def noisy_2d_fx(size_per_fx, dfx, sfx, center, noise_std=1):
    """Yet another generator of random dataset

    """
    # used in projection example
    x = []
    y = []
    labels = []
    for fx in sfx:
        nx = np.random.normal(size=size_per_fx)
        ny = fx(nx) + np.random.normal(size=nx.shape, scale=noise_std)
        x.append(nx)
        y.append(ny)

        # whenever larger than first function value
        labels.append(np.array(ny < dfx(nx), dtype='int'))

    samples = np.array((np.hstack(x), np.hstack(y))).squeeze().T
    labels = np.hstack(labels).squeeze().T

    samples += np.array(center)

    return dataset_wizard(samples=samples, targets=labels)


def linear1d_gaussian_noise(size=100, slope=0.5, intercept=1.0,
                            x_min=-2.0, x_max=3.0, sigma=0.2):
    """A straight line with some Gaussian noise.
    """
    x = np.linspace(start=x_min, stop=x_max, num=size)
    noise = np.random.randn(size)*sigma
    y = x * slope + intercept + noise
    return dataset_wizard(samples=x[:, None], targets=y)


def load_example_fmri_dataset(name='1slice', literal=False):
    """Load minimal fMRI dataset that is shipped with PyMVPA."""
    from mvpa2.datasets.mri import fmri_dataset
    from mvpa2.misc.io import SampleAttributes

    dspath, mask = {
        '1slice': (pymvpa_dataroot, 'mask.nii.gz'),
        '25mm': (os.path.join(
            pymvpa_dataroot,'tutorial_data_25mm', 'data'), 'mask_brain.nii.gz')
    }[name]

    if literal:
        attr = SampleAttributes(os.path.join(dspath, 'attributes_literal.txt'))
    else:
        attr = SampleAttributes(os.path.join(dspath, 'attributes.txt'))
    ds = fmri_dataset(samples=os.path.join(dspath, 'bold.nii.gz'),
                      targets=attr.targets, chunks=attr.chunks,
                      mask=os.path.join(dspath, mask))

    return ds

def load_datadb_tutorial_data(path=os.path.join(
      pymvpa_datadbroot, 'tutorial_data', 'tutorial_data', 'data'),
    roi='brain'):
    """Loads the block-design demo dataset from PyMVPA dataset DB.

    Parameters
    ----------
    path : str
      Path of the directory containing the dataset files.
    roi : str or int or tuple or None
      Region Of Interest to be used for masking the dataset. If a string is
      given a corresponding mask image from the demo dataset will be used
      (mask_<str>.nii.gz). If an int value is given, the corresponding ROI
      is determined from the atlas image (mask_hoc.nii.gz). If a tuple is
      provided it may contain int values that a processed as explained
      before, but the union of a ROIs is taken to produce the final mask.
      If None, no masking is performed.
    """
    import nibabel as nb
    from mvpa2.datasets.mri import fmri_dataset
    from mvpa2.misc.io import SampleAttributes
    if roi is None:
        mask = None
    elif isinstance(roi, str):
        mask = os.path.join(path, 'mask_' + roi + '.nii.gz')
    elif isinstance(roi, int):
        nimg = nb.load(os.path.join(path, 'mask_hoc.nii.gz'))
        tmpmask = nimg.get_data() == roi
        mask = nb.Nifti1Image(tmpmask.astype(int), nimg.get_affine(),
                              nimg.get_header())
    elif isinstance(roi, tuple) or isinstance(roi, list):
        nimg = nb.load(os.path.join(path, 'mask_hoc.nii.gz'))
        if externals.versions['nibabel'] >= '1.2':
            img_shape = nimg.shape
        else:
            img_shape = nimg.get_shape()
        tmpmask = np.zeros(img_shape, dtype='bool')
        for r in roi:
            tmpmask = np.logical_or(tmpmask, nimg.get_data() == r)
        mask = nb.Nifti1Image(tmpmask.astype(int), nimg.get_affine(),
                              nimg.get_header())
    else:
        raise ValueError("Got something as mask that I cannot handle.")
    attr = SampleAttributes(os.path.join(path, 'attributes.txt'))
    ds = fmri_dataset(samples=os.path.join(path, 'bold.nii.gz'),
                      targets=attr.targets, chunks=attr.chunks,
                      mask=mask)
    return ds


load_datadb_demo_blockfmri = load_datadb_tutorial_data
"""For backward compatibility with tutorial_lib which people might be
   "using" already.  Deprecate entirely whenever tutorial_data gets updated.
"""

def autocorrelated_noise(ds, sr, cutoff, lfnl=3.0, bord=10, hfnl=None):
    """Generate a dataset with samples being temporally autocorrelated noise.

    Parameters
    ----------
    ds : Dataset
      Source dataset whose mean samples serves as the pedestal of the new noise
      samples. All attributes of this dataset will also go into the generated
      one.
    sr : float
      Sampling rate (in Hz) of the samples in the dataset.
    cutoff : float
      Cutoff frequency of the low-pass butterworth filter.
    bord : int
      Order of the butterworth filter that is applied for low-pass
      filtering.
    lfnl : float
      Low frequency noise level in percent signal (per feature).
    hfnl : float or None
      High frequency noise level in percent signal (per feature). If None, no
      HF noise is added.
    """
    from scipy.signal import butter, lfilter

    # something to play with
    fds = ds.copy(deep=False)

    # compute the pedestal
    msample = fds.samples.mean(axis=0)

    # noise/signal amplitude relative to each feature mean signal
    noise_amps = msample * (lfnl / 100.)

    # generate gaussian noise for the full dataset
    nsamples = np.random.standard_normal(fds.samples.shape)
    # scale per each feature
    nsamples *= noise_amps

    # nyquist frequency
    nf = sr / 2.0

    # along samples low-pass filtering
    fb, fa = butter(bord, cutoff / nf)
    nsamples = lfilter(fb, fa, nsamples, axis=0)

    # add the pedestal
    nsamples += msample

    # HF noise
    if not hfnl is None:
        noise_amps = msample * (hfnl / 100.)
        nsamples += np.random.standard_normal(nsamples.shape) * noise_amps

    fds.samples = nsamples
    return fds


def random_affine_transformation(ds, scale_fac=100., shift_fac=10.):
    """Distort a dataset by random scale, shift, and rotation.

    The original data samples are transformed by applying a random rotation,
    shifting by a random vector (randomly selected, scaled input sample), and
    scaled by a random factor (randomly selected input feature values, scaled
    by an additional factor). The effective transformation values are stored in
    the output dataset's attribute collection as 'random_rotation',
    'random_shift', and 'random_scale' respectively.

    Parameters
    ----------
    ds : Dataset
      Input dataset. Its sample and features attributes will be assigned to the
      output dataset.
    scale_fac : float
      Factor by which the randomly selected value for data scaling is scaled
      itself.
    shift_fac : float
      Factor by which the randomly selected shift vector is scaled.
    """
    rndidx = np.random.randint
    R = get_random_rotation(ds.nfeatures)
    samples = ds.samples
    # reusing random data from dataset itself
    random_scale = samples[rndidx(len(ds)), rndidx(ds.nfeatures)] * scale_fac
    random_shift = samples[rndidx(len(ds))] * shift_fac
    samples = np.dot(samples, R) * random_scale \
              + random_shift
    return Dataset(samples, sa=ds.sa, fa=ds.fa,
                   a={'random_rotation': R,
                      'random_scale': random_scale,
                      'random_shift': random_shift})

########NEW FILE########
__FILENAME__ = dcov
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Compute dcov/dcorr measures for independence testing

References
----------

http://en.wikipedia.org/wiki/Distance_covariance

"""

"""
TODO: consider use of  numexpr to speed all those up -- there is plenty of temp storage
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import warning, externals

if externals.exists('cran-energy'):
    import rpy2.robjects
    import rpy2.robjects.numpy2ri
    if hasattr(rpy2.robjects.numpy2ri,'activate'):
        rpy2.robjects.numpy2ri.activate()
    RRuntimeError = rpy2.robjects.rinterface.RRuntimeError
    r = rpy2.robjects.r
    r.library('energy')


def _euclidean_distances(x, uv):
    """Compute euclidean distances for samples in columns

    Helper function for dcov computations

    Parameters
    ----------

    uv : bool, optional
      if True, then for each observable distance computed separately
      from the others.  If not -- then distances computed for
      multivariate patterns and output as observable == 1

    output observable x sample x sample
    """
    # TODO: could possibly be optimized to not compute the same i,j
    # and i,i distance twice but I wanted to avoid any explicit Python
    # loop here
    dx = x[:, None, :] - x[:, :, None]
    if uv:
        return np.sqrt(np.square(dx))
    else:
        return np.sqrt(np.sum(np.square(dx), axis=0))[None,:]


def _Aij(d):
    """Given distances matrix observable x sample x sample
    return normalized one where means get subtracted
    """
    mean_i = np.mean(d, axis=1)
    mean_j = np.mean(d, axis=2)
    mean_ij = np.mean(mean_i, axis=1)
    # ain't broadcasting is cool?
    return d - mean_i[:, None] - mean_j[:, :, None] + mean_ij[:, None, None]

if externals.exists('cran-energy'):
    def dCOV_R(x, y, uv=False, all_est=True):
        """Implementation of dCOV interfaced to original R energy library -- used primarily for testing
        """
        # trust no one!
        if uv:
            N = len(x)
            M = len(y)
            dCovs = np.zeros((N, M))
            dCors = np.zeros((N, M))
            Varx = np.zeros((N,))
            Vary = np.zeros((M,))
            for ix, x_ in enumerate(x):
                for iy, y_ in enumerate(y):
                    out = r.DCOR(x_, y_)
                    #outr = r.dcor(x_, y_)
                    #outv = r.dcov(x_, y_)
                    dCovs[ix, iy] = out[0][0]
                    dCors[ix, iy] = out[1][0]
                    Varx[ix] = out[2][0]
                    Vary[iy] = out[3][0]
            outputs = dCovs, dCors, Varx, Vary
        else:
            out = r.DCOR(x.T, y.T)
            outputs = tuple([o[0] for o in out])

        if not all_est:
            outputs = outputs[:1]

        if uv:
            return outputs
        else:
            # return corresponding scalars if it was a multivariate estimate
            return tuple(np.asscalar(np.asanyarray(x)) for x in outputs)


def dCOV(x, y, rowvar=1, uv=False, all_est=True):
    """Estimate dCov measure(s) between x and y.  Allows uni- or multi-variate estimations

    Name dCOV was chosen to match implementation in R energy toolbox:
    http://cran.r-project.org/web/packages/energy/index.html

    Parameters
    ----------
    rowvar : int, optional
        If `rowvar` is 1 (default), then each row represents a
        variable, with observations in the columns.  If 0, the relationship
        is transposed: each column represents a variable, while the rows
        contain observations.
    uv : bool, optional
        dCov is a multivariate measure of dependence so it would
        produce a single estimate for two matrices NxT and MxT.
        With uv=True (univariate estimation) it will return estimates
        for every pair of variables from x and y, thus NxM matrix,
        somewhat similar to what numpy.corrcoef does besides not estimating
        within x or y
    all_est : bool, True
        Since majority of computation of dCor(x,y), dVar(x) and
        dVar(y) is spend while estimating dVar(x, y) it makes sense to
        estimate all of them at the same time if any of the later is
        necessary.  So output would then consist of dCov, dCor, dVar(x),
        dVar(y) tuple, matching the order of energy toolbox dCOV output
        in R.

    """
    # Assure that we have correct dimensionality
    x = np.atleast_2d(x)
    y = np.atleast_2d(y)
    if rowvar == 0:
        # operate on transposes
        x = x.T
        y = y.T
    elif rowvar == 1:
        pass                            # default mode
    else:
        raise ValueError("rowvar must be either 0 (samples are rows) "
                         "or 1 (observables are rows). Got %d" % rowvar)

    # number of samples
    nsamples = x.shape[1]
    assert(nsamples == y.shape[1])

    if nsamples < 3:
        warning("You are trying to estimate dCov on %d sample(s). "
                "Please verify correctness of input" % nsamples)
    Dx = _euclidean_distances(x, uv=uv)
    Dy = _euclidean_distances(y, uv=uv)

    N, M = len(Dx), len(Dy)
    # .reshape is here to combine TxT into a single T**2 dimension to ease sums
    Ax = _Aij(Dx).reshape((N, -1))
    Ay = _Aij(Dy).reshape((M, -1))

    # and once again use cool broadcasting although at the cost of
    # memory since per se temporary storage is not necessary
    Axy = Ax[:, None] * Ay[None, :]
    dCov = np.sqrt(np.mean(Axy, axis=2))

    if not all_est:
        outputs = (dCov,)
    else:
        # if all estimates were requested -- be so
        dVar_x = np.sqrt(np.mean(np.square(Ax), axis=1))
        dVar_y = np.sqrt(np.mean(np.square(Ay), axis=1))
        dVar_xy = np.sqrt(dVar_x[:, None] * dVar_y[None, :])
        dCor = np.zeros(shape=dCov.shape)
        # So that we do not / 0.  R's dCOV seems to return 0s for
        # those cases, so we will
        dVar_xy_nz = dVar_xy.nonzero()
        dCor[dVar_xy_nz] = dCov[dVar_xy_nz] / dVar_xy[dVar_xy_nz]

        outputs = dCov, dCor, dVar_x, dVar_y

    if uv:
        return outputs
    else:
        # return corresponding scalars if it was a multivariate estimate
        return tuple(np.asscalar(x) for x in outputs)


def dcorcoef(x, y,  rowvar=1, uv=False):
    """Return dCor coefficient(s) only (convenience function).

    See :func:`dCOV` for more information
    """
    _, dCor, _, _ = dCOV(x, y, rowvar=rowvar, uv=uv, all_est=True)
    return dCor



########NEW FILE########
__FILENAME__ = errorfx
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Error functions helpers.

PyMVPA can use arbitrary function which takes 2 arguments: predictions
and targets and spits out a scalar value. Functions below are for the
convinience, and they confirm the agreement that 'smaller' is 'better'"""

__docformat__ = 'restructuredtext'


import numpy as np
from numpy import trapz

from mvpa2.base import externals

# Various helper functions
##REF: Name was automagically refactored
def mean_power_fx(data):
    """Returns mean power

    Similar to var but without demeaning
    """
    return np.mean(np.asanyarray(data)**2)

##REF: Name was automagically refactored
def root_mean_power_fx(data):
    """Returns root mean power

    to be comparable against RMSE
    """
    return np.sqrt(mean_power_fx(data))


def rms_error(predicted, target):
    """Computes the root mean squared error of some target and some
    predicted values.

    Both 'predicted' and 'target' can be either scalars or sequences,
    but have to be of the same length.
    """
    return np.sqrt(np.mean(np.subtract(predicted, target)**2))


def mean_mismatch_error(predicted, target):
    """Computes the percentage of mismatches between some target and some
    predicted values.
    Both 'predicted' and 'target' can be either scalars or sequences,
    but have to be of the same length.
    """
    # XXX should we enforce consistent dimensionality and type?
    #     e.g. now lists would lead to incorrect results, and
    #     comparisons of arrays of different lengths would also
    #     tolerate input and produce some incorrect value as output
    #     error
    #  np.equal , np.not_equal -- prohibited -- those do not work on literal labels! uff
    #  so lets use == and != assuring dealing with arrays
    return np.mean( np.asanyarray(predicted) != target )


def mismatch_error(predicted, target):
    """Computes number of mismatches between some target and some
    predicted values.
    Both 'predicted' and 'target' can be either scalars or sequences,
    but have to be of the same length.
    """
    return np.sum( np.asanyarray(predicted) != target )


def match_accuracy(predicted, target):
    """Computes number of matches between some target and some
    predicted values.
    Both 'predicted' and 'target' can be either scalars or sequences,
    but have to be of the same length.
    """
    return np.sum( np.asanyarray(predicted) == target )

def mean_match_accuracy(predicted, target):
    """Computes mean of number of matches between some target and some
    predicted values.
    Both 'predicted' and 'target' can be either scalars or sequences,
    but have to be of the same length.
    """
    return np.mean( np.asanyarray(predicted) == target )


def auc_error(predicted, target):
    """Computes the area under the ROC for the given the
    target and predicted to make the prediction."""
    # sort the target in descending order based on the predicted and
    # set to boolean
    t = np.asanyarray(target)[np.argsort(predicted)[::-1]] > 0

    # calculate the true positives
    tp = np.concatenate(
        ([0], np.cumsum(t)/t.sum(dtype=np.float), [1]))

    # calculate the false positives
    fp = np.concatenate(
        ([0], np.cumsum(~t)/(~t).sum(dtype=np.float), [1]))

    return trapz(tp, fp)


if externals.exists('scipy'):
    from scipy.stats import pearsonr

    def correlation(predicted, target):
        """Computes the correlation between the target and the
        predicted values.

        In case of NaN correlation (no variance in predictors or
        targets) result output error is 0.
        """
        r = pearsonr(predicted, target)[0]
        if np.isnan(r):
            r = 0.0
        return r


    def corr_error_prob(predicted, target):
        """Computes p-value of correlation between the target and the predicted
        values.
        """
        return pearsonr(predicted, target)[1]

else:
    # slower(?) and bogus(p-value) implementations for non-scipy users
    # TODO: implement them more or less correcly with numpy
    #       functionality
    def correlation(predicted, target):
        """Computes the correlation between the target and the predicted
        values. Return 1-CC

        In case of NaN correlation (no variance in predictors or
        targets) result output error is 1.0.
        """
        l = len(predicted)
        r = np.corrcoef(np.reshape(predicted, l),
                       np.reshape(target, l))[0,1]
        if np.isnan(r):
            r = 0.0
        return r


    def corr_error_prob(predicted, target):
        """Computes p-value of correlation between the target and the predicted
        values.
        """
        from mvpa2.base import warning
        warning("p-value for correlation is implemented only when scipy is "
                "available. Bogus value -1.0 is returned otherwise")
        return -1.0


def corr_error(predicted, target):
    """Computes the correlation between the target and the
    predicted values. Resultant value is the 1 - correlation
    coefficient, so minimization leads to the best value (at 0).

    In case of NaN correlation (no variance in predictors or
    targets) result output error is 1.0.
    """
    return 1 - correlation(predicted, target)

def relative_rms_error(predicted, target):
    """Ratio between RMSE and root mean power of target output.

    So it can be considered as a scaled RMSE -- perfect reconstruction
    has values near 0, while no reconstruction has values around 1.0.
    Word of caution -- it is not commutative, ie exchange of predicted
    and target might lead to completely different answers
    """
    return rms_error(predicted, target) / root_mean_power_fx(target)


def variance_1sv(predicted, target):
    """Ratio of variance described by the first singular value component.

    Of limited use -- left for the sake of not wasting it
    """
    data = np.vstack( (predicted, target) ).T
    # demean
    data_demeaned = data - np.mean(data, axis=0)
    u, s, vh = np.linalg.svd(data_demeaned, full_matrices=0)
    # assure sorting
    s.sort()
    s=s[::-1]
    cvar = s[0]**2 / np.sum(s**2)
    return cvar

########NEW FILE########
__FILENAME__ = exceptions
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Exception classes which might get thrown"""

__docformat__ = 'restructuredtext'

class UnknownStateError(Exception):
    """Thrown if the internal state of the class is not yet defined.

    Classifiers and Algorithms classes might have properties, which
    are not defined prior to training or invocation has happened.
    """
    pass

class ConvergenceError(Exception):
    """Thrown if some algorithm does not converge to a solution.
    """
    pass

class InvalidHyperparameterError(Exception):
    """Generic exception to be raised when setting improper values
    as hyperparameters.
    """
    pass

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Tiny snippets to interface with FSL easily."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.misc.io import ColumnData
from mvpa2.misc.support import Event

if __debug__:
    from mvpa2.base import debug


class FslEV3(ColumnData):
    """IO helper to read FSL's EV3 files.

    This is a three-column textfile format that is used to specify stimulation
    protocols for fMRI data analysis in FSL's FEAT module.

    Data is always read as `float`.
    """
    def __init__(self, source):
        """Read and write FSL EV3 files.

        Parameters
        ----------
        source : str
          Filename of an EV3 file
        """
        # init data from known format
        ColumnData.__init__(self, source,
                            header=['onsets', 'durations', 'intensities'],
                            sep=None, dtype=float)

    @property
    def nevs(self):
        """Returns the number of EVs in the file.
        """
        return self.nrows


    ##REF: Name was automagically refactored
    def get_ev(self, evid):
        """Returns a tuple of (onset time, simulus duration, intensity) for a
        certain EV.
        """
        return (self['onsets'][evid],
                self['durations'][evid],
                self['intensities'][evid])


    def tofile(self, filename):
        """Write data to a FSL EV3 file.
        """
        ColumnData.tofile(self, filename,
                          header=False,
                          header_order=['onsets', 'durations', 'intensities'],
                          sep=' ')


    ##REF: Name was automagically refactored
    def to_events(self, **kwargs):
        """Convert into a list of `Event` instances.

        Parameters
        ----------
        kwargs
          Any keyword arugment provided would be replicated, through all
          the entries. Useful to specify label or even a chunk
        """
        return \
            [Event(onset=self['onsets'][i],
                   duration=self['durations'][i],
                   features=[self['intensities'][i]],
                   **kwargs)
             for i in xrange(self.nevs)]


    onsets = property(fget=lambda self: self['onsets'])
    durations = property(fget=lambda self: self['durations'])
    intensities = property(fget=lambda self: self['intensities'])



class McFlirtParams(ColumnData):
    """Read and write McFlirt's motion estimation parameters from and to text
    files.
    """
    header_def = ['rot1', 'rot2', 'rot3', 'x', 'y', 'z']

    def __init__(self, source):
        """Initialize McFlirtParams

        Parameters
        ----------
        source : str
          Filename of a parameter file.
        """
        ColumnData.__init__(self, source,
                            header=McFlirtParams.header_def,
                            sep=None, dtype=float)


    def tofile(self, filename):
        """Write motion parameters to file.
        """
        ColumnData.tofile(self, filename,
                          header=False,
                          header_order=McFlirtParams.header_def,
                          sep=' ')


    def plot(self):
        """Produce a simple plot of the estimated translation and rotation
        parameters using.

        You still need to can pylab.show() or pylab.savefig() if you want to
        see/get anything.
        """
        # import internally as it takes some time and might not be needed most
        # of the time
        import pylab as pl

        # translations subplot
        pl.subplot(211)
        pl.plot(self.x)
        pl.plot(self.y)
        pl.plot(self.z)
        pl.ylabel('Translations in mm')
        pl.legend(('x', 'y', 'z'), loc=0)

        # rotations subplot
        pl.subplot(212)
        pl.plot(self.rot1)
        pl.plot(self.rot2)
        pl.plot(self.rot3)
        pl.ylabel('Rotations in rad')
        pl.legend(('rot1', 'rot2', 'rot3'), loc=0)


    def toarray(self):
        """Returns the data as an array with six columns (same order as in file).
        """
        import numpy as np

        # return as array with time axis first
        return np.array([self[i] for i in McFlirtParams.header_def],
                       dtype='float').T


class FslGLMDesign(object):
    """Load FSL GLM design matrices from file.

    Be aware that such a desig matrix has its regressors in columns and the
    samples in its rows.
    """
    def __init__(self, source):
        """
        Parameters
        ----------
        source : filename
          Compressed files will be read as well, if their filename ends with
          '.gz'.
        """
        # XXX maybe load from array as well
        self._load_file(source)


    ##REF: Name was automagically refactored
    def _load_file(self, fname):
        """Helper function to load GLM definition from a file.
        """
        # header info
        nwaves = 0
        ntimepoints = 0
        matrix_offset = 0

        # open the file compressed or not
        if fname.endswith('.gz'):
            import gzip
            fh = gzip.open(fname, 'r')
        else:
            fh = open(fname, 'r')

        # read header
        for i, line in enumerate(fh):
            if line.startswith('/NumWaves'):
                nwaves = int(line.split()[1])
            if line.startswith('/NumPoints'):
                ntimepoints = int(line.split()[1])
            if line.startswith('/PPheights'):
                self.ppheights = [float(i) for i in line.split()[1:]]
            if line.startswith('/Matrix'):
                matrix_offset = i + 1

        # done with the header, now revert to NumPy's loadtxt for convenience
        fh.close()
        self.mat = np.loadtxt(fname, skiprows=matrix_offset)

        # checks
        if not self.mat.shape == (ntimepoints, nwaves):
            raise IOError, "Design matrix file '%s' did not contain expected " \
                           "matrix size (expected %s, got %s)" \
                           % (fname, str((ntimepoints, nwaves)), self.mat.shape)


    def plot(self, style='lines', **kwargs):
        """Visualize the design matrix.

        Parameters
        ----------
        style: 'lines', 'matrix'
        **kwargs:
          Additional arguments will be passed to the corresponding matplotlib
          plotting functions 'plot()' and 'pcolor()' for 'lines' and 'matrix'
          plots respectively.
        """
        # import internally as it takes some time and might not be needed most
        # of the time
        import pylab as pl

        if style == 'lines':
            # common y-axis
            yax = np.arange(0, self.mat.shape[0])
            axcenters = []
            col_offset = max(self.ppheights)

            # for all columns
            for i in xrange(self.mat.shape[1]):
                axcenter = i * col_offset
                pl.plot(self.mat[:, i] + axcenter, yax, **kwargs)
                axcenters.append(axcenter)

            pl.xticks(np.array(axcenters), range(self.mat.shape[1]))
        elif style == 'matrix':
            pl.pcolor(self.mat, **kwargs)
            ticks = np.arange(1, self.mat.shape[1]+1)
            pl.xticks(ticks - 0.5, ticks)
        else:
            raise ValueError, "Unknown plotting style '%s'" % style

        # labels and turn y-axis upside down
        pl.ylabel('Samples (top to bottom)')
        pl.xlabel('Regressors')
        pl.ylim(self.mat.shape[0],0)


def read_fsl_design(fsf_file):
    """Reads an FSL FEAT design.fsf file and return the content as a dictionary.

    Parameters
    ----------
    fsf_file : filename, file-like
    """
    # This function was originally contributed by Russell Poldrack

    if isinstance(fsf_file, basestring):
        infile = open(fsf_file, 'r')
    else:
        infile = fsf_file

    # target dict
    fsl = {}

    # loop over all lines
    for line in infile:
        line = line.strip()
        # if there is nothing on the line, do nothing
        if not line or line[0] == '#':
            continue

        # strip leading TCL 'set'
        key, value = line.split(None, 2)[1:]

        # fixup the 'y-' thing
        if value == 'y-':
            value = "y"

        # special case of variable keyword
        if line.count('_files('):
            # e.g. feat_files(1) -> feat_files
            key = key.split('(')[0]

        # decide which type we have for the value
        # int?
        if value.isdigit():
            fsl[key] = int(value)
        else:
            # float?
            try:
                fsl[key] = float(value)
            except ValueError:
                # must be string then, but...
                # sometimes there are quotes, sometimes not, but if the value
                # should be a string we remove them, since the value is already
                # of this type
                fsl[key] = value.strip('"')

    return fsl

########NEW FILE########
__FILENAME__ = flobs
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wrapper around FSLs halfcosbasis to generate HRF kernels"""

__docformat__ = 'restructuredtext'

import os
import tempfile
import shutil
import numpy as np
import math

##REF: Name was automagically refactored
def make_flobs(pre=0, rise=5, fall=5, undershoot=5, undershootamp=0.3, 
              nsamples=1, resolution=0.05, nsecs=-1, nbasisfns=2):
    """Wrapper around the FSL tool halfcosbasis.

    This function uses halfcosbasis to generate samples of HRF kernels.
    Kernel parameters can be modified analogous to the Make_flobs GUI
    which is part of FSL. 

    ::

       ^         /-\\
       |        /   \\
       1       /     \\
       |      /       \\
       |     /         \\
       |    /           \\
      -----/             \\     /-----  |
                          \\--/         |  undershootamp
      |    |      |     |        |
      |    |      |     |        |

       pre   rise  fall  undershoot

    Parameters 'pre', 'rise', 'fall', 'undershoot' and 'undershootamp'
    can be specified as 2-tuples (min-max range for sampling) and single 
    value (setting exact values -- no sampling).

    If 'nsec' is negative, the length of the samples is determined 
    automatically to include the whole kernel function (until it returns
    to baseline). 'nsec' has to be an integer value and is set to the next 
    greater integer value if it is not.

    All parameters except for 'nsamples' and 'nbasisfns' are in seconds.
    """
    # create tempdir and temporary parameter file
    pfile, pfilename = tempfile.mkstemp('pyflobs')
    wdir = tempfile.mkdtemp('pyflobs')

    # halfcosbasis can only handle >1 samples
    # so we simply compute two and later ignore the other
    if nsamples < 2:
        rnsamples = 2
    else:
        rnsamples = nsamples

    # make range tuples if not supplied
    if not isinstance(pre, tuple):
        pre = (pre, pre)
    if not isinstance(rise, tuple):
        rise = (rise, rise)
    if not isinstance(fall, tuple):
        fall = (fall, fall)
    if not isinstance(undershoot, tuple):
        undershoot = (undershoot, undershoot)
    if not isinstance(undershootamp, tuple):
        undershootamp = (undershootamp, undershootamp)

    # calc minimum length of hrf if not specified
    # looks like it has to be an integer
    if nsecs < 0:
        nsecs = int( math.ceil( pre[1] \
                                + rise[1] \
                                + fall[1] \
                                + undershoot[1] \
                                + resolution ) )
    else:
        nsecs = math.ceil(nsecs)

    # write parameter file
    pfile = os.fdopen( pfile, 'w' )

    pfile.write(str(pre[0]) + ' ' + str(pre[1]) + '\n')
    pfile.write(str(rise[0]) + ' ' + str(rise[1]) + '\n')
    pfile.write(str(fall[0]) + ' ' + str(fall[1]) + '\n')
    pfile.write(str(undershoot[0]) + ' ' + str(undershoot[1]) + '\n')
    pfile.write('0 0\n0 0\n')
    pfile.write(str(undershootamp[0]) + ' ' + str(undershootamp[1]) + '\n')
    pfile.write('0 0\n')

    pfile.close()

    # call halfcosbasis to generate the hrf samples
    tochild, fromchild, childerror = os.popen3('halfcosbasis' 
                   + ' --hf=' + pfilename
                   + ' --nbfs=' + str(nbasisfns)
                   + ' --ns=' + str(nsecs)
                   + ' --logdir=' + os.path.join(wdir, 'out')
                   + ' --nhs=' + str(rnsamples)
                   + ' --res=' + str(resolution) )
    err = childerror.readlines()
    if len(err) > 0:
        print err
        raise RuntimeError, "Problem while running halfcosbasis."

    # read samples from file into an array
    hrfs = np.fromfile( os.path.join( wdir, 'out', 'hrfsamps.txt' ),
                       sep = ' ' )

    # reshape array to get one sample per row and 1d array only
    # for one sample hrf
    hrfs = \
        hrfs.reshape( len(hrfs)/rnsamples, rnsamples).T[:nsamples].squeeze()

    # cleanup working dir (ignore errors)
    shutil.rmtree( wdir, True )
    # remove paramter file
    os.remove( pfilename )

    # and return an array
    return( hrfs )


########NEW FILE########
__FILENAME__ = melodic
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Wrapper around the output of MELODIC (part of FSL)"""

__docformat__ = 'restructuredtext'

import os
import numpy as np

from mvpa2.base import externals
if externals.exists('nibabel', raise_=True):
    import nibabel as nb


class MelodicResults( object ):
    """Easy access to MELODIC output.

    Only important information is available (important as judged by the
    author).
    """
    def __init__( self, path, fext='.nii.gz'):
        """Reads all information from the given MELODIC output path.
        """
        self._fext = fext
        rpath = None
        lookup = ['', 'filtered_func_data.ica']
        for lu in lookup:
            if os.path.exists(os.path.join(path, lu, 'melodic_IC' + fext)):
                rpath = os.path.join(path, lu)
                break
        if rpath is None:
            raise ValueError("Cannot find Melodic results at '%s'" % path)
        else:
            self._rpath = rpath
        self._ic = nb.load(os.path.join(rpath, 'melodic_IC' + fext))
        if externals.versions['nibabel'] >= '1.2':
            self._icshape = self._ic.shape
        else:
            self._icshape = self._ic.get_shape()
        self._mask = nb.load(os.path.join(rpath, 'mask' + fext))
        self._tmodes = np.loadtxt(os.path.join(rpath, 'melodic_Tmodes' ))
        self._smodes = np.loadtxt(os.path.join(rpath, 'melodic_Smodes'))
        self._icstats = np.loadtxt(os.path.join(rpath, 'melodic_ICstats'))


    def _get_stat(self, type, ic):
        # melodic's IC number is one-based, we do zero-based
        img = nb.load(os.path.join(self._rpath, 'stats',
                                   '%s%i' % (type, ic + 1) + self._fext))
        return img.get_data()

    def get_probmap(self, ic):
        return self._get_stat('probmap_', ic)

    def get_thresh_zstat(self, ic):
        return self._get_stat('thresh_zstat', ic)

    def get_tmodes(self):
        ns = self.smodes.shape[1]
        if ns > 1:
            # in multisession ICA melodic creates rank-1 approximation of a
            # timeseries from all sessions and stores them in the first column
            return self._tmodes.T[::ns+1]
        else:
            return self._tmodes.T

    def get_raw_tmodes(self):
        return self._tmodes

    # properties
    path     = property(fget=lambda self: self._rpath )
    ic       = property(fget=lambda self: np.rollaxis(self._ic.get_data(), -1))
    mask     = property(fget=lambda self: self._mask.get_data())
    nic      = property(fget=lambda self: self._icshape()[3])
    extent   = property(fget=lambda self: self._icshape()[:3])
    tmodes   = property(fget=get_tmodes)
    smodes   = property(fget=lambda self: self._smodes.T )
    icstats = property(fget=lambda self: self._icstats,
            doc="""Per component statistics.

The first two values (from a set of four per component) correspond to the
explained variance of the component in the set of extracted components and
two the total variance in the whole dataset.""")
    relvar_per_ic  = property(fget=lambda self: self._icstats[:, 0])
    truevar_per_ic = property(fget=lambda self: self._icstats[:, 1])

########NEW FILE########
__FILENAME__ = fx
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Misc. functions (in the mathematical sense)"""

__docformat__ = 'restructuredtext'

import numpy as np


##REF: Name was automagically refactored
def single_gamma_hrf(t, A=5.4, W=5.2, K=1.0):
    """Hemodynamic response function model.

    The version consists of a single gamma function (also see
    double_gamma_hrf()).

    Parameters
    ----------
    t : float
      Time.
    A : float
      Time to peak.
    W : float
      Full-width at half-maximum.
    K : float
      Scaling factor.
    """
    A = float(A)
    W = float(W)
    K = float(K)
    return K * (t / A) ** ((A ** 2) / (W ** 2) * 8.0 * np.log(2.0)) \
           * np.e ** ((t - A) / -((W ** 2) / A / 8.0 / np.log(2.0)))


##REF: Name was automagically refactored
def double_gamma_hrf(t, A1=5.4, W1=5.2, K1=1.0, A2=10.8, W2=7.35, K2=0.35):
    """Hemodynamic response function model.

    The version is using two gamma functions (also see single_gamma_hrf()).

    Parameters
    ----------
    t : float
      Time.
    A : float
      Time to peak.
    W : float
      Full-width at half-maximum.
    K : float
      Scaling factor.

    Parameters A, W and K exists individually for each of both gamma
    functions.
    """
    A1 = float(A1)
    W1 = float(W1)
    K1 = float(K1)
    A2 = float(A2)
    W2 = float(W2)
    K2 = float(K2)
    return single_gamma_hrf(t, A1, W1, K1) - single_gamma_hrf(t, A2, W2, K2)


def dual_gaussian(x, amp1=1.0, mean1=0.0, std1=1.0,
                     amp2=1.0, mean2=0.0, std2=1.0):
    """Sum of two Gaussians.

    Parameters
    ----------
    x : array
      Function argument
    amp1: float
      Amplitude parameter of the first Gaussian
    mean1: float
      Mean parameter of the first Gaussian
    std1: float
      Standard deviation parameter of the first Gaussian
    amp2: float
      Amplitude parameter of the second Gaussian
    mean2: float
      Mean parameter of the second Gaussian
    std2: float
      Standard deviation parameter of the second Gaussian
    """
    from scipy.stats import norm
    if std1 <= 0 or std2 <= 0:
        return np.nan
    return (amp1 * norm.pdf(x, mean1, std1)) + (amp2 * norm.pdf(x, mean2, std2))

def dual_positive_gaussian(x, amp1=1.0, mean1=0.0, std1=1.0,
                           amp2=1.0, mean2=0.0, std2=1.0):
    """Sum of two non-negative Gaussians

    Parameters
    ----------
    x : array
      Function argument
    amp1: float
      Amplitude parameter of the first Gaussian
    mean1: float
      Mean parameter of the first Gaussian
    std1: float
      Standard deviation parameter of the first Gaussian
    amp2: float
      Amplitude parameter of the second Gaussian
    mean2: float
      Mean parameter of the second Gaussian
    std2: float
      Standard deviation parameter of the second Gaussian
    """
    if amp1 < 0 or amp2 < 0:
        return np.nan
    return dual_gaussian(x, amp1, mean1, std1, amp2, mean2, std2)


##REF: Name was automagically refactored
def least_sq_fit(fx, params, y, x=None, **kwargs):
    """Simple convenience wrapper around SciPy's optimize.leastsq.

    The advantage of using this wrapper instead of optimize.leastsq directly
    is, that it automatically constructs an appropriate error function and
    easily deals with 2d data arrays, i.e. each column with multiple values for
    the same function argument (`x`-value).

    Parameters
    ----------
    fx : functor
      Function to be fitted to the data. It has to take a vector with
      function arguments (`x`-values) as the first argument, followed by
      an arbitrary number of (to be fitted) parameters.
    params : sequence
      Sequence of start values for all to be fitted parameters. During
      fitting all parameters in this sequences are passed to the function
      in the order in which they appear in this sequence.
    y : 1d or 2d array
      The data the function is fitted to. In the case of a 2d array, each
      column in the array is considered to be multiple observations or
      measurements of function values for the same `x`-value.
    x : Corresponding function arguments (`x`-values) for each datapoint, i.e.
      element in `y` or columns in `y', in the case of `y` being a 2d array.
      If `x` is not provided it will be generated by `np.arange(m)`, where
      `m` is either the length of `y` or the number of columns in `y`, if
      `y` is a 2d array.
    **kwargs
      All additonal keyword arguments are passed to `fx`.

    Returns
    -------
    tuple : as returned by scipy.optimize.leastsq
      i.e. 2-tuple with list of final (fitted) parameters of `fx` and an
      integer value indicating success or failure of the fitting procedure
      (see leastsq docs for more information).
    """
    # import here to not let the whole module depend on SciPy
    from scipy.optimize import leastsq

    y = np.asanyarray(y)

    if len(y.shape) > 1:
        nsamp, ylen = y.shape
    else:
        nsamp, ylen = (1, len(y))

    # contruct matching x-values if necessary
    if x is None:
        x = np.arange(ylen)

    # transform x and y into 1d arrays
    if nsamp > 1:
        x = np.array([x] * nsamp).ravel()
        y = y.ravel()

    # define error function
    def efx(p):
        err = y - fx(x, *p, **kwargs)
        return err

    # do fit
    return leastsq(efx, params)


def fit2histogram(X, fx, params, nbins=20, x_range=None):
    """Fit a function to multiple histograms.

    First histogram is computed for each data row vector individually.
    Afterwards a custom function is fitted to the binned data.

    TODO
    ----

    - ATM requires multiple "samples" although it would be as useful with 1, and it pukes if we add that
      single rudimentary dimension

    Parameters
    ----------
    X : array-like
      Data (nsamples x nfeatures)
    fx : functor
      Function to be fitted. Its interface has to comply to the requirements
      as for `least_sq_fit`.
    params : tuple
      Initial parameter values.
    nbins : int
      Number of histogram bins.
    x_range : None or tuple
      Range spanned by the histogram bins. By default the actual mininum and
      maximum values of the data are used.

    Returns
    -------
    tuple
      (histograms (nsampels x nbins),
       bin locations (left border),
       bin width,
       output of `least_sq_fit`)
    """
    if x_range is None:
        # use global min max to ensure consistent bins across observations
        xrange = (X.min(), X.max())

    nobsrv = len(X)
    # histograms per observation
    H = []
    bin_centers = None
    bin_left = None
    for obsrv in X:
        hi = np.histogram(obsrv, bins=nbins, range=x_range)
        if bin_centers is None:
            bin_left = hi[1][:-1]
            # round to take care of numerical instabilities
            bin_width = np.abs(
                            np.asscalar(
                                np.unique(
                                    np.round(bin_left - hi[1][1:],
                                             decimals=6))))
            bin_centers = bin_left + bin_width / 2
        H.append(hi[0])

    H = np.asarray(H)

    return (H,
            bin_left,
            bin_width,
            least_sq_fit(fx, params, H, bin_centers))


def get_random_rotation(ns, nt=None, data=None):
    """Return some random rotation (or rotation + dim reduction) matrix

    Parameters
    ----------
    ns : int
      Dimensionality of source space
    nt : int, optional
      Dimensionality of target space
    data : array, optional
      Some data (should have rank high enough) to derive
      rotation
    """
    if nt is None:
        nt = ns
    # figure out some "random" rotation
    d = max(ns, nt)
    if data is None:
        data = np.random.normal(size=(d*10, d))
    _u, _s, _vh = np.linalg.svd(data[:, :d])
    R = _vh[:ns, :nt]
    if ns == nt:
        # Test if it is indeed a rotation matrix ;)
        # Lets flip first axis if necessary
        if np.linalg.det(R) < 0:
            R[:, 0] *= -1.0
    return R



########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Some little helper for reading (and writing) common formats from and to
disk."""

__docformat__ = 'restructuredtext'

import numpy as np
import mvpa2.support.copy as copy
from mvpa2.base.dochelpers import enhanced_doc_string
from re import sub
from mvpa2.base import warning

from mvpa2.misc.support import Event

if __debug__:
    from mvpa2.base import debug

__all__ = ['DataReader', 'ColumnData', 'SampleAttributes',
           'SensorLocations', 'XAVRSensorLocations',
           'TuebingenMEGSensorLocations',
           'labels2chunks', 'design2labels']

class DataReader(object):
    """Base class for data readers.

    Every subclass has to put all information into to variable:

    `self._data`: ndarray
        The data array has to have the samples separating dimension along the
        first axis.
    `self._props`: dict
        All other meaningful information has to be stored in a dictionary.

    This class provides two methods (and associated properties) to retrieve
    this information.
    """
    def __init__(self):
        """Initialize base class `DataReader` -- no parameters.
        """
        self._props = {}
        self._data = None


    @property
    def props(self):
        """Return the dictionary with the data properties.
        """
        return self._props


    @property
    def data(self):
        """Return the data array.
        """
        return self._data



class ColumnData(dict):
    """Read data that is stored in columns of text files.

    All read data is available via a dictionary-like interface. If
    column headers are available, the column names serve as dictionary keys.
    If no header exists an articfical key is generated: str(number_of_column).

    Splitting of text file lines is performed by the standard split() function
    (which gets passed the `sep` argument as separator string) and each
    element is converted into the desired datatype.

    Because data is read into a dictionary no two columns can have the same
    name in the header! Each column is stored as a list in the dictionary.
    """
    def __init__(self, source, header=True, sep=None, headersep=None,
                 dtype=float, skiplines=0):
        """Read data from file into a dictionary.

        Parameters
        ----------
        source : str or dict
          If values is given as a string all data is read from the
          file and additonal keyword arguments can be used to
          customize the read procedure. If a dictionary is passed
          a deepcopy is performed.
        header : bool or list of str
          Indicates whether the column names should be read from the
          first line (`header=True`). If `header=False` unique
          column names will be generated (see class docs). If
          `header` is a python list, it's content is used as column
          header names and its length has to match the number of
          columns in the file.
        sep : str or None
          Separator string. The actual meaning depends on the output
          format (see class docs).
        headersep : str or None
          Separator string used in the header. The actual meaning
          depends on the output format (see class docs).
        dtype : type or list(types)
          Desired datatype(s). Datatype per column get be specified by
          passing a list of types.
        skiplines : int
          Number of lines to skip at the beginning of the file.
        """
        # init base class
        dict.__init__(self)

        # intialize with default
        self._header_order = None

        if isinstance(source, basestring):
            self._from_file(source, header=header, sep=sep, headersep=headersep,
                           dtype=dtype, skiplines=skiplines)

        elif isinstance(source, dict):
            for k, v in source.iteritems():
                self[k] = v
            # check data integrity
            self._check()

        else:
            raise ValueError, 'Unkown source for ColumnData [%r]' \
                              % type(source)

        # generate missing properties for each item in the header
        classdict = self.__class__.__dict__
        for k in self.keys():
            if not classdict.has_key(k):
                getter = "lambda self: self._get_attrib('%s')" % (k)
                # Sanitarize the key, substitute ' []' with '_'
                k_ = sub('[[\] ]', '_', k)
                # replace multipe _s
                k_ = sub('__+', '_', k_)
                # remove quotes
                k_ = sub('["\']', '', k_)
                if __debug__:
                    debug("IOH", "Registering property %s for ColumnData key %s"
                          % (k_, k))
                # make sure to import class directly into local namespace
                # otherwise following does not work for classes defined
                # elsewhere
                exec 'from %s import %s' % (self.__module__,
                                            self.__class__.__name__)
                exec "%s.%s = property(fget=%s)" % \
                     (self.__class__.__name__, k_, getter)
                # TODO!!! Check if it is safe actually here to rely on value of
                #         k in lambda. May be it is treated as continuation and
                #         some local space would override it????
                #setattr(self.__class__,
                #        k,
                #        property(fget=lambda x: x._get_attrib("%s" % k)))
                # it seems to be error-prone due to continuation...


    __doc__ = enhanced_doc_string('ColumnData', locals())


    ##REF: Name was automagically refactored
    def _get_attrib(self, key):
        """Return corresponding value if given key is known to current instance

        Is used for automatically added properties to the class.

        Raises
        ------
        ValueError:
          If `key` is not known to given instance

        Returns
        -------
        Value if `key` is known
        """
        if self.has_key(key):
            return self[key]
        else:
            raise ValueError, "Instance %r has no data about %r" \
                % (self, key)


    def __str__(self):
        s = self.__class__.__name__
        if len(self.keys()) > 0:
            s += " %d rows, %d columns [" % \
                 (self.nrows, self.ncolumns)
            s += reduce(lambda x, y: x + " %s" % y, self.keys())
            s += "]"
        return s

    def _check(self):
        """Performs some checks for data integrity.
        """
        length = None
        for k in self.keys():
            if length == None:
                length = len(self[k])
            else:
                if not len(self[k]) == length:
                    raise ValueError, "Data integrity lost. Columns do not " \
                                      "have equal length."


    def _from_file(self, filename, header, sep, headersep,
                   dtype, skiplines):
        """Loads column data from file -- clears object first.
        """
        # make a clean table
        self.clear()

        with open(filename, 'r') as file_:

            self._header_order = None

            [ file_.readline() for x in range(skiplines) ]
            """Simply skip some lines"""
            # make column names, either take header or generate
            if header == True:
                # read first line and split by 'sep'
                hdr = file_.readline().split(headersep)
                # remove bogus empty header titles
                hdr = [ x for x in hdr if len(x.strip()) ]
                self._header_order = hdr
            elif isinstance(header, list):
                hdr = header
            else:
                hdr = [ str(i) for i in xrange(len(file_.readline().split(sep))) ]
                # reset file to not miss the first line
                file_.seek(0)
                [ file_.readline() for x in range(skiplines) ]


            # string in lists: one per column
            tbl = [ [] for i in xrange(len(hdr)) ]

            # store whether dtype should be determined automagically
            auto_dtype = dtype is None

            # do per column dtypes
            if not isinstance(dtype, list):
                dtype = [dtype] * len(hdr)

            # parse line by line and feed into the lists
            for line in file_:
                # get rid of leading and trailing whitespace
                line = line.strip()
                # ignore empty lines and comment lines
                if not line or line.startswith('#'):
                    continue
                l = line.split(sep)

                if not len(l) == len(hdr):
                    raise RuntimeError, \
                          "Number of entries in line [%i] does not match number " \
                          "of columns in header [%i]." % (len(l), len(hdr))

                for i, v in enumerate(l):
                    if not dtype[i] is None:
                        try:
                            v = dtype[i](v)
                        except ValueError:
                            warning("Can't convert %r to desired datatype %r." %
                                    (v, dtype) + " Leaving original type")
                    tbl[i].append(v)

            if auto_dtype:
                attempt_convert_dtypes = (int, float)

                for i in xrange(len(tbl)):
                    values = tbl[i]

                    for attempt_convert_dtype in attempt_convert_dtypes:
                        try:
                            values = map(attempt_convert_dtype, values)
                            tbl[i] = values
                            break
                        except:
                            continue

            # check
            if not len(tbl) == len(hdr):
                raise RuntimeError, "Number of columns read from file does not " \
                                    "match the number of header entries."

            # fill dict
            for i, v in enumerate(hdr):
                self[v] = tbl[i]


    def __iadd__(self, other):
        """Merge column data.
        """
        # for all columns in the other object
        for k, v in other.iteritems():
            if not self.has_key(k):
                raise ValueError, 'Unknown key [%r].' % (k,)
            if not isinstance(v, list):
                raise ValueError, 'Can only merge list data, but got [%r].' \
                                  % type(v)
            # now it seems to be ok
            # XXX check for datatype?
            self[k] += v

        # look for problems, like columns present in self, but not in other
        self._check()

        return self


    ##REF: Name was automagically refactored
    def select_samples(self, selection):
        """Return new ColumnData with selected samples"""

        data = copy.deepcopy(self)
        for k, v in data.iteritems():
            data[k] = [v[x] for x in selection]

        data._check()
        return data

    @property
    def ncolumns(self):
        """Returns the number of columns.
        """
        return len(self.keys())


    def tofile(self, filename, header=True, header_order=None, sep=' '):
        """Write column data to a text file.

        Parameters
        ----------
        filename : str
          Target filename
        header : bool, optional
          If `True` a column header is written, using the column
          keys. If `False` no header is written.
        header_order : None or list of str
          If it is a list of strings, they will be used instead
          of simply asking for the dictionary keys. However
          these strings must match the dictionary keys in number
          and identity. This argument type can be used to
          determine the order of the columns in the output file.
          The default value is `None`. In this case the columns
          will be in an arbitrary order.
        sep : str, optional
          String that is written as a separator between to data columns.
        """
        # XXX do the try: except: dance
        file_ = open(filename, 'w')

        # write header
        if header_order == None:
            if self._header_order is None:
                col_hdr = self.keys()
            else:
                # use stored order + newly added keys at the last columns
                col_hdr = self._header_order + \
                          list(set(self.keys()).difference(
                                                set(self._header_order)))
        else:
            if not len(header_order) == self.ncolumns:
                raise ValueError, 'Header list does not match number of ' \
                                  'columns.'
            for k in header_order:
                if not self.has_key(k):
                    raise ValueError, 'Unknown key [%r]' % (k,)
            col_hdr = header_order

        if header == True:
            file_.write(sep.join(col_hdr) + '\n')

        # for all rows
        for r in xrange(self.nrows):
            # get attributes for all keys
            l = [str(self[k][r]) for k in col_hdr]
            # write to file with proper separator
            file_.write(sep.join(l) + '\n')

        file_.close()


    @property
    def nrows(self):
        """Returns the number of rows.
        """
        # no data no rows (after Bob Marley)
        if not len(self.keys()):
            return 0
        # otherwise first key is as good as any other
        else:
            return len(self[self.keys()[0]])



class SampleAttributes(ColumnData):
    """Read and write PyMVPA sample attribute definitions from and to text
    files.
    """
    def __init__(self, source, literallabels=True, header=None):
        """Read PyMVPA sample attributes from disk.

        Parameters
        ----------
        source : str
          Filename of an atrribute file
        literallabels : bool, optional
          Either labels are given as literal strings
        header : None or bool or list of str
          If None, ['targets', 'chunks'] is assumed. Otherwise the same
          behavior as of `ColumnData`
        """
        if literallabels:
            if header is None:
                dtypes = [str, float]
            else:
                dtypes = None
        else:
            dtypes = float

        if header is None:
            header = ['targets', 'chunks']
        ColumnData.__init__(self, source,
                            header=header,
                            sep=None, dtype=dtypes)


    def tofile(self, filename):
        """Write sample attributes to a text file.
        """
        ColumnData.tofile(self, filename,
                          header=False,
                          header_order=['targets', 'chunks'],
                          sep=' ')


    @property
    def nsamples(self):
        """Returns the number of samples in the file.
        """
        return self.nrows



class SensorLocations(ColumnData):
    """Base class for sensor location readers.

    Each subclass should provide x, y, z coordinates via the `pos_x`, `pos_y`,
    and `pos_z` attrbibutes.

    Axes should follow the following convention:

      x-axis: left -> right
      y-axis: anterior -> posterior
      z-axis: superior -> inferior
    """
    def __init__(self, *args, **kwargs):
        """Pass arguments to ColumnData.
        """
        ColumnData.__init__(self, *args, **kwargs)


    def locations(self):
        """Get the sensor locations as an array.

        Returns
        -------
        (nchannels x 3) array with coordinates in (x, y, z)
        """
        return np.array((self.pos_x, self.pos_y, self.pos_z)).T



class XAVRSensorLocations(SensorLocations):
    """Read sensor location definitions from a specific text file format.

    File layout is assumed to be 5 columns:

      1. sensor name
      2. some useless integer
      3. position on x-axis
      4. position on y-axis
      5. position on z-axis
    """
    def __init__(self, source):
        """Read sensor locations from file.

        Parameters
        ----------
        source : filename of an attribute file
        """
        SensorLocations.__init__(
            self, source,
            header=['names', 'some_number', 'pos_x', 'pos_y', 'pos_z'],
            sep=None, dtype=[str, int, float, float, float])


class TuebingenMEGSensorLocations(SensorLocations):
    """Read sensor location definitions from a specific text file format.

    File layout is assumed to be 7 columns::

      1:   sensor name
      2:   position on y-axis
      3:   position on x-axis
      4:   position on z-axis
      5-7: same as 2-4, but for some outer surface thingie.

    Note that x and y seem to be swapped, ie. y as defined by SensorLocations
    conventions seems to be first axis and followed by x.

    Only inner surface coordinates are reported by `locations()`.
    """
    def __init__(self, source):
        """Read sensor locations from file.

        Parameters
        ----------
        source : filename of an attribute file
        """
        SensorLocations.__init__(
            self, source,
            header=['names', 'pos_y', 'pos_x', 'pos_z',
                    'pos_y2', 'pos_x2', 'pos_z2'],
            sep=None, dtype=[str, float, float, float, float, float, float])


def design2labels(columndata, baseline_label=0,
                  func=lambda x: x > 0.0):
    """Helper to convert design matrix into a list of labels

    Given a design, assign a single label to any given sample

    TODO: fix description/naming

    Parameters
    ----------
    columndata : ColumnData
      Attributes where each known will be considered as a separate
      explanatory variable (EV) in the design.
    baseline_label
      What label to assign for samples where none of EVs was given a value
    func : functor
      Function which decides either a value should be considered

    Returns
    -------
    list of labels which are taken from column names in ColumnData and
    baseline_label

    """
    # doing it simple naive way but it should be of better control if
    # we decide to process columndata with non-numeric entries etc
    keys = columndata.keys()
    labels = []
    for row in xrange(columndata.nrows):
        entries = [ columndata[key][row] for key in keys ]
        # which entries get selected
        selected = [ x for x in zip(keys, entries) if func(x[1]) ]
        nselected = len(selected)

        if nselected > 1:
            # if there is more than a single one -- we are in problem
            raise ValueError, "Row #%i with items %s has multiple entries " \
                  "meeting the criterion. Cannot decide on the label" % \
                  (row, entries)
        elif nselected == 1:
            label = selected[0][0]
        else:
            label = baseline_label
        labels.append(label)
    return labels


__known_chunking_methods = {
    'alllabels': 'Each chunk must contain instances of all labels'
    }

def labels2chunks(labels, method="alllabels", ignore_labels=None):
    """TOBE ASSIGNED BELOW
    """
    chunks = []
    if ignore_labels is None:
        ignore_labels = []
    alllabels = set(labels).difference(set(ignore_labels))
    if method == 'alllabels':
        seenlabels = set()
        lastlabel = None
        chunk = 0
        for label in labels:
            if label != lastlabel:
                if seenlabels == alllabels:
                    chunk += 1
                    seenlabels = set()
                lastlabel = label
                if not label in ignore_labels:
                    seenlabels.update([label])
            chunks.append(chunk)
        chunks = np.array(chunks)
        # fix up a bit the trailer
        if seenlabels != alllabels:
            chunks[chunks == chunk] = chunk - 1
        chunks = list(chunks)
    else:
        errmsg = "Unknown method to derive chunks is requested. Known are:\n"
        for method, descr in __known_chunking_methods.iteritems():
            errmsg += "  %s : %s\n" % (method, descr)
        raise ValueError, errmsg
    return chunks

labels2chunks.__doc__ = \
 """Automagically decide on chunks based on labels

    Parameters
    ----------
    labels
      labels to base chunking on
    method : str
      codename for method to use. Known are %s
    ignore_labels : list of str
      depends on the method. If method `alllabels`, then don't
      seek for such labels in chunks. E.g. some 'reject' samples

    :rtype: list
    """ % __known_chunking_methods.keys()

########NEW FILE########
__FILENAME__ = hamster
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helper for simple storage facility via cPickle and optionally zlib"""

__docformat__ = 'restructuredtext'

import os

from mvpa2.base import externals

_d_geti_ = dict.__getitem__
_d_seti_ = dict.__setitem__

_o_geta_ = dict.__getattribute__
_o_seta_ = dict.__setattr__

if externals.exists('cPickle', raise_=True) and \
   externals.exists('gzip', raise_=True):
    import cPickle, gzip

if __debug__:
    from mvpa2.base import debug

class Hamster(object):
    """Simple container class with basic IO capabilities.

    It is capable of storing itself in a file, or loading from a file using
    cPickle (optionally via zlib from compressed files). Any serializable
    object can be bound to a hamster to be stored.

    To undig burried hamster use Hamster(filename). Here is an example:

    >>> import numpy as np
    >>> import tempfile
    >>> h = Hamster(bla='blai')
    >>> h.boo = np.arange(5)
    >>> tmp = tempfile.NamedTemporaryFile()
    >>> h.dump(tmp.name)
    ...
    >>> h = Hamster(tmp.name)

    Since Hamster introduces methods `dump`, `asdict` and property
    'registered', those names cannot be used to assign an attribute,
    nor provided in among constructor arguments.
    """

    __ro_attr = set(object.__dict__.keys() +
                    ['dump', 'registered', 'asdict'])
    """Attributes which come with being an object"""

    def __new__(cls, *args, **kwargs):
        if len(args) > 0:
            if len(kwargs) > 0:
                raise ValueError, \
                      "Do not mix positional and keyword arguments. " \
                      "Use a single positional argument -- filename, " \
                      "or any number of keyword arguments, without having " \
                      "filename specified"
            if len(args) == 1 and isinstance(args[0], basestring):
                filename = args[0]
                args = args[1:]
                if __debug__:
                    debug('IOH', 'Undigging hamster from %s' % filename)
                # compressed or not -- that is the question
                if filename.endswith('.gz'):
                    f = gzip.open(filename)
                else:
                    f = open(filename)
                result = cPickle.load(f)
                if not isinstance(result, Hamster):
                    warning("Loaded other than Hamster class from %s" % filename)
                return result
            else:
                raise ValueError, "Hamster accepts only a single positional " \
                      "argument and it must be a filename. Got %d " \
                      "arguments" % (len(args),)
        else:
            return object.__new__(cls)


    def __init__(self, *args, **kwargs):
        """Initialize Hamster.

        Providing a single parameter string would treat it as a
        filename from which to undig the data. Otherwise all keyword
        parameters are assigned into the attributes of the object.
        """
        if len(args) > 0:
            if len(args) == 1 and isinstance(args[0], basestring):
                # it was a filename
                args = args[1:]
            else:
                raise RuntimeError, "Should not get here"

        # assign provided attributes
        for k,v in kwargs.iteritems():
            setattr(self, k, v)

        object.__init__(self)


    def dump(self, filename, compresslevel='auto'):
        """Bury the hamster into the file

        Parameters
        ----------
        filename : str
          Name of the target file. When writing to a compressed file the
          filename gets a '.gz' extension if not already specified. This
          is necessary as the constructor uses the extension to decide
          whether it loads from a compressed or uncompressed file.
        compresslevel : 'auto' or int
          Compression level setting passed to gzip. When set to
          'auto', if filename ends with '.gz' `compresslevel` is set
          to 5, 0 otherwise.  However, when `compresslevel` is set to
          0 gzip is bypassed completely and everything is written to
          an uncompressed file.
        """
        if compresslevel == 'auto':
            compresslevel = (0, 5)[int(filename.endswith('.gz'))]
        if compresslevel > 0 and not filename.endswith('.gz'):
            filename += '.gz'
        if __debug__:
            debug('IOH', 'Burying hamster into %s' % filename)
        if compresslevel == 0:
            f = open(filename, 'w')
        else:
            f = gzip.open(filename, 'w', compresslevel)
        cPickle.dump(self, f)
        f.close()


    def __repr__(self):
        reg_attr = ["%s=%s" % (k, repr(getattr(self, k)))
                    for k in self.registered]
        return "%s(%s)" % (self.__class__.__name__,
                           ", ".join(reg_attr))

    # ??? actually seems to be ugly
    #def __str__(self):
    #    registered = self.registered
    #    return "%s with %d elements: %s" \
    #           % (self.__class__.__name__,
    #              len(registered),
    #              ", ".join(self.registered))

    @property
    def registered(self):
        """List registered attributes.
        """
        reg_attr = [k for k in self.__dict__.iterkeys()
                    if not k in self.__ro_attr]
        reg_attr.sort()
        return reg_attr


    def __setattr__(self, k, v):
        """Just to prevent resetting read-only attributes, such as methods
        """
        if k in self.__ro_attr:
            raise ValueError, "'%s' object attribute '%s' is read-only" \
                  % (self.__class__.__name__, k)
        object.__setattr__(self, k, v)


    def asdict(self):
        """Return registered data as dictionary
        """
        return dict([(k, getattr(self, k))
                     for k in self.registered])

########NEW FILE########
__FILENAME__ = meg
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""IO helper for MEG datasets."""

__docformat__ = 'restructuredtext'

import sys
import numpy as np

from mvpa2.base import externals

class TuebingenMEG(object):
    """Reader for MEG data from line-based textfile format.

    This class reads segmented MEG data from a textfile, which is created by
    converting the proprietary binary output files of a MEG device in
    Tuebingen (Germany) with an unkown tool.

    The file format is line-based, i.e. all timepoints for all samples/trials
    are written in a single line. Each line is prefixed with an identifier
    (using a colon as the delimiter between identifier and data). Two lines
    have a special purpose. The first 'Sample Number' is a list of timepoint
    ids, similar to `range(ntimepoints)` for each sample/trial (all
    concatenated into one line. The second 'Time' contains the timing
    information for each timepoint (relative to stimulus onset), again for all
    trials concatenated into a single line.

    All other lines contain various information (channels) recorded during
    the experiment. The meaning of some channels is unknown. Known ones are:

      M*: MEG channels
      EEG*: EEG channels
      ADC*: Analog to digital converter output

    Dataset properties are available from various class attributes. The `data`
    member provides all data from all channels (except for 'Sample Number' and
    'Time') in a NumPy array (nsamples x nchannels x ntimepoints).

    The reader supports uncompressed as well as gzipped input files (or other
    file-like objects).
    """

    def __init__(self, source):
        """Reader MEG data from texfiles or file-like objects.

        Parameters
        ----------
        source : str or file-like
          Strings are assumed to be filenames (with `.gz` suffix
          compressed), while all other object types are treated as file-like
          objects.
        """
        self.ntimepoints = None
        self.timepoints = None
        self.nsamples = None
        self.channelids = []
        self.data = []
        self.samplingrate = None

        # open textfiles
        if isinstance(source, str):
            if source.endswith('.gz'):
                externals.exists('gzip', raise_=True)
                import gzip
                source = gzip.open(source, 'r')
                if sys.version >= '3':
                    # module still can not open text files
                    # in py3: Issue #13989 and #10791 
                    source = source.read().decode('ascii').splitlines()
            else:
                source = open(source, 'r')

        # read file
        for line in source:
            # split ID
            colon = line.find(':')

            # ignore lines without id
            if colon == -1:
                continue

            id = line[:colon]
            data = line[colon+1:].strip()
            if id == 'Sample Number':
                timepoints = np.fromstring(data, dtype=int, sep='\t')
                # one more as it starts with zero
                self.ntimepoints = int(timepoints.max()) + 1
                self.nsamples = int(len(timepoints) / self.ntimepoints)
            elif id == 'Time':
                self.timepoints = np.fromstring(data,
                                               dtype=float,
                                               count=self.ntimepoints,
                                               sep='\t')
                self.samplingrate = self.ntimepoints \
                    / (self.timepoints[-1] - self.timepoints[0])
            else:
                # load data
                self.data.append(
                    np.fromstring(data, dtype=float, sep='\t').reshape(
                        self.nsamples, self.ntimepoints))
                # store id
                self.channelids.append(id)

        # reshape data from (channels x samples x timepoints) to
        # (samples x chanels x timepoints)
        self.data = np.swapaxes(np.array(self.data), 0, 1)


    def __str__(self):
        """Give a short summary.
        """
        return '<TuebingenMEG: %i samples, %i timepoints, %i channels>' \
                  % (self.nsamples, self.ntimepoints, len(self.channelids))


########NEW FILE########
__FILENAME__ = neighborhood
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
""" Neighborhood objects """

import numpy as np
from numpy import array
import sys
import itertools

from mvpa2.base import warning
from mvpa2.base.types import is_sequence_type
from mvpa2.base.dochelpers import borrowkwargs, borrowdoc, _repr_attrs, _repr
from mvpa2.clfs.distance import cartesian_distance

from mvpa2.misc.support import idhash as idhash_

if __debug__:
    from mvpa2.base import debug

class Sphere(object):
    """N-Dimensional hypersphere.

    Use this if you want to obtain all the neighbors within a given
    radius from a point in a space with arbitrary number of dimensions
    assuming that the space is discrete.

    No validation of producing coordinates within any extent is done.

    Examples
    --------
    Create a Sphere of diameter 1 and obtain all coordinates within range for the
    coordinate (1,1,1).

    >>> s = Sphere(1)
    >>> s((2, 1))
    [(1, 1), (2, 0), (2, 1), (2, 2), (3, 1)]
    >>> s((1, ))
    [(0,), (1,), (2,)]

    If elements in discrete space have different sizes across
    dimensions, it might be preferable to specify element_sizes
    parameter.

    >>> s = Sphere(2, element_sizes=(1.5, 2.5))
    >>> s((2, 1))
    [(1, 1), (2, 1), (3, 1)]

    >>> s = Sphere(1, element_sizes=(1.5, 0.4))
    >>> s((2, 1))
    [(2, -1), (2, 0), (2, 1), (2, 2), (2, 3)]

    """

    def __init__(self, radius, element_sizes=None, distance_func=None):
        """ Initialize the Sphere

        Parameters
        ----------
        radius : float
          Radius of the 'sphere'.  If no `element_sizes` provided --
          radius would be effectively in number of voxels (if
          operating on MRI data).
        element_sizes : None or iterable of floats
          Sizes of elements in each dimension.  If None, it is equivalent
          to 1s in all dimensions.
        distance_func : None or lambda
          Distance function to use (choose one from `mvpa2.clfs.distance`).
          If None, cartesian_distance to be used.
        """
        self._radius = radius
        # TODO: make ability to lookup in a dataset
        self._element_sizes = element_sizes
        if distance_func is None:
            distance_func = cartesian_distance
        self._distance_func = distance_func

        self._increments = None
        """Stored template of increments"""
        self._increments_ndim = None
        """Dimensionality of increments"""

    def __repr__(self, prefixes=[]):
        prefixes_ = ['radius=%r' % (self._radius,)] + prefixes
        if self._element_sizes:
            prefixes_.append('element_sizes=%r' % (self._element_sizes,))
        if self._distance_func != cartesian_distance:
            prefixes_.append('distance_func=%r' % self._distance_func)
        return "%s(%s)" % (self.__class__.__name__, ', '.join(prefixes_))

    # Properties to assure R/O behavior for now
    @property
    def radius(self):
        return self._radius

    @property
    def element_sizes(self):
        return self._element_sizes

    @property
    def distance_func(self):
        return self._distance_func

    def _get_increments(self, ndim):
        """Creates a list of increments for a given dimensionality
        """
        # Set element_sizes
        element_sizes = self._element_sizes
        if element_sizes is None:
            element_sizes = np.ones(ndim)
        else:
            if (ndim != len(element_sizes)):
                raise ValueError, \
                      "Dimensionality mismatch: element_sizes %s provided " \
                      "to constructor had %i dimensions, whenever queried " \
                      "coordinate had %i" \
                      % (element_sizes, len(element_sizes), ndim)
        center = np.zeros(ndim)

        element_sizes = np.asanyarray(element_sizes)
        # What range for each dimension
        erange = np.ceil(self._radius / element_sizes).astype(int)

        tentative_increments = np.array(list(np.ndindex(tuple(erange*2 + 1)))) \
                               - erange
        # Filter out the ones beyond the "sphere"
        return array([x for x in tentative_increments
                      if self._distance_func(x * element_sizes, center)
                      <= self._radius])


    def train(self, dataset):
        # XXX YOH:  yeap -- BUT if you care about my note above on extracting
        #     somehow sizes -- some dataset.a might come handy may be?
        #     so actual template get constructed in train and _create_template
        #     could go away and just be returned in some R/O property
        #self.dataset = dataset
        # TODO: extract element_sizes
        pass

    # XXX YOH: should it have this at all?  may be sphere should just generate the
    #         "neighborhood template" -- all those offsets where to jump to get
    #         tentative neighbor... Otherwise there are double checks... some here
    #         some in the query engine... also imho Sphere should not even care about any extent
    def __call__(self, coordinate):
        """Get all coordinates within diameter

        Parameters
        ----------
        coordinate : sequence type of length 3 with integers

        Returns
        -------
        list of tuples of size 3

        """
        # type checking
        coordinate = np.asanyarray(coordinate)
        # XXX This might go into _train ...
        scalar = coordinate.ndim == 0
        if scalar:
            # we are dealing with scalars -- lets add a dimension
            # artificially
            coordinate = coordinate[None]
        # XXX This might go into _train ...
        ndim = len(coordinate)
        if self._increments is None  or self._increments_ndim != ndim:
            if __debug__:
                debug('NBH',
                      "Recomputing neighborhood increments for %dD Sphere"
                      % ndim)
            self._increments = self._get_increments(ndim)
            self._increments_ndim = ndim

        if __debug__:
            if coordinate.dtype.char not in np.typecodes['AllInteger']:
                raise ValueError("Sphere must be called on a sequence of "
                                 "integers of length %i, you gave %s "
                                 % (ndim, coordinate))
            #if dataset is None:
            #    raise ValueError("Sphere object has not been trained yet, use "
            #                     "train(dataset) first. ")

        if len(self._increments):
            # function call
            coord_array = (coordinate + self._increments)
        else:
            # if no increments -- no neighbors -- empty list
            return []

        # XXX may be optionally provide extent checking?
        ## # now filter out illegal coordinates if they really are outside the
        ## # bounds
        ## if (coordinate - self.radius < 0).any() \
        ## or (coordinate + self.radius >= self.extent).any():
        ##     coord_array = array([c for c in coord_array \
        ##                            if (c >= 0).all()
        ##                            and (c < self.extent).all()])
        ## coord_array = coord_array.transpose()

        if scalar:
            # Take just 0th dimension since 1st was artificially introduced
            coord_array = coord_array[:, 0]
            return coord_array.tolist()
        else:
            # Note: converting first full array to list and then
            # "tuppling" it seems to be faster than tuppling each
            # sub-array
            return [tuple(x) for x in coord_array.tolist()]


class HollowSphere(Sphere):
    """N-Dimensional hypersphere with a hollow internal sphere

    See parent class `Sphere` for more information.

    Examples
    --------
    Create a Sphere of diameter 1 and obtain all coordinates within range for the
    coordinate (1,1,1).

    >>> s = HollowSphere(1, 0)
    >>> s((2, 1))
    [(1, 1), (2, 0), (2, 2), (3, 1)]
    >>> s((1, ))
    [(0,), (2,)]

    """

    def __init__(self, radius, inner_radius, **kwargs):
        """ Initialize the Sphere

        Parameters
        ----------
        radius : float
          Radius of the 'sphere'.  If no `element_sizes` provided --
          radius would be effectively in number of voxels (if
          operating on MRI data).
        inner_radius : float
          Inner radius of the 'sphere', describing where hollow
          part starts.  It is inclusive, so `inner_radius` of 0,
          would already remove the center element.
        **kwargs
          See `Sphere` for additional keyword arguments
        """
        if inner_radius > radius:
            raise ValueError, "inner_radius (got %g) should be smaller " \
                  "than the radius (got %g)" % (inner_radius, radius)
        Sphere.__init__(self, radius, **kwargs)
        self._inner_radius = inner_radius

    def __repr__(self, prefixes=[]):
        return super(HollowSphere, self).__repr__(
            ['inner_radius=%r' % (self._inner_radius,)])

    # Properties to assure R/O behavior for now
    @property
    def inner_radius(self):
        return self._inner_radius

    def _get_increments(self, ndim):
        """Creates a list of increments for a given dimensionality

        RF: lame yoh just cut-pasted and tuned up because everything
            depends on ndim...
        """
        # Set element_sizes
        element_sizes = self._element_sizes
        if element_sizes is None:
            element_sizes = np.ones(ndim)
        else:
            if (ndim != len(element_sizes)):
                raise ValueError, \
                      "Dimensionality mismatch: element_sizes %s provided " \
                      "to constructor had %i dimensions, whenever queried " \
                      "coordinate had %i" \
                      % (element_sizes, len(element_sizes), ndim)
        center = np.zeros(ndim)

        element_sizes = np.asanyarray(element_sizes)
        # What range for each dimension
        erange = np.ceil(self._radius / element_sizes).astype(int)

        tentative_increments = np.array(list(np.ndindex(tuple(erange*2 + 1)))) \
                               - erange
        # Filter out the ones beyond the "sphere"
        res = array([x for x in tentative_increments
                      if self._inner_radius
                      < self._distance_func(x * element_sizes, center)
                      <= self._radius])

        if not len(res):
            warning("%s defines no neighbors" % self)
        return res


class QueryEngineInterface(object):
    """Very basic class for `QueryEngine`\s defining the interface

    It should not be used directly, but is used to check either we are
    working with QueryEngine instances
    """

    def __repr__(self, prefixes=[]):
        return _repr(self, *prefixes)

    def train(self, dataset):
        raise NotImplementedError


    def query_byid(self, fid):
        """Return feature ids of neighbors for a given feature id
        """
        raise NotImplementedError


    def query(self, **kwargs):
        """Return feature ids of neighbors given a specific query
        """
        raise NotImplementedError

    #
    # aliases
    #

    def __call__(self, **kwargs):
        return self.query(**kwargs)


    def __getitem__(self, fid):
        return self.query_byid(fid)



class QueryEngine(QueryEngineInterface):
    """Basic class defining interface for querying neighborhood in a dataset

    Derived classes provide specific implementations possibly with trade-offs
    between generality and performance.

    TODO: extend
    """

    def __init__(self, **kwargs):
        """
        Parameters
        ----------
        **kwargs
          a dictionary of query objects. Something like
          dict(voxel_indices=Sphere(3))
        """
        super(QueryEngine, self).__init__()
        self._queryobjs = kwargs
        self._queryattrs = {}
        self._ids = None


    def __repr__(self, prefixes=[]):
        return super(QueryEngine, self).__repr__(
            prefixes=prefixes
            + ['%s=%r' % v for v in self._queryobjs.iteritems()])

    def __len__(self):
        return len(self._ids) if self._ids is not None else 0

    @property
    def ids(self):
        return self._ids

    def train(self, dataset):
        # reset first
        self._queryattrs.clear()
        # store all relevant attributes
        for space in self._queryobjs:
            self._queryattrs[space] = dataset.fa[space].value
        # execute subclass training
        self._train(dataset)
        # by default all QueryEngines should with all features of the dataset
        # Situation might be different in case of e.g. Surface-based
        # searchlights.
        self._ids = range(dataset.nfeatures)


    def query_byid(self, fid):
        """Return feature ids of neighbors for a given feature id
        """
        queryattrs = self._queryattrs
        kwargs = dict([(space, queryattrs[space][fid])
                       for space in queryattrs])
        return self.query(**kwargs)



class IndexQueryEngine(QueryEngine):
    """Provides efficient query engine for discrete spaces.

    Uses dictionary lookups for elements indices and presence in
    general.  Each space obtains a lookup dictionary which performs
    translation from given index/coordinate into the index within an
    index table (with a dimension per each space to search within).

    TODO:
    - extend documentation
    - repr
    """

    def __init__(self, sorted=True, **kwargs):
        """
        Parameters
        ----------
        sorted : bool
          Results of query get sorted
        """
        QueryEngine.__init__(self, **kwargs)
        self._spaceorder = None
        """Order of the spaces"""
        self._lookups = {}
        """Dictionary of lookup dictionaries per each space"""
        self._sliceall = {}
        """Precrafted indexes to cover ':' situation within ix_"""
        self._searcharray = None
        """Actual searcharray"""
        self.sorted = sorted
        """Either to sort the query results"""


    def __repr__(self, prefixes=[]):
        return super(IndexQueryEngine, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['sorted'], default=True))


    def _train(self, dataset):
        # local binding
        qattrs = self._queryattrs
        # in addition to the base class functionality we need to store the
        # order of the query-spaces
        self._spaceorder = qattrs.keys()
        # type check and determine mask dimensions
        dims = []                       # dimensionality of each space
        lookups = self._lookups = {}
        sliceall = self._sliceall = {}
        selector = []
        for space in self._spaceorder:
            # local binding for the attribute
            qattr = qattrs[space]
            # If it is >1D ndarray we need to transform to list of tuples,
            # since ndarray is not hashable
            # XXX would probably work for ANY discrete attribute
            if not qattr.dtype.char in np.typecodes['AllInteger']:
                pass
                #raise ValueError("IndexQueryEngine can only operate on "
                #                 "feature attributes with integer indices "
                #                 "(got: %s)." % str(qattr.dtype))
            if isinstance(qattr, np.ndarray) and len(qattr.shape) > 1:
                qattr = [tuple(x) for x in qattr]

            # determine the dimensions of this space
            # and charge the nonzero selector
            uqattr = list(set(qattr))
            dim = len(uqattr)
            dims.append(dim)
            # Lookup table for elements known to corresponding indices
            # in searcharray
            lookups[space] = lookup = \
                             dict([(u, i) for i, u in enumerate(uqattr)])
            # Precraft "slicing" for all elements for dummy numpy way
            # to select things ;)
            sliceall[space] = np.arange(dim)
            # And fill out selector using current values from qattr
            selector.append([lookup[x] for x in qattr])

        # now check whether we have sufficient information to put each feature
        # id into one unique search array element
        dims = np.array(dims)
        # we can deal with less features (e.g. masked dataset, but not more)
        # XXX (yoh): seems to be too weak of a check... pretty much you are trying
        #            to check either 2 features do not collide in the target
        #            "mask", right?
        if np.prod(dims) < dataset.nfeatures:
            raise ValueError("IndexQueryEngine has insufficient information "
                             "about the dataset spaces. It is required to "
                             "specify an ROI generator for each feature space "
                             "in the dataset (got: %s, #describable: %i, "
                             "#actual features: %i)."
                             % (str(self._spaceorder), np.prod(dims),
                                   dataset.nfeatures))
        # now we can create the search array
        self._searcharray = np.zeros(dims, dtype='int')
        # and fill it with feature ids, but start from ONE to be different from
        # the zeros
        self._searcharray[tuple(selector)] = np.arange(1, dataset.nfeatures + 1)
        # Lets do additional check -- now we should have same # of
        # non-zero elements as features
        if len(self._searcharray.nonzero()[0]) != dataset.nfeatures:
            # TODO:  Figure out how is the bad cow? sad there is no non-unique
            #        function in numpy
            raise ValueError("Multiple features carry the same set of "
                             "attributes %s.  %s engine cannot handle such "
                             "cases -- use another appropriate query engine"
                             % (self._spaceorder, self))


    def query(self, **kwargs):
        # construct the search array slicer
        # need to obey axis order
        slicer = []
        for space in self._spaceorder:
            lookup = self._lookups[space]
            # only generate ROI, if we have a generator
            # otherwise consider all of the unspecified space
            if space in kwargs:
                space_args = kwargs.pop(space)   # so we could check later on
                # if no ROI generator is available, take provided indexes
                # without any additional neighbors etc
                if self._queryobjs[space] is None:
                    roi = np.atleast_1d(space_args)
                else:
                    roi = self._queryobjs[space](space_args)
                # lookup and filter the results
                roi_ind = [lookup[i] for i in roi if (i in lookup)]
                # if no candidate is left, the whole thing does not match
                # regardless of the other spaces
                if not len(roi_ind):
                    return []
                slicer.append(roi_ind)
            else:
                # Provide ":" if no specialization was provided
                slicer.append(self._sliceall[space])
        # check if query had only legal spaces specified
        if len(kwargs):
            raise ValueError, "Do not know how to treat space(s) %s given " \
                  "in parameters of the query" % (kwargs.keys())
        # only ids are of interest -> flatten
        # and we need to back-transfer them into dataset ids by subtracting 1
        res = self._searcharray[np.ix_(*slicer)].flatten() - 1
        res = res[res>=0]              # return only the known ones
        if self.sorted:
            return sorted(res)
        else:
            return res


class CachedQueryEngine(QueryEngineInterface):
    """Provides caching facility for query engines.

    Notes
    -----

    This QueryEngine simply remembers the results of the previous
    queries.  Not much checking is done on either datasets it gets in
    :meth:`train` is the same as the on in previous sweep of queries,
    i.e. either none of the relevant for underlying QueryEngine
    feature attributes was modified.  So, CAUTION should be paid to
    avoid calling the same instance of `CachedQueryEngine` on
    different datasets (which might have different masking etc) .

    :func:`query_byid` should be working reliably and without
    surprises.

    :func:`query` relies on hashid of the queries, so there might be a
    collision! Thus consider it EXPERIMENTAL for now.
    """

    def __init__(self, queryengine):
        """
        Parameters
        ----------
        queryengine : QueryEngine
          Results of which engine to cache
        """
        super(CachedQueryEngine, self).__init__()
        self._queryengine = queryengine
        self._trained_ds_fa_hash = None
        """Will give information about either dataset's FA were changed
        """
        self._lookup_ids = None
        self._lookup = None

    def __repr__(self, prefixes=[]):
        return super(CachedQueryEngine, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['queryengine']))


    def train(self, dataset):
        """'Train' `CachedQueryEngine`.

        Raises
        ------
        ValueError
          If `dataset`'s .fa were changed -- it would raise an
          exception telling to `untrain` explicitly, since the idea is
          to reuse CachedQueryEngine with the same engine and same
          dataset (up to variation of .sa, such as labels permutation)
        """
        ds_fa_hash = idhash_(dataset.fa) + ':%d' % dataset.fa._uniform_length
        if self._trained_ds_fa_hash is None:
            # First time is called
            self._trained_ds_fa_hash = ds_fa_hash
            self._queryengine.train(dataset)     # train the queryengine
            self._lookup_ids = [None] * dataset.nfeatures # lookup for query_byid
            self._lookup = {}           # generic lookup
        elif self._trained_ds_fa_hash != ds_fa_hash:
            raise ValueError, \
                  "Feature attributes of %s (idhash=%r) were changed from " \
                  "what this %s was trained on (idhash=%r). Untrain it " \
                  "explicitly if you like to reuse it on some other data." \
                  % (dataset, ds_fa_hash, self, self._trained_ds_fa_hash)
        else:
            pass

    def untrain(self):
        """Forgetting that CachedQueryEngine was already trained
        """
        self._trained_ds_fa_hash = None


    @borrowdoc(QueryEngineInterface)
    def query_byid(self, fid):
        v = self._lookup_ids[fid]
        if v is None:
            self._lookup_ids[fid] = v = self._queryengine.query_byid(fid)
        return v

    @borrowdoc(QueryEngineInterface)
    def query(self, **kwargs):
        def to_hashable(x):
            """Convert x to something which dict wouldn't mind"""
            try:
                # silly attempt
                d = {x: None}
                return x
            except TypeError:
                pass

            if isinstance(x, dict):
                # keys are already hashable
                # and sort for deterministic order
                return tuple((k, to_hashable(v))
                             for (k, v) in sorted(x.iteritems()))
            elif is_sequence_type(x):
                return tuple(i for i in x)
            elif np.isscalar(x):
                return x
            return x   # and then wait for the report for it to be added

        # idhash_ is somewhat inappropriate since also relies on id
        # (which we should allow to differ) but ATM failing to hash
        # ndarrays etc
        # k = idhash_(kwargs.items())

        # So let's use verbose version of the beastie (could have been
        # also as simple as __repr__ but afraid that order could be
        # changing etc).  This simple function should be sufficient
        # for our typical use, otherwise we might like to use hashing
        # facilities provided by joblib but for paranoid we would
        # still need to store actual values to resolve collisions
        # which would boil down to the same scenario
        k = to_hashable(kwargs)
        v = self._lookup.get(k, None)
        if v is None:
            self._lookup[k] = v = self._queryengine.query(**kwargs)
        return v

    queryengine = property(fget=lambda self: self._queryengine)



def scatter_neighborhoods(neighbor_gen, coords, deterministic=False):
    """Scatter neighborhoods over a coordinate list.

    Neighborhood seeds (or centers) are placed on coordinates drawn from a
    provided list so that no seed is part of any other neighborhood. Depending
    on the actual shape and size of the neighborhoods, their elements can be
    overlapping, only the seeds (or centers) are guaranteed to be
    non-overlapping with any other neighborhood. This can be used to perform
    sparse sampling of a given space.

    Parameters
    ==========

    neighbor_gen : neighborhood generator
      Callable that return a list of neighborhood element coordinates, when
      called with a seed coordinate (cf. Sphere)
    coords : list
      List of candidate coordinates that can serve as neighborhood seeds or
      elements.
    deterministic : bool
      If true, performs seed placement using an OrderedDict (available in
      Python 2.7 or later) to guarantee deterministic placement of neighborhood
      seeds in consecutive runs with identical input arguments.

    Returns
    =======
    coordinates, indices
      Two lists are returned. The first list contains the choosen seed
      coordinates (a subset of the input coordinates), the second list
      contains the indices of the respective seeds coordinates in the input
      coordinate list. If particular coordinates are present multiple times
      the index list will contain all indices corresponding to these
      coordinates.
    """
    hasher = dict
    if deterministic:
        from collections import OrderedDict
        hasher = OrderedDict

    # put coordinates into a dict for fast lookup
    try:
        # quick test to check whether the given coords are hashable. If not,
        # this test avoids a potentially long list zipping
        _ = {coords[0]: None}
        lookup = hasher()
        _ = [lookup.setdefault(c, list()).append(i) for i, c in enumerate(coords)]
    except TypeError:
        # maybe coords not hashable?
        lookup = hasher()
        _ = [lookup.setdefault(tuple(c), list()).append(i) for i, c in enumerate(coords)]

    seeds = []
    while len(lookup):
        # get any remaining coordinate
        # with OrderedDict popitem will return the last inserted item by default
        seed, idx = lookup.popitem()
        # remove all coordinates in the neighborhood
        _ = [lookup.pop(c, None) for c in neighbor_gen(seed)]
        # store seed
        seeds.append((seed, idx))
    # unzip coords and idx again
    coords, idx = zip(*seeds)
    # we need a flat idx list
    # yoh: sum trick replaced list(itertools.chain.from_iterable(idx))
    #      which is not python2.5-compatible
    idx = sum(idx, [])
    return coords, idx

########NEW FILE########
__FILENAME__ = base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Misc. plotting helpers."""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals
externals._set_matplotlib_backend()

import numpy as np

from mvpa2.base.node import ChainNode

if externals.exists('pylab', raise_=True):
    import pylab as pl
    from mvpa2.misc.plot.tools import Pion, Pioff

from mvpa2.misc.attrmap import AttributeMap
from mvpa2.generators.splitters import Splitter
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.clfs.distance import squared_euclidean_distance
from mvpa2.datasets.miscfx import get_samples_by_attr


##REF: Name was automagically refactored
def plot_err_line(data, x=None, errtype='ste', curves=None, linestyle='--',
                fmt='o', perc_sigchg=False, baseline=None, **kwargs):
    """Make a line plot with errorbars on the data points.

    Parameters
    ----------
    data : sequence of sequences
      First axis separates samples and second axis will appear as
      x-axis in the plot.
    x : sequence
      Value to be used as 'x-values' corresponding to the elements of
      the 2nd axis id `data`. If `None`, a sequence of ascending integers
      will be generated.
    errtype : 'ste' or 'std'
      Type of error value to be computed per datapoint: 'ste' --
      standard error of the mean, 'std' -- standard deviation.
    curves : None or list of tuple(x, y)
      Each tuple represents an additional curve, with x and y coordinates of
      each point on the curve.
    linestyle : str or None
      matplotlib linestyle argument. Applied to either the additional
      curve or a the line connecting the datapoints. Set to 'None' to
      disable the line completely.
    fmt : str
      matplotlib plot style argument to be applied to the data points
      and errorbars.
    perc_sigchg : bool
      If `True` the plot will show percent signal changes relative to a
      baseline.
    baseline : float or None
      Baseline used for converting values into percent signal changes.
      If `None` and `perc_sigchg` is `True`, the absolute of the mean of the
      first feature (i.e. [:,0]) will be used as a baseline.
    **kwargs
      Additional arguments are passed on to errorbar().


    Examples
    --------
    Make a dataset with 20 samples from a full sinus wave period,
    computed 100 times with individual noise pattern.

    >>> x = np.linspace(0, np.pi * 2, 20)
    >>> data = np.vstack([np.sin(x)] * 30)
    >>> data += np.random.normal(size=data.shape)

    Now, plot mean data points with error bars, plus a high-res
    version of the original sinus wave.

    >>> x_hd = np.linspace(0, np.pi * 2, 200)
    >>> elines = plot_err_line(data, x, curves=[(x_hd, np.sin(x_hd))])
    >>> # pl.show()

    Returns
    -------
    list
      Of lines which were plotted.
    """
    data = np.asanyarray(data)

    if len(data.shape) < 2:
        data = np.atleast_2d(data)

    return plot_err_line_missing(data.T, x=x, errtype=errtype, curves=curves,
                                 linestyle=linestyle, fmt=fmt,
                                 perc_sigchg=perc_sigchg, baseline=baseline,
                                 **kwargs)


def plot_err_line_missing(data, x=None, errtype='ste', curves=None,
        linestyle='--', fmt='o', perc_sigchg=False, baseline=None, **kwargs):
    """Make a line plot with errorbars on the data points.

    This is essentially the same function as plot_err_line(), but it expects
    the data transposed and tolerates an unequal number of samples, per data
    point.

    Parameters
    ----------
    data : sequence of sequences
      First axis will appear as x-axis in the plot. Along the second axis are
      the sample. Each data point may have a different number of samples.
    x : sequence
      Value to be used as 'x-values' corresponding to the elements of
      the 2nd axis id `data`. If `None`, a sequence of ascending integers
      will be generated.
    errtype : 'ste' or 'std'
      Type of error value to be computed per datapoint: 'ste' --
      standard error of the mean, 'std' -- standard deviation.
    curves : None or list of tuple(x, y)
      Each tuple represents an additional curve, with x and y coordinates of
      each point on the curve.
    linestyle : str or None
      matplotlib linestyle argument. Applied to either the additional
      curve or a the line connecting the datapoints. Set to 'None' to
      disable the line completely.
    fmt : str
      matplotlib plot style argument to be applied to the data points
      and errorbars.
    perc_sigchg : bool
      If `True` the plot will show percent signal changes relative to a
      baseline.
    baseline : float or None
      Baseline used for converting values into percent signal changes.
      If `None` and `perc_sigchg` is `True`, the absolute of the mean of the
      first feature (i.e. [:,0]) will be used as a baseline.
    **kwargs
      Additional arguments are passed on to errorbar().

    Examples
    --------
    Make a dataset with 20 samples from a full sinus wave period,
    computed 100 times with individual noise pattern.

    >>> x = np.linspace(0, np.pi * 2, 20)
    >>> data = np.vstack([np.sin(x)] * 30)
    >>> data += np.random.normal(size=data.shape)

    Now, plot mean data points with error bars, plus a high-res
    version of the original sinus wave.

    >>> x_hd = np.linspace(0, np.pi * 2, 200)
    >>> elines = plot_err_line(data, x, curves=[(x_hd, np.sin(x_hd))])
    >>> # pl.show()

    Returns
    -------
    list
      Of lines which were plotted.
    """
    # compute mean signal course
    md = np.array([np.mean(i) for i in data])

    if baseline is None:
        baseline = np.abs(md[0])

    if perc_sigchg:
        md /= baseline
        md -= 1.0
        md *= 100.0
        data = [np.array(i) / baseline * 100 for i in data]

    # compute matching datapoint locations on x-axis
    if x is None:
        x = np.arange(len(md))
    else:
        if not len(md) == len(x):
            raise ValueError, "The length of `x` (%i) has to match the 2nd " \
                              "axis of the data array (%i)" % (len(x), len(md))

    # collect pylab things that are plotted for later modification
    lines = []

    # plot highres line if present
    if curves is not None:
        for c in curves:
            xc, yc = c
            # scales line array to same range as datapoints
            if not linestyle is None:
                lines.append(pl.plot(xc, yc, linestyle))
            else:
                lines.append(pl.plot(xc, yc))

        # no line between data points
        linestyle = 'None'

    # compute error per datapoint
    if errtype == 'ste':
        err = [np.std(i) / np.sqrt(len(i)) for i in data]
    elif errtype == 'std':
        err = [np.std(i) for i in data]
    else:
        raise ValueError, "Unknown error type '%s'" % errtype

    # plot datapoints with error bars
    lines.append(pl.errorbar(x, md, err, fmt=fmt, linestyle=linestyle, **kwargs))
    return lines




##REF: Name was automagically refactored
def plot_feature_hist(dataset, xlim=None, noticks=True,
                      targets_attr='targets', chunks_attr=None,
                    **kwargs):
    """This function is deprecated and will be removed. Replacement mvpa2.viz.hist()
    """
    import warnings
    warnings.warn("plot_feature_hist() is deprecated and will be removed",
                  DeprecationWarning)
    from mvpa2.viz import hist
    return hist(dataset, xlim=xlim, noticks=noticks, ygroup_attr=targets_attr,
                xgroup_attr=chunks_attr, **kwargs)


##REF: Name was automagically refactored
def plot_samples_distance(dataset, sortbyattr=None):
    """Plot the euclidean distances between all samples of a dataset.

    Parameters
    ----------
    dataset : Dataset
      Providing the samples.
    sortbyattr : None or str
      If None, the samples distances will be in the same order as their
      appearance in the dataset. Alternatively, the name of a samples
      attribute can be given, which wil then be used to sort/group the
      samples, e.g. to investigate the similarity samples by label or by
      chunks.
    """
    if sortbyattr is not None:
        slicer = []
        for attr in dataset.sa[sortbyattr].unique:
            slicer += \
                get_samples_by_attr(dataset, sortbyattr, attr).tolist()
        samples = dataset.samples[slicer]
    else:
        samples = dataset.samples

    ed = np.sqrt(squared_euclidean_distance(samples))

    pl.imshow(ed, interpolation='nearest')
    pl.colorbar()


def plot_decision_boundary_2d(dataset, clf=None,
                              targets=None, regions=None, maps=None,
                              maps_res=50, vals=[-1, 0, 1],
                              data_callback=None):
    """Plot a scatter of a classifier's decision boundary and data points

    Assumes data is 2d (no way to visualize otherwise!!)

    Parameters
    ----------
    dataset : `Dataset`
      Data points to visualize (might be the data `clf` was train on, or
      any novel data).
    clf : `Classifier`, optional
      Trained classifier
    targets : string, optional
      What samples attributes to use for targets.  If None and clf is
      provided, then `clf.params.targets_attr` is used.
    regions : string, optional
      Plot regions (polygons) around groups of samples with the same
      attribute (and target attribute) values. E.g. chunks.
    maps : string in {'targets', 'estimates'}, optional
      Either plot underlying colored maps, such as clf predictions
      within the spanned regions, or estimates from the classifier
      (might not work for some).
    maps_res : int, optional
      Number of points in each direction to evaluate.
      Points are between axis limits, which are set automatically by
      matplotlib.  Higher number will yield smoother decision lines but come
      at the cost of O^2 classifying time/memory.
    vals : array of floats, optional
      Where to draw the contour lines if maps='estimates'
    data_callback : callable, optional
      Callable object to preprocess the new data points.
      Classified points of the form samples = data_callback(xysamples).
      I.e. this can be a function to normalize them, or cache them
      before they are classified.
    """

    if False:
        ## from mvpa2.misc.data_generators import *
        ## from mvpa2.clfs.svm import *
        ## from mvpa2.clfs.knn import *
        ## ds = dumb_feature_binary_dataset()
        dataset = normal_feature_dataset(nfeatures=2, nchunks=5,
                                         snr=10, nlabels=4, means=[ [0,1], [1,0], [1,1], [0,0] ])
        dataset.samples += dataset.sa.chunks[:, None]*0.1 # slight shifts for chunks ;)
        #dataset = normal_feature_dataset(nfeatures=2, nlabels=3, means=[ [0,1], [1,0], [1,1] ])
        #dataset = normal_feature_dataset(nfeatures=2, nlabels=2, means=[ [0,1], [1,0] ])
        #clf = LinearCSVMC(C=-1)
        clf = kNN(4)#LinearCSVMC(C=-1)
        clf.train(dataset)
        #clf = None
        #plot_decision_boundary_2d(ds, clf)
        targets = 'targets'
        regions = 'chunks'
        #maps = 'estimates'
        maps = 'targets'
        #maps = None #'targets'
        res = 50
        vals = [-1, 0, 1]
        data_callback=None
        pl.clf()

    if dataset.nfeatures != 2:
        raise ValueError('Can only plot a decision boundary in 2D')

    Pioff()
    a = pl.gca() # f.add_subplot(1,1,1)

    attrmap = None
    if clf:
        estimates_were_enabled = clf.ca.is_enabled('estimates')
        clf.ca.enable('estimates')

        if targets is None:
            targets = clf.get_space()
        # Lets reuse classifiers attrmap if it is good enough
        attrmap = clf._attrmap
        predictions = clf.predict(dataset)

    targets_sa_name = targets           # bad Yarik -- will rebind targets to actual values
    targets_lit = dataset.sa[targets_sa_name].value
    utargets_lit = dataset.sa[targets_sa_name].unique

    if not (attrmap is not None
            and len(attrmap)
            and set(clf._attrmap.keys()).issuperset(utargets_lit)):
        # create our own
        attrmap = AttributeMap(mapnumeric=True)

    targets = attrmap.to_numeric(targets_lit)
    utargets = attrmap.to_numeric(utargets_lit)

    vmin = min(utargets)
    vmax = max(utargets)
    cmap = pl.cm.RdYlGn                  # argument

    # Scatter points
    if clf:
        all_hits = predictions == targets_lit
    else:
        all_hits = np.ones((len(targets),), dtype=bool)

    targets_colors = {}
    for l in utargets:
        targets_mask = targets==l
        s = dataset[targets_mask]
        targets_colors[l] = c \
            = cmap((l-vmin)/float(vmax-vmin))

        # We want to plot hits and misses with different symbols
        hits = all_hits[targets_mask]
        misses = np.logical_not(hits)
        scatter_kwargs = dict(
            c=[c], zorder=10+(l-vmin))

        if sum(hits):
            a.scatter(s.samples[hits, 0], s.samples[hits, 1], marker='o',
                      label='%s [%d]' % (attrmap.to_literal(l), sum(hits)),
                      **scatter_kwargs)
        if sum(misses):
            a.scatter(s.samples[misses, 0], s.samples[misses, 1], marker='x',
                      label='%s [%d] (miss)' % (attrmap.to_literal(l), sum(misses)),
                      edgecolor=[c], **scatter_kwargs)

    (xmin, xmax) = a.get_xlim()
    (ymin, ymax) = a.get_ylim()
    extent = (xmin, xmax, ymin, ymax)

    # Create grid to evaluate, predict it
    (x,y) = np.mgrid[xmin:xmax:np.complex(0, maps_res),
                    ymin:ymax:np.complex(0, maps_res)]
    news = np.vstack((x.ravel(), y.ravel())).T
    try:
        news = data_callback(news)
    except TypeError: # Not a callable object
        pass

    imshow_kwargs = dict(origin='lower',
            zorder=1,
            aspect='auto',
            interpolation='bilinear', alpha=0.9, cmap=cmap,
            vmin=vmin, vmax=vmax,
            extent=extent)

    if maps is not None:
        if clf is None:
            raise ValueError, \
                  "Please provide classifier for plotting maps of %s" % maps
        predictions_new = clf.predict(news)

    if maps == 'estimates':
        # Contour and show predictions
        trained_targets = attrmap.to_numeric(clf.ca.trained_targets)

        if len(trained_targets)==2:
            linestyles = []
            for v in vals:
                if v == 0:
                    linestyles.append('solid')
                else:
                    linestyles.append('dashed')
            vmin, vmax = -3, 3 # Gives a nice tonal range ;)
            map_ = 'estimates' # should actually depend on estimates
        else:
            vals = (trained_targets[:-1] + trained_targets[1:])/2.
            linestyles = ['solid'] * len(vals)
            map_ = 'targets'

        try:
            clf.ca.estimates.reshape(x.shape)
            a.imshow(map_values.T, **imshow_kwargs)
            CS = a.contour(x, y, map_values, vals, zorder=6,
                           linestyles=linestyles, extent=extent, colors='k')
        except ValueError, e:
            print "Sorry - plotting of estimates isn't full supported for %s. " \
                  "Got exception %s" % (clf, e)
    elif maps == 'targets':
        map_values = attrmap.to_numeric(predictions_new).reshape(x.shape)
        a.imshow(map_values.T, **imshow_kwargs)
        #CS = a.contour(x, y, map_values, vals, zorder=6,
        #               linestyles=linestyles, extent=extent, colors='k')

    # Plot regions belonging to the same pair of attribute given
    # (e.g. chunks) and targets attribute
    if regions:
        chunks_sa = dataset.sa[regions]
        chunks_lit = chunks_sa.value
        uchunks_lit = chunks_sa.value
        chunks_attrmap = AttributeMap(mapnumeric=True)
        chunks = chunks_attrmap.to_numeric(chunks_lit)
        uchunks = chunks_attrmap.to_numeric(uchunks_lit)

        from matplotlib.delaunay.triangulate import Triangulation
        from matplotlib.patches import Polygon
        # Lets figure out convex halls for each chunk/label pair
        for target in utargets:
            t_mask = targets == target
            for chunk in uchunks:
                tc_mask = np.logical_and(t_mask,
                                        chunk == chunks)
                tc_samples = dataset.samples[tc_mask]
                tr = Triangulation(tc_samples[:, 0],
                                   tc_samples[:, 1])
                poly = pl.fill(tc_samples[tr.hull, 0],
                              tc_samples[tr.hull, 1],
                              closed=True,
                              facecolor=targets_colors[target],
                              #fill=False,
                              alpha=0.01,
                              edgecolor='gray',
                              linestyle='dotted',
                              linewidth=0.5,
                              )

    pl.legend(scatterpoints=1)
    if clf and not estimates_were_enabled:
        clf.ca.disable('estimates')
    Pion()
    pl.axis('tight')
    #pl.show()

##REF: Name was automagically refactored
def plot_bars(data, labels=None, title=None, ylim=None, ylabel=None,
               width=0.2, offset=0.2, color='0.6', distance=1.0,
               yerr='ste', xloc=None, **kwargs):
    """Make bar plots with automatically computed error bars.

    Candlestick plot (multiple interleaved barplots) can be done,
    by calling this function multiple time with appropriatly modified
    `offset` argument.

    Parameters
    ----------
    data : array (nbars x nobservations) or other sequence type
      Source data for the barplot. Error measure is computed along the
      second axis.
    labels : list or None
      If not None, a label from this list is placed on each bar.
    title : str
      An optional title of the barplot.
    ylim : 2-tuple
      Y-axis range.
    ylabel : str
      An optional label for the y-axis.
    width : float
      Width of a bar. The value should be in a reasonable relation to
      `distance`.
    offset : float
      Constant offset of all bar along the x-axis. Can be used to create
      candlestick plots.
    color : matplotlib color spec
      Color of the bars.
    distance : float
      Distance of two adjacent bars.
    yerr : {'ste', 'std', None}
      Type of error for the errorbars. If `None` no errorbars are plotted.
    xloc : sequence
      Locations of the bars on the x axis.
    **kwargs
      Any additional arguments are passed to matplotlib's `bar()` function.
    """
    # determine location of bars
    if xloc is None:
        xloc = (np.arange(len(data)) * distance) + offset

    if yerr == 'ste':
        yerr = [np.std(d) / np.sqrt(len(d)) for d in data]
    elif yerr == 'std':
        yerr = [np.std(d) for d in data]
    else:
        # if something that we do not know just pass on
        pass

    # plot bars
    plot = pl.bar(xloc,
                 [np.mean(d) for d in data],
                 yerr=yerr,
                 width=width,
                 color=color,
                 ecolor='black',
                 **kwargs)

    if ylim:
        pl.ylim(*(ylim))
    if title:
        pl.title(title)

    if labels:
        pl.xticks(xloc + width / 2, labels)

    if ylabel:
        pl.ylabel(ylabel)

    # leave some space after last bar
    pl.xlim(0, xloc[-1] + width + offset)

    return plot


##REF: Name was automagically refactored
def inverse_cmap(cmap_name):
    """Create a new colormap from the named colormap, where it got reversed

    """
    import matplotlib._cm as _cm
    import matplotlib as mpl
    try:
        cmap_data = eval('_cm._%s_data' % cmap_name)
    except:
        raise ValueError, "Cannot obtain data for the colormap %s" % cmap_name
    new_data = dict( [(k, [(v[i][0], v[-(i+1)][1], v[-(i+1)][2])
                           for i in xrange(len(v))])
                      for k,v in cmap_data.iteritems()] )
    return mpl.colors.LinearSegmentedColormap('%s_rev' % cmap_name,
                                              new_data, _cm.LUTSIZE)


##REF: Name was automagically refactored
def plot_dataset_chunks(ds, clf_labels=None):
    """Quick plot to see chunk sctructure in dataset with 2 features

    if clf_labels is provided for the predicted labels, then
    incorrectly labeled samples will have 'x' in them
    """
    if ds.nfeatures != 2:
        raise ValueError, "Can plot only in 2D, ie for datasets with 2 features"
    if pl.matplotlib.get_backend() == 'TkAgg':
        pl.ioff()
    if clf_labels is not None and len(clf_labels) != ds.nsamples:
        clf_labels = None
    colors = ('b', 'g', 'r', 'c', 'm', 'y', 'k', 'w')
    labels = ds.uniquetargets
    labels_map = dict(zip(labels, colors[:len(labels)]))
    for chunk in ds.uniquechunks:
        chunk_text = str(chunk)
        ids = ds.where(chunks=chunk)
        ds_chunk = ds[ids]
        for i in xrange(ds_chunk.nsamples):
            s = ds_chunk.samples[i]
            l = ds_chunk.targets[i]
            format = ''
            if clf_labels != None:
                if clf_labels[i] != ds_chunk.targets[i]:
                    pl.plot([s[0]], [s[1]], 'x' + labels_map[l])
            pl.text(s[0], s[1], chunk_text, color=labels_map[l],
                   horizontalalignment='center',
                   verticalalignment='center',
                   )
    dss = ds.samples
    pl.axis((1.1 * np.min(dss[:, 0]),
            1.1 * np.max(dss[:, 1]),
            1.1 * np.max(dss[:, 0]),
            1.1 * np.min(dss[:, 1])))
    pl.draw()
    pl.ion()


########NEW FILE########
__FILENAME__ = erp
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Basic ERP (here ERP = Event Related Plot ;-)) plotting

Can be used for plotting not only ERP but any event-locked data
"""

import pylab as pl
import numpy as np
import matplotlib as mpl

from mvpa2.base import warning
from mvpa2.mappers.boxcar import BoxcarMapper

#
# Few helper functions
#
import matplotlib.transforms as mlt
def _offset(ax, x, y):
    """Provide offset in pixels

    Parameters
    ----------
    x : int
      Offset in pixels for x
    y : int
      Offset in pixels for y

    Idea borrowed from
     http://www.scipy.org/Cookbook/Matplotlib/Transformations
    but then heavily extended to be compatible with many
    reincarnations of matplotlib
    """
    d = dir(mlt)
    if 'offset_copy' in d:
        # tested with python-matplotlib 0.98.3-5
        # ??? if pukes, might need to replace 2nd parameter from
        #     ax to ax.get_figure()
        return mlt.offset_copy(ax.transData, ax, x=x, y=y, units='dots')
    elif 'BlendedAffine2D' in d:
        # some newer versions of matplotlib
        return ax.transData + \
               mlt.Affine2D().translate(x, y)
    elif 'blend_xy_sep_transform' in d:
        trans = mlt.blend_xy_sep_transform(ax.transData, ax.transData)
        # Now we set the offset in pixels
        trans.set_offset((x, y), mlt.identity_transform())
        return trans
    else:
        raise RuntimeError, \
              "Lacking needed functions in matplotlib.transform " \
              "for _offset. Please upgrade"


def _make_centeredaxis(ax, loc, offset=5, ai=0, mult=1.0,
                       format='%4g', label=None, **props):
    """Plot an axis which is centered at loc (e.g. 0)

    Parameters
    ----------
    ax
     Axes from the figure
    loc
     Value to center at
    offset
     Relative offset (in pixels) for the labels
    ai : int
     Axis index: 0 for x, 1 for y
    mult
     Multiplier for the axis labels. ERPs for instance need to be
     inverted, thus labels too manually here since there is no easy
     way in matplotlib to invert an axis
    label : str or None
     If not -- put a label outside of the axis
    **props
     Given to underlying plotting functions

    Idea borrowed from
      http://www.mail-archive.com/matplotlib-users@lists.sourceforge.net \
      /msg05669.html
    It sustained heavy refactoring/extension
    """
    xmin, xmax = ax.get_xlim()
    ymin, ymax = ax.get_ylim()

    xlocs = [l for l in ax.xaxis.get_ticklocs()
            if l >= xmin and l <= xmax]
    ylocs = [l for l in ax.yaxis.get_ticklocs()
            if l >= ymin and l <= ymax]

    if ai == 0:
        hlocs = ylocs
        locs = xlocs
        vrange = [xmin, xmax]
        tdir = mpl.lines.TICKDOWN
        halignment = 'center'
        valignment = 'top'
        lhalignment = 'left'
        lvalignment = 'center'
        lx, ly = xmax, 0
        ticklength = ax.xaxis.get_ticklines()[0]._markersize
    elif ai == 1:
        hlocs = xlocs
        locs = ylocs
        vrange = [ymin, ymax]
        tdir = mpl.lines.TICKLEFT
        halignment = 'right'
        valignment = 'center'
        lhalignment = 'center'
        lvalignment = 'bottom'
        lx, ly = 0, ymax
        ticklength = ax.yaxis.get_ticklines()[0]._markersize
    else:
        raise ValueError, "Illegal ai=%s" % ai

    args = [ (locs, [loc] * len(locs)),
             (vrange, [loc, loc]),
             [locs, (loc,) * len(locs)]
             ]

    offset_abs = offset + ticklength
    if ai == 1:
        # invert
        args = [ [x[1], x[0]] for x in args ]
        # shift the tick labels labels
        trans = _offset(ax, -offset_abs, 0)
        transl = _offset(ax, 0, offset)
    else:
        trans = _offset(ax, 0, -offset_abs)
        transl = _offset(ax, offset, 0)

    tickline, = ax.plot(linestyle='', marker=tdir, *args[0], **props)
    axline, = ax.plot(*args[1], **props)

    tickline.set_clip_on(False)
    axline.set_clip_on(False)


    for i, l in enumerate(locs):
        if l == 0:                    # no origin label
            continue
        coor = [args[2][0][i], args[2][1][i], format % (mult * l)]
        ax.text(horizontalalignment=halignment,
                verticalalignment=valignment, transform=trans, *coor)


    if label is not None:
        ax.text(
            #max(args[2][0]), max(args[2][1]),
            lx, ly,
            label,
            horizontalalignment=lhalignment,
            verticalalignment=lvalignment, fontsize=14,
            # fontweight='bold',
            transform=transl)


##REF: Name was automagically refactored
def plot_erp(data, SR=500, onsets=None,
            pre=0.2, pre_onset=None, post=None, pre_mean=None,
            color='r', errcolor=None, errtype=None, ax=pl,
            ymult=1.0, *args, **kwargs):
    """Plot single ERP on existing canvas

    Parameters
    ----------
    data : 1D or 2D ndarray
      The data array can either be 1D (samples over time) or 2D
      (trials x samples). In the first case a boxcar mapper is used to
      extract the respective trial timecourses given a list of trial onsets.
      In the latter case, each row of the data array is taken as the EEG
      signal timecourse of a particular trial.
    onsets : list(int)
      List of onsets (in samples not in seconds).
    SR : int, optional
      Sampling rate (1/s) of the signal.
    pre : float, optional
      Duration (in seconds) to be plotted prior to onset.
    pre_onset : float or None
      If data is already in epochs (2D) then pre_onset provides information
      on how many seconds pre-stimulus were used to generate them. If None,
      then pre_onset = pre
    post : float
      Duration (in seconds) to be plotted after the onset.
    pre_mean : float
      Duration (in seconds) at the beginning of the window which is used
      for deriving the mean of the signal. If None, pre_mean = pre. If 0,
      then the mean is not subtracted from the signal.
    errtype : None or 'ste' or 'std' or 'ci95' or list of previous three
      Type of error value to be computed per datapoint.  'ste' --
      standard error of the mean, 'std' -- standard deviation 'ci95'
      -- 95% confidence interval (1.96 * ste), None -- no error margin
      is plotted (default)
      Optionally, multiple error types can be specified in a list. In that
      case all of them will be plotted.
    color : matplotlib color code, optional
      Color to be used for plotting the mean signal timecourse.
    errcolor : matplotlib color code
      Color to be used for plotting the error margin. If None, use main color
      but with weak alpha level
    ax :
      Target where to draw.
    ymult : float, optional
      Multiplier for the values. E.g. if negative-up ERP plot is needed:
      provide ymult=-1.0
    *args, **kwargs
      Additional arguments to `pylab.plot`.

    Returns
    -------
    array
      Mean ERP timeseries.
    """
    if pre_mean is None:
        pre_mean = pre

    # set default
    pre_discard = 0

    if onsets is not None: # if we need to extract ERPs
        if post is None:
            raise ValueError, \
                  "Duration post onsets must be provided if onsets are given"
        # trial timecourse duration
        duration = pre + post

        # We are working with a full timeline
        bcm = BoxcarMapper(onsets,
                           boxlength=int(SR * duration),
                           offset= -int(SR * pre))
        erp_data = bcm(data)

        # override values since we are using Boxcar
        pre_onset = pre
    else:
        if pre_onset is None:
            pre_onset = pre

        if pre_onset < pre:
            warning("Pre-stimulus interval to plot %g is smaller than provided "
                    "pre-stimulus captured interval %g, thus plot interval was "
                    "adjusted" % (pre, pre_onset))
            pre = pre_onset

        if post is None:
            # figure out post
            duration = float(data.shape[1]) / SR - pre_discard
            post = duration - pre
        else:
            duration = pre + post

        erp_data = data
        pre_discard = pre_onset - pre

    # Scale the data appropriately
    erp_data *= ymult

    # validity check -- we should have 2D matrix (trials x samples)
    if len(erp_data.shape) != 2:
        raise RuntimeError, \
              "plot_erp() supports either 1D data with onsets, or 2D data " \
              "(trials x sample_points). Shape of the data at the point " \
              "is %s" % erp_data.shape

    if not (pre_mean == 0 or pre_mean is None):
        # mean of pre-onset signal accross trials
        erp_baseline = np.mean(
            erp_data[:, int((pre_onset - pre_mean) * SR):int(pre_onset * SR)])
        # center data on pre-onset mean
        # NOTE: make sure that we make a copy of the data to don't
        #       alter the original. Better be safe than sorry
        erp_data = erp_data - erp_baseline

    # generate timepoints and error ranges to plot filled error area
    # top ->
    # bottom <-
    time_points = np.arange(erp_data.shape[1]) * 1.0 / SR - pre_onset

    # if pre != pre_onset
    if pre_discard > 0:
        npoints = int(pre_discard * SR)
        time_points = time_points[npoints:]
        erp_data = erp_data[:, npoints:]

    # select only time points of interest (if post is provided)
    if post is not None:
        npoints = int(duration * SR)
        time_points = time_points[:npoints]
        erp_data = erp_data[:, :npoints]

    # compute mean signal timecourse accross trials
    erp_mean = np.mean(erp_data, axis=0)

    # give sane default
    if errtype is None:
        errtype = []
    if not isinstance(errtype, list):
        errtype = [errtype]

    for et in errtype:
        # compute error per datapoint
        if et in ['ste', 'ci95']:
            erp_stderr = erp_data.std(axis=0) / np.sqrt(len(erp_data))
            if et == 'ci95':
                erp_stderr *= 1.96
        elif et == 'std':
            erp_stderr = erp_data.std(axis=0)
        else:
            raise ValueError, "Unknown error type '%s'" % errtype

        time_points2w = np.hstack((time_points, time_points[::-1]))

        error_top = erp_mean + erp_stderr
        error_bottom = erp_mean - erp_stderr
        error2w = np.hstack((error_top, error_bottom[::-1]))

        if errcolor is None:
            errcolor = color

        # plot error margin
        pfill = ax.fill(time_points2w, error2w,
                        edgecolor=errcolor, facecolor=errcolor, alpha=0.2,
                        zorder=3)

    # plot mean signal timecourse
    ax.plot(time_points, erp_mean, lw=2, color=color, zorder=4,
            *args, **kwargs)
#    ax.xaxis.set_major_locator(pl.MaxNLocator(4))
    return erp_mean


##REF: Name was automagically refactored
def plot_erps(erps, data=None, ax=None, pre=0.2, post=None,
             pre_onset=None,
             xlabel='time (s)', ylabel='$\mu V$',
             ylim=None, ymult=1.0, legend=None,
             xlformat='%4g', ylformat='%4g',
             loffset=10, alinewidth=2,
             **kwargs):
    """Plot multiple ERPs on a new figure.

    Parameters
    ----------
    erps : list of tuples
      List of definitions of ERPs. Each tuple should consist of
      (label, color, onsets) or a dictionary which defines,
      label, color, onsets, data. Data provided in dictionary overrides
      'common' data provided in the next argument `data`
    data
      Data for ERPs to be derived from 1D (samples)
    ax
      Where to draw (e.g. subplot instance). If None, new figure is
      created
    pre : float, optional
      Duration (seconds) to be plotted prior to onset
    pre_onset : None or float
      If data is already in epochs (2D) then pre_onset provides information
      on how many seconds pre-stimulus were used to generate them. If None,
      then pre_onset = pre
    post : None or float
      Duration (seconds) to be plotted after the onset. If any data is
      provided with onsets, it can't be None. If None -- plots all time
      points after onsets
    ymult : float, optional
      Multiplier for the values. E.g. if negative-up ERP plot is needed:
      provide ymult=-1.0
    xlformat : str, optional
      Format of the x ticks
    ylformat : str, optional
      Format of the y ticks
    legend : None or string
      If not None, legend will be plotted with position argument
      provided in this argument
    loffset : int, optional
      Offset in voxels for axes and tick labels. Different
      matplotlib frontends might have different opinions, thus
      offset value might need to be tuned specifically per frontend
    alinewidth : int, optional
      Axis and ticks line width
    **kwargs
      Additional arguments provided to plot_erp()


    Examples
    --------

    ::

      kwargs  = {'SR' : eeg.SR, 'pre_mean' : 0.2}
      fig = plot_erps((('60db', 'b', eeg.erp_onsets['60db']),
                       ('80db', 'r', eeg.erp_onsets['80db'])),
                      data[:, eeg.sensor_mapping['Cz']],
                      ax=fig.add_subplot(1,1,1,frame_on=False), pre=0.2,
                      post=0.6, **kwargs)

    or

    ::
    
        fig = plot_erps((('60db', 'b', eeg.erp_onsets['60db']),
                          {'color': 'r',
                           'onsets': eeg.erp_onsets['80db'],
                           'data' : data[:, eeg.sensor_mapping['Cz']]}
                         ),
                        data[:, eeg.sensor_mapping['Cz']],
                        ax=fig.add_subplot(1,1,1,frame_on=False), pre=0.2,
                        post=0.6, **kwargs)

    Returns
    -------
    h
      current fig handler
    """

    if ax is None:
        fig = pl.figure(facecolor='white')
        fig.clf()
        ax = fig.add_subplot(111, frame_on=False)
    else:
        fig = pl.gcf()

    # We don't want original axis being on
    ax.axison = True

    labels = []
    for erp_def in erps:
        plot_data = data
        params = {'ymult' : ymult}

        # provide custom parameters per ERP
        if isinstance(erp_def, tuple) and len(erp_def) == 3:
            params.update(
                {'label': erp_def[0],
                 'color': erp_def[1],
                 'onsets': erp_def[2]})
        elif isinstance(erp_def, dict):
            plot_data = erp_def.pop('data', None)
            params.update(erp_def)

        labels.append(params.get('label', ''))

        # absorb common parameters
        params.update(kwargs)

        if plot_data is None:
            raise ValueError, "Channel %s got no data provided" \
                  % params.get('label', 'UNKNOWN')


        plot_erp(plot_data, pre=pre, pre_onset=pre_onset, post=post, ax=ax,
                **params)
        #             plot_kwargs={'label':label})

        if isinstance(erp_def, dict):
            erp_def['data'] = plot_data # return it back

    props = dict(color='black',
                 linewidth=alinewidth, markeredgewidth=alinewidth,
                 zorder=1, offset=loffset)

    def set_limits():
        """Helper to set x and y limits"""
        ax.set_xlim((-pre, post))
        if ylim != None:
            ax.set_ylim(*ylim)

    set_limits()
    _make_centeredaxis(ax, 0, ai=0, label=xlabel, **props)
    set_limits()
    _make_centeredaxis(ax, 0, ai=1, mult=np.sign(ymult), label=ylabel, **props)

    ax.yaxis.set_major_locator(pl.NullLocator())
    ax.xaxis.set_major_locator(pl.NullLocator())

    # legend obscures plotting a bit... seems to be plotting
    # everything twice. Thus disabled by default
    if legend is not None and np.any(np.array(labels) != ''):
        pl.legend(labels, loc=legend)

    fig.canvas.draw()
    return fig


########NEW FILE########
__FILENAME__ = flat_surf
# emacs: -*- coding: utf-8; mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
#   The initial version of the code was contributed by Ingo Fründ and is
#   Coypright (c) 2008 by Ingo Fründ ingo.fruend@googlemail.com
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Plot flat maps of cortical surfaces.

WiP"""

__docformat__ = 'restructuredtext'

import numpy as np
from mvpa2.support.nibabel import surf, afni_suma_1d
from mvpa2.datasets.base import Dataset
import re

from mvpa2.base import externals

if externals.exists("pylab", raise_=True):
    import pylab as pl

if externals.exists("matplotlib", raise_=True):
    import matplotlib.pyplot as plt


if externals.exists("griddata", raise_=True):
    from mvpa2.support.griddata import griddata



def unstructured_xy2grid_xy_vectors(x, y, min_nsteps):
    '''From unstructured x and y values, return lists of x and y coordinates
    to form a grid
    
    Parameters
    ----------
    x: np.ndarray
        x coordinates
    y: np.ndarray
        y coordinates
    min_nsteps: int
        minimal length of output
        
    Returns
    -------
    (xi, yi): tuple of np.ndarray
        xi contains values ranging from (approximately) min(x) to max(x);
        yi is similar. min(len(xi),len(yi))=min_steps.
    '''
    if len(x) != len(y):
        raise ValueError('Shape mismatch')

    xmin, ymin = np.min(x), np.min(y)
    xmax, ymax = np.max(x), np.max(y)

    xran, yran = xmax - xmin, ymax - ymin
    delta = min(xran, yran) / (min_nsteps - 1)
    xsteps = 1 + np.ceil(xran / delta)
    ysteps = 1 + np.ceil(yran / delta)

    # x and y values on the grid
    xi = (np.arange(xsteps) + .5) * delta + xmin
    yi = (np.arange(ysteps) + .5) * delta + ymin

    return xi, yi

def flat_surface2xy(surface):
    '''Returns a tuple with x and y coordinates of a flat surface
    
    Parameters
    ----------
    surface: Surface
        flat surface
    
    Returns
    -------
    x: np.ndarray
        x coordinates
    y: np.ndarray
        y coordinates
    
    Notes
    -----
    If the surface is not flat (any z coordinate is non-zero), an exception
    is raised.
    '''

    s = surf.from_any(surface)
    v = s.vertices
    if any(v[:, 2] != 0):
        raise ValueError("Expected a flat surface with z=0 for all nodes")

    x = v[:, 0]
    y = v[:, 1]

    return x, y


def flat_surface2grid_mask(surface, min_nsteps):
    '''Computes a mask and corresponding coordinates from a flat surface 
    
    Parameters
    ----------
    surface: Surface
        flat surface
    min_nsteps: int
        minimum number of pixels in x and y direction
        
    Returns
    -------
    x: np.ndarray
        x coordinates of surface
    y: np.ndarray
        y coordinates of surface
    m: np.ndarray
        mask array of size PxQ, with min(P,Q)==min_nsteps.
        m[i,j]==True iff the position at (i,j) is 'inside' the flat surface
    xi: np.ndarray
        vector of length Q with interpolated x coordinates
    yi: np.ndarray
        vector of length P with interpolated y coordinates
    
    Notes
    -----
    The output of this function can be used with scipy.interpolate.griddata
    '''

    surface = surf.from_any(surface)
    x, y = flat_surface2xy(surface)
    xmin = np.min(x)

    xi, yi = unstructured_xy2grid_xy_vectors(x, y , min_nsteps)
    delta = xi[1] - xi[0]
    vi2xi = (x - xmin) / delta

    # compute paths of nodes on the border
    pths = surface.nodes_on_border_paths()

    # map x index to segments that cross the x coordinate
    # (a segment is a pair (i,j) where nodes i and j share a triangle
    #  and are on the border)
    xidx2segments = dict()

    for pth in pths:
        # make a tour across pairs (i,j)
        j = pth[-1]
        for i in pth:
            pq = vi2xi[i], vi2xi[j]
            p, q = min(pq), max(pq)
            # always go left (p) to right (q)
            for pqs in np.arange(np.ceil(p), np.ceil(q)):
                # take each point in between
                ipqs = int(pqs)

                # add to xidx2segments
                if not ipqs in xidx2segments:
                    xidx2segments[ipqs] = list()
                xidx2segments[ipqs].append((i, j))

            # take end point from last iteration as starting point
            # in next iteration
            j = i


    # space for the mask
    yxshape = len(yi), len(xi)
    msk = np.zeros(yxshape, dtype=np.bool_)

    # see which nodes are *inside* a surface 
    # (there can be multiple surfaces)
    for ii, xpos in enumerate(xi):
        if not ii in xidx2segments:
            continue
        segments = xidx2segments[ii]
        for jj, ypos in enumerate(yi):
            # based on PNPOLY (W Randoph Franklin)
            # http://www.ecse.rpi.edu/~wrf/Research/Short_Notes/pnpoly.html
            # retrieved Apr 2013
            c = False
            for i, j in segments:
                if ypos < (y[j] - y[i]) * (xpos - x[i]) / (x[j] - x[i]) + y[i]:
                    c = not c
            msk[jj, ii] = np.bool(c)

    return x, y, msk, xi, yi

def _scale(xs, target_min=0., target_max=1., source_min=None, source_max=None):
    '''Scales from [smin,smax] to [tmin,tmax]'''
    mn = np.nanmin(xs, axis= -1)[np.newaxis].T if source_min is None\
                                                             else source_min
    mx = np.nanmax(xs, axis= -1)[np.newaxis].T if source_max is None\
                                                             else source_max

    scaled = (xs - mn) / (mx - mn)
    return scaled * (target_max - target_min) + target_min


def flat_surface_curvature2rgba(curvature):
    '''Computes an RGBA colormap in black and white, based on curvature'''
    curvature = curvature_from_any(curvature)

    cmap = plt.get_cmap('binary')

    # invert it so match traditional 'sulcus=dark', then scale to [0,1]
    c = _scale(-curvature)

    return cmap(c)

def _range2min_max(range_, xs):
    '''Converts a range description to a minimum and maximum value

    Parameters
    ----------
    range_: str or float or tuple
        If a tuple (a,b), then this tuple is returned.
        If a float a, then (-a,a) is returned.
        "R(a)", where R(a) denotes the string representation
        of float a, is equivalent to range_=a.
        "R(a)_R(b)" is equivalent to range_=(a,b).
        "R(a)_R(b)%" indicates that the a-th and b-th 
        percentile of xs is taken to define the range.
        "R(a)%" is equivalent to "R(a)_R(100-a)%"
    xs: np.ndarray
        Data array - used to define the range if range_ ends
        with '%'.

    Returns
    -------
    mn, mx: tuple of float
        minimum and maximum value according to the range
    '''

    try:
        r = float(range_)
        if r < 0:
            raise RuntimeError("Single value should be positive")
        return _range2min_max((-r, r), xs)
    except (ValueError, TypeError):
        if isinstance(range_, basestring):
            pat = '(?P<mn>\d*)_?(?P<mx>\d+)?(?P<pct>%)?'

            m = re.match(pat, range_)
            g = m.groups()
            mn, mx, p = g
            if mn != 0 and not mn:
                raise ValueError("Not understood: %s" % range_)
            mn = float(mn)

            percentage = p == '%'
            if percentage:
                xmn = np.nanmin(xs)
                xmx = np.nanmax(xs)

                mx = 100 - mn if mx != 0 and not mx else float(mx)

                mn *= .01
                mx *= .01

                mn, mx = np.asarray([mn, mx]) * (xmx - xmn) + xmn
            else:
                mx = float(mx)

        else:
            mn, mx = map(float, range_)
        return mn, mx




def flat_surface_data2rgba(data, range_='2_98%', threshold=None, color_map=None):
    '''Computes an RGBA colormap for surface data'''

    if isinstance(data, Dataset):
        data = data.samples

    cmap = plt.get_cmap(color_map)

    mn, mx = _range2min_max(range_, data)
    scaled = _scale(data, 0., 1., mn, mx)
    rgba = cmap(scaled)
    if not threshold is None:
        mn, mx = _range2min_max(threshold, data)
        to_remove = np.logical_and(data > mn, data < mx)
        rgba[to_remove, :] = np.nan

    return rgba

def curvature_from_any(c):
    '''Reads curvature'''
    if isinstance(c, basestring):
        from mvpa2.support.nibabel import afni_suma_1d
        c = afni_suma_1d.from_any(c)

        # typical SUMA use case: first column has node indices,
        # second column 
        if len(c.shape) > 1 and c.shape[1] == 2 and \
                set(c[:, 0]) == set(range(1 + int(max(c[:, 0])))):
            cc = c
            n = cc.shape[0]
            c = np.zeros((n,))
            idxs = np.asarray(cc[:, 0], dtype=np.int_)
            c[idxs] = cc[:, 1]

    return np.asarray(c)

class FlatSurfacePlotter(object):
    '''Plots data on a flat surface'''
    def __init__(self, surface, curvature=None, min_nsteps=500,
                        range_='2_98%', threshold=None, color_map=None):
        '''
        Parameters
        ----------
        surface: surf.Surface
            a flat surface
        curvature: str or np.ndarray
            (Filename of) data representing curvature at each node. 
        min_steps: int
            Minimal side of output plots in pixel
        range_: str or float or tuple
            If a tuple (a,b), then this tuple is returned.
            If a float a, then (-a,a) is returned.
            "R(a)", where R(a) denotes the string representation
            of float a, is equivalent to range_=a.
            "R(a)_R(b)" is equivalent to range_=(a,b).
            "R(a)_R(b)%" indicates that the a-th and b-th 
            percentile of xs is taken to define the range.
            "R(a)%" is equivalent to "R(a)_R(100-a)%"
        threshold: str or float or tuple
            Indicates which values will be shown. Syntax as in range_
        color_map: str
            colormap to use
        '''

        self._surface = surf.from_any(surface)

        if curvature is None:
            self._curvature = None
        else:
            self._curvature = curvature_from_any(curvature)
            if self._surface.nvertices != self._curvature.size:
                raise ValueError("Surface has %d vertices, but curvature %d" %
                                  (self._surface.nvertices, self._curvature.size))

        self._min_nsteps = min_nsteps
        self._range_ = range_
        self._threshold = threshold
        self._color_map = color_map

        self._grid_def = None
        self._underlay = None

    def set_underlay(self, u):
        '''Sets the underlay'''
        self._underlay = u.copy()


    def _set_underlay_from_curvature(self):
        if self._curvature is None:
            raise ValueError("Curvature is not set")

        if self._grid_def is None:
            self._set_grid_def()

        x, y, msk, xi, yi = self._grid_def

        ulay = griddata(x, y, self._curvature, xi, yi)
        ulay[-msk] = np.nan

        rgba = flat_surface_curvature2rgba(ulay)
        self.set_underlay(rgba)

    def _pre_setup(self):
        if self._grid_def is None:
            self._grid_def = flat_surface2grid_mask(self._surface, \
                                                    self._min_nsteps)

        if self._underlay is None and not self._curvature is None:
            self._set_underlay_from_curvature()


    def __call__(self, data):
        '''
        Parameters
        ----------
        data: np.ndarray
            Surface data to be plotted. Should have the same number of data
            points as the surface
        
        Returns
        -------
        rgba: np.ndarray
            Bitmap with RGBA values that can be plotted.
        '''
        self._pre_setup()
        x, y, msk, xi, yi = self._grid_def
        olay = griddata(x, y, data, xi, yi)
        olay[-msk] = np.nan

        o_rgba = flat_surface_data2rgba(olay, self._range_ , self._threshold,
                                                self._color_map)
        o_rgba[-msk] = np.nan # apply the mask again, to be sure

        if not self._underlay is None:
            o_msk = -np.isnan(np.sum(o_rgba, 2))

            u_rgba = self._underlay
            u_msk = np.logical_and(-o_msk, msk)

            o_rgba[u_msk] = u_rgba[u_msk]

        return o_rgba


########NEW FILE########
__FILENAME__ = lightbox
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Basic (f)MRI plotting with ability to interactively perform thresholding

"""

import pylab as pl
import numpy as np
import matplotlib as mpl

from mvpa2.base import warning, externals
from mvpa2.misc.plot.tools import Pion, Pioff, mpl_backend_isinteractive

if __debug__:
    from mvpa2.base import debug

if externals.exists('nibabel'):
    import nibabel as nb
    from nibabel.spatialimages import SpatialImage
else:
    class SpatialImage(object):
        """Just a helper to allow plot_lightbox be used even if no
        nibabel module available for plotting regular 2D/3D images
        (ndarrays)"""
        def __init__(self, filename):
            raise ValueError, "plot_lightbox was provided a filename %s.  " \
                  "By now we only support loading data from Nifti/Analyze " \
                  "files, but nibabel module is not available" % filename


def plot_lightbox(background=None, background_mask=None, cmap_bg='gray',
            overlay=None, overlay_mask=None, cmap_overlay='autumn',
            vlim=(0.0, None), vlim_type=None,
            do_stretch_colors=False,
            add_info=True, add_hist=True, add_colorbar=True,
            fig=None, interactive=None,
            nrows=None, ncolumns=None,
            slices=None, slice_title="k=%(islice)s"
            ):
    """Very basic plotting of 3D data with interactive thresholding.

    `background`/`overlay` and corresponding masks could be nifti
    files names or `SpatialImage` objects, or 3D `ndarrays`. If no mask
    is provided, only non-0 elements are plotted.

    Notes
    -----
    No care is taken to deduce the orientation (e.g. Left-to-Right,
    Posterior-to-Anterior) of fMRI volumes.  Therefore all input
    volumes should be in the same orientation.

    Parameters
    ----------
    do_stretch_colors : bool, optional
      Stratch color range to the data (not just to visible data)
    vlim : tuple, optional
      2 element tuple of low/upper bounds of values to plot
    vlim_type : None or 'symneg_z'
      If not None, then vlim would be treated accordingly:
       symneg_z
         z-score values of symmetric normal around 0, estimated
         by symmetrizing negative part of the distribution, which
         often could be assumed when total distribution is a mixture of
         by-chance performance normal around 0, and some other in the
         positive tail
    ncolumns : int or None
      Explicit starting number of columns into which position the
      slice renderings.
      If None, square arrangement would be used
    nrows : int or None
      Explicit starting number of rows into which position the
      slice renderings.
      If None, square arrangement would be used
    add_hist : bool or tuple (int, int)
      If True, add histogram and position automagically.
      If a tuple -- use as (row, column)
    add_info : bool or tuple (int, int)
      If True, add information and position automagically.
      If a tuple -- use as (row, column).
    slices : None or list of int
      If not to plot whole volume, what slices to plot.
    slice_title : None or str
      Desired title of slices.  Use string comprehension and assume
      `islice` variable present with current slice index.

    Available colormaps are presented nicely on
      http://www.scipy.org/Cookbook/Matplotlib/Show_colormaps

    TODO:
      * Make interface more attractive/usable
      * allow multiple overlays... or just unify for them all to be just a list of entries
      * handle cases properly when there is only one - background/overlay
    """

    def handle_arg(arg):
        """Helper which would read in SpatialImage if necessary
        """
        if arg is None:
            return arg
        if isinstance(arg, basestring):
            arg = nb.load(arg)
            argshape = arg.get_shape()
            # Assure that we have 3D (at least)
            if len(argshape)<3:
                arg = nb.Nifti1Image(
                        arg.get_data().reshape(argshape + (1,)*(3-len(argshape))),
                        arg.get_affine(),
                        arg.get_header())
        else:
            argshape = arg.shape

        if len(argshape) == 4:
            if argshape[-1] > 1:
                warning("For now plot_lightbox can handle only 3d, 4d data was provided."
                        " Plotting only the first volume")
            if isinstance(arg, SpatialImage):
                arg = nb.Nifti1Image(arg.get_data()[..., 0], arg.get_affine(), arg.get_header())
            else:
                arg = arg[..., 0]
        elif len(argshape) != 3:
            raise ValueError, "For now just handling 3D volumes"
        return arg

    bg = handle_arg(background)
    if isinstance(bg, SpatialImage):
        # figure out aspect
        # fov = (np.array(bg.header['pixdim']) * bg.header['dim'])[3:0:-1]
        # aspect = fov[1]/fov[2]
        # just scale by voxel-size ratio (extent is disabled)
        bg_hdr = bg.get_header()
        aspect = bg_hdr.get_zooms()[2] / bg_hdr.get_zooms()[1]

        bg = bg.get_data()
    else:
        aspect = 1.0

    bg_mask = None
    if bg is not None:
        bg_mask = handle_arg(background_mask)
        if isinstance(bg_mask, SpatialImage):
            bg_mask = bg_mask.get_data()
        if bg_mask is not None:
            bg_mask = bg_mask != 0
        else:
            bg_mask = bg != 0

    func = handle_arg(overlay)

    if func is not None:
        if isinstance(func, SpatialImage):
            func = func.get_data()

        func_mask = handle_arg(overlay_mask)
        if isinstance(func_mask, SpatialImage):
            func_mask = func_mask.get_data() #[..., ::-1, :] # XXX
        if func_mask is not None:
            func_mask = func_mask != 0
        else:
            func_mask = func != 0

    # Lets assure that that we are dealing with <= 3D
    for v in (bg, bg_mask, func, func_mask):
        if v is not None:
            if v.ndim > 3:
                # we could squeeze out first bogus dimensions
                if np.all(np.array(v.shape[3:]) == 1):
                    v.shape = v.shape[:3]
                else:
                    raise ValueError, \
                          "Original shape of some data is %s whenever we " \
                          "can accept only 3D images (or ND with degenerate " \
                          "first dimensions)" % (v.shape,)

    # process vlim
    vlim = list(vlim)
    vlim_orig = vlim[:]
    add_dist2hist = []
    if isinstance(vlim_type, basestring):
        if vlim_type == 'symneg_z':
            func_masked = func[func_mask]
            fnonpos = func_masked[func_masked<=0]
            fneg = func_masked[func_masked<0]
            # take together with sign-reverted negative values
            fsym = np.hstack((-fneg, fnonpos))
            nfsym = len(fsym)
            # Estimate normal std under assumption of mean=0
            std = np.sqrt(np.mean(abs(fsym)**2))
            # convert vlim assuming it is z-scores
            for i,v in enumerate(vlim):
                if v is not None:
                    vlim[i] = std * v
            # add a plot to histogram
            add_dist2hist = [(lambda x: nfsym/(np.sqrt(2*np.pi)*std) \
                                        *np.exp(-(x**2)/(2*std**2)),
                              {})]
        else:
            raise ValueError, 'Unknown specification of vlim=%s' % vlim + \
                  ' Known is: symneg'


    class Plotter(object):
        """
        TODO
        """

        #_store_attribs = ('vlim', 'fig', 'bg', 'bg_mask')

        def __init__(self, _locals):
            """TODO"""
            self._locals = _locals
            self.fig = _locals['fig']

        def do_plot(self):
            """TODO"""
            # silly yarik didn't find proper way
            vlim = self._locals['vlim']
            bg = self._locals['bg']
            bg_mask = self._locals['bg_mask']
            ncolumns = self._locals['ncolumns']
            nrows = self._locals['nrows']
            add_info = self._locals['add_info']
            add_hist = self._locals['add_hist']
            slices = self._locals['slices']
            slice_title = self._locals['slice_title']
            if np.isscalar(vlim): vlim = (vlim, None)
            if vlim[0] is None: vlim = (np.min(func), vlim[1])
            if vlim[1] is None: vlim = (vlim[0], np.max(func))
            if __debug__ and 'PLLB' in debug.active:
                debug('PLLB', "Maximum %g at %s, vlim is %s" %
                      (np.max(func), np.where(func==np.max(func)), str(vlim)))
            invert = vlim[1] < vlim[0]
            if invert:
                vlim = (vlim[1], vlim[0])
                print "Not yet fully supported"

            # adjust lower bound if it is too low
            # and there are still multiple values ;)
            func_masked = func[func_mask]
            if vlim[0] < np.min(func_masked) and \
                   np.min(func_masked) != np.max(func_masked):
                vlim = list(vlim)
                vlim[0] = np.min(func[func_mask])
                vlim = tuple(vlim)

            bound_above = (max(vlim) < np.max(func))
            bound_below = (min(vlim) > np.min(func))

            #
            # reverse the map if needed
            cmap_ = cmap_overlay
            if not bound_below and bound_above:
                if cmap_.endswith('_r'):
                    cmap_ = cmap_[:-2]
                else:
                    cmap_ += '_r'

            func_cmap = eval("pl.cm.%s" % cmap_)
            bg_cmap = eval("pl.cm.%s" % cmap_bg)

            if do_stretch_colors:
                clim = (np.min(func), np.max(func))#vlim
            else:
                clim = vlim

            #
            # figure out 'extend' for colorbar and threshold string
            extend, thresh_str = {
                (True, True) : ('both', 'x in [%.3g, %.3g]' % tuple(vlim)),
                (True, False): ('min', 'x in [%.3g, +inf]' % vlim[0]),
                (False, True): ('max', 'x in (-inf, %.3g]' % vlim[1]),
                (False, False): ('neither', 'none') }[(bound_below,
                                                       bound_above)]

            #
            # Figure out subplots
            dshape = func.shape
            if slices is None:
                slices = range(func.shape[-1])
            nslices = len(slices)

            # more or less square alignment ;-)
            if ncolumns is None:
                ncolumns = int(np.sqrt(nslices))
            ndcolumns = ncolumns
            nrows = max(nrows, int(np.ceil(nslices*1.0/ncolumns)))

            # Check if additional column/row information was provided
            # and extend nrows/ncolumns
            for v in (add_hist, add_info):
                if v and not isinstance(v, bool):
                    ncolumns = max(ncolumns, v[1]+1)
                    nrows = max(nrows, v[0]+1)



            # Decide either we need more cells where to add hist and/or info
            nadd = bool(add_info) + bool(add_hist)
            while ncolumns*nrows - (nslices + nadd) < 0:
                ncolumns += 1

            locs = ['' for i in xrange(ncolumns*nrows)]

            # Fill in predefined locations
            for v,vl in ((add_hist, 'hist'),
                         (add_info, 'info')):
                if v and not isinstance(v, bool):
                    locs[ncolumns*v[0] + v[1]] = vl

            # Fill in slices
            for islice in slices:
                locs[locs.index('')] = islice

            # Fill the last available if necessary
            if add_hist and isinstance(add_hist, bool):
                locs[locs.index('')] = 'hist'
            if add_info and isinstance(add_info, bool):
                locs[locs.index('')] = 'info'

            Pioff()

            if self.fig is None:
                self.fig = pl.figure(facecolor='white',
                                    figsize=(4*ncolumns, 4*nrows))
            fig = self.fig
            fig.clf()
            #
            # how to threshold images
            thresholder = lambda x: np.logical_and(x>=vlim[0],
                                                  x<=vlim[1]) ^ invert

            #
            # Draw all slices
            self.slices_ax = []
            im0 = None
            for islice in slices[::-1]: #range(nslices)[::-1]:
                ax = fig.add_subplot(nrows, ncolumns,
                                     locs.index(islice) + 1,
                                     frame_on=False)
                self.slices_ax.append(ax)
                ax.axison = False
                slice_bg_nvoxels = None
                if bg is not None:
                    slice_bg = bg[:, :, islice]

                    slice_bg_ = np.ma.masked_array(slice_bg,
                                                  mask=np.logical_not(bg_mask[:, :, islice]))
                                                  #slice_bg<=0)
                    slice_bg_nvoxels = len(slice_bg_.nonzero()[0])
                    if __debug__:
                        debug('PLLB', "Plotting %i background elements in slice %i"
                              % (slice_bg_nvoxels, islice))

                slice_sl  = func[:, :, islice]

                in_thresh = thresholder(slice_sl)
                out_thresh = np.logical_not(in_thresh)
                slice_sl_ = np.ma.masked_array(slice_sl,
                                mask=np.logical_or(out_thresh,
                                                  np.logical_not(func_mask[:, :, islice])))

                slice_func_nvoxels = len(slice_sl_.nonzero()[0])
                if __debug__:
                    debug('PLLB', "Plotting %i foreground elements in slice %i"
                          % (slice_func_nvoxels, islice))

                kwargs = dict(aspect=aspect, origin='upper')
                              #extent=(0, slice_bg.shape[0],
                              #        0, slice_bg.shape[1]))

                # paste a blank white background first, since otherwise
                # recent matplotlib screws up those masked imshows
                im = ax.imshow(np.ones(slice_sl_.shape).T,
                               cmap=bg_cmap,
                               **kwargs)
                im.set_clim((0,1))

                # ax.clim((0,1))
                if slice_bg_nvoxels:
                    ax.imshow(slice_bg_.T,
                             # let's stay close to the ugly truth ;-)
                             #interpolation='bilinear',
                             interpolation='nearest',
                             cmap=bg_cmap,
                             **kwargs)

                if slice_func_nvoxels:
                    alpha = slice_bg_nvoxels and 0.9 or 1.0
                    im = ax.imshow(slice_sl_.T,
                                   interpolation='nearest',
                                   cmap=func_cmap,
                                   alpha=alpha,
                                   **kwargs)
                    im.set_clim(*clim)
                    im0 = im

                if slice_title:
                    pl.title(slice_title % locals())

            # func_masked = func[func_mask]

            #
            # Add summary information
            func_thr = func[np.logical_and(func_mask, thresholder(func))]
            if add_info and len(func_thr):
                self.info_ax = ax = fig.add_subplot(nrows, ncolumns,
                                                    locs.index('info')+1,
                                                    frame_on=False)
                #    cb = pl.colorbar(shrink=0.8)
                #    #cb.set_clim(clim[0], clim[1])
                ax.axison = False
                #if add_colorbar:
                #    cb = pl.colorbar(im, shrink=0.8, pad=0.0, drawedges=False,
                #                    extend=extend, cmap=func_cmap)

                stats = {'v':len(func_masked),
                         'vt': len(func_thr),
                         'm': np.mean(func_masked),
                         'mt': np.mean(func_thr),
                         'min': np.min(func_masked),
                         'mint': np.min(func_thr),
                         'max': np.max(func_masked),
                         'maxt': np.max(func_thr),
                         'mm': np.median(func_masked),
                         'mmt': np.median(func_thr),
                         'std': np.std(func_masked),
                         'stdt': np.std(func_thr),
                         'sthr': thresh_str}
                pl.text(0, 0.5, """
 Original:
  voxels = %(v)d
  range = [%(min).3g, %(max).3g]
  mean = %(m).3g
  median = %(mm).3g
  std = %(std).3g

 Thresholded: %(sthr)s:
  voxels = %(vt)d
  range = [%(mint).3g, %(maxt).3g]
  median = %(mt).3g
  mean = %(mmt).3g
  std = %(stdt).3g
  """ % stats,
                    horizontalalignment='left',
                    verticalalignment='center',
                    transform = ax.transAxes,
                    fontsize=14)

            cb = None
            if add_colorbar and im0 is not None:
                kwargs_cb = {}
                #if add_hist:
                #    kwargs_cb['cax'] = self.hist_ax
                self.cb_ax = cb = pl.colorbar(
                    im0, #self.hist_ax,
                    shrink=0.8, pad=0.0, drawedges=False,
                    extend=extend, cmap=func_cmap, **kwargs_cb)
                cb.set_clim(*clim)

            # Add histogram
            if add_hist:
                self.hist_ax = fig.add_subplot(nrows, ncolumns,
                                               locs.index('hist') + 1,
                                               frame_on=True)

                minv, maxv = np.min(func_masked), np.max(func_masked)
                if minv<0 and maxv>0:               # then make it centered on 0
                    maxx = max(-minv, maxv)
                    range_ = (-maxx, maxx)
                else:
                    range_ = (minv, maxv)
                H = np.histogram(func_masked, range=range_, bins=31)
                # API changed since v0.99.0-641-ga7c2231
                halign = externals.versions['matplotlib'] >= '1.0.0' \
                         and 'mid' or 'center'
                H2 = pl.hist(func_masked, bins=H[1], align=halign,
                             facecolor='#FFFFFF', hold=True)
                for a, kwparams in add_dist2hist:
                    dbin = (H[1][1] - H[1][0])
                    pl.plot(H2[1], [a(x) * dbin for x in H2[1]], **kwparams)
                if add_colorbar and cb:
                    cbrgba = cb.to_rgba(H2[1])
                    for face, facecolor, value in zip(H2[2], cbrgba, H2[1]):
                        if not thresholder(value):
                            color = '#FFFFFF'
                        else:
                            color = facecolor
                        face.set_facecolor(color)


            fig.subplots_adjust(left=0.01, right=0.95, hspace=0.25)
            # , bottom=0.01
            if ncolumns - int(bool(add_info) or bool(add_hist)) < 2:
                fig.subplots_adjust(wspace=0.4)
            else:
                fig.subplots_adjust(wspace=0.1)

            Pion()

        def on_click(self, event):
            """Actions to perform on click
            """
            if id(event.inaxes) != id(plotter.hist_ax):
                return
            xdata, ydata, button = event.xdata, event.ydata, event.button
            vlim = self._locals['vlim']
            if button == 1:
                vlim[0] = xdata
            elif button == 3:
                vlim[1] = xdata
            elif button == 2:
                vlim[0], vlim[1] = vlim[1], vlim[0]
            self.do_plot()

    plotter = Plotter(locals())
    plotter.do_plot()

    if interactive is None:
        interactive = mpl_backend_isinteractive

    # Global adjustments
    if interactive:
        # if pl.matplotlib.is_interactive():
        pl.connect('button_press_event', plotter.on_click)
        pl.show()

    plotter.fig.plotter = plotter
    return plotter.fig

########NEW FILE########
__FILENAME__ = tools
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Various utilities to help plotting"""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals

externals.exists("pylab", raise_=True)
import pylab as pl

interactive_backends = ['GTKAgg', 'TkAgg']

# Backends can be modified only prior importing matplotlib, so it is
# safe to just assign current backend right here
mpl_backend = pl.matplotlib.get_backend()
mpl_backend_isinteractive = mpl_backend in interactive_backends

if mpl_backend_isinteractive:
    Pioff = pl.ioff
    def Pion():
        """Little helper to call pl.draw() and pl.ion() if backend is interactive
        """
        pl.draw()
        pl.ion()
else:
    def _Pnothing():
        """Dummy function which does nothing
        """
        pass
    Pioff = Pion = _Pnothing

########NEW FILE########
__FILENAME__ = topo
# emacs: -*- coding: utf-8; mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
#   The initial version of the code was contributed by Ingo Fründ and is
#   Coypright (c) 2008 by Ingo Fründ ingo.fruend@googlemail.com
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Plot parameter distributions on a head surface (topography plots)."""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import externals

if externals.exists("pylab", raise_=True):
    import pylab as pl

if externals.exists("griddata", raise_=True):
    from mvpa2.support.griddata import griddata

if externals.exists("scipy", raise_=True):
    from scipy.optimize import leastsq

if externals.versions['numpy'] > '1.1.0':
    from numpy import ma
else:
    from matplotlib.numerix import ma

# TODO : add optional plotting labels for the sensors
##REF: Name was automagically refactored
def plot_head_topography(topography, sensorlocations, plotsensors=False,
                       resolution=51, masked=True, plothead=True,
                       plothead_kwargs=None, **kwargs):
    """Plot distribution to a head surface, derived from some sensor locations.

    The sensor locations are first projected onto the best fitting sphere and
    finally projected onto a circle (by simply ignoring the z-axis).

    Parameters
    ----------
    topography : array
      A vector of some values corresponding to each sensor.
    sensorlocations : (nsensors x 3) array
      3D coordinates of each sensor. The order of the sensors has to match
      with the `topography` vector.
    plotsensors : bool
      If True, sensor will be plotted on their projected coordinates.
      No sensor are shown otherwise.
    plothead : bool
      If True, a head outline is plotted.
    plothead_kwargs : dict
      Additional keyword arguments passed to `plot_head_outline()`.
    resolution : int
      Number of surface samples along both x and y-axis.
    masked : bool
      If True, all surface sample extending to head outline will be
      masked.
    **kwargs
      All additional arguments will be passed to `pylab.imshow()`.

    Returns
    -------
    (map, head, sensors)
      The corresponding matplotlib objects are returned if plotted, ie.
      if plothead is set to `False`, `head` will be `None`.

          map
            The colormap that makes the actual plot, a
            matplotlib.image.AxesImage instance.
          head
            What is returned by `plot_head_outline()`.
          sensors
            The dots marking the electrodes, a matplotlib.lines.Line2d
            instance.
    """
    # give sane defaults
    if plothead_kwargs is None:
        plothead_kwargs = {}

    # error function to fit the sensor locations to a sphere
    def err(params):
        r, cx, cy, cz = params
        return (sensorlocations[:, 0] - cx) ** 2 \
               + (sensorlocations[:, 1] - cy) ** 2 \
               + (sensorlocations[:, 2] - cz) ** 2 \
               - r ** 2

    # initial guess of sphere parameters (radius and center)
    params = (1, 0, 0, 0)

    # do fit
    (r, cx, cy, cz), stuff = leastsq(err, params)

    # size of each square
    ssh = float(r) / resolution         # half-size
    ss = ssh * 2.0                      # full-size

    # Generate a grid and interpolate using the griddata module
    x = np.arange(cx - r, cx + r, ss) + ssh
    y = np.arange(cy - r, cy + r, ss) + ssh
    x, y = pl.meshgrid(x, y)

    # project the sensor locations onto the sphere
    sphere_center = np.array((cx, cy, cz))
    sproj = sensorlocations - sphere_center
    sproj = r * sproj / np.c_[np.sqrt(np.sum(sproj ** 2, axis=1))]
    sproj += sphere_center

    # fit topology onto xy projection of sphere
    topo = griddata(sproj[:, 0], sproj[:, 1],
            np.ravel(np.array(topography)), x, y)

    # mask values outside the head
    if masked:
        notinhead = np.greater_equal((x - cx) ** 2 + (y - cy) ** 2,
                                    (1.0 * r) ** 2)
        topo = ma.masked_where(notinhead, topo)

    # show surface
    map = pl.imshow(topo, origin="lower", extent=(-r, r, -r, r), **kwargs)
    pl.axis('off')

    if plothead:
        # plot scaled head outline
        head = plot_head_outline(scale=r, shift=(cx/2.0, cy/2.0), **plothead_kwargs)
    else:
        head = None

    if plotsensors:
        # plot projected sensor locations

        # reorder sensors so the ones below plotted first
        # TODO: please fix with more elegant solution
        zenum = [x[::-1] for x in enumerate(sproj[:, 2].tolist())]
        zenum.sort()
        indx = [ x[1] for x in zenum ]
        sensors = pl.plot(sproj[indx, 0] - cx/2.0, sproj[indx, 1] - cy/2.0, 'wo')
    else:
        sensors = None

    return map, head, sensors


##REF: Name was automagically refactored
def plot_head_outline(scale=1, shift=(0, 0), color='k', linewidth='5', **kwargs):
    """Plots a simple outline of a head viewed from the top.

    The plot contains schematic representations of the nose and ears. The
    size of the head is basically a unit circle for nose and ears attached
    to it.

    Parameters
    ----------
    scale : float
      Factor to scale the size of the head.
    shift : 2-tuple of floats
      Shift the center of the head circle by these values.
    color : matplotlib color spec
      The color the outline should be plotted in.
    linewidth : int
      Linewidth of the head outline.
    **kwargs
      All additional arguments are passed to `pylab.plot()`.

    Returns
    -------
    Matplotlib lines2D object
      can be used to tweak the look of the head outline.
    """

    rmax = 0.5
    # factor used all the time
    fac = 2 * np.pi * 0.01

    # Koordinates for the ears
    EarX1 =  -1 * np.array(
            [.497, .510, .518, .5299,
            .5419, .54, .547, .532, .510,
            rmax * np.cos(fac * (54 + 42))])
    EarY1 = np.array(
            [.0655, .0775, .0783, .0746, .0555,
            -.0055, -.0932, -.1313, -.1384,
            rmax * np.sin(fac * (54 + 42))])
    EarX2 = np.array(
            [rmax * np.cos(fac * (54 + 42)),
            .510, .532, .547, .54, .5419,
            .5299, .518, .510, .497] )
    EarY2 = np.array(
            [rmax * np.sin(fac * (54 + 42)),
            -.1384, -.1313, -.0932, -.0055,
            .0555, .0746, .0783, .0775, .0655] )

    # Coordinates for the Head
    HeadX1 = np.fromfunction(
            lambda x: rmax * np.cos(fac * (x + 2)), (21,))
    HeadY1 = np.fromfunction(
            lambda y: rmax * np.sin(fac * (y + 2)), (21,))
    HeadX2 = np.fromfunction(
            lambda x: rmax * np.cos(fac * (x + 28)), (21,))
    HeadY2 = np.fromfunction(
            lambda y: rmax * np.sin(fac * (y + 28)), (21,))
    HeadX3 = np.fromfunction(
            lambda x: rmax * np.cos(fac * (x + 54)), (43,))
    HeadY3 = np.fromfunction(
            lambda y: rmax * np.sin(fac * (y + 54)), (43,))

    # Coordinates for the Nose
    NoseX = np.array([.18 * rmax, 0, -.18 * rmax])
    NoseY = np.array([rmax - 0.004, rmax * 1.15, rmax - 0.004])

    # Combine to one
    X = np.concatenate((EarX2, HeadX1, NoseX, HeadX2, EarX1, HeadX3))
    Y = np.concatenate((EarY2, HeadY1, NoseY, HeadY2, EarY1, HeadY3))

    X *= 2 * scale
    Y *= 2 * scale
    X += shift[0]
    Y += shift[1]

    return pl.plot(X, Y, color=color, linewidth=linewidth)

########NEW FILE########
__FILENAME__ = sampleslookup
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helper to map and validate samples' origids into indices"""

import numpy as np

if __debug__:
    from mvpa2.base import debug

class SamplesLookup(object):
    """Map to translate sample origids into unique indices.
    """

    def __init__(self, ds):
        """
        Parameters
        ----------
        ds : Dataset
            Dataset for which to create the map
        """

        # TODO: Generate origids and magic_id in Dataset!!
        # They are simply added here for development convenience, but they
        # should be removed.  We should also consider how exactly to calculate
        # the magic ids and sample ids as this is not necessarily the fastest/
        # most robust method --SG
        try:
            sample_ids = ds.sa.origids
        except AttributeError:
            # origids not yet generated
            if __debug__:
                debug('SAL',
                      "Generating dataset origids in SamplesLookup for %(ds)s",
                      msgargs=dict(ds=ds))

            ds.init_origids('samples')  # XXX may be both?
            sample_ids = ds.sa.origids

        try:
            self._orig_ds_id = ds.a.magic_id
        except AttributeError:
            ds.a.update({'magic_id': hash(ds)})
            self._orig_ds_id = ds.a.magic_id
            if __debug__:
                debug('SAL',
                      "Generating dataset magic_id in SamplesLookup for %(ds)s",
                      msgargs=dict(ds=ds))

        nsample_ids = len(sample_ids)
        self._map = dict(zip(sample_ids,
                             range(nsample_ids)))
        if __debug__:
            # some sanity checks
            if len(self._map) != nsample_ids:
                raise ValueError, \
                    "Apparently samples' origids are not uniquely identifying" \
                    " samples in %s.  You must change them so they are unique" \
                    ". Use ds.init_origids('samples')" % ds

    def __call__(self, ds):
        """
        .. note:
           Will raise KeyError if lookup for sample_ids fails, or ds has not
           been mapped at all
           """
        if (not 'magic_id' in ds.a) or ds.a.magic_id != self._orig_ds_id:
            raise KeyError, \
                  'Dataset %s is not indexed by %s' % (ds, self)

        _map = self._map
        _origids = ds.sa.origids

        res = np.array([_map[i] for i in _origids])
        if __debug__:
            debug('SAL',
                  "Successful lookup: %(inst)s on %(ds)s having "
                  "origids=%(origids)s resulted in %(res)s",
                  msgargs=dict(inst=self, ds=ds, origids=_origids, res=res))
        return res

########NEW FILE########
__FILENAME__ = stats
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Little statistics helper"""

__docformat__ = 'restructuredtext'

from mvpa2.base import externals

if externals.exists('scipy', raise_=True):
    import scipy.stats as st
    # evaluate once the fact of life
    __scipy_prior0101 = externals.versions['scipy'] < '0.10.1'

import numpy as np
import copy

def chisquare(obs, exp='uniform'):
    """Compute the chisquare value of a contingency table with arbitrary
    dimensions.

    Parameters
    ----------
    obs : array
      Observations matrix
    exp : ('uniform', 'indep_rows') or array, optional
      Matrix of expected values of the same size as `obs`.  If no
      array is given, then for 'uniform' -- evenly distributes all
      observations.  In 'indep_rows' case contingency table takes into
      account frequencies relative across different columns, so, if
      the contingency table is predictions vs targets, it would
      account for dis-balance among different targets.  Although
      'uniform' is the default, for confusion matrices 'indep_rows' is
      preferable.

    Returns
    -------
    tuple
     chisquare-stats, associated p-value (upper tail)
    """
    obs = np.array(obs)

    # get total number of observations
    nobs = np.sum(obs)

    # if no expected value are supplied assume equal distribution
    if not isinstance(exp, np.ndarray):
        ones = np.ones(obs.shape, dtype=float)
        if exp == 'indep_rows':
            # multiply each column
            exp = np.sum(obs, axis=0)[None, :] * ones / obs.shape[0]
        elif exp == 'indep_cols':
            # multiply each row
            exp = np.sum(obs, axis=1)[:, None] * ones / obs.shape[1]
        elif exp == 'uniform':
            # just evenly distribute
            exp = nobs * np.ones(obs.shape, dtype=float) / np.prod(obs.shape)
        else:
            raise ValueError, \
                  "Unknown specification of expected values exp=%r" % (exp,)
    else:
        assert(exp.shape == obs.shape)

    # make sure to have floating point data
    exp = exp.astype(float)

    # compute chisquare value
    exp_zeros = exp == 0
    exp_nonzeros = np.logical_not(exp_zeros)
    if np.sum(exp_zeros) != 0 and (obs[exp_zeros] != 0).any():
        raise ValueError, \
              "chisquare: Expected values have 0-values, but there are actual" \
              " observations -- chi^2 cannot be computed"
    chisq = np.sum(((obs - exp) ** 2)[exp_nonzeros] / exp[exp_nonzeros])

    # return chisq and probability (upper tail)
    # taking only the elements with something expected
    return chisq, st.chisqprob(chisq, np.sum(exp_nonzeros) - 1)


def _chk_asanyarray(a, axis):
    a = np.asanyarray(a)
    if axis is None:
        a = a.ravel()
        outaxis = 0
    else:
        outaxis = axis
    return a, outaxis


def ttest_1samp(a, popmean=0, axis=0, mask=None, alternative='two-sided'):
    """
    Calculates the T-test for the mean of ONE group of scores `a`.

    This is a refinement for the :func:`scipy.stats.ttest_1samp` for
    the null hypothesis testing that the expected value (mean) of a
    sample of independent observations is equal to the given
    population mean, `popmean`.  It adds ability to test carry single
    tailed test as well as operate on samples with varying number of
    active measurements, as specified by `mask` argument.

    Since it is only a refinement and otherwise it should perform the
    same way as the original ttest_1samp -- the name was overloaded.

    Note
    ----

    Initially it was coded before discovering scipy.mstats which
    should work with masked arrays.  But ATM (scipy 0.10.1) its
    ttest_1samp does not support axis argument making it of limited
    use anyways.


    Parameters
    ----------
    a : array_like
        sample observations
    popmean : float or array_like
        expected value in null hypothesis, if array_like than it must have the
        same shape as `a` excluding the axis dimension
    axis : int, optional, (default axis=0)
        Axis can equal None (ravel array first), or an integer (the axis
        over which to operate on a).
    mask : array_like, bool
        bool array to specify which measurements should participate in the test
    alternative : ('two-sided', 'greater', 'less')
        alternative two test

    Returns
    -------
    t : float or array
        t-statistic
    prob : float or array
        p-value

    Examples
    --------
    TODO

    """

    # would also flatten if no axis specified
    a, axis = _chk_asanyarray(a, axis)

    if isinstance(a, np.ma.core.MaskedArray):
        if mask is not None:
            raise ValueError(
                "Provided array is already masked, so no additional "
                "mask should be provided")
        n = a.count(axis=axis)
    elif mask is not None:
        # Create masked array
        a = np.ma.masked_array(a, mask= ~np.asanyarray(mask))
        n = a.count(axis=axis)
    else:
        # why bother doing anything?
        n = a.shape[axis]

    df = n - 1

    d = np.mean(a, axis) - popmean
    # yoh: there is a bug in old (e.g. 1.4.1) numpy's while operating on
    #      masked arrays -- for some reason refuses to compute var
    #      correctly whenever only 2 elements are available and it is
    #      multi-dimensional:
    # (Pydb) print np.var(a[:, 9:11], axis, ddof=1)
    # [540.0 --]
    # (Pydb) print np.var(a[:, 10:11], axis, ddof=1)
    # [--]
    # (Pydb) print np.var(a[:, 10], axis, ddof=1)
    # 648.0
    v = np.var(a, axis, ddof=1)
    denom = np.sqrt(v / n)

    t = np.divide(d, denom)

    # t, prob might be full arrays if no masking was actually done
    def _filled(a):
        if isinstance(a, np.ma.core.MaskedArray):
            return a.filled(np.nan)
        else:
            return a

    t, prob = _ttest_finish(_filled(df), _filled(t), alternative=alternative)

    return t, prob


def _ttest_finish(df, t, alternative):
    """Common code between all 3 t-test functions."""
    dist_gen = st.distributions.t
    if alternative == 'two-sided':
        prob = dist_gen.sf(np.abs(t), df) * 2 # use np.abs to get upper alternative
    elif alternative == 'greater':
        prob = dist_gen.sf(t, df)
    elif alternative == 'less':
        prob = dist_gen.cdf(t, df)
    else:
        raise ValueError("Unknown alternative %r" % alternative)

    t_isnan = np.isnan(t)
    if np.any(t_isnan) and __scipy_prior0101:
        # older scipy's would return 0 for nan values of the argument
        # which is incorrect
        if np.isscalar(prob):
            prob = np.nan
        else:
            prob[t_isnan] = np.nan

    if t.ndim == 0:
        t = np.asscalar(t)

    return t, prob

########NEW FILE########
__FILENAME__ = support
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support function -- little helpers in everyday life"""

__docformat__ = 'restructuredtext'

import itertools
import math
import random
import re, os, sys

# for SmartVersion
from distutils.version import Version

import numpy as np
#import numpy.random as npr

from mvpa2.base import warning
from mvpa2.support.copy import copy, deepcopy
from mvpa2.base.types import is_sequence_type

if __debug__:
    from mvpa2.base import debug


##REF: Name was automagically refactored
def reuse_absolute_path(file1, file2, force=False):
    """Use path to file1 as the path to file2 is no absolute
    path is given for file2

    Parameters
    ----------
    force : bool
      if True, force it even if the file2 starts with /
    """
    if not file2.startswith(os.path.sep) or force:
        # lets reuse path to file1
        return os.path.join(os.path.dirname(file1), file2.lstrip(os.path.sep))
    else:
        return file2


##REF: Name was automagically refactored
def transform_with_boxcar(data, startpoints, boxlength, offset=0, fx=np.mean):
    """This function extracts boxcar windows from an array. Such a boxcar is
    defined by a starting point and the size of the window along the first axis
    of the array (`boxlength`). Afterwards a customizable function is applied
    to each boxcar individually (Default: averaging).

    :param data: An array with an arbitrary number of dimensions.
    :type data: array
    :param startpoints: Boxcar startpoints as index along the first array axis
    :type startpoints: sequence
    :param boxlength: Length of the boxcar window in #array elements
    :type boxlength: int
    :param offset: Optional offset between the configured starting point and the
      actual begining of the boxcar window.
    :type offset: int
    :rtype: array (len(startpoints) x data.shape[1:])
    """
    if boxlength < 1:
        raise ValueError, "Boxlength lower than 1 makes no sense."

    # check for illegal boxes
    for sp in startpoints:
        if ( sp + offset + boxlength - 1 > len(data)-1 ) \
           or ( sp + offset < 0 ):
            raise ValueError, \
                  'Illegal box: start: %i, offset: %i, length: %i' \
                  % (sp, offset, boxlength)

    # build a list of list where each sublist contains the indexes of to be
    # averaged data elements
    selector = [ range( i + offset, i + offset + boxlength ) \
                 for i in startpoints ]

    # average each box
    selected = [ fx( data[ np.array(box) ], axis=0 ) for box in selector ]

    return np.array( selected )


def xunique_combinations(L, n):
    """Generator of unique combinations form a list L of objects in
    groups of size n.

    Parameters
    ----------
    L : list
      list of unique ids
    n : int
      grouping size

    Adopted from Li Daobing
    http://code.activestate.com/recipes/190465/
    (MIT license, according to activestate.com's policy)

    Also good discussions on combinations/variations/permutations
    with various implementations are available at
    http://mail.python.org/pipermail/python-list/2004-October/286054.html
    """
    if n == 0:
        yield []
    else:
        for i in xrange(len(L)-n+1):
            for cc in xunique_combinations(L[i+1:], n-1):
                yield [L[i]]+cc

def __xrandom_unique_combinations(L, n, k=None):
    """Generator of unique combinations form a list L of objects in
    groups of size n produced in random order

    "Elegant" but incorrect since pretty much samples the "tail"

    Parameters
    ----------
    L : list
      list of unique ids
    n : int
      grouping size
    k : int or None, optional
      limit number of combinations.  All of combinations are produced
      if k is None (default)

    Based on xunique_combinations adopted from Li Daobing
    http://code.activestate.com/recipes/190465/
    (MIT license, according to activestate.com's policy)
    """
    if k is not None:
        # Just a helper for convenient limiting
        g = xrandom_unique_combinations(L, n)
        for i in xrange(k):
            yield next(g)
    elif n == 0:
        yield []
    else:
        for i in npr.permutation(len(L)-n+1):
            for cc in xrandom_unique_combinations(
                npr.permutation(L[i+1:]), n-1):
                yield [L[i]]+cc


def ncombinations(n, k):
    """
    A fast way to calculate binomial coefficients by Andrew Dalke

    Source: http://stackoverflow.com/questions/3025162/statistics-combinations-in-python/3025194

    Alternative implementations:
       scipy.misc.comb() -- approximation
    """
    if 0 <= k <= n:
        ntok = 1
        ktok = 1
        for t in xrange(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0


def xrandom_unique_combinations(L, n, k=None):
    """Generator of unique combinations form a list L of objects in
    groups of size n produced in random order

    Parameters
    ----------
    L : list
      list of unique ids
    n : int
      grouping size
    k : int or None
      limit number of combinations.  All of combinations are produced
      if k is None (default)

    """
    ncomb = ncombinations(len(L), n)
    if k is None:
        k = ncomb

    if (ncomb < 1e6 or k > math.sqrt(ncomb)) \
           and sys.version_info[:2] >= (2, 6):
        # so there is no sense really to mess with controlling for
        # non-repeats -- we can pre-generate all of them and just
        # choose needed number of random samples
        # Python2.5 doesn't have itertools.combinations
        for s in random.sample(list(itertools.combinations(L, n)), k):
            yield list(s)
    else:
        # Let's cycle through permutations while tracking
        # repeats
        seen = set()
        indexes = range(len(L)) # switch to indices so we could
                                # reliably hash them
        while len(seen) < min(k, ncomb):
            np.random.shuffle(indexes)
            sample = tuple(sorted(indexes[:n]))
            if not (sample in seen):
                yield [L[x] for x in sample]
                seen.add(sample)


def unique_combinations(L, n, sort=False):
    """Return unique combinations form a list L of objects in groups of size n.

    Parameters
    ----------
    L : list
      list of unique ids
    n : int
      length of the subsets to return
    sort : bool, optional
      if True -- result is sorted before returning

    If you are intended to use only a small subset of possible
    combinations, it is advised to use a generator
    `xunique_combinations`.
    """
    res = list(xunique_combinations(L, n))
    if sort:
        res = sorted(res)
    return res


##REF: Name was automagically refactored
def indent_doc(v):
    """Given a `value` returns a string where each line is indented

    Needed for a cleaner __repr__ output
    `v` - arbitrary
    """
    return re.sub('\n', '\n  ', str(v))


def idhash(val):
    """Craft unique id+hash for an object
    """
    res = "%s" % id(val)
    if isinstance(val, list):
        val = tuple(val)
    elif isinstance(val, dict):
        val = tuple(val.items())
    try:
        if sys.version_info[0] >= 3:
            # TODO: bytes is just a workaround and is slower
            # Anyway -- research joblib for hashing
            res += ":%s" % hash(bytes(val))
        else:
            res += ":%s" % hash(buffer(val))
    except:
        try:
            res += ":%s" % hash(val)
        except:
            pass
        pass
    return res

##REF: Name was automagically refactored
def is_sorted(items):
    """Check if listed items are in sorted order.

    Parameters
    ----------
      `items`: iterable container

    :return: `True` if were sorted. Otherwise `False` + Warning
    """
    items_sorted = deepcopy(items)
    items_sorted.sort()
    equality = items_sorted == items
    # XXX yarik forgotten analog to isiterable
    if hasattr(equality, '__iter__'):
        equality = np.all(equality)
    return equality


##REF: Name was automagically refactored
def is_in_volume(coord, shape):
    """For given coord check if it is within a specified volume size.

    Returns True/False. Assumes that volume coordinates start at 0.
    No more generalization (arbitrary minimal coord) is done to save
    on performance
    """
    for i in xrange(len(coord)):
        if coord[i] < 0 or coord[i] >= shape[i]:
            return False
    return True


def array_whereequal(a, x):
    """Reliable comparison for `numpy.ndarray`

    `numpy.ndarray` (as of 1.5.0.dev) fails to compare tuples in array of
    dtype object, e.g.

    >>> import numpy as np; a=np.array([1, (0,1)], dtype=object); print a == (0,1),  a[1] == (0,1)
    [False False] True

    This function checks if dtype is object and just does list
    comprehension in that case
    """
    if a.dtype is np.dtype('object'):
        return np.array([i==x for i in a], dtype=bool)
    else:
        return a == x


def version_to_tuple(v):
    """Convert literal string into a tuple, if possible of ints

    Tuple of integers constructed by splitting at '.' or interleaves
    of numerics and alpha numbers
    """
    if isinstance(v, basestring):
        v = v.split('.')
    elif isinstance(v, tuple) or isinstance(v, list):
        # assure tuple
        pass
    else:
        raise ValueError, "Do not know how to treat version '%s'" % str(v)

    # Try to convert items into ints
    vres = []

    regex = re.compile('(?P<numeric>[0-9]*)'
                       '(?P<alpha>[~+-]*[A-Za-z]*)(?P<suffix>.*)')
    for x in v:
        try:
            vres += [int(x)]
        except ValueError:
            # try to split into sequences of literals and numerics
            suffix = x
            while suffix != '':
                res = regex.search(suffix)
                if res:
                    resd = res.groupdict()
                    if resd['numeric'] != '':
                        vres += [int(resd['numeric'])]
                    if resd['alpha'] != '':
                        vres += [resd['alpha']]
                    suffix = resd['suffix']
                else:
                    # We can't detech anything meaningful -- let it go as is
                    resd += [suffix]
                    break
    v = tuple(vres)

    return v

class SmartVersion(Version):
    """A bit evolved comparison of versions

    The reason for not using python's distutil.version is that it
    seems to have no clue about somewhat common conventions of using
    '-dev' or 'dev' or 'rc' suffixes for upcoming releases (so major
    version does contain upcoming release already).

    So here is an ad-hoc and not as nice implementation
    """

    def parse(self, vstring):
        self.vstring = vstring
        self.version = version_to_tuple(vstring)

    def __str__(self):
        try:
            return self.vstring
        except AttributeError:
            # Version.__init__ doesn't take care about assigning
            # .vstring if None is given, so let's just treat as it is
            # an empty string
            return ""

    def __cmp__(self, other):
        if isinstance(other, (str, tuple, list)):
            other = SmartVersion(other)
        elif isinstance(other, SmartVersion):
            pass
        elif isinstance(other, Version):
            other = SmartVersion(other.vstring)
        else:
            raise ValueError("Do not know how to treat version %s"
                             % str(other))

        if sys.version >= '3':
            def cmp(a, b):
                """Compatibility with Python3 -- regular (deprecated
                in 3) cmp operation should be sufficient for our needs"""
                return (a > b) - (a < b)
        else:
            # having above cmp overloads builtin cmp for this function so we
            # need manually rebind it or just resort to above cmp in general
            # (why not?)
            from __builtin__ import cmp

        # Do ad-hoc comparison of strings
        i = 0
        s, o = self.version, other.version
        regex_prerelease = re.compile('~|-?dev|-?rc|-?svn|-?pre|-?beta|-?alpha', re.I)
        for i in xrange(max(len(s), len(o))):
            if i < len(s): si = s[i]
            else: si = None
            if i < len(o): oi = o[i]
            else: oi = None

            if si == oi:
                continue

            for x,y,mult in ((si, oi, 1), (oi, si, -1)):
                if x is None:
                    if isinstance(y, int):
                        return -mult #  we got '.1' suffix
                    if isinstance(y, str):
                        if (regex_prerelease.match(y)):
                            return mult        # so we got something to signal
                                               # pre-release, so first one won
                        else:
                            # otherwise the other one wins
                            return -mult
                    else:
                        raise RuntimeError, "Should not have got here with %s" \
                              % y
                elif isinstance(x, int):
                    if not isinstance(y, int):
                        return mult
                    return mult*cmp(x, y) # both are ints
                elif isinstance(x, str):
                    if isinstance(y, str):
                        return mult*cmp(x,y)
        return 0

    if sys.version >= '3':
        # version.py logic in python3 does not rely on deprecated
        # __cmp__ but renames it into _cmp  and wraps in those various
        # comparators...  thus our good old __cmp__ should be ok for our
        # purposes here
        _cmp = __cmp__

##REF: Name was automagically refactored
def get_break_points(items, contiguous=True):
    """Return a list of break points.

    Parameters
    ----------
    items : iterable
      list of items, such as chunks
    contiguous : bool
      if `True` (default) then raise Value Error if items are not
      contiguous, i.e. a label occur in multiple contiguous sets

    :raises: ValueError

    :return: list of indexes for every new set of items
    """
    prev = None # pylint happiness event!
    known = []
    """List of items which was already seen"""
    result = []
    """Resultant list"""
    for index in xrange(len(items)):
        item = items[index]
        if item in known:
            if index > 0:
                if prev != item:            # breakpoint
                    if contiguous:
                        raise ValueError, \
                        "Item %s was already seen before" % str(item)
                    else:
                        result.append(index)
        else:
            known.append(item)
            result.append(index)
        prev = item
    return result


##REF: Name was automagically refactored
def rfe_history_to_maps(history):
    """Convert history generated by RFE into the array of binary maps

    Example:
      history2maps(np.array( [ 3,2,1,0 ] ))
    results in
      array([[ 1.,  1.,  1.,  1.],
             [ 1.,  1.,  1.,  0.],
             [ 1.,  1.,  0.,  0.],
             [ 1.,  0.,  0.,  0.]])
    """

    # assure that it is an array
    history = np.array(history)
    nfeatures, steps = len(history), max(history) - min(history) + 1
    history_maps = np.zeros((steps, nfeatures))

    for step in xrange(steps):
        history_maps[step, history >= step] = 1

    return history_maps


class MapOverlap(object):
    """Compute some overlap stats from a sequence of binary maps.

    When called with a sequence of binary maps (e.g. lists or arrays) the
    fraction of mask elements that are non-zero in a customizable proportion
    of the maps is returned. By default this threshold is set to 1.0, i.e.
    such an element has to be non-zero in *all* maps.

    Three additional maps (same size as original) are computed:

      * overlap_map: binary map which is non-zero for each overlapping element.
      * spread_map:  binary map which is non-zero for each element that is
                     non-zero in any map, but does not exceed the overlap
                     threshold.
      * ovstats_map: map of float with the raw elementwise fraction of overlap.

    All maps are available via class members.
    """
    def __init__(self, overlap_threshold=1.0):
        """Nothing to be seen here.
        """
        self.__overlap_threshold = overlap_threshold

        # pylint happiness block
        self.overlap_map = None
        self.spread_map = None
        self.ovstats_map = None


    def __call__(self, maps):
        """Returns fraction of overlapping elements.
        """
        ovstats = np.mean(maps, axis=0)

        self.overlap_map = (ovstats >= self.__overlap_threshold )
        self.spread_map = np.logical_and(ovstats > 0.0,
                                        ovstats < self.__overlap_threshold)
        self.ovstats_map = ovstats

        return np.mean(ovstats >= self.__overlap_threshold)


class Event(dict):
    """Simple class to define properties of an event.

    The class is basically a dictionary. Any properties can
    be passed as keyword arguments to the constructor, e.g.:

      >>> ev = Event(onset=12, duration=2.45)

    Conventions for keys:

    `onset`
      The onset of the event in some unit.
    `duration`
      The duration of the event in the same unit as `onset`.
    `label`
      E.g. the condition this event is part of.
    `chunk`
      Group this event is part of (if any), e.g. experimental run.
    `features`
      Any amount of additional features of the event. This might include
      things like physiological measures, stimulus intensity. Must be a mutable
      sequence (e.g. list), if present.
    """
    _MUSTHAVE = ['onset']

    def __init__(self, **kwargs):
        """
        Parameters
        ----------
        **kwargs : dict
          All keys to describe the Event to initialize its dict.
        """
        # store everything
        dict.__init__(self, **kwargs)

        # basic checks
        for k in Event._MUSTHAVE:
            if not self.has_key(k):
                raise ValueError, "Event must have '%s' defined." % k


    ##REF: Name was automagically refactored
    def as_descrete_time(self, dt, storeoffset=False, offsetattr='offset'):
        """Convert `onset` and `duration` information into descrete timepoints.

        Parameters
        ----------
        dt : float
          Temporal distance between two timepoints in the same unit as `onset`
          and `duration`.
        storeoffset : bool
          If True, the temporal offset between original `onset` and
          descretized onset is stored as an additional item.
        offsetattr : str
          The name of the attribute that is used to store the computed offset
          in case the `storeoffset` is enabled.

        Returns
        -------
        A copy of the original `Event` with `onset` and optionally `duration`
        replaced by their corresponding descrete timepoint. The new onset will
        correspond to the timepoint just before or exactly at the original
        onset. The new duration will be the number of timepoints covering the
        event from the computed onset timepoint till the timepoint exactly at
        the end, or just after the event.

        Note again, that the new values are expressed as #timepoint and not
        in their original unit!
        """
        dt = float(dt)
        onset = self['onset']
        out = copy(self)

        # get the timepoint just prior the onset
        out['onset'] = int(np.floor(onset / dt))

        if storeoffset:
            # compute offset
            offset = onset - (out['onset'] * dt)
            out[offsetattr] = offset

        if out.has_key('duration'):
            # how many timepoint cover the event (from computed onset
            # to the one timepoint just after the end of the event
            out['duration'] = int(np.ceil((onset + out['duration']) / dt) \
                                  - out['onset'])

        return out


def value2idx(val, x, solv='round'):
    """Convert a value into an index of the closes matching array element.

    Parameters
    ----------
    val : scalar
      Value that is to be converted.
    x : array or sequence
      One-dimensional array whose elements are used for comparision.
    solv : {'round', 'floor', 'ceil'}
      Resolver strategie: absolute closest element (round), closest smaller
      element (floor), or closest larger element (ceil).

    Returns
    -------
    int
    """
    # distance to val
    x = np.asanyarray(x) - val
    if solv == 'round':
        pass
    elif solv == 'ceil':
        x[x<0] = np.inf
    elif solv == 'floor':
        x[x>0] = np.inf
    else:
        ValueError("Unkown resolving method '%s'." % solv)
    x = np.abs(x)
    idx = np.argmin(x)
    return idx


def mask2slice(mask):
    """Convert a boolean mask vector into an equivalent slice (if possible).

    Parameters
    ----------
    mask: boolean array
      The mask.

    Returns
    -------
    slice or boolean array
      If possible the boolean mask is converted into a `slice`. If this is not
      possible the unmodified boolean mask is returned.
    """
    # the filter should be a boolean array
    # TODO Could be easily extended to also accept index arrays
    if not len(mask):
        raise ValueError("Got an empty mask.")
    # get indices of non-zero filter elements
    idx = mask.nonzero()[0]
    if not len(idx):
        return slice(0)
    idx_start = idx[0]
    idx_end = idx[-1] + 1
    idx_step = None
    if len(idx) > 1:
        # we need to figure out if there is a regular step-size
        # between elements
        stepsizes = np.unique(idx[1:] - idx[:-1])
        if len(stepsizes) > 1:
            # multiple step-sizes -> slicing is not possible -> return
            # orginal filter
            return mask
        else:
            idx_step = stepsizes[0]

    sl = slice(idx_start, idx_end, idx_step)
    if __debug__:
        debug("SPL", "Boolean mask conversion to slice is possible (%s)." % sl)
    return sl


def get_limit_filter(limit, collection):
    """Create a filter array from a limit definition.

    Parameters
    -----------
    limit : None or str or dict
      If ``None`` all elements will be included in the filter. If an single
      attribute name is given, its unique values will be used to define
      chunks of data that are marked in the filter as unique integers. Finally,
      if a dictionary is provided, its keys define attribute names and its
      values (single value or sequence thereof) attribute value, where all
      key-value combinations across all given items define a "selection" of
      elements to be included in the filter (OR combination).
    collection : Collection
      Dataset attribute collection instance that contains all attributes
      referenced in the limit specification, as well as defines the shape of
      the filter.

    Returns
    -------
    array
      This array is either boolean, where a `True` elements represent including
      in the filter, or the array is numerical, where it unique integer values
      defines individual chunks of a filter.
    """
    attr_length = collection.attr_length

    if limit is None:
        # no limits
        limit_filter = np.ones(attr_length, dtype='bool')
    elif isinstance(limit, str):
        # use the unique values of this attribute to permute each chunk
        # individually
        lattr = collection[limit]
        lattr_data = lattr.value
        limit_filter = np.zeros(attr_length, dtype='int')
        for i, uv in enumerate(lattr.unique):
            limit_filter[lattr_data == uv] = i
    elif isinstance(limit, dict):
        limit_filter = np.zeros(attr_length, dtype='bool')
        for a in limit:
            if is_sequence_type(limit[a]):
                for v in limit[a]:
                    # enable the samples matching the value 'v' of the
                    # current limit attribute 'a'
                    limit_filter[collection[a].value == v] = True
            else:
                limit_filter[collection[a].value == limit[a]] = True
    else:
        raise RuntimeError("Unhandle condition")

    return limit_filter


def get_nelements_per_value(data):
    """Returns the number of elements per unique value of some sequence.

    Parameters
    ----------
    data : sequence
      This can be any sequence. In addition also ArrayCollectables are supported
      and this function will make use of any available pre-cached list of unique
      values.

    Returns
    -------
    dict with the number of elements (value) per unique value (key) in the
    sequence.
    """
    if hasattr(data, 'unique'):
        # if this is an ArrayAttribute save some time by using pre-cached unique
        # values
        uniquevalues = data.unique
        values = data.value
    else:
        uniquevalues = np.unique(data)
        values = data

    # use dictionary to cope with arbitrary values
    result = dict(zip(uniquevalues, [ 0 ] * len(uniquevalues)))
    for l in values:
        result[l] += 1

    return result


########NEW FILE########
__FILENAME__ = queryengine
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""QueryEngine for querying feature ids based on the surface nodes

References
----------
NN Oosterhof, T Wiestler, PE Downing, J Diedrichsen (2011). A comparison of volume-based
and surface-based multi-voxel pattern analysis. Neuroimage, 56(2), pp. 593-600

'Surfing' toolbox: http://surfing.sourceforge.net
(and the associated documentation)
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base.dochelpers import _repr_attrs

from mvpa2.base.dataset import AttrDataset
from mvpa2.misc.neighborhood import QueryEngineInterface

from mvpa2.misc.surfing import volgeom, surf_voxel_selection
from mvpa2.base import warning


class SurfaceQueryEngine(QueryEngineInterface):
    '''
    Query-engine that maps center nodes to indices of features
    (nodes) that are near each center node.

    This class is for mappings from surface to surface features;
    for mappings from surface to voxel features, use
    SurfaceVerticesQueryEngine.
    '''

    def __init__(self, surface, radius, distance_metric='dijkstra',
                    fa_node_key='node_indices'):
        '''Make a new SurfaceQueryEngine

        Parameters
        ----------
        surface: surf.Surface or str
            surface object, or filename of a surface
        radius: float
            size of neighborhood.
        distance_metric: str
            'euclidean' or 'dijkstra' (default).
        fa_node_key: str
            Key for feature attribute that contains node indices
            (default: 'node_indices').

        Notes
        -----
        After training this instance on a dataset and calling it with
        self.query_byid(vertex_id) as argument,
        '''
        self.surface = surface
        self.radius = radius
        self.distance_metric = distance_metric
        self.fa_node_key = fa_node_key
        self._vertex2feature_map = None

        allowed_metrics = ('dijkstra', 'euclidean')
        if not self.distance_metric in allowed_metrics:
            raise ValueError('distance_metric %s has to be in %s' %
                                    (self.distance_metric, allowed_metrics))

        if self.distance_metric == 'dijkstra':
            # Pre-compute neighbor information (and ignore the output).
            surface.neighbors

    def __repr__(self, prefixes=[]):
        return super(SurfaceQueryEngine, self).__repr__(
                   prefixes=prefixes
                   + _repr_attrs(self, ['surface'])
                   + _repr_attrs(self, ['radius'])
                   + _repr_attrs(self, ['distance_metric'],
                                   default='dijkstra')
                   + _repr_attrs(self, ['fa_node_key'],
                                   default='node_indices'))

    def __reduce__(self):
        return (self.__class__, (self.surface,
                                 self.radius,
                                 self.distance_metric,
                                 self.fa_node_key),
                            dict(_vertex2feature_map=self._vertex2feature_map))

    def __str__(self):
        return '%s(%s, radius=%s, distance_metric=%s, fa_node_key=%s)' % \
                                               (self.__class__.__name__,
                                                self.surface,
                                                self.radius,
                                                self.distance_metric,
                                                self.fa_node_key)

    def _check_trained(self):
        if self._vertex2feature_map is None:
            raise ValueError('Not trained on dataset: %s' % self)


    @property
    def ids(self):
        self._check_trained()
        return self._vertex2feature_map.keys()

    def untrain(self):
        self._vertex2feature_map = None

    def train(self, ds):
        '''
        Train the queryengine

        Parameters
        ----------
        ds: Dataset
            dataset with surface data. It should have a field
            .fa.node_indices that indicates the node index of each
            feature.
        '''

        fa_key = self.fa_node_key
        nvertices = self.surface.nvertices
        nfeatures = ds.nfeatures

        if not fa_key in ds.fa.keys():
            raise ValueError('Attribute .fa.%s not found.', fa_key)

        vertex_ids = ds.fa[fa_key].value.ravel()

        # check that vertex_ids are not outside 0..nfeatures
        delta = np.setdiff1d(vertex_ids, np.arange(nvertices))

        if len(delta):
            raise ValueError("Vertex id '%s' found that is not in "
                             "np.arange(%d)" % (delta[0], nvertices))

        # vertex_ids can have multiple occurences of the same node index
        # for different features, hence use a list.
        # initialize each vertex with an empty list
        self._vertex2feature_map = v2f = dict((vertex_id, list())
                                            for vertex_id in xrange(nvertices))

        for feature_id, vertex_id in enumerate(vertex_ids):
            v2f[vertex_id].append(feature_id)


    def query(self, **kwargs):
        raise NotImplementedError


    def query_byid(self, vertex_id):
        '''
        Return feature ids of features near a vertex

        Parameters
        ----------
        vertex_id: int
            Index of vertex (i.e. node) on the surface

        Returns
        -------
        feature_ids: list of int
            Indices of features in the neighborhood of the vertex indexed
            by 'vertex_id'
        '''
        self._check_trained()

        if vertex_id < 0 or vertex_id >= self.surface.nvertices or \
                        round(vertex_id) != vertex_id:
            raise KeyError('vertex_id should be integer in range(%d)' %
                                                self.surface.nvertices)

        nearby_nodes = self.surface.circlearound_n2d(vertex_id,
                                                    self.radius,
                                                    self.distance_metric)

        v2f = self._vertex2feature_map
        return sum((v2f[node] for node in nearby_nodes), [])



class SurfaceVerticesQueryEngine(QueryEngineInterface):
    '''
    Query-engine that maps center nodes to indices of features
    (voxels) that are near each center node.

    In a typical use case such an instance is generated using
    the function 'disc_surface_queryengine'

    This class is for mappings from surface to voxel features;
    for mappings from surface to surface features, use
    SurfaceQueryEngine.
    '''

    def __init__(self, voxsel, space='voxel_indices', add_fa=None):
        '''Makes a new SurfaceVerticesQueryEngine

        Parameters
        ----------
        voxsel: volume_mask_dict.VolumeMaskDictionary
            mapping from center node indices to indices of voxels
            in a searchlight
        space: str (default: 'voxel_indices')
            defines by which space voxels are indexed.
        add_fa: list of str
            additional feature attributes that should be returned
            when this instance is called with a center node id.
        '''
        super(SurfaceVerticesQueryEngine, self).__init__()
        self.voxsel = voxsel
        self.space = space
        self._map_voxel_coord = None
        self._add_fa = add_fa

    def __repr__(self, prefixes=[]):
        return super(SurfaceVerticesQueryEngine, self).__repr__(
            prefixes=prefixes
            + _repr_attrs(self, ['voxsel'])
            + _repr_attrs(self, ['space'], default='voxel_indices')
            + _repr_attrs(self, ['add_fa'], []))

    def __reduce__(self):
        return (self.__class__, (self.voxsel, self.space, self._add_fa),
                            dict(_map_voxel_coord=self._map_voxel_coord))

    def __str__(self):
        return '%s(%s, space=%s, add_fa=%s)' % (self.__class__.__name__,
                                                self.voxsel,
                                                self.space,
                                                self.add_fa)

    @property
    def ids(self):
        return self.voxsel.keys()

    def train(self, dataset):
        vg = self.voxsel.volgeom
        # We are creating a map from big unmasked indices of voxels
        # known to voxsel into the dataset's feature indexes.
        # We verify that the current dataset has the necessary
        # features (i.e. are not masked out) and that the volume
        # geometry matches that of the original voxel selection

        vg_ds = None
        try:
            vg_ds = volgeom.from_any(dataset)
        except:
            vg_ds = None

        if vg_ds:
            eps = .0001
            if np.max(np.abs(vg_ds.affine - vg.affine)) > eps:
                raise ValueError("Mismatch in affine matrix: %r !+ %r" %
                                        (vg_ds.affine, vg.affine))
            if not vg_ds.same_shape(vg):
                raise ValueError("Mismatch in shape: (%s,%s,%s) != "
                                 "(%s,%s,%s)" %
                                        (vg_ds.shape[:3], vg.shape[:3]))
        else:
            warning("Could not find dataset volume geometry for %r" % dataset)


        self._map_voxel_coord = map_voxel_coord = {}
        long_is = vg.ijk2lin(dataset.fa[self.space].value)
        long_is_invol = vg.contains_lin(long_is)
        for i, long_i in enumerate(long_is):
            if not long_is_invol[i]:
                raise ValueError('Feature id %d (with voxel id %d)'
                                 ' is not in the (possibly masked) '
                                 'volume geometry %r)' % (i, long_i, vg))
            if long_i in map_voxel_coord:
                map_voxel_coord[long_i].append(i)
            else:
                map_voxel_coord[long_i] = [i]


    def untrain(self):
        self._map_voxel_coord = None

    def query_byid(self, vertexid):
        """Given a vertex ID give us indices of dataset features (voxels)

        Parameters
        ----------
        vertexid: int
            Index of searchlight center vertex on the surface.
            This value should be an element in self.ids

        Returns
        -------
        voxel_ids: list of int or AttrDataset
            The linear indices of voxels near the vertex with index vertexid.
            If the instance was constructed with add_fa=None, then voxel_ids
            is a list; otherwise it is a AttrDataset with additional feature
            attributes stored in voxel_ids.fa.

        """
        if self._map_voxel_coord is None:
            raise ValueError("No voxel mapping - did you train?")

        voxel_unmasked_ids = self.voxsel.get(vertexid)

        # map into dataset
        voxel_dataset_ids = [self._map_voxel_coord[i]
                             for i in voxel_unmasked_ids]
        voxel_dataset_ids_flat = sum(voxel_dataset_ids, [])

        if self._add_fa is not None:
            # optionally add additional information from voxsel
            ds = AttrDataset(np.asarray(voxel_dataset_ids_flat)[np.newaxis])
            for n in self._add_fa:
                fa_values = self.voxsel.get_aux(vertexid, n)
                assert(len(fa_values) == len(voxel_dataset_ids))
                ds.fa[n] = sum([[x] * len(ids)
                                for x, ids in zip(fa_values,
                                                  voxel_dataset_ids)], [])
            return ds
        return voxel_dataset_ids_flat



    def query(self, **kwargs):
        raise NotImplementedError

    def get_masked_nifti_image(self):
        '''Returns a nifti image indicating which voxels are included
        in one or more searchlights.

        Returns
        -------
        img: nibabel.Nifti1Image
            Nifti image with value zero for voxels that we not selected, and
            non-zero values for selected voxels.
        '''
        msk = self.voxsel.get_mask()
        import nibabel as nb
        img = nb.Nifti1Image(msk, self.voxsel.volgeom.affine)
        return img

    def linear_voxel_id2feature_id(self, linear_voxel_id):
        if type(linear_voxel_id) in (list, tuple):
            return map(self.linear_voxel_id2feature_id, linear_voxel_id)

        return self._map_voxel_coord[linear_voxel_id]

    def feature_id2linear_voxel_ids(self, feature_id):
        if type(feature_id) in (list, tuple):
            return map(self.feature_id2linear_voxel_ids, feature_id)

        return [i for i, j in self._map_voxel_coord.iteritems()
                                  if feature_id in j]

    def feature_id2nearest_vertex_id(self, feature_id,
                                     fallback_euclidean_distance=False):
        '''Computes the index of the vertex nearest to a given voxel.

        Parameters
        ----------
        feature_id: int
            Feature index (referring to a voxel).
        fallback_euclidean_distance: bool (default: False)
            If the voxel indexed by feature_id was not selected by any searchlight,
            then None is returned if fallback_euclidean_distance is False, but
            vertex_id with the nearest Euclidean distance is returned if True.

        Returns
        -------
        vertex_id: int
            Vertex index of vertex nearest to the feature with id feature_id.
            By default this function only considers vertices that are in one
            or more searchlights

        '''

        if type(feature_id) in (list, tuple):
            return map(self.feature_id2nearest_vertex_id, feature_id)

        lin_voxs = self.feature_id2linear_voxel_ids(feature_id)

        return self.voxsel.target2nearest_source(lin_voxs,
                      fallback_euclidean_distance=fallback_euclidean_distance)

    def vertex_id2nearest_feature_id(self, vertex_id):
        '''Computes the index of the voxel nearest to a given vertex.

        Parameters
        ----------
        vertex_id: int
            Vertex id (referring to a node on the surface).

        Returns
        -------
        feature_id: int
            Index of feature nearest to the vertex with id vertex_id.

        Notes
        -----
        This function only considers feature ids that are selected by
        at least one vertex_id..
        '''
        if type(vertex_id) in (list, tuple):
            return map(self.vertex_id2nearest_feature_id, vertex_id)

        lin_vox = self.voxsel.source2nearest_target(vertex_id)

        return self.linear_voxel_id2feature_id(lin_vox)

    def _set_add_fa(self, add_fa):
        if add_fa is not None:
            if not set(self.voxsel.aux_keys()).issuperset(add_fa):
                raise ValueError(
                    "add_fa should list only those known to voxsel %s"
                    % self.voxsel)
        self._add_fa = add_fa

    add_fa = property(fget=lambda self:self._add_fa, fset=_set_add_fa)


class SurfaceVoxelsQueryEngine(SurfaceVerticesQueryEngine):
    '''
    Query-engine that maps center voxels (indexed by feature ids)
    to indices of features (voxels) that are near each center voxel.

    In a typical use case such an instance is generated using
    the function 'disc_surface_queryengine' with the output_space='voxels'
    argument
    '''
    def __init__(self, voxsel, space='voxel_indices', add_fa=None,
                 fallback_euclidean_distance=True):
        '''Makes a new SurfaceVoxelsQueryEngine

        Parameters
        ----------
        voxsel: volume_mask_dict.VolumeMaskDictionary
            mapping from center node indices to indices of voxels
            in a searchlight
        space: str (default: 'voxel_indices')
            defines by which space voxels are indexed.
        add_fa: list of str
            additional feature attributes that should be returned
            when this instance is called with a center node id.
        fallback_euclidean_distance: bool (default: True)
            If True then every feature id will have voxels associated with
            it. That means that the number of self.ids is then equal to the
            number of features as the input dataset.
            If False, only feature ids that are selected by at least one
            searchlight are used. The number of self.ids is then equal
            to the number of voxels that are selected by at least one
            searchlight.
        '''
        super(SurfaceVoxelsQueryEngine, self).__init__(voxsel=voxsel,
                                                       space=space,
                                                       add_fa=add_fa)

        self._feature_id2vertex_id = None
        self.fallback_euclidean_distance = fallback_euclidean_distance


    def __repr__(self, prefixes=[]):
        prefixes_ = prefixes + _repr_attrs(self,
                                          ['fallback_euclidean_distance'],
                                          default=False)
        return super(SurfaceVoxelsQueryEngine, self).__repr__(
                            prefixes=prefixes_)

    def __reduce__(self):
        return (self.__class__, (self.voxsel, self.space,
                                 self._add_fa,
                                 self.fallback_euclidean_distance),
                                dict(_feature_id2vertex_id=self._feature_id2vertex_id))

    @property
    def ids(self):
        if self._feature_id2vertex_id is None:
            raise ValueError("No feature id mapping. Did you train?")
        return self._feature_id2vertex_id.keys()

    def query_byid(self, feature_id):
        vertex_id = self._feature_id2vertex_id[feature_id]
        return super(SurfaceVoxelsQueryEngine, self).query_byid(vertex_id)

    def train(self, ds):
        super(SurfaceVoxelsQueryEngine, self).train(ds)

        # Compute the mapping from voxel (feature) ids to node ids

        fallback = self.fallback_euclidean_distance
        if fallback:
            # can use any feature id in ds
            feature_ids = range(ds.nfeatures)
        else:
            # see which feature ids were mapped to
            feature_ids = set()
            for v in self._map_voxel_coord.itervalues():
                feature_ids.update(set(v))

        f = lambda x:self.feature_id2nearest_vertex_id(x, fallback)

        fv = [(fid, f(fid)) for fid in feature_ids]

        # in the case of not fallback, some feature ids do not map to
        # a voxel id (i.e. they are mapped to None). Remove those from the
        # output
        self._feature_id2vertex_id = dict((f, v) for f, v in fv
                                                if not v is None)


    def untrain(self):
        super(SurfaceVoxelsQueryEngine, self).untrain(ds)
        self._feature_id2vertex_id = None


def disc_surface_queryengine(radius, volume, white_surf, pial_surf,
                             source_surf=None, source_surf_nodes=None,
                             volume_mask=False, distance_metric='dijkstra',
                             start_mm=0, stop_mm=0, start_fr=0., stop_fr=1.,
                             nsteps=10, eta_step=1, add_fa=None, nproc=None,
                             outside_node_margin=None,
                             results_backend=None,
                             tmp_prefix='tmpvoxsel',
                             output_modality='surface',
                             node_voxel_mapping='maximal'):
    """
    Voxel selection wrapper for multiple center nodes on the surface

    WiP
    XXX currently the last parameter 'output_modality' determines
    what kind of query engine is returned - is that bad?

    XXX: have to decide whether to use minimal_voxel_mapping=True as default

    Parameters
    ----------
    radius: int or float
        Size of searchlight. If an integer, then it indicates the number of
        voxels. If a float, then it indicates the radius of the disc
    volume: Dataset or NiftiImage or volgeom.Volgeom
        Volume in which voxels are selected.
    white_surf: str of surf.Surface
        Surface of white-matter to grey-matter boundary, or filename
        of file containing such a surface.
    pial_surf: str of surf.Surface
        Surface of grey-matter to pial-matter boundary, or filename
        of file containing such a surface.
    source_surf: surf.Surface or None
        Surface used to compute distance between nodes. If omitted, it is
        the average of the gray and white surfaces.
    source_surf_nodes: list of int or numpy array or None
        Indices of nodes in source_surf that serve as searchlight center.
        By default every node serves as a searchlight center.
    volume_mask: None (default) or False or int
        Mask from volume to apply from voxel selection results. By default
        no mask is applied. If volume_mask is an integer k, then the k-th
        volume from volume is used to mask the data. If volume is a Dataset
        and has a property volume.fa.voxel_indices, then these indices
        are used to mask the data, unless volume_mask is False or an integer.
    distance_metric: str
        Distance metric between nodes. 'euclidean' or 'dijksta' (default)
    start_fr: float (default: 0)
            Relative start position of line in gray matter, 0.=white
            surface, 1.=pial surface
    stop_fr: float (default: 1)
        Relative stop position of line (as in start_fr)
    start_mm: float (default: 0)
        Absolute start position offset (as in start_fr)
    stop_mm: float (default: 0)
        Absolute start position offset (as in start_fr)
    nsteps: int (default: 10)
        Number of steps from white to pial surface
    eta_step: int (default: 1)
        After how many searchlights an estimate should be printed of the
        remaining time until completion of all searchlights
    add_fa: None or list of strings
        Feature attributes from a dataset that should be returned if the
        queryengine is called with a dataset.
    nproc: int or None
        Number of parallel threads. None means as many threads as the
        system supports. The pprocess is required for parallel threads; if
        it cannot be used, then a single thread is used.
    outside_node_margin: float or None (default)
        By default nodes outside the volume are skipped; using this
        parameters allows for a marign. If this value is a float (possibly
        np.inf), then all nodes within outside_node_margin Dijkstra
        distance from any node within the volume are still assigned
        associated voxels. If outside_node_margin is True, then a node is
        always assigned voxels regardless of its position in the volume.
    results_backend : 'native' or 'hdf5' or None (default).
        Specifies the way results are provided back from a processing block
        in case of nproc > 1. 'native' is pickling/unpickling of results by
        pprocess, while 'hdf5' would use h5save/h5load functionality.
        'hdf5' might be more time and memory efficient in some cases.
        If None, then 'hdf5' if used if available, else 'native'.
    tmp_prefix : str, optional
        If specified -- serves as a prefix for temporary files storage
        if results_backend == 'hdf5'.  Thus can specify the directory to use
        (trailing file path separator is not added automagically).
    output_modality: 'surface' or 'volume' (default: 'surface')
        Indicates whether the output is surface-based
    node_voxel_mapping: 'minimal' or 'maximal'
        If 'minimal' then each voxel is associated with at most one node.
        If 'maximal' it is associated with as many nodes that contain the
        voxel (default: 'maximal')

    Returns
    -------
    qe: SurfaceVerticesQueryEngine
        Query-engine that maps center nodes to indices of features
        (voxels) that are near each center node.
        If output_modality=='volume' then qe is of type subclass
        SurfaceVoxelsQueryEngine.
    """

    modality2class = dict(surface=SurfaceVerticesQueryEngine,
                        volume=SurfaceVoxelsQueryEngine)

    if not output_modality in modality2class:
        raise KeyError("Illegal modality %s: should be in %s" %
                            (output_modality, modality2class.keys()))

    voxsel = surf_voxel_selection.run_voxel_selection(
                                radius=radius, volume=volume,
                                white_surf=white_surf, pial_surf=pial_surf,
                                source_surf=source_surf,
                                source_surf_nodes=source_surf_nodes,
                                volume_mask=volume_mask,
                                distance_metric=distance_metric,
                                start_fr=start_fr, stop_fr=stop_fr,
                                start_mm=start_mm, stop_mm=stop_mm,
                                nsteps=nsteps, eta_step=eta_step, nproc=nproc,
                                outside_node_margin=outside_node_margin,
                                results_backend=results_backend,
                                tmp_prefix=tmp_prefix,
                                node_voxel_mapping=node_voxel_mapping)


    qe = modality2class[output_modality](voxsel, add_fa=add_fa)

    return qe

########NEW FILE########
__FILENAME__ = surf_voxel_selection
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Functionality for surface-based voxel selection

Created on Feb 13, 2012

WiP.

References
----------
NN Oosterhof, T Wiestler, PE Downing, J Diedrichsen (2011). A comparison of volume-based
and surface-based multi-voxel pattern analysis. Neuroimage, 56(2), pp. 593-600

'Surfing' toolbox: http://surfing.sourceforge.net
(and the associated documentation)
'''

__docformat__ = 'restructuredtext'


import time
import collections
import operator
import datetime
import math
import os

import numpy as np

from mvpa2.base import warning, externals

from mvpa2.misc.surfing import volgeom, volsurf, volume_mask_dict
from mvpa2.support.nibabel import surf
from mvpa2.base.progress import ProgressBar, seconds2prettystring

if externals.exists('h5py'):
    from mvpa2.base.hdf5 import h5save, h5load


# TODO: see if we use these contants, or let it be up to the user
# possibly also rename them
LINEAR_VOXEL_INDICES = "linear_voxel_indices"
CENTER_DISTANCES = "center_distances"
GREY_MATTER_POSITION = "grey_matter_position"

from mvpa2.base import debug
if __debug__:
    if not "SVS" in debug.registered:
        debug.register("SVS", "Surface-based voxel selection "
                       " (a.k.a. 'surfing')")

class VoxelSelector(object):
    '''Voxel selection for surface-based searchlights'''

    def __init__(self, radius, distance_surf, n2v, distance_metric='dijkstra',
                            outside_node_margin=None):
        '''
        Voxel selection using cortical surfaces.

        Parameters
        ----------
        radius: int or float
            Searchlight radius. If the type is int, then this set the number of
            voxels in each searchlight (with variable size of the disc across
            searchlights). If the type is float, then this sets the disc radius in
            metric distance (with variable number of voxels across searchlights).
            In the latter case, the distance unit is usually in milimeters
            (which is the unit used for FreeSurfer surfaces).
            If radius is zero then only the center node itself is considered.
        distance_surf: surf.Surface
            A surface to be used for distance measurement. Usually this is the
            intermediate distance constructed by taking the node-wise average of
            the pial and white surface.
        n2v: dict
            Mapping from center nodes to surrounding voxels (and their distances).
            Usually this is the output from volsurf.node2voxels.
        distance_metric: str
            Distance measure used to define distances between nodes on the surface.
            Currently supports 'dijkstra' and 'euclidean'
        outside_node_margin: float or True or None (default)
            By default nodes outside the volume are skipped; using this
            parameter allows for a marign. If this value is a float (possibly
            np.inf), then all nodes within outside_node_margin Dijkstra
            distance from any node within the volume are still assigned
            associated voxels. If outside_node_margin is True, then a node is
            always assigned voxels regardless of its position in the volume.
        '''
        tp = type(radius)
        if tp is int: # fixed number of voxels
            self._fixedradius = False
            # use a (very arbitary) way to estimate how big the radius should
            # be initally to select the required number of voxels
            initradius_mm = .001 + 1.5 * float(radius) ** .5
        elif tp is float: # fixed metric radius
            self._fixedradius = True
            initradius_mm = radius
        else:
            raise TypeError("Illegal type for radius: expected int or float")
        self._targetradius = radius # radius to achieve (float or int)
        self._initradius_mm = initradius_mm # initial radius in mm
        self._optimizer = _RadiusOptimizer(initradius_mm)
        self._distance_metric = distance_metric # }
        self._surf = distance_surf                     # } save input
        self._n2v = n2v                       # }
        self._outside_node_margin = outside_node_margin

    def _select_approx(self, voxprops, count=None):
        '''
        Select approximately a certain number of voxels.

        Parameters
        ----------
        voxprops: dict
            Various voxel properties, where voxprops[key] is a list with
            voxprops[key][i] property key for voxel i.
            Should at least have a key 'distances'.
            Each of voxprops[key] should be list-like of equal length.
        count: int (default: None)
            How many voxels should be selected, approximately

        Returns
        -------
        v2d_sel: dict
            Similar mapping as the input, with approximately 'count' voxels
            with the smallest distance in 'voxprops'
            If 'count is None' then 'v2d_sel is None'
            If voxprops has fewer than 'count' elemens then 'v2d_sel' is None

        Notes
        -----
        Distances are only computed along the surface; the relative position of
        a voxel within the gray matter is ignored. Therefore, multiple voxels
        can have the same distance from a center node. See node2voxels.
        '''

        if count is None:
            return voxprops

        distkey = CENTER_DISTANCES
        if not distkey in voxprops:
            raise KeyError("No distance key %s in it - cannot select voxels" %
                           distkey)
        allds = voxprops[distkey]

        #assert sorted(allds)==allds #that's what voxprops should give us

        n = len(allds)

        if n < count or n == 0:
            return None

        # here, a 'chunk' is a set of voxels at the same distance. voxels are
        # selected in chunks with increasing distance. either all voxels in a
        # chunk are selected or none.
        curchunk = []
        prevd = allds[0]
        chunkcount = 1
        for i in xrange(n):
            d = allds[i] # distance
            if i > 0 and prevd != d:
                if i >= count: # we're done, use the chunk we have now
                    break
                curchunk = [i] # start a new chunk
                chunkcount += 1
            else:
                curchunk.append(i)

            prevd = d

        # see if the last chunk should be added or not to be as close as
        # possible to count
        firstpos = curchunk[0]
        lastpos = curchunk[-1]

        # difference in distance between desired count and positions
        delta = (count - firstpos) - (lastpos - count)
        if delta > 0:
            # lastpos is closer to count
            cutpos = lastpos + 1
        elif delta < 0:
            # firstpos is closer to count
            cutpos = firstpos
        else:
            # it's a tie, choose quasi-randomly based on chunkcount
            cutpos = firstpos if chunkcount % 2 == 0 else (lastpos + 1)

        for k in voxprops.keys():
            voxprops[k] = voxprops[k][:cutpos]

        return voxprops


    def disc_voxel_attributes(self, src):
        '''
        Voxel selection for single center node

        Parameters
        ----------
        src: int
            Index of center node to be used as searchlight center

        Returns
        -------
        voxprops: dict
            Various voxel properties, where voxprops[key] is a list with
            voxprops[key][i] property key for voxel i.
            Has at least a key 'distances'.
            Each of voxprops[key] should be list-like of equal length.
        '''
        optimizer = self._optimizer
        dist_surf = self._surf
        n2v = self._n2v
        outside_node_margin = self._outside_node_margin

        def node_in_vol(nd):
            return nd in n2v and not n2v[nd] is None

        if not node_in_vol(src) and not outside_node_margin is True:
            skip = True
            if not outside_node_margin is None:
                if math.isinf(outside_node_margin):
                    if __debug__:
                        debug("SVS", "")
                        debug("SVS", "node %s is outside - considering all other "
                                     "nodes that may be inside" % (src,))
                    for nd in n2v:
                        if node_in_vol(nd):
                            skip = False
                            break
                else:
                    node_distances = dist_surf.circlearound_n2d(src,
                                                radius=outside_node_margin,
                                                metric=self._distance_metric)

                    if __debug__:
                        debug("SVS", "")
                        debug("SVS", "node %s is outside - considering %d distances"
                                    " to other nodes that may be inside." % ((src,), len(node_distances)))
                    for nd, d in node_distances.iteritems():
                        if nd in n2v and not n2v[nd] is None and d <= outside_node_margin:
                            if __debug__:
                                debug("SVS", "node #%s is distance %s <= %s from #%d "
                                      " and kept" %
                                      ((src,), d, outside_node_margin, nd))
                            skip = False
                            break

            if skip:
                # no voxels associated with this node, skip
                if __debug__:
                    debug("SVS", "Skipping node %s (no voxels associated)" %
                                        (src,), cr=True)

                return []

        radius_mm = optimizer.get_start()
        radius = self._targetradius

        maxiter = 100
        for counter in xrange(maxiter):
            if radius_mm == 0:
                # only the node itself.
                # this should work except for very strange surfaces where
                # multiple nodes occupy exactly the same spatial location
                around_n2d = {src:0.}
            else:
                around_n2d = dist_surf.circlearound_n2d(src, radius_mm,
                                               self._distance_metric)

            allvxdist = self.nodes2voxel_attributes(around_n2d, n2v)

            if not allvxdist:
                voxel_attributes = []

            if self._fixedradius:
                # select all voxels
                voxel_attributes = self._select_approx(allvxdist, count=None)
            else:
                # select only certain number
                voxel_attributes = self._select_approx(allvxdist, count=radius)

            if voxel_attributes is None:
                # coult not find enough voxels, stay in loop and try again
                # with bigger radius
                radius_mm = optimizer.get_next()
            else:
                break

        if counter + 1 >= maxiter:
            raise ValueError("Failure to increase radius to get %d voxels for "
                             " node #%d" % (radius, src))

        if voxel_attributes and len(voxel_attributes[CENTER_DISTANCES]):
            # found at least one voxel; update our optimizer
            maxradius = voxel_attributes[CENTER_DISTANCES][-1]
            optimizer.set_final(maxradius)

        return voxel_attributes

    def disc_voxel_indices_and_attributes(self, src):
        ''' For now this is a wrapper
        TODO integrate with calling function'''
        attrs = self.disc_voxel_attributes(src)

        if not attrs:
            return None, None

        idxs = attrs.pop(LINEAR_VOXEL_INDICES)
        return idxs, attrs


    def nodes2voxel_attributes(self, n2d, n2v, distancesummary=min):
        '''
        Computes voxel distances

        Parameters
        ----------
        n2d: dict
            A mapping from node indices to distances (to a center node)
            Usually this is the output from surf.circlearound_n2d and thus
            only contains voldata for voxels surrounding a single center node
        n2v: dict
            A mapping from nodes to surrounding voxel indices and distances.
            n2v[i]=v2d is a dict mapping node i to a dict v2d, which in turn
            maps voxel indices to distances to the center node (i.e. v2d[j]=d
            means that the distance from voxel with linear index j to the
            node with index i is d
        distancesummary: function
            This is by default the min function. It is used to summarize
            cases where a single voxels has multiple distances (and nodes)
            associated with it. By default we take the minimum distance, and
            the node that gives rise to this distance, as a representative
            for the distance.

        Returns
        -------
        voxelprops: dict
            Mapping from keys to lists that contain voxel properties.
            Each list should have the same length
            It has at least a key sparse_volmasks._VOXIDXSLABEL which maps to
            the linear voxel indices. It may also have 'distances' (distance from
            center node along the cortex)  and 'gmpositions' (relative position in
            the gray matter)

        '''

        # mapping from voxel indices to all distances
        v2dps = collections.defaultdict(set)

        # get node indices and associated (distance, grey matter positions)
        for nd, d in n2d.iteritems():
            if nd in n2v:
                vps = n2v[nd] # all voxels associated with this node
                if not vps is None:
                    for vx, pos in vps.items():
                        v2dps[vx].add((d, pos)) # associate voxel with tuple of distance and relative position


        # converts a tuple (vx, set([(d0,p0),(d1,p1),...]) to a triple (vx,pM,dM)
        # where dM is the minimum across all d*
        def unpack_dp(vx, dp, distancesummary=distancesummary):
            d, p = distancesummary(dp) # implicit sort by first elemnts first, i.e. distance
            return vx, d, p


        # make triples of (voxel index, distance to center node, relative position in grey matter)
        vdp = [unpack_dp(vx, dp) for vx, dp in v2dps.iteritems()]

        # sort triples by distance to center node
        vdp.sort(key=operator.itemgetter(1))

        if not vdp:
            vdp_tup = ([], [], []) # empty
        else:
            vdp_tup = zip(*vdp) # unzip triples into three lists

        vdp_tps = (np.int32, np.float32, np.float32)
        vdp_labels = (LINEAR_VOXEL_INDICES, CENTER_DISTANCES, GREY_MATTER_POSITION)

        voxel_attributes = dict()
        for i in xrange(3):
            voxel_attributes[vdp_labels[i]] = np.asarray(vdp_tup[i], dtype=vdp_tps[i])

        return voxel_attributes

def voxel_selection(vol_surf_mapping, radius, source_surf=None, source_surf_nodes=None,
                    distance_metric='dijkstra',
                    eta_step=10, nproc=None,
                    outside_node_margin=None,
                    results_backend=None, tmp_prefix='tmpvoxsel'):

    """
    Voxel selection for multiple center nodes on the surface

    Parameters
    ----------
    vol_surf_mapping: volsurf.VolSurfMapping
        Contains gray and white matter surface, and volume geometry
    radius: int or float
        Size of searchlight. If an integer, then it indicates the number of
        voxels. If a float, then it indicates the radius of the disc
    source_surf: surf.Surface or None
        Surface used to compute distance between nodes. If omitted, it is
        the average of the gray and white surfaces.
    source_surf_nodes: list of int or numpy array or None
        Indices of nodes in source_surf that serve as searchlight center.
        By default every node serves as a searchlight center.
    distance_metric: str
        Distance metric between nodes. 'euclidean' or 'dijksta' (default)
    eta_step: int
        Report progress every eta_step (default: 10).
    nproc: int or None
        Number of parallel threads. None means as many threads as the
        system supports. The pprocess is required for parallel threads; if
        it cannot be used, then a single thread is used.
    outside_node_margin: float or True or None (default)
        By default nodes outside the volume are skipped; using this
        parameter allows for a marign. If this value is a float (possibly
        np.inf), then all nodes within outside_node_margin Dijkstra
        distance from any node within the volume are still assigned
        associated voxels. If outside_node_margin is True, then a node is
        always assigned voxels regardless of its position in the volume.
    results_backend : 'native' or 'hdf5' or None (default).
        Specifies the way results are provided back from a processing block
        in case of nproc > 1. 'native' is pickling/unpickling of results by
        pprocess, while 'hdf5' would use h5save/h5load functionality.
        'hdf5' might be more time and memory efficient in some cases.
        If None, then 'hdf5' if used if available, else 'native'.
    tmp_prefix : str, optional
        If specified -- serves as a prefix for temporary files storage
        if results_backend == 'hdf5'.  Thus can specify the directory to use
        (trailing file path separator is not added automagically).

    Returns
    -------
    sel: volume_mask_dict.VolumeMaskDictionary
        Voxel selection results, that associates, which each node, the indices
        of the surrounding voxels.
    """

    # construct the intermediate surface, which is used
    # to measure distances
    intermediate_surf = (vol_surf_mapping.pial_surface * .5) + \
                        (vol_surf_mapping.white_surface * .5)

    if source_surf is None:
        source_surf = intermediate_surf
    else:
        source_surf = surf.from_any(source_surf)

    if _debug():
        debug('SVS', "Generated high-res intermediate surface: "
              "%d nodes, %d faces" %
              (intermediate_surf.nvertices, intermediate_surf.nfaces))
        debug('SVS', "Mapping source to high-res surface:"
              " %d nodes, %d faces" %
              (source_surf.nvertices, source_surf.nfaces))


    if distance_metric[0].lower() == 'e' and outside_node_margin:
        # euclidean distance: identity mapping
        # this is *slow*
        n = source_surf.nvertices
        xyz = source_surf.vertices
        src2intermediate = dict((i, tuple(xyz[i])) for i in xrange(n))
    else:
        # find a mapping from nodes in source_surf to those in
        # intermediate surface
        src2intermediate = source_surf.map_to_high_resolution_surf(\
                                                        intermediate_surf)

    # if no sources are given, then visit all ndoes
    if source_surf_nodes is None:
        source_surf_nodes = np.arange(source_surf.nvertices)

    n = len(source_surf_nodes)

    if _debug():
        debug('SVS',
              "Performing surface-based voxel selection"
              " for %d centers" % n)

    # visit in random order, for for better ETA estimate
    visitorder = list(np.random.permutation(len(source_surf_nodes)))

    # construct mapping from nodes to enclosing voxels
    n2v = vol_surf_mapping.get_node2voxels_mapping()

    if __debug__:
        debug('SVS', "Generated mapping from nodes"
              " to intersecting voxels")

    # build voxel selector
    voxel_selector = VoxelSelector(radius, intermediate_surf, n2v,
                                   distance_metric,
                                   outside_node_margin=outside_node_margin)

    if _debug():
        debug('SVS', "Instantiated voxel selector (radius %r)" % radius)


    # structure to keep output data. Initialize with None, then
    # make a sparse_attributes instance when we know what the attributes are
    node2volume_attributes = None

    attribute_mapper = voxel_selector.disc_voxel_indices_and_attributes

    srcs_order = [source_surf_nodes[node] for node in visitorder]
    src_trg_nodes = [(src, src2intermediate[src]) for src in srcs_order]

    if nproc is not None and nproc > 1 and not externals.exists('pprocess'):
        raise RuntimeError("The 'pprocess' module is required for "
                           "multiprocess searchlights. Please either "
                           "install python-pprocess, or reduce `nproc` "
                           "to 1 (got nproc=%i) or set to default None"
                           % nproc)

    if nproc is None:
        if externals.exists('pprocess'):
            try:
                import pprocess
                nproc = pprocess.get_number_of_cores() or 1
                if _debug() :
                    debug("SVS", 'Using pprocess with %d cores' % nproc)
            except:
                if _debug():
                    debug("SVS", 'pprocess not available')

        if nproc is None:
            # importing pprocess failed - so use a single core
            nproc = 1
            debug("SVS", 'Using %d cores - pprocess not available' % nproc)

    # get the the voxel selection parameters
    parameter_dict = vol_surf_mapping.get_parameter_dict()
    parameter_dict.update(dict(radius=radius,
                               outside_node_margin=outside_node_margin,
                               distance_metric=distance_metric),
                               source_nvertices=source_surf.nvertices)


    init_output = lambda: volume_mask_dict.VolumeMaskDictionary(
                                    vol_surf_mapping.volgeom,
                                    intermediate_surf,
                                    meta=parameter_dict)

    if nproc > 1:
        if results_backend == 'hdf5':
            externals.exists('h5py', raise_=True)
        elif results_backend is None:
            if externals.exists('h5py') and externals.versions['hdf5'] >= '1.8.7':
                results_backend = 'hdf5'
            else:
                results_backend = 'native'
        if _debug():
            debug('SVS', "Using '%s' backend" % (results_backend,))

        if not results_backend in ('native', 'hdf5'):
            raise ValueError('Illegal results backend %r' % results_backend)

        import pprocess
        n_srcs = len(src_trg_nodes)
        blocks = np.array_split(np.arange(n_srcs), nproc)

        results = pprocess.Map(limit=nproc)
        reducer = results.manage(pprocess.MakeParallel(_reduce_mapper))

        if __debug__:
            debug('SVS', "Starting %d child processes", (len(blocks),))

        for i, block in enumerate(blocks):
            empty_dict = init_output()

            src_trg = []
            for idx in block:
                src_trg.append(src_trg_nodes[idx])

            if _debug():
                debug('SVS', "  starting block %d/%d: %d centers" %
                            (i + 1, nproc, len(src_trg)), cr=True)

            reducer(empty_dict, attribute_mapper, src_trg,
                    eta_step=eta_step, proc_id='%d' % (i + 1,),
                    results_backend=results_backend, tmp_prefix=tmp_prefix)
        if _debug():
            debug('SVS', '')
            debug('SVS', 'Started all %d child processes' % (len(blocks)))
            tstart = time.time()

        node2volume_attributes = None
        for i, result in enumerate(results):
            if result is None:
                continue

            if results_backend == 'hdf5':
                result_fn = result
                result = h5load(result_fn)
                os.remove(result_fn)

            if node2volume_attributes is None:
                # first time we have actual results.
                # Use as a starting point
                node2volume_attributes = result
                if _debug():
                    debug('SVS', '')
                    debug('SVS', "Merging results from %d child "
                                 "processes using '%s' backend" %
                                 (len(blocks), results_backend))
            else:
                # merge new with current data
                node2volume_attributes.merge(result)
            if _debug():
                debug('SVS', "  merged result block %d/%d" % (i + 1, nproc),
                                cr=True)

        if _debug():
            telapsed = time.time() - tstart
            debug('SVS', "")
            debug('SVS', 'Merged results from %d child processed - '
                         'took %s' %
                         (len(blocks), seconds2prettystring(telapsed)))

    else:
        empty_dict = init_output()
        node2volume_attributes = _reduce_mapper(empty_dict,
                                                attribute_mapper,
                                                src_trg_nodes,
                                                eta_step=eta_step)
        debug('SVS', "")

    if _debug():
        if node2volume_attributes is None:
            msgs = ["Voxel selection completed: none of %d nodes have "
                    "voxels associated" % len(visitorder)]
        else:
            nvox_selected = np.sum(node2volume_attributes.get_mask() != 0)
            vg = vol_surf_mapping.volgeom

            msgs = ["Voxel selection completed: %d / %d nodes have "
                    "voxels associated" %
                    (len(node2volume_attributes.keys()), len(visitorder)),
                    "Selected %d / %d  voxels (%.0f%%) in the mask at least once" %
                    (nvox_selected, vg.nvoxels_mask,
                     100. * nvox_selected / vg.nvoxels_mask)]

        for msg in msgs:
            debug("SVS", msg)


    if node2volume_attributes is None:
        warning('No voxels associated with any of %d nodes' %
                        len(visitorder))
    return node2volume_attributes

def _reduce_mapper(node2volume_attributes, attribute_mapper,
                   src_trg_indices, eta_step=1, proc_id=None,
                   results_backend='native', tmp_prefix='tmpvoxsel'):
    '''applies voxel selection to a list of src_trg_indices
    results are added to node2volume_attributes.
    '''

    if not src_trg_indices:
        return None

    if not results_backend in ('native', 'hdf5'):
        raise ValueError('Illegal results backend %r' % results_backend)


    def _pat(index, xs=src_trg_indices, f=max):
        try:
            if not xs:
                y = 1
            else:
                y = f(x[index] for x in xs)
            if y < 1:
                y = 1
            p = '%%%dd' % math.ceil(math.log10(y))
        except:
            p = '%s'
        return p

    progresspat = '(node %s -> %s)' % (_pat(0), _pat(1))

    # start the clock
    bar = ProgressBar()
    n = len(src_trg_indices)

    for i, (src, trg) in enumerate(src_trg_indices):
        idxs, misc_attrs = attribute_mapper(trg)

        if idxs is not None:
            node2volume_attributes.add(int(src), idxs, misc_attrs)

        if _debug() and eta_step and (i % eta_step == 0 or i == n - 1):
            msg = bar(float(i + 1) / n, progresspat % (src, trg))
            if not proc_id is None:
                msg += ' (#%s)' % proc_id
            debug('SVS', msg, cr=True)

    if results_backend == 'hdf5':
        tmp_postfix = ('__tmp__%d_%s.h5py' %
                                 (hash(time.time()), proc_id))
        tmp_fn = tmp_prefix + tmp_postfix
        h5save(tmp_fn, node2volume_attributes)
        return tmp_fn
    else:
        return node2volume_attributes

def _debug():
    return __debug__ and 'SVS' in debug.active



def run_voxel_selection(radius, volume, white_surf, pial_surf,
                         source_surf=None, source_surf_nodes=None,
                         volume_mask=None, distance_metric='dijkstra',
                         start_mm=0, stop_mm=0, start_fr=0., stop_fr=1.,
                         nsteps=10, eta_step=1, nproc=None,
                         outside_node_margin=None,
                         results_backend=None, tmp_prefix='tmpvoxsel',
                         node_voxel_mapping='maximal'):

    """
    Voxel selection wrapper for multiple center nodes on the surface

    Parameters
    ----------
    radius: int or float
        Size of searchlight. If an integer, then it indicates the number of
        voxels. If a float, then it indicates the radius of the disc
    volume: Dataset or NiftiImage or volgeom.Volgeom
        Volume in which voxels are selected.
    white_surf: str of surf.Surface
        Surface of white-matter to grey-matter boundary, or filename
        of file containing such a surface.
    pial_surf: str of surf.Surface
        Surface of grey-matter to pial-matter boundary, or filename
        of file containing such a surface.
    source_surf: surf.Surface or None
        Surface used to compute distance between nodes. If omitted, it is
        the average of the gray and white surfaces.
    source_surf_nodes: list of int or numpy array or None
        Indices of nodes in source_surf that serve as searchlight center.
        By default every node serves as a searchlight center.
    volume_mask: None (default) or False or int
        Mask from volume to apply from voxel selection results. By default
        no mask is applied. If volume_mask is an integer k, then the k-th
        volume from volume is used to mask the data. If volume is a Dataset
        and has a property volume.fa.voxel_indices, then these indices
        are used to mask the data, unless volume_mask is False or an integer.
    distance_metric: str
        Distance metric between nodes. 'euclidean' or 'dijksta' (default)
    start_fr: float (default: 0)
            Relative start position of line in gray matter, 0.=white
            surface, 1.=pial surface
    stop_fr: float (default: 1)
        Relative stop position of line (as in see start)
    start_mm: float (default: 0)
        Absolute start position offset (as in start_fr)
    stop_mm: float (default: 0)
        Absolute start position offset (as in start_fr)
    nsteps: int (default: 10)
        Number of steps from white to pial surface
    eta_step: int (default: 1)
        After how many searchlights an estimate should be printed of the
        remaining time until completion of all searchlights
    nproc: int or None
        Number of parallel threads. None means as many threads as the
        system supports. The pprocess is required for parallel threads; if
        it cannot be used, then a single thread is used.
    outside_node_margin: float or None (default)
        By default nodes outside the volume are skipped; using this
        parameter allows for a marign. If this value is a float (possibly
        np.inf), then all nodes within outside_node_margin Dijkstra
        distance from any node within the volume are still assigned
        associated voxels. If outside_node_margin is True, then a node is
        always assigned voxels regardless of its position in the volume.
    results_backend : 'native' or 'hdf5' or None (default).
        Specifies the way results are provided back from a processing block
        in case of nproc > 1. 'native' is pickling/unpickling of results by
        pprocess, while 'hdf5' would use h5save/h5load functionality.
        'hdf5' might be more time and memory efficient in some cases.
        If None, then 'hdf5' if used if available, else 'native'.
    tmp_prefix : str, optional
        If specified -- serves as a prefix for temporary files storage
        if results_backend == 'hdf5'.  Thus can specify the directory to use
        (trailing file path separator is not added automagically).
    node_voxel_mapping: 'minimal' or 'maximal' or 'minimal_lowres'
        If 'minimal' then each voxel is associated with at most one node.
        If 'maximal' it is associated with as many nodes that contain the
        voxel (default: 'maximal').
        If 'minimal_lowres' then each voxel is associated with at most one
        node, and each node that is mapped onto has a corresponding node
        (at the same spatial location) in source_surf.


    Returns
    -------
    sel: volume_mask_dict.VolumeMaskDictionary
        Voxel selection results, that associates, which each node, the indices
        of the surrounding voxels.
    """

    vg = volgeom.from_any(volume, volume_mask)

    mapper_dict = dict(maximal=volsurf.VolSurfMaximalMapping,
                       minimal=volsurf.VolSurfMinimalMapping,
                       minimal_lowres=volsurf.VolSurfMinimalLowresMapping)

    mapper = mapper_dict[node_voxel_mapping]

    vsm = mapper(vg, white=white_surf, pial=pial_surf,
                 intermediate=source_surf, nsteps=nsteps, start_fr=start_fr,
                 stop_fr=stop_fr, start_mm=start_mm, stop_mm=stop_mm)

    sel = voxel_selection(vol_surf_mapping=vsm, radius=radius,
                          source_surf=source_surf,
                          source_surf_nodes=source_surf_nodes,
                          distance_metric=distance_metric,
                          eta_step=eta_step, nproc=nproc,
                          outside_node_margin=outside_node_margin,
                          results_backend=results_backend,
                          tmp_prefix=tmp_prefix)

    return sel

class _RadiusOptimizer():
    '''
    Internal class to optimize the initial radius used for voxel selection.

    In the case of selecting a fixed number of voxels in each searchlight, the
    required radius will vary across searchlights. The general strategy is to take
    some initial radius, find the nodes that are within that radius, select the
    corresponding voxels, and see if enough voxels are selected. If not, the radius is
    increased and these steps repeated.

    A larger initial radius means a decrease in the probability that not enough voxels are
    selected, but an increase in time to compute distances and select voxels.

    The challenge therefore to find the optimal initial radius so that overall computational
    time is minimized.

    The present implementation is very stupid and just increases the radius every time
    by a factor of 1.5.

    NNO: as of August 2012 it seems that voxel selection is actually quite fast,
    so maybe this function is good as is
    '''
    def __init__(self, initradius):
        '''new instance, with certain initial radius'''
        self._initradius = initradius
        self._initmult = 1.5

    def get_start(self):
        '''get an (initial) radius for a new searchlight.'''
        self._curradius = self._initradius
        self._count = 0
        return self._curradius

    def get_next(self):
        '''get a new (better=larger) radius for the current searchlight'''
        self._count += 1
        self._curradius *= self._initmult
        return self._curradius

    def set_final(self, finalradius):
        '''to tell what the final radius was that satisfied the number of required voxels'''
        pass

    def __repr__(self):
        return 'radius is %f, %d steps' % (self._curradius, self._count)

def from_any(s):
    '''
    Loads or returns voxel selection results

    Parameters
    ----------
    s: basestring or volume_mask_dict.VolumeMaskDictionary
        if a string it is assumed to be a file name and loaded using h5load. If
        a volume_mask_dict.VolumeMaskDictionary then it is returned.

    Returns
    -------
    r: volume_mask_dict.VolumeMaskDictionary
    '''

    # this is just a convenience function
    return volume_mask_dict.from_any(s)


########NEW FILE########
__FILENAME__ = volgeom
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Volume geometry to map between world and voxel coordinates.

Supports conversion between linear and sub indexing of voxels.
The rationale is that volumes use sub indexing that incorporate the spatial
locations of voxels, but for voxel selection (and subsequent MVPA) it is
often more appropriate to abstract from the temporal locations of voxels.

Created on Feb 12, 2012

@author: nick
'''

__docformat__ = 'restructuredtext'


import numpy as np

from mvpa2.base import warning, externals

if externals.exists('nibabel'):
    # nibabel is optional dependency here, those how would reach those points
    # without having nibabel should suffer
    import nibabel as nb

from mvpa2.misc.neighborhood import Sphere

class VolGeom(object):
    '''Defines a mapping between sub and linear indices and world coordinate
    in volumatric fmri datasets'''

    def __init__(self, shape, affine, mask=None):
        '''
        Parameters
        ----------
        shape: tuple
            Number of values in each dimension.
            Typically the first three dimensions are spatial and the remaining ones
            temporal. Only the first three dimensions are stored
        affine: numpy.ndarray
            4x4 affine transformation array that maps voxel to world coordinates.
        mask: numpy.ndarray (default: None)
            voxel mask that indicates which voxels are included. Values of zero in
            mask mean that a voxel is not included. If mask is None, then all
            voxels are included.

        '''
        if not type(shape) is tuple or len(shape) < 3:
            raise ValueError("Shape should be a tuple with at least 3 values")

        self._shape = (shape[0], shape[1], shape[2])
        self._affine = np.asarray(affine)

        if self._affine.shape != (4, 4):
            raise ValueError('Affine matrix should be 4x4')

        if not mask is None:
            if mask.size != self.nvoxels:
                raise ValueError("%d voxels, but mask has %d" %
                                 (self.nvoxels, mask.size))
            if len(mask.shape) >= 3 and shape[:3] != mask.shape[:3]:
                raise ValueError("Shape mismatch for mask")

            mask = np.reshape(mask != 0, (-1,))
        self._mask = mask

    def same_geometry(self, other):
        '''Compares this geometry with another instance

        Parameters
        ----------
        other: VolGeom
            instance to which the current instance is compared

        Returns
        -------
        same: boolean
            True iff it has the same geometry. It does not compare
            whether the mask is the same'''

        return (self.same_shape(other) and
                np.all(self.affine == other.affine))

    def same_shape(self, other):
        '''Compares the shape of the spatial dimensions with another instance

        Parameters
        ----------
        other: VolGeom
            instance to which the current instance is compared

        Returns
        -------
        same: boolean
            True iff it has the same shape in the first three dimensions'''

        if not isinstance(other, self.__class__):
            return False

        return self.shape == other.shape

    def same_mask(self, other):
        '''Compares the mask with another instance

        Parameters
        ----------
        other: VolGeom
            instance to which the current instance is compared

        Returns
        -------
        same: boolean
            True iff it has effectively the same mask'''

        if not self.same_shape(other):
            return False

        p = self.mask
        q = other.mask

        if p is None:
            return q.nvoxels_mask == self.nvoxels_mask
        else:
            if q is None:
                return q.nvoxels_mask == self.nvoxels_mask
            else:
                return np.all(self.mask == other.mask)

    def __eq__(self, other):
        return (self.same_geometry(other) and
                np.all(self.affine == other.affine))

    def __ne__(self, other):
        return not self.__eq__(other)

    def __repr__(self, prefixes=[]):
        prefixes_ = ['shape=(%s)' % ','.join(['%r' % i for i in self._shape]),
                     'affine=%r' % self._affine] + prefixes
        if not self._mask is None:
            prefixes_ += ['mask=%r' % self._mask]
        return "%s(%s)" % (self.__class__.__name__, ', '.join(prefixes_))

    def __str__(self):
        sh = self.shape[:3]
        s = '%s(%s = %d voxels' % (self.__class__.__name__,
                             '%d x %d x %d' % sh, self.nvoxels)
        if not self.mask is None:
            s += ', %d voxels survive the mask' % self.nvoxels_mask

        s += ')'
        return s


    def as_pickable(self):
        '''
        Returns a pickable instance.

        Returns
        -------
        dict
            A dictionary that contains all information from this instance
            (and can be saved using pickle).
        '''
        d = dict(shape=self.shape, affine=self.affine, mask=self.mask)
        return d

    def __reduce__(self):
        return (self.__class__, (self._shape, self._affine, self._mask))

    @property
    def mask(self):
        '''
        Returns the mask.

        Returns
        -------
        mask: np.ndarray
            boolean vector indicating which voxels are included.
            If no mask is associated with this instance then mask is None.
        '''
        if self._mask is None:
            return None

        m = self._mask.view()
        m.flags.writeable = False
        return m

    @property
    def linear_mask(self):
        '''Returns the mask as a vector

        Returns
        -------
        mask: np.ndarray (vector with P values)
            boolean vector indicating which voxels are included.
            If no mask is associated with this instance then mask is None.
        '''
        if self._mask is None:
            return None

        return np.reshape(self.mask, (self.nvoxels,))

    def _ijkmultfac(self):
        '''multiplication factors for ijk <--> linear indices conversion'''
        sh = self.shape
        return [sh[1] * sh[2], sh[2], 1]

    def _contains_ijk_unmasked(self, ijk):
        '''helper function to see if ijk indices are in the volume'''
        shape = self.shape

        m = reduce(np.logical_and, [0 <= ijk[:, 0], ijk[:, 0] < shape[0],
                                   0 <= ijk[:, 1], ijk[:, 1] < shape[1],
                                   0 <= ijk[:, 2], ijk[:, 2] < shape[2]])
        return m

    def _outside_vol(self, ijk, lin, apply_mask=True):
        '''helper function to see if ijk and lin indices are in the volume.
        It is assumed that these indices are matched (i.e. ijk[i,:] and lin[i]
        refer to the same voxel, for all i).

        The rationale for providing both is that ijk is necessary to determine
        whether a voxel is within the volume, while lin is necessary for
        considering which voxels are in the mask'''
        invol = self._contains_ijk_unmasked(ijk)
        invol[np.logical_or(lin < 0, lin >= self.nvoxels)] = np.False_
        invol[np.isnan(lin)] = np.False_

        if apply_mask and not self.mask is None and invol.size:
            invol[invol] = np.logical_and(invol[invol], self.mask[lin[invol]])

        return np.logical_not(invol)

    def _ijk2lin_unmasked(self, ijk):
        '''helper function to convert sub to linear indices.'''
        m = np.zeros((3,), dtype=int)
        fs = self._ijkmultfac()

        # make a 3x1 vector with multiplication factors
        for i, f in enumerate(fs):
            m[i] = f

        lin = np.dot(ijk, m)
        return lin

    def _lin2ijk_unmasked(self, lin):
        '''Converts sub to linear voxel indices

        Parameters
        ----------
            Px1 array with linear voxel indices

        Returns
        -------
        ijk: numpy.ndarray
            Px3 array with sub voxel indices
        '''

        if not isinstance(lin, np.ndarray):
            lin = np.asarray(lin, dtype=np.int_)
        else:
            lin = lin.astype(np.int_)

        lin = lin.ravel()

        n = np.shape(lin)[0]
        fs = self._ijkmultfac()

        ijk = np.zeros((n, 3), dtype=int)
        for i, f in enumerate(fs):
            v = lin // f
            ijk[:, i] = v[:]
            lin -= v * f

        return ijk

    def ijk2triples(self, ijk):
        '''Converts sub indices to a list of triples

        Parameters
        ----------
        ijk: np.ndarray (Px3)
            sub indices

        Returns
        -------
        triples: list with P triples
            the indices from ijk, so that triples[i][j]==ijk[i,j]
        '''

        return map(tuple, ijk)

    def triples2ijk(self, tuples):
        '''Converts triples to sub indices

        Parameters
        ----------
        triples: list with P triples

        Returns
        -------
        ijk: np.ndarray(Px3)
            an array from triples, so that ijk[i,j]==triples[i][j]
        '''

        return np.asarray(tuples)

    def ijk2lin(self, ijk):
        '''Converts sub to linear voxel indices.

        Parameters
        ----------
        ijk: numpy.ndarray
            Px3 array with sub voxel indices.

        Returns
        -------
        lin: Px1 array with linear voxel indices.
            If ijk[i,:] is outside the volume, then lin[i]==self.nvoxels.
        '''
        ijk = to_three_column_array(ijk)

        lin = self._ijk2lin_unmasked(ijk)
        lin[self._outside_vol(ijk, lin)] = self.nvoxels

        return lin

    def lin2ijk(self, lin):
        '''Converts sub to linear voxel indices.

        Parameters
        ----------
            Px1 array with linear voxel indices.

        Returns
        -------
        ijk: numpy.ndarray
            Px3 array with sub voxel indices.
            If lin[i] is outside the volume, then ijk[i,:]==self.shape.
        '''

        lin = to_vector(lin)

        ijk = self._lin2ijk_unmasked(lin)
        ijk[self._outside_vol(ijk, lin), :] = self.shape[:3]

        return ijk

    @property
    def affine(self):
        '''Returns the affine transformation matrix.

        Returns
        -------
        affine : numpy.ndarray
            4x4 array that maps voxel to world coordinates.
        '''

        a = self._affine.view()
        a.flags.writeable = False
        return a


    def xyz2ijk(self, xyz):
        '''Maps world coordinates to sub voxel indices.

        Parameters
        ----------
        xyz : numpy.ndarray (float)
            Px3 array with world coordinates.

        Returns
        -------
        ijk: numpy.ndarray (int)
            Px3 array with sub voxel indices.
            If xyz[i,:] is outside the volume, then ijk[i,:]==self.shape
        '''
        xyz = to_three_column_array(xyz)

        m = self.affine
        minv = np.linalg.inv(m)

        ijkfloat = self.apply_affine3(minv, xyz)

        # add .5 so that positions are rounded instead of floored CHECKME
        ijk = np.array(ijkfloat + .5, dtype=int)

        lin = self._ijk2lin_unmasked(ijk)

        ijk[self._outside_vol(ijk, lin), :] = self.shape[:3]
        return ijk

    def ijk2xyz(self, ijk):
        '''Maps sub voxel indices to world coordinates.

        Parameters
        ----------
        ijk: numpy.ndarray (int)
            Px3 array with sub voxel indices.

        Returns
        -------
        xyz : numpy.ndarray (float)
            Px3 array with world coordinates.
            If ijk[i,:] is outside the volume, then xyz[i,:] is NaN.
        '''

        ijk = to_three_column_array(ijk)

        m = self.affine
        ijkfloat = np.array(ijk, dtype=float)
        xyz = self.apply_affine3(m, ijkfloat)

        lin = self._ijk2lin_unmasked(ijk)
        self._outside_vol(ijk, lin)

        xyz[self._outside_vol(ijk, lin), :] = np.NaN
        return xyz


    def xyz2lin(self, xyz):
        '''Maps world coordinates to linear voxel indices.

        Parameters
        ----------
        xyz : numpy.ndarray (float)
            Px3 array with world coordinates

        Returns
        -------
        ijk: numpy.ndarray (int)
            Px1 array with linear indices.
            If xyz[i,:] is outside the volume, then lin[i]==self.nvoxels.
        '''
        return self.ijk2lin(self.xyz2ijk(xyz))

    def lin2xyz(self, lin):
        '''Maps linear voxel indices to world coordinates.

        Parameters
        ----------
        ijk: numpy.ndarray (int)
            Px3 array with linear voxel indices.

        Returns
        -------
        xyz : np.ndarray (float)
            Px1 array with world coordinates.
            If lin[i] is outside the volume, then xyz[i,:] is NaN.
        '''

        return self.ijk2xyz(self.lin2ijk(lin))

    def apply_affine3(self, mat, v):
        '''Applies an affine transformation matrix.

        Parameters
        ----------
        mat : numpy.ndarray (float)
            Matrix with size at least 3x4
        v : numpy.ndarray (float)
            Px3 values to which transformation is applied

        Returns
        -------
        w : numpy.ndarray(float)
            Px3 transformed values
        '''

        r = mat[:3, :3]
        t = mat[:3, 3].transpose()

        return np.dot(v, r) + t

    @property
    def nvoxels(self):
        '''
        Returns the number of voxels.

        Returns
        -------
        nv: int
            Number of spatial points (i.e. number of voxels)
        '''
        return np.prod(self.shape[:3])

    @property
    def shape(self):
        '''
        Returns the shape.

        Returns
        -------
        sh: tuple of int
            Number of values in each dimension
        '''

        return self._shape

    @property
    def nvoxels_mask(self):
        '''
        Returns
        -------
        nv: int
            Number of voxels that survive the mask'''
        return self.nvoxels if self.mask is None else np.sum(self.mask)


    def contains_ijk(self, ijk, apply_mask=True):
        '''
        Returns whether a set of sub voxel indices are contained
        within this instance.

        Parameters
        ----------
        ijk : numpy.ndarray
            Px3 array with sub voxel indices

        Returns
        -------
        numpy.ndarray (boolean)
            P boolean values indicating which voxels are within the volume.
        '''
        ijk = to_three_column_array(ijk)

        lin = self._ijk2lin_unmasked(ijk)

        return np.logical_not(self._outside_vol(ijk, lin, \
                                        apply_mask=apply_mask))



    def contains_lin(self, lin, apply_mask=True):
        '''
        Returns whether a set of linear voxel indices are contained
        within this instance.

        Parameters
        ----------
        lin : numpy.ndarray
            Px1 array with linear voxel indices.

        Returns
        -------
        numpy.ndarray (boolean)
            P boolean values indicating which voxels are within the volume.
        '''
        lin = to_vector(lin)

        ijk = self._lin2ijk_unmasked(lin)

        return np.logical_not(self._outside_vol(ijk, lin, \
                                        apply_mask=apply_mask))

    def get_empty_array(self, nt=None):
        '''
        Returns an empty array with size according to the volume

        Parameters
        ----------
        nt: int or None
            Number of timepoints (or samples). Each feature has the
            same value (1 if in the mask, 0 otherwise) for each
            sample. If nt is None, then the output is 3D; otherwise
            it is 4D with 'nt' values in the last dimension.

        Returns
        -------
        arr: numpy.ndarray
            An array with value zero everywhere.
        '''
        sh = self.shape

        if not nt is None:
            sh = (sh[0], sh[1], sh[2], nt)

        data = np.zeros(sh)
        return data

    def get_empty_nifti_image(self, nt=None):
        '''
        Returns an empty nifti image with size according to the volume

        Parameters
        ----------
        nt: int or None
            Number of timepoints (or samples). Each feature has the
            same value (1 if in the mask, 0 otherwise) for each
            sample. If nt is None, then the output is 3D; otherwise
            it is 4D with 'nt' values in the last dimension.

        Returns
        -------
        arr: nibabel.Nifti1Image
            A Nifti image with value zero everywhere.
        '''
        data = self.get_empty_array(nt=nt)
        img = nb.Nifti1Image(data, self.affine)
        return img

    def get_masked_array(self, nt=None, dilate=None):
        '''Provides a masked numpy array

        Parameters
        ----------
        nt: int or None
            Number of timepoints (or samples). Each feature has the
            same value (1 if in the mask, 0 otherwise) for each
            sample. If nt is None, then the output is 3D; otherwise
            it is 4D with 'nt' values in the last dimension.
        dilate: callable or int or None
            Speficiation of mask dilation.
            If a callable, it should be a a neighborhood function
            (like Sphere(..)) that can map a single voxel coordinate
            (represented as a triple of indices) to a list of voxel
            coordinates that define the neighboorhood of that
            coordinate. For example, Sphere(3) can be used to dilate the
            original mask by 3 voxels. If an int, then it uses
            Sphere(dilate) to dilate the mask. If set to None
            the mask is not dilated.

        Returns
        -------
        msk: numpy.ndarray
            an array with values 1. for values inside the mask
            and values of 0 elsewhere. If the instance has no mask,
            then all values are 1.
        '''

        data_vec = np.zeros((self.nvoxels,), dtype=np.float32)
        if self.mask is None:
            data_vec[:] = 1
        else:
            data_vec[self.mask] = 1


        # see if the mask has to be dilated.
        # if all voxels are already in the mask this can be omitted
        if not dilate is None and \
                    self.nvoxels_mask != self.nvoxels:

            if type(dilate) is int:
                dilate = Sphere(dilate)

            # offsets
            deltas = dilate((0, 0, 0))

            # positions of nonzero voxels
            data_ijks = self.lin2ijk(np.nonzero(data_vec)[0])

            # helper function
            def add_tuple(x, y):
                return (x[0] + y[0], x[1] + y[1], x[2] + y[2])

            # gather all subindices ehre
            dilate_ijk = set()

            # all combinations of offsets and positions of voxels in the mask
            for delta in deltas:
                if delta != (0, 0, 0):
                    for data_ijk in data_ijks:
                        pos = add_tuple(delta, data_ijk)
                        dilate_ijk.add(pos)

            if dilate_ijk:
                dilate_lin = self._ijk2lin_unmasked(list(dilate_ijk))
                lin_mask = self.contains_lin(dilate_lin, apply_mask=False)
                data_vec[dilate_lin[lin_mask]] = 1

        sh = self.shape
        data_t1 = np.reshape(data_vec, sh[:3])

        if not nt is None:
            sh = (sh[0], sh[1], sh[2], nt)
            data = np.zeros(sh, data_vec.dtype)
            for t in xrange(nt):
                data[:, :, :, t] = data_t1
            return data
        else:
            return data_t1

    def get_masked_nifti_image(self, nt=None, dilate=None):
        '''Provides a masked nifti image

        Parameters
        ----------
        nt: int or None
            Number of timepoints (or samples). Each feature has the
            same value (1 if in the mask, 0 otherwise) for each
            sample. If nt is None, then the output is 3D; otherwise
            it is 4D with 'nt' values in the last dimension.
        dilate: callable or int or None
            If a callable, it should be a a neighborhood function
            (like Sphere(..)) that can map a single voxel coordinate
            (represented as a triple of indices) to a list of voxel
            coordinates that define the neighboorhood of that
            coordinate. For example, Sphere(3) can be used to dilate the
            original mask by 3 voxels. If an int, then it uses
            Sphere(dilate) to dilate the mask. If set to None
            the mask is not dilated.

        Returns
        -------
        msk: Nifti1image
            a nifti image with values 1. for values inside the mask
            and values of 0 elsewhere. If the instance has no mask,
            then all values are 1.
        '''

        data = self.get_masked_array(nt=nt, dilate=dilate)
        img = nb.Nifti1Image(data, self.affine)
        return img

def from_any(s, mask_volume=None):
    """Constructs a VolGeom instance from any reasonable type of input.

    Parameters
    ----------
    s : str or VolGeom or nibabel SpatialImage-like or
                mvpa2.datasets.base.Dataset-like with nifti-image header.
        Input to use to construct the VolGeom instance. If s is a string,
        then it is assumed to refer to the file name of a NIFTI image.
    mask_volume: boolean or int or None (default: False)
        If an int is provided, then the mask-volume-th volume in s
        is used as a voxel mask. True is equivalent to 0. If None or
        False are provided, no mask is applied.
        Fmri-dataset-like objects are treated specifally: If s is
        such an object an mask_volume is None, it will automatically use
        s.fa['voxel_indices'] to define the mask (if that attribute is
        present). Alternatively, if mask_volume is a string, then the
        mask is defined based on the voxel indices that are assumed
        to be present s.fa[mask_volume].

    Returns
    -------
    vg: VolGeom
        Volume geometry associated with s.
    """
    if s is None or isinstance(s, VolGeom):
        return s

    if isinstance(s, basestring):
        # try to find a function to load the data
        load_function = None

        if s.endswith('.nii') or s.endswith('.nii.gz'):
            load_function = nb.load
        elif s.endswith('.h5py'):
            if externals.exists('h5py'):
                from mvpa2.base.hdf5 import h5load
                load_function = h5load
            else:
                raise ValueError("Cannot load h5py file - no externals")

        if load_function:
            # do a recursive call
            return from_any(load_function(s), mask_volume=mask_volume)

        raise ValueError("Unrecognized extension for file %s" % s)

    if mask_volume is True:
        # assign a specific index -- the very first volume
        mask_volume = 0

    elif mask_volume is False:
        # do not use a mask
        mask_volume = None

    try:
        # see if s behaves like a spatial image (nifti image)
        shape = s.shape
        affine = s.get_affine()

        if isinstance(mask_volume, int):
            data = s.get_data()
            ndim = len(data.shape)
            if ndim <= 3:
                mask = data
                if mask_volume > 0:
                    warning("There is no 4th dimension (t) to select "
                            "the %d-th volume." % (mask_volume,))
            else:
                mask = data[:, :, :, mask_volume]
        else:
            mask = None
    except:
        try:
            # see if s behaves like a Dataset with image header
            hdr = s.a.imghdr
            try:
                shape = hdr.get_data_shape()
                affine = hdr.get_best_affine()
            except AttributeError:
                # maybe there are shape and voxel dimensions
                shape = s.a.voxel_dim

                # set the affine matrix with origin (0,0,0)
                affine = np.zeros((4, 4))
                affine[0, 0], affine[1, 1], affine[2, 2] = s.a.voxel_eldim
                affine[:2, -1] = -.5 * np.diag(affine)[:2]
                affine[3, 3] = 1

            mask = None
            if isinstance(mask_volume, int):
                mask = np.asarray(s.samples[mask_volume, :])
            else:
                mask_volume_indices = None
                if mask_volume is None and (hasattr(s, 'fa') and
                                           hasattr(s.fa, 'voxel_indices')):
                    mask_volume_indices = s.fa['voxel_indices']
                elif isinstance(mask_volume, basestring):
                    if not mask_volume in s.fa:
                        raise ValueError('Key not found in s.fa: %r' % mask_volume)
                    mask_volume_indices = s.fa[mask_volume]

                if mask_volume_indices:
                    sh = shape[:3]
                    mask = np.zeros(sh)

                    for idx in mask_volume_indices.value:
                        mask[tuple(idx)] = 1
        except:
            #no idea what type of beast this is.
            raise ValueError('Unrecognized input %r - not a VolGeom, '
                             '(filename of) Nifti image, or (mri-)Dataset' % s)

    return VolGeom(shape=shape, affine=affine, mask=mask)


def _to_X_column_array(v, x):
    # TODO: some fancy checking of size/shape of input

    if not isinstance(v, np.ndarray):
        v = np.asarray(v)
    if len(v.shape) == 1:
        if x > 1 and len(v) != x:
            raise ValueError("Cannot cast to %d columns: %r" % x)
        v = v.reshape((-1, x))
    if v.shape[1] != x:
        raise ValueError("Not %dx3" % x)

    return v


def to_three_column_array(v):
    '''Converts input to a Px3 array'''

    return _to_X_column_array(v, 3)

def to_one_column_array(v):
    '''Converts input to a Px1 array'''
    return _to_X_column_array(v, 1)

def to_vector(v):
    '''Converts input to a linear vector'''
    if not isinstance(v, np.ndarray):
        v = np.asarray(v)
    if len(v.shape) > 1:
        if v.shape[0] != 1 and v.shape(1) != 1:
            raise ValueError("Matrix of shape %d x %d: cannot make linear" %
                                        (v.shape[0], v.shape[1]))
        v = v.ravel()
    return v



def distance(p, q, r=2):
    '''Returns the distances between vectors in two arrays

    Parameters
    ----------
    p: np.ndarray (PxM)
        first array
    q: np.ndarray (QxM)
        second array
    nrm: float (default: 2)
        Norm used for distance computation. By default Euclidean distances
        are computed.

    Returns
    -------
    pq: np.ndarray (PxQ)
        Distance between p[j] and q[j] is in pq[i,j]

    Notes
    -----
    If p or q are vectors (one-dimensional) then pq is also a vector
    '''
    ravel = 0

    if len(p.shape) == 1:
        p = np.reshape(p, (1, -1))
        ravel += 1
    if len(q.shape) == 1:
        q = np.reshape(q, (1, -1))
        ravel += 1

    if p.shape[1] != q.shape[1]:
        raise ValueError("Shape mismatch")

    m, n = len(p), len(q)
    ds = np.zeros((m, n), dtype=p.dtype)

    def dist_func(a, b, r):
        delta = a - b
        if np.isinf(r):
            return np.max(np.abs(delta), 1)
        else:
            return np.sum(delta ** r, 1) ** (1. / r)

    for i, pi in enumerate(p):
        ds[i, :] = dist_func(pi, q, r)

    if ravel > 0:
        # we could also return just a single number if
        # ravel==2 but for consistency always return an array
        ds = ds.ravel()
    return ds




########NEW FILE########
__FILENAME__ = volsurf
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##'''
"""
Associate volume geometry with two surface meshes (typically pial and white
matter boundaries of the grey matter).

@author: nick
"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import externals
from mvpa2.misc.surfing import volgeom
from mvpa2.support.nibabel import surf

from mvpa2.base.progress import ProgressBar
from mvpa2.base import debug

if externals.exists('nibabel'):
        import nibabel as nb

class VolSurf(object):
    '''
    Associates a volume geometry with two surfaces (pial and white).
    '''
    def __init__(self, vg, white, pial, intermediate=None):
        '''
        Parameters
        ----------
        volgeom: volgeom.VolGeom
            Volume geometry
        white: surf.Surface
            Surface representing white-grey matter boundary
        pial: surf.Surface
            Surface representing pial-grey matter boundary
        intermediate: surf.Surface (default: None).
            Surface representing intermediate surface. If omitted
            it is the node-wise average of white and pial.
            This parameter is usually ignored, except when used
            in a VolSurfMinimalLowresMapping.

        Notes
        -----
        'pial' and 'white' should have the same topology.
        '''
        self._volgeom = volgeom.from_any(vg)
        self._pial = surf.from_any(pial)
        self._white = surf.from_any(white)

        if not self._pial.same_topology(self._white):
            raise Exception("Not same topology for white and pial")

        #if intermediate is None:
        #    intermediate = (self.pial_surface * .5) + (self.white_surface * .5)
        self._intermediate = surf.from_any(intermediate)


    def __repr__(self, prefixes=[]):
        prefixes_ = ['vg=%r' % self._volgeom,
                     'white=%r' % self._white,
                     'pial=%r' % self._pial] + prefixes
        return "%s(%s)" % (self.__class__.__name__, ', '.join(prefixes_))

    def __str__(self):
        return '%s(volgeom=%s, pial=%s, white=%s)' % (
                                            self.__class__.__name__,
                                            self._volgeom,
                                            self._white,
                                            self._pial)

    @property
    def pial_surface(self):
        '''
        Returns the pial surface

        Returns
        -------
        pial: surf.Surface
        '''
        return self._pial

    @property
    def white_surface(self):
        '''
        Returns the white surface

        Returns
        -------
        white: surf.Surface
        '''

        return self._white

    @property
    def intermediate_surface(self):
        '''
        Returns the node-wise average of the pial and white surface

        Returns
        -------
        intermediate: surf.Surface
        '''
        return self._intermediate

    @property
    def volgeom(self):
        '''
        Returns the volume geometry

        Returns
        -------
        vg: volgeom.VolGeom
        '''

        return self._volgeom

    def __reduce__(self):
        return (self.__class__, (self._volgeom, self._white,
                                 self._pial, self._intermediate))


    def surf_project_nodewise(self, xyz):
        '''
        Projects coordinates on lines connecting pial and white matter.

        Parameters
        ----------
        xyz: numpy.ndarray (float)
            Px3 array with coordinates, assuming 'white' and 'pial'
            surfaces have P nodes each

        Returns
        -------
        xyz_proj: numpy.ndarray (float)
            Px3 array with coordinates the constraints that xyz_proj[i,:]
            lies on the line connecting node 'i' on the white and pial
            surface, and that xyz_proj[i,:] is closest to xyz[i,:] of all
            points on this line.
        '''


        pxyz = self.white_surface.vertices
        weights = self.surf_project_weights_nodewise(xyz)
        return pxyz + np.reshape(weights, (-1, 1)) * pxyz

    def surf_unproject_weights_nodewise(self, weights):
        '''Maps relative positions in grey matter to coordinates

        Parameters
        ----------
        weights: numpy.ndarray (float)
            P values of relative grey matter positions, where 0=white surface
            and 1=pial surface.

        Returns
        -------
        xyz: numpy.ndarray (float)
            Px3 array with coordinates, assuming 'white' and 'pial' surfaces
            have P nodes each.
        '''
        # compute relative to pial_xyz
        pxyz = self._pial.vertices
        qxyz = self._white.vertices

        dxyz = (pxyz - qxyz)  # difference vector


        if len([s for s in weights.shape if s > 1]) != 1:
            raise ValueError("Weights should be a vector, but found "
                             "shape %s" % (weights.shape,))

        weights_lin = np.reshape(weights.ravel(), (-1, 1))
        return qxyz + weights_lin * dxyz


    def surf_project_weights_nodewise(self, xyz):
        '''
        Computes relative position of xyz on lines from pial to white matter.

        Parameters
        ----------
        xyz: numpy.ndarray (float)
            Px3 array with coordinates, assuming 'white' and 'pial' surfaces
            have P nodes each.

        Returns
        -------
        weights: numpy.ndarray (float)
            If nodes is True, P values of relative grey matter positions
            (0=white surface and 1=pial surface), where
            the i-th element are the projection weights for xyz[i] relative
            to the i-th node in the pial and white surface.
            Otherwise it returns an PxQ array with the projected weights
            for each node. If nodes is an int, then a P-vector is returned

        '''
        return self.surf_project_weights(True, xyz)


    def surf_project_weights(self, nodes, xyz):
        '''
        Computes relative position of xyz on lines from pial to white matter.

        Parameters
        ----------
        nodes: True or np.ndarray or int
            Q node indices for each the weights are computed. If True, then
            weights are computed node-wise, otherwise separately for each node.
        xyz: numpy.ndarray (float)
            Px3 array with coordinates. IF nodes is True then the 'white' and
            'pial' surfaces must have P nodes each.

        Returns
        -------
        weights: numpy.ndarray (float)
            If nodes is True, P values of relative grey matter positions
            (0=white surface and 1=pial surface), where
            the i-th element are the projection weights for xyz[i] relative
            to the i-th node in the pial and white surface.
            Otherwise it returns an PxQ array with the projected weights
            for each node. If nodes is an int, then a P-vector is returned

        '''
        node_wise = nodes is True

        if node_wise:
            one_node = True
            nodes = [False]  # placeholder
        else:
            one_node = type(nodes) is int
            nodes = np.asarray(nodes).ravel()
        nnodes = len(nodes)

        nxyz, three = xyz.shape
        if three != 3:
            raise ValueError('Coordinates should be Px3')


        pial = self._pial.vertices
        white = self._white.vertices

        weights = np.zeros((nxyz, nnodes))
        for i, node in enumerate(nodes):
            pxyz = pial if node_wise else pial[node, :]
            qxyz = white if node_wise else white[node, :]

            dxyz = pxyz - qxyz
            ndim = len(dxyz.shape)
            scale = np.sum(dxyz * dxyz, axis=ndim - 1)
            # weights = np.zeros((self._pial.nvertices,), dtype=pxyz.dtype)

            if node_wise:
                nan_mask = scale == 0
                weights[nan_mask, i] = np.nan

                non_nan_mask = np.logical_not(nan_mask)
                ps = (xyz - qxyz)
                proj = np.sum(ps * dxyz, axis=1)

                weights[non_nan_mask, i] = proj[non_nan_mask] / scale[non_nan_mask]
            else:
                if scale == 0:
                    weights[:, i] = np.NAN
                else:
                    ps = (xyz - qxyz)
                    proj = np.sum(ps * dxyz, axis=1)

                    weights[:, i] = proj / scale

        if one_node:
            weights = weights.ravel()

        return weights



    def coordinates_to_grey_distance_mm(self, nodes, xyz):
        '''Computes the grey position of coordinates in metric units

        Parameters
        ----------
        nodes: int or np.ndarray
            Single index, or Q indices of nodes relative to which the
            coordinates are computed. If True then grey distances
            are computed node-wise.
        xyz: Px3 array with coordinates, assuming 'white' and 'pial' surfaces
            have P nodes each.

        Returns
        -------
        grey_position_mm: np.ndarray
            Vector with P elements (if type(nodes) is int) or PxQ array
            (with type(nodes) is np.ndarray) containing the signed 'distance' to the
            grey matter. Values of zero indicate a node is within the grey
            matter. Negative values indicate that a node is 'below' the white
            matter (i.e. farther from the pial surface than the white surface),
            whereas Positive values indicate that a node is 'above' the pial
            matter.
        '''
        node_wise = nodes is True

        if node_wise:
            one_node = True
            all_nodes = [False]  # placeholder
        else:
            one_node = type(nodes) is int
            all_nodes = np.asarray(nodes).ravel()

        nnodes = len(all_nodes)

        nxyz, three = xyz.shape
        if three != 3:
            raise ValueError('Coordinates should be Px3')

        white = self.white_surface.vertices
        pial = self.pial_surface.vertices

        in_white = lambda x:x < 0
        in_pial = lambda x:x > 1

        # compute relative position
        pos = self.surf_project_weights(nodes, xyz)
        ds = np.zeros((nxyz, nnodes))  # space for output

        for i, node in enumerate(all_nodes):
            d = np.zeros(nxyz) + np.nan
            for sgn, s, f in ((-1, white, in_white), (1, pial, in_pial)):
                if node_wise:
                    msk = f(pos)
                    delta = s[msk] - xyz[msk] # difference in coordinates
                else:
                    # mask of voxels outside grey matter
                    msk = f(pos if one_node else pos[:, i])
                    delta = s[node, :] - xyz[msk, :]

                dst = np.sum(delta ** 2, 1) ** .5
                d[msk] = sgn * dst # compute signed distance
            d[np.isnan(d)] = 0
            ds[:, i] = d

        if one_node:
            ds = ds.ravel()

        return ds

class VolSurfMapping(VolSurf):
    '''General mapping between volume and surface.

    Subclasses have to implement node2voxels'''
    def __init__(self, vg, white, pial, intermediate=None,
                   nsteps=10, start_fr=0.0, stop_fr=1.0, start_mm=0, stop_mm=0):
        '''
        Parameters
        ----------
        volgeom: volgeom.VolGeom
            Volume geometry
        white: surf.Surface
            Surface representing white-grey matter boundary
        pial: surf.Surface
            Surface representing pial-grey matter boundary
        intermediate: surf.Surface (default: None).
            Surface representing intermediate surface. If omitted
            it is the node-wise average of white and pial.
        nsteps: int (default: 10)
            Number of steps from white to pial surface
        start_fr: float (default: 0)
            Relative start position of line in gray matter, 0.=white
            surface, 1.=pial surface.
        stop_fr: float (default: 1)
            Relative stop position of line (as in see start).
        start_mm: float (default: 0)
            Absolute start position offset (as in start_fr).
        stop_mm: float (default: 0)
            Absolute start position offset (as in start_fr).


        Notes
        -----
        'pial' and 'white' should have the same topology.
        '''
        super(VolSurfMapping, self).__init__(vg=vg, white=white, pial=pial,
                                             intermediate=intermediate)
        self.nsteps = nsteps
        self.start_fr = start_fr
        self.stop_fr = stop_fr
        self.start_mm = start_mm
        self.stop_mm = stop_mm


    def get_node2voxels_mapping(self):
        '''
        Returns
        -------
        n2v: dict
        A mapping from node indices to voxels. In this mapping, the
        'i'-th node is associated with 'n2v[i]=v2p' which contains the
        mapping from linear voxel indices to grey matter positions. In
        other words, 'n2v[i][idx]=v2p[idx]=pos' means that the voxel with
        linear index 'idx' is associated with node 'i' and has has
        relative position 'pos' in the gray matter.

        If node 'i' is outside the volume, then 'n2v[i]=None'.

        Notes
        -----
        The typical use case is selecting voxels in the grey matter. The
        rationale of this method is that (assuming a sufficient dense cortical
        surface mesh, combined with a sufficient number of nsteps, the grey
        matter is sampled dense enough so that 'no voxels are left out'.

        '''

        raise NotImplementedError

    def voxel_count_nifti_image(self):
        '''
        Returns a NIFTI image indicating how often each voxel is selected.

        Parameters
        ==========
        n2v: dict
            Node to voxel mapping, typically from node2voxels. If omitted
            then the output from node2voxels() is used.

        Returns
        =======
        img: nifti.Nifti1Image
            Image where the value in each voxel indicates how often
            each voxel was selected by n2v.
        '''

        n2v = self.get_node2voxels_mapping()

        v = self._volgeom
        nv = v.nvoxels

        voldata = np.zeros((nv,), dtype=float)

        for vx2d in n2v.itervalues():
            if vx2d:
                for vx in vx2d:
                    voldata[vx] += 1

        rs = np.reshape(voldata, v.shape)
        img = nb.Nifti1Image(rs, v.affine)
        return img

    def _get_node_voxels_maximal_mapping(self):
        '''Internal helper function to return all possible voxels associated
        with each node. Each voxel can be associated with multiple nodes

        It returns a node to voxel mapping and a voxel to node mapping'''

        nsteps = self.nsteps
        start_fr = self.start_fr
        stop_fr = self.stop_fr
        start_mm = self.start_mm
        stop_mm = self.stop_mm

        if start_fr > stop_fr or nsteps < 1:
            raise ValueError("Illegal start/stop combination, "
                             "or not enough steps")

        # make a list of the different relative gray matter positions
        if nsteps > 1:
            step = (stop_fr - start_fr) / float(nsteps - 1)
        else:
            step = 0.
            start_fr = stop_fr = .5

        center_ids = range(self._pial.nvertices)
        nv = len(center_ids)  # number of nodes on the surface

        vg = self._volgeom
        same_surfaces = self.white_surface == self.pial_surface

        surf_start = self.white_surface + start_mm
        surf_stop = self.pial_surface + stop_mm

        # allocate space for output
        # if n2v[i]=vs, then node i is associated with the voxels vs
        #
        # vs is a mapping from indices to relative position in grey matter
        # where 0 means white surface and 1 means pial surface
        # vs[k]=pos means that voxel with linear index k is
        # associated with relative positions pos0
        #
        # CHECKME that I did not confuse (the order of) pial and white surface
        #
        # v2ns is a mapping from voxel indices to sets of nodes.

        n2vs = dict()  # node to voxel indices mapping
        v2ns = dict()

        # by default, no voxels associated with each node
        for j in xrange(nv):
            n2vs[j] = None

        # different 'layers' (depths) in the grey matter
        for i in xrange(nsteps):
            whiteweight = start_fr + step * float(i)  # ensure float
            pialweight = 1 - whiteweight

            # compute weighted intermediate surface in between pial and white
            surf_weighted = surf_stop * pialweight + surf_start * whiteweight

            # coordinates
            surf_xyz = surf_weighted.vertices

            # linear indices of voxels containing nodes
            lin_vox = vg.xyz2lin(surf_xyz)

            # which of these voxels are actually in the volume
            is_vox_in_vol = vg.contains_lin(lin_vox)

            if same_surfaces:
                # prevent division by zero - simply assign it whatever weight is here
                grey_matter_pos = np.zeros(lin_vox.shape) + whiteweight
            else:
                # coordinates of voxels
                vol_xyz = vg.lin2xyz(lin_vox)

                # compute relative position of each voxel in grey matter
                grey_matter_pos = self.surf_project_weights_nodewise(vol_xyz)

            for center_id in center_ids:  # for each node on the surface
                # associate voxels with the present center node.
                # If a node is not in the volume, then no voxels are
                # associated with it.

                if is_vox_in_vol[center_id]:
                    # no voxels (yet) associated with this node - make space
                    if n2vs[center_id] is None:
                        n2vs[center_id] = dict()

                    vox_id = lin_vox[center_id]

                    # node to voxel mapping
                    n2vs[center_id][vox_id] = grey_matter_pos[center_id]

                    # voxel to node mapping
                    if not vox_id in v2ns:
                        v2ns[vox_id] = set()

                    v2ns[vox_id].add(center_id)

        return n2vs, v2ns

    def get_parameter_dict(self):
        '''
        Returns a dictionary with the most important parameters
        of this instance'''

        parameter_dict = dict(volgeom=self.volgeom,
                              volsurf_nvertices=self.white_surface.nvertices,
                              nsteps=self.nsteps,
                              start_fr=self.start_fr, stop_fr=self.stop_fr,
                              start_mm=self.start_mm, stop_mm=self.stop_mm)
        parameter_dict['class'] = self.__class__.__name__
        return parameter_dict



class VolSurfMaximalMapping(VolSurfMapping):
    def __init__(self, vg, white, pial, intermediate=None,
                   nsteps=10, start_fr=0.0,
                   stop_fr=1.0, start_mm=0, stop_mm=0):
        '''
        Represents the maximal mapping from nodes to voxels.
        'maximal', in this context, means that to each node all voxels
        are associated that are contained in lines connecting white
        and grey matter.

        Each voxel can be associated with multiple nodes.

        Parameters
        ----------
        volgeom: volgeom.VolGeom
            Volume geometry
        white: surf.Surface
            Surface representing white-grey matter boundary
        pial: surf.Surface
            Surface representing pial-grey matter boundary
        intermediate: surf.Surface (default: None).
            Surface representing intermediate surface. If omitted
            it is the node-wise average of white and pial.
        nsteps: int (default: 10)
            Number of steps from white to pial surface
        start_fr: float (default: 0)
            Relative start position of line in gray matter, 0.=white
            surface, 1.=pial surface.
        stop_fr: float (default: 1)
            Relative stop position of line (as in see start).
        start_mm: float (default: 0)
            Absolute start position offset (as in start_fr).
        stop_mm: float (default: 0)
            Absolute start position offset (as in start_fr).


        Notes
        -----
        'pial' and 'white' should have the same topology.
        '''

        super(VolSurfMaximalMapping, self).__init__(vg=vg, white=white, pial=pial,
                                intermediate=intermediate, nsteps=nsteps, start_fr=start_fr,
                                stop_fr=stop_fr, start_mm=start_mm, stop_mm=stop_mm)

    def get_node2voxels_mapping(self):
        '''Returns a mapping from nodes to voxels'''
        node2voxels_mapping, _ = self._get_node_voxels_maximal_mapping()
        return node2voxels_mapping


class VolSurfMinimalMapping(VolSurfMapping):
    def __init__(self, vg, white, pial, intermediate=None,
                   nsteps=10, start_fr=0.0,
                   stop_fr=1.0, start_mm=0, stop_mm=0):
        '''
        Represents the minimal mapping from nodes to voxels.
        'minimal', in this context, means that the mapping from
        voxels to nodes is many-to-one (i.e. each voxel is associated
        with at most one node)

        Each voxel can be associated with just a single node.

        Parameters
        ----------
        volgeom: volgeom.VolGeom
            Volume geometry
        white: surf.Surface
            Surface representing white-grey matter boundary
        pial: surf.Surface
            Surface representing pial-grey matter boundary
        intermediate: surf.Surface (default: None).
            Surface representing intermediate surface. If omitted
            it is the node-wise average of white and pial.
        nsteps: int (default: 10)
            Number of steps from white to pial surface
        start_fr: float (default: 0)
            Relative start position of line in gray matter, 0.=white
            surface, 1.=pial surface.
        stop_fr: float (default: 1)
            Relative stop position of line (as in see start).
        start_mm: float (default: 0)
            Absolute start position offset (as in start_fr).
        stop_mm: float (default: 0)
            Absolute start position offset (as in start_fr).

        Notes
        -----
        'pial' and 'white' should have the same topology.
        '''

        super(VolSurfMinimalMapping, self).__init__(vg=vg, white=white, pial=pial,
                                intermediate=intermediate, nsteps=nsteps, start_fr=start_fr,
                                stop_fr=stop_fr, start_mm=start_mm, stop_mm=stop_mm)


    def get_node2voxels_mapping(self):
        # start out with the maximum mapping, then prune it
        n2vs_max, v2ns_max = self._get_node_voxels_maximal_mapping()

        if __debug__ and 'SVS' in debug.active:
            nnodes = len(n2vs_max)
            nvoxels_max = sum(map(len, v2ns_max.itervalues()))
            nvoxels_max_per_node = float(nvoxels_max) / nnodes
            debug('SVS', 'Maximal node-to-voxel mapping: %d nodes, '
                            '%d voxels, %.2f voxels/node' %
                            (nnodes, nvoxels_max, nvoxels_max_per_node))
            debug('SVS', 'Starting injective pruning')


        # initialize mapping
        n2vs_min = dict((n, None if vs is None else dict())
                                    for n, vs in n2vs_max.iteritems())
        v2n_min = dict()

        # helper function to compute distance to intermediate surface
        dist_func = lambda (_, p): abs(p - .5)

        for v, ns in v2ns_max.iteritems():
            # get pairs os nodes and the voxel positions
            ns_pos = [(n, n2vs_max[n].get(v)) for n in ns]

            # get node nearest to intermediate surface
            min_node, min_pos = min(ns_pos, key=dist_func)


            if n2vs_min[min_node] is None:
                n2vs_min[min_node] = dict()
            assert(not v in n2vs_min[min_node]) # no duplicates
            n2vs_min[min_node][v] = min_pos

            assert(not v in v2n_min)
            v2n_min[v] = min_node

        if __debug__ and 'SVS' in debug.active:
            nvoxels_min = len(v2n_min)
            nvoxels_min_per_node = float(nvoxels_min) / nnodes
            nvoxels_delta = nvoxels_max - nvoxels_min
            nvoxels_pruned_ratio = float(nvoxels_delta) / nvoxels_max

            debug('SVS', 'Pruned %d/%d voxels (%.1f%%), %.2f voxels/node'
                             % (nvoxels_delta, nvoxels_max,
                                nvoxels_pruned_ratio * 100,
                                nvoxels_min_per_node))

        return n2vs_min

class VolSurfMinimalLowresMapping(VolSurfMinimalMapping):
    def __init__(self, vg, white, pial, intermediate=None,
                   nsteps=10, start_fr=0.0,
                   stop_fr=1.0, start_mm=0, stop_mm=0):
        '''
        Represents the minimal mapping from nodes to voxels,
        incorporating the intermediate surface that can
        be of lower-res.
        'minimal', in this context, means that the mapping from
        voxels to nodes is many-to-one (i.e. each voxel is associated
        with at most one node). Each node mapped must be
        present in the intermediate surface

        Each voxel can be associated with just a single node.

        Parameters
        ----------
        volgeom: volgeom.VolGeom
            Volume geometry
        white: surf.Surface
            Surface representing white-grey matter boundary
        pial: surf.Surface
            Surface representing pial-grey matter boundary
        intermediate: surf.Surface (default: None).
            Surface representing intermediate surface.
            Unlike in its superclass this argument cannot be ommited here.
        nsteps: int (default: 10)
            Number of steps from white to pial surface
        start_fr: float (default: 0)
            Relative start position of line in gray matter, 0.=white
            surface, 1.=pial surface.
        stop_fr: float (default: 1)
            Relative stop position of line (as in see start).
        start_mm: float (default: 0)
            Absolute start position offset (as in start_fr).
        stop_mm: float (default: 0)
            Absolute start position offset (as in start_fr).

        Notes
        -----
        'pial' and 'white' should have the same topology.
        '''

        if intermediate is None:
            raise RuntimeError("intermediate surface has to be specified")

        super(VolSurfMinimalLowresMapping, self).__init__(vg=vg, white=white,
                pial=pial, intermediate=intermediate, nsteps=nsteps,
                start_fr=start_fr, stop_fr=stop_fr, start_mm=start_mm,
                stop_mm=stop_mm)

    def get_node2voxels_mapping(self):
        n2v = super(VolSurfMinimalLowresMapping, self).\
                                get_node2voxels_mapping()

        # set low and high res intermediate surfaces
        lowres = surf.from_any(self._intermediate)
        highres = (self.pial_surface * .5) + \
                                (self.white_surface * .5)

        high2high_in_low = lowres.vonoroi_map_to_high_resolution_surf(highres)

        n_in_low2v = dict()
        ds = []

        for n, v2pos in n2v.iteritems():
            (n_in_low, d) = high2high_in_low[n]
            if v2pos is None:
                continue

            ds.append(d)


            if not n_in_low in n_in_low2v:
                # not there - just set the dictionary
                n_in_low2v[n_in_low] = v2pos
            else:
                # is there - see if it is none
                cur = n_in_low2v[n_in_low]
                if cur is None and not v2pos is None:
                    # also overwrite (v2pos can also be None, that's fine)
                    n_in_low2v[n_in_low] = v2pos
                elif v2pos is not None:
                    # update
                    for v, pos in v2pos.iteritems():
                        # minimal mapping, so voxel should not be there already
                        assert(not v in n_in_low2v[n_in_low])
                        cur[v] = pos

        if __debug__ and 'SVS' in debug.active:
            ds = np.asarray(ds)
            mu = np.mean(ds)
            n = len(ds)
            s = np.std(ds)

            debug('SVS', 'Reassigned %d nodes by moving %.2f +/- %.2f to low-res',
                        (n, mu, s))



        return n_in_low2v





class VolumeBasedSurface(surf.Surface):
    '''A surface based on a volume, where every voxel is a node.
    It has the empty topology, meaning there are no edges between
    nodes (voxels)

    Use case: provide volume-based searchlight behaviour. In that
    case finding neighbouring nodes is supposed to be faster
    using the circlearound_n2d method.

    XXX make a separate module?'''
    def __init__(self, vg):
        '''
        Parameters
        ----------
        vg: Volgeom.volgeom or str or NiftiImage
            volume to be used as a surface
        '''
        self._vg = volgeom.from_any(vg)

        n = self._vg.nvoxels
        vertices = self._vg.lin2xyz(np.arange(n))
        faces = np.zeros((0, 3), dtype=np.int)

        # call the parent's class constructor
        super(VolumeBasedSurface, self).__init__(vertices, faces, check=False)

    def __repr__(self, prefixes=[]):
        prefixes_ = ['vg=%r' % self._vg] + prefixes
        return "%s(%s)" % (self.__class__.__name__, ', '.join(prefixes_))

    def __str__(self):
        return '%s(volgeom=%s)' % (self.__class__.__name__,
                                     self._vg)

    def __reduce__(self):
        return (self.__class__, (self._vg,))


    def __eq__(self, other):
        if not isinstance(other, VolumeBasedSurface):
            return False
        return self._vg == other._vg

    def circlearound_n2d(self, src, radius, metric='euclidean'):
        shortmetric = metric[0].lower()

        if shortmetric == 'e':
            v = self.vertices

            # make sure src is a 1x3 array
            if type(src) is tuple and len(src) == 3:
                src = np.asarray(src)

            if isinstance(src, np.ndarray):
                if src.shape not in ((1, 3), (3,), (3, 1)):
                    raise ValueError("Illegal shape: should have 3 elements")

                src_coord = src if src.shape == (1, 3) else np.reshape(src, (1, 3))
            else:
                src_coord = np.reshape(v[src, :], (1, 3))

            # ensure it is a float
            src_coord = np.asanyarray(src_coord, dtype=np.float)

            # make a mask around center
            voxel2world = self._vg.affine
            world2voxel = np.linalg.inv(voxel2world)

            nrm = np.linalg.norm(voxel2world, 2)

            max_extent = np.ceil(radius / nrm + 1)

            src_ijk = self._vg.xyz2ijk(src_coord)

            # min and max ijk coordinates
            mn = (src_ijk.ravel() - max_extent).astype(np.int_)
            mx = (src_ijk.ravel() + max_extent).astype(np.int_)


            # set boundaries properly
            mn[mn < 0] = 0

            sh = np.asarray(self._vg.shape[:3])
            mx[mx > sh] = sh[mx > sh]

            msk_ijk = np.zeros(self._vg.shape[:3], np.int)
            msk_ijk[mn[0]:mx[0], mn[1]:mx[1], mn[2]:mx[2]] = 1

            msk_lin = msk_ijk.ravel()

            # indices of voxels around the mask
            idxs = np.nonzero(msk_lin)[0]

            d = volgeom.distance(src_coord, v[idxs])[0, :]

            n = d.size
            node2dist = dict((idxs[i], d[i]) for i in np.arange(n)
                                    if d[i] <= radius)

            return node2dist

        elif shortmetric == 'd':
            return {src:0.}
        else:
            raise ValueError("Illegal metric: %s" % metric)


def from_volume(v):
    '''Makes a pseudo-surface from a volume.
    Each voxels corresponds to a node; there is no topology.
    A use case is mimicking traditional volume-based searchlights

    Parameters
    ----------
    v: str of NiftiImage
        input volume

    Returns
    -------
    s: surf.Surface
        Surface with an equal number as nodes as there are voxels
        in the input volume. The associated topology is empty.
    '''
    vg = volgeom.from_any(v)
    vs = VolumeBasedSurface(vg)

    return VolSurfMaximalMapping(vg, vs, vs, vs)




########NEW FILE########
__FILENAME__ = volume_mask_dict
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Dictionary (mapping) for storing several volume masks.

A typical use case is storing results from (surface-based)
voxel selection.

@author: nick

WiP.
TODO: make target to sources dictionary computations 'lazy'-only
compute the first time this is asked. XXX How to deal with __setstate__
and __getstate__ - have to flag somehow whether this mapping was present.
"""

__docformat__ = 'restructuredtext'

from collections import Mapping

import numpy as np

from mvpa2.base import externals
from mvpa2.misc.surfing import volgeom

from mvpa2.support.utils import deprecated

if externals.exists('nibabel'):
    import nibabel as nb
if externals.exists('h5py'):
    from mvpa2.base.hdf5 import h5save, h5load


class VolumeMaskDictionary(Mapping):
    """Collection of 3D volume masks, indexed by integers or strings.

    Voxels in a mask are represented sparsely, that is by linear indices in
    the [0, n-1] range, where n is the number of voxels in the volume.

    A typical use case is storing voxel selection results, which can
    subsequently be used for running searchlights. An alternative
    use case is storing a set of regions-of-interest masks.

    Because this class extends Mapping, it can be indexed as any other
    mapping (like dicts). Currently masks cannot be removed, however, and
    adding masks is performed through the add(...) function rather than
    item assignment.

    In some functions the terminology 'source' and 'target' is used. A source
    refers to the label associated with masks (and can be used as a key
    in the mapping), while a target is an element from the value associated
    in this mapping (i.e. typically a target is a linear voxel index).

     Besides storing voxel indices, each mask can also have additional
    'auxiliary' information associated, such as distance from the center
    or its relative position in grey matter.
    """
    # TODO: docs for src2nbr and src2aux
    def __init__(self, vg, source, meta=None, src2nbr=None, src2aux=None):
        """
        Parameters
        ----------
        vg: volgeom.VolGeom or fmri_dataset-like or str
            data structure that contains volume geometry information.
        source: Surface.surf or numpy.ndarray or None
            structure that contains the geometric information of
            (the centers of) each mask. In the case of surface-searchlights this
            should be a surface used as the center for searchlights.
        meta: dict or None
            Optional meta data stored with this instance (such as searchlight
            radius and volumetric information). A use case is storing an instance
            and loading it later, and then checking whether the meta information
            is correct when it used to run a searchlight analysis.
        src2nbr: dict or None
            In a typical use case it contains a mapping from node center
            indices to lists of voxel indices.
        src2aux: dict or None
            In a typical use case it can contain auxiliary information such as
            distance of each voxel to each center.
        """
        self._volgeom = volgeom.from_any(vg)
        self._source = source


        self._src2nbr = dict() if src2nbr is None else src2nbr
        self._src2aux = dict() if src2nbr is None else src2aux

        self._meta = meta

        # this attribute is initially set to None
        # upon the first call that requires an inverse mapping
        # it is generated.
        self._lazy_nbr2src = None

    def __repr__(self, prefixes=[]):
        prefixes_ = ['vg=%r' % self._volgeom,
                    'source=%r' % self._source] + prefixes

        if not self._meta is None:
            prefixes_.append('meta=%r' % self._meta)

        if not self._src2nbr is None:
            prefixes_.append('src2nbr=%r' % self._src2nbr)
        if not self._src2aux is None:
            prefixes_.append('src2aux=%r' % self._src2aux)

        return "%s(%s)" % (self.__class__.__name__, ','.join(prefixes_))

    def __str__(self):
        return '%s(%d centers, volgeom=%s)' % (self.__class__.__name__,
                                               len(self._src2nbr),
                                               self._volgeom)

    def add(self, src, nbrs, aux=None):
        """Add a volume mask

        Parameters
        ----------
        src: int or str
            index or name of volume mask. src should not be already
            present in this dictionary
        nbrs: list of int
            linear voxel indices of the voxels in the mask
        aux: dict or None
            auxiliary properties associated with (the voxels in) the volume
            mask. If the current dictionary instance alraedy has stored
            auxiliary properties for other masks, then the set of keys in
            the current mask should be the same as for other masks. In
            addition, the length of each value in aux should be either
            the number of elements in nbrs or one.
        """

        if not type(src) in [int, basestring]:
            # for now to avoid unhasbable type
            raise TypeError("src should be int or str")

        if src in self._src2nbr:
            raise ValueError('%s already in %s' % (src, self))


        self._src2nbr[src] = np.asarray(nbrs, dtype=np.int)

        if not self._lazy_nbr2src is None:
            self._add_target2source(src)

        if aux:
            n = len(nbrs)
            expected_keys = set(self.aux_keys())
            if expected_keys and (set(aux) != expected_keys):
                raise ValueError("aux label mismatch: %s != %s" %
                                (set(aux), expected_keys))
            for k, v in aux.iteritems():
                if not k in self._src2aux:
                    self._src2aux[k] = dict()

                # ensure that values have the same datatype for different keys
                if len(self._src2aux[k]) == 0:
                    v_dtype = None
                else:
                    v_dtype = next(self._src2aux[k].itervalues()).dtype

                if isinstance(v, (list, tuple, int, float, np.ndarray)):
                    v_arr = np.asanyarray(v, dtype=v_dtype).ravel()
                else:
                    raise ValueError('illegal type %s for %s' % (type(v), v))

                if len(v_arr) not in (n, 1):
                    raise ValueError('size mismatch: size %d != %d or 1' %
                                        (len(v_arr), n))

                self._src2aux[k][src] = v_arr



    def get_tuple_list(self, src, *labels):
        """Return a list of tuples with mask indices and/or aux information.

        Parameters
        ----------
        src: int or str
            index of mask
        *labels: str or None
            List of labels to return. None refers to the voxel indices of the
            mask.

        Returns
        -------
        tuples: list of tuple
            N tuples each with len(labels) values, where N is the number of
            voxels in the mask indexed by src
        """
        idxs = self[src]
        n = len(idxs)

        tuple_lists = []
        for label in labels:
            if label is None:
                tuple_list_elem = idxs.tolist()
            else:
                vs = self.get_aux(src, label)
                if len(vs) == 1:
                    tuple_list_elem = [vs[0]] * n
                else:
                    tuple_list_elem = vs.tolist()

            tuple_lists.append(tuple_list_elem)

        return zip(*tuple_lists)

    @deprecated("use .get_tuple_list instead")
    def get_tuple_list_dict(self, *labels):
        """Return a dictionary of mapping that maps each mask index
        to tuples with mask indices and/or auxiliary
        information.

        Parameters
        ----------
        *labels: str or None
            List of labels to return. None refers to the voxel indices of the
            mask.

        tuple_dict: dict
            a mapping so that
            get_tuple_list(s, labels)==get_tuple_list_dict(labels)[s]
        """
        d = dict()
        for src in self.keys():
            d[src] = self.get_tuple_list(src, *labels)
        return d

    # XXX:  should not be logically 'get_indices'?
    #       while 'def get' could just return the mask?
    # YYY:  it's primary purpose is to store a mapping from
    #       node indices to lists of voxel indices. In that use case
    #       it would make sense to keep it as a mapping.
    #
    # XXX:  It overloads original Mapping.get which also
    #       had default... should it be the same here may be?
    # YYY:  I don't see any solution with a reasonable default.
    def get(self, src):
        """Return the linear voxel indices of a mask

        Parameters
        ----------
        src: int
            index of mask

        Returns
        -------
        idxs: list of int
            linear voxel indices indexed by src
        """
        return self._src2nbr[src].tolist()

    @deprecated("use .get_aux instead")
    def aux_get(self, src, label):
        return self.get_aux(src, label)

    def get_aux(self, src, label):
        '''Auxiliary information of a mask

        Parameters
        ----------
        src: int
            index of mask
        label: str
            label of auxiliary information

        Returns
        -------
        vals: list
            auxiliary information labelled label for mask src
        '''
        labels = self.aux_keys()
        if not label in labels:
            raise ValueError("%s not in %r" % (label, labels))
        if not label in self._src2aux:
            msg = ("Mismatch for key %r" if src in self._src2nbr
                                        else "Unknown key %r")
            raise ValueError((msg + ', label %r') % (src, label))
        return self._src2aux[label][src].tolist()

    # XXX:  get_aux_labels?
    # YYY:  aux is also a dictionary (actually a dictionary with dictionaries)
    #       so it seems more logical to use 'keys' instead of 'labels'.
    def aux_keys(self):
        '''Names of auxiliary labels

        Returns
        -------
        keys: list of str
            Names of auxiliary labels that are supported by get_aux
        '''
        return self._src2aux.keys()

    def _ensure_has_target2sources(self):
        '''Helper function to ensure that inverse mapping is set properly'''
        if not self._lazy_nbr2src:
            self._lazy_nbr2src = dict()
            for src in self.keys():
                self._add_target2source(src)

    def _add_target2source(self, src, targets=None):
        if targets is None:
            targets = self[src]

        contains = self.volgeom.contains_lin(np.asarray(targets))
        for i, target in enumerate(targets):
            if not contains[i]:
                raise ValueError("Target not in volume: %s" % target)
            if not target in self._lazy_nbr2src:
                self._lazy_nbr2src[target] = set()
            self._lazy_nbr2src[target].add(src)


    def target2sources(self, nbr):
        """Find the indices of masks that map to a linear voxel index

        Parameters
        ----------
        nbr: int
            Linear voxel index

        Returns
        -------
        srcs: list of int
            Indices i for which get(i) contains nbr
        """
        if type(nbr) in (list, tuple):
            return map(self.target2sources, nbr)

        self._ensure_has_target2sources()

        if not nbr in self._lazy_nbr2src:
            return None

        return self._lazy_nbr2src[nbr]

    def get_targets(self):
        """Return list of voxels that are in one or more masks

        Returns
        -------
        idxs: list of int
            Linear indices of voxels in one or more masks
        """
        self._ensure_has_target2sources()

        return sorted(self._lazy_nbr2src.keys())

    def get_mask(self):
        """Return a mask for voxels that are included in one or more masks

        Returns
        -------
        msk: np.ndarray
            Three-dimensional array with True for voxels that are
            included in one or more masks, and False elsewhere
        """

        self._ensure_has_target2sources()
        m_lin = np.zeros((self.volgeom.nvoxels, 1), dtype=np.int8)
        for nbr in self._lazy_nbr2src.iterkeys():
            m_lin[nbr] = 1

        return np.reshape(m_lin, self.volgeom.shape[:3])

    def get_nifti_image_mask(self):
        """Return a NIfTI image with the voxels included in any mask

        Returns
        -------
        msk: nibabel.Nifti1Image
            Nifti image where voxels have the value 1 for voxels that are
            included in one or more masks, and 0 elsewhere
        """
        # XXX:  my above dtype=np.int8 might kick back here, e.g.
        #       fslview iirc had difficulty with those
        # YYY:  should we change to a different data type, e.g. int32?
        return nb.Nifti1Image(self.get_mask(), self.volgeom.affine)


    def get_voxel_indices(self):
        """Returns voxel indices at least once selected

        Returns
        -------
        voxel_indices: list of tuple
            List of triples with sub-voxel indices that were selected
            at least once since the initalization of this class.
            That is, a triple (i,j,k) referring to a voxel V is an element
            of voxel_indices iff there is at least one key k so that
            self.get(k) contains the linear index of voxel V.
        """
        # get linear voxel indices
        lin_vox_set = set.union(*(set(self[k]) for k in self.keys()))

        # convert to array
        lin_vox_arr = np.asarray(list(lin_vox_set))

        return map(tuple, self.volgeom.lin2ijk(lin_vox_arr))

    def get_dataset_feature_mask(self, ds):
        """For a dataset return a mask of features that were selected
        at least once

        Parameters
        ----------
        ds: Dataset
            A dataset with field .fa.voxel_indices

        Returns
        -------
        mask: np.ndarray (boolean)
            binary mask with ds.nfeatures values, with True for features that
            were selected at least once once since the initalization of this class.
            That is, for a voxel (feature) with feature index i it holds that
            mask[I] is True iff there is at least one key k so that
            i in self.get(k).

        """
        # convert to tuples
        ds_voxel_indices = map(tuple, ds.fa.voxel_indices)
        sel_voxel_indices = map(tuple, self.get_voxel_indices())

        set_ds_voxel_indices = set(ds_voxel_indices)
        set_sel_voxel_indices = set(sel_voxel_indices)

        not_in_ds = set_sel_voxel_indices - set_ds_voxel_indices
        if not_in_ds:
            raise ValueError('Found %d voxel indices selected that were '
                             'not in dataset, first one is %s' %
                                (len(not_in_ds), not_in_ds.pop()))

        return np.asarray([d in sel_voxel_indices for d in ds_voxel_indices])

    def get_minimal_dataset(self, ds):
        """For a dataset return only portion with features which were selected

        Parameters
        ----------
        ds: Dataset
            A dataset with field .fa.voxel_indices

        Returns
        -------
        Dataset
            A dataset containing features that were selected at least once

        Notes
        -----
        The rationale of this function is that voxel selection can be run
        first (without using a mask for a dataset), then the dataset
        can be reduced to contain only voxels that were selected by
        voxel selection
        """

        ds_mask = self.get_dataset_feature_mask(ds)
        return ds[:, ds_mask]

    def __getitem__(self, key):
        return self.get(key)

    def __len__(self):
        return len(self.__keys__())

    def __keys__(self):
        return self._src2nbr.keys()

    def __iter__(self):
        return iter(self.__keys__())

    def __reduce__(self):
        return (self.__class__,
                (self._volgeom, self._source),
                self.__getstate__())

    @deprecated("should be used for testing compatibility only - "
                            "otherwise use .__reduce__ instead")
    def _reduce_legacy(self):
        return (self.__class__,
                (self._volgeom, self._source),
                self._getstate_legacy())


    def _getstate(self):
        s = (self._volgeom, self._source, self._meta, self._src2nbr, self._src2aux)
        return s

    @deprecated("should be used for testing compatibility only - "
                            "otherwise use ._getstate instead")
    def _getstate_legacy(self):
        return self._getstate()

    def __getstate__(self):
        # Note: due to issues with saving self_nbr2src, it is not returned
        # as part of the current state. instead it is derived when
        #  __setstate__ is called.

        # new as of Dec 2013: support more efficient storage method for
        # h5save/load

        s = self._getstate()
        # because of h5py issues when assigning values in the tuple s,
        # here a new tuple is created
        s3 = _dict_with_arrays2array_tuple(s[3])
        s4 = _dict_with_arrays2array_tuple(s[4])
        ss = s[:3] + (s3, s4)

        return ss


    def _setstate(self, s):
        # helper function that actually sets the state
        # it can be called by either __setstate__ or _setstate_legacy.
        # the rationale for a separate function is that it allows for
        # setting the state either with the up-to-date or the legacy method

        if len(s) == 4:
            # computatibilty thing: previous version (before Sep 12, 2013) did
            # not store meta
            self._volgeom, self._source, self._src2nbr, self._src2aux = s
            self._meta = False # signal that it is old (no meta information)
            warning('Using old (pre-12 Sep 2013) mapping - no meta data')
        else:
            self._volgeom, self._source, self._meta, self._src2nbr, self._src2aux = s


    @deprecated("should be used for testing compatibility only - "
                            "otherwise use ._setstate instead")
    def _setstate_legacy(self, s):
        # helper function to set the state as it was done prior to Dec 2013.
        # this function is defined separately so that unit tests can
        # override the __setstate__ method by this method and check for
        # compatibility
        self._setstate(s)


    def __setstate__(self, s):
        self._setstate(s)
        # new as of Dec 2013: use more efficient storage method for h5save/load
        self._src2nbr = _array_tuple2dict_with_arrays(self._src2nbr)
        self._src2aux = _array_tuple2dict_with_arrays(self._src2aux)


    def __eq__(self, other):
        """Compares this instance with another instance

        Parameters
        ----------
        other: VolumeMaskDictionary
            instance with which the current instance is compared.

        Returns
        -------
        eq: bool
            True iff the current instance has the same layout as other,
            the same keys, and matching masks.
            This function does *not* consider auxiliary properties.
        """
        if not self.is_same_layout(other):
            return False

        if set(self.keys()) != set(other.keys()):
            return False

        for k in self.keys():
            if self[k] != other[k]:
                return False

        if set(self.aux_keys()) != set(other.aux_keys()):
            return False

        for lab in self.aux_keys():
            for k in self.keys():
                if self.get_aux(k, lab) != other.get_aux(k, lab):
                    return False

        if self.meta != False or other.meta != False:
            # both are 'new' ones with
            if self.meta != other.meta:
                return False

        return True

    @property
    def meta(self):
        # make a copy if a dict, otherwise return directly
        return dict(self._meta) if type(self._meta) is dict else self._meta

    def is_same_layout(self, other):
        '''Check whether another instance has the same spatial layout

        Parameters
        ----------
        other: VolumeMaskDictionary
            the instance that is compared to the current instance

        Returns
        -------
        same: boolean
            True iff the other instance has the same volume geometry
            and source as the current instance
        '''
        if not isinstance(other, self.__class__):
            return False

        return self.volgeom == other.volgeom and self.source == other.source

    # XXX:  shouldn't it be 'update'  mimicing dict.update?
    # YYY:  'update' does not raise an error if the key to be added is
    #       is already present; this method does.
    def merge(self, other):
        """Add masks from another instance

        Parameters
        ----------
        other: VolumeMaskDictionary
            The instance from which masks are added to the current one. The
            keys in the current and other instance should be disjoint, and
            auxiliary properties (if present) should have the same labels.
        """

        if not self.is_same_layout(other):
            raise ValueError("Cannot merge %s with %s" % (self, other))

        if not other:
            # nothing to add, so we're done
            return

        aks = self.aux_keys()
        if set(aks) != set(other.aux_keys()):
            if len(self.keys()) == 0:
                # current instance is empty, so use the keys from
                # the other (necessarily non-empty because of the check above)
                # instance
                aks = other.aux_keys()
            else:
                raise ValueError('Different keys in merge: %s != %s' %
                                (aks, other.aux_keys()))

# TODO: optimization in case either one or both already have the
#       inverse mapping from voxels to nodes
#       For now simply set everything to empty.
#        if self._lazy_nbr2src is None and not other._lazy_nbr2src is None:
#            self._ensure_has_target2sources()
#        elif other._lazy_nbr2src is None and not self._lazy_nbr2src is None:
#            other._ensure_has_target2sources()
#        elif not (other._lazy_nbr2src is None or self._lazy_nbr2src is None):
#            for k, vs in other._lazy_nbr2src.iteritems():
#                self._add_target2source(k, vs)

        self._lazy_nbr2src = None

        for k in other.keys():
            idxs = other[k]

            a_dict = dict()
            for ak in aks:
                a_dict[ak] = other.get_aux(k, ak)

            if not a_dict:
                a_dict = None

            self.add(k, idxs, a_dict)

    def xyz_target(self, ts=None):
        """Compute the x,y,z coordinates of one or more voxels

        Parameters
        ----------
        ts: list of int or None
            list of voxels for which coordinates should be computed. If
            ts is None, then coordinates for all voxels that are mapped
            are computed

        Returns
        -------
        xyz: numpy.ndarray
            Array with size len(ts) x 3 with x,y,z coordinates
        """

        if ts is None:
            ts = list(self.get_targets())
        t_arr = np.reshape(np.asarray(ts), (-1,))
        return self.volgeom.lin2xyz(t_arr)

    def xyz_source(self, ss=None):
        """Computes the x,y,z coordinates of one or more mask centers

        Parameters
        ----------
        ss: list of int or None
            list of mask center indices for which coordinates should be
            computed. If ss is None, then coordinates for all masks that
            are mapped are computed.
            If is required that when the current instance was initialized,
            the source-argument was either a surf.Surface or a numpy.ndarray.
        """

        # TODO add dataset and volgeom support
        coordinate_labels = [None, 'vertices', 'coordinates']
        coordinates = None
        for coordinate_label in coordinate_labels:
            s = self.source
            if coordinate_label and hasattr(s, coordinate_label):
                s = getattr(s, coordinate_label)
            if isinstance(s, np.ndarray):
                coordinates = s

        if coordinates is None:
            raise ValueError("Cannot find coordinates in %r" % self.source)

        if ss is None:
            ss = self.keys()
        if not isinstance(ss, np.ndarray):
            if type(ss) is int:
                ss = [ss]
            ss = np.asarray(list(ss)).ravel()

        return coordinates[ss]

    @property
    def volgeom(self):
        """Volume geometry information."""
        return self._volgeom

    @property
    def source(self):
        """Geometric information of (the centers of) each mask.

        In the case of surface-searchlights this
        should be a surface used as the center for searchlights.
        """
        return self._source

    def target2nearest_source(self, target, fallback_euclidean_distance=False):
        """Find the voxel nearest to a mask center

        Parameters
        ==========
        target: int
            linear index of a voxel
        fallback_euclidean_distance: bool (default: False)
            Whether to use a euclidean distance metric if target is not in
            any of the masks in this instance

        Returns
        =======
        src: int
            key index for the mask that contains target and is nearest to
            target. If target is not contained in any mask, then None is
            returned if fallback_euclidean_distance is False, and the
            index of the source nearest to target using a Euclidean distance
            metric is returned if fallback_euclidean_distance is True
        """
        targets = []
        if type(target) in (list, tuple):
            for t in target:
                targets.append(t)
        else:
            targets = [target]

        xyz_trg = self.xyz_target(np.asarray(targets))

        src = self.target2sources(targets)
        flat_srcs = []
        for s in src:
            if s:
                for j in s:
                    flat_srcs.append(j)

        if not flat_srcs:
            if fallback_euclidean_distance:
                flat_srcs = self.keys()
            else:
                return None



        xyz_srcs = self.xyz_source(flat_srcs)
        d = volgeom.distance(xyz_srcs, xyz_trg)
        i = np.argmin(d)
        # d is a 2D array, get the row number with the lowest d
        source = flat_srcs[i // xyz_trg.shape[0]]

        return source

    def source2nearest_target(self, source):
        """Find the voxel nearest to a mask center

        Parameters
        ==========
        src: int
            mask index

        Returns
        =======
        target: int
            linear index of the voxel that is contained in the mask associated
            with src and nearest (Euclidean distance) to src.
        """

        trgs = self.__getitem__(source)
        trg_xyz = self.xyz_target(trgs)

        src_xyz = self.xyz_source(source)
        d = volgeom.distance(trg_xyz, src_xyz)
        i = np.argmin(d)

        return trgs[i / src_xyz.shape[0]]


def _dict_with_arrays2array_tuple(d):
    '''
    Helper function to convert canonical representation of _src2nbr and
    _src2aux as dicts to a more efficient tuple-based representation.

    Input: dict d where for each key k, each value v[k] is a numpy array
    Output: a tuple (keys, lengths, data) with each element a numpu array

    It holds that:
    - keys.tolist()==d.keys()
    - if k is the i-th element in d.keys(), then
          v[k]==data[offset+lengths[i]] where offset=np.sum(lenghts[:i])
    '''

    if d is None:
        return None
    if all(type(v) is dict for v in d.values()):
        # probably src2aux, so run recursively
        return dict((k, _dict_with_arrays2array_tuple(v))
                        for k, v in d.iteritems())

    keys = np.asarray(d.keys())

    lengths = np.asarray([len(d[key]) for key in keys])

    ntotal = np.sum(lengths)
    data = None # in case there are no keys
    pos = 0

    def _same_dtype(p, q):
        # helper function that returns whether p and q are of the same subtype.
        # this function returns True if, for example:
        #    p.dtype==np.float32 and q.dtype==np.float64
        pt, qt = pqt = p.dtype, q.dtype
        return pt == qt or np.issubdtype(pt, qt)


    for i, (key, length) in enumerate(zip(keys, lengths)):
        v = d[key]

        if i == 0:
            # allocate space for all data in d
            data = np.zeros((ntotal,), dtype=v.dtype)
        elif v.dtype != data.dtype:
            # ensure all values in the dict have the same datatype
            raise ValueError('Type mismatch for keys %s and %s: %s != %s' %
                                (keys[0], key, data.dtype, v.dtype))

        idxs = np.arange(length) + pos
        data[idxs] = v
        pos += length

    return keys, lengths, data


def _array_tuple2dict_with_arrays(kld):
        '''
        Helper function to convert a more efficient tuple-based representation
        of _src2nbr and _src2aux to canonical dicts

        Input: a tuple (keys, lengths, data) with each element a numpu array
        Output: dict d where for each key k, each value v[k] is a numpy array

        It holds that:
        - keys.tolist()==d.keys()
        - if k is the i-th element in d.keys(), then
              v[k]==data[offset+lengths[i]] where offset=np.sum(lenghts[:i])

        '''
        if kld is None:
            return None
        if type(kld) is dict:
            if all(type(v) in (tuple, dict) for v in kld.values()):
                # probably src2aux, so run recursively
                return dict((k, _array_tuple2dict_with_arrays(v))
                            for k, v in kld.iteritems())
            elif all(isinstance(v, np.ndarray) for v in kld.values()):
                # old-style mapping
                return kld
            else:
                raise ValueError('Unrecognized dict: %s' % kld)

        keys, lengths, data = kld

        # keys must be python int or str, not numpy int or str
        keys = keys.tolist()

        # space for output
        d = dict()

        pos = 0
        for key, length in zip(keys, lengths):
            d[key] = data[pos + np.arange(length)]
            pos += length

        if pos != data.size:
            raise ValueError('data size mismatch: expected %s, found %s' %
                                    (data.size, pos))
        return d


def from_any(s):
    """Load (if a string) or just return voxel selection

    Parameters
    ----------
    s: basestring or volume_mask_dict.VolumeMaskDictionary
        if a string it is assumed to be a file name and loaded using h5load. If
        a volume_mask_dict.VolumeMaskDictionary then it is returned.

    Returns
    -------
    r: volume_mask_dict.VolumeMaskDictionary
    """
    if isinstance(s, basestring):
        vs = h5load(s)
        return from_any(vs)
    elif isinstance(s, VolumeMaskDictionary):
        return s
    else:
        raise ValueError("Unknown type %s" % (type(s)))

########NEW FILE########
__FILENAME__ = transformers
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Simply functors that transform something."""

_DEV_DOC = """
Follow the convetion that functions start with lower case, and classes
with uppercase letter.
"""

__docformat__ = 'restructuredtext'


import numpy as np

from mvpa2.base import externals, warning
from mvpa2.base.state import ConditionalAttribute, ClassWithCollections

if __debug__:
    from mvpa2.base import debug


def Absolute(x):
    """
    Returns the elementwise absolute of any argument.

    Parameters
    ----------
    x : scalar or sequence

    """
    return np.absolute(x)


##REF: Name was automagically refactored
def one_minus(x):
    """Returns elementwise '1 - x' of any argument."""
    return 1 - x


def Identity(x):
    """Return whatever it was called with."""
    return x


##REF: Name was automagically refactored
def first_axis_mean(x):
    """Mean computed along the first axis."""
    return np.mean(x, axis=0)

##REF: Name was automagically refactored
def first_axis_sum_not_zero(x):
    """Sum computed over first axis of whether the values are not
    equal to zero."""
    return (np.asarray(x)!=0).sum(axis=0)


##REF: Name was automagically refactored
def second_axis_mean(x):
    """Mean across 2nd axis

    Use cases:
     - to combine multiple sensitivities to get sense about
       mean sensitivity across splits
    """
    return np.mean(x, axis=1)


def sum_of_abs(x):
    """Sum of absolute values along the 2nd axis

    Use cases:
     - to combine multiple sensitivities to get sense about
       what features are most influential
    """
    return np.abs(x).sum()


def max_of_abs(x):
    """Max of absolute values along the 2nd axis
    """
    return np.abs(x).max()


##REF: Name was automagically refactored
def grand_mean(x):
    """Just what the name suggests."""
    return np.mean(x)


##REF: Name was automagically refactored
def l2_normed(x, norm=1.0, reverse=False):
    """Norm the values so that regular vector norm becomes `norm`

    More verbose: Norm that the sum of the squared elements of the
    returned vector becomes `norm`.
    """
    xnorm = np.linalg.norm(x)
    return x * (norm/xnorm)

##REF: Name was automagically refactored
def l1_normed(x, norm=1.0, reverse=False):
    """Norm the values so that L_1 norm (sum|x|) becomes `norm`"""
    xnorm = np.sum(np.abs(x))
    return x * (norm/xnorm)


##REF: Name was automagically refactored
def rank_order(x, reverse=False):
    """Rank-order by value. Highest gets 0"""

    # XXX was Yarik on drugs? please simplify this beast
    nelements = len(x)
    indexes = np.arange(nelements)
    t_indexes = indexes
    if not reverse:
        t_indexes = indexes[::-1]
    tosort = zip(x, indexes)
    tosort.sort()
    ztosort = zip(tosort, t_indexes)
    rankorder = np.empty(nelements, dtype=int)
    rankorder[ [x[0][1] for x in ztosort] ] = \
               [x[1] for x in ztosort]
    return rankorder


##REF: Name was automagically refactored
def reverse_rank_order(x):
    """Convinience functor"""
    return rank_order(x, reverse=True)


class OverAxis(object):
    """Helper to apply transformer over specific axis
    """

    def __init__(self, transformer, axis=None):
        """Initialize transformer wrapper with an axis.

        Parameters
        ----------
        transformer
          A callable to be used
        axis : None or int
          If None -- apply transformer across all the data. If some
          int -- over that axis
        """
        self.transformer = transformer
        # sanity check
        if not (axis is None or isinstance(axis, int)):
            raise ValueError, "axis must be specified by integer"
        self.axis = axis


    def __call__(self, x, *args, **kwargs):
        transformer = self.transformer
        axis = self.axis
        if axis is None:
            return transformer(x, *args, **kwargs)

        x = np.asanyarray(x)
        shape = x.shape
        if axis >= len(shape):
            raise ValueError, "Axis given in constructor %d is higher " \
                  "than dimensionality of the data of shape %s" % (axis, shape)

        # WRONG! ;-)
        #for ind in xrange(shape[axis]):
        #    results.append(transformer(x.take([ind], axis=axis),
        #                              *args, **kwargs))

        # TODO: more elegant/speedy solution?
        shape_sweep = shape[:axis] + shape[axis+1:]
        shrinker = None
        """Either transformer reduces the dimensionality of the data"""
        #results = np.empty(shape_out, dtype=x.dtype)
        for index_sweep in np.ndindex(shape_sweep):
            if axis > 0:
                index = index_sweep[:axis]
            else:
                index = ()
            index = index + (slice(None),) + index_sweep[axis:]
            x_sel = x[index]
            x_t = transformer(x_sel, *args, **kwargs)
            if shrinker is None:
                if np.isscalar(x_t) or x_t.shape == shape_sweep:
                    results = np.empty(shape_sweep, dtype=x.dtype)
                    shrinker = True
                elif x_t.shape == x_sel.shape:
                    results = np.empty(x.shape, dtype=x.dtype)
                    shrinker = False
                else:
                    raise RuntimeError, 'Not handled by OverAxis kind of transformer'

            if shrinker:
                results[index_sweep] = x_t
            else:
                results[index] = x_t

        return results


class DistPValue(ClassWithCollections):
    """Converts values into p-values under vague and non-scientific assumptions
    """

    nulldist_number = ConditionalAttribute(enabled=True,
        doc="Number of features within the estimated null-distribution")

    positives_recovered = ConditionalAttribute(enabled=True,
        doc="Number of features considered to be positives and which were recovered")


    def __init__(self, sd=0, distribution='rdist', fpp=None, nbins=400, **kwargs):
        """L2-Norm the values, convert them to p-values of a given distribution.

        Parameters
        ----------
        sd : int
          Samples dimension (if len(x.shape)>1) on which to operate
        distribution : string
          Which distribution to use. Known are: 'rdist' (later normal should
          be there as well)
        fpp : float
          At what p-value (both tails) if not None, to control for false
          positives. It would iteratively prune the tails (tentative real positives)
          until empirical p-value becomes less or equal to numerical.
        nbins : int
          Number of bins for the iterative pruning of positives

        WARNING: Highly experimental/slow/etc: no theoretical grounds have been
        presented in any paper, nor proven
        """
        externals.exists('scipy', raise_=True)
        ClassWithCollections.__init__(self, **kwargs)

        self.sd = sd
        if not (distribution in ['rdist']):
            raise ValueError, "Actually only rdist supported at the moment" \
                  " got %s" % distribution
        self.distribution = distribution
        self.fpp = fpp
        self.nbins = nbins


    def __call__(self, x):
        from mvpa2.support.scipy.stats import scipy
        import scipy.stats as stats

        # some local bindings
        distribution = self.distribution
        sd = self.sd
        fpp = self.fpp
        nbins = self.nbins

        x = np.asanyarray(x)
        shape_orig = x.shape
        ndims = len(shape_orig)

        # (very) old numpy had different format of returned bins --
        # there were not edges but center points.  We care here about
        # center points, so we will transform edge points into center
        # points for newer versions of numpy
        numpy_center_points = externals.versions['numpy'] < (1, 1)

        # XXX May be just utilize OverAxis transformer?
        if ndims > 2:
            raise NotImplementedError, \
                  "TODO: add support for more than 2 dimensions"
        elif ndims == 1:
            x, sd = x[:, np.newaxis], 0

        # lets transpose for convenience
        if sd == 0: x = x.T

        # Output p-values of x in null-distribution
        pvalues = np.zeros(x.shape)
        nulldist_number, positives_recovered = [], []

        # finally go through all data
        nd = x.shape[1]
        if __debug__:
            if nd < x.shape[0]:
                warning("Number of features in DistPValue lower than number of"
                        " items -- may be incorrect sd=%d was provided" % sd)
        for i, xx in enumerate(x):
            dist = stats.rdist(nd-1, 0, 1)
            xx /= np.linalg.norm(xx)

            if fpp is not None:
                if __debug__:
                    debug('TRAN_', "starting adaptive adjustment i=%d" % i)

                # Adaptive adjustment for false negatives:
                Nxx, xxx, pN_emp_prev = len(xx), xx, 1.0
                Nxxx = Nxx
                indexes = np.arange(Nxx)
                """What features belong to Null-distribution"""
                while True:
                    hist, bins = np.histogram(xxx, bins=nbins, normed=False)
                    pdf = hist.astype(float)/Nxxx
                    if not numpy_center_points:
                        # if we obtain edge points for bins -- take centers
                        bins = 0.5 * (bins[0:-1] + bins[1:])
                    bins_halfstep = (bins[1] - bins[2])/2.0

                    # theoretical CDF
                    # was really unstable -- now got better ;)
                    dist_cdf = dist.cdf(bins)

                    # otherwise just recompute manually
                    # dist_pdf = dist.pdf(bins)
                    # dist_pdf /= np.sum(dist_pdf)

                    # XXX can't recall the function... silly
                    #     probably could use np.integrate
                    cdf = np.zeros(nbins, dtype=float)
                    #dist_cdf = cdf.copy()
                    dist_prevv = cdf_prevv = 0.0
                    for j in range(nbins):
                        cdf_prevv = cdf[j] = cdf_prevv + pdf[j]
                        #dist_prevv = dist_cdf[j] = dist_prevv + dist_pdf[j]

                    # what bins fall into theoretical 'positives' in both tails
                    p = (0.5 - np.abs(dist_cdf - 0.5) < fpp/2.0)

                    # amount in empirical tails -- if we match theoretical, we
                    # should have total >= p

                    pN_emp = np.sum(pdf[p]) # / (1.0 * nbins)

                    if __debug__:
                        debug('TRAN_', "empirical p=%.3f for theoretical p=%.3f"
                              % (pN_emp, fpp))

                    if pN_emp <= fpp:
                        # we are done
                        break

                    if pN_emp > pN_emp_prev:
                        if __debug__:
                            debug('TRAN_', "Diverging -- thus keeping last result "
                                  "with p=%.3f" % pN_emp_prev)
                        # we better restore previous result
                        indexes, xxx, dist = indexes_prev, xxx_prev, dist_prev
                        break

                    pN_emp_prev = pN_emp
                    # very silly way for now -- just proceed by 1 bin
                    keep = np.logical_and(xxx > bins[0], # + bins_halfstep,
                                         xxx < bins[-1]) # - bins_halfstep)
                    if __debug__:
                        debug('TRAN_', "Keeping %d out of %d elements" %
                              (np.sum(keep), Nxxx))

                    # Preserve them if we need to "roll back"
                    indexes_prev, xxx_prev, dist_prev = indexes, xxx, dist

                    # we should remove those which we assume to be positives and
                    # which should not belong to Null-dist
                    xxx, indexes = xxx[keep], indexes[keep]
                    # L2 renorm it
                    xxx = xxx / np.linalg.norm(xxx)
                    Nxxx = len(xxx)
                    dist = stats.rdist(Nxxx-1, 0, 1)

                Nindexes = len(indexes)
                Nrecovered = Nxx - Nindexes

                nulldist_number += [Nindexes]
                positives_recovered += [Nrecovered]

                if __debug__:
                    if  distribution == 'rdist':
                        assert(dist.args[0] == Nindexes-1)
                    debug('TRAN', "Positives recovery finished with %d out of %d "
                          "entries in Null-distribution, thus %d positives "
                          "were recovered" % (Nindexes, Nxx, Nrecovered))

                # And now we need to perform our duty -- assign p-values
                #dist = stats.rdist(Nindexes-1, 0, 1)
            pvalues[i, :] = dist.cdf(xx)

        # XXX we might add an option to transform it to z-scores?
        result = pvalues

        # charge conditional attributes
        # XXX might want to populate them for non-adaptive handling as well
        self.ca.nulldist_number = nulldist_number
        self.ca.positives_recovered = positives_recovered

        # transpose if needed
        if sd == 0:
            result = result.T

        return result

########NEW FILE########
__FILENAME__ = vproperty
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""C++-like virtual properties"""

__docformat__ = 'restructuredtext'


class VProperty(object):
    """Provides "virtual" property: uses derived class's method
    """

    def __init__(self, fget=None, fset=None, fdel=None, doc=''):
        """
        Parameters are the same as of generic `property`.
        """
        for attr in ('fget', 'fset'):
            func = locals()[attr]
            if callable(func):
                setattr(self, attr, func.func_name)
        setattr(self, '__doc__', doc)

    def __get__(self, obj=None, type=None):
        if not obj:
            return 'property'
        if self.fget:
            return getattr(obj, self.fget)()

    def __set__(self, obj, arg):
        if self.fset:
            return getattr(obj, self.fset)(arg)

########NEW FILE########
__FILENAME__ = multiclass
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Functions to deal with pair-wise multiclass classification

Result of the Yarik's brain-block forbidding proper formalization
within PyMVPA.  For now only used/tested only in a single test within
test_usecases.py
"""

import numpy as np

from mvpa2.datasets import Dataset

# Multi-class classification
def _get_unique_in_axis(a, axis_=-1):
    """Obtain pairs of present targets from the dataset

    Parameters
    ----------
    a : array
    axis_ : int
      Axis (!=0) to get unique values for each element of 
    """
    assert axis_ == -1, "others not implemented yet"
    return np.apply_along_axis(
        np.unique, 0,
        ## reshape slmap so we have only simple pairs in the columns
        np.reshape(a, (-1, a.shape[axis_]))).T


def get_pairwise_hits_misses(a, targets, pairs=None, select=None, axis=-1):
    """For all-pairs results extract results per each pair as hits and misses

    This function assumes that the last dimension is the one
    sweeping through the pairs, thus it could readily be applied
    to the results from searchlight

    Parameters
    ---------
    select : list, optional
      Deal only with those targets listed here, omitting the others
    axis : int, optional
      Contains predictions over which to gather hits/misses
    """
    results = []
    result_pairs = []
    a = np.asanyarray(a)
    assert axis == -1, "others not implemented yet"
    if pairs is None:
        pairs = _get_unique_in_axis(a)
    # This is a somewhat slow implementation for now. optimize later
    for i, p in enumerate(pairs):
        if select is not None and len(set(p).difference(select)):
            # skip those which are not among 'select'
            continue
        # select only those samples which have targets in the pair
        idx = np.in1d(targets, p)
        p_samples = a[idx, ..., i]
        p_targets = targets[idx]

        if p_samples.ndim > 1:
            p_targets = p_targets[:, None]
        hits_all = (p_samples == p_targets)
        hits = np.sum(hits_all, axis=0)
        misses = len(hits_all) - hits
        results.append((hits, misses))
        result_pairs.append(p)
    return result_pairs, results

# Probably it should become a mapper -- may be later and in a more
# generic way
def get_pairwise_accuracies(ds, stat='acc', pairs=None, select=None, space='targets'):
    """Extract pair-wise classification performances as a dataset

    Converts a dataset of classifications for all pairs of
    classifiers (e.g. obtained from raw_predictions_ds of a
    MulticlassClassifier) into a dataset of performances for each
    pair of stimuli categories.  I.e. only the pair-wise results
    where the target label matches one of the targets in the pair
    will contribute to the count.

    Parameters
    ----------
    pairs : 
      Pairs of targets corresponding to the last dimension in the
      provided dataset
    select : list, optional
      Deal only with those targets listed here, omitting the others
    """
    pairs, hits_misses = get_pairwise_hits_misses(
        ds.samples, ds.sa[space].value, pairs=pairs, select=select)
    hits_misses = np.array(hits_misses)
    if stat in ['acc']:
        stat_values = hits_misses[:, 0].astype(float)/np.sum(hits_misses, axis=1)
        stat_fa = [stat]
    elif stat == 'hits_misses':
        stat_values = hits_misses
        stat_fa = ['hits', 'misses']
    else:
        raise NotImplementedError("%s statistic not there yet" % stat)

    return Dataset(stat_values, sa={space: pairs}, fa={'stat': stat_fa})

########NEW FILE########
__FILENAME__ = suite
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""MultiVariate Pattern Analysis -- load helper

If you don't like to specify exact location of any particular
functionality within PyMVPA, please simply::

  from mvpa2.suite import *

or

  import mvpa2.suite

"""

__docformat__ = 'restructuredtext'


from mvpa2 import *

if __debug__ and 'SUITE' in debug.active:
    __sdebug = lambda msg: debug('SUITE', "%s" % msg)
else:
    __sdebug = lambda *args: None
__sdebug.__doc__ = "Shortcut to output debug messages for suite imports"

__sdebug('base')
from mvpa2.base import *
from mvpa2.base.attributes import *
from mvpa2.base.collections import *
from mvpa2.base.constraints import *
from mvpa2.base.config import *
from mvpa2.base.dataset import *
from mvpa2.base.externals import *
from mvpa2.base.info import *
from mvpa2.base.types import *
from mvpa2.base.verbosity import *
from mvpa2.base.param import *
from mvpa2.base.state import *
from mvpa2.base.node import *
from mvpa2.base.learner import *
from mvpa2.base.progress import *

__sdebug('h5py')
if externals.exists('h5py'):
    from mvpa2.base.hdf5 import *

__sdebug('reportlab')
if externals.exists('reportlab'):
    from mvpa2.base.report import *
else:
    from mvpa2.base.report_dummy import Report

__sdebug('algorithms')
from mvpa2.algorithms.hyperalignment import *

__sdebug('clfs')
from mvpa2 import clfs
__sdebug('clfs distance')
from mvpa2.clfs.distance import *
__sdebug('clfs base')
from mvpa2.clfs.base import *
__sdebug('clfs meta')
from mvpa2.clfs.meta import *
__sdebug('clfs kNN')
from mvpa2.clfs.knn import *
__sdebug('clfs lars')
if externals.exists('lars'):
    from mvpa2.clfs.lars import *
__sdebug('clfs enet')
if externals.exists('elasticnet'):
    from mvpa2.clfs.enet import *
__sdebug('clfs glmnet')
if externals.exists('glmnet'):
    from mvpa2.clfs.glmnet import *
__sdebug('clfs skl')
if externals.exists('skl'):
    if externals.versions['skl'] >= '0.9':
        import sklearn as skl
    else:
        import scikits.learn as skl
    from mvpa2.clfs.skl import *
__sdebug('clfs smlr')
from mvpa2.clfs.smlr import *
from mvpa2.clfs.blr import *
from mvpa2.clfs.gnb import *
from mvpa2.clfs.stats import *
from mvpa2.clfs.similarity import *
if externals.exists('libsvm') or externals.exists('shogun'):
    __sdebug('clfs svm')
    from mvpa2.clfs.svm import *
from mvpa2.clfs.transerror import *
__sdebug('clfs warehouse')
from mvpa2.clfs.warehouse import *

__sdebug('kernels')
from mvpa2 import kernels
from mvpa2.kernels.base import *
from mvpa2.kernels.np import *
if externals.exists('libsvm'):
    from mvpa2.kernels.libsvm import *
if externals.exists('shogun'):
    from mvpa2.kernels.sg import *

__sdebug('datasets')
from mvpa2 import datasets
from mvpa2.datasets import *
# just to make testsuite happy
from mvpa2.datasets.base import *
from mvpa2.datasets.formats import *
from mvpa2.datasets.miscfx import *
from mvpa2.datasets.eep import *
from mvpa2.datasets.eventrelated import *
if externals.exists('nibabel') :
    from mvpa2.datasets.mri import *
from mvpa2.datasets.sources import *
from mvpa2.datasets import niml
from mvpa2.datasets.niml import from_niml, to_niml
from mvpa2.datasets import eeglab
from mvpa2.datasets.eeglab import eeglab_dataset


__sdebug('generators')
from mvpa2.generators.base import *
from mvpa2.generators.partition import *
from mvpa2.generators.splitters import *
from mvpa2.generators.permutation import *
from mvpa2.generators.resampling import *

__sdebug('featsel')
from mvpa2 import featsel
from mvpa2.featsel.base import *
from mvpa2.featsel.helpers import *
from mvpa2.featsel.ifs import *
from mvpa2.featsel.rfe import *

__sdebug('mappers')
from mvpa2 import mappers
#from mvpa2.mappers import *
from mvpa2.mappers.base import *
from mvpa2.mappers.slicing import *
from mvpa2.mappers.flatten import *
from mvpa2.mappers.shape import *
from mvpa2.mappers.prototype import *
from mvpa2.mappers.projection import *
from mvpa2.mappers.staticprojection import *
from mvpa2.mappers.svd import *
from mvpa2.mappers.procrustean import *
from mvpa2.mappers.boxcar import *
from mvpa2.mappers.fx import *
from mvpa2.mappers.fxy import *
from mvpa2.mappers.som import *
from mvpa2.mappers.zscore import *
if externals.exists('scipy'):
    from mvpa2.mappers.detrend import *
    from mvpa2.mappers.filters import *
if externals.exists('mdp'):
    from mvpa2.mappers.mdp_adaptor import *
if externals.exists('mdp ge 2.4'):
    from mvpa2.mappers.lle import *
from mvpa2.mappers.glm import *

__sdebug('measures')
from mvpa2 import measures
from mvpa2.measures.anova import *
if externals.exists('statsmodels'):
    from mvpa2.measures.statsmodels_adaptor import *
from mvpa2.measures.irelief import *
from mvpa2.measures.base import *
from mvpa2.measures.noiseperturbation import *
from mvpa2.misc.neighborhood import *
from mvpa2.measures.searchlight import *
from mvpa2.measures.gnbsearchlight import *
from mvpa2.measures.nnsearchlight import *
from mvpa2.measures.corrstability import *
from mvpa2.measures.winner import *

__sdebug('misc')
from mvpa2.support.copy import *
from mvpa2.misc.fx import *
from mvpa2.misc.attrmap import *
from mvpa2.misc.errorfx import *
from mvpa2.misc.cmdline import *
from mvpa2.misc.data_generators import *
from mvpa2.misc.exceptions import *
from mvpa2.misc import *
from mvpa2.misc.io import *
from mvpa2.misc.io.base import *
from mvpa2.misc.io.meg import *
if externals.exists('cPickle') and externals.exists('gzip'):
    from mvpa2.misc.io.hamster import *
from mvpa2.misc.fsl import *
from mvpa2.misc.bv import *
from mvpa2.misc.bv.base import *
from mvpa2.misc.support import *
from mvpa2.misc.transformers import *
from mvpa2.misc.dcov import dCOV, dcorcoef

__sdebug("nibabel")
if externals.exists("nibabel"):
    from mvpa2.misc.fsl.melodic import *

if externals.exists("pylab"):
    from mvpa2.viz import *
    from mvpa2.misc.plot import *
    from mvpa2.misc.plot.erp import *
    if externals.exists(['griddata', 'scipy']):
        from mvpa2.misc.plot.topo import *
    from mvpa2.misc.plot.lightbox import plot_lightbox

    if externals.exists(['matplotlib', 'griddata']):
        from mvpa2.misc.plot.flat_surf import \
                FlatSurfacePlotter, curvature_from_any

__sdebug("scipy dependents")
if externals.exists("scipy"):
    from mvpa2.support.scipy.stats import scipy
    from mvpa2.measures.corrcoef import *
    from mvpa2.measures.rsa import *
    from mvpa2.clfs.ridge import *
    from mvpa2.clfs.plr import *
    from mvpa2.misc.stats import *
    from mvpa2.clfs.gpr import *
    from mvpa2.support.nipy import *

__sdebug("mappers wavelet")
if externals.exists("pywt"):
    from mvpa2.mappers.wavelet import *

__sdebug("pylab")
if externals.exists("pylab"):
    import pylab as pl

__sdebug("atlases")
if externals.exists("lxml") and externals.exists("nibabel"):
    from mvpa2.atlases import *

__sdebug("surface searchlight")
from mvpa2.misc.surfing.queryengine import SurfaceVerticesQueryEngine, \
                                           SurfaceVoxelsQueryEngine, \
                                            disc_surface_queryengine

from mvpa2.misc.surfing import surf_voxel_selection, volgeom, \
                                volsurf, volume_mask_dict

from mvpa2.misc.surfing.volume_mask_dict import VolumeMaskDictionary

__sdebug("nibabel afni")
from mvpa2.support.nibabel import afni_niml_dset, afni_suma_1d, \
                                  afni_suma_spec, surf_fs_asc, surf, \
				                  surf_caret, \
                                  afni_niml_roi, afni_niml_annot
if externals.exists('nibabel'):
    from mvpa2.support.nibabel import surf_gifti


__sdebug("ipython goodies")
if externals.exists("running ipython env"):
    try:
        from mvpa2.support.ipython import *
        ipy_activate_pymvpa_goodies()
    except Exception, e:
        warning("Failed to activate custom IPython completions due to %s" % e)

def suite_stats(scope_dict={}):
    """Return cruel dict of things which evil suite provides
    """

    scope_dict = scope_dict or globals()
    import types
    # Compatibility layer for Python3
    try:
        from io import FileIO as BuiltinFileType
    except ImportError:
        BuiltinFileType = types.FileType

    try:
        from types import ClassType as OldStyleClassType
    except ImportError:
        OldStyleClassType = type(None)

    def _get_path(e):
        """Figure out basic path for the beast... probably there is already smth which could do that for me
        """
        if str(e).endswith('(built-in)>'):
            return "BUILTIN"
        if hasattr(e, '__file__'):
            return e.__file__
        elif hasattr(e, '__path__'):
            return e.__path__[0]
        elif hasattr(e, '__module__'):
            if isinstance(e.__module__, str):
                return e.__module__
            else:
                return _get_path(e.__module__)
        elif hasattr(e, '__class__'):
            return _get_path(e.__class__)
        else:
            raise RuntimeError, "Could not figure out path for %s" % e


    class EnvironmentStatistics(dict):
        def __init__(self, d):
            dict.__init__(self, foreign={})
            # compute cruel stats
            mvpa_str = '%smvpa' % os.path.sep
            for k, e in d.iteritems():
                found = False
                for ty, tk, check_path in (
                    (list, "lists", False),
                    (str, "strings", False),
                    (unicode, "strings", False),
                    (BuiltinFileType, "files", False),
                    (types.BuiltinFunctionType, None, True),
                    (types.BuiltinMethodType, None, True),
                    (types.ModuleType, "modules", True),
                    (OldStyleClassType, "classes", True),
                    (type, "types", True),
                    (types.LambdaType, "functions", True),
                    (object, "objects", True),
                    ):
                    if isinstance(e, ty):
                        found = True
                        if tk is None:
                            break
                        if not tk in self:
                            self[tk] = {}
                        if check_path:
                            mpath = _get_path(e)
                            if mvpa_str in mpath or mpath.startswith('mvpa2.'):
                                self[tk][k] = e
                            else:
                                self['foreign'][k] = e
                        else:
                            self[tk][k] = e
                        break
                if not found:
                    raise ValueError, \
                          "Could not figure out placement for %s %s" % (k, e)

        def __str__(self):
            s = ""
            for k in sorted(self.keys()):
                s += "\n%s [%d entries]:" % (k, len(self[k]))
                for i in sorted(self[k].keys()):
                    s += "\n  %s" % i
                    # Lets extract first line in doc
                    try:
                        doc = self[k][i].__doc__.strip()
                        try:
                            ind = doc.index('\n')
                        except:
                            ind = 1000
                        s += ": " + doc[:min(ind, 80)]
                    except:
                        pass
            return s

    return EnvironmentStatistics(scope_dict)

__sdebug("THE END of mvpa2.suite imports")

########NEW FILE########
__FILENAME__ = afni_surface_alphasim
#!/usr/bin/python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''

attempt for simple alphasim based on simple 2nd level residuals (a la SPM)
uses SUMA's SurfClust, SurfFWHM, and a couple of other AFNI programs

Note: this method has not been validated properly yet.

NNO Oct 2012
'''

import os, fnmatch, datetime, re, argparse, math

from mvpa2.support.afni import afni_utils as utils


def _fn(config, infix, ext=None):
    '''Returns a file name with a particular infix'''
    if ext is None:
        ext = _ext(config)
    return './%s%s%s' % (config['prefix'], infix, ext)

def _is_surf(config):
    '''Returns True iff we are on the surface'''
    return 'surface_file' in config and config['surface_file']

def _ext(config, for1D=False):
    '''Returns the extension for a file name'''
    if _is_surf(config):
        return '.1D.dset' if for1D else '.niml.dset'
    else:
        fn = config['data_files'][0]
        return ''.join(utils.afni_fileparts(fn)[2:])

def _mask_expr(config):
    '''returns an expression that can be used as an infix in 
    running other commands (depending on whether in volume or on surface)'''
    m = config['mask']
    if not m:
        return ''
    else:
        if _is_surf(config):
            return '-b_mask %s' % m
        else:
            return '-mask %s' % m

def compute_fwhm(config):
    # helper function - called by critical_clustersize 
    # computes FWHM of residuals of input data and stores in config
    output_dir = c['output_dir']

    is_surf = _is_surf(config)
    ext, ext1D = _ext(config), _ext(config, for1D=True)
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)


    cmds = ['cd "%s"' % output_dir]

    # if surfaces and needs padding, do that first
    pad_to_node = config['pad_to_node']
    if is_surf and pad_to_node:
        data_files = []

        for i, fn in enumerate(c['data_files']):
            fn_pad = 'pad_%d%s' % (i, ext)
            cmds.append("; ConvertDset -overwrite -pad_to_node %d -input %s'[%d]' -prefix ./%s" %
                                (pad_to_node, fn, config['brik_index'], fn_pad))
            data_files.append(fn_pad)
        pad_files = data_files
        brik_index = 0
    else:
        data_files = c['data_files']
        pad_files = []
        brik_index = c['brik_index']

    # bucket data from all participants into a single file
    buck_fn = _fn(config, 'buck')

    cmds.append('; 3dbucket -overwrite -prefix %s' % buck_fn)
    for fn in data_files:
        cmds.append(" %s'[%d]'" % (fn, brik_index))

    # also store as 1D (won't hurt)
    if is_surf:
        buck_fn_1D = _fn(config, 'buck', ext1D)
        cmds.append('; ConvertDset -overwrite -o_1D -prefix %s -input %s' %
                    (buck_fn_1D, buck_fn))
    else:
        buck_fn_1D = buck_fn

    # compute group mean
    mean_fn = _fn(config, 'mean')
    cmds.append('; 3dTstat -overwrite -prefix %s %s' % (mean_fn, buck_fn))

    # compute residuals, and estimate FWHM for each of them
    # store FWHM output in fwhm_fn
    fwhm_fn = os.path.join(output_dir, _fn(config, 'fwhm', '.1D'))
    cmds.append('; echo > "%s"' % fwhm_fn)

    resid_fns = []
    for i in xrange(len(c['data_files'])):
        fn = _fn(config, 'resid_%d' % i)
        cmds.append("; 3dcalc -overwrite -prefix %s -a %s -b %s'[%d]' -expr 'a-b'"
                    % (fn, mean_fn, buck_fn, i))
        msk = _mask_expr(config)
        if is_surf:
            surf_fn = c['surface_file']
            cmds.append("; SurfFWHM %s -input %s -i_fs %s"
                        "| grep ^FWHM  | cut -f2 -d'=' >> '%s'" %
                        (msk, fn, surf_fn, fwhm_fn))
        else:
            cmds.append('; 3dFWHMx %s %s | cut -c18- >> %s' % (msk, fn, fwhm_fn))
        resid_fns.append(fn)

    cmd = ''.join(cmds)
    utils.run_cmds(cmd)

    # read FWHM values and store in config
    with open(fwhm_fn) as f:
        fwhms = f.read().split()

    print fwhms
    print fwhm_fn

    config['all_fwhms'] = fwhms # all FWHMs (for each participant)
    config['fwhm'] = sum(map(float, fwhms)) / len(fwhms) # average FWHM
    config['buck_fn'] = buck_fn
    config['buck_fn_1D'] = buck_fn_1D

    mean_fwhm_fn = os.path.join(output_dir, _fn(config, 'mean_fwhm', '.1D'))
    with open(mean_fwhm_fn, 'w') as f:
        f.write('%.3f\n' % config['fwhm'])

    tmpfns = resid_fns + pad_files + [mean_fn]
    print "TEMP"
    print tmpfns
    _remove_files(config, tmpfns)

def null_clustersize(config):
    # helper function - called by critical_clustersize
    # computes maxmimum cluster size of a single null permutation
    output_dir = config['output_dir']
    tthr = config['tthr']
    fwhm = config['fwhm']
    buck_fn_1D = config['buck_fn_1D']
    msk = _mask_expr(config)
    is_surf = _is_surf(config)
    if is_surf:
        surf_fn = config['surface_file']
    ext, ext1D = _ext(config), _ext(config, for1D=True)

    ns = len(config['data_files'])
    cmds = ['cd "%s"' % output_dir]

    # generate N random data files (N=number of participants)
    # use the output bucket to get datasets with the right size
    null_fns = []
    for i in xrange(ns):
        fn = _fn(config, 'rand_%d' % i, ext1D)
        if is_surf:
            cmds.append("; 1deval -ok_1D_text -a %s'[0]' -expr 'gran(0,1)' > '%s'" % (buck_fn_1D, fn))
        else:
            cmds.append("; 3dcalc -overwrite -prefix %s -a %s'[0]' -expr 'gran(0,1)'" % (fn, buck_fn_1D))
        null_fns.append(fn)

    # bucket random data
    buck_fn = _fn(config, 'rand_buck', ext1D)
    null_fns_list = ' '.join(null_fns)
    if is_surf:
        cmds.append('; 1dcat %s > "%s"' % (null_fns_list, buck_fn))
    else:
        cmds.append('; 3dbucket -overwrite -prefix %s %s' % (buck_fn, null_fns_list))

    # smooth all data at once, using estimated FWHM
    smooth_fn = _fn(config, 'rand_buck_smooth', ext1D)
    if is_surf:
        if config['sigma'] > 0:
            sigma_str = '-sigma %s' % config['sigma']
        else:
            sigma_str = ''
        cmds.append('; SurfSmooth -overwrite %s -met HEAT_07 -i_fs %s -input %s '
                    ' -fwhm %f -output %s %s' % (msk, surf_fn, buck_fn, fwhm, smooth_fn, sigma_str))
    else:
        cmds.append('; 3dBlurInMask -overwrite %s -FWHM %f -prefix %s -input %s' %
                      (msk, fwhm, smooth_fn, buck_fn))

    # run ttest
    if is_surf:
        msk = '' # cannot use mask on surface, but that's fine
               # as it was used in SurfSmooth
    ttest_fn = _fn(config, 'rand_buck_smooth_t', ext1D)
    cmds.append('; 3dttest++ %s -overwrite -prefix %s -setA %s' %
                    (msk, ttest_fn, smooth_fn))


    # extract maximum cluster size (in mm^2 or number of voxels) from output
    # and pipe into size_fn

    size_fn = _fn(config, 'rand_size', '.1D')
    if is_surf:
        postfix = "| grep --invert-match '#' | head -1 | cut -c 18-28"
        cmds.append('; SurfClust -i_fs %s -input %s 1 -rmm -1 '
                    ' -thresh %f -thresh_col 1 %s > "%s"' %
                        (surf_fn, ttest_fn, tthr, postfix, size_fn))
    else:
        postfix = " | grep --invert-match '#' | head -1 | cut -c1-8"
        cmds.append("; 3dclust -quiet -1noneg -1clip %f 0 0 %s'[1]' %s > '%s'" %
                     (tthr, ttest_fn, postfix, size_fn))

    utils.run_cmds(''.join(cmds))

    # read maximum cluster size form size_fn
    sz_str = None
    with open(os.path.join(output_dir, size_fn)) as f:
        sz_str = f.read()

    try:
        sz = float(sz_str)
    except:
        sz = 0. # CHECKME whether this makes sense

    print "Null data: maximum size %f" % sz

    if is_surf:
        smoothing_fn_rec = os.path.join(output_dir, _fn(config, 'rand_buck_smooth', '.1D.dset.1D.smrec'))
        if not os.path.exists(smoothing_fn_rec):
            raise ValueError("Smoothing did not succeed. Please check the error"
                             " messaged. You may have to set sigma manually")
        with open(smoothing_fn_rec) as f:
            s = f.read()

        final_fwhm = float(s.split()[-2])
        ratio = fwhm / final_fwhm
        thr = 0.9
        if ratio < thr or 1. / ratio < thr:
            raise ValueError('FWHM converged to %s but expected %s. Consider '
                             'setting sigma manually' % (final_fwhm, fwhm))

    # clean up - remove all temporary files

    tmpfns = null_fns + [buck_fn, smooth_fn, ttest_fn, size_fn]
    _remove_files(config, tmpfns)

    return sz

def _remove_files(config, list_of_files):
    # removes a list of files, if config allows for it
    # in the case of AFNI volume files, it removes HEAD and BRIK files
    if not config['keep_files']:
        for fn in list_of_files:
            fp = utils.afni_fileparts(fn)

            if fp[2]:
                # AFNI HEAD/BRIK combination
                # ensure we delete all of them
                exts = ['.HEAD', '.BRIK', '.BRIK.gz']
                fn = ''.join(fp[1:3])
            else:
                exts = ['']

            for ext in exts:
                full_fn = os.path.join(config['output_dir'], fn + ext)
                if os.path.exists(full_fn):
                    os.remove(full_fn)


def critical_clustersize(config):
    '''computes the critical cluster sizes
    it does so by calling compute_fwhm and null_clustersize
    
    config['max_size'] is a list with he maximum cluster size of
    each iteration'''

    compute_fwhm(config)

    # this takes a long time
    niter = config['niter']
    sz = []
    for i in xrange(niter):
        sz.append(null_clustersize(config))
        print "Completed null iteration %d / %d" % (i + 1, niter)

    config['max_size'] = sz

    # store the results in a file
    clsize_fn = _fn(config, 'critical_cluster_size', '.1D')
    with open(os.path.join(config['output_dir'], clsize_fn), 'w') as f:
        f.write('# Critical sizes for tthr=%.3f, fwhm=%.3f, %d files\n' % (
                    config['tthr'], config['fwhm'], len(config['data_files'])))
        for s in sz:
            f.write('%.5f\n' % s)
    return sz

def _critical_size_index(config):
    '''computes the index of the critical cluster size
    (assuming that these sizes are sorted)'''
    pthr = config['pthr']
    nsize = config['niter']

    idx = math.ceil((1. - pthr) * nsize) # index of critical cluster size
    if idx >= nsize or idx == 0:
        raise ValueError("Illegal critical index (p=%s): %s" % (pthr, idx))

    return int(idx)


def apply_clustersize(config):
    # applies the critical cluster size to the original data
    #
    # assumes that critical_clustersize(config) has been run

    output_dir = config['output_dir']
    pthr = config['pthr']
    tthr = config['tthr']
    niter = config['niter']
    buck_fn_1D = config['buck_fn_1D']
    is_surf = _is_surf(config)

    if is_surf:
        surf_fn = config['surface_file']

    cmds = ['cd "%s"' % output_dir]

    # run ttest on original data
    infix = 'ttest_t%(tthr)s' % config
    ttest_fn = _fn(config, infix)
    msk = _mask_expr(config)

    # NOTE: for surfaces, apply mask below (SurfClust)
    #       but in volumes, apply it here
    if is_surf:
        cmds.append('; 3dttest++ -ok_1D_text -overwrite -prefix %s -setA %s' % (ttest_fn, buck_fn_1D))
    else:
        cmds.append('; 3dttest++ %s -overwrite -prefix %s -setA %s' % (msk, ttest_fn, buck_fn_1D))

    # sort cluster sizes
    clsize = list(config['max_size'])
    clsize.sort()

    # get critical cluster size
    idx = _critical_size_index(config)
    critical_size = clsize[idx]

    print "critical size %s (p=%s)" % (critical_size, pthr)

    # apply critical size to t-test of original data
    infix += '_clustp%s_%dit' % (pthr, niter)

    if not is_surf:
        # for surfaces the size is included in the filename automatically
        infix += '_%svx' % critical_size

    # set file names
    dset_out = _fn(config, infix)
    log_out = _fn(config, infix, '.txt')

    if is_surf:
        cmds.append('; SurfClust %s -i_fs %s -input %s 1 -rmm -1 '
                    ' -thresh %f -thresh_col 1 -amm2 %f -out_clusterdset -prefix %s > %s' %
                        (msk, surf_fn, ttest_fn, tthr, critical_size, dset_out, log_out))
    else:
        dset_out_msk = _fn(config, infix + '_msk')
        cmds.append("; 3dclust -overwrite -1noneg -1clip %f  "
                    " -prefix %s -savemask %s 0 -%f %s'[1]' > %s" %
                    (tthr, dset_out, dset_out_msk, critical_size, ttest_fn, log_out))

    cmd = "".join(cmds)
    utils.run_cmds(cmd)

def run_all(config):
    '''main function to estimate critical cluster size
    and apply to group t-test result'''
    critical_clustersize(config)
    apply_clustersize(config)


def get_testing_config():
    # for testing only
    in_vol = True
    c = dict(mask='')

    if in_vol:
        d = '/Users/nick/organized/210_smoothness/afnidata/glm/'
        sids = ['%02d' % i for i in xrange(1, 13)]
        fn_pat = 'glm_SUB%s_REML+tlrc.HEAD'
        fns = ['%s/%s' % (d, fn_pat % sid) for sid in sids]
        c['output_dir'] = '/Users/nick/organized/210_smoothness/_tst'
        c['brik_index'] = 2
        c['mask'] = d + '../../all/mask+tlrc.HEAD'
        c['pad_to_node'] = None
    else:


        d = '/Users/nick/organized/212_raiders_fmri/ref'
        sids = ['ab', 'ag', 'aw', 'jt']
        fn = 'cross_ico16-128_mh_100vx_8runs_t3.niml.dset'
        fns = ['%s/%s/%s' % (d, sid, fn) for sid in sids]

        surffn = '%s/%s/ico16_mh.intermediate_al.asc' % (d, sids[0])

        c['output_dir'] = '/Users/nick//tmp/alphasim/'

        c['surface_file'] = surffn
        c['brik_index'] = 0
        c['ext'] = '.niml.dset'
        c['pad_to_node'] = 5124 - 1



    c['data_files'] = fns
    c['tthr'] = 4.4
    c['niter'] = 1000
    c['pthr'] = .05
    c['prefix'] = 'alphasim_'
    c['keep_files'] = False


    return c


def get_options():
    description = '''
    Experimental implementation of alternative AlphaSim for volumes and surfaces.\n
    
    Currently only supports group analysis with t-test against 0.
    
    Input paramaters are an uncorrected t-test threshold (tthr)
    and cluster-size corrected p-value (a.k.a. alpha level) (pthr).
    
    This program takes the following steps:
    (0) Participant's map are t-tested against zero, thresholded by tthr and clustered.
    (1) residuals are computed by subtracting the group mean from each participants' map 
    (2) the average smoothness of these residual maps is estimated
    (3) null maps are generated from random gaussian noise that is smoothened
    with the estimated smoothness from step (2), thresholded by tthr, and clustered. 
    The maximum cluster size is stored for each null map.
    (4) A cluster from (0) survive pthr if a smaller ratio than pthr of the
    null maps generated in (3) have a maximum cluster size.
    
    Copyright 2012 Nikolaas N. Oosterhof <nikolaas.oosterhof@unitn.it>
    '''

    epilog = '''This function is *experimental* and may delete files in output_dir or elsewhere.
    As of Oct 2012 it has not been validated properly against other
    approaches to correct for multiple comparisons. Also, it is slow.'''


    parser = argparse.ArgumentParser(description=description, epilog=epilog)
    parser.add_argument("-d", "--data_files", required=True, help="Data input files (that are tested against 0 with a one-sample t-test)", nargs='+')
    parser.add_argument("-o", "--output_dir", required=True, help="Output directory")
    parser.add_argument("-s", "--surface_file", required=False, help="Anatomical surface file (.asc)", default=None)
    parser.add_argument("-t", "--tthr", required=True, help="t-test uncorrected threshold", type=float)
    parser.add_argument("-p", "--pthr", required=False, help="p-value for corrected threshold", type=float, default=.05)
    parser.add_argument("-n", "--niter", required=False, help="Number of iterations", type=int, default=1000)
    parser.add_argument("-i", "--brik_index", required=False, help="Brik index in input files", type=int, default=0)
    parser.add_argument("-P", "--prefix", required=False, help="Prefix for data output", default='alphasim_')
    parser.add_argument("-m", "--mask", required=False, help="Mask dataset", default=None)
    parser.add_argument("-k", "--keep_files", required=False, help="Keep temporary files", default=False, action="store_true")
    parser.add_argument("-N", "--pad_to_node", required=False, help="pad_to_node (for surfaces)", default=0, type=int)
    parser.add_argument('-S', "--sigma", required=False, help='sigma for SurfSmooth', default=0., type=float)

    args = None
    namespace = parser.parse_args(args)
    return vars(namespace)

def fix_options(config):
    # makes everything an absolute path
    # also verifies a few input parameters
    def f(x, check=True):
        y = os.path.abspath(x)
        if check and not os.path.exists(y):
            raise ValueError("Not found: %s" % x)
        return y

    for i in xrange(len(config['data_files'])):
        config['data_files'][i] = f(config['data_files'][i])

    config['output_dir'] = f(config['output_dir'], check=False)

    if _is_surf(config):
        config['surface_file'] = f(config['surface_file'])

    if config['mask']:
        config['mask'] = f(config['mask'])

    p = config['pthr']
    if p <= 0 or p >= 1:
        raise ValueError('Require 0 < pthr < 1')
    _critical_size_index(config) # raises error if p too small



if __name__ == '__main__':
    #c = get_testing_config()
    c = get_options()
    fix_options(c)

    run_all(c)

########NEW FILE########
__FILENAME__ = afni_utils
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Utility functions for AFNI files

Created on Feb 19, 2012

@author: Nikolaas. N. Oosterhof (nikolaas.oosterhof@unitn.it)
'''

import os, subprocess, time, datetime, collections
import os.path as op

def as_list(v):
    '''makes this a singleton list if the input is not a list'''
    if type(v) not in [list, tuple]:
        v = [v]
    return v

def afni_fileparts(fn):
    '''File parts for afni filenames.

    Returns a tuple with these four parts.

    Also works for .nii files, in which case the third part is the empty

    Not tested for other file types

    Parameters
    ----------
    whole filename
      PATH/TO/FILE/NAME+orig.HEAD


    Returns
    -------
    fullpath: str
      PATH/TO/FILE
    rootname: str
      NAME
    orientation: str
      +orig
    extensions: str
      .HEAD

    '''

    tail, head = os.path.split(fn)

    s = head.split('+')
    name = s[0]
    orient = '+' + s[1] if len(s) == 2  else ''

    afniorients = ['+orig', '+tlrc', '+acpc']
    ext = None

    for a in afniorients:
        if orient.startswith(a):
            #ext=orient[len(a):]
            orient = a
            ext = ".HEAD"
            #if ext=='.':
            #    ext=''

    if ext is None:
        s = name.split(".")
        if len(s) > 1:
            ext = "." + ".".join(s[1:])
            name = s[0]
        else:
            ext = ''

    return tail, name, orient, ext

def afni_fileexists(fn):
    '''
    Parameters
    ----------
    fn : str
        AFNI filename (possibly without .HEAD or .BRIK extension)

    Returns
    -------
    bool
        True iff fn exists as AFNI file
    '''
    p, n, o, e = afni_fileparts(fn)

    if o:
        return os.path.isfile('%s/%s%s.HEAD' % (p, n, o))
    else:
        return (e in ['.nii', '.nii.gz']) and os.path.isfile(fn)

def run_cmds(cmds, env=None, dryrun=False):
    '''exectute a list of commands in the shell'''
    if env is None:
        env = os.environ

    # if cmds is just one command, make a singleton list
    cmds = as_list(cmds)

    # run each command
    for cmd in cmds:
        print("** Will execute the following commands:")
        for c in cmd.split(';'):
            print '** - %s' % c
        if not dryrun:
            print("**>> Starting now:")

            subprocess.check_call(cmd, env=env, shell=True)

            print("**<< ... completed execution")

def cmd_capture_output(cmd, env=None):
    if env is None:
        env = os.environ
    return subprocess.check_output(cmd, env=env, shell=True)

def which(f, env=None):
    '''Finds the full path to a file in the path

    Parameters
    ----------
    f: str
        Filename of executable
    env (optional):
        Environment in which path is found.
        By default this is the environment in which python runs


    Returns
    str
        Full path of 'f' if 'f' is executable and in the path, 'f' itself
        if 'f' is a path, None otherwise
    '''
    if env == None:
        env = os.environ

    def is_executable(fullpath):
        return os.path.exists(fullpath) and os.access(fullpath, os.X_OK)

    [p, n] = os.path.split(f)
    if p:
        return f
    else:
        for path in env['PATH'].split(os.pathsep):
            fullfn = os.path.join(path, n)
            if is_executable(fullfn):
                return fullfn
        return None



def _package_afni_nibabel_for_standalone(outputdir, rootname='python'):
    '''
    helper function to put mvpa2.support.{afni,nibabel} into another
    directory (outputdir) where it can function as a stand-alone package
    '''

    outputdir_files = os.path.join(outputdir, rootname)
    for d in (outputdir, outputdir_files):
        if not os.path.exists(d):
            os.mkdir(d)

    fullpath = op.realpath(__file__)
    fullpath_parts = fullpath.split('/')
    if (len(fullpath_parts) < 4 or
            fullpath.split('/')[-4:-1] != ['mvpa2', 'support', 'afni']):
        raise ValueError('This script is not in mvpa2.support.afni. '
                         'Packaging for stand-alone is not supported')

    replacements = {'from mvpa2.base import warning':'def warning(x): print x'}

    rootdir = os.path.join(op.split(fullpath)[0], '..')
    parent_pkg = 'mvpa2.support'
    pkgs = ['afni', 'nibabel']
    srcdirs = [os.path.join(rootdir, pkg) for pkg in pkgs]


    input_path_fns = [os.path.join(d, f) for d in srcdirs
                                         for f in os.listdir(d)
                                         ]

    is_python_file = lambda fn: fn.endswith('.py') and not fn.endswith('__.py')
    input_path_fns = filter(is_python_file, input_path_fns)

    print input_path_fns

    outputfns = []
    for path_fn in input_path_fns:
        fn = os.path.split(path_fn)[1]

        with open(path_fn) as f:
            lines = f.read().split('\n')

        newlines = []
        for line in lines:
            newline = None

            for old, new in replacements.iteritems():
                line = line.replace(old, new)

            if 'import' in line:
                words = line.split()

                for pkg in pkgs:
                    full_pkg = parent_pkg + '.' + pkg
                    trgwords = ['from', full_pkg, 'import']
                    n = len(trgwords)

                    if len(words) >= n and words[:n] == trgwords:
                        # find how many trailing spaces
                        i = 0
                        while line.find(' ', i) == i:
                            i += 1
                        # get everything from import to end of line
                        # with enough spaces in front
                        newline = (' ' * i) + ' '.join(words[(n - 1):])
                        #print line
                        #print ' -> ', newline
                        break
                    else:
                        if pkg in words:
                            print("Not supported in %s: %s" % (path_fn, line))
                            newline=False
                            break

            if newline is False:
                newlines=[]
                break

            if newline is None:
                newline = line

            newlines.append(newline)

        if not len(newlines):
            print "skipping %s" % fn
            continue

        if fn.startswith('lib_'):
            repls = [('lib_', 'pymvpa2-'), ('.py', ''), ('_', '-')]
            srcbinfn = fn
            for src, trg in repls:
                srcbinfn = srcbinfn.replace(src, trg)

            parentfn = os.path.join(rootdir, '..', '..', 'bin', srcbinfn)
            print parentfn
            if os.path.exists(parentfn):
                with open(parentfn) as pf:
                    plines = pf.read().split('\n')
                    in_main = False
                    for line in plines:
                        if '__main__' in line:
                            in_main = True
                        if in_main:
                            newlines.append(line)
                newlines = [plines[0]] + newlines
            else:
                raise ValueError("not found: %s" % parentfn)

            trgfn = os.path.join(outputdir_files, fn.replace('lib_', ''))
        else:
            trgfn = op.join(outputdir_files, fn)

        with open(trgfn, 'w') as f:
            f.write('\n'.join(newlines))

        is_executable = newlines[0].startswith('#!')
        chmod_ = 0777 if is_executable else 0666
        os.chmod(trgfn, chmod_)

        print "Written file %s in %s" % (fn, outputdir_files)
        outputfns.append(os.path.split(trgfn)[1])




    readme = ('''
AFNI I/O and wrapper functions in python

Copyright 2010-2014 Nikolaas N. Oosterhof <nikolaas.oosterhof@unitn.it>

The software in the following files is covered under the MIT License
(included below):
''' +
    '\n'.join(map(lambda x:' - ' + x, outputfns)) +
    '''
Parts of this software is or will be included in PyMVPA.
For information see http://www.pymvpa.org.

-------------------------------------------------------------------------
The MIT License

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and / or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
''')

    readmefn = op.join(outputdir_files, 'COPYING')
    with open(readmefn, 'w') as f:
        f.write(readme)


########NEW FILE########
__FILENAME__ = lib_plot_slices
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
A simple slice plotter, mostly aimed at AFNI files
'''

import numpy as np
import nibabel as nb
import argparse

from mvpa2.base import externals
externals.exists('matplotlib', raise_=True)

import matplotlib
matplotlib.use('Agg') # this seems to work on non-(X windows) systems
import matplotlib.pyplot as plt


import os
import subprocess
from os.path import join, split, abspath

_VERSION = ".1"
_NAME = 'crisp - co-registered image slice plotter'
_AUTHOR = "Nov 2013, Nikolaas N. Oosterhof"
_NP_FLOAT_DTYPE = np.float16

def reduce_size(a):
    '''
    Reduces storage space for a numpy array

    Parameters
    ----------
    a: np.ndarray

    Returns
    -------
    b: np.ndarray stored as _NP_FLOAT_DTYPE
    '''
    return np.asarray(a, dtype=_NP_FLOAT_DTYPE)

def load_afni(fn, master_fn=None):
    '''
    Loads an AFNI volume.

    Parameters
    ----------
    fn: string
        Filename of AFNI file to load.
    master_fn: string or None
        If a string, then 'fn' is resampled to the grid of 'master_fn'.

    Returns
    -------
    img: np.ndarray
        Volumetric data in 'fn'.
    '''

    [p, nm] = split(abspath(fn))

    # find a unique filename for temporary storage
    tmp_pat = '__tmp%d.nii'
    d = 0
    while True:
        tmp_fn = join(p, tmp_pat % d)
        if not os.path.exists(tmp_fn):
            break
        d += 1

    # resample if 'master_fn' is supplied, otherwise just copy.
    # in both cases convert to nifti

    if master_fn is None:
        cmd = '3dbucket -overwrite -prefix %s %s[0]' % \
                (tmp_fn, fn)
    else:
        cmd = '3dresample -overwrite -prefix %s -master %s -inset %s[0]' % \
                (tmp_fn, master_fn, fn)

    # convert file
    subprocess.check_call(cmd.split())

    # load nifti file
    data = load_nii(tmp_fn)

    # remove temporary file
    os.unlink(tmp_fn)

    return data


def load_nii(fn):
    '''
    Loads a NIFTI volume.

    Parameters
    ----------
    fn: string
        Filename of NIFTI file to load.

    Returns
    -------
    img: np.ndarray
        Volumetric data in 'fn'.
    '''
    img = nb.load(fn)
    data = reduce_size(img.get_data())
    if len(data.shape) != 3:
        raise ValueError('Expected 3D array, but shape is %s' %
                                                (data.shape,))
    return data

def load_vol(fn, master_fn=None):
    '''
    Loads an AFNI or NIFTI volume.

    Parameters
    ----------
    fn: string
        Filename of AFNI file to load.
    master_fn: string or None
        If a string, then 'fn' is resampled to the grid of 'master_fn'
        if 'fn' is an AFNI volume. If 'fn' is a NIFTI volume then
        'master_fn' is ignored.

    Returns
    -------
    img: np.ndarray
        Volumetric data in 'fn'.
    '''

    is_nifti = any(fn.endswith(e) for e in ['.nii', '.nii.gz'])

    if is_nifti:
        if master_fn is not None:
            print "Warning: Ignoring master %s for %s" % (master_fn, fn)
        return load_nii(fn)
    else:
        return load_afni(fn, master_fn)

def slice_data(data, dim, where):
    '''
    Slices data along a dimension

    Parameters
    ----------
    data: np.ndarray
        Image data
    dim: int or list of int
        Dimension along which to slice
    where: float or list of float
        Position where to slice (in range 0..1)

    Returns
    -------
    sl: (list of (list of)) np.ndarray
        Depending on whether 'dim' and/or 'where' are lists or not,
        sl is either a single np.ndarray, a list of such arrays,
        or a list of a list of such arrays
    '''
    if type(dim) in (list, tuple):
        # use recursion
        return [slice_data(data, d, where) for d in dim]

    # make first dimension of interest
    if dim > 0 and not data is None:
        data = data.swapaxes(0, dim)

    if type(where) in (list, tuple):
        # use recursion
        return [slice_data(data, 0, w) for w in where]

    if data is None:
        return None

    if type(where) is float:
        # convert to integer index
        n = data.shape[0]
        where = int(float(n) * where)

    return np.squeeze(data[where, :, :]).T[::-1, ::-1]

def color_slice(data, cmap, min_max=None):
    '''
    Applies a colormap to data

    Parameters
    ----------
    data: (list of) np.ndarray
        Data to be colored
    cmap: colormap instance, bool or string
        If a string then a colormap with the name 'cmap' is used.
        True is equivalent to 'hot'. False is equivalent to 'gray'.
    min_max: pair of float or None
        Range to which values in 'data' or scaled. If None then
        the minimum and maximum values in 'data' are used.

    Returns
    -------
    c: np.ndarray
        Color value array with size PxQx4.
    '''

    # get colormap
    if type(cmap) is bool:
        cmap = 'hot' if cmap else 'gray'
    if isinstance(cmap, basestring):
        cmap = plt.get_cmap(cmap)

    if type(data) is list:
        return [color_slice(d, cmap, min_max) for d in data]

    if data is None:
        return None

    if min_max is None:
        max_ = np.max(data)
        min_ = np.min(data)

    # make sure it floats & scale to 0..1
    scale_inplace(data)
    clip_inplace(data)

    sh = data.shape

    # colormapping requires an array or matrix, so
    # ravel array here, then reshape after mapping
    cs_lin = cmap(data.ravel())

    cs = np.reshape(cs_lin, sh + cs_lin.shape[-1:])
    cs = reduce_size(cs)

    return cs

def blend(datas, weights=None):
    '''
    Blends different color value arrays

    Parameters
    ----------
    datas: tuple of np.ndarray
        Arrays to be blended
    weights: list of float or None
        Weight for each array. If None then weights is an array of ones.

    Returns
    -------
    b: np.ndarray
        Weighted sum of 'datas'. Data is clipped to range 0..1
    '''
    if type(datas) is list:
        return [blend(d, weights) for d in datas]

    n = len(datas)
    if weights is None:
        weights = np.ones((n, 1))

    #  space for output
    b = None

    for (data, weight) in zip(datas, weights):
        if data is None:
            continue

        if b is None:
            b = np.zeros(data.shape, dtype=_NP_FLOAT_DTYPE)

        # ensure shape is the same for all inputs
        if data.shape != b.shape:
            raise ValueError('Shape mismatch: %s %s %s' %
                        slice_.shape, b.sh)
        b += weight * data

    # clip range
    clip_inplace(b)

    return b

def scale_inplace(x, range_=None):
    '''
    Scales array in-place to 0..1

    Parameters
    ----------
    x: np.ndarray
        Data to be scaled. Data is modified in-place.
    range_: None or [min_, max_]
        Range to scale the data to. None is equivalent to
        [np.min(x), np.max(x)]
    '''
    if x is None:
        return

    if range_ is None:
        range_ = [np.min(x), np.max(x)]

    min_, max_ = range_
    x -= min_
    x /= (max_ - min_)

def clip_inplace(x, range_=(0., 1.)):
    '''
    Clips data in-place

    Parameters
    ----------
    x: np.ndarray
        Data to be clipped (in-place)
    range_: None or [min_, max_]
        Range to clip to. None is equivalent to [0,1]
    '''
    if x is None:
        return
    min_, max_ = range_
    x[x < min_] = min_
    x[x > max_] = max_

def make_plot(ulay, olay, dims, pos, title=None,
                ulay_range=None, olay_range=None, output_fn=None):
    '''
    Generates a plot of slices with different overlays and underlays

    Parameters
    ----------
    ulay: np.ndarray or str or None
        (filename of) underlay
    olay: np.ndarray or str or None
        (filename of) overlay
    dims: list of int
        dimensions to plot (0=x, 1=y, 2=z)
    pos: list of float
        relative positions to slice (in range 0..1)
    title: str or None
        title of plot
    ulay_range: None or [min_, max_]
        range to scale underlay to
    olay_range: None or [min_, max_]
        range to scale overlay to
    output_fn: None or str
        If not None the output is saved to this file

    Returns
    -------
    plt: plt
    '''

    # set some defaults
    figsize = (15, 12)
    imglength = 200
    fontsize = 30
    fontcolor = 'white'
    bgcolor = 'black'

    # load underlay and overlay
    if isinstance(ulay, basestring):
        ulay_name = split(ulay)[1]
        u = load_vol(ulay)
    else:
        ulay_name = ''
        u = ulay

    if isinstance(olay, basestring):
        olay_name = split(olay)[1]
        o = load_vol(olay)
    else:
        olay_name = ''
        o = olay

    title = '%s / %s' % (ulay_name, olay_name)

    # scale
    scale_inplace(u, ulay_range)
    scale_inplace(o, olay_range)

    # slice data, so that u and o become lists of lists of 2D-arrays
    u = slice_data(u, dims, pos)
    o = slice_data(o, dims, pos)

    # color slices, so that u and o become lists of lists of 3D-arrays (X*Y*4)
    u = color_slice(u, False, (0, 1))
    o = color_slice(o, True, (0, 1))

    # blend underlay and overlay
    uo = map(zip, *zip(*zip(u, o)))
    sl = blend(uo)

    # free up some space
    del(o)
    del(u)

    # generate slices
    ncol = len(sl[0])
    nrow = len(sl)

    # define subplots: one row per dimension, one column per position
    [f, axs] = plt.subplots(nrow, ncol, sharex=True, figsize=figsize)
    f.set_facecolor(bgcolor)
    counter = 0
    for j in xrange(ncol):
        for i in xrange(nrow):
            counter += 1
            ax = axs[i, j]

            ax.imshow(sl[i][j], extent=(0, imglength) * 2)
            ax.axis('off')
            ax.set_axis_bgcolor(bgcolor)

    # attempt to set a tight layout (doesn't work greatly)
    f.tight_layout()
    if not title is None:
        # set title
        axs[0][0].text(0, imglength, title, color=fontcolor, fontsize=fontsize)

    plt.draw()

    if output_fn is not None:
        plt.savefig(output_fn, facecolor='black')


def make_scatter(ulay, olay, output_fn=None):
    '''
    Generates a scatter plot between intensity values of underlay and overlay

    Generates a plot of slices with different overlays and underlays

    Parameters
    ----------
    ulay: np.ndarray or str or None
        (filename of) underlay
    olay: np.ndarray or str or None
        (filename of) overlay
    output_fn: None or str
        If not None the output is saved to this file

    Returns
    -------
    plt: plt
    '''

    cutoff_rel = .1  # ignore lowest 10% of values
    figsize = (15, 12) # size of figure
    histbincount = 25 # number of bins in histogram
    internal_rel = .25 # only show voxels within 50% of center of mass

    # load data
    u = load_vol(ulay)
    o = load_vol(olay, ulay)

    # define cutoff function
    def cutoff(x, cutoff_rel=cutoff_rel):
        xr = x.ravel()
        x_sorted = np.sort(xr)
        cutoff_abs = xr[np.round(cutoff_rel * x.size)]
        return cutoff_abs

    # only keep voxels that survive cutoff in both underlay and overlay
    msk = np.logical_and(o > cutoff(o), u > cutoff(u))
    apply_msk = lambda x:np.asarray(x[msk], dtype=np.float_)

    um, om = map(apply_msk, (u, o))

    ## compute distance of each voxel from center of mass
    sh = np.asarray(u.shape)
    ndim = len(sh)
    nf = np.prod(sh)

    #
    xyz = np.zeros((nf, ndim), dtype=u.dtype)

    for dim in xrange(ndim):
        r = np.arange(sh[dim])
        vec_dim = np.ones(ndim)
        vec_dim[dim] = sh[dim]
        r_shaped = np.reshape(r, vec_dim)

        tile_dim = sh.copy()
        tile_dim[dim] = 1
        dim_coord = np.tile(r_shaped, tile_dim)
        xyz[:, dim] = dim_coord.ravel()

    xyzm = np.asarray(xyz[msk.ravel(), :], dtype=np.float_) # mask & get to native float dtype
    com = np.mean(xyzm, 0) # center of mass

    delta = xyzm - com
    c = np.sqrt(np.sum(delta ** 2, 1)) # distance from center of mass

    c[c > np.max(c) * internal_rel] = np.Inf

    # indices to sort by distance
    ii = np.argsort(c)
    ii = ii[np.isfinite(c[ii])] # remove infinite values
    ii = ii[::-1] # reverse order - small ones on top

    # apply indices
    c = c[ii]
    um = um[ii]
    om = om[ii]


    # build the scatter plot
    # inspired by http://matplotlib.org/examples/pylab_examples/scatter_hist.html
    # by the matplotlib development team

    # definitions for the axes
    left, width = 0.1, 0.65
    bottom, height = 0.1, 0.65
    bottom_h = left_h = left + width + 0.02

    rect_scatter = [left, bottom, width, height]
    rect_histx = [left, bottom_h, width, 0.2]
    rect_histy = [left_h, bottom, 0.2, height]

    # start with a rectangular Figure
    plt.figure(figsize=figsize)

    axScatter = plt.axes(rect_scatter)
    axHistx = plt.axes(rect_histx)
    axHisty = plt.axes(rect_histy)

    nullfmt = plt.NullFormatter()
    axHistx.xaxis.set_major_formatter(nullfmt)
    axHisty.yaxis.set_major_formatter(nullfmt)

    axScatter.scatter(um, om, c=c)
    axScatter.set_xlabel('Underlay intensity')
    axScatter.set_ylabel('Overlay intensity')

    umax, omax = map(np.max, (um, om))

    axScatter.set_xlim((0, umax))
    axScatter.set_ylim((0, omax))

    ubins = np.arange(histbincount + 1) * (umax / histbincount)
    obins = np.arange(histbincount + 1) * (omax / histbincount)

    axHistx.hist(um, bins=ubins)
    axHisty.hist(om, bins=obins, orientation='horizontal')

    axHistx.set_xlim(axScatter.get_xlim())
    axHisty.set_ylim(axScatter.get_ylim())

    title = '%s / %s (r=%.3f)' % (split(ulay)[1], split(olay)[1], np.corrcoef(um, om)[0, 1])
    axHistx.set_title(title)

    plt.draw()

    if output_fn is not None:
        plt.savefig(output_fn)

def get_parser():
    description = '%s version %s\n%s' % \
                        (_NAME, _VERSION, _AUTHOR)
    epilog = '''
    Example:

        %s  ulay+tlrc.HEAD olay+tlrc.HEAD combined_image_out.png

    If NIFTI files are supplied to this function then they have to be in
    correspondence. If AFNI files are supplied the overlay is resampled to
    the grid of the underlay.
        ''' % __file__

    formatter_class = argparse.ArgumentDefaultsHelpFormatter

    p = argparse.ArgumentParser(description=description,
                                     epilog=epilog,
                                     formatter_class=formatter_class)
    p.add_argument("ulay", help="Underlay file name")
    p.add_argument("olay", help="Overlay file name")
    p.add_argument("output_fn", help="Output file name")
    p.add_argument('-u', '--ulay_range', nargs=2, default=None, type=float, help='Underlay range (min max)')
    p.add_argument('-o', '--olay_range', nargs=2, default=None, type=float, help='Overlay range  (min max)')
    p.add_argument('-d', '--dims', nargs='?', default=[0, 1, 2], type=int, help='Dimensions to plot; 0=x, 1=y, 2=z')
    p.add_argument('-p', '--pos', nargs='?', default=[.35, .45, .55, .65], type=float, help='Relative slice positions in range 0..1')
    p.add_argument('-t', '--title', default=None, help='Plot title')

    return p
'''
if __name__ == '__main__':
    p = get_parser()
    args = p.parse_args()
    v = vars(args)
    make_plot(v)
'''

########NEW FILE########
__FILENAME__ = lib_prep_afni_surf
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
This is a library for anatomical preprocessing for surface-based voxel
selection. Typically it is used by a wrapper function prep_afni_surf.

%s

Created on Jan 29, 2012

@author: nick
'''

__usage_doc__ = """
It provides functionality to:

- convert FreeSurfer surfaces to AFNI/SUMA format (using SUMA_Make_Spec_FS).

- resample surfaces to standard topology (using MapIcosahedron) at various
resolutions.

- generate additional surfaces by averaging existing ones.

- coregistration of freesurfer output to AFNI/SUMA anatomical or functional
volume (using align_epi_anat.py).

- run @AddEdge to visualize coregistration.

- merge left (lh) and right (rh) hemispheres into single files (mh).

- generate various views of left+right inflated surfaces.

- generate SUMA specification files, and see_suma* shell scripts.

This script assumes a processing pipeline with FreeSurfer for surface
reconstruction, and AFNI/SUMA for coregistration and visualization.
Specifically it is assumed that surfaces have been reconstructed
using FreeSurfer's recon-all. More details can be found in the
documentation available at http://surfing.sourceforge.net

If EPIs from multiple sessions are aligned, this script should use
different directories for refdir for each session, otherwise naming
conflicts may occur.

This function does not resample or transform any functional data. Instead,
surfaces are transformed to be in alignment with ANATVOL or EPIVOL.

For typical usage it requires three arguments:
(1) "-e epi_filename"  or  "-a anat_filename"
(2) "-d freesurfer/directory/surf"
(3) "-r outputdir"'
(*) "-T" [if "epi_filename" / "anat_filename" is in template (MNI/Tal) space"

Notes:

- Please check alignment visually.

- Nifti (.nii or .nii.gz) files are supported, but AFNI may not be able
to read the {S,Q}-form information properly. If functional data was
preprocessed with a program other than AFNI, please check alignment
visually with another program than AFNI, such as MRIcron.

"""

__doc__ %= __usage_doc__


from mvpa2.support.nibabel import surf, afni_suma_spec
from mvpa2.support.afni import afni_utils as utils


import os
import fnmatch
import datetime
import re
import argparse
import sys

_VERSION = "0.1"
__all__ = ['run_afni_anat_preproc']

def getdefaults():
    '''set up default parameters - for testing now'''
    d = { #  options that must be set properly

       # usually these defaults are fine
       'overwrite':False, # delete directories and files before running commands - BE CAREFUL with this option
       'hemi':'l+r', # hemispheres to process, usually l+r
       'mi_icopat':'ico%d_', # pattern for icosehedron output; placeholder is for ld
       'expvol_ss':True,
       'verbose':True,
       'al2expsuffix':'_al2exp',
       'sssuffix':'_ss',
       'alsuffix':'_al',
       'hemimappingsuffix':'hemi_correspondence.1D',
       'outvol_space':None, # output space, one of 'MNI', 'TLRC', 'MNI_ANAT', None, 'ORIG'
       'outvol_ext': None, # '+tlrc','+orig','.nii','.nii.gz'
       'outvol_fullext':None
       }

    return d

def format2extension(fmt):
    if type(fmt) is dict:
        fmt = fmt['surfformat'] # config
    return dict(ascii='.asc', gifti='.surf.gii')[fmt]

def format2type(fmt):
    if type(fmt) is dict:
        fmt = fmt['surfformat'] # config
    return dict(ascii='fs', gifti='gii')[fmt]

def format2spectype(fmt):
    if type(fmt) is dict:
        fmt = fmt['surfformat'] # config
    return dict(ascii='FreeSurfer', gifti='GIFTI')[fmt]

def checkconfig(config):
    # for now, assume input is valid

    pass

def augmentconfig(c):
    '''add more configuration values that are *derived* from the current configuration,
    and also checks whether some options are set properly'''

    # ensure surfdir is set properly.
    # normally it should end in 'surf'

    # try to be smart and deduce subject id from surfdir (if not set explicitly)
    surfdir = c['surfdir']
    if surfdir:
        surfdir = os.path.abspath(surfdir)
        parent, nm = os.path.split(surfdir)

        if nm != 'surf':
            jn = os.path.join(surfdir, 'surf')
            if os.path.exists(jn):
                surfdir = jn
        elif nm == 'SUMA':
            surfdir = parent

        if not (os.path.exists(surfdir) or os.path.split(surfdir)[1] == 'surf'):
            print('Warning: surface directory %s not does exist or does not end in "surf"' % surfdir)
            surfdir = None

        c['surfdir'] = surfdir

        # set SUMADIR as well
        c['sumadir'] = '%(surfdir)s/SUMA/' % c

    # derive subject id from surfdir

    sid = os.path.split(os.path.split(surfdir)[0])[1] if surfdir else None

    if c.get('sid') is None:
        c['sid'] = sid

    if c['sid'] is None:
        print"Warning: no subject id specified"


    c['prefix_sv2anat'] = 'SurfVol2anat'

    # update steps
    if c.get('steps', 'all') == 'all':
        c['steps'] = 'toafni+mapico+moresurfs+skullstrip+align+makespec+makespecboth+makesurfmasks'



    hasanatvol = 'anatvol' in c and c['anatvol']
    hasepivol = 'epivol' in c and c['epivol']
    hasexpvol = 'expvol' in c and c['expvol']
    hasisepi = 'isepi' in c

    if hasexpvol:
        if hasanatvol or hasepivol:
            raise Exception("expvol specified, but also anatvol or epivol - illegal!")
        if not hasisepi:
            raise Exception("not specified whether expvol is EPI (yes) or anat (no)")

    else:
        if hasanatvol:
            if 'epivol' in c and c['epivol']:
                raise Exception("Cannot have both anatvol and epivol")
            else:
                c['expvol'] = c['anatvol']
                c['isepi'] = False
                del(c['anatvol'])
        else:
            if hasepivol:
                c['expvol'] = c['epivol']
                c['isepi'] = True
                del(c['epivol'])
            else:
                print("Warning: no anatomical or functional experimental voume defined")

    def yesno2bool(d, k): # dict, key
        '''Assuming d[k] contains the word 'yes' or 'no', makes d[k] a boolean
        (True=='yes',False=='no'); otherwise an exception is thrown. The dict is updated'''
        val = d[k]
        if val is None:
            b = False
        elif type(val) == bool:
            b = val
        else:
            v = val.lower()
            if v == 'yes':
                b = True
            elif v == 'no':
                b = False
            else:
                raise Exception("Not yes or no: %s" % val)
        d[k] = b

    yesno2bool(c, 'AddEdge')

    if c['identity']:
        c['expvol_ss'] = c['anatval_ss'] = False
    else:
        yesno2bool(c, 'expvol_ss')
        yesno2bool(c, 'isepi')

    # see if we can get the fs_sid
    # (only if surfdir is set properly)
    # XXX not sure if this still makes sense
    c['fs_sid'] = None
    surfdir = c.get('surfdir', None)
    if not surfdir is None and os.path.exists(surfdir):
        fs_log_fn = os.path.join(surfdir, '..', 'scripts', 'recon-all.done')
        print "Looking in %s" % fs_log_fn
        if os.path.exists(fs_log_fn):
            with open(fs_log_fn) as f:
                lines = f.read().split('\n')
                for line in lines:
                    if line.startswith('SUBJECT'):
                        fs_sid = line[8:]
                        c['fs_sid'] = fs_sid
                        print "Found Freesurfer sid %s" % fs_sid
                        break

    if c['fs_sid'] is None:
        c['fs_sid'] = sid
        print "Unable to find proper Freesurfer sid"

    pathvars = ['anatvol', 'expvol', 'epivol', 'refdir', 'surfdir']
    for pathvar in pathvars:
        if pathvar in c and c[pathvar]:
            c[pathvar] = os.path.abspath(c[pathvar])
            print "Set absolute path for %s: %s" % (pathvar, c[pathvar])

    if c['template'] and c['notemplate']:
        error('Cannot have both template and notemplate')

    if 'expvol' in c:
        p, n, o, e = utils.afni_fileparts(c['expvol'])

        if c.get('outvol_space', None) is None:
            cmd = '3dAttribute TEMPLATE_SPACE %s' % c['expvol']
            outvol_space = utils.cmd_capture_output(cmd)
            outvol_space = outvol_space.split('~')[0].strip()
            if len(outvol_space) and not c['notemplate'] and outvol_space.lower() != 'orig':
                print "Detected TEMPLATE_SPACE=%s" % outvol_space
                c['outvol_space'] = outvol_space
                if o == '+orig':
                    o = '+tlrc'
                    print "Template space '%s' detected: output has extension %s" % (outvol_space, o)
                c['template'] = True
            else:
                c['outvol_space'] = '+orig'

        if len(o): #'+orig' or '+tlrc'
            c['outvol_ext'] = o
            c['outvol_fullext'] = o + e
            c['outvol_view'] = o[1:]
        else:
            # For NIFTI - output in orig or tlrc
            c['outvol_view'] = 'tlrc' if c['template'] else 'orig'
            c['outvol_ext'] = '+' + c['outvol_view']
            c['outvol_fullext'] = c['outvol_ext'] + '.HEAD'


    return c

def getenv():
    '''returns the path environment
    As a side effect we ensure to set for FreeSurfer's HOME'''
    env = os.environ

    if 'FREESURFER_HOME' not in env:
        env['FREESURFER_HOME'] = env['HOME'] # FreeSurfer requires this var, even though we don't use it

    return env

def run_toafni(config, env):
    '''convert surfaces to AFNI (or SUMA, rather) format'''
    cmds = []

    sd = config['sumadir']
    sid = config['sid']

    if sid is None:
        raise ValueError("Subject id is not set, cannot continue")
    fs_sid = config['fs_sid']

    # files that should exist if Make_Spec_FS was run successfully
    checkfns = ['brainmask.nii',
              'T1.nii',
              'aseg.nii']

    filesexist = all([os.path.exists('%s/%s' % (sd, fn)) for fn in checkfns])

    if config['overwrite'] or not filesexist:
        if config['overwrite']:
            if filesexist:
                cmds.append('rm -rf "%s"' % sd)
        cmds.append('cd %(surfdir)s;@SUMA_Make_Spec_FS -sid %(sid)s -no_ld' % config)
        utils.run_cmds(cmds, env)
    else:
        print "SUMA conversion appears to have been performed already for %s in %s" % (sid, sd)



def run_mapico(config, env):
    '''run MapIcosehedron to convert surfaces to standard topology'''
    sumadir = config['sumadir']
    firstcmd = 'cd "%s" || exit 1' % sumadir
    cmds = []
    icolds, hemis = _get_hemis_icolds(config)
    sid = config['sid'] # subject id
    ext = '.asc' # input is always ascii
    for icold in icolds:
        icoprefix = config['mi_icopat'] % icold
        spherefns = []
        for hemi in hemis:
            if not config['overwrite']:
                # the last file that is generated by MapIcosahedron
                lastsurffn = '%s/%s%sh.sphere.reg%s' % (sumadir, config['mi_icopat'] % icold, hemi, ext)
                spherefns.append(lastsurffn)
                if os.path.exists(lastsurffn):
                    print("Seems MapIcosahedron was already run for %sh with ld=%d" % (hemi, icold))
                    continue

            cmd = ('MapIcosahedron -overwrite -spec %s_%sh.spec -ld %d -fix_cut_surfaces -prefix %s' %
                       (sid, hemi, icold, icoprefix))
            cmds.append(cmd)
        if cmds:
            cmd = '%s;%s' % (firstcmd, ';'.join(cmds))
            utils.run_cmds(cmd, env)
            cmds = []
        if len(spherefns) == 2 and 'l' in hemis and 'r' in hemis:
            spheres = map(surf.read, spherefns)

            mapfn = (config['mi_icopat'] % icold) + config['hemimappingsuffix']
            mappathfn = os.path.join(sumadir, mapfn)


            if config['overwrite'] or not os.path.exists(mappathfn):
                eps = .001
                print "Computing bijection between nodes (ico=%d) - this may take a while" % icold
                bijection = surf.get_sphere_left_right_mapping(spheres[0],
                                                                spheres[1],
                                                                eps)

                with open(mappathfn, 'w') as f:
                    f.write('\n'.join(map(str, bijection)))

                    print "Written bijection to %s" % mappathfn




def run_moresurfs(config, env):
    '''Generates additional surfaces in the SUMA dir by averaging existing ones. '''

    # surface1, surface2, and output (averaged) output
    moresurfs = [("smoothwm", "pial", "intermediate"),
                ("intermediate", "inflated", "semiinflated"),
                ("semiinflated", "inflated", "tqinflated")]

    icolds, hemis = _get_hemis_icolds(config)
    d = config['sumadir']
    ext = '.asc' # always AFNI in this stage
    for icold in icolds:
        icoprefix = config['mi_icopat'] % icold
        for hemi in hemis:
            for surftriple in moresurfs:
                fns = ['%s/%s%sh.%s%s' % (d, icoprefix, hemi, surfname, ext)
                     for surfname in surftriple]
                if config['overwrite'] or not os.path.exists(fns[2]):
                    average_fs_asc_surfs(fns[0], fns[1], fns[2])
                else:
                    print "%s already exists" % fns[2]

def _set_vol_space_cmd(fn, config):
    param = ''
    del_cmd = ''

    p, n, o, e = utils.afni_fileparts(fn)

    space = config.get('outvol_space', None)
    if space is not None:
        param += ' -space %s' % space

    view = config.get('outvol_view')
    if view is not None:
        param += ' -view %s' % view

        if len(o) and o[0] == '+' and o[1:].lower() != view.lower():
            trgfn = '%s/%s+%s' % (p, n, view.lower())
            del_cmd = 'rm %s*; ' % trgfn


    if param:
        p, n, o, e = utils.afni_fileparts(fn)
        cmd = 'cd %s; %s 3drefit -overwrite %s %s' % (p, del_cmd, param, n + o + e)
    else:
        cmd = 'echo "Not changing view or space for %s"' % fn
    return cmd

def _convert_vol_space_to_orig_cmd(fn):
    p, n, o, e = utils.afni_fileparts(fn)
    if o == '+orig':
        return "echo '%s' is already in orig space - no conversion" % fn

    t = '__tmp_foo_'
    while os.path.exists(p + '/' + t + o + e):
        t += 'x'

    cmds = ['cd %s' % p,
          '3drename -overwrite %s%s %s%s' % (n, o, t, o),
          '3drefit -view orig %s%s' % (t, o),
          '3drename -overwrite %s+orig %s+orig' % (t, n)]

    return ';'.join(cmds)



def run_skullstrip(config, env):
    fullext = config['outvol_fullext']

    overwrite = config['overwrite']
    refdir = config['refdir']
    cmds = []
    if not os.path.exists(refdir):
        cmds.append('mkdir %(refdir)s' % config)

    sumadir = config['sumadir']
    sid = config['sid']
    fs_sid = config['fs_sid']

    if not sid:
        raise ValueError("Subject id is not set, cannot continue")

    # process the surfvol anatomical.
    # because it's already skull stripped by freesurfer
    # simply copy it over; rename brain.nii to surfvol_ss
    surfvol_srcs = ['%s/%s' % (sumadir, fn)
                  for fn in ['brain.nii',
                             'T1.nii']]

    surfvol_trgs = ['%s/%s' % (refdir, fn)
                  for fn in ['%s_SurfVol_ss%s' % (sid, fullext),
                             '%s_SurfVol%s' % (sid, fullext)]]

    for src, trg in zip(surfvol_srcs, surfvol_trgs):
        if os.path.exists(trg) and not overwrite:
            print '%s already exists' % trg
        else:
            t_p, t_n, t_o, t_e = utils.afni_fileparts(trg)
            trg_short = '%s%s' % (t_n, t_o)
            cmds.append('cd "%s"; 3dresample -overwrite -orient LPI -inset %s -prefix ./%s' %
                        (refdir, src, trg_short))
            cmds.append(_set_vol_space_cmd('%s/%s+orig' % (refdir, t_n), config))

    # process experimental volume.
    expvol_src = config['expvol']
    do_ss = config['expvol_ss']
    [e_p, e_n, e_o, e_e] = utils.afni_fileparts(expvol_src)

    expvol_trg_prefix = '%s%s' % (e_n, config['sssuffix'] if do_ss else '')
    expvol_trg_tmp_prefix = '__tmp_%s' % expvol_trg_prefix

    expvol_trg = '%s/%s%s' % (refdir, expvol_trg_prefix, fullext)

    print "Attempt %s -> %s" % (expvol_src, expvol_trg)

    ext = config['outvol_ext']

    if overwrite or not utils.afni_fileexists(expvol_trg):
        if do_ss:
            cmds.append('cd "%s";3dSkullStrip -overwrite -prefix ./%s%s -input %s' %
                            (refdir, expvol_trg_tmp_prefix, ext, expvol_src))
        else:
            cmds.append('cd "%s";3dbucket -overwrite -prefix ./%s%s %s' %
                            (refdir, expvol_trg_tmp_prefix, ext, expvol_src))
        cmds.append('cd "%s"; 3dresample -overwrite -orient LPI -prefix %s -inset %s%s' %
                            (refdir, expvol_trg_prefix, expvol_trg_tmp_prefix, ext))
        cmds.append('rm %s/%s*' % (refdir, expvol_trg_tmp_prefix))
        cmds.append(_set_vol_space_cmd(expvol_trg, config))
    else:
        print "No skull strip because already exists: %s%s" % (expvol_trg_prefix, ext)

    utils.run_cmds(cmds, env)

def run_alignment(config, env):
    '''Aligns anat (which is assumed to be aligned with EPI data) to FreeSurfer SurfVol

    This function strips the anatomicals (by default), then uses align_epi_anat.py
    to estimate the alignment, then applies this transformation to the non-skull-stripped
    SurfVol and also to the surfaces. Some alignment headers will be nuked'''
    overwrite = config['overwrite']
    alignsuffix = config['al2expsuffix']
    refdir = config['refdir']

    fullext = config['outvol_fullext']
    ext = config['outvol_ext']

    if config['sid'] is None:
        raise ValueError('Need sid')

    cmds = []
    if not os.path.exists(config['refdir']):
        cmds.append('mkdir %(refdir)s' % config)

    # two volumes may have to be stripped: the inpput anatomical, and the surfvol.
    # put them in a list here and process them similarly
    surfvol = '%(refdir)s/%(sid)s_SurfVol%(outvol_fullext)s' % config
    surfvol_ss = '%(refdir)s/%(sid)s_SurfVol%(sssuffix)s%(outvol_fullext)s' % config

    e_p, e_n, _, _ = utils.afni_fileparts(config['expvol'])
    if config['expvol_ss']:
        e_n = '%s%s' % (e_n, config['sssuffix'])
    expvol = '%s/%s%s' % (refdir, e_n, fullext)

    volsin = [surfvol_ss, expvol]
    for volin in volsin:
        if not os.path.exists(volin):
            raise ValueError('File %s does not exist' % volin)

    a_n = utils.afni_fileparts(volsin[0])[1] # surfvol input root name
    ssalprefix = '%s%s' % (a_n, alignsuffix)

    unity = "1 0 0 0 0 1 0 0 0 0 1 0" # we all like unity, don't we?

    fullmatrixfn = '%s_mat.aff12.1D' % ssalprefix
    aloutfns = ['%s%s' % (ssalprefix, fullext), fullmatrixfn] # expected output files if alignment worked
    if config['overwrite'] or not all([os.path.exists('%s/%s' % (refdir, f)) for f in aloutfns]):
        alignedfn = '%s/%s%s' % (refdir, ssalprefix, fullext)

        if config['identity']:
            fullmatrix_content = '"MATRIX(%s)"' % unity.replace(" ", ",")

            cmd = 'cd "%s"; cat_matvec %s > %s; 3dcopy -overwrite %s%s %s%s%s' % (refdir, fullmatrix_content, fullmatrixfn, a_n, ext, a_n, alignsuffix, ext)
        else:
            # use different inputs depending on whether expvol is EPI or ANAT
            twovolpat = ('-anat %s -epi %s -anat2epi -epi_base 0 -anat_has_skull no -epi_strip None' if config['isepi']
                       else '-dset1 %s -dset2 %s -dset1to2 -dset1_strip None -dset2_strip None')
            # use this pattern to generate a suffix
            twovolsuffix = twovolpat % (volsin[0], volsin[1])

            aea_opts = config['aea_opts']
            if config['template']:
                aea_opts += " -Allineate_opts '-maxrot 10 -maxshf 10 -maxscl 1.5'"
            # align_epi_anat.py
            cmd = 'cd "%s"; align_epi_anat.py -overwrite -suffix %s %s %s' % (refdir, alignsuffix, twovolsuffix, aea_opts)

        cmds.append(cmd)

        cmds.append(_set_vol_space_cmd(alignedfn, config))

        utils.run_cmds(cmds, env)

    else:
        print "Alignment already done - skipping"

        # run these commands first, then check if everything worked properly


    cmds = []

    # see if the expected transformation file was found
    if not config['identity'] and not os.path.exists('%s/%s' % (refdir, fullmatrixfn)):
        raise Exception("Could not find %s in %s" % (fullmatrixfn, refdir))

    # now make a 3x4 matrix
    matrixfn = '%s%s.A2E.1D' % (a_n, alignsuffix)
    if overwrite or not os.path.exists('%s/%s' % (refdir, matrixfn)):
        cmds.append('cd "%s"; cat_matvec %s > %s || exit 1' % (refdir, fullmatrixfn, matrixfn))


    # make an aligned, non-skullstripped version of SurfVol in refdir
    alprefix = '%s_SurfVol%s' % (config['sid'], alignsuffix)
    svalignedfn = '%s/%s%s' % (refdir, alprefix, fullext)

    newgrid = 1 # size of anatomical grid in mm. We'll have to resample, otherwise 3dWarp does
              # not respect the corners of the volume (as of April 2012)

    if overwrite or not os.path.exists(svalignedfn):
        #if not config['fs_sid']:
        #    raise ValueError("Don't have a freesurfer subject id - cannot continue")

        #surfvolfn = '%s/%s_SurfVol+orig' % (config['sumadir'], config['fs_sid'])
        surfvolfn = '%s/T1.nii' % config['sumadir']
        cmds.append('cd "%s";3dWarp -overwrite -newgrid %f -matvec_out2in `cat_matvec -MATRIX %s` -prefix ./%s %s' %
                    (refdir, newgrid, matrixfn, alprefix, surfvolfn))
        cmds.append(_set_vol_space_cmd('%s/%s+orig' % (refdir, alprefix), config))

    else:
        print '%s already exists - skipping Warp' % svalignedfn

    utils.run_cmds(cmds, env)
    cmds = []

    # nuke afni headers
    headernukefns = ['%s%s' % (f, fullext) for f in [ssalprefix, alprefix]]
    headernukefields = ['ALLINEATE_MATVEC_B2S_000000',
                      'ALLINEATE_MATVEC_S2B_000000',
                      'WARPDRIVE_MATVEC_FOR_000000',
                      'WARPDRIVE_MATVEC_INV_000000']

    for fn in headernukefns:
        for field in headernukefields:
            # nuke transformation - otherwise AFNI does this unwanted transformation for us
            fullfn = '%s/%s' % (refdir, fn)

            if not (os.path.exists(fullfn) or config['identity']):
                raise ValueError("File %r does not exist" % fullfn)

            refitcmd = "3drefit -atrfloat %s '%s' %s" % (field, unity, fn)

            # only refit if not already in AFNI history (which is stored in HEADfile)
            cmd = 'cd "%s"; m=`grep "%s" %s | wc -w`; if [ $m -eq 0 ]; then %s; else echo "File %s seems already 3drefitted"; fi' % (refdir, refitcmd, fn, refitcmd, fn)
            cmds.append(cmd)
    utils.run_cmds('; '.join(cmds), env)
    cmds = []

    # run AddEdge so that volumes can be inspected visually for alignment
    if config['AddEdge']:
        use_ss = config['expvol_ss']

        # ae_{e,s}_n are AddEdge names for expvol and surfvol
        ae_e_n = utils.afni_fileparts(config['expvol'])[1]
        if use_ss:
            ae_e_n += config['sssuffix']
        ae_s_n = ssalprefix #if use_ss else alprefix

        # *_ne have the output extension as well
        ae_e_ne = ae_e_n + ext
        ae_s_ne = ae_s_n + ext

        addedge_fns = ['%s/_ae.ExamineList.log' % refdir]

        exts = ['HEAD', 'BRIK']
        orig_ext = '+orig'
        addedge_rootfns = ['%s_%s%%s' % (ae_e_n, postfix)
                            for postfix in ['e3', 'ec', ae_s_n + '_ec']]
        addedge_rootfns.extend(['%s_%s%%s' % (ae_s_n, postfix)
                            for postfix in ['e3', 'ec']])

        addedge_fns_pat = ['%s.%s' % (fn, e) for fn in addedge_rootfns for e in exts]

        addegde_pathfns_orig = map(lambda x:os.path.join(refdir, x % '+orig'), addedge_fns_pat) + addedge_fns
        addegde_pathfns_ext = map(lambda x:os.path.join(refdir, x % ext), addedge_fns_pat)
        addegde_exists = map(os.path.exists, addegde_pathfns_ext)
        if overwrite or not all(addegde_exists):
            ae_ns = (ae_e_n, ae_s_n)

            cmds.extend(map(lambda fn : 'if [ -e "%s" ]; then rm "%s"; fi' % (fn, fn), addegde_pathfns_orig + addegde_pathfns_ext))
            cmds.append(';'.join(['cd %s' % refdir] +
                                 [_convert_vol_space_to_orig_cmd('%s/%s%s' % (refdir, n, ext))
                                            for n in ae_ns] +
                                 ['\@AddEdge %s+orig %s+orig' % ae_ns]))

            set_space_fns = addegde_pathfns_orig + ['%s/%s%s.%s' % (refdir, fn, orig_ext, exts[0]) for fn in ae_ns]

            for fn in set_space_fns: #['%s/%s' % (refdir, fn % orig_ext) for fn in addedge_fns_pat]:
                if fn.endswith('.log'):
                    continue
                cmds.append('if [ -e %s ]; then %s; fi' % (fn, _set_vol_space_cmd(fn, config)))

            utils.run_cmds(cmds, env)
            cmds = []

        else:
            print "AddEdge seems to have been run already"

        sid = config['sid']
        plot_slice_fns = [(ae_e_n + '_e3', ae_s_n + '_e3', '%s_qa_e3.png' % sid),
                          (None, ae_e_n + '_' + ae_s_n + '_ec', '%s_qa_ec.png' % sid)]


        plot_slice_imgfns = ['%s/%s' % (refdir, fn) for fn in plot_slice_fns]
        if overwrite or not all(map(os.path.exists, plot_slice_imgfns)):
            slice_dims = [0, 1, 2]
            slice_pos = [.35, .45, .55, .65]
            for fns in plot_slice_fns:
                input_fns = []
                for i, fn in enumerate(fns):
                    if fn is not None:
                        fn = '%s/%s' % (refdir, fn)
                        if i <= 1:
                            fn += ext
                    input_fns.append(fn)

                fn1, fn2, fnout = input_fns
                if not os.path.exists(fnout):
                    _make_slice_plot(fn1, fn2, fnout)
                    print "QA Image saved to %s" % fnout
                else:
                    print "Already exists: %s" % fnout
        else:
            print "QA images already exist"



    # because AFNI uses RAI orientation but FreeSurfer LPI, make a new
    # affine transformation matrix in which the signs of
    # x and y coordinates are negated before and after the transformation
    matrixfn_LPI2RAI = '%s.A2E_LPI.1D' % ssalprefix
    if overwrite or not os.path.exists('%s/%s' % (refdir, matrixfn_LPI2RAI)):
        lpirai = '"MATRIX(-1,0,0,0,0,-1,0,0,0,0,1,0)"'
        cmd = ('cd %s; cat_matvec -ONELINE %s `cat_matvec -MATRIX %s` %s > %s' %
             (refdir, lpirai, matrixfn, lpirai, matrixfn_LPI2RAI))
        cmds.append(cmd)

    # apply transformation to surfaces
    [icolds, hemis] = _get_hemis_icolds(config)
    sumadir = config['sumadir']
    sumafiles = os.listdir(sumadir)


    origext = '.asc'
    ext = format2extension(config)
    tp = format2type(config)
    # process all hemispheres and ld values
    for icold in icolds:
        for hemi in hemis:
            pat = '%s%sh.?*%s' % (config['mi_icopat'] % icold, hemi, origext)
            for sumafile in sumafiles:
                if fnmatch.fnmatch(sumafile, pat):
                    if not sumafile.endswith(origext):
                        raise ValueError("%s does not end with %s" % (sumafile, origext))
                    #s = sumafile.split(".")
                    #s[len(s) - 2] += config['alsuffix'] # insert '_al' just before last dot
                    #alsumafile = ".".join(s)
                    extsumafile = sumafile[:-len(origext)]
                    alsumafile = extsumafile + config['alsuffix'] + ext

                    if config['overwrite'] or not os.path.exists('%s/%s' % (refdir, alsumafile)):
                        # now apply transformation
                        cmd = 'cd "%s";ConvertSurface -overwrite -i_fs %s/%s -o_%s ./%s -ixmat_1D %s' % \
                              (refdir, sumadir, sumafile, tp, alsumafile, matrixfn_LPI2RAI)
                        cmds.append(cmd)

                    # as of June 2012 copy the original sphere.reg (not aligned) as well
                    if sumafile == ('%s.sphere.reg%s' % (pat, ext)):
                        sumaout = '%s/%s' % (refdir, extsumafile + ext)
                        if config['overwrite'] or not os.path.exists(sumaout):
                            s = surf.read('%s/%s' % (sumadir, sumafile))
                            surf.write(s, sumaout)
                            #cmds.append('cp %s/%s %s/%s' % (sumadir, sumafile, refdir, sumafile))


        mapfn = (config['mi_icopat'] % icold) + config['hemimappingsuffix']
        srcpathfn = os.path.join(sumadir, mapfn)

        if os.path.exists(srcpathfn):
            trgpathfn = os.path.join(refdir, mapfn)
            if not os.path.exists(trgpathfn) or config['overwrite']:
                cmds.append('cp %s %s' % (srcpathfn, trgpathfn))

    utils.run_cmds(cmds, env)

def _make_slice_plot(ulay, olay, fnout, raise_=False):
    if raise_:
        from mvpa2.support.afni import lib_plot_slices
    else:
        try:
            from mvpa2.support.afni import lib_plot_slices
        except:
            print "No slice plotting supported"
            return

    slice_dims = [0, 1, 2]
    slice_pos = [.35, .45, .55, .65]

    lib_plot_slices.make_plot(ulay, olay, slice_dims, slice_pos, output_fn=fnout)


def run_makespec(config, env):
    '''Generates the SUMA specifcation files for all hemispheres and ld values'''
    refdir = config['refdir']
    icolds, hemis = _get_hemis_icolds(config)
    for icold in icolds:
        for hemi in hemis:

            # make spec file
            surfprefix = '%s%sh' % (config['mi_icopat'] % icold, hemi)
            specfn = afni_suma_spec.canonical_filename(icold, hemi,
                                                       config['alsuffix'])
            specpathfn = os.path.join(refdir, specfn)

            if config['overwrite'] or not os.path.exists(specpathfn):
                suma_makespec(refdir, surfprefix, config['surfformat'], specpathfn, removepostfix=config['alsuffix'])
            else:
                print "Skipping spec for %s" % specpathfn

            # make simple script to run afni and suma
            runsumafn = '%s/%sh_ico%d_runsuma.sh' % (refdir, hemi, icold)
            surfvol = '%(sid)s_SurfVol%(al2expsuffix)s+orig' % config

            if config['overwrite'] or not os.path.exists(runsumafn):
                suma_makerunsuma(runsumafn, specfn, surfvol)

def run_makespec_bothhemis(config, env):
    refdir = config['refdir']
    overwrite = config['overwrite']
    icolds, hemis = _get_hemis_icolds(config)

    ext = format2extension(config)

    if hemis != ['l', 'r']:
        raise ValueError("Cannot run without left and right hemisphere")

    for icold in icolds:
        specs = []
        for hemi in hemis:
            #surfprefix = '%s%sh' % (config['mi_icopat'] % icold, hemi)
            specfn = afni_suma_spec.canonical_filename(icold, hemi,
                                                       config['alsuffix'])
            specpathfn = os.path.join(refdir, specfn)
            s = afni_suma_spec.read(specpathfn)

            specs.append(afni_suma_spec.read(specpathfn))

        add_states = ['inflated', 'full.patch.flat', 'sphere.reg']
        add_states_required = [True, False, True] # flat surface is optional
        for add_state, is_req in zip(add_states, add_states_required):
            has_state = all([len(spec.find_surface_from_state(add_state)) == 1
                                    for spec in specs])

            if not has_state:
                if is_req:
                    error('cannot find state %s' % add_state)
                else:
                    # skip this state
                    print "Optional state %s not found - skipping" % add_state
                    continue

            specs = afni_suma_spec.hemi_pairs_add_views(specs,
                            add_state, ext, refdir, overwrite=overwrite)


        spec_both = afni_suma_spec.combine_left_right(specs)


        # generate spec files for both hemispheres
        hemiboth = 'b'
        specfn = afni_suma_spec.canonical_filename(icold, hemiboth, config['alsuffix'])
        specpathfn = os.path.join(refdir, specfn)
        spec_both.write(specpathfn, overwrite=overwrite)

        # merge left and right into one surface
        # and generate the spec files as well
        hemimerged = 'm'
        specfn = afni_suma_spec.canonical_filename(icold, hemimerged, config['alsuffix'])
        specpathfn = os.path.join(refdir, specfn)

        if config['overwrite'] or not os.path.exists(specpathfn):
            spec_merged, surfs_to_join = afni_suma_spec.merge_left_right(spec_both)
            spec_merged.write(specpathfn, overwrite=overwrite)

            full_path = lambda x:os.path.join(refdir, x)
            for fn_out, fns_in in surfs_to_join.iteritems():
                surfs_in = [surf.read(full_path(fn)) for fn in fns_in]

                if all(['full.patch.flat' in fn for fn in fns_in]):
                    # left hemi of flat; rotate 180 degrees, reposition again
                    surfs_in[0] = surfs_in[0] * [-1, -1, 1]
                    surfs_in = surf.reposition_hemisphere_pairs(surfs_in[0], surfs_in[1], 'm')

                surf_merged = surf.merge(*surfs_in)

                if config['overwrite'] or not os.path.exists(full_path(fn_out)):
                    surf.write(full_path(fn_out), surf_merged)
                    print "Merged surfaces written to %s" % fn_out

def run_makesurfmasks(config, env):
    refdir = config['refdir']
    overwrite = config['overwrite']

    if config['sid'] is None:
        raise ValueError('Need sid')

    sumfn = '%s_qa_surf_mask' % config['sid'] # output file

    fullext = config['outvol_fullext']
    volor = config['outvol_ext']

    sumfn_path = '%s/%s%s' % (refdir, sumfn, fullext)
    qafn_path = '%s/%s.png' % (refdir, sumfn)
    checkfn_paths = (sumfn_path, qafn_path)
    if all(map(os.path.exists, checkfn_paths)) and not overwrite:
        print "Already exist: %s" % (", ".join(checkfn_paths))
        return

    icolds, hemis = _get_hemis_icolds(config)

    volexts = ['%s%s' % (volor, e) for e in '.HEAD', '.BRIK*']




    sssuffix = config['sssuffix'] if config['expvol_ss'] else ''
    expvol_fn = '%s%s%s' % (utils.afni_fileparts(config['expvol'])[1],
                            sssuffix,
                            volor)


    #if overwrite or not os.path.exists('%s/%s' % (refdir, sv_al_nii_fn)):
    #    cmd = 'cd %s; 3dcopy -overwrite %s %s' % (refdir, sv_al_orig_fn, sv_al_nii_fn)
    #    utils.run_cmds(cmd, env)


    if hemis != ['l', 'r']:
        raise ValueError("Cannot run without left and right hemisphere")

    icold = max(icolds)

    oneDfn = '__t.1D'
    oneDtfn = '__tt.1D' # transposed
    cmds = ['cd %s' % refdir,
             '1deval -1D: -num %d -expr 1 > %s' % (icold ** 2 * 10 + 1, oneDfn),
             '1dtranspose %s > %s' % (oneDfn, oneDtfn)]

    utils.run_cmds(';'.join(cmds), env)


    tmpfns = [oneDfn, oneDtfn]

    s2v_cmd = ('3dSurf2Vol -map_func mask2 -data_expr "a*%%d" -spec %%s %%s -sv %s'
             ' -grid_parent %s. -prefix %%s -sdata %s -overwrite') % \
                                (expvol_fn, expvol_fn, oneDtfn)

    infix2val = {'-surf_A pial':1,
               '-surf_A smoothwm':2,
               '-surf_A smoothwm -surf_B pial -f_steps 20': 4}

    volfns = []
    for hemi in hemis:
        specfn = afni_suma_spec.canonical_filename(icold, hemi,
                                                       config['alsuffix'])

        for infix, val in infix2val.iteritems():
            fnprefix = '__m%d_%sh' % (val, hemi)
            cmd = s2v_cmd % (val, specfn, infix, fnprefix)
            utils.run_cmds('cd %s;%s' % (refdir, cmd))
            tmpfns.extend(['%s%s' % (fnprefix, e) for e in volexts])
            volfns.append(fnprefix + volor)

    cmds = ['cd %s' % refdir]
    catfn = '__cat'
    cmds.extend(['3dTcat -overwrite -prefix %s %s' % (catfn, ' '.join(volfns)),
                 '3dTstat -overwrite -sum -prefix %s %s%s' % (sumfn, catfn, volor)])
    tmpfns.extend(['%s%s' % (catfn, e) for e in volexts])


    cmds.extend('rm %s' % fn for fn in tmpfns)
    cmds.append('echo "Surface mask in %s"' % sumfn)

    utils.run_cmds(';'.join(cmds), env)

    # make plot
    if overwrite or not os.path.exists(qafn_path):
        expvol_path = '%s/%s' % (refdir, expvol_fn)
        _make_slice_plot(expvol_path,
                         sumfn_path,
                         qafn_path)


def suma_makerunsuma(fnout, specfn, surfvol):
    '''Generate a simple script to launch AFNI and SUMA with NIML enabled
    Scripts can be run with ./lh_ico100_seesuma.sh (for left hemisphere and ld=100)'''

    shortspecfn = os.path.split(specfn)[1] # remove path

    lines = ['export SUMA_AllowDsetReplacement=YES',
           'killall afni',
           'afni -niml &'
           'suma -spec %s -sv %s' % (shortspecfn, surfvol)]

    with open(fnout, 'w') as f:
        f.write('\n'.join(lines))
        f.close()
        os.chmod(fnout, 0777)


    print 'Generated run suma file in %s' % fnout


def suma_makespec(directory, surfprefix, surf_format, fnout=None, removepostfix=''):
    '''Generates a SUMA specification file that contains information about
    the different surfaces'''



    postfix = format2extension(surf_format)
    tp = format2type(surf_format)
    pat = '%s.?*%s' % (surfprefix, postfix)

    #removepostfix = config['alsuffix']

    fns = os.listdir(directory)
    surfname2filename = dict()
    for fn in fns:
        if fnmatch.fnmatch(fn, pat):
            surfname = fn[len(surfprefix) + 1:(len(fn) - len(postfix))]

            if surfname.endswith(removepostfix):
                surfname = surfname[:-len(removepostfix)]
            surfname2filename[surfname] = fn

    # only include these surfaces
    usesurfs = ['smoothwm', 'intermediate', 'pial', 'semiinflated',
                 'tqinflated', 'inflated', 'full.patch.flat', 'sphere.reg']
    isanatomical = dict(zip(usesurfs, [True, True, True] + [False] * 5))


    # make the spec file
    lines = []
    lines.append('# Created %s' % str(datetime.datetime.now()))
    lines.append('Group = all')
    lines.append('')

    lines.extend('StateDef = %s' % f for f in usesurfs if f in surfname2filename)

    lines.append('')
    localdomainparent = surfname2filename.get('smoothwm', None)

    ndim = lambda x:2 if 'full.patch' in x else 3

    for surfname in usesurfs:
        if surfname in surfname2filename:
            ldp = ('SAME' if (not localdomainparent or
                              localdomainparent == surfname2filename[surfname])
                         else localdomainparent)
            lines.extend(['NewSurface',
                          'SurfaceFormat = %s' % surf_format.upper(),
                          'SurfaceType = %s' % format2spectype(surf_format),
                          'FreeSurferSurface = %s' % surfname2filename[surfname],
                          'LocalDomainParent = %s' % ldp,
                          'LocalCurvatureParent = %s' % ldp,
                          'SurfaceState = %s' % surfname,
                          'EmbedDimension = %d' % ndim(surfname),
                          'Anatomical = %s' % ('Y' if isanatomical[surfname] else 'N'),
                         ''])
            if localdomainparent is None:
                localdomainparent = surfname2filename[surfname]



    if fnout:
        f = open(fnout, 'w')
        f.write('\n'.join(lines))
        f.close()
        print 'Generated SUMA spec file in %s' % fnout
    else:
        print "No output"




def average_fs_asc_surfs(fn1, fn2, fnout):
    '''averages two surfaces'''
    surf1 = surf.read(fn1)
    surf2 = surf.read(fn2)
    surfavg = surf1 * .5 + surf2 * .5
    surf.write(fnout, surfavg)

def _get_hemis_icolds(config):
    '''returns the icolds (as a list of integers) and the hemispheres (as a list of single characters)'''
    icolds = [int(v) for v in config['ld'].split('+')] # linear divisions for MapIcosehedron
    hemis = config['hemi'].split('+') # list of hemis (usually ['l','r'])
    return (icolds, hemis)


def run_all(config, env):
    '''run commands from all steps specified in config'''
    cmds = []

    print config

    steps = config['steps'].split('+')
    step2func = {'toafni':run_toafni,
               'mapico':run_mapico,
               'moresurfs':run_moresurfs,
               'skullstrip':run_skullstrip,
               'align':run_alignment,
               'makespec':run_makespec,
               'makespecboth':run_makespec_bothhemis,
               'makesurfmasks':run_makesurfmasks}

    if not all(s in step2func for s in steps):
        raise Exception("Illegal step in %s" % steps)

    for step in steps:
        if step in step2func:
            print "Running: %s" % step
            step2func[step](config, env)
        else:
            raise ValueError('Step not recognized: %r' % step)

    return cmds

def getparser():
    description = '''
Anatomical preprocessing to align FreeSurfer surfaces with AFNI data.

%s

Copyright 2010-2012 Nikolaas N. Oosterhof <nikolaas.oosterhof@unitn.it>

''' % __usage_doc__

    epilog = '''
This function is *experimental*; using the --overwrite option may
remove and/or overwrite existing files.'''

    yesno = ["yes", "no"]
    parser = argparse.ArgumentParser(description=description, epilog=epilog, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-v', '--version', action='version', version='prep_afni_surf %s' % _VERSION)
    parser.add_argument("-s", "--sid", required=False, help="subject id used in @SUMA_Make_Spec_FS ")
    parser.add_argument("-d", "--surfdir", required=False, help="FreeSurfer surf/ directory")
    parser.add_argument("-a", "--anatvol", required=False, help="Anatomical that is assumed to be in alignment with the EPI data of interest")
    parser.add_argument("-e", "--epivol", required=False, help="EPI data of interest")
    parser.add_argument('-x', "--expvol", required=False, help="Experimental volume to which SurfVol is aligned")
    parser.add_argument('-E', "--isepi", required=False, choices=yesno, help="Is the experimental volume an EPI (yes) or anatomical (no)")
    parser.add_argument('-r', '-o', "--refdir", required=True, help="Output directory in which volumes and surfaces are in reference to ANATVOL or EPIVOL")
    parser.add_argument("-p", "--steps", default='all', help='Processing steps separated by "+"-characters. "all" is the default and equivalent to "toafni+mapico+moresurfs+skullstrip+align+makespec+makespecboth+makesurfmasks"')
    parser.add_argument("-l", "--ld", default="4+8+16+32+64+128", help="MapIcosahedron linear devisions, e.g. 80, or 16+96 (for both 16 or 96). The default is 4+8+16+32+64+128")
    parser.add_argument("--overwrite", action='store_true', default=False, help="Overwrite existing files")
    parser.add_argument('--hemi', default='l+r', choices=['l', 'r', 'l+r'], help='Hemispheres to process ([l+r])')
    parser.add_argument('-S', "--expvol_ss", default='yes', choices=yesno, help='Skull strip experimental volume ([yes],no)')
    parser.add_argument('--aea_opts', default='-cmass cmass+xyz -big_move', help="Options given to align_epi_anat ([-cmass cmass+xyz -big_move])")
    parser.add_argument('-I', '--identity', action="store_true", default=False, help="Use identity transformation between SurfVol and anat/epivol (no alignment)")
    parser.add_argument('-A', '--AddEdge', default='yes', choices=yesno, help="Run AddEdge on aligned volumes ([yes])")
    parser.add_argument('-f', '--surfformat', default='ascii', choices=['gifti', 'ascii'], help="Output format of surfaces: 'ascii' (default - for now) or 'gifti'")
    parser.add_argument('-T', '--template', action="store_true", default=False, help="Indicate that the experimental volume (suppplied by '-e', '-a', or '-x') is in template space. This will add \"-Allineate_opts '-maxrot 10 -maxshf 10 -maxscl 1.5'\" to --aea_opts")
    parser.add_argument('-t', '--notemplate', action="store_true", default=False, help="Indicate that the experimental volume (suppplied by '-e', '-a', or '-x') is not in template space. ")

    # expvol_space  (template space) MNI, TLRC, MNI_ANAT, None
    # expvol_ext      +tlrc, +orig, .nii, .nii.gz
    # expvol_fullext  +tlrc.HEAD (set based on

    return parser

def getoptions():
    parser = getparser()
    args = None

    namespace = parser.parse_args(args)
    return vars(namespace)

def _test_me(config):
    datadir = os.path.abspath('.') + '/' #/Users/nick/Downloads/subj1/'
    refdir = datadir + '_test_ref'
    surfdir = datadir + '/subj1/surf'

    refs = ['-e bold_mean.nii', '-a anat.nii', '-e bold_mean+orig', '-a anat+orig',
            '-e bold_mean_ss.nii', '-a anat_ss+orig']


    for i, ref in enumerate(refs):
        tp, fn = ref.split(' ')
        do_ss = not ('_ss' in fn)
        c = getdefaults()

        if 'refdir' in config:
            refdir = os.path.abspath('%s%s_%d' % (datadir, config['refdir'], i))

        c.update(dict(isepi=tp == '-e', refdir=refdir, expvol=datadir + fn, surfdir=surfdir,
                      identity=False, expvol_ss=do_ss, AddEdge=True, steps='all', ld="4+32",
                      aea_opts='-cmass cmass+xyz -big_move', alsuffix='_al', verbose=True))
        #c.update(config)

        c['overwrite'] = False # i == 0 and utils.which('mris_convert')

        env = getenv()
        c = augmentconfig(c)
        print c
        run_all(c, env)

def run_prep_afni_surf(config_dict):
    config = getdefaults()
    config.update(config_dict) # overwrite default input arguments

    checkconfig(config)
    augmentconfig(config)

    environment = getenv()
    run_all(config, environment)

# this is a little hack so that python documentation
# is added from the parser defined above
def _set_run_surf_anat_preproc_doc():
    import textwrap
    p = getparser()
    aa = p._actions
    ds = []
    for a in aa:
        if a.nargs != 0:
            ch = 'str'
            if a.choices:
                ch = ' or '.join('%r' % c for c in a.choices)

            if a.default:
                ch += ' [%r]' % a.default

            bd = map(lambda x:'    ' + x, textwrap.wrap(a.help))
            ds.append('%s: %s\n%s' % (a.dest, ch, '\n'.join(bd)))

    # text to include in between the modules' docstring and the
    # generated parameter documentation
    intermediate = '''
Parameters
----------
'''

    run_prep_afni_surf.__doc__ = __doc__ + intermediate + '\n'.join(ds)

# apply setting the documentation
_set_run_surf_anat_preproc_doc()




########NEW FILE########
__FILENAME__ = multivariate_polya
# emacs: coding: utf-8; -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""PDF of the multivariate Pólya distribution.

See: http://en.wikipedia.org/wiki/Multivariate_P%C3%B3lya_distribution
"""

import numpy as np
from scipy.misc import factorial
from scipy.special import gamma, gammaln


def multivariate_polya(x, alpha):
    """Multivariate Pólya PDF. Basic implementation.
    """
    x = np.atleast_1d(x).flatten()
    alpha = np.atleast_1d(alpha).flatten()
    assert(x.size==alpha.size)
    N = x.sum()
    A = alpha.sum()
    likelihood = factorial(N) * gamma(A) / gamma(N + A)
    # likelihood = gamma(A) / gamma(N + A)
    for i in range(len(x)):
        likelihood /= factorial(x[i])
        likelihood *= gamma(x[i] + alpha[i]) / gamma(alpha[i])
    return likelihood


def log_multivariate_polya_vectorized(X, alpha):
    """Multivariate Pólya log PDF. Vectorized and stable implementation.
    """
    X = np.atleast_1d(X)
    alpha = np.atleast_1d(alpha)
    assert(X.size==alpha.size)
    N = X.sum()
    A = alpha.sum()
    log_likelihood = gammaln(N+1) - gammaln(X+1).sum() # log(\frac{N!}{\prod_i (X_i)!})
    log_likelihood += gammaln(A) - gammaln(alpha).sum() # log(\frac{\Gamma(\sum_i alpha_i)}{\prod_i(\Gamma(\alpha_i))})
    log_likelihood += gammaln(X + alpha).sum() - gammaln(N + A) # log(\frac{\prod_i(\Gamma(X_i +\alpha_i))}{\Gamma(\sum_i X_i+\alpha_i)})
    return log_likelihood


if __name__ == '__main__':

    import numpy as np
    np.random.seed(0)

    # x = np.array([1,2,2,1])
    # x = np.array([6,0,0,0])
    # alpha = np.array([1,1,1,1])

    x = np.array([100,0,0])
    # x = np.array([0,50,50])
    alpha = np.array([1,10,10])
    print "x:", x
    print "alpha:", alpha
    print "Likelihood:"
    print "log of the basic formula:", np.log(multivariate_polya(x, alpha))
    print "log of the basic vectorized formula:", np.log(multivariate_polya_vectorized(x, alpha))
    print "Log-scale stable formula:", log_multivariate_polya_vectorized(x, alpha)
    print "Monte Carlo estimations in log-scale:"
    for i in range(5):
        print "\t", i, log_multivariate_polya_mc(x, alpha, iterations=1e5)


########NEW FILE########
__FILENAME__ = partial_independence
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Partial independence model: one margin fixed (rows margin).

Analytical solution + Monte-Carlo checks.
"""

import numpy as np
from mvpa2.support.bayes.multivariate_polya \
  import log_multivariate_polya_vectorized as log_multivariate_polya

from mvpa2.base import externals
externals.exists('scipy', raise_=True)
from scipy.special import gammaln

def compute_logp_independent_block(X, alpha=None):
    """Compute the analytical log likelihood of a matrix under the
    assumption of independence.
    """
    if alpha is None: alpha = np.ones(X.shape[1])
    logp_ib = gammaln(alpha.sum()) - (gammaln(alpha)).sum()
    logp_ib += gammaln(X.sum(0) + alpha).sum() - gammaln(X.sum() + alpha.sum())
    logp_ib += gammaln(X.sum(1) + 1).sum() - gammaln(X + 1).sum()
    return logp_ib


def compute_logp_H(X, psi, alpha=None):
    """Compute the analytical log likelihood of the confusion matrix X
    with hyper-prior alpha (in a multivariate-Dirichlet sense)
    according to a partitioning scheme psi.
    """
    if alpha is None: alpha = np.ones(X.shape)
    logp_H = 0.0
    for group in psi:
        if len(group) == 1: logp_H += log_multivariate_polya(X[group[0],:], alpha[group[0],:])
        else:
            nogroup = filter(lambda a: a not in group, range(X.shape[1]))
            logp_H += np.sum([log_multivariate_polya([X[i,group].sum()] + X[i,nogroup].tolist(),
                                                     [alpha[i,group].sum()] + alpha[i,nogroup].tolist())
                              for i in group])
            logp_H += compute_logp_independent_block(X[np.ix_(group,group)],
                                                     alpha[np.ix_(group,group)].sum(0)) # should we use sum(0) or mean(0)? or else?
    return logp_H


if __name__ == '__main__':

    np.random.seed(0)

    X = np.array([[10,10, 0],
                  [10,10, 0],
                  [ 0, 0,20]])
    # X = np.array([[13, 3, 4],
    #               [ 3,14, 3],
    #               [ 4, 3,13]])
    # X = np.array([[10, 10, 10,  0,  0],
    #               [10, 10, 10,  0,  0],
    #               [10, 10, 10,  0,  0],
    #               [ 0,  0,  0, 30,  0],
    #               [ 0,  0,  0,  0, 30]], dtype=np.float32)

    print "X:"
    print X

    psi = [[0,1],[2]]
    # psi = [[0],[1],[2]]
    # psi = [[0],[1,2]]
    # psi = [[0,1,2]]
    # psi = [[0],[1],[2],[3],[4]]
    # psi = [[0,1],[2],[3],[4]]
    # psi = [[0,1,2],[3],[4]]
    # psi = [[0,1,2,3],[4]]
    # psi = [[0,1,2],[3,4]]
    # psi = [[0,1,2,3,4]]

    print "psi:", psi

    alpha = np.ones(X.shape)
    print "alpha:"
    print alpha

    logp_H = compute_logp_H(X, psi, alpha)

    print "Analytical estimate:", logp_H

########NEW FILE########
__FILENAME__ = partitioner
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Generate all partitions of a set.

Copyright 2010 Anton Vredegoor
License: MIT
See http://code.activestate.com/recipes/577211-generate-the-partitions-of-a-set-by-index/
"""

from collections import defaultdict

class Partition:

    def __init__(self, S):
        self.data = list(S)
        self.m = len(S)
        self.table = self.rgf_table()

    def __getitem__(self, i):
        #generates set partitions by index
        if i > len(self) - 1:
             raise IndexError
        L =  self.unrank_rgf(i)
        result = self.as_set_partition(L)
        return result

    def __len__(self):
        return self.table[self.m,0]

    def as_set_partition(self, L):
        # Transform a restricted growth function into a partition
        n = max(L[1:]+[1])
        m = self.m
        data = self.data
        P = [[] for _ in range(n)]
        for i in range(m):
            P[L[i+1]-1].append(data[i])
        return P

    def rgf_table(self):
        # Compute the table values 
        m = self.m
        D = defaultdict(lambda:1)
        for i in range(1,m+1):
            for j in range(0,m-i+1):
                D[i,j] = j * D[i-1,j] + D[i-1,j+1]
        return D

    def unrank_rgf(self, r):
        # Unrank a restricted growth function
        m = self.m
        L = [1 for _ in range(m+1)]
        j = 1
        D = self.table
        for i in range(2,m+1):
            v = D[m-i,j]
            cr = j*v
            if cr <= r:
                L[i] = j + 1
                r -= cr
                j += 1
            else:
                L[i] = r // v + 1
                r  %= v
        return L

########NEW FILE########
__FILENAME__ = copy
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Support for python's copy module.

"""

__docformat__ = 'restructuredtext'

import sys

# We have to use deepcopy from python 2.5, since otherwise it fails to
# copy sensitivity analyzers with assigned combiners which are just
# functions not functors
if sys.version_info[:2] >= (2, 6):
    # enforce absolute import
    _copy = __import__('copy', globals(), locals(), [], 0)
    copy = _copy.copy
    deepcopy = _copy.deepcopy
else:
    from mvpa2.support._copy import copy, deepcopy

########NEW FILE########
__FILENAME__ = griddata
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Import griddata with preference to the version from matplotlib
"""

__docformat__ = 'restructuredtext'

import sys
from mvpa2.base import externals

if externals.exists('griddata', raise_=True):
    if __debug__:
        from mvpa2.base import debug

    try:
        if sys.version_info[:2] >= (2, 5):
            # enforce absolute import
            griddata = __import__('griddata', globals(),
                                  locals(), [], 0).griddata
        else:
            # little trick to be able to import 'griddata' package (which
            # has same name)
            oldname = __name__
            # crazy name with close to zero possibility to cause whatever
            __name__ = 'iaugf9zrkjsbdv91'
            try:
                from griddata import griddata
                # restore old settings
                __name__ = oldname
            except ImportError:
                # restore old settings
                __name__ = oldname
                raise
            if __debug__:
                debug('EXT', 'Using python-griddata')
    except ImportError:
        from matplotlib.mlab import griddata
        if __debug__:
            debug('EXT', 'Using matplotlib.mlab.griddata')

########NEW FILE########
__FILENAME__ = ipy_pymvpa_completer
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Attributes-aware tab completion.

This module provides a custom tab-completer that intelligently reveals the names
of attributes in the :class:`~mvpa2.base.collections.Collection`\s of PyMVPA.

Activation
==========

To use this, put in your ~/.ipython/ipy_user_conf.py file:

    import ipy_pymvpa_completer as _ic
    _ic.activate()


Usage
=====

The system works as follows.  If t is an `Collection` object, then

In [7]: t.<TAB>

shows not only `dict` interface but also `Attribute`\s present in the collection.

Notes
-----

It is a rip-off from IPython's ipy_traits_completer.py
"""

#############################################################################
# External imports
import mvpa2.base.collections as col
from mvpa2.base import externals

if externals.exists('running ipython env', raise_=True):
    # IPython imports
    from IPython.ipapi import TryNext, get as ipget
    from IPython.genutils import dir2

#############################################################################
# Code begins

def pymvpa_completer(self, event):
    """A custom IPython tab-completer that is collections-aware.
    """

    symbol_parts = event.symbol.split('.')
    base = '.'.join(symbol_parts[:-1])

    oinfo = self._ofind(base)
    if not oinfo['found']:
        raise TryNext

    obj = oinfo['obj']
    # OK, we got the object.  See if it's traits, else punt
    if not isinstance(obj, col.Collection):
        #print "exiting for %s" % obj
        raise TryNext

    # it's a Collection object, lets add its keys
    attrs = dir2(obj)
    #print "adding ", obj.keys()
    attrs += obj.keys()

    # Let's also respect the user's readline_omit__names setting:
    omit__names = ipget().IP.Completer.omit__names
    if omit__names == 1:
        attrs = [a for a in attrs if not a.startswith('__')]
    elif omit__names == 2:
        attrs = [a for a in attrs if not a.startswith('_')]

    # The base of the completion, so we can form the final results list
    bdot = base+'.'

    tcomp = [bdot+a for a in attrs]
    return tcomp

def activate():
    """Activate the PyMVPA Collections completer.
    """
    ipget().set_hook('complete_command', pymvpa_completer, re_key = '.*')


#############################################################################
if __name__ == '__main__':
    # Testing/debugging, can be done only under interactive IPython session
    from mvpa2.datasets.base import dataset_wizard
    t = dataset_wizard([1, 2, 3], targets=1, chunks=2)

    ip = ipget().IP

    assert(not 'targets' in  ip.complete('t.sa.'))
    assert(not 'chunks' in  ip.complete('t.sa.'))

    from ipy_pymvpa_completer import activate
    activate()
    # A few simplistic tests
    assert ip.complete('t.ed') == []
    assert('targets' in  ip.complete('t.sa.'))
    assert('chunks' in  ip.complete('t.sa.'))
    print 'Tests OK'

########NEW FILE########
__FILENAME__ = lapack_svd
"""ctypes wrapper for LAPACK svd implementation - DGESVD"""
import numpy

from mvpa2.base import externals
from mvpa2.base.types import as_char

if externals.exists('ctypes', raise_=True):
    from ctypes import cdll, c_char, c_int, c_double, c_void_p, byref

from numpy.linalg import LinAlgError

if externals.exists('liblapack.so'):
    lapacklib = cdll.LoadLibrary('liblapack.so')

__all__ = ['svd']

def svd(a, full_matrices=True, algo='svd', **kwargs):
    """ ctypes warpepr for LAPACK SVD (DGESVD)
    Factorizes the matrix a into two unitary matrices U and Vh and
    an 1d-array s of singular values (real, non-negative) such that
    a == U S Vh  if S is an suitably shaped matrix of zeros whose
    main diagonal is s.

    Parameters
    ----------
    a : array, shape (M, N)
        Matrix to decompose
    full_matrices : boolean [Default is True]
        If true,  U, Vh are shaped  (M,M), (N,N)
        If false, the shapes are    (M,K), (K,N) where K = min(M,N)
    algo : 'svd' or 'sdd'
    Returns
    -------
    U:  array, shape (M,M) or (M,K) depending on full_matrices
    s:  array, shape (K,)
        The singular values, sorted so that s[i] >= s[i+1]. K = min(M, N)
    Vh: array, shape (N,N) or (K,N) depending on full_matrices

    Raises LinAlgError if SVD computation does not converge
    """
    if full_matrices:
        flag='A'
    else:
        flag='S'
    jobu=c_char(as_char(flag))
    jobv=c_char(as_char(flag))
    info=c_int(0)
    x, y = a.shape
    m=c_int(x)
    n=c_int(y)
    lda=c_int(x)
    s=(c_double*min(x,y))()
    ldu=c_int(x)


    if full_matrices:
        ldvt = c_int(y)
        u  = numpy.zeros((x,x),dtype=float)
        vt = numpy.zeros((y,y),dtype=float)
    else:
        ldvt = c_int(min(x,y))
        u  = numpy.zeros((x,min(x,y)),dtype=float)
        vt = numpy.zeros((min(x,y),y),dtype=float)


    if algo == 'svd':
        lwork=c_int(7*max(x,y))
        work = (c_double*7*min(x,y))()
        lapacklib.dgesvd_(byref(jobu), byref(jobv), byref(m), byref(n), 
                a.ctypes.data_as(c_void_p), byref(lda), s, 
                u.ctypes.data_as(c_void_p), byref(ldu), vt.ctypes.data_as(c_void_p), 
                byref(ldvt), work, byref(lwork), byref(info))
    else:
        lwork=c_int(7*min(x,y)+4*min(x,y)*min(x,y))
        work = (c_double*(7*min(x,y)+4*min(x,y)*min(x,y)))()
        iwork = (c_int*8*min(x,y))()
        lapacklib.dgesdd_(byref(jobu), byref(m), byref(n), 
                a.ctypes.data_as(c_void_p), byref(lda), s, 
                u.ctypes.data_as(c_void_p), byref(ldu), vt.ctypes.data_as(c_void_p), 
                byref(ldvt), work, byref(lwork), iwork, byref(info))
        
    if info.value >= 1:
        print "DBSQR did not converge for %i superdiagonals"%(info.value)
    #    raise LinAlgError
    #if info.value <= -1:
    #    print "Interesting!!! \nQuick, go find swaroop"
    if info.value == 0:
        return vt, numpy.frombuffer(s), u

########NEW FILE########
__FILENAME__ = afni_niml
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
General AFNI NIML I/O support

Created on Feb 16, 2012

@author: Nikolaas. N. Oosterhof (nikolaas.oosterhof@unitn.it)

This function reads a NIML file and returns a dict that contains all
NIML information in a tree-like structure (dicts for which some values
are dicts themselves). Branches are stored in a 'nodes' field.

For specific types of data, consider afni_niml_dset or afni_niml_annot
files which provide easier access to the data.

WiP

TODO: some nice refactoring of the code. Currently it's a bit of
      a mess.
'''

import re, numpy as np, random, os, time, sys, base64, copy, math
from io import BytesIO

from mvpa2.support.nibabel import afni_niml_types as types
_RE_FLAGS = re.DOTALL # regular expression matching spans across new lines

from mvpa2.base import debug
if __debug__:
    if not "NIML" in debug.registered:
        debug.register("NIML", "NeuroImaging Markup Language")

_TEXT_ROWSEP = "\n"
_TEXT_COLSEP = " "

# define NIML specific escape characters
_ESCAPE = {'&lt;':'<',
         '&gt;':'>',
         '&quot;':'"',
         '&amp;':'&',
         '&apos;':"'"}

def support_lists(f):
    '''Decorater to allow a function to support list input (and output)

    Used as decorator with a function f, it will
    apply f element-wise to an argument xs if xs is a list or tuple
    Otherwise it just applies f to xs.

    XXX should this be a more universal function for PyMVPA
    '''
    def apply_f(x):
        if isinstance(x, (list, tuple)):
            # support nested lists/tuples
            return map(apply_f, x)
        else:
            return f(x)
    return apply_f

@support_lists
def decode_escape(s):
    '''Undoes NIML-specific escape characters'''
    for k, v in _ESCAPE.iteritems():
        s = s.replace(k, v)
    return s

@support_lists
def encode_escape(s):
    '''Applies NIML-specific escape characters'''
    for k, v in _ESCAPE.iteritems():
        s = s.replace(v, k)
    return s

def _parse_keyvalues(s):
    '''parse K0=V0 K1=V1 ... and return a dict(K0=V0,K1=V1,...)'''

    e = b'\s*(?P<lhs>\w+)\s*=\s*"(?P<rhs>[^"]+)"'

    m = re.findall(e, s, _RE_FLAGS)
    return dict([(k.decode(), v.decode()) for k, v in m])

def _mixedtypes_datastring2rawniml(s, niml):
    '''Converts data with mixed types to raw NIML'''
    tps = niml['vec_typ']
    ncols = len(tps)
    nrows = niml['vec_len']

    s = s.decode() # convert bytearray to string

    lines = s.strip().split(_TEXT_ROWSEP)
    if len(lines) != nrows:
        raise ValueError("Expected %d rows, but found %d" % (nrows, len(lines)))

    elems = map(lambda x : x.strip().split(_TEXT_COLSEP), lines)
    fs = map(types.code2python_convertor, tps)

    data = []
    for col in xrange(ncols):
        f = fs[col]
        if types.sametype(tps[col], 'String'):
            d = map(f, [elems[r][col] for r in xrange(nrows)])
        else:
            tp = types.code2numpy_type(tps[col])
            niform = niml.get('ni_form', None)
            if not niform is None:
                raise ValueError('Not supported: have ni_form with mixed types')

            d = np.zeros((nrows,), dtype=tp) # allocate one-dimensional array
            for r in xrange(nrows):
                d[r] = f(elems[r][col])

        data.append(d)

    return data


def _datastring2rawniml(s, niml):
    '''Converts data with uniform type to raw NIML'''
    debug('NIML', 'Raw string to NIML: %d characters', len(s))

    tps = niml['vec_typ']

    onetype = types.findonetype(tps)

    if onetype is None or ([onetype] == types.str2codes('string') and
                            len(tps) > 1):
        return _mixedtypes_datastring2rawniml(s, niml)

    if [onetype] == types.str2codes('string'):
        # single string
        return decode_escape(s.decode()) # do not string2rawniml

    # numeric, either int or float
    ncols = niml['vec_num']
    nrows = niml['vec_len']
    tp = types.code2numpy_type(onetype)

    niform = niml.get('ni_form', None)

    if not niform or niform == 'text':
        data = np.zeros((nrows, ncols), dtype=tp) # allocate space for data
        convertor = types.code2python_convertor(onetype) # string to type convertor

        vals = s.split(None) # split by whitespace seperator
        if len(vals) != ncols * nrows:
            raise ValueError("unexpected number of elements")

        for i, val in enumerate(vals):
            data[i // ncols, i % ncols] = convertor(val)

    else:
        dtype = np.dtype(tp)
        dtype = types.byteorder_from_niform(niform, dtype)

        if 'base64' in niform:
            debug('NIML', 'base64, %d chars: %s',
                            (len(s), _partial_string(s, 0)))

            s = base64.b64decode(s)
        elif not 'binary' in niform:
            raise ValueError('Illegal niform %s' % niform)

        data_1d = np.fromstring(s, dtype=tp)

        debug('NIML', 'data vector has %d elements, reshape to %d x %d = %d',
                        (np.size(data_1d), nrows, ncols, nrows * ncols))

        data = np.reshape(data_1d, (nrows, ncols))

    return data

def getnewidcode():
    '''Provides a new (random) id code for a NIML dataset'''
    return ''.join(map(chr, [random.randint(65, 65 + 25) for _ in xrange(24)]))

def setnewidcode(s):
    '''Sets a new (random) id code in a NIML dataset'''
    tp = type(s)
    if tp is list:
        for v in s:
            setnewidcode(v)
    elif tp is dict:
        key = 'self_idcode'
        for k, v in s.iteritems():
            if k == key:
                s[key] = getnewidcode()
            else:
                setnewidcode(v)

def find_attribute_node(niml_dict, key, value, just_one=True):
    '''Finds a NIML node that matches a particular key and value

    Parameters
    ----------
    niml_dict: dict
        NIML dictionary in which the node is to be found
    key: str
        Key for a node that is to be found
    value: str
        Value associated with key that is to be found
    just_one: boolean (default: True)
        Indicates whether exactly one matching node is to be found.

    Returns
    -------
    nd: dict or list.
        NIML dictionary matching key and value. If just_one is True then, if
        a single node is found, it returns a dict containing that node;
        otherwise an exception is raised. If just_one is False then the output
        is a list with matching nodes; this list is empty if no matching nodes
        were found.
    '''

    tp = type(niml_dict)
    if tp is list:
        r = sum([find_attribute_node(d, key, value, False)
                            for d in niml_dict], [])

    elif tp is dict:
        r = [niml_dict] if niml_dict.get(key, None) == value else []
        r.extend(find_attribute_node(niml_dict[k], key, value, False)
                        for k, v in niml_dict.iteritems() if type(v) in (list, dict))

    else:
        return []

    r = [ri for ri in r if ri]
    if just_one:
        while type(r) is list:
            if len(r) != 1:
                raise ValueError('Found %d elements matching %s=%s, '
                             ' but expected 1' % (len(r), key, value))
            r = r[0]

    return r



def rawniml2string(p, form='text'):
    '''Converts a raw NIML element to string representation

    Parameters
    ----------
    niml: dict
        Raw NIML element
    form: 'text', 'binary', 'base64'
        Output form of niml

    Returns
    -------
    s: bytearray
        String representation of niml in output form 'form'.
    '''
    if type(p) is list:
        nb = '\n'.encode()
        return nb.join(rawniml2string(v, form) for v in p)

    if not form in ['text', 'binary', 'base64']:
        raise ValueError("Illegal form %s" % form)

    q = p.copy() # make a shallow copy


    if 'nodes' in q:
        s_body = rawniml2string(q.pop('nodes'), form) # recursion
    else:
        data = q.pop('data')
        data = types.nimldataassupporteddtype(data) # ensure the data format is supported by NIML
        s_body = _data2string(data, form)

        if form == 'text':
            q.pop('ni_form', None) # defaults to text, remove if already there
        else:
            byteorder = types.data2ni_form(data, form)
            if byteorder:
                q['ni_form'] = byteorder

        # remove some unncessary fields
        for f in ['vec_typ', 'vec_len', 'vec_num']:
            q.pop(f, None)

    s_name = q.pop('name', None).encode()
    s_header = _header2string(q)

    d = map(lambda x:x.encode(), ['<', '\n', ' >', '</', '>'])
    return b''.join((d[0], s_name, d[1], s_header, d[2], s_body, d[3], s_name, d[4]))

def _data2string(data, form):
    '''Converts a data element to binary, text or base64 representation'''
    if isinstance(data, basestring):
        return ('"%s"' % encode_escape(data)).encode()

    elif type(data) is np.ndarray:
        if form == 'text':
            f = types.numpy_data2printer(data)
            nrows, ncols = data.shape
            return _TEXT_ROWSEP.join([_TEXT_COLSEP.join([f(data[row, col])
                                                         for col in xrange(ncols)])
                                                         for row in xrange(nrows)]).encode()
        elif form == 'binary':
            data_reshaped = data.reshape((data.shape[1], data.shape[0]))
            r = data_reshaped.tostring()
            debug('NIML', 'Binary encoding (len %d -> %d): [%s]' %
                            (data_reshaped.size, len(r), _partial_string(r, 0)))
            return r
        elif form == 'base64':
            data_reshaped = data.reshape((data.shape[1], data.shape[0]))
            r = base64.b64encode(data_reshaped.tostring())
            debug('NIML', 'Encoding ok: [%s]', _partial_string(r, 0))
            return r
        else:
            raise ValueError("illegal format %s" % format)

    elif type(data) is list:
        # mixed types, each column in its own container
        # always use text output format, even if requested form is binary of base64

        ncols = len(data)
        if ncols == 0:
            return "".encode()
        else:
            nrows = len(data[0])

            # separate formatter functions for each column
            # if list of strings then take first element of the list to get a string formattr
            # else use the entire np array to get a numeric formatter
            fs = [types.numpy_data2printer(d[0] if type(d) is list else d) for d in data]

            return _TEXT_ROWSEP.join([_TEXT_COLSEP.join([fs[col](data[col][row])
                                                         for col in xrange(ncols)])
                                                         for row in xrange(nrows)]).encode()

    else:
        raise TypeError("Unknown type %r" % type(data))

def _header2string(p, keyfirst=['dset_type', 'self_idcode', 'filename', 'data_type'], keylast=['ni_form']):
    '''Converts a header element to a string'''
    otherkeys = list(set(p.keys()) - (set(keyfirst) | set(keylast)))

    added = set()
    keyorder = [keyfirst, otherkeys, keylast]
    kvs = []
    for keys in keyorder:
        for k in keys:
            if k in p and not k in added:
                kvs.append((k, p[k]))
                added.add(k)

    rs = map(lambda x : '   %s="%s"' % x, kvs)
    return ("\n".join(rs)).encode()

def read(fn, itemifsingletonlist=True, postfunction=None):
    '''Reads a NIML dataset

    Parameters
    ----------
    fn: str
        Filename of NIML dataset
    itemifsingletonlist: boolean
        If True and the NIML dataset contains of a single NIML element, then
        that element is returned. Otherwise a list of NIML element is returned.
    postfunction: None or callable
        If not None then postfunction is applied to the result from reading
        the NIML dataset.

    Returns
    -------
    niml: list or dict
        (list of) NIML element(s)
    '''

    import io
    with io.FileIO(fn) as f:
        s = f.read()

    r = string2rawniml(s)
    if not postfunction is None:
        r = postfunction(r)

    if itemifsingletonlist and type(r) is list and len(r) == 1:
        return r[0]
    else:
        return r

def _partial_string(s, i, maxlen=100):
    '''Prints a string partially'''

    # length of the string to print
    n = len(s) - i
    if n <= 0 or maxlen == 0:
        return '' # nothing to print

    if maxlen < 0 or maxlen > n:
        maxlen = n # print the whole string
    elif maxlen > n:
        maxlen = n

    # half the size of a segment
    startsize = maxlen // 2
    stopsize = startsize + maxlen % 2

    infix = ' ... ' if n > maxlen else ''

    return '%s%s%s' % (s[i:(i + startsize)], infix, s[-stopsize:])

def string2rawniml(s, i=None):
    '''Parses a NIML string to a raw NIML tree-like structure

    Parameters
    ----------
    s: bytearray
        string to be converted
    i: int
        Starting position in the string.
        By default None is used, which means that the entire string is
        converted.

    Returns
    -------
    r: the NIML result.
        If input parameter i is None then a dictionary with NIML elements, or
        a list containing such elements, is returned. If i is an integer,
        then a tuple j, d is returned with d the new starting position and a
        dictionary or list with the elements parsed so far.
    '''

    # return new starting position?
    return_pos = not i is None
    if not return_pos:
        i = 0

    debug('NIML', 'Parsing at %d, total length %d', (i, len(s)))
    # start parsing from header
    #
    # the tricky part is that binary data can contain characters that also
    # indicate the end of a data segment, so 'typical' parsing with start
    # and end markers cannot be done. Instead the header of each part is
    # read first, then the number of elements is computed based on the
    # header information, and the required number of bytes is converted.
    # From then on the remainder of the string is parsed as above.


    headerpat = b'\W*<(?P<name>\w+)\W(?P<header>.*?)>'

    nimls = [] # here all found parts are stored


    # Keep on reading new parts
    while True:
        # ignore any xml tags
        if s.startswith(b'<?xml', i):
            i = s.index(b'>', i) + 1

        # try to read a name and header part
        m = re.match(headerpat, s[i:], _RE_FLAGS)

        if m is None:
            # no header - was it the end of a section?
            m = re.match(b'\W*</\w+>\s*', s[i:], _RE_FLAGS)

            if m is None:
                if len(s[i:].strip()) == 0:
                    if return_pos:
                        return i, nimls
                    else:
                        return nimls
                else:
                    raise ValueError("No match towards end of header end: [%s] " % _partial_string(s, i))

            else:
                # for NIFTI extensions there can be some null bytes left
                # so get rid of them here
                remaining = s[i + m.end():].replace(chr(0).encode(), b'').strip()

                if len(remaining) > 0:
                    # there is more stuff to parse
                    i += m.end()
                    continue


                # entire file was parsed - we are done
                debug('NIML', 'Completed parsing, length %d (%d elements)', (len(s), len(nimls)))
                if return_pos:
                    return i, nimls
                else:
                    return nimls



        else:
            # get values from header
            d = m.groupdict()
            name, header = d['name'], d['header']

            # update current position
            i += m.end()

            # parse the keys and values in the header
            debug('NIML', 'Parsing header %s, header end position %d',
                                                (name, i + m.end()))
            niml = _parse_keyvalues(header)

            debug('NIML', 'Found keys %s.', (", ".join(niml.keys())))
            # set the name of this element
            niml['name'] = name.decode()

            if niml.get('ni_form', None) == 'ni_group':
                # it's a group. Parse the group using recursion
                debug("NIML", "Starting a group %s >>>" , niml['name'])
                i, niml['nodes'] = string2rawniml(s, i)
                debug("NIML", "<<< ending a group %s", niml['name'])
            else:
                # it's a normal element with data
                debug('NIML', 'Parsing element %s from position %d, total '
                                    'length %d', (niml['name'], i, len(s)))

                # set a few data elements
                datatypes = niml['ni_type']
                niml['vec_typ'] = types.str2codes(datatypes)
                niml['vec_len'] = int(niml['ni_dimen'])
                niml['vec_num'] = len(niml['vec_typ'])

                debug('NIML', 'Element of type %s' % niml['vec_typ'])

                # data can be in string form, binary or base64.
                is_string = niml['ni_type'] == 'String' or \
                                not 'ni_form' in niml
                if is_string:
                    # string form is handled separately. It's easy to parse
                    # because it cannot contain any end markers in the data

                    debug("NIML", "Parsing string body for %s", name)

                    vec_typ = niml['vec_typ']
                    is_mixed_data = len(set(vec_typ)) > 1
                    is_multiple_string_data = len(vec_typ) > 1 and types._one_str2code('String') == types.findonetype(vec_typ)

                    if is_mixed_data or is_multiple_string_data:
                        debug("NIML", "Data is mixed type (string=%s)" % is_multiple_string_data)
                        #strpat = ('\s*(?P<data>.*)\s*</%s>' % \
                        #                        (name.decode())).encode()
                        strpat = ('\s*(?P<data>.*?)\s*</%s>' % \
                                                (name.decode())).encode()

                        m = re.match(strpat, s[i:], _RE_FLAGS)
                        is_string_data = is_multiple_string_data
                    else:
                        # If the data type is string, it is surrounded by quotes
                        # Otherwise (numeric data) there are no quotes
                        is_string_data = niml['ni_type'] == 'String'
                        quote = '"' if is_string_data else ''

                        # construct the regular pattern for this string
                        strpat = ('\s*%s(?P<data>[^"]*)[^"]*%s\s*</%s>' % \
                                                        (quote, quote, name.decode())).encode()

                        m = re.match(strpat, s[i:], _RE_FLAGS)

                    if m is None:
                        # something went wrong
                        raise ValueError("Could not parse string data from "
                                         "pos %d: %s" %
                                                (i, _partial_string(s, i)))

                    # parse successful - get the parsed data
                    data = m.groupdict()['data']

                    # convert data to raw NIML
                    data = _datastring2rawniml(data, niml)

                    # if string data, replace escape characters
                    if is_multiple_string_data or is_string_data:
                        data = decode_escape(data)

                    # store data
                    niml['data'] = data

                    # update position
                    i += m.end()

                    debug('NIML', 'Completed %s, now at %d', (name, i))

                else:
                    # see how many bytes (characters) to read

                    # convert this part of the string
                    if 'base64' in niml['ni_form']:
                        # base 64 has no '<' character - so we should be fine
                        endpos = s.index(b'<', i + 1)
                        datastring = s[i:endpos]
                        nbytes = len(datastring)
                    else:
                        # hardcode binary data - see how many bytes we need
                        nbytes = _binary_data_bytecount(niml)
                        debug('NIML', 'Raw data with %d bytes - total length '
                                    '%d, starting at %d', (nbytes, len(s), i))
                        datastring = s[i:(i + nbytes)]

                    niml['data'] = _datastring2rawniml(datastring, niml)

                    # update position
                    i += nbytes

                    # ensure that immediately after this segment there is an
                    # end-part marker
                    endstr = '</%s>' % name.decode()
                    if s[i:(i + len(endstr))].decode() != endstr:
                        raise ValueError("Not found expected end string %s"
                                         "  (found %s...)" %
                                            (endstr, _partial_string(s, i)))
                    i += len(endstr)

            debug('NIML', "Adding element '%s' with keys %r" % (niml['name'], niml.keys()))
            nimls.append(niml)


    # we should never end up here.
    raise ValueError("this should never happen")


def _binary_data_bytecount(niml):
    '''helper function that returns how many bytes a NIML binary data
    element should have'''
    niform = niml['ni_form']
    if not 'binary' in niform:
        raise ValueError('Illegal niform %s' % niform)

    tps = niml['vec_typ']
    onetype = types.findonetype(tps)

    if onetype is None:
        debug('NIML', 'Not unique type: %r', tps)
        return None

    # numeric, either int or float
    ncols = niml['vec_num']
    nrows = niml['vec_len']
    tp = types.code2numpy_type(onetype)
    bytes_per_elem = types.numpy_type2bytecount(tp)

    if bytes_per_elem is None:
        raise ValueError("Type not supported: %r" % onetype)

    nb = ncols * nrows * bytes_per_elem

    debug('NIML', 'Number of bytes for %s: %d x %d with %d bytes / element',
                                    (niform, ncols, nrows, bytes_per_elem))

    return nb


def write(fnout, niml, form='binary', prefunction=None):
    if not prefunction is None:
        niml = prefunction(niml)

    s = rawniml2string(niml, form=form)

    import io
    with io.FileIO(fnout, 'w') as f:
        n = f.write(s)
    if n != len(s):
        raise ValueError("Not all bytes written to %s" % fnout)

########NEW FILE########
__FILENAME__ = afni_niml_annot
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Experimental support for AFNI NIML annotation files 

Created on Feb 19, 2012

@author: Nikolaas. N. Oosterhof (nikolaas.oosterhof@unitn.it)
'''

import numpy as np

from mvpa2.support.nibabel import afni_niml as niml
from mvpa2.support.nibabel import afni_niml_dset as dset

import os

def rawniml2annot(p):
    '''Converts raw NIML to annotation format'''

    if type(p) is list:
        return map(rawniml2annot, p)
    r = dset.rawniml2dset(p)

    for nd in p['nodes']:
        if nd.get('dset_type', None) == 'LabelTableObject':
            r[r'AFNI_labeltable'] = dset.rawniml2dset(nd)

    return r

def annot2rawniml(a):
    '''Converts annotation to raw NIML format'''
    a = a.copy()
    t = a.pop('AFNI_labeltable')
    t['labels'] = ['R', 'G', 'B', 'A', 'key', 'name']
    t['dset_type'] = 'LabelTableObject_data'
    t['node_indices'] = None
    r = dset.dset2rawniml(t).copy()

    _fix_rawniml_table_output(r)

    t = dset.dset2rawniml(a)
    _fix_rawniml_main_output(t)

    # add the table to the nodes
    t['nodes'].insert(2, r)

    return t

def _fix_rawniml_table_output(r):
    colms_tp = niml.find_attribute_node(r, 'atr_name', 'COLMS_TYPE')
    colms_tp['data'] = ('R_col;G_col;B_col;A_col;'
                        'Node_Index_Label;Node_String_Label')

    colms_st = niml.find_attribute_node(r, 'atr_name', 'COLMS_STATSYM')
    colms_st['data'] = "none;none;none;none;none;none"

    dset = niml.find_attribute_node(r, 'name', 'AFNI_dataset')
    dset['name'] = 'AFNI_labeltable'
    dset['dset_type'] = 'LabelTableObject'
    dset['flipped'] = '0'
    dset['Sgn'] = '0'

    table = niml.find_attribute_node(r, 'data_type', 'Node_Bucket_data')
    table['data_type'] = 'LabelTableObject_data'

def _fix_rawniml_main_output(r):
    main = niml.find_attribute_node(r, 'data_type', 'Node_Bucket_data')
    main['data_type'] = 'Node_Label_data'

    idx = niml.find_attribute_node(r, 'data_type', 'Node_Bucket_node_indices')
    idx['data_type'] = 'Node_Label_node_indices'

    #header = niml.find_attribute_node(r, 'dset_type', 'LabelTableObject')
    #header['dset_type'] = 'Node_Label'


def _merge_indices_addition_values(idxs, last_index_function=np.max):
    n = len(idxs)

    if any(np.sum(idx < 0) for idx in idxs):
        raise ValueError("Unexpected negative values")

    nidxs = map(last_index_function, idxs)
    last_indices = np.cumsum(nidxs)

    addition_values = []
    last_index = 0
    for i, idx in enumerate(idxs):
        addition_values.append(last_index)
        last_index += last_indices[i] + 1

    return addition_values


def merge(annots):
    '''Merges multiple annotations. One use case is merging two hemispheres'''
    n = len(annots)

    def annot2idx_table_data(annot):
        return (annot['node_indices'],
                annot['AFNI_labeltable']['data'],
                annot['data'])

    idxs, tables, datas = map(list, zip(*map(annot2idx_table_data, annots)))

    to_add_idx = _merge_indices_addition_values(idxs)
    idx = np.vstack(idxs[i] + to_add_idx[i] for i in xrange(n))

    # join the table
    ncols = len(tables[0])
    table = []
    for i in xrange(ncols):
        columns = [d[i] for d in tables]

        if all(isinstance(d[i], np.ndarray) and \
                    np.issubdtype(m.dtype, np.int) for m in columns):
            to_add_table = _merge_indices_addition_values(columns)

            for j in xrange(n):
                columns[j] = columns[j] + to_add_table[j]

        table.append(np.hstack(columns))

    data = np.vstack([datas[i] + to_add_table[i] for i in xrange(n)])

    output = annots[0].copy()
    output['node_indices'] = idx
    output['AFNI_labeltable']['data'] = table
    output['data'] = data

    return output


def read(fn, itemifsingletonlist=True):
    return niml.read(fn, itemifsingletonlist, rawniml2annot)

def write(fnout, niml_annot):
    fn = os.path.split(fnout)[1]

    if not type(fn) is str:
        if not isinstance(fnout, basestring):
            raise ValueError("Filename %s should be string" % str)
        fn = str(fn) # ensure that unicode is converted to string

    niml_annot['filename'] = fn
    form = 'text'
    niml.write(fnout, niml_annot, form, annot2rawniml)

########NEW FILE########
__FILENAME__ = afni_niml_dset
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
AFNI NIML dataset I/O support.
Usually this type of datasets are used for functional data (timeseries,
preprocessed data), statistical maps or searchlight results.

Created on Feb 19, 2012

@author: Nikolaas. N. Oosterhof (nikolaas.oosterhof@unitn.it)

Files that are read with the afni_niml_dset.read function contain a dict
with the following fields:

   .data           PxN data for P nodes and N columns (values per node).
   .node_indices   P indices of P nodes that data refers to (base 0)
   .history        String with history information
   .stats          list with statistical information for each column.
   .labels         list with labels of the data columns
   .dset_type      String with the data set type

Similarly, such a dict can be saved to a .niml.dset file using the
afni_niml_dset.write function
'''

import random, numpy as np, os, time, sys, socket
from mvpa2.support.nibabel import afni_niml_types as types
from mvpa2.support.nibabel import afni_niml as niml

from mvpa2.base import warning, debug

def _string2list(s, SEP=";", warn_if_no_string=True):
    '''splits a string by SEP; if the last element is empty then it is not returned

    The rationale is that AFNI/NIML like to close the string with a ';' which
    would return one (empty) value too many

    If the input is already a list that has lists or tuples,
    by default a warning is thrown because SUMA may puke over it.
    '''
    if isinstance(s, (list, tuple)):
        if warn_if_no_string and \
                    any(isinstance(s_elem, (list, tuple)) for s_elem in s):
            # a short representation of the offending structure
            s_str = '%r' % s if len(s) <= 1 else '[%s ... %s]' % (s[0], s[-1])
            warning('Two-dimensional string structure %s found - '
                        'it may not be readable by SUMA.' % s_str)
        return s

    r = s.split(SEP)
    if not r[-1]:
        r = r[:-1]
    return r

def rawniml2dset(p):
    if type(p) is list:
        return map(rawniml2dset, p)

    assert type(p) is dict and all([f in p for f in ['dset_type', 'nodes']]), p
    #assert type(p) is dict and all([f in p for f in ['nodes']]), p

    r = dict()
    r['dset_type'] = p['dset_type']

    for node in p['nodes']:
        assert 'name' in node

        name = node['name']
        data = node.get('data', None)

        if name == 'INDEX_LIST':
            r['node_indices'] = data
        elif name == 'SPARSE_DATA':
            r['data'] = data
        elif name == 'AFNI_atr':
            atr = node['atr_name']

            if atr == 'HISTORY_NOTE':
                r['history'] = data
            elif atr == 'COLMS_STATSYM':
                r['stats'] = _string2list(data)
            elif atr == 'COLMS_LABS':
                r['labels'] = _string2list(data)
        else:
            r[name] = data
            #raise ValueError("Unexpected node %s" % name)

    return r


def _dset2rawniml_header(s):
    r = dict()

    # set dataset type, default is Node_Bucket
    r['dset_type'] = s.get('dset_type', 'Node_Bucket')

    # make a new id code of 24 characters, all uppercase letters
    r['self_idcode'] = niml.getnewidcode()
    r['filename'] = s.get('filename', 'null')
    r['label'] = r['filename']
    r['name'] = 'AFNI_dataset'
    r['ni_form'] = 'ni_group'

    return r

def _dset2rawniml_data(s):
    return dict(data_type='Node_Bucket_data',
                name='SPARSE_DATA',
                data=s['data'])

def _dset_nrows_ncols(s):
    if type(s) is dict and 'data' in s:
        data = s['data']
    else:
        data = s

    if isinstance(data, np.ndarray):
        sh = s['data'].shape
        nrows = sh[0]
        ncols = 1 if len(sh) == 1 else sh[1]
    elif isinstance(data, list):
        lengths = set([len(d) for d in data])
        if len(lengths) != 1:
            raise ValueError('nonmatching lengths', lengths)
        nrows = lengths.pop()
        ncols = len(data)
    else:
        raise ValueError('not understood: %s' % data)

    return nrows, ncols

def _dset2rawniml_nodeidxs(s):
    nrows, _ = _dset_nrows_ncols(s)

    node_idxs = s.get('node_indices') if 'node_indices' in s else np.arange(nrows, dtype=np.int32)

    if not node_idxs is None:
        if not type(node_idxs) is np.ndarray:
            node_idxs = np.asarray(node_idxs, dtype=np.int32)

        if node_idxs.size != nrows:
            raise ValueError("Size mismatch for node indices (%r) and data (%r)" %
                             (node_idxs.size, nrows))

        if node_idxs.shape != (nrows, 1):
            node_idxs = np.reshape(node_idxs, ((nrows, 1))) # reshape to column vector if necessary

    def is_sorted(v): # O(1) in best case and O(n) in worst case (unlike sorted())
        if v is None:
            return None
        n = len(v)
        return n == 0 or all(v[i] <= v[i + 1] for i in xrange(n - 1))

    return dict(data_type='Node_Bucket_node_indices',
                name='INDEX_LIST',
                data=node_idxs,
                sorted_node_def='Yes' if is_sorted(node_idxs) else 'No')

def _dset2rawniml_datarange(s):
    data = s['data']

    try:
        minpos = np.argmin(data, axis=0)
        maxpos = np.argmax(data, axis=0)

        f = types.numpy_data2printer(data) # formatter function
        r = []
        for i in xrange(len(minpos)):
            mnpos = minpos[i]
            mxpos = maxpos[i]
            r.append('%s %s %d %d' % (f(data[mnpos, i]), f(data[mxpos, i]), mnpos, mxpos))

        # range of data in each column
        return dict(atr_name='COLMS_RANGE',
                    data=r)
    except:
        return dict(atr_name='COLMS_RANGE',
                    data=None)

def _dset2rawniml_labels(s):
    _, ncols = _dset_nrows_ncols(s)

    labels = s.get('labels', None)
    if labels is None:
        labels = ['col_%d' % i for i in xrange(ncols)]
    elif type(labels) != list:
        labels = list(labels)
    if len(labels) != ncols:
        raise ValueError("Wrong number of labels (%s): found %d but expected %d" %
                         (labels, len(labels), ncols))
    return dict(atr_name='COLMS_LABS',
                data=labels)

def _dset2rawniml_history(s):
    try:
        logprefix = ('[%s@%s: %s]' % (os.environ.get('USER', 'UNKNOWN'),
                                      socket.gethostname(),
                                      time.asctime()))
    except:
        logprefix = ''
    # history
    history = s.get('history', '')
    if history and not history.endswith('\n'):
        history += ('\n')
    history += '%s Saved by %s:%s' % (logprefix,
                                    __file__,
                                    sys._getframe().f_code.co_name)

    return dict(atr_name='HISTORY_NOTE',
                data=history)

def _dset2rawniml_datatypes(s):
    data = s['data']
    _, ncols = _dset_nrows_ncols(s)
    # XXX does not support mixed types
    datatype = ['Generic_Int' if types.numpy_data_isint(data) else 'Generic_Float'] * ncols
    return dict(atr_name='COLMS_TYPE',
                data=datatype)

def _dset2rawniml_stats(s):
    data = s['data']
    _, ncols = _dset_nrows_ncols(s)
    stats = s.get('stats', None)

    if stats is None:
        stats = ['none'] * ncols
    return dict(atr_name='COLMS_STATSYM',
                data=stats)

def _dset2rawniml_anything_else(s):
    ignore_keys = ['data', 'stats', 'labels', 'history', 'dset_type', 'node_indices']

    ks = s.keys()
    niml = []
    for k in ks:
        if k in ignore_keys:
            continue
        niml_elem = dict(data=s[k], name=k)

        try:
            niml.append(_dset2rawniml_complete(niml_elem))
        except TypeError:
            debug('NIML', 'Warning: unable to convert value for key %s' % k)

    return niml

def _dset2rawniml_complete(r):
    '''adds any missing information and ensures data is formatted properly'''

    # if data is a list of strings, join it and store it as a string
    # otherwise leave data untouched
    if types.numpy_data_isstring(r['data']):
        r['data'] = list(r['data'])

    while True:
        data = r['data']
        tp = type(data)

        if types.numpy_data_isstring(r['data']):
            r['data'] = list(r['data'])

        elif tp is list:
            if all(isinstance(d, basestring)  for d in data):
                r['data'] = ";".join(data)
            else:
                tp = 'mixed'
                break

        else:
            break # we're done

    if tp == 'mixed':
        #data = [types.nimldataassupporteddtype(d) for d in data]
        #r['data'] = data

        nrows, ncols = _dset_nrows_ncols(data)
        r['ni_dimen'] = str(nrows)
        tpstrs = []
        for d in data:
            if isinstance(d, basestring) or \
                    (type(d) is list and
                            all(isinstance(di, basestring) for di in d)):
                tpstr = 'String'
            elif isinstance(d, np.ndarray):
                tpstr = types.numpy_type2name(d.dtype)
            else:
                raise ValueError('unrecognized type %s' % type(d))
            tpstrs.append(tpstr)
        r['ni_type'] = ','.join(tpstrs)

    elif issubclass(tp, basestring):
        r['ni_dimen'] = '1'
        r['ni_type'] = 'String'
    elif tp is np.ndarray:
        data = types.nimldataassupporteddtype(data)
        if len(data.shape) == 1:
            data = np.reshape(data, (data.shape[0], 1))

        r['data'] = data # ensure we store a supported type


        nrows, ncols = data.shape
        r['ni_dimen'] = str(nrows)
        tpstr = types.numpy_type2name(data.dtype)
        r['ni_type'] = '%d*%s' % (ncols, tpstr) if nrows > 1 else tpstr
    elif not data is None:
        raise TypeError('Illegal type %r in %r' % (tp, data))

    if not 'name' in r:
        r['name'] = 'AFNI_atr'

    return r

def _remove_empty_nodes(nodes):
    tp = type(nodes)
    if tp is list:
        i = 0
        while i < len(nodes):
            node = nodes[i]
            if type(node) is dict and 'data' in node and node['data'] is None:
                nodes.pop(i)
            else:
                i += 1
    elif tp is dict:
        for v in nodes.itervalues():
            _remove_empty_nodes(v)


def dset2rawniml(s):
    if type(s) is list:
        return map(dset2rawniml, s)
    elif type(s) is np.ndarray:
        s = dict(data=s)

    if not 'data' in s:
        raise ValueError('No data?')

    r = _dset2rawniml_header(s)
    builders = [_dset2rawniml_data,
              _dset2rawniml_nodeidxs,
              _dset2rawniml_labels,
              _dset2rawniml_datarange,
              _dset2rawniml_history,
              _dset2rawniml_datatypes,
              _dset2rawniml_stats]

    nodes = [_dset2rawniml_complete(build(s)) for build in builders]
    _remove_empty_nodes(nodes)

    more_nodes = filter(lambda x:not x is None, _dset2rawniml_anything_else(s))

    r['nodes'] = nodes + more_nodes
    return r

def read(fn, itemifsingletonlist=True):
    return niml.read(fn, itemifsingletonlist, rawniml2dset)

def write(fnout, dset, form='binary'):
    fn = os.path.split(fnout)[1]

    if not type(fn) is str:
        if not isinstance(fnout, basestring):
            raise ValueError("Filename %s should be string" % str)
        fn = str(fn) # ensure that unicode is converted to string

    dset['filename'] = fn
    niml.write(fnout, dset, form, dset2rawniml)

def sparse2full(dset, pad_to_ico_ld=None, pad_to_node=None,
                ico_ld_surface_count=1, set_missing_values=0):
    '''
    Creates a 'full' dataset which has values associated with all nodes

    Parameters
    ----------
    dset: dict
        afni_niml_dset-like dictionary with at least a field 'data'
    pad_to_node_ico_ld: int
        number of linear divisions (only applicable if used through
        AFNI's MapIcosehedron) of the surface this dataset refers to.
    pad_to_node: int
        number of nodes of the surface this data
    ico_ld_surface_count: int (default: 1)
        if pad_to_ico_ld is set, this sets the number of surfaces that
        were origingally used. The typical use case is using a 'merged'
        surface originally based on a left and right hemisphere
    set_missing_values: int or float (default: 0)
        value to which nodes not present in dset are set.

    Returns
    -------
    dset: dict
        afni_niml_dset-like dictionary with at least fields 'data' and
        'node_indices'.
    '''

    if not pad_to_ico_ld is None:
        if pad_to_node:
            raise ValueError("Cannot have both ico_ld and pad_to_node")
        pad_to_node = ico_ld_surface_count * (pad_to_ico_ld ** 2 * 10 + 2)
    else:
        if pad_to_node is None:
            raise ValueError("Need either pad_to_ico_ld or pad_to_node")

    data = dset['data']
    nrows, ncols = data.shape

    node_indices = dset.get('node_indices', np.reshape(np.arange(nrows),
                                                            (-1, 1)))

    # a few sanity checks
    n = len(node_indices)

    if nrows != n:
        raise ValueError('element count mismatch between data (%d) and '
                         'node indices (%d)' % (nrows, n))

    if n > pad_to_node:
        raise ValueError('data has more rows (%d) than pad_to_node (%d)',
                                                (n, pad_to_node))

    full_node_indices_vec = np.arange(pad_to_node)
    full_node_indices = np.reshape(full_node_indices_vec, (pad_to_node, 1))

    full_data = np.zeros((pad_to_node, ncols), dtype=data.dtype) + \
                                                        set_missing_values
    full_data[np.reshape(node_indices, (n,)), :] = data[:, :]

    fulldset = dict(dset) # make a (superficial) copy
    fulldset['data'] = full_data
    fulldset['node_indices'] = full_node_indices

    return fulldset

def from_any(s, itemifsingletonlist=True):
    if isinstance(s, dict) and 'data' in s:
        return s
    elif isinstance(s, basestring):
        return read(s, itemifsingletonlist)
    elif isinstance(s, np.ndarray):
        return dict(data=s)
    else:
        raise ValueError('not recognized input: %r' % s)


def label2index(dset, label):
    if type(label) is list:
        return [label2index(dset, x) for x in label]

    if type(label) is int:
        sh = dset['data'].shape
        if label < 0 or label >= sh[1]:
            raise ValueError('label index %d out of bounds (0.. %d)' %
                                    (label, sh[1]))
        return label

    labels = dset.get('labels', None)
    if labels is None:
        raise ValueError('No labels found')

    for i, k in enumerate(labels):
        if k == label:
            return i

    return None

def ttest(dsets, sa_labels=None, return_values='mt',
          set_NaN_to=0., compare_to=0.):
    '''Runs a one-sample t-test across datasets

    Parameters
    ----------
    dsets: str or list of dicts
        (filenames of) NIML dsets, each referring to PxQ data for
        P nodes (features) and Q values per node (samples)
    sa_labels: list of (int or str)
        indices or labels of columns to compare
    return_values: str (default: 'mt')
        'm' or 't' or 'mt' to return sample mean, t-value, or both
    set_NaN_to: float or None (default: 0.)
        the value that NaNs in dsets replaced by. If None then NaNs are kept.
    compare_to: float (default: 0.)
        t-tests are compared against the null hypothesis of a mean of
        compare_to.

    Returns
    -------
    dset: dict
        NIML dset-compatible dict with fields 'data', 'labels',
        'stats' and 'node_indices' set.
    '''

    do_m = 'm' in return_values
    do_t = 't' in return_values

    if not (do_m or do_t):
        raise ValueError("Have to return at least m or t")

    ns = len(dsets)

    for i, dset in enumerate(dsets):
        dset = from_any(dset)
        dset_data = dset['data']
        if i == 0:
            sh = dset_data.shape
            if sa_labels is None:
                if 'labels' in dset:
                    sa_labels = dset['labels']
                    dset_labels = sa_labels
                else:
                    sa_labels = range(sh[1])
                    dset_labels = ['%d' % j for j in sa_labels]
            else:
                dset_labels = sa_labels


            nc = len(dset_labels) if dset_labels else sh[1]
            nn = sh[0]

            data = np.zeros((nn, nc, ns), dset_data.dtype) # number of nodes, columns, subjects

        if 'node_indices' in dset:
            node_idxs = np.reshape(dset['node_indices'], (-1,))
        else:
            node_idxs = np.arange(nn)

        if i == 0:
            node_idxs0 = node_idxs
        else:
            if set(node_idxs0) != set(node_idxs):
                raise ValueError("non-matching node indices for %d and %d" %
                                    (0, i))

        col_idxs = np.asarray(label2index(dset, sa_labels))

        data[node_idxs, :, i] = dset_data[:, col_idxs]

    # subtract the value it is compared to
    # so that it now tests against a mean of zero
    if do_m:
        m = np.mean(data, axis=2)

    if do_t:
        from scipy import stats
        t = stats.ttest_1samp(data - compare_to, 0., axis=2)[0]

    if do_m and do_t:
        r = np.zeros((nn, 2 * nc), dtype=m.dtype)
        r[:, np.arange(0, 2 * nc, 2)] = m
        r[:, np.arange(1, 2 * nc, 2)] = t
    elif do_t:
        r = t
    elif do_m:
        r = m

    pf = []
    stats = []
    if do_m:
        pf.append('m')
        stats.append('None')
    if do_t:
        pf.append('t')
        stats.append('Ttest(%d)' % (ns - 1))

    labs = sum([['%s_%s' % (p, lab) for p in pf] for lab in dset_labels], [])
    stats = stats * nc

    if not set_NaN_to is None:
        r[np.logical_not(np.isfinite(r))] = set_NaN_to


    return dict(data=r, labels=labs, stats=stats, node_indices=node_idxs0)


########NEW FILE########
__FILENAME__ = afni_niml_roi
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
AFNI NIML ROI (region of interest) read support

@author: Nikolaas. N. Oosterhof (nikolaas.oosterhof@unitn.it)

'''

import numpy as np
from mvpa2.support.nibabel import afni_niml

def read(fn):
    '''Reads a NIML ROI file (typical extension .niml.roi)
    
    Parameters
    ----------
    fn: str
        filename of NIML ROI file
    
    Returns
    -------
    rois: list of dict
        A list of ROIs found in fn. Each element represents a single ROI and
        is a dictionary d with keys from the header information. In addition
        it contains d['edges'], a list of numpy arrays with the node indices
        for each edge, and d['areas'], a list of numpy arrays with the node
        indices of each node
    '''

    with open(fn) as f:
        lines = f.read().split('\n')

    # some headers should be converted 
    color2array = lambda x:np.asarray(map(float, x.split()))
    header_convertors = dict(iLabel=int,
                             Type=int,
                             FillColor=color2array,
                             EdgeColor=color2array,
                             EdgeThickness=int,
                             ni_dimen=int)

    rois = []
    for line in lines:
        if not line:
            continue
        elif line.startswith('#'):

            # process header line
            if line.startswith('# <Node_ROI'):
                roi = dict()
                rois.append(roi)
            elif line.startswith('# </Node_ROI>') or line == '# >':
                continue
            elif ' = ' in line:
                k, v = line.lstrip('#').strip().split(' = ')
                v = afni_niml.decode_escape(v.strip('"'))

                if k in header_convertors:
                    f = header_convertors[k]
                    v = f(v)
                roi[k] = v
            else:
                raise ValueError("Illegal line: %s" % line)
        else:
            v = np.asarray(map(int, line.split()), dtype=np.int).ravel()

            tp = v[1]
            n = v[2]
            nodes = v[3:]

            k = {4:'edges', 1:'areas'}[tp]
            if not k in roi:
                roi[k] = []
            roi[k].append(nodes)

    return rois


def niml_roi2roi_mapping(rois):
    '''Converts NIML ROI representation in mapping from ROI labels 
    to node indices
    
    Parameters
    ----------
    roi: list of dictionaries
        NIML ROIs representation, e.g. from read()
    
    Returns
    -------
    roi_mapping: dict
        A mapping from ROI labels to numpy arrays with node indices 
        
    Notes
    -----
    It is assumed that all labels in the rois are unique, otherwise
    an exception is raised
    '''


    n = len(rois)
    keys = [roi['Label'] for roi in rois]

    if len(set(keys)) != n:
        raise ValueError("Not unique keys in %r" % keys)

    roi_mapping = dict()
    for roi in rois:
        key = roi['Label']
        all_nodes = np.zeros((0,), dtype=np.int)
        for nodes in roi['areas']:
            all_nodes = np.union1d(all_nodes, nodes)
        roi_mapping[key] = all_nodes

    return roi_mapping

def read_mapping(roi):
    '''Converts NIML ROI representation in mapping from ROI labels 
    to node indices
    
    Parameters
    ----------
    roi: list of dictionaries or str
        NIML ROIs representation, e.g. from read(), or a filename
        with ROIs specifications 
    
    Returns
    -------
    roi_mapping: dict
        A mapping from ROI labels to numpy arrays with node indices 
        
    Notes
    -----
    It is assumed that all labels in the rois are unique, otherwise
    an exception is raised
    '''
    return from_any(roi, postproc=niml_roi2roi_mapping)

def from_any(roi, postproc=None):
    '''Returns an ROI representation
    
    Parameters
    ----------
    roi: str or list
        Filename or list of ROI representations
    postproc: callable or None (default: None)
        Postprocessing that is applied after processing the ROIs
    '''

    if type(roi) is list:
        if not all(['Label' in r for r in roi]):
            raise ValueError("Not understood: list %r" % roi)
    elif isinstance(roi, basestring):
        roi = read(roi)

    if not postproc is None:
        roi = postproc(roi)

    return roi

########NEW FILE########
__FILENAME__ = afni_niml_types
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Type definitions for AFNI NIML format
Taken from afni_ni_defs() function in AFNI matlab library
which is based on niml.h

Created on Feb 16, 2012

@author: Nikolaas. N. Oosterhof (nikolaas.oosterhof@unitn.it)
'''
import numpy as np, sys

'''Currently no support for RGB and RGBA'''

np_types = [np.byte, np.int16, np.int32,
            np.float32, np.float64, np.complex64,
            None, None, str]

np_bytecounts = [1, 2, 4, 4, 8, 16, None, None, None]

python_types = [int, int, int,
              float, float, complex,
              None, None, str]

type_names = ['byte'  , 'short'  , 'int'     ,
          'float' , 'double' , 'complex' ,
          'rgb'   , 'rgba'   , 'String' ]

type_alias = [ 'uint8'   , 'int16'   , 'int32'     ,
            'float32' , 'float64' , 'complex64' ,
            'rgb8'    , 'rgba8'   , 'CString' ]
type_sep = ","

def code2python_convertor(i):
    if i in [0, 1, 2]:
        return int
    if i in [3, 4, 5]:
        return float
    if i in [8]:
        return lambda x:x.strip('"') # remove quotes
    return None

def numpy_type2bytecount(tp):
    for i, t in enumerate(np_types):
        if t is tp:
            return np_bytecounts[i]
    return None

def numpy_type2name(tp):
    code = numpy_type2code(tp)
    return _one_code2str(code)

def numpy_data_isint(data):
    return type(data) is np.ndarray and np.issubdtype(data.dtype, int)

def numpy_data_isfloat(data):
    return type(data) is np.ndarray and np.issubdtype(data.dtype, float)

def numpy_data_isdouble(data):
    return type(data) is np.ndarray and np.issubdtype(data.dtype, np.double)

def numpy_data_isstring(data):
    return type(data) is np.ndarray and np.issubdtype(data.dtype, np.str)

def numpy_data2printer(data):
    tp = type(data)
    if tp is list:
        return map(numpy_data2printer, data)
    elif tp is str:
        return lambda x : '"%s"' % x
    elif tp == np.ndarray:
        if numpy_data_isint(data):
            return lambda x : '%d' % x
        elif numpy_data_isdouble(data):
            return str
        elif numpy_data_isfloat(data):
            return lambda x : '%f' % x
        elif numpy_data_isstring(data):
            return lambda x : '"%s"' % x


    raise ValueError("Not understood type %r in %r" % (tp, data))


def code2python_type(i):
    if type(i) is list:
        return map(code2python_type, i)
    else:
        return python_types[i]

def nimldataassupporteddtype(data):
    tp = type(data)

    if tp is list:
        return map(nimldataassupporteddtype, data)

    if not type(data) is np.ndarray:
        return data

    tp = data.dtype
    if numpy_data_isfloat(data) and not np.issubdtype(tp, np.float32):
        return np.asarray(data, np.float32)

    if numpy_data_isint(data) and not np.issubdtype(tp, np.int32):
        return np.asarray(data, np.int32)

    return data


def numpy_type2code(tp):
    if type(tp) is list:
        return map(numpy_type2code, tp)
    else:
        for i, t in enumerate(np_types):
            if t == tp:
                return i

        # hack because niml does not support int64
        if tp == np.int64:
            return 2

        # bit of a hack to get string arrays converted properly 
        # XXX should we do this for other types as well?
        if isinstance(tp, np.dtype) and tp.char in ('S', 'a', 'U'):
            return 8

        raise ValueError("Unknown type %r" % tp)


def code2numpy_type(i):
    if type(i) is list:
        return map(code2numpy_type, i)
    else:
        return np_types[i]

def num_codes():
    return len(type_names)

def _one_str2code(name):
    lname = name.lower()
    for lst in [type_names, type_alias]:
        for i, v in enumerate(lst):
            if v.lower() == lname:
                return i
    return None

def _one_code2str(code):
    return type_alias[code]

def sametype(p, q):
    ascode = lambda x:_one_str2code(x) if type(x) is str else x
    pc, qc = ascode(p), ascode(q)

    if pc is None or qc is None:
        raise ValueError("Illegal type %r or %r " % (p, q))

    return pc == qc


def codes2str(codes):
    if not type(codes) is list:
        codes = [codes]
    names = [_one_code2str(code) for code in codes]
    return type_sep.join(names)

def byteorder_from_niform(niform, dtype):
    if not (niform and type(niform) is str):
        return None
    if not type(dtype) is np.dtype:
        raise ValueError("Expected numpy.dtype")

    split = niform.split(".")
    ns = len(split)
    if ns == 1:
        prefix = niform
        byteorder = 'msbfirst' # the default
    elif ns == 2:
        prefix, byteorder = split
    else:
        raise ValueError('Not understood niform')

    if prefix in ['binary', 'base64']:
        d = dict(lsbfirst='<', msbfirst='>')
        order = d.get(byteorder, None)
        return order and dtype.newbyteorder(order)
    else:
        raise ValueError("Prefix %s not understood" % prefix)

    raise ValueError('Not understood niform')

def data2ni_form(data, form):
    if (type(data) is np.ndarray and
            (numpy_data_isint(data) or numpy_data_isfloat(data))):
        byteorder = data.dtype.byteorder

        if not form in ['binary', 'base64']:
            raise ValueError('illegal form %s' % form)

        if byteorder == '=': # native order
            byteorder = '<' if sys.byteorder == 'little' else '>'

        if byteorder in '<>':
            return '%s.%s' % (form, 'lsbfirst' if byteorder == '<' else 'msbfirst')
        else:
            raise ValueError("Unrecognized byte order %s" % byteorder)

    return None


def str2codes(names):
    parts = names.lower().split(type_sep)
    codes = []
    for part in parts:
        # support either 'float' or '4*float'
        p = part.split('*')
        np = len(p)

        if np == 1:
            fac = 1 # how many 
        elif np == 2:
            fac = int(p[0])
        else:
            raise ValueError("Not understood: %s" % part)

        code = _one_str2code(p[-1]) # last element for type
        codes.extend([code] * fac)


    return codes

def findonetype(tps):
    '''tps is a list of vec_typ'''
    typeorder = str2codes(type_sep.join(['string', 'int', 'float']))

    # find correct type, default to float if not string or int
    for tp in typeorder:
        if all([i == tp for i in tps]):
            return tp

    return None



########NEW FILE########
__FILENAME__ = afni_suma_1d
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Very simple AFNI 1D support

Created on Feb 12, 2012

@author: Nikolaas. N. Oosterhof (nikolaas.oosterhof@unitn.it)
'''

import numpy as np

def write(fnout, data, nodeidxs=None):
    data = np.array(data)
    nv = data.shape[0]
    nt = 1 if data.ndim == 1 else data.shape[1]
    if nodeidxs != None:
        # make space
        alldata = np.zeros((nv, nt + 1))

        # ensure all in good shape
        nodeidxs = np.reshape(np.array(nodeidxs), (-1, 1))
        data = np.reshape(data, (nv, -1))

        # first column for node indices, remaining columns for data
        alldata[:, 0] = nodeidxs[:, 0]
        alldata[:, 1:] = data[:]
        data = alldata
        fmt = ['%d']
    else:
        fmt = []

    # 5 decimal places should be enough for everyone
    fmt.extend(['%.5f' for _ in xrange(nt)])

    np.savetxt(fnout, data, fmt, ' ')

def read(fn):
    not_empty = lambda x:len(x) > 0 and not x.startswith('#')

    with open(fn) as f:
        lines = filter(not_empty, f.read().split('\n'))

    ys = [map(float, line.split()) for line in lines]
    return np.asarray(ys)

def from_any(s):
    if isinstance(s, np.ndarray):
        return s.copy()
    elif isinstance(s, basestring):
        return read(s)

    raise TypeError("Not understood: %s" % s)

########NEW FILE########
__FILENAME__ = afni_suma_spec
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##

'''Support for ANFI SUMA surface specification (.spec) files
Includes I/O support and generating spec files that combine both hemispheres'''


import re, datetime, os, copy, glob
#from mvpa2.misc.surfing import utils, surf_fs_asc, surf
#import mvpa2.support.nibabel.afni_utils as utils
from mvpa2.support.nibabel import surf_fs_asc, surf

_COMPREFIX = 'CoM' #  for surfaces that were rotated around center of mass

class SurfaceSpec(object):
    def __init__(self, surfaces, states=None, groups=None, directory=None):
        self.surfaces = surfaces
        self.directory = directory

        if states is None:
            if surfaces is None:
                raise ValueError('No surfaces given')
            states = list(set(surface['SurfaceState'] for surface in surfaces))

        self.states = states

        if groups is None:
            groups = ['all']

        self.groups = groups
        self._fix()

    def _fix(self):
        # performs replacements of aliases to ensure consistent naming
        repls = [('freesurfersurface', 'SurfaceName')]


        for s in self.surfaces:
            for src, trg in repls:
                keys = s.keys()
                for i in xrange(len(keys)):
                    key = keys[i]
                    if key.lower() == src:
                        v = s.pop(key)
                        s[trg] = v



    def __repr__(self):
        return 'SurfaceSpec(%r)' % self.surfaces

    def __str__(self):
        return ('SurfaceSpec instance with %d surfaces, %d states (%s), ' %
                        (len(self.surfaces), len(self.states),
                         ", ".join(self.states)))


    def as_string(self):
        lines = []
        lines.append('# Created %s' % str(datetime.datetime.now()))
        lines.append('')
        lines.append('# Define the group')
        lines.extend('    Group = %s' % g for g in self.groups)
        lines.append('')
        lines.append('# Define the states')
        lines.extend('    StateDef = %s' % s for s in self.states)
        lines.append('')
        for surface in self.surfaces:
            lines.append('NewSurface')
            lines.extend('    %s = %s' % kv for kv in surface.iteritems())
            lines.append('')

        return "\n".join(lines)

    def add_surface(self, state):
        self.surfaces.append(state)
        surfstate = state['SurfaceState']
        if not surfstate in self.states:
            self.states.append(surfstate)

    def find_surface_from_state(self, surfacestate):
        return [(i, surface) for (i, surface) in enumerate(self.surfaces)
                 if surface['SurfaceState'] == surfacestate]

    def same_states(self, other):
        '''
        Returns whether another surface has the same surface states

        Parameters
        ----------
        other: SurfaceSpec

        Returns
        -------
            True iff other has the same states
        '''

        return set(self.states) == set(other.states)

    def write(self, fnout, overwrite=True):
        '''
        Writes spec to a file

        Parameters
        ----------
        fn: str
            filename where the spec is written to
        overwrite: boolean (default: True)
            overwrite the file even if it exists already.
        '''

        if not overwrite and os.path.exists(fnout):
            print '%s already exists - not overwriting' % fnout
        with open(fnout, 'w') as f:
            f.write(self.as_string())


    def get_surface(self, *args):
        '''
        Wizard-like function to get a surface

        Parameters
        ----------
        *args: list of str
            parts of the surface file name or description, such as
            'pial' (for pial surface), 'wm' (for white matter), or
            'lh' (for left hemisphere').

        Returns
        -------
        surf: surf.Surface

        '''
        return surf.from_any(self.get_surface_file(*args))

    def get_surface_file(self, *args):
        '''
        Wizard-like function to get the filename of a surface

        Parameters
        ----------
        *args: list of str
            parts of the surface file name or description, such as
            'pial' (for pial surface), 'wm' (for white matter), or
            'lh' (for left hemisphere').

        Returns
        -------
        filename: str
            filename of the surface specified, or None if no unique
            match was found.
        '''

        _FIELD_MATCH_ORDER = ['SurfaceState', 'SurfaceName']

        # start with all surfaces
        # then take fist field and see for which args match
        # if just one left, return it
        # if not succesful, try second field. etc etc

        surfs = list(self.surfaces) # list of all candidates

        for field in _FIELD_MATCH_ORDER:
            for arg in args:
                if not arg is str:
                    arg = '%s' % arg
                funcs = [lambda x: x.startswith(arg),
                         lambda x: arg in x]
                for func in funcs:
                    surfs_filter = filter(lambda x:func(x[field]), surfs)
                    if not surfs_filter:
                        continue
                    elif len(surfs_filter) == 1:
                        return os.path.join(self.directory,
                                            surfs_filter[0]['SurfaceName'])
                    # reduce list of candidates
                    surfs = surfs_filter

        return None # (redundant code, just for clarity)


def hemi_pairs_add_views(spec_both, state, ext, directory=None, overwrite=False):
    '''adds views for medial, superior, inferior, anterior, posterior viewing
    of two surfaces together. Also generates these surfaces'''

    spec_left, spec_right = spec_both[0], spec_both[1]

    if directory is None:
        directory = os.path.curdir

    if not spec_left.same_states(spec_right):
        raise ValueError('Incompatible states for left and right')

    #views = collections.OrderedDict(m='medial', s='superior', i='inferior', a='anterior', p='posterior')
    # for compatibility use a normal dict

    if state == 'inflated':
        views = dict(m='medial', s='superior', i='inferior', a='anterior', p='posterior')
        viewkeys = ['m', 's', 'i', 'a', 'p']
    else:
        views = dict(m='medial')
        viewkeys = 'm'

    spec_both = [spec_left, spec_right]
    spec_both_new = map(copy.deepcopy, spec_both)

    for view in viewkeys:
        longname = views[view]
        oldfns = []
        newfns = []
        for i, spec in enumerate(spec_both):

            idxdef = spec.find_surface_from_state(state)
            if len(idxdef) != 1:
                raise ValueError('Not unique surface with state %s' % state)
            surfidx, surfdef = idxdef[0]

            # take whichever is there (in order of preference)
            # shame that python has no builtin foldr
            surfnamelabels = ['SurfaceName', 'FreeSurferSurface']
            for surfnamelabel in surfnamelabels:
                surfname = surfdef.get(surfnamelabel)
                if not surfname is None:
                    break
            #surfname = utils.foldr(surfdef.get, None, surfnamelabels)

            fn = os.path.join(directory, surfname)
            if not os.path.exists(fn):
                raise ValueError("File not found: %s" % fn)

            if not surfname.endswith(ext):
                raise ValueError('Expected extension %s for %s' % (ext, fn))
            oldfns.append(fn) # store old name

            shortfn = surfname[:-(len(ext))]
            newsurfname = '%s%s%s%s' % (shortfn, _COMPREFIX, longname, ext)
            newfn = os.path.join(directory, newsurfname)

            newsurfdef = copy.deepcopy(surfdef)

            # ensure no naming cnoflicts
            for surfnamelabel in surfnamelabels:
                if surfnamelabel in newsurfdef:
                    newsurfdef.pop(surfnamelabel)
            newsurfdef['SurfaceName'] = newsurfname
            newsurfdef['SurfaceState'] = '%s%s%s' % (_COMPREFIX, view, state)
            spec_both_new[i].add_surface(newsurfdef)
            newfns.append(newfn)

        if all(map(os.path.exists, newfns)) and not overwrite:
            print "Output already exist for %s" % longname
        else:
            surf_left, surf_right = map(surf.read, oldfns)
            surf_both_moved = surf.reposition_hemisphere_pairs(surf_left,
                                                               surf_right,
                                                               view)

            for fn, surf_ in zip(newfns, surf_both_moved):
                surf.write(fn, surf_, overwrite)

    return tuple(spec_both_new)


def combine_left_right(leftright):
    left, right = leftright[0], leftright[1]

    if set(left.states) != set(right.states):
        raise ValueError('Incompatible states')

    mergeable = lambda x : ((x['Anatomical'] == 'Y') or
                             x['SurfaceState'].startswith(_COMPREFIX))
    to_merge = map(mergeable, left.surfaces)

    s_left, s_right = left.surfaces, right.surfaces

    hemis = ['l', 'r']
    states = [] # list of states
    surfaces = [] # surface specs
    for i, merge in enumerate(to_merge):
        ll, rr = map(copy.deepcopy, [s_left[i], s_right[i]])

        # for now assume they are in the same order for left and right
        if ll['SurfaceState'] != rr['SurfaceState']:
            raise ValueError('Different states for left (%r) and right (%r)' %
                                                                     (ll, rr))

        if merge:
            state = ll['SurfaceState']
            states.append(state)
            surfaces.extend([ll, rr])
        else:
            for hemi, surf_ in zip(hemis, [ll, rr]):
                state = '%s_%sh' % (surf_['SurfaceState'], hemi)
                states.append(state)
                surf_['SurfaceState'] = state
                surfaces.append(surf_)

    spec = SurfaceSpec(surfaces, states, groups=left.groups)

    return spec

def merge_left_right(both):
    # merges the result form combine_left_right
    # output is a tuple with the surface defintion, and a list
    # of pairs of filenames of surfaces that have to be merged

    lr_infixes = ['_lh', '_rh']
    m_infix = '_mh'

    m_surfaces = []
    m_states = []
    m_groups = both.groups

    # mapping from output filename to tuples with input file names
    # of surfaces to be merged
    merge_filenames = dict()

    _STATE = 'SurfaceState'
    _NAME = 'SurfaceName'

    for i, left in enumerate(both.surfaces):
        for j, right in enumerate(both.surfaces):
            if j <= i:
                continue

            if left[_STATE] == right[_STATE]:
                # apply transformation in naming to both
                # surfaces. result should be the same

                fns = []
                mrg = [] # versions ok to be merged

                for ii, surf_ in enumerate([left, right]):
                    newsurf = dict()
                    fns.append(surf_[_NAME])
                    for k, v in surf_.iteritems():
                        newsurf[k] = v.replace(lr_infixes[ii], m_infix)

                        # ensure that right hemi identical to left
                        if ii > 0 and newsurf[k] != mrg[ii - 1][k]:
                            raise ValueError("No match: %r -> %r" % (k, v))
                    mrg.append(newsurf)

                m_states.append(left[_STATE])
                m_surfaces.append(mrg[0])
                merge_filenames[newsurf[_NAME]] = tuple(fns)

    m = SurfaceSpec(m_surfaces, states=m_states, groups=m_groups,
                    directory=both.directory)

    return m, merge_filenames



def write(fnout, spec, overwrite=True):
    if type(spec) is str and isinstance(fnout, SurfaceSpec):
        fnout, spec = spec, fnout
    spec.write(fnout, overwrite=overwrite)

def read(fn):
    surfaces = []
    states = []
    groups = []
    current_surface = None


    surface_names = []

    with open(fn) as f:
        lines = f.read().split('\n')
        for line in lines:
            m = re.findall(r'\W*([\w\.]*)\W*=\W*([\w\.]*)\W*', line)
            if len(m) == 1:
                k, v = m[0]
                if k == 'StateDef':
                    states.append(v)
                elif k == 'Group':
                    groups.append(v)
                elif not current_surface is None:
                    current_surface[k] = v
            elif 'NewSurface' in line:
                #current_surface = collections.OrderedDict()
                # for comppatibility use a normal dict (which loses the order)
                current_surface = dict()
                surfaces.append(current_surface)

    d = os.path.abspath(os.path.split(fn)[0])

    return SurfaceSpec(surfaces=surfaces or None,
                      states=states or None,
                      groups=groups or None,
                      directory=d)


def canonical_filename(icold=None, hemi=None, suffix=None):
    if suffix is None:
        suffix = ''
    if icold is None or hemi is None:
        raise ValueError('icold (%s) or hemi (%s) are None' % (icold, hemi))
    return '%sh_ico%d%s.spec' % (hemi, icold, suffix)

def find_file(directory, icold=None, hemi=None, suffix=None):
    fn = os.path.join(directory, canonical_filename(icold=icold,
                                                    hemi=hemi,
                                                    suffix=suffix))
    if not os.path.exists(fn):
        suffix = '*'
        pat = os.path.join(directory, canonical_filename(icold=icold,
                                                         hemi=hemi,
                                                         suffix=suffix))
        fn = glob.glob(pat)

        if not fn:
            raise ValueError("not found: %s " % pat)
        elif len(fn) > 1:
            raise ValueError("not unique: %s (found %d)" % (pat, len(fn)))

        fn = fn[0]

    return fn

def from_any(*args):
    """
    Wizard-like function to get a SurfaceSpec instance from any
    kind of reasonable input.

    Parameters
    ==========
    *args: one or multiple arguments.
        If one argument and a SurfaceSpec, this is returned immediately.
        If one argument and the name of a file, it returns the contents
        of the file.
        Otherwise each argument may refer to a path, a hemisphere (if one
        of 'l','r','b','m', optionally followed by the string 'h'), a
        suffix, or an int (this is interpreted as icold); these elments
        are used to construct a canonical filename using
        afni_suma_spec.canonical_filename whose result is returned.

    Returns
    =======
    spec: SurfaceSpec
        spec as defined by parameters (if it is found)
    """

    if args is None or not args:
        return None
    if len(args) == 1:
        a = args[0]
        if isinstance(a, SurfaceSpec):
            return a
        if (isinstance(a, basestring)):
            for ext in ['', '.spec']:
                fn = a + ext
                if os.path.isfile(fn):
                    return read(fn)

    # try to be smart
    directory = icold = suffix = hemi = None
    for arg in args:
        if type(arg) is int:
            icold = arg
        elif arg in ['l', 'r', 'b', 'm', 'lh', 'rh', 'bh', 'mh']:
            hemi = arg[0]
        elif isinstance(arg, basestring):
            if os.path.isdir(arg):
                directory = arg
            else:
                suffix = arg

    if directory is None:
        directory = '.'

    fn = find_file(directory, icold=icold, suffix=suffix, hemi=hemi)
    return read(fn)

########NEW FILE########
__FILENAME__ = surf
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##'''
'''
General support for cortical surface meshes

Created on Feb 11, 2012

@author: nick
'''

import os, collections, datetime, time, heapq, math

import numpy as np

_COORD_EPS = 1e-14 # maximum allowed difference between coordinates
                   # in order to be considered equal

class Surface(object):
    '''Cortical surface mesh

    A surface consists of a set of vertices (each with an x, y, and z
    coordinate) and a set of faces (triangles; each has three indices
    referring to the vertices that make up a triangle).

    In the present implementation new surfaces should be made using the
    __init__ constructor; internal fields should not be changed manually

    Parameters
    ----------
    vertices : numpy.ndarray (float)
        Px3 array with coordinates for P vertices.
    faces : numpy.ndarray (int)
        Qx3 array with vertex indices for Q faces (triangles).
    check: boolean (default=True)
        Do some sanity checks to ensure that vertices and faces have proper
        size and values.

    Returns
    -------
    s : Surface
        a surface specified by vertices and faces
    '''
    def __init__(self, v, f=None, check=True):
        # set vertices
        v = np.asarray(v)
        if len(v.shape) != 2 or v.shape[1] != 3:
            raise ValueError("Expected Px3 array for coordinates")
        self._v = v

        # set faces
        if f is None:
            f = np.zeros((0, 3), dtype=np.int)
        else:
            f = np.asarray(f)
            if len(f.shape) != 2 or f.shape[1] != 3:
                raise ValueError("Expected Qx3 array for faces")
        self._f = f

        self._nv = v.shape[0]
        self._nf = f.shape[0]

        if check:
            self._check()

    def _check(self):
        '''ensures that different fields are sort of consistent'''
        fields = ['_v', '_f', '_nv', '_nf']
        if not all(hasattr(self, field) for field in fields):
            raise Exception("Incomplete surface!")

        if self._v.shape != (self._nv, 3):
            raise Exception("Wrong shape for vertices")

        if self._f.shape != (self._nf, 3):
            raise Exception("Wrong shape for faces")

        # see if all faces have a corresponding node.
        # actually this would not invalidate the surface, so
        # we only give a warning
        unqf = np.unique(self._f)
        if unqf.size != self._nv:
            from mvpa2.base import warning
            warning("Count mismatch for face range (%d!=%d), "
                            "faces without node: %r" % (unqf.size, self._nv,
                                    len(set(range(self._nv)) - set(unqf))))


        if np.any(unqf != np.arange(self._nv)):
            from mvpa2.base import warning
            warning("Missing values in faces")

    @property
    def node2faces(self):
        '''
        A mapping from node indices to the faces that contain those nodes.

        Returns
        -------
        n2v : dict
            A dict "n2v" so that "n2v[i]=faceidxs" contains a list of the faces
            (indexed by faceidxs) that contain node "i".

        '''

        if not hasattr(self, '_n2f'):
            # run the first time this function is called
            n2f = dict()
            for i in xrange(self._nf):
                fi = self._f[i]
                for j in xrange(3):
                    p = fi[j]
                    if not p in n2f:
                        n2f[p] = []
                    n2f[p].append(i)
            self._n2f = n2f

        return self._n2f

    @property
    def face_edge_length(self):
        '''
        Length of edges associated with each face

        Returns
        -------
        f2el: np.ndarray
            Px3 array where P==self.nfaces. f2el[i,:] contains the
            length of the (three) edges that make up face i.
        '''

        if not hasattr(self, '_f2el'):
            n, f, v = self.nfaces, self.faces, self.vertices

            f2el = np.zeros((n, 3))
            p = v[f[:, 0]] # first coordinate
            for i in xrange(3):
                q = v[f[:, (i + 1) % 3]] # second coordinate
                d = p - q # difference vector

                f2el[:, i] = np.sum(d * d, 1) ** .5 # length
                p = q

            v = f2el.view()
            v.flags.writeable = False
            self._f2el = v

        return self._f2el

    @property
    def average_node_edge_length(self):
        '''
        Average length of edges associated with each face

        Returns
        -------
        n2el: np.ndarray
            P-valued vector where P==self.nvertices, where n2el[i] is the
            average length of the edges that contain node i.
        '''
        if not hasattr(self, '_n2ael'):
            n, v, f = self.nvertices, self.vertices, self.faces

            sum_dist = np.zeros((n,))
            count_dist = np.zeros((n,))
            a = f[:, 0]
            p = v[a]
            for j in xrange(3):
                b = f[:, (j + 1) % 3]
                q = v[b]

                d = np.sum((p - q) ** 2, 1) ** .5

                count_dist[a] += 1
                count_dist[b] += 1

                sum_dist[a] += d
                sum_dist[b] += d

                a = b

            sum_dist[count_dist == 0] = 0
            count_dist[count_dist == 0] = 1

            v = (sum_dist / count_dist).view()
            v.flags.writeable = False
            self._v2ael = v

        return self._v2ael


    @property
    def edge2face(self):
        '''A mapping from edges to the face that contains that edge

        Returns
        -------
        e2f: dict
            a mapping from edges to faces. e2f[(i,j)]==f means that
            the edge connecting nodes i and j contains node f.
            It is assumed that faces are consistent with respect to
            the direction of their normals: if self.faces[j,:]==[p,q,r]
            then the normal of vectors pq and pr should all either point
            'inwards' or 'outwards'.
        '''

        if not hasattr(self, '_e2f'):
            faces = self.faces


            e2f = dict()
            for i in xrange(self.nfaces):
                for j in xrange(3):
                    e = (faces[i, j], faces[i, (j + 1) % 3])
                    if e in e2f:
                        raise ValueError('duplicate key (%d,%d). Do all normals'
                                         ' point in the same "direction"?' % e)
                    e2f[e] = i
            self._e2f = e2f

        return dict(self._e2f) # make a copy



    @property
    def neighbors(self):
        '''Finds the neighbours for each node and their (Euclidean) distance.

        Returns
        -------
        nbrs : dict
            A dict "nbrs" so that "nbrs[i]=n2d" contains the distances from
            node i to the neighbours of node "i" in "n2d". "n2d" is, in turn,
            a dict so that "n2d[k]=d" is the distance "d" from node "i" to
            node "j". In other words, nbrs[i][j]=d means that the distance from
            node i to node j is d. It holds that nbrs[i][j]=nbrs[j][i].

        Note
        ----
        This function computes nbrs if called for the first time, otherwise
        it caches the results and returns these immediately on the next call'''


        if not hasattr(self, '_nbrs'):
            nbrs = dict()
            for i in xrange(self._nf):
                fi = self._f[i]

                for j in xrange(3):
                    p = fi[j]
                    q = fi[(j + 1) % 3]

                    if p in nbrs and q in nbrs[p]:
                        continue

                    pv = self._v[p]
                    qv = self._v[q]

                    # writing this out seems a bit quicker - but have to test
                    sqdist = ((pv[0] - qv[0]) * (pv[0] - qv[0])
                           + (pv[1] - qv[1]) * (pv[1] - qv[1])
                           + (pv[2] - qv[2]) * (pv[2] - qv[2]))

                    dist = math.sqrt(sqdist)
                    if not p in nbrs:
                        nbrs[p] = dict()
                    if not q in nbrs:
                        nbrs[q] = dict()

                    nbrs[q][p] = dist
                    nbrs[p][q] = dist

            self._nbrs = nbrs

        return dict(self._nbrs) # make a copy

    def circlearound_n2d(self, src, radius, metric='euclidean'):
        '''Finds the distances from a center node to surrounding nodes.

        Parameters
        ----------
        src : int
            Index of center node
        radius : float
            Maximum distance for other nodes to qualify as a 'surrounding'
            node.
        metric : string (default: euclidean)
            'euclidean' or 'dijkstra': distance metric


        Returns
        -------
        n2d : dict
            A dict "n2d" so that n2d[j]=d" is the distance "d" from node
            "src" to node "j".
        '''

        shortmetric = metric.lower()[0] # only take first letter - for now

        if shortmetric == 'e':
            ds = self.euclidean_distance(src)
            c = dict((nd, d) for (nd, d) in zip(xrange(self._nv), ds)
                                            if d <= radius)

        elif shortmetric == 'd':
            c = self.dijkstra_distance(src, maxdistance=radius)

        else:
            raise Exception("Unknown metric %s" % metric)

        return c


    def dijkstra_distance(self, src, maxdistance=None):
        '''Computes Dijkstra distance from one node to surrounding nodes

        Parameters
        ----------
        src : int
            Index of center (source) node
        maxdistance: float (default: None)
            Maximum distance for a node to qualify as a 'surrounding' node.
            If 'maxdistance is None' then the distances to all nodes is
            returned/

        Returns:
        --------
        n2d : dict
            A dict "n2d" so that n2d[j]=d" is the distance "d" from node
            "src" to node "j".

        Note
        ----
        Preliminary analyses show that the Dijkstra distance gives very similar
        results to geodesic distances (unpublished results, NNO)
        '''


        tdist = {src:0} # tentative distances
        fdist = dict()  # final distances
        candidates = []

        # queue of candidates, sorted by tentative distance
        heapq.heappush(candidates, (0, src))

        nbrs = self.neighbors

        # algorithm from wikipedia
        # (http://en.wikipedia.org/wiki/Dijkstra's_algorithm)
        while candidates:
            # distance and index of current candidate
            d, i = heapq.heappop(candidates)

            if i in fdist:
                continue # we already have a final distance for this node

            nbr = nbrs[i] # neighbours of current candidate

            for nbr_i, nbr_d in nbr.items():
                dnew = d + nbr_d

                if not maxdistance is None and dnew > maxdistance:
                    continue # skip if too far away

                if nbr_i not in tdist or dnew < tdist[nbr_i]:
                    # set distance and append to queue
                    tdist[nbr_i] = dnew
                    heapq.heappush(candidates, (tdist[nbr_i], nbr_i))

            fdist[i] = tdist[i] # set final distance

        return fdist

    def dijkstra_shortest_path(self, src, maxdistance=None):
        '''Computes Dijkstra shortest path from one node to surrounding nodes.

        Parameters
        ----------
        src : int
            Index of center (source) node
        maxdistance: float (default: None)
            Maximum distance for a node to qualify as a 'surrounding' node.
            If 'maxdistance is None' then the shortest path to all nodes is
            returned.

        Returns:
        --------
        n2dp : dict
            A dict "n2d" so that n2d[j]=(d,p)" contains the distance "d" from
            node  "src" to node "j", and p is a list of the nodes of the path
            with p[0]==src and p[-1]==j.

        Note
        ----
        Preliminary analyses show that the Dijkstra distance gives very similar
        results to geodesic distances (unpublished results, NNO)
        '''


        tdist = {src:(0, [src])} # tentative distances and path
        fdist = dict()  # final distances
        candidates = []

        # queue of candidates, sorted by tentative distance
        heapq.heappush(candidates, (0, src))

        nbrs = self.neighbors

        # algorithm from wikipedia
        #(http://en.wikipedia.org/wiki/Dijkstra's_algorithm)
        while candidates:
            # distance and index of current candidate
            d, i = heapq.heappop(candidates)

            if i in fdist:
                continue # we already have a final distance for this node

            nbr = nbrs[i] # neighbours of current candidate

            for nbr_i, nbr_d in nbr.items():
                dnew = d + nbr_d

                if not maxdistance is None and dnew > maxdistance:
                    continue # skip if too far away

                if nbr_i not in tdist or dnew < tdist[nbr_i][0]:
                    # set distance and append to queue
                    pnew = tdist[i][1] + [nbr_i] # append current node to path
                    tdist[nbr_i] = (dnew, pnew)
                    heapq.heappush(candidates, (tdist[nbr_i][0], nbr_i))

            fdist[i] = tdist[i] # set final distance
        return fdist

    def dijkstra_shortest_path_visiting(self, to_visit):
        '''Computes a list of paths that visit specific nodes

        Parameters
        ----------
        to_visit: list of int
            P indices of nodes to visit

        Returns
        -------
        path_distances: list of tuple (int, list of int)
            List with (P-1) elements, where the i-th element is a tuple
            (d_i, q_i) with distance d_i between nodes i and (i+1), and
            q_i a list of node indices on the path between nodes i and (i+1)
            so that q_i[0]==i and q_i[-1]==(i+1)
        '''
        if to_visit is None or len(to_visit) == 0:
            raise ValueError("Cannot operate on empty list")

        src = to_visit[0]
        if not src in np.arange(self.nvertices):
            raise ValueError("%d is not a valid node index" % src)
        if len(to_visit) == 1:
            return []

        trg = to_visit[1]
        if not trg in np.arange(self.nvertices):
            raise ValueError("%d is not a valid node index" % trg)

        tdist = {src:(0, [src])} # tentative distances and path
        fdist = dict()  # final distances
        candidates = []

        # queue of candidates, sorted by tentative distance
        heapq.heappush(candidates, (0, src))

        nbrs = self.neighbors

        # algorithm from wikipedia
        #(http://en.wikipedia.org/wiki/Dijkstra's_algorithm)
        while candidates:
            # distance and index of current candidate
            d, i = heapq.heappop(candidates)

            if i in fdist:
                if i == trg:
                    break
                else:
                    continue # we already have a final distance for this node

            nbr = nbrs[i] # neighbours of current candidate

            for nbr_i, nbr_d in nbr.items():
                dnew = d + nbr_d

                if nbr_i not in tdist or dnew < tdist[nbr_i][0]:
                    # set distance and append to queue
                    pnew = tdist[i][1] + [nbr_i] # append current node to path
                    tdist[nbr_i] = (dnew, pnew)
                    heapq.heappush(candidates, (tdist[nbr_i][0], nbr_i))

            fdist[i] = tdist[i] # set final distance
            if i == trg:
                break

        if i != trg:
            raise ValueError('Node %d could not be reached from %d' %
                                                        (trg, src))

        pth = [fdist[i]]

        # recursion to find remaining paths (if any)
        pth.extend(self.dijkstra_shortest_path_visiting(to_visit[1:]))
        return pth



    def euclidean_distance(self, src, trg=None):
        '''Computes Euclidean distance from one node to other nodes

        Parameters
        ----------
        src : int or numpy.ndarray
            Index of center (source) node, or a 1x3 array with coordinates
            of the center (source) node.
        trg : int
            Target node(s) to which the distance is computed.
            If 'trg is None' then distances to all nodes are computed

        Returns:
        --------
        n2d : dict
            A dict "n2d" so that n2d[j]=d" is the distance "d" from node
            "src" to node "j".
        '''

        if type(src) is tuple and len(src) == 3:
            src = np.asarray(src)

        if isinstance(src, np.ndarray):
            if src.shape not in ((1, 3), (3,), (3, 1)):
                raise ValueError("Illegal shape: should have 3 elements")

            src_coord = src if src.shape == (1, 3) else np.reshape(src, (1, 3))
        else:
            src_coord = self._v[src]


        if trg is None:
            delta = self._v - src_coord
        else:
            delta = self._v[trg] - src_coord

        delta2 = delta * delta
        ss = np.sum(delta2, axis=delta.ndim - 1)
        d = np.power(ss, .5)
        return d

    def nearest_node_index(self, src_coords, node_mask_indices=None):
        '''Computes index of nearest node to src

        Parameters
        ----------
        src_coords: numpy.ndarray (Px3 array)
            Coordinates of center
        node_mask_idxs numpy.ndarray (default: None):
            Indices of nodes to consider. By default all nodes are considered

        Returns
        -------
        idxs: numpy.ndarray (P-valued vector)
            Indices of nearest nodes
        '''

        if not isinstance(src_coords, np.ndarray):
            src_coords = np.asarray(src_coords)
        if len(src_coords.shape) == 1:
            if src_coords.shape[0] != 3:
                raise ValueError("no three values for src_coords")
            else:
                src_coords = np.reshape(src_coords, (1, -1))
        elif len(src_coords.shape) != 2 or src_coords.shape[1] != 3:
            raise ValueError("Expected Px3 array for src_coords")

        use_mask = not node_mask_indices is None
        # vertices to consider
        v = self.vertices[node_mask_indices] if use_mask else self.vertices

        # indices of these vertices
        all_idxs = np.arange(self.nvertices)
        masked_idxs = all_idxs[node_mask_indices] if use_mask else all_idxs

        n = src_coords.shape[0]
        idxs = np.zeros((n,), dtype=np.int)
        for i in xrange(n):
            delta = v - src_coords[i]
            minidx = np.argmin(np.sum(delta ** 2, 1))
            idxs[i] = masked_idxs[minidx]

        return idxs

    def nodes_on_border(self, node_indices=None):
        '''Determines which nodes are on the border of the surface

        Parameters
        ----------
        node_indices: np.ndarray or None
            Vector with node indices for which their bordership status is to
            be deteremined. None means all node indices

        Returns
        -------
        on_border: np.ndarray
            Boolean array of shape (len(node_indices),). A node i is
            considered on the border if there is a face that contains node i
            and another node j so that no other face contains both i and j.
            In other words a node i is *not* on the border if there is a path
            of nodes p1,...pN so that N>1, p1==pN, pj!=pk if j!=k<N, and
            each node pk (and no other node) is a neighbor of node i.
        '''

        if node_indices is None:
            node_indices = np.arange(self.nvertices)

        if not isinstance(node_indices, np.ndarray):
            node_indices = np.asarray(node_indices)[np.newaxis]

        if len(node_indices.shape) != 1:
            raise ValueError("Only supported for vectors")

        n = len(node_indices)
        on_border = np.zeros((n,), dtype=np.bool_) # allocate space for output

        n2f = self.node2faces
        f = self.faces

        def except_(vs, x):
            return filter(lambda y:y != x, vs)

        for i, node_index in enumerate(node_indices):
            face_indices = n2f[node_index]
            nf = len(face_indices)

            # node indices of neighbouring nodes (one for each face containing
            # node with index node_index)
            fs = [except_(f[fi], node_index) for fi in face_indices]

            a = np.asarray(fs)
            if a.size == 0:
                continue

            # initial position and value
            ipos, jpos = 0, 0
            a_init = a[ipos, jpos]

            for j in xrange(nf):
                # go over the faces that contain node_index
                # for each row take the other value, and try to match
                # it to another face
                jpos_ = (jpos + 1) % 2
                target = a[ipos, jpos_]
                a[ipos, jpos_] = -1 # is visited

                ijpos = np.nonzero(a == target)
                if len(ijpos[0]) != 1:
                    #
                    on_border[i] = True
                    break
                ipos, jpos = ijpos[0], ijpos[1]

            on_border[i] = on_border[i] or target != a_init

        return on_border


    def nodes_on_border_paths(self):
        '''Find paths of nodes on the border

        Returns
        -------
        paths: list of lists
            paths[i]=[k_0,...k_N] means that there is path of N+1 edges
            [(k_0,k_1),(k_1,...,k_N),(k_N,k_0)] where each k_i is on the
            border of the surface

        '''
        border_mask = self.nodes_on_border()
        faces = self.faces
        nbrs = self.neighbors
        border_nodes = set(np.nonzero(border_mask)[0])
        if not len(border_nodes):
            return []

        # for each edge, see which is the next edge
        # in the same triangle (clock-wise)
        edge2next = dict()
        for i in xrange(self.nfaces):
            for j in xrange(3):
                p, q, r = faces[i]

                # make edges
                pp, qq, rr = (p, q), (q, r), (r, p)

                edge2next[pp] = qq
                edge2next[qq] = rr
                edge2next[rr] = pp

        # mapping from edge to face
        e2f = self.edge2face

        pths = [] # space for output
        while border_nodes:
            b0 = border_nodes.pop() # select a random node on the border
            ns = [b for b in nbrs[b0] if b in border_nodes]
            if not ns:
                # not a proper node - no neighbors - so skip
                continue

            # find an edge on the border
            for n in ns:
                edge = (b0, n)
                if edge in edge2next:
                    break

            if not edge in edge2next:
                # this should not happen really
                raise ValueError("no edge on border found")

            # start a path
            pth = []
            pths.append(pth)
            while True:
                p, q = edge2next[edge]

                if (q, p) in e2f:
                    # node q is 'inside' - not on the border
                    # continue looking
                    edge = (q, p)
                else:
                    # on the border, so swap
                    edge = (p, q)
                    pth.append(p) # p is on the border
                    if p in border_nodes:
                        border_nodes.remove(p)
                    else:
                        # we made a tour and back to the starting point
                        break

        return pths




    def pairwise_near_nodes(self, max_distance=None, src=None, trg=None):
        '''Finds the distances between pairs of nodes

        Parameters
        ----------
        max_distance: None or float
            maximum distance (None: no maximum distance)
        src: array of int or None
            source indices
        trg: array of int or None
            target indices

        Returns
        -------
        source_target2distance: dict
            A dictionary so that source_target2distance[i,j]=d means that the
            Euclidean distance between nodes i and j is d, where i in src
            and j in trg.

        Notes
        -----
        If src and trg are both None, then this function checks if the surface
        has two components; if so they are taken as source and target. A use
        case for this behaviour is a surface consisting of two hemispheres
        '''

        if src is None and trg is None:
            components = self.connected_components()
            if len(components) != 2:
                raise ValueError("Empty src and trg: requires two components")
            src, trg = (np.asarray([i for i in c]) for c in components)

        v = self.vertices
        if not max_distance is None:
            # hopefully we can reduce the number of vertices significantly
            # if src and trg can be seperated easily (as in the case of
            # two hemispheres).

            # vector connecting centers of mass of src and trg
            n = np.mean(v[src], 0) - np.mean(v[trg], 0)

            # normalize
            n /= np.sum(n ** 2) ** .5

            # compute projection on normal
            ps = self.project_vertices(n, v[src])
            pt = self.project_vertices(n, v[trg])

            def remove_far(s, t, ps, pt, max_distance=max_distance):
                keep_idxs = np.arange(len(s))
                for sign in (-1, 1):
                    far_idxs = np.nonzero(sign * ps[keep_idxs] + \
                                            max_distance < min(sign * pt))[0]

                    keep_idxs = np.setdiff1d(keep_idxs, far_idxs)

                return s[keep_idxs]

            src, trg = remove_far(src, trg, ps, pt), \
                                remove_far(trg, src, pt, ps)

        st2d = dict() # source-target pair to distance
        for s in src:
            ds = self.euclidean_distance(s, trg)
            for t, d in zip(trg, ds):
                if max_distance is None or d <= max_distance:
                    st2d[(s, t)] = d


        return st2d

    def project_vertices(self, n, v=None):
        '''Projects vertex coordinates onto a vector

        Parameters
        ----------
        n: np.ndarray
            Vector with 3 elements
        v: np.ndarray or None
            coordinates to be projected. If None then the vertices of the
            current instance are used.

        Returns
        -------
        p: np.ndarray
            Vector with coordinates projected onto n
        '''

        if not isinstance(n, np.ndarray):
            n = np.asarray(n)
        if n.shape != (3,):
            raise ValueError("Expected vector with 3 elements, found %s" % ((n.shape,)))

        if v is None:
            v = self.vertices

        return np.dot(v, n)

    def sub_surface(self, src, radius):
        '''Makes a smaller surface consisting of nodes around a center node

        Parameters
        ----------
        src : int
            Index of center (source) node
        radius : float
            Lower bound of (Euclidean) distance to 'src' in order to be part
            of the smaller surface. In other words, if a node 'j' is within
            'radius' from 'src', then 'j' is also part of the resulting surface.

        Returns
        -------
        small_surf: Surface
            a smaller surface containing nodes surrounding 'src'
        nsel: np.array (int)
            indices of nodes selected from the original surface
        fsel: np.array (int)
            indices of faces selected from the original surface
        orig_src: int
            index of 'src' in the original surface

        Note
        ----
        This function is a port from the Matlab surfing toolbox function
        'surfing_subsurface' (see http://surfing.sourceforge.net)

        With the 'dijkstra_distance' function, this function is more or
        less obsolete.


        '''
        n2f = self.node2faces

        msk = self.euclidean_distance(src) <= radius

        # node indices of those within distance r
        vidxs = [i for i, m in enumerate(msk) if m]

        # unique face indices that contain nodes within that distance
        funq = list(set.union(*[set(n2f[vidx]) for vidx in vidxs]))

        # these are the node indices contained in one of the faces
        fsel = self._f[funq, :]

        # selected nodes
        nsel, q = np.unique(fsel, return_inverse=True)

        nsel = np.array(nsel, dtype=int)
        fsel = np.array(fsel, dtype=int)

        sv = self._v[nsel, :] # sub_surface from selected nodes
        sf = np.reshape(q, (-1, 3)) # corresponding faces

        ss = Surface(v=sv, f=sf, check=False) # make a new sub_surface

        # find the src node corresponding to the sub_surface
        for ss_src, sel in enumerate(nsel):
            if sel == src:
                break
        else:
            # do not expect this, but for now it's ok
            ss_src = None

        return ss, nsel, fsel, ss_src

    def __repr__(self, prefixes=[]):
        prefixes_ = ['v=%r' % self._v, 'f=%r' % self._f] + prefixes
        return "%s(%s)" % (self.__class__.__name__, ', '.join(prefixes_))

    def __str__(self):
        # helper function to print coordinates. f should be np.min or np.max
        func_coord2str = lambda f: '%.3f %.3f %.3f' % tuple(
                                                        f(self.vertices, 0))

        return '%s(%d vertices (range %s ... %s), %d faces)' % (
                        self.__class__.__name__,
                        self.nvertices,
                        func_coord2str(np.min),
                        func_coord2str(np.max),
                        self.nfaces)


    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False

        sv = self.vertices
        ov = other.vertices

        # must be identical where NaNs occur
        if np.any(np.logical_xor(np.isnan(sv), np.isnan(ov))):
            return False

        # difference in vertices
        v = np.abs(self.vertices - other.vertices)

        return (np.all(np.logical_or(v < _COORD_EPS, np.isnan(v)))
                and np.all(self.faces == other.faces))

    def __ne__(self, other):
        return not self.__eq__(other)

    def __reduce__(self):
        # these are lazily computed on the first call to e.g. node2faces
        lazy_keys = ('_n2f', '_f2el', '_v2ael', '_e2f', '_nbrs')
        lazy_dict = dict()
        # TODO: add in efficient way to translate these dictionaries
        #       to something like a numpy array, and implement the 
        #       translation back. Types for these dicts:
        #       _n2f: int -> [int]
        #       _f2el: array
        #       _v2ael: array
        #       _e2f: (int,int) -> int
        #       _nbrs: int -> (int -> float)
        #       
        # For now this this functionaltiy is switched off,
        # because pickling it (also with hdf5) takes a long time
        #for lazy_key in lazy_keys:
        #    if lazy_key in self.__dict__:
        #        lazy_dict[lazy_key] = self.__dict__[lazy_key]


        return (self.__class__, (self._v, self._f), lazy_dict)

    def same_topology(self, other):
        '''
        Returns whether another surface has the same topology

        Parameters
        ----------
        other: surf.Surface
            another surface

        Returns
        -------
        bool
            True iff the current surface has the same number of coordinates and the
            same faces as 'other'. '''

        return self._v.shape == other._v.shape and np.array_equal(self._f, other._f)

    def __add__(self, other):
        '''coordinate-wise addition of two surfaces with the same topology'''
        if isinstance(other, Surface):
            if not self.same_topology(other):
                raise Exception("Different topologies - cannot add")
            vother = other.vertices
        else:
            vother = other

        return Surface(v=self.vertices + vother, f=self.faces, check=False)

    def __mul__(self, other):
        '''coordinate-wise scaling'''
        return Surface(v=self._v * other, f=self.faces, check=False)

    def __neg__(self, other):
        '''coordinate-wise inversion with respect to addition'''
        return Surface(v=-self.vertices, f=self.faces, check=False)

    def __sub__(self, other):
        '''coordiante-wise subtraction'''
        return self +(-other)

    def rotate(self, theta, center=None, unit='rad'):
        '''Rotates the surface

        Parameters
        ----------
        theta:
            np.array with 3 values for rotation along x, y, z axes
        center:
            np.array with center around which surface is rotated. If None,
            then rotation is around the origin (0,0,0).
        unit:
            'rad' or 'deg' for angles in theta in either radians or degrees.

        Returns
        -------
        surf.Surface
            the result after rotating with angles theta around center.
        '''

        if unit.startswith('rad'):
            fac = 1.
        elif unit.startswith('deg'):
            fac = math.pi / 180.
        else:
            raise ValueError('Illegal unit for rotation: %r' % unit)

        theta = map(lambda x:x * fac, theta)


        cx, cy, cz = np.cos(theta)
        sx, sy, sz = np.sin(theta)

        # rotation matrix *in row-first order*
        # in other words, we compute vertices*R'
        m = np.asarray(
                [[cy * cz, -cy * sz, sy],
                 [cx * sz + sx * sy * cz, cx * cz - sx * sy * sz, -sx * cy],
                 [sx * sz - cx * sy * cz, sx * cz + cx * sy * sz, cx * cy]])

        if center is None:
            center = 0
        center = np.reshape(np.asarray(center), (1, -1))

        v_rotate = center + np.dot(self._v - center, m)

        return Surface(v=v_rotate, f=self._f)

    @property
    def center_of_mass(self):
        '''Computes the center of mass

        Returns
        -------
        np.array
            3-value vector with x,y,z coordinates of center of mass
        '''
        return np.mean(self.vertices, axis=0)

    def merge(self, *others):
        '''Merges the present surface with other surfaces

        Parameters
        ----------
        others: list of surf.Surface
            List of other surfaces to be merged with present one

        Returns
        -------
        surf.Surface
            A surface that has all the nodes of the current surface
            and the surfaces in others, and has the topologies combined
            from these surfaces as well.
            If the current surface has v_0 vertices and f_0 faces, and the
            i-th surface has v_i and f_i faces, then the output has
            sum_j (v_j) vertices and sum_j (f_j) faces.
        '''

        all = [self]
        all.extend(list(others))
        n = len(all)

        def border_positions(xs, f, dt):
            # positions of transitions between surface
            # faces should return number of relevant values (nodes or vertices)
            n = len(xs)

            fxs = map(f, all)

            positions = [0]
            for i in xrange(n):
                positions.append(positions[i] + fxs[i])

            zeros_arr = np.zeros((positions[-1], xs[0].vertices.shape[1]),
                                        dtype=dt)
            return positions, zeros_arr


        pos_v, all_v = border_positions(all, lambda x:x.nvertices,
                                                self.vertices.dtype)
        pos_f, all_f = border_positions(all, lambda x:x.nfaces,
                                                self.faces.dtype)

        for i in xrange(n):
            all_v[pos_v[i]:pos_v[i + 1], :] = all[i].vertices
            all_f[pos_f[i]:pos_f[i + 1], :] = all[i].faces + pos_v[i]

        return Surface(v=all_v, f=all_f)


    def split_by_connected_components(self):
        '''Splits a surface by its connected components

        Returns
        -------
        splits: list of surf.Surface
            A list of all surfaces that make up the original surface,
            split when they are not connected to each other.
            (If all nodes in the original surface are connected
            then a list is returned with a single surface that is
            identical to the input).
            The output is sorted by the number of vertices.

        '''
        components = self.connected_components()
        n2f = self.node2faces

        n = len(components)
        splits = []

        face_mask = np.zeros((self.nfaces,), dtype=np.False_)
        for i, component in enumerate(components):
            face_mask[:] = False

            node_idxs = np.asarray(list(component))
            for node_idx in node_idxs:
                face_mask[n2f[node_idx]] = True

            nodes = self.vertices[node_idxs, :]

            face_idxs = np.nonzero(face_mask)[0]
            unq, unq_inv = np.unique(self.faces[face_idxs], False, True)
            faces = np.reshape(unq_inv, (-1, 3))

            s = Surface(nodes, faces)
            splits.append(s)

        splits.sort(key=lambda x:x.nvertices)
        return splits



    @property
    def vertices(self):
        '''
        Returns
        -------
        vertices: numpy.ndarray (int)
            Px3 coordinates for P vertices
        '''

        v = self._v.view()
        v.flags.writeable = False

        return v

    @property
    def faces(self):
        '''
        Returns
        -------
        faces: numpy.ndarray (float)
            Qx3 coordinates for Q vertices
        '''
        f = self._f.view()
        f.flags.writeable = False

        return f

    @property
    def nvertices(self):
        '''
        Returns
        -------
        nvertices: int
            Number of vertices
        '''
        return self._nv

    @property
    def nfaces(self):
        '''
        Returns
        -------
        nfaces: int
            Number of faces
        '''
        return self._nf



    def map_to_high_resolution_surf_slow(self, highres, epsilon=.001,
                                         accept_only_icosahedron=False):
        '''
        Finds a mapping to a higher resolution (denser) surface.
        A typical use case is mappings between surfaces generated by
        MapIcosahedron, where the lower resolution surface defines centers
        in a searchlight whereas the higher resolution surfaces is used to
        delineate the grey matter for voxel selection. Unlike the function
        named "map_to_high_resolution_surf", this function is both slow
        and exact---and is actually used in case the former function does
        not find a solution.

        Parameters
        ----------
        highres: surf.Surface
            high resolution surface
        epsilon: float
            maximum margin (distance) between nodes mapped from low to
            high resolution surface
        accept_only_icosahedron: bool
            if True, then this function raises an error if the number of
            nodes does not match those which would be expected from
            MapIcosahedorn.

        Returns
        -------
        low2high: dict
            mapping so that low2high[i]==j means that node i in the current
            (low-resolution) surface is mapped to node j in the highres
            surface.

        '''
        nx = self.nvertices
        ny = highres.nvertices

        if accept_only_icosahedron:
            def getld(n):
                # a mapicosahedron surface with LD linear divisions has
                # N=10*LD^2+2 nodes
                ld = ((nx - 2) / 10) ** 2
                if ld != int(ld):
                    raise ValueError("Not from mapicosahedron with %d nodes" % n)
                return int(ld)

            ldx, ldy = map(getld, (nx, ny))
            r = ldy / ldx # ratio

            if int(r) != r:
                raise ValueError("ico linear divisions for high res surface (%d)"
                                 "should be multiple of that for low res surface (%d)",
                                 (ldy, ldx))

        mapping = dict()
        x = self.vertices
        y = highres.vertices

        # shortcut in case the surfaces are the same
        # if this fails, then we just continue normally
        if self.same_topology(highres):
            d = np.sum((x - y) ** 2, axis=1) ** .5
            if all(d[np.logical_not(np.isnan(d))] < epsilon):
                for i in xrange(nx):
                    mapping[i] = i
                return mapping

        if nx > ny:
            raise ValueError("Other surface has fewer nodes (%d) than this one (%d)" %
                             (nx, ny))

        for i in xrange(nx):
            ds = np.sum((x[i, :] - y) ** 2, axis=1)
            minpos = np.argmin(ds)

            mind = ds[minpos] ** .5

            if not epsilon is None and mind > epsilon:
                print minpos
                raise ValueError("Not found near node for node %i (min distance %f > %f)" %
                                 (i, mind, epsilon))
            mapping[i] = minpos

        return mapping

    def coordinates_to_box_indices(self, box_size, min_coord=None,
                                                   master=None):
        ''''Boxes' coordinates into triples

        Parameters
        ----------
        box_sizes:

        min_coord: triple or ndarray
            Minimum coordinates; maps to (0,0,0).
            If omitted, it defaults to the mininum coordinates in this surface.
        max_coord: triple or ndarray
            Minimum coordinates; maps to (nboxes[0]-1,nboxes[1]-1,nboxes[2]-1)).
            If omitted, it defaults to the maximum coordinates in this surface.
        master: Surface.surf (default: None)
            If provided, then min_coord and max_coord are taken from master.

        Returns
        -------
        boxes_indices: np.ndarray of float
            Array of size Px3, where P is the number of vertices
        '''

        box_sizes = np.asarray([box_size, box_size, box_size]).ravel()
        box_sizes = np.reshape(box_sizes, (1, 3))

        if not master is None:
            if min_coord:
                raise ValueError('Cannot have both {min,max}_coord and master')
            c = master.vertices
        else:
            c = self.vertices

        if min_coord is None:
            min_coord = np.min(c, 0)
        else:
            min_coord = np.asarray(min_coord).ravel()

        return (self.vertices - min_coord) / box_sizes


    def map_to_high_resolution_surf(self, highres, epsilon=.001,
                                    accept_only_icosahedron=False):
        '''
        Finds a mapping to a higher resolution (denser) surface.
        A typical use case is mappings between surfaces generated by
        MapIcosahedron, where the lower resolution surface defines centers
        in a searchlight whereas the higher resolution surfaces is used to
        delineate the grey matter for voxel selection.
        This function implements an optimization which in most cases
        yields solutions much faster than map_to_high_resolution_surf_exact,
        but may fail to find the correct solution for larger values
        of epsilon.

        Parameters
        ----------
        highres: surf.Surface
            high resolution surface
        epsilon: float
            maximum margin (distance) between nodes mapped from low to
            high resolution surface. Default None, which implies .001.
        accept_only_icosahedron: bool
            if True, then this function raises an error if the number of
            nodes does not match those which would be expected from
            MapIcosahedorn.

        Returns
        -------
        low2high: dict
            mapping so that low2high[i]==j means that node i in the current
            (low-resolution) surface is mapped to node j in the highres
            surface.

        '''

        nx = self.nvertices
        ny = highres.nvertices

        if accept_only_icosahedron:
            def getld(n):
                # a mapicosahedron surface with LD linear divisions has
                # N=10*LD^2+2 nodes
                ld = ((nx - 2) / 10) ** 2
                if ld != int(ld):
                    raise ValueError("Not from mapicosahedron with %d nodes" % n)
                return int(ld)

            ldx, ldy = map(getld, (nx, ny))
            r = ldy / ldx # ratio

            if int(r) != r:
                raise ValueError("ico linear divisions for high res surface (%d)"
                                 "should be multiple of that for low res surface (%d)",
                                 (ldy, ldx))

        mapping = dict()
        x = self.vertices
        y = highres.vertices

        # shortcut in case the surfaces are the same
        # if this fails, then we just continue normally
        if self.same_topology(highres):
            d = np.sum((x - y) ** 2, axis=1) ** .5

            if all(d[np.logical_not(np.isnan(d))] < epsilon):
                for i in xrange(nx):
                    mapping[i] = i
                return mapping

        if nx > ny:
            raise ValueError("Other surface has fewer nodes (%d) than "
                             "this one (%d)" % (nx, ny))


        # use a fast approach
        # slice up the high and low res in smaller boxes
        # and index them, so that when finding the nearest coordinates
        # it only requires to consider a limited number of nodes
        n_boxes = 20
        box_size = max(np.max(x, 0) - np.min(x, 0)) / n_boxes

        x_boxed = self.coordinates_to_box_indices(box_size, master=highres) + .5
        y_boxed = highres.coordinates_to_box_indices(box_size) + .5

        # get indices of nodes that are very near a box boundary
        delta = epsilon / box_size
        on_borders = np.nonzero(np.logical_or(\
                        np.floor(y_boxed + delta) - np.floor(y_boxed) > 0,
                        np.floor(y_boxed) - np.floor(y_boxed - delta) > 0))[0]

        # on_borders may have duplicates - so get rid of those.
        msk = np.zeros((ny,), dtype=np.int)
        msk[on_borders] = 1
        on_borders = np.nonzero(msk)[0]

        # convert to tuples with integers for indexing
        # (tuples are hashable so can be used as keys in dictionary)
        x_tuples = map(tuple, np.asarray(x_boxed, dtype=np.int))
        y_tuples = map(tuple, np.asarray(y_boxed, dtype=np.int))

        # maps box indices in low-resolution surface to indices
        # of potentially nearby nodes in highres surface
        x_tuple2near_indices = dict()

        # add border nodes to all low-res surface
        # this is a bit inefficient
        # TODO optimize to consider neighboorhood
        for x_tuple in x_tuples:
            x_tuple2near_indices[x_tuple] = list(on_borders)

        # for non-border nodes in high-res surface, add them to
        # a single box
        for i, y_tuple in enumerate(y_tuples):
            if i in on_borders:
                continue # because it was added above

            if not y_tuple in x_tuple2near_indices:
                x_tuple2near_indices[y_tuple] = list()
            x_tuple2near_indices[y_tuple].append(i)

        # it now holds that for every node i in low-res surface (which is
        # identified by t=x_tuples[i]), there is no node j in the high-res surface
        # within distance epsilon for which j in x_tuple2near_indices[t]

        for i, x_tuple in enumerate(x_tuples):
            i_xyz = x[i, :]
            if np.any(np.isnan(i_xyz)):
                continue

            idxs = np.asarray(x_tuple2near_indices[x_tuple])

            ds = np.sum((x[i, :] - y[idxs, :]) ** 2, axis=1)

            not_nan_idxs = np.nonzero(np.logical_not(np.isnan(ds)))[0]
            if len(not_nan_idxs) == 0:
                raise ValueError("Empty sequence: is center %d (%r)"
                                 " illegal?" % (i, (x[i],)))

            minpos = not_nan_idxs[np.argmin(ds[not_nan_idxs])]

            mind = ds[minpos] ** .5

            if not epsilon is None and not (mind < epsilon):
                raise ValueError("Not found for node %i: %s > %s" %
                                        (i, mind, epsilon))

            mapping[i] = idxs[minpos]

        return mapping

    def vonoroi_map_to_high_resolution_surf(self, highres_surf,
                                highres_indices=None, epsilon=.001,
                                    accept_only_icosahedron=False):
        '''
        Computes a Vonoroi mapping for the current (low-res) surface

        Parameters
        ----------
        highres_surf: Surface
            High-resolution surface.
        highres_indices: np.ndarray
            List of indices in high-res surface that have to be mapped.
        epsilon: float
            maximum margin (distance) between nodes mapped from low to
            high resolution surface. Default None, which implies .001.
        accept_only_icosahedron: bool
            if True, then this function raises an error if the number of
            nodes does not match those which would be expected from
            MapIcosahedorn.

        Returns
        -------
        high2high_in_low: dict
            A mapping so that high2high_in_low[high_idx]=(high_in_low_idx,d)
            means that the node on the high-res surface indexed by high_idx is
            nearest (in a Dijsktra distance sense) distance d to the node on the
            high-res surface high_in_low_idx that has a corresponding
            node on the low-res surface
        '''

        # the set of indidces that will serve as keys in high2high_in_low
        if highres_indices is None:
            highres_indices = np.arange(highres_surf.nvertices)
        highres_indices = set(highres_indices)


        low2high = self.map_to_high_resolution_surf(highres_surf, epsilon,
                                                  accept_only_icosahedron)



        # reverse mapping, only containing nodes that are both in
        # highres_indices and have a partner in self (lowres)
        high2low = dict((v, k) for k, v in low2high.iteritems()
                                if v in highres_indices)

        # node indices in high-res surface that have a mapping
        # and thus are acceptable
        highres_center_set = set(high2low)


        # starting value for radius
        radius = np.mean(self.average_node_edge_length)
        max_radius = radius * 10000.

        # set of node indices of low-res surface
        lowres_node_set = set(xrange(self.nvertices))

        # space for output
        high2high_in_low = dict()

        # continue increasing radius until all high-res nodes
        # have been mapped to a low-res node
        while set(high2high_in_low) != highres_indices:
            for highres_index in highres_indices:
                if highres_index in high2high_in_low:
                    # already has a low-res node mapped to it
                    continue

                # compute distances in high-res surface
                ds = highres_surf.dijkstra_distance(highres_index, radius)

                common = set.intersection(set(ds), highres_center_set)

                if len(common):
                    # keep only distances to allowed nodes
                    small_ds = dict((k, v) for k, v in ds.iteritems() if k in common)

                    # find nearest node
                    nearest_node_highres = min(small_ds, key=small_ds.get)
                    d = small_ds[nearest_node_highres]

                    # store the result
                    high2high_in_low[highres_index] = (nearest_node_highres, d)

            radius *= 2

            if radius > max_radius:
                # safety mechanism to avoid endless loop
                raise RuntimeError("Radius increased to %d - too big" % radius)


        return high2high_in_low


    @property
    def face_areas(self):
        if not hasattr(self, '_face_areas'):
            f = self.faces
            v = self.vertices

            # consider three sides of each triangle
            a = v[f[:, 0]]
            b = v[f[:, 1]]
            c = v[f[:, 2]]

            # vectors of two sides
            ab = a - b
            ac = a - c

            # area (from wikipedia)
            f2a = .5 * np.sqrt(np.sum(ab * ab, 1) * np.sum(ac * ac, 1) -
                               np.sum(ab * ac, 1) ** 2)

            vw = f2a.view()
            vw.flags.writeable = False
            self._face_areas = vw

        return self._face_areas

    @property
    def node_areas(self):
        if not hasattr(self, '_node_areas'):
            f2a = self.face_areas

            # area is one third of sum of faces that contain the node
            n2a = np.zeros((self.nvertices,))
            for v, fs in self.node2faces.iteritems():
                n2a[v] = sum(f2a[fs]) / 3.

            vw = n2a.view()
            vw.flags.writeable = False
            self._node_areas = vw

        return self._node_areas

    @property
    def face_normals(self):
        if not hasattr(self, '_face_normals'):
            f = self.faces
            v = self.vertices

             # consider three sides of each triangle
            a = v[f[:, 0]]
            b = v[f[:, 1]]
            c = v[f[:, 2]]

            # vectors of two sides
            ab = a - b
            ac = a - c

            abXac = np.cross(ab, ac)
            n = normalized(abXac)

            vw = n.view()
            vw.flags.writeable = False

            self._face_normals = vw

        return self._face_normals

    @property
    def node_normals(self):
        if not hasattr(self, '_node_normals'):
            f = self.faces
            v = self.vertices
            n = self.nfaces

            f_nrm = self.face_normals

            v_sum = np.zeros(v.shape, dtype=v.dtype)
            for i in xrange(3):
                for j in xrange(n):
                    v_sum[f[j, i]] += f_nrm[j]

            v_nrm = normalized(v_sum)

            vw = v_nrm.view()
            vw.flags.writeable = False

            self._node_normals = vw

        return self._node_normals

    def connected_components(self):
        nv = self.nvertices

        components = []
        visited = set()

        nbrs = self.neighbors
        for i in xrange(nv):
            if i in visited:
                continue

            component = set([i])
            components.append(component)

            nbr = nbrs[i]
            candidates = set(nbr)

            visited.add(i)
            while candidates:
                candidate = candidates.pop()
                component.add(candidate)
                visited.add(candidate)
                nbr = nbrs[candidate]

                for n in nbr:
                    if not n in visited:
                        candidates.add(n)

        return components

    def connected_components_slow(self):
        f, nv, nf = self.faces, self.nvertices, self.nfaces

        node2component = dict()

        def index_component(x):
            if not x in node2component:
                return x, None

            k, v = x, node2component[x]
            while not type(v) is set:
                k, v = v, node2component[v]

            return k, v

        for i in xrange(nf):
            p, q, r = f[i]

            pk, pv = index_component(p)
            qk, qv = index_component(q)
            rk, rv = index_component(r)

            if pv is None:
                if qv is None:
                    if rv is None:
                        node2component[p] = set([p, q, r])
                        node2component[q] = node2component[r] = p
                    else:
                        rv.add(p)
                        rv.add(q)
                        node2component[p] = node2component[q] = rk
                else:
                    if rv is None:
                        qv.add(p)
                        qv.add(r)
                        node2component[p] = node2component[r] = qk
                    else:
                        qv.add(p)
                        node2component[p] = qk
                        if qk != rk:
                            qv.update(rv)
                            node2component[rk] = qk
            else:
                if qv is None:
                    if rv is None:
                        pv.add(q)
                        pv.add(r)
                        node2component[q] = node2component[r] = pk
                    else:
                        if pk != rk:
                            pv.update(rv)
                            node2component[rk] = pk
                        pv.add(q)
                        node2component[q] = pk
                else:
                    if rv is None:
                        if pk != qk:
                            pv.update(qv)
                            node2component[qk] = pk
                        pv.add(r)
                        node2component[r] = pk
                    else:
                        if pk != qk:
                            pv.update(qv)
                            node2component[qk] = pk
                        if pk != rk:
                            if rk != qk:
                                pv.update(rv)
                            node2component[rk] = pk

        components = list()
        for node in xrange(nv):
            v = node2component[node]
            if type(v) is set:
                components.append(v)

        return components


    def write(self, fn):
        write(fn, self)

def reposition_hemisphere_pairs(surf_left, surf_right, facing_side,
                          min_distance=10.):
    '''moves and rotates pairs of hemispheres so that they are facing each
    other on one side, good for visualization. It is assumed that the input
    surfaces were generated by FreeSurfer's recon-all.

    Parameters
    ----------
    surf_left: surf.Surface
        surface of left hemisphere
    surf_right: surf.Surface
        surface of right hemisphere
    facing_side: str
        determines on which sides the surfaces should be facing each other.
        'm'=medial,'i'=inferior, 's'=superior, 'a'=anterior,'p'=posterior


    '''
    facing_side = facing_side[0].lower()

    mn, mx = np.min, np.max
    #min=-1, max=1
    side2dimsigns = dict(m=(0, -1), i=(1, 1), s=(1, -1), a=(2, 1), p=(2, -1))

    dim, rotatesign = side2dimsigns[facing_side]
    if dim == 0:
        rotate_axis = None
    else:
        rotate_axis = dim #1+((dim+1) % 2)
        rotate_angle = 90

    surfs = [surf_left, surf_right]
    nsurfs = len(surfs)
    hemisigns = [1, -1]
    if not rotate_axis is None:
        theta = [0] * 3

        for i in xrange(nsurfs):
            theta[rotate_axis] = rotate_angle * hemisigns[i] * rotatesign
            surfs[i] = surfs[i].rotate(theta, unit='deg')


    for i in xrange(nsurfs):
        hemisign = hemisigns[i]
        sign = rotatesign * hemisign
        coords = surfs[i].vertices

        xtreme = np.min(coords[:, 0] * -hemisign)

        # sometimes the surfaces are not properly aligned along x and y
        # so fix it by moving by center of mass values along x and y

        delta = -np.reshape(surfs[i].center_of_mass, (1, 3))
        delta[0, 0] = hemisign * (xtreme - min_distance * .5)
        surfs[i] = surfs[i] + delta # make an implicit copy

    return tuple(surfs)




def get_sphere_left_right_mapping(surf_left, surf_right, eps=.001):
    '''finds the mapping from left to right hemisphere and vice versa
    (the mapping is symmetric)

    this only works on sphere.reg.asc files made with AFNI/SUMA's mapicosehedron'''

    if not surf_left.same_topology(surf_right):
        raise ValueError('topology mismatch')

    nv = surf_left.nvertices

    # to swap right surface along x-axis (i.e. mirror along x=0 plane)
    swapLR = np.array([[-1, 1, 1]])

    vL, vR = surf_left.vertices, surf_right.vertices * swapLR
    nL, nR = surf_left.neighbors, surf_right.neighbors


    # flip along x-axis
    #vR = vR * np.asarray([[-1., 1., 1.]])

    def find_nearest(src_coords, trgs_coords, eps=eps):
        # finds the index of the nearest node in trgs_coords to src_coords.
        # if the distance is more than eps, an exception is thrown
        d2 = np.sum((src_coords - trgs_coords) ** 2, 1)
        nearest = np.argmin(d2)
        if d2[nearest] > eps ** 2:
            raise ValueError('eps too big: %r > %r' % (d2[nearest] ** .5, eps))
        return nearest

    # get a (random) starting point
    pivotL = 0
    pivotR = find_nearest(vL[pivotL, :], vR)

    # mapping from left to right
    l2r = {pivotL:pivotR}
    to_visit = nL[pivotL].keys()

    # for each node (in the left hemispehre) still to visit, keep track of its
    # 'parent'
    to_visit2source = dict(zip(to_visit, [pivotL] * len(to_visit)))

    # invariants:
    # 1) if to_visit2source[v]==s, then s in l2r.keys()
    # 2) if to_visit2source[v]==s, then v in nL[s].keys()

    while to_visit2source:
        # find the corresponding node in right hemi for pivotL,
        # using sourceL as a neighbor which a corresponding node
        # on the other hemisphere is already known
        pivotL, sourceL = to_visit2source.popitem()

        # get the corresponding node of sourceL on the other hemisphere
        sourceR = l2r[sourceL]

        # of all the neighbors of sourceR, one of them should be
        # corresponding to pivotL
        nbr_surf_right = nR[sourceR].keys()
        nearestR = nbr_surf_right[find_nearest(vL[pivotL, :],
                                               vR[nbr_surf_right, :])]

        # store result
        l2r[pivotL] = nearestR

        # add new neighbors to to_visit2source; but not those that
        # have already a corresponding node on the other hemisphere
        for nbrL in nL[pivotL].keys():
            if not nbrL in l2r:
                to_visit2source[nbrL] = pivotL

    # store values in an array - this is easier for indexing
    l2r_arr = np.zeros((nv,), dtype=np.int32)
    for p, q in l2r.iteritems():
        l2r_arr[p] = q

    v_range = np.arange(nv)

    # final check: make sure it's a bijection
    if not np.all(l2r_arr[l2r_arr] == v_range):
        raise ValueError('Not found a bijection - this should not happen')

    return l2r_arr




def normalized(v):
    '''Normalizes vectors

    Parameters
    ==========
    v: np.ndarray
        PxQ matrix for P vectors that are all Q-dimensional

    Returns
    =======
    n: np.ndarray
        P-valued vector with the norm for each vector
    '''

    v_norm = np.sqrt(np.sum(v ** 2, 1))
    return v / np.reshape(v_norm, (-1, 1))



def merge(*surfs):
    if not surfs:
        return None
    s0 = surfs[0]
    return s0.merge(*surfs[1:])

def generate_cube():
    '''
    Generates a cube with sides 2 centered at the origin.

    Returns
    -------
    cube: surf.Surface
        A cube with 8 vertices at coordinates (+/-1.,+/-1.,+/-1.),
        and with 12 faces (two for each square side).
    '''

    # map (0,1) to (-1.,1.)
    f = lambda x:float(x) * 2 - 1

    # compute coordinates
    cs = [[f(i / (2 ** j) % 2) for j in xrange(3)] for i in xrange(8)]
    vs = np.asarray(cs)

    # manually set topology
    trias = [(0, 1, 3), (0, 3, 2), (1, 0, 5), (5, 0, 4),
           (3, 1, 5), (3, 5, 7), (3, 7, 6), (3, 6, 2),
           (2, 6, 0), (0, 6, 4), (5, 4, 6), (5, 6, 7)]


    fs = np.asarray(trias, dtype=np.int)

    return Surface(vs, fs)


def generate_sphere(density=10):
    '''
    Generates a sphere-like surface with unit radius centered at the origin.

    Parameters
    ----------
    d: int (default: 10)
        Level of detail

    Returns
    -------
    sphere: surf.Surface
        A sphere with d**2+2 vertices and 2*d**2 faces. Seen as the planet
        Earth, node 0 and 1 correspond to the north and south poles.
        The remaining d**2 vertices are in d circles of latitute, each with
        d vertices in them.
    '''

    hsteps = density # 'horizontal' steps (in each circle of latitude)
    vsteps = density # 'vertical' steps (number of circles of latitude,
                     #                   excluding north and south poles)

    vs = [(0., 0., 1.), (0., 0., -1)] # top and bottom nodes
    fs = []

    # z values for each ring (excluding top and bottom), equally spaced
    zs = [-1. + 2 * (1. / (vsteps + 1)) * (i + 1) for i in xrange(vsteps)]

    # angles for x and y
    alphastep = 2. * math.pi / hsteps
    alphas = [float(i) * alphastep for i in xrange(hsteps)]

    # generate coordinates, one ring at a time
    for vi in xrange(vsteps):
        z = math.sin(zs[vi] * math.pi * .5) # z coordinate
        scz = (1 - z * z) ** .5 # scaling for z

        alphaplus = vi * alphastep * .5 # each next ring is offset by half
                                        # of the length of a triangle

        # x and y coordinates
        xs = map(lambda x:scz * math.cos(x + alphaplus), alphas)
        ys = map(lambda x:scz * math.sin(x + alphaplus), alphas)

        vs.extend((xs[i], ys[i], z) for i in xrange(hsteps))

    # set topology, one ring at a time
    top = [1] * hsteps
    cur = [2 + i for i in xrange(hsteps)]

    for vi in xrange(vsteps):
        bot = ([0] * hsteps if vi == vsteps - 1
                            else map(lambda x:x + hsteps, cur))

        for hi in xrange(hsteps):
            left = cur[hi]
            right = cur[(hi + 1) % hsteps]

            fs.append((left, right, top[hi]))
            fs.append((right, left, bot[hi]))

        top, cur = cur, [bot[-1]] + bot[:-1]

    return Surface(np.array(vs), np.array(fs))

def generate_plane(x00, x01, x10, n01, n10):
    '''
    Generates a plane.

    Parameters
    ----------
    x00: np.array with 3 values
        origin
    x01: np.array with 3 values
        vector indicating first direction of plane
    x10: np.array with 3 values
        vector indicating second direction of plane
    n01: int
        number of points in first direction
    n10: int
        number of points in second direction

    Returns
    -------
    surf.Surface
        surface with n01*n10 nodes and (n01-1)*(n10-1)*2 faces.
        The (i,j)-th point is at coordinate x01+i*x01+j*x10 and
        is stored as the (i*n10+j)-th vertex.
    '''
    def as_three_vec(v):
        a = np.reshape(np.asarray(v, dtype=np.float), (-1,))
        if len(a) != 3:
            raise ValueError('expected three values for %r' % v)
        return a

    # ensure they are proper vectors
    x00, x01, x10 = map(as_three_vec, (x00, x01, x10))

    vs = np.zeros((n01 * n10, 3))
    fs = np.zeros((2 * (n01 - 1) * (n10 - 1), 3), dtype=np.int)
    for i in xrange(n01):
        for j in xrange(n10):
            vpos = i * n10 + j
            vs[vpos, :] = x00 + i * x01 + j * x10
            if i < n01 - 1 and j < n10 - 1: # not at upper borders
                # make a square pqrs from two triangles
                p = vpos
                q = vpos + 1
                r = vpos + n10
                s = r + 1

                fpos = (i * (n10 - 1) + j) * 2
                fs[fpos, :] = [p, q, r]
                fs[fpos + 1, :] = [s, r, q]

    return Surface(vs, fs)

def generate_bar(start, stop, radius, poly=10):
    '''Generates a bar-like surface

    Parameters
    ----------
    start: np.ndarray
        3-elemtent vector indicating top part of the bar
    stop: np.ndarray
        3-elemtent vector indicating bottom side of the bar
    radius: float
        radius of the bar
    poly: int
        the top and bottom part will be a regular polygon.

    Returns
    -------
    bar: surf.Surface
        A surface with poly*2+2 vertices and poly*4 faces

    Example
    -------
    generate_bar((0,0,0),(0,0,177.6),14.1,4)

    This generates a surface resembling the new One World Trade center, New York
    '''

    start = np.asarray(start)
    stop = np.asarray(stop)

    nv = poly * 2 + 2
    delta = start - stop
    delta_n = delta / np.sqrt(np.sum(delta ** 2))

    # get a normal vector
    # make sure that we don't use zero values
    i = np.argsort(np.abs(delta_n))
    vec_x = np.zeros(3)
    vec_x[i] = delta_n[i[[0, 2, 1]]] * np.asarray((0, -1, 1))
    vec_y = np.cross(delta_n, vec_x)

    coords = np.zeros((nv, 3))
    sc = 2 * np.pi / poly # angle scaling
    alpha = np.arange(poly) * sc # for top art
    beta = alpha + sc / 2

    # first and last node are top and bottom.
    # nodes in between are the edges at top and bottom
    coords[0, :] = start
    dtop = np.cos(alpha)[np.newaxis].T * vec_x[np.newaxis] + \
                        np.sin(alpha)[np.newaxis].T * vec_y[np.newaxis]
    dbot = np.cos(beta)[np.newaxis].T * vec_x[np.newaxis] + \
                        np.sin(beta)[np.newaxis].T * vec_y[np.newaxis]

    coords[1:-1:2, :] = dtop * radius + start
    coords[2::2, :] = dbot * radius + stop
    coords[-1, :] = stop

    # set up faces
    nf = poly * 4
    faces = np.zeros((nf, 3), dtype=np.int_)
    for i in xrange(poly):
        j = i * 2
        faces[j + 0, :] = (j + 1, j + 2, j + 3) # top part
        faces[j + 1, :] = (j + 2, j + 4, j + 3) # side with top
        faces[j + 2 * poly, :] = (j + 3, 0, j + 1) # side with bottom
        faces[j + 2 * poly + 1, :] = (j + 2, nv - 1, j + 4) # bottom part

    nrm = lambda x: (x - 1) % (2 * poly) + 1
    faces[:2 * poly, :] = nrm(faces[:2 * poly, :])
    faces[2 * poly:, 0] = nrm(faces[2 * poly:, 0])
    faces[2 * poly:, 2] = nrm(faces[2 * poly:, 2])

    s = Surface(coords, faces)

    return s


def read(fn):
    '''General read function for surfaces

    Parameters
    ----------
    fn: str
        Surface filename. The extension determines how the file is read as
        follows. '.asc', FreeSurfer ASCII format; '.coord'; Caret, '.gii',
        GIFTI; anything else: FreeSurfer geometry.

    Returns
    -------
    surf_: surf.Surface
        Surface object

    '''
    if fn.endswith('.asc'):
        from mvpa2.support.nibabel import surf_fs_asc
        return surf_fs_asc.read(fn)
    elif fn.endswith('.coord'):
        from mvpa2.support.nibabel import surf_caret
        return surf_caret.read(fn)
    elif fn.endswith('.gii'):
        # XXX require .surf.gii? Not for now - but may want to change
        from mvpa2.support.nibabel import surf_gifti
        return surf_gifti.read(fn)
    else:
        import nibabel.freesurfer.io as fsio
        coords, faces = fsio.read_geometry(fn)
        return Surface(coords, faces)

def write(fn, s, overwrite=True):
    '''General write function for surfaces

    Parameters
    ----------
    fn: str
        Surface filename. The extension determines how the file is written as
        follows. '.asc', FreeSurfer ASCII format; '.gii', GIFTI.
        Other formats are not supported.
    '''
    if fn.endswith('.asc'):
        from mvpa2.support.nibabel import surf_fs_asc
        surf_fs_asc.write(fn, s, overwrite=overwrite)
    elif fn.endswith('.gii'):
        if not fn.endswith('.surf.gii'):
            raise ValueError("GIFTI output requires extension .surf.gii")
        from mvpa2.support.nibabel import surf_gifti
        surf_gifti.write(fn, s, overwrite=overwrite)
    else:
        raise ValueError("Not implemented (based on extension): %r" % fn)

def from_any(s):
    if s is None or isinstance(s, Surface):
        return s
    elif isinstance(s, basestring):
        return read(s)
    elif type(s) is tuple and len(ts) == 2:
        return Surface(ts[0], ts[1])
    else:
        raise ValueError("Not understood: %r" % s)


########NEW FILE########
__FILENAME__ = surf_caret
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Caret binary file support 

@author: nick
'''

import numpy as np, os, datetime

from mvpa2.support.nibabel import surf

def _end_header_position(s):
    '''Finds where the caret header ends'''
    END_HEADER = 'EndHeader\n'
    end_header_pos = s.find(END_HEADER)
    return end_header_pos + len(END_HEADER) if end_header_pos > 0 else None


def read_topology(fn):
    '''Reads Caret .topo file
    
    Parameters
    ----------
    fn: str
        filename
    
    Returns
    -------
    faces: np.ndarray
        Px3 array for P faces
    '''

    with open(fn) as f:
        s = f.read()

    end_header_pos = _end_header_position(s)
    body = s[end_header_pos:]

    if body.startswith('tag-version'):
        body = body[body.find('\n') + 1:]

    nfaces = np.fromstring(body[:4], dtype='>i4')
    faces = np.fromstring(body[4:], dtype='>i4').reshape((-1, 3))

    return faces


def read(fn, topology_fn=None):
    '''Reads Caret .coord file and also (if it exists) the topology file
    
    Parameters
    ----------
    fn: str
        filename of .coord file
    topology_fn: str or None
        filename of .topo file. If None then it is attempted to deduce
        this filename from the header in fn.
    
    Returns
    -------
    s: surf.Surface
        Surface with the nodes as in fn, and the topology form topology_fn
    '''
    with open(fn) as f:
        s = f.read()

    end_header_pos = _end_header_position(s)

    header = s[:end_header_pos]
    body = s[end_header_pos:]

    # read body
    vertices = np.fromstring(body[4:], dtype='>f4').reshape((-1, 3))

    # see if we can find the topology
    faces = None
    if topology_fn is None:
        lines = header.split('\n')

        for line in lines:
            if line.startswith('topo_file'):
                topo_file = line[10:]

                topo_fn = os.path.join(os.path.split(fn)[0], topo_file)
                if os.path.exists(topo_fn):
                    faces = read_topology(topo_fn)
                    break
    else:
        faces = read_topology(topology_fn)

    if faces is None:
        # XXX print a warning?
        # For now no warning as the Surface.__init__ should print something
        faces = np.zeros((0, 3), dtype=np.int32)

    return surf.Surface(vertices, faces)

########NEW FILE########
__FILENAME__ = surf_fs_asc
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
Simple FreeSurfer ASCII surface file I/O functions

Reads and writes surface anatomy files as understood by AFNI SUMA (and maybe other programs)
The format for a surface with NV vertices and NF faces is:

NV NF
x_0 y_0 z_0 0
x_1 y_1 z_1 0
...
x_[NV-1] y_[NV-1] z_[NV-1] 0
f_00 f01 f02 0
f_10 f11 f12 0
...
f_[NF-1]0 f_[NF-1]1 f_[NF-1]2 0

where the (x,y,z) triples are coordinates and fi(p,q,r) are faces so that vertices with
indices p, q and r form a single triangle

Created on Feb 12, 2012

@author: nick
'''

import numpy as np, os, datetime

from mvpa2.support.nibabel import surf

def read(fn):
    '''
    Reads a AFNI SUMA ASCII surface

    Parameters
    ----------
    fn : str
        Filename of ASCII surface file

    Returns
    -------
    s : Surface
        a surf.Surface as defined in 'fn'
    '''

    if not os.path.exists(fn):
        raise Exception("File not found: %s" % fn)

    with open(fn) as f:
        r = f.read().split("\n")

    row = 0
    nv = nf = None # number of vertices and faces
    while True:
        line = r[row]
        row += 1

        if line.startswith("#"):
            continue

        try:
            nvnf = line.split(" ")
            nv = int(nvnf[0])
            nf = int(nvnf[1])
            break

        except:
            continue

    if not nf:
        raise Exception("Not found in %s: number of nodes and faces" % fn)

    # helper function to get a numpy Cx3 ndarray
    def getrows(c, s): # c: number of rows, s is string with data
        vs = np.fromstring(s, count=4 * c, sep=" ")
        vx = np.reshape(vs, (c, 4))
        return vx[:, :3]

    # coordinates should start at pos...
    v = getrows(nv, "\n".join(r[row:(row + nv)]))

    # and the faces just after those
    ffloat = getrows(nf, "\n".join(r[(row + nv):(row + nv + nf)]))
    f = ffloat.astype(int)

    return surf.Surface(v=v, f=f)

def write(fn, surface, overwrite=False, comment=None):
    '''
    Writes a AFNI SUMA ASCII surface

    Parameters
    ----------
    surface: surface.Surface
        surface to be written
    fn : str
        Output filename of ASCII surface file
    overwrite : bool
        Whether to overwrite 'fn' if it exists
    comment : str
        Comments to add to 'fn'
    '''

    if isinstance(surface, str) and isinstance(fn, surf.Surface):
        surface, fn = fn, surface

    if not overwrite and os.path.exists(fn):
        raise Exception("File already exists: %s" % fn)

    s = []
    if comment == None:
        comment = '# Created %s' % str(datetime.datetime.now())
    s.append(comment)

    nv, nf = surface.nvertices, surface.nfaces,
    v, f = surface.vertices, surface.faces

    # number of vertices and faces
    s.append('%d %d' % (nv, nf))

    # add vertices and faces
    s.extend('%f %f %f 0' % (v[i, 0], v[i, 1], v[i, 2]) for i in xrange(nv))
    s.extend('%d %d %d 0' % (f[i, 0], f[i, 1], f[i, 2]) for i in xrange(nf))

    # write to file
    f = open(fn, 'w')
    f.write("\n".join(s))
    f.close()


########NEW FILE########
__FILENAME__ = surf_gifti
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''
GIFTI surface functions (wrapper) using nibabel.gifti
'''
from mvpa2.base import externals

if externals.exists("nibabel", raise_=True):
    from nibabel.gifti import gifti, giftiio

import numpy as np, os, datetime, re

from mvpa2.support.nibabel import surf
import io


def _get_single_array(g, intent):
        ar = g.getArraysFromIntent(intent)
        n = len(ar)
        if n != 1:
            len_str = 'no' if n == 0 else '%d' % n
            raise ValueError('Found %s arrays matching %s, expected 1' %
                                (len_str, intent))
        return ar[0]

def read(fn):
    '''Reads a GIFTI surface file

    Parameters
    ----------
    fn: str
        Filename

    Returns
    -------
    surf_: surf.Surface
        Surface

    Notes
    -----
    Any meta-information stored in the GIFTI file is not present in surf_.
    '''

    g = giftiio.read(fn)

    vertices = _get_single_array(g, 'NIFTI_INTENT_POINTSET').data
    faces = _get_single_array(g, 'NIFTI_INTENT_TRIANGLE').data

    return surf.Surface(vertices, faces)


def filename2vertices_faces_metadata(fn):
    '''Attempts to get meta data based on the filename

    Parameters
    ----------
    fn: str
        Filename

    Returns
    -------
    meta: tuple
        Tuple with two gifti.GiftiMetaData objects for vertices
        and faces. If the filename contains exactly one of 'lh', 'rh', or
        'mh' then it is assumed to be of left, right or merged hemispheres.
        If the filename contains exactly one of 'pial,'smoothwm',
        'intermediate',''inflated','sphere','flat', then the geometric
        type is set
    '''
    _, fn = os.path.split(fn)

    vertex_map = dict(AnatomicalStructurePrimary=dict(
                                lh='CortexLeft',
                                rh='CortexRight',
                                mh='CortexRightLeft'),
                      AnatomicalStructureSecondary=dict(
                                pial='Pial',
                                smoothwm='GrayWhite',
                                intermediate='MidThickness'),
                      GeometricType=dict(
                                pial='Anatomical',
                                smoothwm='Anatomical',
                                intermediate='Anatomical',
                                inflated='Inflated',
                                sphere='Spherical',
                                flat='Flat'))

    def just_one(dict_, fn=fn):
        vs = [v for k, v in dict_.iteritems() if k in fn]
        return vs[0] if len(vs) == 1 else None

    v_meta = [gifti.GiftiNVPairs('Name', fn)]

    for key, dict_ in vertex_map.iteritems():
        v = just_one(dict_)
        if not v is None:
            v_meta.append(gifti.GiftiNVPairs(key, v))


    f_meta = [gifti.GiftiNVPairs('Name', fn)]
    # XXX maybe also closed or open topology? that's a bit tricky though

    v = gifti.GiftiMetaData()
    v.data.extend(v_meta)

    f = gifti.GiftiMetaData()
    f.data.extend(f_meta)

    return v, f

def to_gifti_image(s, add_indices=False, swap_LPI_RAI=False):
    '''
    Converts a surface to nibabel's gifti format.

    Parameters
    ----------
    s: surf
        Input surface
    add_indices: True or False (default: False)
        if True then indices of the nodes are added.
        Note: caret may not be able to read these
    swap_LPI_RAI: True or False (default: False)
        If True then the diagonal elements of the xform matrix
        are set to [-1,-1,1,1], otherwise to [1,1,1,1].


    Returns
    -------
    img: gifti.GiftiImage
        Surface representated as GiftiImage
    '''

    vertices = gifti.GiftiDataArray(np.asarray(s.vertices, np.float32))
    vertices.intent = gifti.intent_codes.field1['pointset']
    vertices.datatype = 16 # this is what gifti likes

    if add_indices:
        nvertices = s.nvertices
        indices = gifti.GiftiDataArray(np.asarray(np.arange(nvertices), np.int32))
        indices.datatype = 8 # this is what gifti likes
        indices.coordsys = None # otherwise SUMA might complain
        indices.intent = gifti.intent_codes.field1['node index']


    faces = gifti.GiftiDataArray(np.asarray(s.faces, np.int32))
    faces.intent = gifti.intent_codes.field1['triangle']
    faces.datatype = 8 # this is what gifti likes
    faces.coordsys = None # otherwise SUMA might complain

    # set some fields common to faces and vertices
    for arr in (vertices, faces) + ((indices,) if add_indices else ()):
        arr.ind_ord = 1
        arr.encoding = 3
        arr.endian = 'LittleEndian' # XXX this does not work (see below)
        arr.dims = list(arr.data.shape)
        arr.num_dim = len(arr.dims)

    # make the image
    meta = gifti.GiftiMetaData()
    labeltable = gifti.GiftiLabelTable()

    img = gifti.GiftiImage(meta=meta, labeltable=labeltable)

    if swap_LPI_RAI:
        xform = np.asarray(vertices.coordsys.xform)
        xform[0, 0] = -1
        xform[1, 1] = -1
        vertices.coordsys.xform = xform

    if add_indices:
        img.add_gifti_data_array(indices)
    img.add_gifti_data_array(vertices)
    img.add_gifti_data_array(faces)

    return img


def to_xml(img, meta_fn_hint=None):
    '''Converts to XML

    Parameters
    ----------
    img: gifti.GiftiImage or surf
        Input surface
    meta_fn_hint: str or None
        If not None, it should be a string (possibly a filename that
        describes what kind of surface this is.
        See filename2vertices_faces_metadata.

    Returns
    -------
    xml: bytearray
        Representation of input surface in XML format
    '''

    if isinstance(img, surf.Surface):
        img = to_gifti_image(img)

    if not meta_fn_hint is None:
        vertices = _get_single_array(img, 'pointset')
        faces = _get_single_array(img, 'triangle')
        vertices.meta, faces.meta = \
                        filename2vertices_faces_metadata(meta_fn_hint)

    # XXX FIXME from here on it's a bit of a hack
    # The to_xml() method adds newlines in <DATA>...</DATA> segments
    # and also writes GIFTI_ENDIAN_LITTLE instead of LittleEndian.
    # For now we just replace these offending parts
    # TODO: report issue to nibabel developers

    xml = img.to_xml().encode('utf-8')

    # split by data segements. Odd elements are data, even are surroudning
    sps = re.split(b'([<]Data[>][^<]*?[<][/]Data[>])', xml, re.DOTALL)

    # fix the LittleEndian issue for even segments and newline for odd ones
    fix_odd_even = lambda x, i: x.replace(b'\n', b'') \
                                if i % 2 == 1 \
                                else x.replace(b'Endian="GIFTI_ENDIAN_LITTLE"',
                                               b'Endian="LittleEndian"')

    xml_fixed = b''.join(fix_odd_even(sp, i) for i, sp in enumerate(sps))

    return xml_fixed

def write(fn, s, overwrite=True):
    '''Writes a GIFTI surface file

    Parameters
    ----------
    fn: str
        Filename
    s: surf.Surface
        Surface
    overwrite: bool (default: False)
        If set to False an error is raised if the file exists
    '''

    if not overwrite and os.path.exists(fn):
        raise ValueError("Already exists: %s" % fn)

    EXT = '.surf.gii'
    if not fn.endswith(EXT):
        raise ValueError("Filename %s does not end with required"
                         " extension %s" % (fn, EXT))


    xml = to_xml(s, fn)

    with io.FileIO(fn, 'wb') as f:
        n = f.write(xml)
    if n != len(xml):
        raise ValueError("Not all bytes written to %s" % fn)

########NEW FILE########
__FILENAME__ = pylab
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Import helper for pylab which would enforce setting of a backend
"""

__docformat__ = 'restructuredtext'

import sys
from mvpa2.base import externals

__all__ = ['pl']

if externals.exists('pylab', raise_=True):
    # Assure that we have correct backend
    externals._set_matplotlib_backend()

    if sys.version_info[:2] >= (2, 5):
        # enforce absolute import
        pl = __import__('pylab', globals(),
                              locals(), [], 0)
    else:
        # little trick to be able to import 'griddata' package (which
        # has same name)
        oldname = __name__
        # crazy name with close to zero possibility to cause whatever
        __name__ = 'iaugf9zrkjsasdf1'
        try:
            import pylab as pl
            # restore old settings
            __name__ = oldname
        except ImportError:
            # restore old settings
            __name__ = oldname
            raise

########NEW FILE########
__FILENAME__ = rpy2_addons
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Misc support utility for making us compatible with liquid RPy2 atm
"""

__docformat__ = 'restructuredtext'

from mvpa2.base.externals import exists, versions

#if __debug__:
#    from mvpa2.base import debug

__all__ = []

if exists('rpy2', raise_=True):
    __all__ = [ 'Rrx', 'Rrx2' ]

    if versions['rpy2'] >= '2.1.0beta':
        def Rrx(self, x):
            return self.rx(x)
        def Rrx2(self, x):
            return self.rx2(x)
    elif versions['rpy2'] >= '2.0':
        def Rrx(self, x):
            return self.r[x]
        def Rrx2(self, x):
            return self.r[x][0]
    else:
        raise ValueError, \
              "We do not have support for rpy2 version %(rpy2)s" % versions

    Rrx.__doc__ = "Access delegator for R function ["
    Rrx2.__doc__ = "Access delegator for R function [["

########NEW FILE########
__FILENAME__ = signal
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Some functionality which was fixed in later versions of SciPy -- signal processing"""

__docformat__ = 'restructuredtext'

import numpy as np

from mvpa2.base import externals
if externals.exists('scipy', raise_=True):
    if externals.versions['scipy'] >= '0.11':
        from scipy.signal import filtfilt
    else:
        if externals.versions['scipy'] >= '0.10':
            from scipy.signal._arraytools import axis_reverse, axis_slice, odd_ext, const_ext
        else:
            from ._arraytools import axis_reverse, axis_slice, odd_ext, const_ext

        from scipy.signal import lfilter, lfilter_zi

        # Taken from scipy 0.11, all version before are broken see
        # https://bugs.debian.org/736185
        def filtfilt(b, a, x, axis=-1, padtype='odd', padlen=None):
            """
            A forward-backward filter.

            This function applies a linear filter twice, once forward
            and once backwards.  The combined filter has linear phase.

            Before applying the filter, the function can pad the data along the
            given axis in one of three ways: odd, even or constant.  The odd
            and even extensions have the corresponding symmetry about the end point
            of the data.  The constant extension extends the data with the values
            at end points.  On both the forward and backwards passes, the
            initial condition of the filter is found by using `lfilter_zi` and
            scaling it by the end point of the extended data.

            Parameters
            ----------
            b : (N,) array_like
                The numerator coefficient vector of the filter.
            a : (N,) array_like
                The denominator coefficient vector of the filter.  If a[0]
                is not 1, then both a and b are normalized by a[0].
            x : array_like
                The array of data to be filtered.
            axis : int, optional
                The axis of `x` to which the filter is applied.
                Default is -1.
            padtype : str or None, optional
                Must be 'odd', 'even', 'constant', or None.  This determines the
                type of extension to use for the padded signal to which the filter
                is applied.  If `padtype` is None, no padding is used.  The default
                is 'odd'.
            padlen : int or None, optional
                The number of elements by which to extend `x` at both ends of
                `axis` before applying the filter. This value must be less than
                `x.shape[axis]-1`.  `padlen=0` implies no padding.
                The default value is 3*max(len(a),len(b)).

            Returns
            -------
            y : ndarray
                The filtered output, an array of type numpy.float64 with the same
                shape as `x`.

            See Also
            --------
            lfilter_zi, lfilter

            Examples
            --------
            First we create a one second signal that is the sum of two pure sine
            waves, with frequencies 5 Hz and 250 Hz, sampled at 2000 Hz.

            >>> t = np.linspace(0, 1.0, 2001)
            >>> xlow = np.sin(2 * np.pi * 5 * t)
            >>> xhigh = np.sin(2 * np.pi * 250 * t)
            >>> x = xlow + xhigh

            Now create a lowpass Butterworth filter with a cutoff of 0.125 times
            the Nyquist rate, or 125 Hz, and apply it to x with filtfilt.  The
            result should be approximately xlow, with no phase shift.

            >>> from scipy import signal
            >>> b, a = signal.butter(8, 0.125)
            >>> y = filtfilt(b, a, x, padlen=150)
            >>> print('%.5g' % np.abs(y - xlow).max())
            9.1086e-06

            We get a fairly clean result for this artificial example because
            the odd extension is exact, and with the moderately long padding,
            the filter's transients have dissipated by the time the actual data
            is reached.  In general, transient effects at the edges are
            unavoidable.

            """

            if padtype not in ['even', 'odd', 'constant', None]:
                raise ValueError(("Unknown value '%s' given to padtype.  padtype must "
                                 "be 'even', 'odd', 'constant', or None.") %
                                    padtype)

            b = np.asarray(b)
            a = np.asarray(a)
            x = np.asarray(x)

            ntaps = max(len(a), len(b))

            if padtype is None:
                padlen = 0

            if padlen is None:
                # Original padding; preserved for backwards compatibility.
                edge = ntaps * 3
            else:
                edge = padlen

            # x's 'axis' dimension must be bigger than edge.
            if x.shape[axis] <= edge:
                raise ValueError("The length of the input vector x must be at least "
                                 "padlen, which is %d." % edge)

            if padtype is not None and edge > 0:
                # Make an extension of length `edge` at each
                # end of the input array.
                if padtype == 'even':
                    ext = even_ext(x, edge, axis=axis)
                elif padtype == 'odd':
                    ext = odd_ext(x, edge, axis=axis)
                else:
                    ext = const_ext(x, edge, axis=axis)
            else:
                ext = x

            # Get the steady state of the filter's step response.
            zi = lfilter_zi(b, a)

            # Reshape zi and create x0 so that zi*x0 broadcasts
            # to the correct value for the 'zi' keyword argument
            # to lfilter.
            zi_shape = [1] * x.ndim
            zi_shape[axis] = zi.size
            zi = np.reshape(zi, zi_shape)
            x0 = axis_slice(ext, stop=1, axis=axis)

            # Forward filter.
            (y, zf) = lfilter(b, a, ext, axis=axis, zi=zi * x0)

            # Backward filter.
            # Create y0 so zi*y0 broadcasts appropriately.
            y0 = axis_slice(y, start=-1, axis=axis)
            (y, zf) = lfilter(b, a, axis_reverse(y, axis=axis), axis=axis, zi=zi * y0)

            # Reverse y.
            y = axis_reverse(y, axis=axis)

            if edge > 0:
                # Slice the actual signal from the extended signal.
                y = axis_slice(y, start=edge, stop=-edge, axis=axis)

            return y

########NEW FILE########
__FILENAME__ = stats
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Fixer for rdist in scipy
"""
# For scipy import
from __future__ import absolute_import

__docformat__ = 'restructuredtext'

from mvpa2.base import externals, warning, cfg

if __debug__:
    from mvpa2.base import debug

if externals.exists('scipy', raise_=True):
    import scipy
    import scipy.stats
    import scipy.stats as stats

if not externals.exists('good scipy.stats.rdist'):
    if __debug__:
        debug("EXT", "Fixing up scipy.stats.rdist")
    # Lets fix it up, future imports of scipy.stats should carry fixed
    # version, isn't python is \emph{evil} ;-)
    import numpy as np

    from scipy.stats.distributions import rv_continuous
    from scipy import special
    import scipy.integrate

    # NB: Following function is copied from scipy SVN rev.5236
    #     and fixed with pow -> np.power (thanks Josef!)
    # FIXME: PPF does not work.
    class rdist_gen(rv_continuous):
        def _pdf(self, x, c):
            return np.power((1.0-x*x),c/2.0-1) / special.beta(0.5,c/2.0)
        def _cdf_skip(self, x, c):
            #error inspecial.hyp2f1 for some values see tickets 758, 759
            return 0.5 + x/special.beta(0.5,c/2.0)* \
                   special.hyp2f1(0.5,1.0-c/2.0,1.5,x*x)
        def _munp(self, n, c):
            return (1-(n % 2))*special.beta((n+1.0)/2,c/2.0)

    # Lets try to avoid at least some of the numerical problems by removing points
    # around edges
    rdist = rdist_gen(a=-1.0, b=1.0, name="rdist", longname="An R-distributed",
                      shapes="c", extradoc="""

    R-distribution

    rdist.pdf(x,c) = (1-x**2)**(c/2-1) / B(1/2, c/2)
    for -1 <= x <= 1, c > 0.
    """
                      )
    # Fix up number of arguments for veccdf's vectorize
    if rdist.veccdf.nin == 1:
        if __debug__:
            debug("EXT", "Fixing up veccdf.nin to make 2 for rdist")
        rdist.veccdf.nin = 2

    scipy.stats.distributions.rdist_gen = scipy.stats.rdist_gen = rdist_gen
    scipy.stats.distributions.rdist = scipy.stats.rdist = rdist

    try: # Retest
        externals.exists('good scipy.stats.rdist', force=True,
                         raise_=True)
    except RuntimeError:
        warning("scipy.stats.rdist was not fixed with a monkey-patch. "
                "It remains broken")
    # Revert so if configuration stored, we know the true flow of things ;)
    cfg.set('externals', 'have good scipy.stats.rdist', 'no')


if not externals.exists('good scipy.stats.rv_discrete.ppf'):
    # Local rebindings for ppf7 (7 is for the scipy version from
    # which code was borrowed)
    arr = np.asarray
    from scipy.stats.distributions import valarray, argsreduce
    from numpy import shape, place, any

    def ppf7(self,q,*args,**kwds):
        """
        Percent point function (inverse of cdf) at q of the given RV

        Parameters
        ----------
        q : array-like
            lower tail probability
        arg1, arg2, arg3,... : array-like
            The shape parameter(s) for the distribution (see docstring of the
            instance object for more information)
        loc : array-like, optional
            location parameter (default=0)

        Returns
        -------
        k : array-like
            quantile corresponding to the lower tail probability, q.

        """
        loc = kwds.get('loc')
        args, loc = self._rv_discrete__fix_loc(args, loc)
        q,loc  = map(arr,(q,loc))
        args = tuple(map(arr,args))
        cond0 = self._argcheck(*args) & (loc == loc)
        cond1 = (q > 0) & (q < 1)
        cond2 = (q==1) & cond0
        cond = cond0 & cond1
        output = valarray(shape(cond),value=self.badvalue,typecode='d')
        #output type 'd' to handle nin and inf
        place(output,(q==0)*(cond==cond), self.a-1)
        place(output,cond2,self.b)
        if any(cond):
            goodargs = argsreduce(cond, *((q,)+args+(loc,)))
            loc, goodargs = goodargs[-1], goodargs[:-1]
            place(output,cond,self._ppf(*goodargs) + loc)

        if output.ndim == 0:
            return output[()]
        return output

    scipy.stats.distributions.rv_discrete.ppf = ppf7
    try:
        externals.exists('good scipy.stats.rv_discrete.ppf', force=True,
                         raise_=True)
    except RuntimeError:
        warning("rv_discrete.ppf was not fixed with a monkey-patch. "
                "It remains broken")
    cfg.set('externals', 'have good scipy.stats.rv_discrete.ppf', 'no')

if externals.versions['scipy'] >= '0.8.0' and \
       not externals.exists('good scipy.stats.rv_continuous._reduce_func(floc,fscale)'):
    if __debug__:
        debug("EXT", "Fixing up scipy.stats.rv_continuous._reduce_func")

    # Borrowed from scipy v0.4.3-5978-gce90df2
    # Copyright: 2001, 2002 Enthought, Inc.; 2003-2012 SciPy developers
    # License: BSD-3
    def _reduce_func_fixed(self, args, kwds):
        args = list(args)
        Nargs = len(args)
        fixedn = []
        index = range(Nargs)
        names = ['f%d' % n for n in range(Nargs - 2)] + ['floc', 'fscale']
        x0 = []
        for n, key in zip(index, names):
            if kwds.has_key(key):
                fixedn.append(n)
                args[n] = kwds[key]
            else:
                x0.append(args[n])

        if len(fixedn) == 0:
            func = self.nnlf
            restore = None
        else:
            if len(fixedn) == len(index):
                raise ValueError("All parameters fixed. There is nothing to optimize.")
            def restore(args, theta):
                # Replace with theta for all numbers not in fixedn
                # This allows the non-fixed values to vary, but
                #  we still call self.nnlf with all parameters.
                i = 0
                for n in range(Nargs):
                    if n not in fixedn:
                        args[n] = theta[i]
                        i += 1
                return args

            def func(theta, x):
                newtheta = restore(args[:], theta)
                return self.nnlf(newtheta, x)

        return x0, func, restore, args

    stats.rv_continuous._reduce_func = _reduce_func_fixed

########NEW FILE########
__FILENAME__ = _arraytools
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See __init__.py for copyright/license information
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Borrowed code from scipy.signal._arraytools (from v0.4.3-9297-gc08dd62)

Functions for acting on a axis of an array.
"""
from __future__ import division, print_function, absolute_import

import numpy as np


def axis_slice(a, start=None, stop=None, step=None, axis=-1):
    """Take a slice along axis 'axis' from 'a'.

    Parameters
    ----------
    a : numpy.ndarray
        The array to be sliced.
    start, stop, step : int or None
        The slice parameters.
    axis : int
        The axis of `a` to be sliced.

    Examples
    --------
    >>> a = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    >>> axis_slice(a, start=0, stop=1, axis=1)
    array([[1],
           [4],
           [7]])
    >>> axis_slice(a, start=1, axis=0)
    array([[4, 5, 6],
           [7, 8, 9]])

    Notes
    -----
    The keyword arguments start, stop and step are used by calling
    slice(start, stop, step).  This implies axis_slice() does not
    handle its arguments the exacty the same as indexing.  To select
    a single index k, for example, use
        axis_slice(a, start=k, stop=k+1)
    In this case, the length of the axis 'axis' in the result will
    be 1; the trivial dimension is not removed. (Use numpy.squeeze()
    to remove trivial axes.)
    """
    a_slice = [slice(None)] * a.ndim
    a_slice[axis] = slice(start, stop, step)
    b = a[a_slice]
    return b


def axis_reverse(a, axis=-1):
    """Reverse the 1-d slices of `a` along axis `axis`.

    Returns axis_slice(a, step=-1, axis=axis).
    """
    return axis_slice(a, step=-1, axis=axis)


def odd_ext(x, n, axis=-1):
    """Generate a new ndarray by making an odd extension of x along an axis.

    Parameters
    ----------
    x : ndarray
        The array to be extended.
    n : int
        The number of elements by which to extend x at each end of the axis.
    axis : int
        The axis along which to extend x.  Default is -1.

    Examples
    --------
    >>> a = array([[1.0,2.0,3.0,4.0,5.0], [0.0, 1.0, 4.0, 9.0, 16.0]])
    >>> _odd_ext(a, 2)
    array([[-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
           [-4., -1,   0.,  1.,  4.,  9., 16., 23., 28.]])
    """
    if n < 1:
        return x
    if n > x.shape[axis] - 1:
        raise ValueError(("The extension length n (%d) is too big. " +
                         "It must not exceed x.shape[axis]-1, which is %d.")
                         % (n, x.shape[axis] - 1))
    left_end = axis_slice(x, start=0, stop=1, axis=axis)
    left_ext = axis_slice(x, start=n, stop=0, step=-1, axis=axis)
    right_end = axis_slice(x, start=-1, axis=axis)
    right_ext = axis_slice(x, start=-2, stop=-(n + 2), step=-1, axis=axis)
    ext = np.concatenate((2 * left_end - left_ext,
                          x,
                          2 * right_end - right_ext),
                         axis=axis)
    return ext


def even_ext(x, n, axis=-1):
    """Create an ndarray that is an even extension of x along an axis.

    Parameters
    ----------
    x : ndarray
        The array to be extended.
    n : int
        The number of elements by which to extend x at each end of the axis.
    axis : int
        The axis along which to extend x.  Default is -1.

    Examples
    --------
    >>> a = array([[1.0,2.0,3.0,4.0,5.0], [0.0, 1.0, 4.0, 9.0, 16.0]])
    >>> _even_ext(a, 2)
    array([[  3.,   2.,   1.,   2.,   3.,   4.,   5.,   4.,   3.],
           [  4.,   1.,   0.,   1.,   4.,   9.,  16.,   9.,   4.]])
    """
    if n < 1:
        return x
    if n > x.shape[axis] - 1:
        raise ValueError(("The extension length n (%d) is too big. " +
                         "It must not exceed x.shape[axis]-1, which is %d.")
                         % (n, x.shape[axis] - 1))
    left_ext = axis_slice(x, start=n, stop=0, step=-1, axis=axis)
    right_ext = axis_slice(x, start=-2, stop=-(n + 2), step=-1, axis=axis)
    ext = np.concatenate((left_ext,
                          x,
                          right_ext),
                         axis=axis)
    return ext


def const_ext(x, n, axis=-1):
    """Create an ndarray that is a constant extension of x along an axis.

    The extension repeats the values at the first and last element of
    the axis.

    Parameters
    ----------
    x : ndarray
        The array to be extended.
    n : int
        The number of elements by which to extend x at each end of the axis.
    axis : int
        The axis along which to extend x.  Default is -1.

    Examples
    --------
    >>> a = array([[1.0,2.0,3.0,4.0,5.0], [0.0, 1.0, 4.0, 9.0, 16.0]])
    >>> _const_ext(a, 2)
    array([[  1.,   1.,   1.,   2.,   3.,   4.,   5.,   5.,   5.],
           [  0.,   0.,   0.,   1.,   4.,   9.,  16.,  16.,  16.]])
    """
    if n < 1:
        return x
    left_end = axis_slice(x, start=0, stop=1, axis=axis)
    ones_shape = [1] * x.ndim
    ones_shape[axis] = n
    ones = np.ones(ones_shape, dtype=x.dtype)
    left_ext = ones * left_end
    right_end = axis_slice(x, start=-1, axis=axis)
    right_ext = ones * right_end
    ext = np.concatenate((left_ext,
                          x,
                          right_ext),
                         axis=axis)
    return ext

########NEW FILE########
__FILENAME__ = utils
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Various utils of general utility
"""

__docformat__ = 'restructuredtext'

import warnings

from mvpa2.base import externals, warning, cfg

if __debug__:
    from mvpa2.base import debug


class deprecated(object):
    """Decorator to mark a function or class as deprecated.

    Issue a warning when the function is called/the class is instantiated and
    adds a warning to the docstring.

    The optional extra argument will be appended to the deprecation message
    and the docstring. Note: to use this with the default value for extra, put
    in an empty of parentheses:

    >>> from sklearn.utils import deprecated
    >>> deprecated() # doctest: +ELLIPSIS
    <sklearn.utils.deprecated object at ...>

    >>> @deprecated()
    ... def some_function(): pass
    """

    # Taken from sklearn 0.10-branching-696-g19b5e33
    # Copyright: 2007-2012, sklearn developers
    # License:   BSD-3

    # Adapted from http://wiki.python.org/moin/PythonDecoratorLibrary,
    # but with many changes.

    def __init__(self, extra=''):
        """
        Parameters
        ----------
        extra: string
          to be added to the deprecation messages

        """
        self.extra = extra

    def __call__(self, obj):
        if isinstance(obj, type):
            return self._decorate_class(obj)
        else:
            return self._decorate_fun(obj)

    def _decorate_class(self, cls):
        msg = "Class %s is deprecated" % cls.__name__
        if self.extra:
            msg += "; %s" % self.extra

        # FIXME: we should probably reset __new__ for full generality
        init = cls.__init__

        def wrapped(*args, **kwargs):
            warnings.warn(msg, category=DeprecationWarning)
            return init(*args, **kwargs)
        cls.__init__ = wrapped

        wrapped.__name__ = '__init__'
        wrapped.__doc__ = self._update_doc(init.__doc__)
        wrapped.deprecated_original = init

        return cls

    def _decorate_fun(self, fun):
        """Decorate function fun"""

        msg = "Function %s is deprecated" % fun.__name__
        if self.extra:
            msg += "; %s" % self.extra

        def wrapped(*args, **kwargs):
            warnings.warn(msg, category=DeprecationWarning)
            return fun(*args, **kwargs)

        wrapped.__name__ = fun.__name__
        wrapped.__dict__ = fun.__dict__
        wrapped.__doc__ = self._update_doc(fun.__doc__)

        return wrapped

    def _update_doc(self, olddoc):
        newdoc = "DEPRECATED"
        if self.extra:
            newdoc = "%s: %s" % (newdoc, self.extra)
        if olddoc:
            newdoc = "%s\n\n%s" % (newdoc, olddoc)
        return newdoc

########NEW FILE########
__FILENAME__ = _copy
"""Generic (shallow and deep) copying operations.

Interface summary:

        import copy

        x = copy.copy(y)        # make a shallow copy of y
        x = copy.deepcopy(y)    # make a deep copy of y

For module specific errors, copy.Error is raised.

The difference between shallow and deep copying is only relevant for
compound objects (objects that contain other objects, like lists or
class instances).

- A shallow copy constructs a new compound object and then (to the
  extent possible) inserts *the same objects* into it that the
  original contains.

- A deep copy constructs a new compound object and then, recursively,
  inserts *copies* into it of the objects found in the original.

Two problems often exist with deep copy operations that don't exist
with shallow copy operations:

 a) recursive objects (compound objects that, directly or indirectly,
    contain a reference to themselves) may cause a recursive loop

 b) because deep copy copies *everything* it may copy too much, e.g.
    administrative data structures that should be shared even between
    copies

Python's deep copy operation avoids these problems by:

 a) keeping a table of objects already copied during the current
    copying pass

 b) letting user-defined classes override the copying operation or the
    set of components copied

This version does not copy types like module, class, function, method,
nor stack trace, stack frame, nor file, socket, window, nor array, nor
any similar types.

Classes can use the same interfaces to control copying that they use
to control pickling: they can define methods called __getinitargs__(),
__getstate__() and __setstate__().  See the documentation for module
"pickle" for information on these methods.
"""

import types
from copy_reg import dispatch_table

class Error(Exception):
    pass
error = Error   # backward compatibility

try:
    from org.python.core import PyStringMap
except ImportError:
    PyStringMap = None

__all__ = ["Error", "copy", "deepcopy"]

def copy(x):
    """Shallow copy operation on arbitrary Python objects.

    See the module's __doc__ string for more info.
    """

    cls = type(x)

    copier = _copy_dispatch.get(cls)
    if copier:
        return copier(x)

    copier = getattr(cls, "__copy__", None)
    if copier:
        return copier(x)

    reductor = dispatch_table.get(cls)
    if reductor:
        rv = reductor(x)
    else:
        reductor = getattr(x, "__reduce_ex__", None)
        if reductor:
            rv = reductor(2)
        else:
            reductor = getattr(x, "__reduce__", None)
            if reductor:
                rv = reductor()
            else:
                raise Error("un(shallow)copyable object of type %s" % cls)

    return _reconstruct(x, rv, 0)


_copy_dispatch = d = {}

def _copy_immutable(x):
    return x
for t in (type(None), int, long, float, bool, str, tuple,
          frozenset, type, xrange, types.ClassType,
          types.BuiltinFunctionType, type(Ellipsis),
          types.FunctionType):
    d[t] = _copy_immutable
for name in ("ComplexType", "UnicodeType", "CodeType"):
    t = getattr(types, name, None)
    if t is not None:
        d[t] = _copy_immutable

def _copy_with_constructor(x):
    return type(x)(x)
for t in (list, dict, set):
    d[t] = _copy_with_constructor

def _copy_with_copy_method(x):
    return x.copy()
if PyStringMap is not None:
    d[PyStringMap] = _copy_with_copy_method

def _copy_inst(x):
    if hasattr(x, '__copy__'):
        return x.__copy__()
    if hasattr(x, '__getinitargs__'):
        args = x.__getinitargs__()
        y = x.__class__(*args)
    else:
        y = _EmptyClass()
        y.__class__ = x.__class__
    if hasattr(x, '__getstate__'):
        state = x.__getstate__()
    else:
        state = x.__dict__
    if hasattr(y, '__setstate__'):
        y.__setstate__(state)
    else:
        y.__dict__.update(state)
    return y
d[types.InstanceType] = _copy_inst

del d

def deepcopy(x, memo=None, _nil=[]):
    """Deep copy operation on arbitrary Python objects.

    See the module's __doc__ string for more info.
    """

    if memo is None:
        memo = {}

    d = id(x)
    y = memo.get(d, _nil)
    if y is not _nil:
        return y

    cls = type(x)

    copier = _deepcopy_dispatch.get(cls)
    if copier:
        y = copier(x, memo)
    else:
        try:
            issc = issubclass(cls, type)
        except TypeError: # cls is not a class (old Boost; see SF #502085)
            issc = 0
        if issc:
            y = _deepcopy_atomic(x, memo)
        else:
            copier = getattr(x, "__deepcopy__", None)
            if copier:
                y = copier(memo)
            else:
                reductor = dispatch_table.get(cls)
                if reductor:
                    rv = reductor(x)
                else:
                    reductor = getattr(x, "__reduce_ex__", None)
                    if reductor:
                        rv = reductor(2)
                    else:
                        reductor = getattr(x, "__reduce__", None)
                        if reductor:
                            rv = reductor()
                        else:
                            raise Error(
                                "un(deep)copyable object of type %s" % cls)
                y = _reconstruct(x, rv, 1, memo)

    memo[d] = y
    _keep_alive(x, memo) # Make sure x lives at least as long as d
    return y

_deepcopy_dispatch = d = {}

def _deepcopy_atomic(x, memo):
    return x
d[type(None)] = _deepcopy_atomic
d[type(Ellipsis)] = _deepcopy_atomic
d[int] = _deepcopy_atomic
d[long] = _deepcopy_atomic
d[float] = _deepcopy_atomic
d[bool] = _deepcopy_atomic
try:
    d[complex] = _deepcopy_atomic
except NameError:
    pass
d[str] = _deepcopy_atomic
try:
    d[unicode] = _deepcopy_atomic
except NameError:
    pass
try:
    d[types.CodeType] = _deepcopy_atomic
except AttributeError:
    pass
d[type] = _deepcopy_atomic
d[xrange] = _deepcopy_atomic
d[types.ClassType] = _deepcopy_atomic
d[types.BuiltinFunctionType] = _deepcopy_atomic
d[types.FunctionType] = _deepcopy_atomic

def _deepcopy_list(x, memo):
    y = []
    memo[id(x)] = y
    for a in x:
        y.append(deepcopy(a, memo))
    return y
d[list] = _deepcopy_list

def _deepcopy_tuple(x, memo):
    y = []
    for a in x:
        y.append(deepcopy(a, memo))
    d = id(x)
    try:
        return memo[d]
    except KeyError:
        pass
    for i in range(len(x)):
        if x[i] is not y[i]:
            y = tuple(y)
            break
    else:
        y = x
    memo[d] = y
    return y
d[tuple] = _deepcopy_tuple

def _deepcopy_dict(x, memo):
    y = {}
    memo[id(x)] = y
    for key, value in x.iteritems():
        y[deepcopy(key, memo)] = deepcopy(value, memo)
    return y
d[dict] = _deepcopy_dict
if PyStringMap is not None:
    d[PyStringMap] = _deepcopy_dict

def _keep_alive(x, memo):
    """Keeps a reference to the object x in the memo.

    Because we remember objects by their id, we have
    to assure that possibly temporary objects are kept
    alive by referencing them.
    We store a reference at the id of the memo, which should
    normally not be used unless someone tries to deepcopy
    the memo itself...
    """
    try:
        memo[id(memo)].append(x)
    except KeyError:
        # aha, this is the first one :-)
        memo[id(memo)]=[x]

def _deepcopy_inst(x, memo):
    if hasattr(x, '__deepcopy__'):
        return x.__deepcopy__(memo)
    if hasattr(x, '__getinitargs__'):
        args = x.__getinitargs__()
        args = deepcopy(args, memo)
        y = x.__class__(*args)
    else:
        y = _EmptyClass()
        y.__class__ = x.__class__
    memo[id(x)] = y
    if hasattr(x, '__getstate__'):
        state = x.__getstate__()
    else:
        state = x.__dict__
    state = deepcopy(state, memo)
    if hasattr(y, '__setstate__'):
        y.__setstate__(state)
    else:
        y.__dict__.update(state)
    return y
d[types.InstanceType] = _deepcopy_inst

def _reconstruct(x, info, deep, memo=None):
    if isinstance(info, str):
        return x
    assert isinstance(info, tuple)
    if memo is None:
        memo = {}
    n = len(info)
    assert n in (2, 3, 4, 5)
    callable, args = info[:2]
    if n > 2:
        state = info[2]
    else:
        state = {}
    if n > 3:
        listiter = info[3]
    else:
        listiter = None
    if n > 4:
        dictiter = info[4]
    else:
        dictiter = None
    if deep:
        args = deepcopy(args, memo)
    y = callable(*args)
    memo[id(x)] = y
    if listiter is not None:
        for item in listiter:
            if deep:
                item = deepcopy(item, memo)
            y.append(item)
    if dictiter is not None:
        for key, value in dictiter:
            if deep:
                key = deepcopy(key, memo)
                value = deepcopy(value, memo)
            y[key] = value
    if state:
        if deep:
            state = deepcopy(state, memo)
        if hasattr(y, '__setstate__'):
            y.__setstate__(state)
        else:
            if isinstance(state, tuple) and len(state) == 2:
                state, slotstate = state
            else:
                slotstate = None
            if state is not None:
                y.__dict__.update(state)
            if slotstate is not None:
                for key, value in slotstate.iteritems():
                    setattr(y, key, value)
    return y

del d

del types

# Helper for instance creation without calling __init__
class _EmptyClass:
    pass

def _test():
    l = [None, 1, 2L, 3.14, 'xyzzy', (1, 2L), [3.14, 'abc'],
         {'abc': 'ABC'}, (), [], {}]
    l1 = copy(l)
    print l1==l
    l1 = map(copy, l)
    print l1==l
    l1 = deepcopy(l)
    print l1==l
    class C:
        def __init__(self, arg=None):
            self.a = 1
            self.arg = arg
            if __name__ == '__main__':
                import sys
                file = sys.argv[0]
            else:
                file = __file__
            self.fp = open(file)
            self.fp.close()
        def __getstate__(self):
            return {'a': self.a, 'arg': self.arg}
        def __setstate__(self, state):
            for key, value in state.iteritems():
                setattr(self, key, value)
        def __deepcopy__(self, memo=None):
            new = self.__class__(deepcopy(self.arg, memo))
            new.a = self.a
            return new
    c = C('argument sketch')
    l.append(c)
    l2 = copy(l)
    print l == l2
    print l
    print l2
    l2 = deepcopy(l)
    print l == l2
    print l
    print l2
    l.append({l[1]: l, 'xyz': l[2]})
    l3 = copy(l)
    import repr
    print map(repr.repr, l)
    print map(repr.repr, l1)
    print map(repr.repr, l2)
    print map(repr.repr, l3)
    l3 = deepcopy(l)
    import repr
    print map(repr.repr, l)
    print map(repr.repr, l1)
    print map(repr.repr, l2)
    print map(repr.repr, l3)

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = _emp_null
"""
this module contains a class that fits a gaussian model to the central
part of an histogram, following schwartzman et al, 2009. This is
typically necessary to estimate a fdr when one is not certain that the
data behaves as a standard normal under H_0.

Author : Bertrand Thirion, 2008-2009
"""
# For scipy import
from __future__ import absolute_import

import numpy as np
from numpy.linalg import pinv

from mvpa2.base import externals
if externals.exists('scipy', raise_=True):
    import scipy.stats as st

class FDR(object):
    """
    This is the basic class to handle false discovery rate computation
    parameter:
    fdr.x the samples from which the fdr is derived
    x is assumed to be a normal variate

    The Benjamini-Horchberg procedure is used 
    """
    
    def __init__(self, x):
        """
        x is assumed to be a 1-d array
        """
        self.x = np.squeeze(x)
        
    def all_fdr(self, x=None, verbose=0):
        """
        Returns all the FDR (false discovery rates) values for the sample x
        
        Parameters
        -----------
        x : ndarray of shape (n)
            The normal variates
        
        Results
        -------
        fdr : ndarray of shape (n)
            The set of all FDRs
        """
        if x==None:x=self.x
        pvals = st.norm.sf(x)
        return(self.all_fdr_from_pvals(pvals,verbose))

    def all_fdr_from_pvals(self, pv, verbose=0):
        """
        Returns the fdr associated with each the values

        Parameters
        -----------
        pv : ndarray of shape (n)
            The samples p-value
        
        Returns
        --------
        q : array of shape(n)
            The corresponding fdrs
        """
        pv = self.check_pv(pv)
        if pv==None:
            pv = self.pv
        n = np.size(pv)
        isx = np.argsort(pv)
        q = np.zeros(n)
        for ip in range(n):
            q[isx[ip]] = np.minimum(1, 
                                np.maximum(n*pv[isx[ip]]/(ip+1), q[isx[ip]]))
            if (ip<n-1):
                q[isx[ip+1]] = q[isx[ip]]
        
        if verbose:
            import matplotlib.pylab as mp
            mp.figure()
            mp.plot(pv, q, '.')
        return q

    def check_pv(self, pv):
        """
        Do some basic checks on the pv array: each value should be within [0,1]
        
        Parameters
        ----------
        pv : array of shape (n)
            The sample p-values

        Returns
        --------
        pv : array of shape (n)
            The sample p-values
        """
        pv = np.squeeze(pv)
        if pv.min()<0:
            print pv.min()
            raise ValueError, "Negative p-values"
        if pv.max()>1:
            print pv.max()
            raise ValueError, "P-values greater than 1!"
        return pv

    def pth_from_pvals(self, pv, alpha=0.05):
        """
        Given a set pv of p-values, returns the critical
        p-value associated with an FDR alpha
        
        Parameters
        -----------
        alpha : float
            The desired FDR significance
        pv : array of shape (n) 
            The samples p-value
        
        Returns
        -------
        pth: float
            The p value corresponding to the FDR alpha
        """
        pv = self.check_pv(pv)
        
        npv = np.size(pv)
        pcorr = alpha/npv
        spv = np.sort(pv)
        ip = 0
        pth = 0.
        while (spv[ip]<pcorr*(ip+1))&(ip<npv):
            pth = spv[ip]
            ip = ip+1
        return pth

    def threshold_from_student(self, df, alpha=0.05, x=None):
        """
        Given an array t of student variates with df dofs, returns the 
        critical p-value associated with alpha.
        
        Parameters
        -----------
        df : float
            The number of degrees of freedom
        alpha : float, optional
            The desired significance
        x : ndarray, optional
            The variate. By default self.x is used
        
        Returns
        --------
        th : float
            The threshold in variate value
        """
        df = float(df)
        if x is None:
            x = self.x
        pvals = st.t.sf(x, df)
        pth = self.pth_from_pvals(pvals, alpha)
        return st.t.isf(pth, df)

    def threshold(self, alpha=0.05, x=None):
        """
        Given an array x of normal variates, this function returns the
        critical p-value associated with alpha.
        x is explicitly assumed to be normal distributed under H_0
        
        Parameters
        -----------
        alpha: float, optional
            The desired significance, by default 0.05
        x : ndarray, optional
            The variate. By default self.x is used

        Returns
        --------
        th : float
            The threshold in variate value
        """
        if x==None:x=self.x
        pvals = st.norm.sf(x)
        pth = self.pth_from_pvals(pvals,alpha)
        return st.norm.isf(pth)
    

class ENN(object):
    """
    Class to compute the empirical null normal fit to the data.

    The data which is used to estimate the FDR, assuming a gaussian null
    from Schwartzmann et al., NeuroImage 44 (2009) 71--82
    """
    
    def __init__(self, x):
        """
        Initiate an empirical null normal object.

        Parameters
        -----------
        x : 1D ndarray
            The data used to estimate the empirical null.
        """
        x = np.reshape(x,(-1, ))
        self.x = np.sort(x)
        self.n = np.size(x)
        self.learned = 0

    def learn(self, left=0.2, right=0.8):
        """
        Estimate the proportion, mean and variance of a gaussian distribution 
        for a fraction of the data

        Parameters
        -----------
        left : float, optional
            Left cut parameter to prevent fitting non-gaussian data         
        right : float, optional 
            Right cut parameter to prevent fitting non-gaussian data

        Notes
        ------

        This method stores the following attributes:
         * mu = mu
         * p0 = min(1, np.exp(lp0))
         * sqsigma : standard deviation of the estimated normal
           distribution
         * sigma = np.sqrt(sqsigma) : variance of the estimated
           normal distribution
        """
        # take a central subsample of x
        x = self.x[int(self.n*left):int(self.n*right)]
    
        # generate the histogram
        step = 3.5*np.std(self.x)/np.exp(np.log(self.n)/3)
        bins = max(10, (self.x.max() - self.x.min())/step)
        hist, ledge = np.histogram(x, bins=bins)
        step = ledge[1]-ledge[0]
        medge = ledge + 0.5*step
    
        # remove null bins
        whist = hist>0
        hist = hist[whist]
        medge = medge[whist]
        hist = hist.astype('f')

        # fit the histogram
        DMtx = np.ones((3, np.sum(whist)))
        DMtx[1] = medge
        DMtx[2] = medge**2
        coef = np.dot(np.log(hist), pinv(DMtx))
        sqsigma = -1.0/(2*coef[2])
        mu = coef[1]*sqsigma
        lp0 = (coef[0]- np.log(step*self.n) 
                + 0.5*np.log(2*np.pi*sqsigma) + mu**2/(2*sqsigma))
        self.mu = mu
        self.p0 = min(1, np.exp(lp0))
        self.sigma = np.sqrt(sqsigma)
        self.sqsigma = sqsigma

    def fdrcurve(self):
        """
        Returns the fdr associated with any point of self.x
        """
        import scipy.stats as st
        if self.learned==0:
            self.learn()
        efp = ( self.p0*st.norm.sf(self.x, self.mu, self.sigma)
               *self.n/np.arange(self.n,0,-1))
        efp = np.minimum(efp, 1)
        return efp

    def threshold(self, alpha=0.05, verbose=0):
        """
        Compute the threshold correponding to an alpha-level fdr for x

        Parameters
        -----------
        alpha : float, optional
            the chosen false discovery rate threshold.
        verbose : boolean, optional
            the verbosity level, if True a plot is generated.
        
        Results
        --------
        theta: float
            the critical value associated with the provided fdr
        """
        efp = self.fdrcurve()
        if verbose:
            self.plot(efp, alpha)
    
        if efp[-1] > alpha:
            print "the maximal value is %f , the corresponding fdr is %f " \
                    % (self.x[-1], efp[-1])
            return np.infty
        j = np.argmin(efp[::-1] < alpha) + 1
        return 0.5*(self.x[-j] + self.x[-j+1])

    def uncorrected_threshold(self, alpha=0.001, verbose=0):
        """
        Compute the threshold correponding to a specificity alpha for x

        Parameters
        -----------
        alpha : float, optional
            the chosen false discovery rate threshold.
        verbose : boolean, optional
            the verbosity level, if True a plot is generated.
        
        Results
        --------
        theta: float
            the critical value associated with the provided p-value
        """
        if self.learned==0:
            self.learn()
        threshold = st.norm.isf(alpha, self.mu, self.sigma)
        if not np.isfinite(threshold):
            threshold = np.inf
        if verbose:
            self.plot()
        return threshold

    def fdr(self,theta):
        """
        given a threshold theta, find the estimated fdr
        """
        import scipy.stats as st
        if self.learned==0:
            self.learn()
        efp = self.p0*st.norm.sf(theta,self.mu,self.sigma)\
              *float(self.n)/np.sum(self.x>theta)
        efp = np.minimum(efp,1)
        return efp

    def plot(self, efp=None, alpha=0.05, bar=1, mpaxes=None):
        """
        plot the  histogram of x
        
        Parameters
        ------------
        efp : float, optional 
            The empirical fdr (corresponding to x)
            if efp==None, the false positive rate threshod plot is not 
            drawn.
        alpha : float, optional 
            The chosen fdr threshold
        bar=1 : bool, optional
        mpaxes=None: if not None, handle to an axes where the fig.
        will be drawn. Avoids creating unnecessarily new figures.
        """ 
        if not self.learned:
            self.learn()
        n = np.size(self.x)
        bins = max(10, int(2*np.exp(np.log(n)/3.)))
        hist, ledge = np.histogram(self.x, bins=bins)
        hist = hist.astype('f')/hist.sum()
        step = ledge[1]-ledge[0]
        medge = ledge + 0.5*step
        import scipy.stats as st
        g = self.p0*st.norm.pdf(medge, self.mu, self.sigma) 
        hist /= step
        
        import matplotlib.pylab as mp
        if mpaxes==None:
            mp.figure()
            ax = mp.subplot(1,1,1)
        else:
            ax = mpaxes 
        if bar:
            # We need to cut ledge to len(hist) to accomodate for pre and
            # post numpy 1.3 hist semantic change.
            ax.bar(ledge[:len(hist)], hist, step)
        else:
            ax.plot(medge[:len(hist)], hist, linewidth=2)
        ax.plot(medge, g, 'r', linewidth=2)
        ax.set_title('Robust fit of the histogram', fontsize=16)
        l = ax.legend(('empiricall null', 'data'), loc=0)
        for t in l.get_texts():
            t.set_fontsize(16)
        ax.set_xticklabels(ax.get_xticks(), fontsize=16)
        ax.set_yticklabels(ax.get_yticks(), fontsize=16)

        if efp != None:
            ax.plot(self.x, np.minimum(alpha, efp), 'k')
    

 
def three_classes_GMM_fit(x, test=None, alpha=0.01, prior_strength=100,
                          verbose=0, fixed_scale=False, mpaxes=None, bias=0, 
                          theta=0, return_estimator=False):
    """
     Fit the data with a 3-classes Gaussian Mixture Model,
    i.e. computing some probability that the voxels of a certain map
    are in class disactivated, null or active

    
    Parameters
    ----------
    x array of shape (nvox,1): the map to be analysed
    test=None array of shape(nbitems,1):
      the test values for which the p-value needs to be computed
      by default, test=x
    alpha = 0.01 the prior weights of the positive and negative classes
    prior_strength = 100 the confidence on the prior
                   (should be compared to size(x))
    verbose=0 : verbosity mode
    fixed_scale = False, boolean, variance parameterization
                if True, the variance is locked to 1
                otherwise, it is estimated from the data
    mpaxes=None: axes handle used to plot the figure in verbose mode
                 if None, new axes are created
    bias = 0: allows a recaling of the posterior probability
         that takes into account the thershold theta. Not rigorous.
    theta = 0 the threshold used to correct the posterior p-values
          when bias=1; normally, it is such that test>theta
          note that if theta = -np.infty, the method has a standard behaviour
    return_estimator: boolean, optional
            If return_estimator is true, the estimator object is
            returned.
    
    Results
    -------
    bfp : array of shape (nbitems,3):
        the posterior probability of each test item belonging to each component
        in the GMM (sum to 1 across the 3 classes)
        if np.size(test)==0, i.e. nbitem==0, None is returned
    estimator : nipy.neurospin.clustering.GMM object
        The estimator object, returned only if return_estimator is true.

    Note
    ----
    Our convention is that
    - class 1 represents the negative class
    - class 2 represenst the null class
    - class 3 represents the positsive class
    """
    nvox = np.size(x)
    x = np.reshape(x,(nvox,1))
    if test==None:
        test = x
    if np.size(test)==0:
        return None
    
    from nipy.neurospin.clustering.bgmm import VBGMM
    from nipy.neurospin.clustering.gmm import grid_descriptor
    
    sx = np.sort(x,0)   
    nclasses=3
    
    # set the priors from a reasonable model of the data (!)

    # prior means 
    mb0 = np.mean(sx[:alpha*nvox])
    mb2 = np.mean(sx[(1-alpha)*nvox:])
    prior_means = np.reshape(np.array([mb0,0,mb2]),(nclasses,1))
    if fixed_scale:
        prior_scale = np.ones((nclasses,1,1)) * 1./(prior_strength)
    else:
        prior_scale = np.ones((nclasses,1,1)) * 1./(prior_strength*np.var(x))
    prior_dof = np.ones(nclasses) * prior_strength
    prior_weights = np.array([alpha,1-2*alpha,alpha]) * prior_strength
    prior_shrinkage = np.ones(nclasses) * prior_strength

    # instantiate the class and set the priors
    BayesianGMM = VBGMM(nclasses,1,prior_means,prior_scale,
                        prior_weights, prior_shrinkage,prior_dof)
    BayesianGMM.set_priors(prior_means, prior_weights, prior_scale,
                           prior_dof, prior_shrinkage)

    # estimate the model
    BayesianGMM.estimate(x,delta = 1.e-8,verbose=verbose)

    # create a sampling grid
    if (verbose or bias):
        gd = grid_descriptor(1) 
        gd.getinfo([x.min(),x.max()],100)
        gdm = gd.make_grid().squeeze()
        lj = BayesianGMM.likelihood(gd.make_grid())
    
    # estimate the prior weights
    bfp = BayesianGMM.likelihood(test)
    if bias:
        lw = np.sum(lj[gdm>theta],0)
        weights = BayesianGMM.weights/(BayesianGMM.weights.sum())
        bfp = (lw/weights)*BayesianGMM.slikelihood(test)
    
    if verbose>1:
        BayesianGMM.show_components(x,gd,lj,mpaxes)

    bfp = (bfp.T/bfp.sum(1)).T
    if not return_estimator:
        return bfp
    else:
        return bfp, BayesianGMM


def Gamma_Gaussian_fit(x, test=None, verbose=0, mpaxes=None,
                       bias=1, gaussian_mix=0, return_estimator=False):
    """
    Computing some prior probabilities that the voxels of a certain map
    are in class disactivated, null or active uning a gamma-Gaussian mixture
    
    Parameters
    ------------
    x: array of shape (nvox,)
        the map to be analysed
    test: array of shape (nbitems,), optional
        the test values for which the p-value needs to be computed
        by default, test = x
    verbose: 0, 1 or 2, optional
        verbosity mode, 0 is quiet, and 2 calls matplotlib to display
        graphs.
    mpaxes: matplotlib axes, option.
        axes handle used to plot the figure in verbose mode
        if None, new axes are created
    bias: float, optional
            lower bound on the gaussian variance (to avoid shrinkage)
    gaussian_mix: float, optional
            if nonzero, lower bound on the gaussian mixing weight 
            (to avoid shrinkage)
    return_estimator: boolean, optional
            If return_estimator is true, the estimator object is
            returned.
    
    Returns
    -------
    bfp: array of shape (nbitems,3)
            The probability of each component in the mixture model for each 
            test value
    estimator: nipy.neurospin.clustering.ggmixture.GGGM object
        The estimator object, returned only if return_estimator is true.
    """
    from nipy.neurospin.clustering import ggmixture
    Ggg = ggmixture.GGGM()
    Ggg.init_fdr(x)
    Ggg.estimate(x, niter=100, delta=1.e-8, bias=bias, verbose=0,
                    gaussian_mix=gaussian_mix)
    if verbose>1:
        # hyper-verbose mode
        Ggg.show(x, mpaxes=mpaxes)
        Ggg.parameters()
    if test is None:
        test = x

    test = np.reshape(test, np.size(test))
   
    bfp = np.array(Ggg.posterior(test)).T
    if return_estimator:
        return bfp, Ggg
    return bfp

########NEW FILE########
__FILENAME__ = clfs
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Provides `clfs` dictionary with instances of all available classifiers."""

__docformat__ = 'restructuredtext'

# Global modules
import numpy as np

# Some global imports useful through out unittests
from mvpa2.base import cfg

# Base classes
from mvpa2.clfs.base import Classifier
from mvpa2.datasets.base import Dataset
from mvpa2.measures.base import FeaturewiseMeasure

#
# first deal with classifiers which do not have external deps
#
from mvpa2.clfs.dummies import *
from mvpa2.clfs.smlr import SMLR
from mvpa2.clfs.knn import *

from mvpa2.clfs.warehouse import clfswh, regrswh
from mvpa2.base import externals


__all__ = ['clfswh', 'regrswh', 'Classifier', 'SameSignClassifier',
           'Less1Classifier', 'sample_clf_nl', 'sample_clf_lin',
           'sample_clf_reg', 'cfg', 'SillySensitivityAnalyzer']

# if have ANY svm implementation
if externals.exists('libsvm') or externals.exists('shogun'):
    from mvpa2.clfs.svm import *
    __all__ += ['LinearCSVMC']
    if externals.exists('libsvm'):
        __all__ += ['libsvm', 'LinearNuSVMC']
    if externals.exists('shogun'):
        __all__ += ['sg']


class SillySensitivityAnalyzer(FeaturewiseMeasure):
    """Simple one which just returns xrange[-N/2, N/2], where N is the
    number of features
    """
    is_trained = True

    def __init__(self, mult=1, **kwargs):
        FeaturewiseMeasure.__init__(self, **kwargs)
        self.__mult = mult

    def _call(self, dataset):
        """Train linear SVM on `dataset` and extract weights from classifier.
        """
        sens = self.__mult *( np.arange(dataset.nfeatures) - int(dataset.nfeatures/2) )
        return Dataset(sens[np.newaxis])



# Sample universal classifiers (linear and non-linear) which should be
# used whenever it doesn't matter what classifier it is for testing
# some higher level creations -- chosen so it is the fastest universal
# one. Also it should not punch state.py in the face how it is
# happening with kNN...
sample_clf_lin = SMLR(lm=0.1)#sg.svm.LinearCSVMC(svm_impl='libsvm')

#if externals.exists('shogun'):
#    sample_clf_nl = sg.SVM(kernel_type='RBF', svm_impl='libsvm')
#else:
#classical one which was used for a while
#and surprisingly it is not bad at all for the unittests
sample_clf_nl = kNN(k=5)

# and also a regression-based classifier
r = clfswh['linear', 'regression_based', 'has_sensitivity']
if len(r) > 0: sample_clf_reg = r[0]
else: sample_clf_reg = None

########NEW FILE########
__FILENAME__ = datasets
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Provides convenience datasets for unittesting.

Also performs testing of storing/reloading datasets into hdf5 file if
cfg.getboolean('tests', 'use hdf datasets'
"""

__docformat__ = 'restructuredtext'

import tempfile
import shutil
import os
import traceback as tbm
import sys
import numpy as np

from mvpa2 import cfg, externals
from mvpa2.datasets.base import Dataset, HollowSamples
from mvpa2.generators.partition import OddEvenPartitioner
from mvpa2.misc.data_generators import *
from mvpa2.testing.tools import reseed_rng

__all__ = [ 'datasets', 'get_random_rotation', 'saveload_warehouse',
            'pure_multivariate_signal']

# Define datasets to be used all over. Split-half later on is used to
# split into training/testing
#
snr_scale = cfg.get_as_dtype('tests', 'snr scale', float, default=1.0)

specs = {'large' : { 'perlabel': 99, 'nchunks': 11,
                     'nfeatures': 20, 'snr': 8 * snr_scale},
         'medium' :{ 'perlabel': 24, 'nchunks': 6,
                     'nfeatures': 14, 'snr': 8 * snr_scale},
         'small' : { 'perlabel': 12, 'nchunks': 4,
                     'nfeatures': 6, 'snr' : 14 * snr_scale} }

# to assure reproducibility -- lets reseed the RNG at this point
@reseed_rng()
def generate_testing_datasets(specs):
    # Lets permute upon each invocation of test, so we could possibly
    # trigger some funny cases
    nonbogus_pool = np.random.permutation([0, 1, 3, 5])

    datasets = {}

    # use a partitioner to flag odd/even samples as training and test
    ttp = OddEvenPartitioner(space='train', count=1)

    for kind, spec in specs.iteritems():
        # set of univariate datasets
        for nlabels in [ 2, 3, 4 ]:
            basename = 'uni%d%s' % (nlabels, kind)
            nonbogus_features = nonbogus_pool[:nlabels]

            dataset = normal_feature_dataset(
                nlabels=nlabels,
                nonbogus_features=nonbogus_features,
                **spec)

            # full dataset
            datasets[basename] = list(ttp.generate(dataset))[0]

        # sample 3D
        total = 2*spec['perlabel']
        nchunks = spec['nchunks']
        data = np.random.standard_normal(( total, 3, 6, 6 ))
        labels = np.concatenate( ( np.repeat( 0, spec['perlabel'] ),
                                  np.repeat( 1, spec['perlabel'] ) ) )
        data[:, 1, 0, 0] += 2*labels           # add some signal
        chunks = np.asarray(range(nchunks)*(total//nchunks))
        mask = np.ones((3, 6, 6), dtype='bool')
        mask[0, 0, 0] = 0
        mask[1, 3, 2] = 0
        ds = Dataset.from_wizard(samples=data, targets=labels, chunks=chunks,
                                 mask=mask, space='myspace')
        # and to stress tests on manipulating sa/fa possibly containing
        # attributes of dtype object
        ds.sa['test_object'] = [['a'], [1, 2]] * (ds.nsamples//2)
        datasets['3d%s' % kind] = ds


    # some additional datasets
    datasets['dumb2'] = dumb_feature_binary_dataset()
    datasets['dumb'] = dumb_feature_dataset()
    # dataset with few invariant features
    _dsinv = dumb_feature_dataset()
    _dsinv.samples = np.hstack((_dsinv.samples,
                               np.zeros((_dsinv.nsamples, 1)),
                               np.ones((_dsinv.nsamples, 1))))
    datasets['dumbinv'] = _dsinv

    # Datasets for regressions testing
    datasets['sin_modulated'] = list(ttp.generate(multiple_chunks(sin_modulated, 4, 30, 1)))[0]
    # use the same full for training
    datasets['sin_modulated_train'] = datasets['sin_modulated']
    datasets['sin_modulated_test'] = sin_modulated(30, 1, flat=True)

    # simple signal for linear regressors
    datasets['chirp_linear'] = multiple_chunks(chirp_linear, 6, 50, 10, 2, 0.3, 0.1)
    datasets['chirp_linear_test'] = chirp_linear(20, 5, 2, 0.4, 0.1)

    datasets['wr1996'] = multiple_chunks(wr1996, 4, 50)
    datasets['wr1996_test'] = wr1996(50)

    datasets['hollow'] = Dataset(HollowSamples((40,20)),
                                 sa={'targets': np.tile(['one', 'two'], 20)})

    return datasets

# avoid treating it as a test by nose
generate_testing_datasets.__test__ = False

def saveload_warehouse():
    """Store all warehouse datasets into HDF5 and reload them.
    """
    import h5py
    from mvpa2.base.hdf5 import obj2hdf, hdf2obj

    tempdir = tempfile.mkdtemp()

    # store the whole datasets warehouse in one hdf5 file
    hdf = h5py.File(os.path.join(tempdir, 'myhdf5.hdf5'), 'w')
    for d in datasets:
        obj2hdf(hdf, datasets[d], d)
    hdf.close()

    hdf = h5py.File(os.path.join(tempdir, 'myhdf5.hdf5'), 'r')
    rc_ds = {}
    for d in hdf:
        rc_ds[d] = hdf2obj(hdf[d])
    hdf.close()

    #cleanup temp dir
    shutil.rmtree(tempdir, ignore_errors=True)

    # return the reconstructed datasets (for use in datasets warehouse)
    return rc_ds


datasets = generate_testing_datasets(specs)

if cfg.getboolean('tests', 'use hdf datasets', False):
    if not externals.exists('h5py'):
        raise RuntimeError(
            "Cannot perform HDF5 dump of all datasets in the warehouse, "
            "because 'h5py' is not available")

    datasets = saveload_warehouse()
    print "Replaced all dataset warehouse for HDF5 loaded alternative."

########NEW FILE########
__FILENAME__ = sweep
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Submodule to provide sweepargs decorator for unittests"""

__docformat__ = 'restructuredtext'

import sys
import traceback as tbm

from mvpa2 import cfg
from mvpa2.testing.tools import SkipTest

if __debug__:
    from mvpa2.base import debug

__all__ = [ 'sweepargs' ]


def sweepargs(**kwargs):
    """Decorator function to sweep over a given set of classifiers

    Parameters
    ----------
    clfs : list of `Classifier`
      List of classifiers to run method on

    Often some unittest method can be ran on multiple classifiers.
    So this decorator aims to do that
    """

    from mvpa2.clfs.base import Classifier
    from mvpa2.base.state import ClassWithCollections

    def unittest_method(method):
        def do_sweep(*args_, **kwargs_):
            """Perform sweeping over provided keyword arguments
            """
            def untrain_clf(argvalue):
                """Little helper"""
                if isinstance(argvalue, Classifier):
                    # clear classifier after its use -- just to be sure ;-)
                    argvalue.params.retrainable = False
                    argvalue.untrain()

            failed_tests = {}
            skipped_tests = []
            report_progress = cfg.get('tests', 'verbosity', default=1) > 1
            for argname in kwargs.keys():
                for argvalue in kwargs[argname]:
                    if isinstance(argvalue, Classifier):
                        # clear classifier before its use
                        argvalue.untrain()
                    if isinstance(argvalue, ClassWithCollections):
                        argvalue.ca.reset()
                    # update kwargs_
                    kwargs_[argname] = argvalue
                    # do actual call
                    try:
                        if __debug__:
                            debug('TEST', 'Running %s on args=%r and kwargs=%r'
                                  % (method.__name__, args_, kwargs_))
                        method(*args_, **kwargs_)
                        status = '+'
                    except SkipTest, e:
                        skipped_tests += [e]
                        status = 'S'
                    except AssertionError, e:
                        status = 'F'
                        estr = str(e)
                        etype, value, tb = sys.exc_info()
                        # literal representation of exception tb, so
                        # we could group them later on
                        eidstr = '  '.join(
                            [l for l in tbm.format_exception(etype, value, tb)
                             if not ('do_sweep' in l
                                     or 'unittest.py' in l
                                     or 'AssertionError' in l
                                     or 'Traceback (most' in l)])

                        # Store exception information for later on groupping
                        if not eidstr in failed_tests:
                            failed_tests[eidstr] = []

                        sargvalue = str(argvalue)
                        if not (__debug__ and 'TEST' in debug.active):
                            # by default lets make it of sane length
                            if len(sargvalue) > 100:
                                sargvalue = sargvalue[:95] + ' ...'
                        failed_tests[eidstr].append(
                            # skip top-most tb in sweep_args
                            (argname, sargvalue, tb.tb_next, estr))

                        if __debug__:
                            msg = "%s on %s=%s" % (estr, argname, argvalue)
                            debug('TEST', 'Failed unittest: %s\n%s'
                                  % (eidstr, msg))
                    if report_progress:
                        sys.stdout.write(status)
                        sys.stdout.flush()

                    untrain_clf(argvalue)
                    # TODO: handle different levels of unittests properly
                    if cfg.getboolean('tests', 'quick', False):
                        # on TESTQUICK just run test for 1st entry in the list,
                        # the rest are omitted
                        # TODO: proper partitioning of unittests
                        break
            if report_progress:
                sys.stdout.write(' ')
                sys.stdout.flush()
            if len(failed_tests):
                # Lets now create a single AssertionError exception
                # which would nicely incorporate all failed exceptions
                multiple = len(failed_tests) != 1 # is it unique?
                # if so, we don't need to reinclude traceback since it
                # would be spitted out anyways below
                estr = ""
                cestr = "lead to failures of unittest %s" % method.__name__
                if multiple:
                    estr += "\n Different scenarios %s "\
                            "(specific tracebacks are below):" % cestr
                else:
                    estr += "\n Single scenario %s:" % cestr
                for ek, els in failed_tests.iteritems():
                    estr += '\n'
                    if multiple:
                        estr += ek
                    estr += "  on\n    %s" % ("    ".join(
                            ["%s=%s%s\n" %
                             (ea, eav,
                              # Why didn't I just do regular for loop? ;)
                              ":\n     ".join([xx for xx in [' ', es]
                                               if xx != '']))
                             for ea, eav, etb, es in els]))
                    # take first one... they all should be identical
                    etb = els[0][2]
                raise AssertionError(estr), None, etb
            if len(skipped_tests):
                # so if nothing has failed, lets at least report that some were
                # skipped -- for now just  a simple SkipTest message
                raise SkipTest("%d tests were skipped in testing %s"
                               % (len(skipped_tests), method.func_name))
        do_sweep.func_name = method.func_name
        do_sweep.__doc__ = method.__doc__
        return do_sweep

    if len(kwargs) > 1:
        raise NotImplementedError, \
              "No sweeping over multiple arguments in sweepargs. Meanwhile " \
              "use two @sweepargs decorators for the test."

    return unittest_method

########NEW FILE########
__FILENAME__ = tools
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""A Collection of tools found useful in unittests.

Primarily the ones from nose.tools
"""
__docformat__ = 'restructuredtext'

import glob, os, sys, shutil
import tempfile
import unittest
from contextlib import contextmanager

import numpy as np

import mvpa2
from mvpa2.base import externals, warning

if __debug__:
    from mvpa2.base import debug

if externals.exists('nose'):
    # We use nose now
    from nose import SkipTest
    from nose.tools import (
        ok_, eq_,
        # Asserting (pep8-ed from unittest)
        assert_true, assert_false, assert_raises,
        assert_equal, assert_equals, assert_not_equal, assert_not_equals,
        # Decorators
        timed, with_setup, raises, istest, nottest, make_decorator )
else:
    # Lets make it possible to import testing.tools even if nose is
    # NA, and run unittests which do not require nose yet
    def _need_nose(*args, **kwargs):
        """Catcher for unittests requiring nose functionality
        """
        raise unittest.TestCase.failureException(
            "Unittest requires nose testing framework")

    ok_ = eq_ = assert_true = assert_false = assert_raises = \
    assert_equal = assert_equals = assert_not_equal = asserte_not_equals = \
    timed = with_setup = raises = istest = nottest = make_decorator = _need_nose

    class SkipTest(Exception):
        """Raise this exception to mark a test as skipped.
        """
        pass

# Some pieces are useful from numpy.testing
from numpy.testing import (
    assert_almost_equal, assert_approx_equal,
    assert_array_almost_equal, assert_array_equal, assert_array_less,
    assert_string_equal)

def assert_array_lequal(x, y):
    assert_array_less(-y, -x)

if sys.version_info < (2, 7):
    # compatibility helpers for testing functions introduced in more recent versions
    # of unittest/nose

    def assert_is_instance(obj, cls, msg=None):
        assert_true(isinstance(obj, cls), msg=msg)

if externals.exists('mock'):
    import mock

    @contextmanager
    def assert_warnings(messages):
        with mock.patch("warnings.warn") as mock_warnings:
            yield
            #import pydb; pydb.debugger()
            if externals.versions['mock'] >= '0.8.0':
                mock_calls = mock_warnings.mock_calls
            else:
                # lacks leading element
                mock_calls = [(None,) + mock_warnings.call_args]

            warning_list = [(call[2]['category'], call[1][0])
                            for call in mock_calls]
            assert_equal(
                messages,
                warning_list
            )
else:
    @contextmanager
    def assert_warnings(messages):
        yield
        raise SkipTest, "install python-mock for testing either warnings were issued"

def skip_if_no_external(dep, ver_dep=None, min_version=None, max_version=None):
    """Raise SkipTest if external is missing

    Parameters
    ----------
    dep : string
      Name of the external
    ver_dep : string, optional
      If for version checking use some different key, e.g. shogun:rev.
      If not specified, `dep` will be used.
    min_version : None or string or tuple
      Minimal required version
    max_version : None or string or tuple
      Maximal required version
    """

    if not externals.exists(dep):
        raise SkipTest, \
              "External %s is not present thus tests battery skipped" % dep

    if ver_dep is None:
        ver_dep = dep

    if min_version is not None and externals.versions[ver_dep] < min_version:
        raise SkipTest, \
              "Minimal version %s of %s is required. Present version is %s" \
              ". Test was skipped." \
              % (min_version, ver_dep, externals.versions[ver_dep])

    if max_version is not None and externals.versions[ver_dep] > max_version:
        raise SkipTest, \
              "Maximal version %s of %s is required. Present version is %s" \
              ". Test was skipped." \
              % (min_version, ver_dep, externals.versions[ver_dep])


def with_tempfile(*targs, **tkwargs):
    """Decorator function to provide a temporary file name and remove it at the end.

    All arguments are passed into the call to tempfile.mktemp(), and
    resultant temporary filename is passed as the first argument into
    the test.  If no 'prefix' argument is provided, it will be
    constructed using module and function names ('.' replaced with
    '_').

    Example use::

        @with_tempfile()
        def test_write(tfile):
            open(tfile, 'w').write('silly test')
    """

    def decorate(func):
        def newfunc(*arg, **kw):
            if len(targs)<2 and not 'prefix' in tkwargs:
                try:
                    tkwargs['prefix'] = 'tempfile_%s.%s' \
                                        % (func.__module__, func.func_name)
                except:
                    # well -- if something wrong just proceed with defaults
                    pass

            filename = tempfile.mktemp(*targs, **tkwargs)
            if __debug__:
                debug('TEST', 'Running %s with temporary filename %s'
                      % (func.__name__, filename))
            try:
                func(*(arg + (filename,)), **kw)
            finally:
                # glob here for all files with the same name (-suffix)
                # would be useful whenever we requested .img filename,
                # and function creates .hdr as well
                lsuffix = len(tkwargs.get('suffix', ''))
                filename_ = lsuffix and filename[:-lsuffix] or filename
                filenames = glob.glob(filename_ + '*')
                if len(filename_) < 3 or len(filenames) > 5:
                    # For paranoid yoh who stepped into this already ones ;-)
                    warning("It is unlikely that it was intended to remove all"
                            " files matching %r. Skipping" % filename_)
                    return
                for f in filenames:
                    try:
                        # Can also be a directory
                        if os.path.isdir(f):
                            shutil.rmtree(f)
                        else:
                            os.unlink(f)
                    except OSError:
                        pass
        newfunc = make_decorator(func)(newfunc)
        return newfunc

    return decorate


def reseed_rng():
    """Decorator to assure the use of MVPA_SEED while running the test

    It resets random number generators (both python and numpy) to the
    initial value of the seed value which was set while importing
    :mod:`mvpa`, which could be controlled through
    configuration/environment.

    Examples
    --------
    >>> @reseed_rng()
    ... def test_random():
    ...     import numpy.random as rnd
    ...     print rnd.randint(100)

    """

    def decorate(func):
        def newfunc(*arg, **kwargs):
            mvpa2.seed(mvpa2._random_seed)
            return func(*arg, **kwargs)
        newfunc = make_decorator(func)(newfunc)
        return newfunc

    return decorate


def nodebug(entries=None):
    """Decorator to temporarily turn off some debug targets

    Parameters
    ----------
    entries : None or list of string, optional
      If None, all debug entries get turned off.  Otherwise only provided
      ones
    """

    def decorate(func):
        def newfunc(*arg, **kwargs):
            if __debug__:
                from mvpa2.base import debug
                # store a copy
                old_active = debug.active[:]
                if entries is None:
                    # turn them all off
                    debug.active = []
                else:
                    for e in entries:
                        if e in debug.active:
                            debug.active.remove(e)
            try:
                res = func(*arg, **kwargs)
                return res
            finally:
                # we should return the debug states to the original
                # state regardless either test passes or not!
                if __debug__:
                    # turn debug targets back on
                    debug.active = old_active

        newfunc = make_decorator(func)(newfunc)
        return newfunc

    return decorate


def labile(niter=3, nfailures=1):
    """Decorator for labile tests -- runs multiple times

    Let's reduce probability of random failures but re-running the
    test multiple times allowing to fail few in a row.  Makes sense
    only for tests which run on random data, so usually decorated with
    reseed_rng.  Otherwise it is unlikely that result would change if
    algorithms are deterministic and operate on the same data

    Parameters
    ----------
    niter: int, optional
      How many iterations to run maximum
    nfailures: int, optional
      How many failures to allow

    """
    def decorate(func):
        def newfunc(*arg, **kwargs):
            nfailed, i = 0, 0           # define i just in case
            for i in xrange(niter):
                try:
                    ret = func(*arg, **kwargs)
                    if i + 1 - nfailed  >= niter - nfailures:
                        # so we know already that we wouldn't go over
                        # nfailures
                        break
                except AssertionError, e:
                    nfailed += 1
                    if __debug__:
                        debug('TEST', "Upon %i-th run, test %s failed with %s",
                              (i, func.__name__, e))

                    if nfailed > nfailures:
                        if __debug__:
                            debug('TEST', "Ran %s %i times. Got %d failures, "
                                  "while was allowed %d "
                                  "-- re-throwing the last failure %s",
                                  (func.__name__, i+1, nfailed, nfailures, e))
                        exc_info = sys.exc_info()
                        raise exc_info[1], None, exc_info[2]
            if __debug__:
                debug('TEST', "Ran %s %i times. Got %d failures.",
                      (func.__name__, i+1, nfailed))
            return ret
        newfunc = make_decorator(func)(newfunc)
        return newfunc
    assert(niter > nfailures)
    return decorate


def assert_objectarray_equal(x, y, xorig=None, yorig=None, strict=True):
    """Wrapper around assert_array_equal to compare object arrays

    See http://projects.scipy.org/numpy/ticket/2117
    for the original report on oddity of dtype object arrays comparisons

    Parameters
    ----------

    strict: bool
        Assure also that dtypes are the same.  Otherwise it is pretty much
        value comparison
    """
    try:
        assert_array_equal(x, y)
    except AssertionError, e:
        if not ((x.dtype == object) and (y.dtype == object)):
            raise
        # pass inside original arrays for a meaningful assertion
        # failure msg
        if xorig is None:
            xorig = x
        if yorig is None:
            yorig = y
        try:
            # we will try harder comparing each element the same way
            # and also enforcing equal dtype
            for x_, y_ in zip(x, y):
                assert(type(x_) == type(y_))
                if strict and isinstance(x_, np.ndarray) and not (x_.dtype == y_.dtype):
                    raise AssertionError("dtypes %r and %r do not match" %
                                         (x_.dtype, y_.dtype))
                assert_objectarray_equal(x_, y_, xorig, yorig)
        except Exception, e:
            if not isinstance(e, AssertionError):
                raise AssertionError("%r != %r, thus %s != %s" %
                                     (x, y, xorig, yorig))
            raise

########NEW FILE########
__FILENAME__ = cPickle_disabled
raise ImportError

########NEW FILE########
__FILENAME__ = ctypes
raise ImportError

########NEW FILE########
__FILENAME__ = griddata
raise ImportError

########NEW FILE########
__FILENAME__ = gzip
raise ImportError

########NEW FILE########
__FILENAME__ = h5py
raise ImportError

########NEW FILE########
__FILENAME__ = hcluster
raise ImportError

########NEW FILE########
__FILENAME__ = libsvm
raise ImportError

########NEW FILE########
__FILENAME__ = lxml
raise ImportError

########NEW FILE########
__FILENAME__ = mdp
raise ImportError

########NEW FILE########
__FILENAME__ = nibabel
raise ImportError

########NEW FILE########
__FILENAME__ = nifti
raise ImportError

########NEW FILE########
__FILENAME__ = nipy
raise ImportError

########NEW FILE########
__FILENAME__ = openopt
raise ImportError

########NEW FILE########
__FILENAME__ = pandas
raise ImportError

########NEW FILE########
__FILENAME__ = pprocess
raise ImportError

########NEW FILE########
__FILENAME__ = psutil
raise ImportError

########NEW FILE########
__FILENAME__ = pylab
raise ImportError

########NEW FILE########
__FILENAME__ = pywt
raise ImportError

########NEW FILE########
__FILENAME__ = reportlab
raise ImportError

########NEW FILE########
__FILENAME__ = rpy
raise ImportError

########NEW FILE########
__FILENAME__ = rpy2
raise ImportError

########NEW FILE########
__FILENAME__ = scipy
raise ImportError

########NEW FILE########
__FILENAME__ = shogun
raise ImportError

########NEW FILE########
__FILENAME__ = skimage
raise ImportError

########NEW FILE########
__FILENAME__ = sklearn
raise ImportError

########NEW FILE########
__FILENAME__ = statsmodels
raise ImportError

########NEW FILE########
__FILENAME__ = weave
raise ImportError

########NEW FILE########
__FILENAME__ = runner
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helper module to enable profiling of the testcase

 If environment variable PROFILELEVEL is set it uses hotshot profiler
 for unittest.main() call. Value of PROFILELEVEL defines number of top
 busy functions to report.

 Environment variable PROFILELINES=1 makes hotshot store information
 per each line, so it could be easily inspected later on.

 Output:
   Profiler stores its Stats into a file named after original script
   (sys.argv[0]) with suffix".prof" appended

 Usage:
   Replace unittest.main() with import runner

 Visualization:
   kcachegrind provides nice interactive GUI to inspect profiler
   results. If PROFILELINES was set to 1, it provides information per
   each line.

   To convert .prof file into a file suitable for kcachegrind, use
   utility hotshot2calltree which comes in package
   kcachegrind-converters.

 Example:

 # profile and output 3 most expensive function calls
 PROFILELEVEL=3 PROFILELINES=1 PYTHONPATH=../ python test_searchlight.py
 # convert to kcachegrind format
 hotshot2calltree -o test_searchlight.py.kcache  test_searchlight.py.prof
 # inspect
 kcachegrind test_searchlight.py.kcache

"""

__test__ = False

import unittest
import sys

from os import environ

from mvpa2 import _random_seed

# Extend TestProgram to print out the seed which was used
class TestProgramPyMVPA(unittest.TestProgram):
    ##REF: Name was automagically refactored
    def run_tests(self):
        if self.verbosity:
            print "MVPA_SEED=%s:" % _random_seed,
            sys.stdout.flush()
        super(TestProgramPyMVPA, self).run_tests()

def run():
    profilelevel = None

    if environ.has_key('PROFILELEVEL'):
        profilelevel = int(environ['PROFILELEVEL'])


    if profilelevel is None:
        TestProgramPyMVPA()
    else:
        profilelines = environ.has_key('PROFILELINES')

        import hotshot, hotshot.stats
        pname = "%s.prof" % sys.argv[0]
        prof = hotshot.Profile(pname, lineevents=profilelines)
        try:
            # actually return values are never setup
            # since unittest.main sys.exit's
            benchtime, stones = prof.runcall( unittest.main )
        except SystemExit:
            pass
        print "Saving profile data into %s" % pname
        prof.close()
        if profilelevel > 0:
            # we wanted to see the summary right here
            # instead of just storing it into a file
            print "Loading profile data from %s" % pname
            stats = hotshot.stats.load(pname)
            stats.strip_dirs()
            stats.sort_stats('time', 'calls')
            stats.print_stats(profilelevel)

########NEW FILE########
__FILENAME__ = test_args
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA args helpers"""

import unittest
from mvpa2.misc.args import *

if __debug__:
    from mvpa2.base import debug

class ArgsHelpersTest(unittest.TestCase):

    def test_basic(self):
        """Test if we are not missing basic parts"""
        kwargs = {'a':1, 'slave_a':3, 'slave_z':4, 'slave_slave_z':5, 'c':3}

        res = split_kwargs(kwargs, ['slave_'])
        self.assertTrue(res.has_key('slave_') and res.has_key(''))
        self.assertTrue(res['slave_'] == {'a':3, 'z':4, 'slave_z':5})
        self.assertTrue(res[''] == {'a':1, 'c':3})

        res = split_kwargs(kwargs)
        self.assertTrue(res.keys() == [''])
        self.assertTrue(res[''] == kwargs)


    def test_decorator(self):
        """Test the group_kwargs decorator"""

        selftop = self

        class C1(object):

            @group_kwargs(prefixes=['slave_'], assign=True)
            def __init__(self, **kwargs):
                selftop.assertTrue(hasattr(self, '_slave_kwargs'))
                self.method_passedempty()
                self.method_passed(1, custom_p1=144, bugax=1)
                self.method_filtered(1, custom_p1=123)

            @group_kwargs(prefixes=['custom_'], passthrough=True)
            def method_passedempty(self, **kwargs):
                # we must have it even though it is empty
                selftop.assertTrue('custom_kwargs' in kwargs)

            @group_kwargs(prefixes=['custom_', 'buga'], passthrough=True)
            def method_passed(self, a, custom_kwargs, bugakwargs, **kwargs):
                # we must have it even though it is empty
                selftop.assertTrue(custom_kwargs == {'p1':144})
                selftop.assertTrue(bugakwargs == {'x':1})
                selftop.assertTrue(not hasattr(self, '_custom_kwargs'))

            @group_kwargs(prefixes=['custom_'])
            def method_filtered(self, a, **kwargs):
                # we must have it even though it is empty
                selftop.assertEqual(a, 1)
                selftop.assertTrue(not 'custom_kwargs' in kwargs)

            def method(self):
                return 123

            @group_kwargs(prefixes=['xxx'])
            def method_decorated(self):
                return 124

        c1 = C1(slave_p1=1, p1=2)
        self.assertTrue(c1.method() == 123)
        self.assertTrue(c1.method_decorated() == 124)


def suite():  # pragma: no cover
    return unittest.makeSuite(ArgsHelpersTest)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_arraymapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA dense array mapper"""


from mvpa2.mappers.flatten import mask_mapper
from mvpa2.featsel.base import StaticFeatureSelection

from mvpa2.testing.tools import assert_raises, assert_equal, assert_array_equal

import numpy as np

def test_forward_dense_array_mapper():
    mask = np.ones((3,2), dtype='bool')
    map_ = mask_mapper(mask)

    # test shape reports
    assert_equal(map_.forward1(mask).shape, (6,))

    # test 1sample mapping
    assert_array_equal(map_.forward1(np.arange(6).reshape(3,2)),
                       [0,1,2,3,4,5])

    # test 4sample mapping
    foursample = map_.forward(np.arange(24).reshape(4,3,2))
    assert_array_equal(foursample,
                       [[0,1,2,3,4,5],
                        [6,7,8,9,10,11],
                        [12,13,14,15,16,17],
                        [18,19,20,21,22,23]])

    # check incomplete masks
    mask[1,1] = 0
    map_ = mask_mapper(mask)
    assert_equal(map_.forward1(mask).shape, (5,))
    assert_array_equal(map_.forward1(np.arange(6).reshape(3,2)),
                       [0,1,2,4,5])

    # check that it doesn't accept wrong dataspace
    assert_raises(ValueError, map_.forward, np.arange(4).reshape(2,2))

    # check fail if neither mask nor shape
    assert_raises(ValueError, mask_mapper)

    # check that a full mask is automatically created when providing shape
    m = mask_mapper(shape=(2, 3, 4))
    mp = m.forward1(np.arange(24).reshape(2, 3, 4))
    assert_array_equal(mp, np.arange(24))


def test_reverse_dense_array_mapper():
    mask = np.ones((3,2), dtype='bool')
    mask[1,1] = 0
    map_ = mask_mapper(mask)

    rmapped = map_.reverse1(np.arange(1,6))
    assert_equal(rmapped.shape, (3,2))
    assert_equal(rmapped[1,1], 0)
    assert_equal(rmapped[2,1], 5)


    # check that it doesn't accept wrong dataspace
    assert_raises(ValueError, map_.forward, np.arange(6))

    rmapped2 = map_.reverse(np.arange(1,11).reshape(2,5))
    assert_equal(rmapped2.shape, (2,3,2))
    assert_equal(rmapped2[0,1,1], 0 )
    assert_equal(rmapped2[1,1,1], 0 )
    assert_equal(rmapped2[0,2,1], 5 )
    assert_equal(rmapped2[1,2,1], 10 )


def test_mapper_aliases():
    mm=mask_mapper(np.ones((3,4,2), dtype='bool'))
    assert_array_equal(mm.forward(np.ones((2,3,4,2))),
                       mm.forward(np.ones((2,3,4,2))))


def test_selects():
    mask = np.ones((3,2), dtype='bool')
    mask[1,1] = 0
    mask0 = mask.copy()
    data = np.arange(6).reshape(mask.shape)
    map_ = mask_mapper(mask)

    # check if any exception is thrown if we get
    # out of the outIds
    #assert_raises(IndexError, map_.select_out, [0,1,2,6])

    # remove 1,2
    map_.append(StaticFeatureSelection([0,3,4]))
    assert_array_equal(map_.forward1(data), [0, 4, 5])
    # remove 1 more
    map_.append(StaticFeatureSelection([0,2]))
    assert_array_equal(map_.forward1(data), [0, 5])

    # check if original mask wasn't perturbed
    assert_array_equal(mask, mask0)

    # check if original mask wasn't perturbed
    assert_array_equal(mask, mask0)

########NEW FILE########
__FILENAME__ = test_atlases
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA atlases"""

import unittest, re
import numpy as np

from mvpa2.testing import *

skip_if_no_external('nibabel')
skip_if_no_external('lxml')

from mvpa2.base import externals
from mvpa2.atlases import *

import os
from mvpa2 import pymvpa_dataroot

"""Basic tests for support of atlases such as the ones
shipped with FSL
"""

def test_transformations():
    """TODO"""
    raise SkipTest, "Please test application of transformations"

@sweepargs(name=KNOWN_ATLASES.keys())
def test_atlases(name):
    """Basic testing of atlases"""

    #filename = KNOWN_ATLASES[name] % {'name': name}
    try:
        atlas = Atlas(name=name)
    except IOError, e:
        # so we just don't have it
        raise SkipTest('Skipped atlas %s due to %s' % (name, e))
    #print isinstance(atlas.atlas, objectify.ObjectifiedElement)
    #print atlas.header.images.imagefile.get('offset')
    #print atlas.label_voxel( (0, -7, 20) )
    #print atlas[ 0, 0, 0 ]
    coord = (-63, -12, 22)

    # Atlas must have at least 1 level and that one must
    # have some labels
    ok_(len(atlas.levels[0].labels) > 0)

    for res in [ atlas(coord),
                 atlas.label_point(coord) ]:
        ok_(res.get('coord_queried', None) == coord,
                        '%s: Comparison failed. Got %s and %s'
                        % (name, res.get('coord_queried', None), coord))
        ok_('labels' in res)
        # all atlases so far are based on voxels
        ok_('voxel_queried' in res)

    # test explicit level specification via slice, although bogus here
    # XXX levels in queries should be deprecated -- too much of
    # performance hit
    res0 = atlas(coord, range(atlas.nlevels))
    ok_(res0 == res)

    #print atlas[ 0, -7, 20, [1,2,3] ]
    #print atlas[ (0, -7, 20), 1:2 ]
    #print atlas[ (0, -7, 20) ]
    #print atlas[ (0, -7, 20), : ]
    #   print atlas.get_labels(0)


def test_fsl_hox_queries():
    skip_if_no_external('atlas_fsl')

    tshape = (182, 218, 182)        # target shape of fsl atlas chosen by default
    atl = Atlas(name='HarvardOxford-Cortical')
    atl.levels[0].find('Frontal Pole')
    assert_equal(len(atl.find(re.compile('Fusiform'), unique=False)),
                 4)

    m = atl.get_map(1)
    assert_equal(m.shape, tshape)
    ok_(np.max(m)==100)
    ok_(np.min(m)==0)

    ms = atl.get_maps('Fusiform')
    assert_equal(len(ms), 4)
    for l, m in ms.iteritems():
        assert_equal(m.shape, tshape)

    ms = atl.get_maps('ZaZaZa')
    ok_(not len(ms))

    assert_raises(ValueError, atl.get_map, 'Fusiform')
    ok_(len(atl.find('Fusiform', unique=False)) == 4)
    ff_map = atl.get_map('Fusiform', strategy='max')
    assert_equal(ff_map.shape, tshape)

    # atlas has very unfortunate shape -- the same under .T ... heh heh
    # Lets validate either map is in correct orientation
    ok_(ff_map[119, 91, 52] > 60)
    ok_(ff_map[52, 91, 119] == 0)

    # Lets validate some coordinates queries
    r_gi = atl(-48, -75, 19)
    r_point = atl.label_point((-48, -75, 19))
    r_voxel = atl.label_voxel((138, 51, 91))

    # by default, __getitem__ queries coordinates in voxels
    ok_(r_voxel == atl[(138, 51, 91)] == atl[138, 51, 91])
    # by default -- opens at highest-available resolution,
    # i.e. 1mm since a while
    ok_(atl.resolution == 1.)

    # by default, __call__ queries coordinates in space
    ok_(r_point == atl(-48, -75, 19) == atl((-48, -75, 19)))

    ok_(r_point['labels'] == r_voxel['labels'] ==
         [[{'index': 21, 'prob': 64,
            'label': 'Lateral Occipital Cortex, superior division'},
           {'index': 22, 'prob': 22,
            'label': 'Lateral Occipital Cortex, inferior division'}]])
    ok_(r_point['voxel_atlas'] == r_point['voxel_queried'] ==
        list(r_voxel['voxel_queried']) == [138, 51, 91])
    # TODO: unify list/tuple in above -- r_point has lists

    # Test loading of custom atlas
    # for now just on the original file
    atl2 = Atlas(name='HarvardOxford-Cortical',
                 image_file=atl._image_file)

    # we should get exactly the same maps from both in this dummy case
    ok_((atl.get_map('Frontal Pole') == atl2.get_map('Frontal Pole')).all())


    # Lets falsify and feed some crammy file as the atlas
    atl2 = Atlas(name='HarvardOxford-Cortical',
                 image_file=os.path.join(pymvpa_dataroot, 'example4d.nii.gz'))

    # we should get not even comparable maps now ;)
    ok_(atl.get_map('Frontal Pole').shape != atl2.get_map('Frontal Pole').shape)

    # Lets check unique resolution for the atlas
    maps = atl.get_maps('Fusiform')
    maps_max = atl.get_maps('Fusiform', overlaps='max')

    mk = maps.keys()
    ok_(set(mk) == set(maps_max.keys()))

    maps_ab = np.array([maps[k]!=0 for k in mk])
    maps_max_ab = np.array([maps_max[k]!=0 for k in mk])

    # We should have difference
    ok_(np.any(maps_ab != maps_max_ab))
    maps_max_ab_sum = np.sum(maps_max_ab, axis=0)
    ok_(np.all(0<=maps_max_ab_sum))
    ok_(np.all(maps_max_ab_sum<=1))
    ok_(np.any(np.sum(maps_ab, axis=0)>1))

    # we should still cover the same set of voxels
    assert_array_equal(np.max(maps_ab, axis=0), np.max(maps_max_ab, axis=0))

# Basic testing of Talairach atlases in its original space
def test_pymvpa_talairach():
    skip_if_no_external('atlas_pymvpa')

    atl = Atlas(name='talairach')
    atld = Atlas(name='talairach-dist',
                 reference_level='Closest Gray',
                 distance=10)

    p = [-22, -40, 8]
    pl  = atl.label_point(p)
    pld = atld.label_point(p)

    ok_(np.any(pl['voxel_queried'] != pld['voxel_queried']))
    ok_(pld['distance'] >0)
    # Common labels
    for l in pl, pld:
        assert_equal(l['labels'][0]['label'].text, 'Left Cerebrum')
        assert_equal(l['labels'][1]['label'].text, 'Sub-lobar')

    # different ones
    assert_equal(pl['labels'][3]['label'].text, 'Cerebro-Spinal Fluid')
    assert_equal(pld['labels'][3]['label'].text, 'Gray Matter')

    assert_equal(pl['labels'][4]['label'].text, 'None')
    assert_equal(pld['labels'][4]['label'].text, 'Caudate Tail')

########NEW FILE########
__FILENAME__ = test_attrmap
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##

import numpy as np

from mvpa2.testing.tools import assert_raises, ok_, assert_false, assert_equal, \
     assert_array_equal

from mvpa2.misc.attrmap import AttributeMap


def test_attrmap():
    map_default = {'eins': 0, 'zwei': 2, 'sieben': 1}
    map_custom = {'eins': 11, 'zwei': 22, 'sieben': 33}
    literal = ['eins', 'zwei', 'sieben', 'eins', 'sieben', 'eins']
    literal_nonmatching = ['uno', 'dos', 'tres']
    num_default = [0, 2, 1, 0, 1, 0]
    num_custom = [11, 22, 33, 11, 33, 11]

    # no custom mapping given
    am = AttributeMap()
    assert_false(am)
    ok_(len(am) == 0)
    assert_array_equal(am.to_numeric(literal), num_default)
    assert_array_equal(am.to_literal(num_default), literal)
    ok_(am)
    ok_(len(am) == 3)

    #
    # Tests for recursive mapping + preserving datatype
    class myarray(np.ndarray):
        pass

    assert_raises(KeyError, am.to_literal, [(1, 2), 2, 0])
    literal_fancy = [(1, 2), 2, [0], np.array([0, 1]).view(myarray)]
    literal_fancy_tuple = tuple(literal_fancy)
    literal_fancy_array = np.array(literal_fancy, dtype=object)

    for l in (literal_fancy, literal_fancy_tuple,
              literal_fancy_array):
        res = am.to_literal(l, recurse=True)
        assert_equal(res[0], ('sieben', 'zwei'))
        assert_equal(res[1], 'zwei')
        assert_equal(res[2], ['eins'])
        assert_array_equal(res[3], ['eins', 'sieben'])

        # types of result and subsequences should be preserved
        ok_(isinstance(res, l.__class__))
        ok_(isinstance(res[0], tuple))
        ok_(isinstance(res[1], str))
        ok_(isinstance(res[2], list))
        ok_(isinstance(res[3], myarray))

    # yet another example
    a = np.empty(1, dtype=object)
    a[0] = (0, 1)
    res = am.to_literal(a, recurse=True)
    ok_(isinstance(res[0], tuple))

    #
    # with custom mapping
    am = AttributeMap(map=map_custom)
    assert_array_equal(am.to_numeric(literal), num_custom)
    assert_array_equal(am.to_literal(num_custom), literal)

    # if not numeric nothing is mapped
    assert_array_equal(am.to_numeric(num_custom), num_custom)
    # even if the map doesn't fit
    assert_array_equal(am.to_numeric(num_default), num_default)

    # need to_numeric first
    am = AttributeMap()
    assert_raises(RuntimeError, am.to_literal, [1,2,3])
    # stupid args
    assert_raises(ValueError, AttributeMap, map=num_custom)

    # map mismatch
    am = AttributeMap(map=map_custom)
    if __debug__:
        # checked only in __debug__
        assert_raises(KeyError, am.to_numeric, literal_nonmatching)
    # needs reset and should work afterwards
    am.clear()
    assert_array_equal(am.to_numeric(literal_nonmatching), [2, 0, 1])
    # and now reverse
    am = AttributeMap(map=map_custom)
    assert_raises(KeyError, am.to_literal, num_default)

    # dict-like interface
    am = AttributeMap()

    ok_([(k, v) for k, v in am.iteritems()] == [])


def test_attrmap_conflicts():
    am_n = AttributeMap({'a':1, 'b':2, 'c':1})
    am_t = AttributeMap({'a':1, 'b':2, 'c':1}, collisions_resolution='tuple')
    am_l = AttributeMap({'a':1, 'b':2, 'c':1}, collisions_resolution='lucky')
    q_f = ['a', 'b', 'a', 'c']
    # should have no effect on forward mapping
    ok_(np.all(am_n.to_numeric(q_f) == am_t.to_numeric(q_f)))
    ok_(np.all(am_t.to_numeric(q_f) == am_l.to_numeric(q_f)))

    assert_raises(ValueError, am_n.to_literal, [2])
    r_t = am_t.to_literal([2, 1])
    r_l = am_l.to_literal([2, 1])

def test_attrmap_repr():
    assert_equal(repr(AttributeMap()), "AttributeMap()")
    d = dict(a=2, b=1)
    assert_equal(repr(AttributeMap(d)),
                 "AttributeMap(%r)" % (d,))
    assert_equal(repr(AttributeMap(dict(a=2, b=1), mapnumeric=True)),
                 "AttributeMap(%r, mapnumeric=True)" % (d,))
    assert_equal(repr(AttributeMap(dict(a=2, b=1), mapnumeric=True, collisions_resolution='tuple')),
                 "AttributeMap(%r, mapnumeric=True, collisions_resolution='tuple')" % (d,))

########NEW FILE########
__FILENAME__ = test_base
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Test some base functionality which did not make it into a separate unittests"""

import os
import unittest

from mvpa2.base.info import wtf
from mvpa2.testing.tools import *

@with_tempfile()
def test_wtf(filename):
    """Very basic testing of wtf()"""

    sinfo = str(wtf())
    sinfo_excludes = str(wtf(exclude=['runtime']))
    ok_(len(sinfo) > len(sinfo_excludes),
        msg="Got not less info when excluded runtime."
        " Original one was:\n%s and without process:\n%s"
        % (sinfo, sinfo_excludes))
    ok_(not 'RUNTIME' in sinfo_excludes)

    # check if we could store and load it back
    wtf(filename)
    try:
        sinfo_from_file = '\n'.join(open(filename, 'r').readlines())
    except Exception, e:
        raise AssertionError(
            'Testing of loading from a stored a file has failed: %r'
            % (e,))

########NEW FILE########
__FILENAME__ = test_boxcarmapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Boxcar mapper"""


import numpy as np

from mvpa2.testing.tools import ok_, assert_raises, assert_false, assert_equal, \
        assert_true, assert_array_equal

from mvpa2.mappers.boxcar import BoxcarMapper
from mvpa2.datasets import Dataset
from mvpa2.mappers.flatten import FlattenMapper
from mvpa2.mappers.base import ChainMapper



def test_simpleboxcar():
    data = np.atleast_2d(np.arange(10)).T
    sp = np.arange(10)

    # check if stupid thing don't work
    assert_raises(ValueError, BoxcarMapper, sp, 0)

    # now do an identity transformation
    bcm = BoxcarMapper(sp, 1)
    trans = bcm.forward(data)
    # ,0 is a feature below, so we get explicit 2D out of 1D
    assert_array_equal(trans[:,0], data)

    # now check for illegal boxes
    if __debug__:
        # condition is checked only in __debug__
        assert_raises(ValueError, BoxcarMapper(sp, 2).train, data)

    # now something that should work
    nbox = 9
    boxlength = 2
    sp = np.arange(nbox)
    bcm = BoxcarMapper(sp, boxlength)
    trans = bcm.forward(data)
    # check that is properly upcasts the dimensionality
    assert_equal(trans.shape, (nbox, boxlength) + data.shape[1:])
    # check actual values, squeezing the last dim for simplicity
    assert_array_equal(trans.squeeze(), np.vstack((np.arange(9), np.arange(9)+1)).T)


    # now test for proper data shape
    data = np.ones((10,3,4,2))
    sp = [ 2, 4, 3, 5 ]
    trans = BoxcarMapper(sp, 4).forward(data)
    assert_equal(trans.shape, (4,4,3,4,2))

    # test reverse
    data = np.arange(240).reshape(10, 3, 4, 2)
    sp = [ 2, 4, 3, 5 ]
    boxlength = 2
    m = BoxcarMapper(sp, boxlength)
    m.train(data)
    mp = m.forward(data)
    assert_equal(mp.shape, (4, 2, 3, 4, 2))

    # try full reconstruct
    mr = m.reverse(mp)
    # shape has to match
    assert_equal(mr.shape, (len(sp) * boxlength,) + data.shape[1:])
    # only known samples are part of the results
    assert_true((mr >= 24).all())
    assert_true((mr < 168).all())

    # check proper reconstruction of non-conflicting sample
    assert_array_equal(mr[0].ravel(), np.arange(48, 72))

    # check proper reconstruction of samples being part of multiple
    # mapped samples
    assert_array_equal(mr[1].ravel(), np.arange(72, 96))

    # test reverse of a single sample
    singlesample = np.arange(48).reshape(2, 3, 4, 2)
    assert_array_equal(singlesample, m.reverse1(singlesample))
    # now in a dataset
    ds = Dataset([singlesample])
    assert_equal(ds.shape, (1,) + singlesample.shape)
    # after reverse mapping the 'sample axis' should vanish and the original 3d
    # shape of the samples should be restored
    assert_equal(ds.shape[1:], m.reverse(ds).shape)
    # multiple samples should just be concatenated along the samples axis
    ds = Dataset([singlesample, singlesample])
    assert_equal((np.prod(ds.shape[:2]),) + singlesample.shape[1:],
                 m.reverse(ds).shape)
    # should not work for shape mismatch, but it does work and is useful when
    # reverse mapping sample attributes
    #assert_raises(ValueError, m.reverse, singlesample[0])

    # check broadcasting of 'raw' samples into proper boxcars on forward()
    bc = m.forward1(np.arange(24).reshape(3, 4, 2))
    assert_array_equal(bc, np.array(2 * [np.arange(24).reshape(3, 4, 2)]))


def test_datasetmapping():
    # 6 samples, 4X2 features
    data = np.arange(48).reshape(6,4,2)
    ds = Dataset(data,
                 sa={'timepoints': np.arange(6),
                     'multidim': data.copy()},
                 fa={'fid': np.arange(4)})
    # with overlapping and non-overlapping boxcars
    startpoints = [0, 1, 4]
    boxlength = 2
    bm = BoxcarMapper(startpoints, boxlength, space='boxy')
    # train is critical
    bm.train(ds)
    mds = bm.forward(ds)
    assert_equal(len(mds), len(startpoints))
    assert_equal(mds.nfeatures, boxlength)
    # all samples attributes remain, but the can rotated/compressed into
    # multidimensional attributes
    assert_equal(sorted(mds.sa.keys()), ['boxy_onsetidx'] + sorted(ds.sa.keys()))
    assert_equal(mds.sa.multidim.shape,
            (len(startpoints), boxlength) + ds.shape[1:])
    assert_equal(mds.sa.timepoints.shape, (len(startpoints), boxlength))
    assert_array_equal(mds.sa.timepoints.flatten(),
                       np.array([(s, s+1) for s in startpoints]).flatten())
    assert_array_equal(mds.sa.boxy_onsetidx, startpoints)
    # feature attributes also get rotated and broadcasted
    assert_array_equal(mds.fa.fid, [ds.fa.fid, ds.fa.fid])
    # and finally there is a new one
    assert_array_equal(mds.fa.boxy_offsetidx, range(boxlength))

    # now see how it works on reverse()
    rds = bm.reverse(mds)
    # we got at least something of all original attributes back
    assert_equal(sorted(rds.sa.keys()), sorted(ds.sa.keys()))
    assert_equal(sorted(rds.fa.keys()), sorted(ds.fa.keys()))
    # it is not possible to reconstruct the full samples array
    # some samples even might show up multiple times (when there are overlapping
    # boxcars
    assert_array_equal(rds.samples,
                       np.array([[[ 0,  1], [ 2,  3], [ 4,  5], [ 6,  7]],
                                 [[ 8,  9], [10, 11], [12, 13], [14, 15]],
                                 [[ 8,  9], [10, 11], [12, 13], [14, 15]],
                                 [[16, 17], [18, 19], [20, 21], [22, 23]],
                                 [[32, 33], [34, 35], [36, 37], [38, 39]],
                                 [[40, 41], [42, 43], [44, 45], [46, 47]]]))
    assert_array_equal(rds.sa.timepoints, [0, 1, 1, 2, 4, 5])
    assert_array_equal(rds.sa.multidim, ds.sa.multidim[rds.sa.timepoints])
    # but feature attributes should be fully recovered
    assert_array_equal(rds.fa.fid, ds.fa.fid)

    # popular dataset configuration (double flatten + boxcar)
    cm= ChainMapper([FlattenMapper(), bm, FlattenMapper()])
    cm.train(ds)
    bflat = ds.get_mapped(cm)
    assert_equal(bflat.shape, (len(startpoints), boxlength * np.prod(ds.shape[1:])))
    # add attributes
    bflat.fa['testfa'] = np.arange(bflat.nfeatures)
    bflat.sa['testsa'] = np.arange(bflat.nsamples)
    # now try to go back
    bflatrev = bflat.mapper.reverse(bflat)
    # data should be same again, as far as the boxcars match
    assert_array_equal(ds.samples[:2], bflatrev.samples[:2])
    assert_array_equal(ds.samples[-2:], bflatrev.samples[-2:])
    # feature axis should match
    assert_equal(ds.shape[1:], bflatrev.shape[1:])


########NEW FILE########
__FILENAME__ = test_clf
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA basic Classifiers"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing import _ENFORCE_CA_ENABLED

from mvpa2.testing.datasets import *
from mvpa2.testing.clfs import *

from mvpa2.support.copy import deepcopy
from mvpa2.base.node import ChainNode
from mvpa2.base import externals

from mvpa2.datasets.base import dataset_wizard
from mvpa2.generators.partition import NFoldPartitioner, OddEvenPartitioner
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.generators.resampling import Balancer
from mvpa2.generators.splitters import Splitter

from mvpa2.misc.exceptions import UnknownStateError
from mvpa2.misc.errorfx import mean_mismatch_error

from mvpa2.base.learner import DegenerateInputError, FailedToTrainError, \
        FailedToPredictError
from mvpa2.clfs.meta import CombinedClassifier, \
     BinaryClassifier, MulticlassClassifier, \
     SplitClassifier, MappedClassifier, FeatureSelectionClassifier, \
     TreeClassifier, RegressionAsClassifier, MaximalVote
from mvpa2.measures.base import TransferMeasure, ProxyMeasure, CrossValidation
from mvpa2.mappers.flatten import mask_mapper
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.mappers.fx import mean_sample, BinaryFxNode


# What exceptions to allow while testing degenerate cases.
# If it pukes -- it is ok -- user will notice that something
# is wrong
_degenerate_allowed_exceptions = [
    DegenerateInputError, FailedToTrainError, FailedToPredictError]


class ClassifiersTests(unittest.TestCase):

    def setUp(self):
        self.clf_sign = SameSignClassifier()
        self.clf_less1 = Less1Classifier()

        # simple binary dataset
        self.data_bin_1 = dataset_wizard(
            samples=[[0,0],[-10,-1],[1,0.1],[1,-1],[-1,1]],
            targets=[1, 1, 1, -1, -1], # labels
            chunks=[0, 1, 2,  2, 3])  # chunks

    def _get_clf_ds(self, clf):
        """Little helper to provide a dataset for classifier testing

        For some classifiers (e.g. density modeling ones, such as QDA)
        it is mandatory to provide enough samples to more or less adequately
        model the distributions, thus "large" dataset would be provided
        instead of the default medium.

        Also choosing large one for the classifiers with
        feature-selection since some feature selections might rely on
        a % of features, which would be degenerate in a small dataset
        """
        # unfortunately python 2.5 doesn't have 'isdisjoint'
        #return {True: 'medium',
        #        False: 'large'}[
        #    set(['lda', 'qda', 'feature_selection']).isdisjoint(clf.__tags__)]
        if 'lda' in clf.__tags__ or 'qda' in clf.__tags__ \
                or 'feature_selection' in clf.__tags__:
            return 'large'
        else:
            return 'medium'

    def test_dummy(self):
        clf = SameSignClassifier(enable_ca=['training_stats'])
        clf.train(self.data_bin_1)
        self.assertRaises(UnknownStateError, clf.ca.__getattribute__,
                              "predictions")
        """Should have no predictions after training. Predictions
        state should be explicitely disabled"""

        if not _ENFORCE_CA_ENABLED:
            self.assertRaises(UnknownStateError,
                clf.ca.__getattribute__, "trained_dataset")

        self.assertEqual(clf.ca.training_stats.percent_correct,
                             100,
                             msg="Dummy clf should train perfectly")
        self.assertEqual(clf.predict(self.data_bin_1.samples),
                             list(self.data_bin_1.targets))

        self.assertEqual(len(clf.ca.predictions),
            self.data_bin_1.nsamples,
            msg="Trained classifier stores predictions by default")

        clf = SameSignClassifier(enable_ca=['trained_dataset'])
        clf.train(self.data_bin_1)
        assert_array_equal(clf.ca.trained_dataset.samples,
                           self.data_bin_1.samples)
        assert_array_equal(clf.ca.trained_dataset.targets,
                           self.data_bin_1.targets)


    def test_boosted(self):
        # XXXXXXX
        # silly test if we get the same result with boosted as with a single one
        bclf = CombinedClassifier(clfs=[self.clf_sign.clone(),
                                        self.clf_sign.clone()])

        self.assertEqual(list(bclf.predict(self.data_bin_1.samples)),
                             list(self.data_bin_1.targets),
                             msg="Boosted classifier should work")
        self.assertEqual(bclf.predict(self.data_bin_1.samples),
                             self.clf_sign.predict(self.data_bin_1.samples),
                             msg="Boosted classifier should have the same as regular")


    def test_boosted_state_propagation(self):
        bclf = CombinedClassifier(clfs=[self.clf_sign.clone(),
                                        self.clf_sign.clone()],
                                  enable_ca=['training_stats'])

        # check ca enabling propagation
        self.assertEqual(self.clf_sign.ca.is_enabled('training_stats'),
                             _ENFORCE_CA_ENABLED)
        self.assertEqual(bclf.clfs[0].ca.is_enabled('training_stats'), True)

        bclf2 = CombinedClassifier(clfs=[self.clf_sign.clone(),
                                         self.clf_sign.clone()],
                                  propagate_ca=False,
                                  enable_ca=['training_stats'])

        self.assertEqual(self.clf_sign.ca.is_enabled('training_stats'),
                             _ENFORCE_CA_ENABLED)
        self.assertEqual(bclf2.clfs[0].ca.is_enabled('training_stats'),
                             _ENFORCE_CA_ENABLED)



    def test_binary_decorator(self):
        ds = dataset_wizard(samples=[ [0,0], [0,1], [1,100], [-1,0], [-1,-3], [ 0,-10] ],
                     targets=[ 'sp', 'sp', 'sp', 'dn', 'sn', 'dp'])
        testdata = [ [0,0], [10,10], [-10, -1], [0.1, -0.1], [-0.2, 0.2] ]
        # labels [s]ame/[d]ifferent (sign), and [p]ositive/[n]egative first element

        clf = SameSignClassifier()
        # lets create classifier to descriminate only between same/different,
        # which is a primary task of SameSignClassifier
        bclf1 = BinaryClassifier(clf=clf,
                                 poslabels=['sp', 'sn'],
                                 neglabels=['dp', 'dn'])

        orig_labels = ds.targets[:]
        bclf1.train(ds)

        self.assertTrue(bclf1.predict(testdata) ==
                        [['sp', 'sn'], ['sp', 'sn'], ['sp', 'sn'],
                         ['dp', 'dn'], ['dp', 'dn']])

        self.assertTrue((ds.targets == orig_labels).all(),
                        msg="BinaryClassifier should not alter labels")


    # TODO: XXX finally just make regression/clf separation cleaner
    @sweepargs(clf=clfswh[:])
    def test_classifier_generalization(self, clf):
        """Simple test if classifiers can generalize ok on simple data
        """
        te = CrossValidation(clf, NFoldPartitioner(), postproc=mean_sample())
        # check the default
        #self.assertTrue(te.transerror.errorfx is mean_mismatch_error)

        nclasses = 2 * (1 + int('multiclass' in clf.__tags__))

        ds = datasets['uni%d%s' % (nclasses, self._get_clf_ds(clf))]
        try:
            cve = te(ds).samples.squeeze()
        except Exception, e:
            self.fail("Failed with %s" % e)

        if cfg.getboolean('tests', 'labile', default='yes'):
            if nclasses > 2 and \
                   ((clf.descr is not None and 'on 5%(' in clf.descr)
                    or 'regression_based' in clf.__tags__):
                # skip those since they are barely applicable/testable here
                raise SkipTest("Skip testing of cve on %s" % clf)

            self.assertTrue(cve < 0.25, # TODO: use multinom distribution
                            msg="Got transfer error %g on %s with %d labels"
                            % (cve, ds, len(ds.UT)))


    # yoh: I guess we have skipped meta constructs because they would
    #      need targets attribute specified in both slave and wrapper
    @sweepargs(lrn=clfswh['!meta']+regrswh['!meta'])
    def test_custom_targets(self, lrn):
        """Simple test if a learner could cope with custom sa not targets
        """

        # Since we are comparing performances of two learners, we need
        # to assure that if they depend on some random seed -- they
        # would use the same value.  Currently we have such stochastic
        # behavior in SMLR
        # yoh: we explicitly seed right before calling a CVs below so
        #      this setting of .seed is of no real effect/testing
        if 'seed' in lrn.params:
            from mvpa2 import _random_seed
            lrn = lrn.clone()              # clone the beast
            lrn.params.seed = _random_seed # reuse the same seed
        lrn_ = lrn.clone()
        lrn_.set_space('custom')

        te = CrossValidation(lrn, NFoldPartitioner())
        te_ = CrossValidation(lrn_, NFoldPartitioner())
        nclasses = 2 * (1 + int('multiclass' in lrn.__tags__))
        dsname = ('uni%dsmall' % nclasses,
                  'sin_modulated')[int(lrn.__is_regression__)]
        ds = datasets[dsname]
        ds_ = ds.copy()
        ds_.sa['custom'] = ds_.sa['targets']
        ds_.sa.pop('targets')
        self.assertTrue('targets' in ds.sa,
                        msg="'targets' should remain in original ds")

        try:
            mvpa2.seed()
            cve = te(ds)

            mvpa2.seed()
            cve_ = te_(ds_)
        except Exception, e:
            self.fail("Failed with %r" % e)

        assert_array_almost_equal(cve, cve_)
        "We should have got very similar errors while operating on "
        "'targets' and on 'custom'. Got %r and %r." % (cve, cve_)

        # TODO: sg/libsvm segfaults
        #       GPR  -- non-linear sensitivities
        if ('has_sensitivity' in lrn.__tags__
            and not 'libsvm' in lrn.__tags__
            and not ('gpr' in lrn.__tags__
                     and 'non-linear' in lrn.__tags__)
            ):
            mvpa2.seed()
            s = lrn.get_sensitivity_analyzer()(ds)
            mvpa2.seed()
            s_ = lrn_.get_sensitivity_analyzer()(ds_)

            isreg = lrn.__is_regression__
            # ^ is XOR so we shouldn't get get those sa's in
            # regressions at all
            self.assertTrue(('custom' in s_.sa) ^ isreg)
            self.assertTrue(('targets' in s.sa) ^ isreg)
            self.assertTrue(not 'targets' in s_.sa)
            self.assertTrue(not 'custom' in s.sa)
            if not 'smlr' in lrn.__tags__ or \
               cfg.getboolean('tests', 'labile', default='yes'):
                assert_array_almost_equal(s.samples, s_.samples)


    @sweepargs(clf=clfswh[:] + regrswh[:])
    def test_summary(self, clf):
        """Basic testing of the clf summary
        """
        summary1 = clf.summary()
        self.assertTrue('not yet trained' in summary1)
        # Need 2 different datasets for regressions/classifiers
        dsname = ('uni2small', 'sin_modulated')[int(clf.__is_regression__)]
        clf.train(datasets[dsname])
        summary = clf.summary()
        # It should get bigger ;)
        self.assertTrue(len(summary) > len(summary1))
        self.assertTrue(not 'not yet trained' in summary)


    @sweepargs(clf=clfswh[:] + regrswh[:])
    def test_degenerate_usage(self, clf):
        """Test how clf handles degenerate cases
        """
        # Whenever we have only 1 feature with only 0s in it
        ds1 = datasets['uni2small'][:, [0]]
        # XXX this very line breaks LARS in many other unittests --
        # very interesting effect. but screw it -- for now it will be
        # this way
        ds1.samples[:] = 0.0             # all 0s
        # For regression we need numbers
        if clf.__is_regression__:
            ds1.targets = AttributeMap().to_numeric(ds1.targets)
        #ds2 = datasets['uni2small'][[0], :]
        #ds2.samples[:] = 0.0             # all 0s

        clf.ca.change_temporarily(
            enable_ca=['estimates', 'training_stats'])

        # Good pukes are good ;-)
        # TODO XXX add
        #  - ", ds2):" to test degenerate ds with 1 sample
        #  - ds1 but without 0s -- just 1 feature... feature selections
        #    might lead to 'surprises' due to magic in combiners etc
        for ds in (ds1, ):
            try:
                try:
                    clf.train(ds)                   # should not crash or stall
                except (ValueError), e:
                    self.fail("Failed to train on degenerate data. Error was %r" % e)
                except DegenerateInputError:
                    # so it realized that data is degenerate and puked
                    continue
                # could we still get those?
                _ = clf.summary()
                cm = clf.ca.training_stats
                # If succeeded to train/predict (due to
                # training_stats) without error -- results better be
                # at "chance"
                continue
                if 'ACC' in cm.stats:
                    self.assertEqual(cm.stats['ACC'], 0.5)
                else:
                    self.assertTrue(np.isnan(cm.stats['CCe']))
            except tuple(_degenerate_allowed_exceptions):
                pass
        clf.ca.reset_changed_temporarily()


    # TODO: sg - remove our limitations, meta, lda, qda and skl -- also
    @sweepargs(clf=clfswh['!sg', '!plr', '!meta', '!lda', '!qda', '!glmnet'])
    def test_single_class(self, clf):
        """Test if binary and multiclass can handle single class training/testing
        """
        ds = datasets['uni2small']
        ds = ds[ds.sa.targets == 'L0']  #  only 1 label
        assert(ds.sa['targets'].unique == ['L0'])

        ds_ = list(OddEvenPartitioner().generate(ds))[0]
        # Here is our "nice" 0.6 substitute for TransferError:
        trerr = TransferMeasure(clf, Splitter('train'),
                                postproc=BinaryFxNode(mean_mismatch_error,
                                                      'targets'))
        try:
            err = np.asscalar(trerr(ds_))
        except Exception, e:
            self.fail(str(e))
        self.assertTrue(err == 0.)

    # TODO: validate for regressions as well!!!
    def test_split_classifier(self):
        ds = self.data_bin_1
        clf = SplitClassifier(clf=SameSignClassifier(),
                enable_ca=['stats', 'training_stats',
                               'feature_ids'])
        clf.train(ds)                   # train the beast
        error = clf.ca.stats.error
        tr_error = clf.ca.training_stats.error

        clf2 = clf.clone()
        cv = CrossValidation(clf2, NFoldPartitioner(), postproc=mean_sample(),
            enable_ca=['stats', 'training_stats'])
        cverror = cv(ds)
        cverror = cverror.samples.squeeze()
        tr_cverror = cv.ca.training_stats.error

        self.assertEqual(error, cverror,
                msg="We should get the same error using split classifier as"
                    " using CrossValidation. Got %s and %s"
                    % (error, cverror))

        self.assertEqual(tr_error, tr_cverror,
                msg="We should get the same training error using split classifier as"
                    " using CrossValidation. Got %s and %s"
                    % (tr_error, tr_cverror))

        self.assertEqual(clf.ca.stats.percent_correct,
                             100,
                             msg="Dummy clf should train perfectly")
        # CV and SplitClassifier should get the same confusion matrices
        assert_array_equal(clf.ca.stats.matrix,
                           cv.ca.stats.matrix)

        self.assertEqual(len(clf.ca.stats.sets),
                             len(ds.UC),
                             msg="Should have 1 confusion per each split")
        self.assertEqual(len(clf.clfs), len(ds.UC),
                             msg="Should have number of classifiers equal # of epochs")
        self.assertEqual(clf.predict(ds.samples), list(ds.targets),
                             msg="Should classify correctly")

        # feature_ids must be list of lists, and since it is not
        # feature-selecting classifier used - we expect all features
        # to be utilized
        #  NOT ANYMORE -- for BoostedClassifier we have now union of all
        #  used features across slave classifiers. That makes
        #  semantics clear. If you need to get deeper -- use upcoming
        #  harvesting facility ;-)
        # self.assertEqual(len(clf.feature_ids), len(ds.uniquechunks))
        # self.assertTrue(np.array([len(ids)==ds.nfeatures
        #                         for ids in clf.feature_ids]).all())

        # Just check if we get it at all ;-)
        summary = clf.summary()


    @sweepargs(clf_=clfswh['binary', '!meta'])
    def test_split_classifier_extended(self, clf_):
        clf2 = clf_.clone()
        ds = datasets['uni2%s' % self._get_clf_ds(clf2)]
        clf = SplitClassifier(clf=clf_, #SameSignClassifier(),
                enable_ca=['stats', 'feature_ids'])
        clf.train(ds)                   # train the beast
        error = clf.ca.stats.error

        cv = CrossValidation(clf2, NFoldPartitioner(), postproc=mean_sample(),
            enable_ca=['stats', 'training_stats'])
        cverror = cv(ds).samples.squeeze()

        if not 'non-deterministic' in clf.__tags__:
            self.assertTrue(abs(error-cverror)<0.01,
                    msg="We should get the same error using split classifier as"
                        " using CrossValidation. Got %s and %s"
                        % (error, cverror))

        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(error < 0.25,
                msg="clf should generalize more or less fine. "
                    "Got error %s" % error)
        self.assertEqual(len(clf.ca.stats.sets), len(ds.UC),
            msg="Should have 1 confusion per each split")
        self.assertEqual(len(clf.clfs), len(ds.UC),
            msg="Should have number of classifiers equal # of epochs")
        #self.assertEqual(clf.predict(ds.samples), list(ds.targets),
        #                     msg="Should classify correctly")

    def test_split_clf_on_chainpartitioner(self):
        # pretty much a smoke test for #156
        ds = datasets['uni2small']
        part = ChainNode([NFoldPartitioner(cvtype=1),
                          Balancer(attr='targets', count=2,
                                   limit='partitions', apply_selection=True)])
        partitions = list(part.generate(ds))
        sclf = SplitClassifier(sample_clf_lin, part, enable_ca=['stats', 'splits'])
        sclf.train(ds)
        pred = sclf.predict(ds)
        assert_equal(len(pred), len(ds))  # rudimentary check
        assert_equal(len(sclf.ca.splits), len(partitions))
        assert_equal(len(sclf.clfs), len(partitions))

        # now let's do sensitivity analyzer just in case
        sclf.untrain()
        sensana = sclf.get_sensitivity_analyzer()
        sens = sensana(ds)
        # basic check that sensitivities varied across splits
        from mvpa2.mappers.fx import FxMapper
        sens_stds = FxMapper('samples', np.std, uattrs=['targets'])(sens)
        assert_true(np.any(sens_stds != 0))

    def test_mapped_classifier(self):
        samples = np.array([ [ 0,  0, -1], [ 1, 0, 1],
                            [-1, -1,  1], [-1, 0, 1],
                            [ 1, -1,  1] ])
        for mask, res in (([1, 1, 0], [ 1, 1,  1, -1, -1]),
                          ([1, 0, 1], [-1, 1, -1, -1,  1]),
                          ([0, 1, 1], [-1, 1, -1,  1, -1])):
            clf = MappedClassifier(clf=self.clf_sign,
                                   mapper=mask_mapper(np.array(mask,
                                                              dtype=bool)))
            self.assertEqual(clf.predict(samples), res)


    def test_feature_selection_classifier(self):
        from mvpa2.featsel.base import \
             SensitivityBasedFeatureSelection
        from mvpa2.featsel.helpers import \
             FixedNElementTailSelector

        # should give lowest weight to the feature with lowest index
        sens_ana = SillySensitivityAnalyzer()
        # should give lowest weight to the feature with highest index
        sens_ana_rev = SillySensitivityAnalyzer(mult=-1)

        # corresponding feature selections
        feat_sel = SensitivityBasedFeatureSelection(sens_ana,
            FixedNElementTailSelector(1, mode='discard'))

        feat_sel_rev = SensitivityBasedFeatureSelection(sens_ana_rev,
            FixedNElementTailSelector(1))

        samples = np.array([ [0, 0, -1], [1, 0, 1], [-1, -1, 1],
                            [-1, 0, 1], [1, -1, 1] ])

        testdata3 = dataset_wizard(samples=samples, targets=1)
        # dummy train data so proper mapper gets created
        traindata = dataset_wizard(samples=np.array([ [0, 0, -1], [1, 0, 1] ]),
                            targets=[1, 2])

        # targets
        res110 = [1, 1, 1, -1, -1]
        res011 = [-1, 1, -1, 1, -1]

        # first classifier -- 0th feature should be discarded
        clf011 = FeatureSelectionClassifier(self.clf_sign, feat_sel,
                    enable_ca=['feature_ids'])

        self.clf_sign.ca.change_temporarily(enable_ca=['estimates'])
        clf011.train(traindata)

        self.assertEqual(clf011.predict(testdata3.samples), res011)
        # just silly test if we get values assigned in the 'ProxyClassifier'
        self.assertTrue(len(clf011.ca.estimates) == len(res110),
                        msg="We need to pass values into ProxyClassifier")
        self.clf_sign.ca.reset_changed_temporarily()

        self.assertEqual(clf011.mapper._oshape, (2,))
        "Feature selection classifier had to be trained on 2 features"

        # first classifier -- last feature should be discarded
        clf011 = FeatureSelectionClassifier(self.clf_sign, feat_sel_rev)
        clf011.train(traindata)
        self.assertEqual(clf011.predict(testdata3.samples), res110)

    def test_feature_selection_classifier_with_regression(self):
        from mvpa2.featsel.base import \
             SensitivityBasedFeatureSelection
        from mvpa2.featsel.helpers import \
             FixedNElementTailSelector
        if sample_clf_reg is None:
            # none regression was found, so nothing to test
            return
        # should give lowest weight to the feature with lowest index
        sens_ana = SillySensitivityAnalyzer()

        # corresponding feature selections
        feat_sel = SensitivityBasedFeatureSelection(sens_ana,
            FixedNElementTailSelector(1, mode='discard'))

        # now test with regression-based classifier. The problem is
        # that it is determining predictions twice from values and
        # then setting the values from the results, which the second
        # time is set to predictions.  The final outcome is that the
        # values are actually predictions...
        dat = dataset_wizard(samples=np.random.randn(4, 10),
                      targets=[-1, -1, 1, 1])
        clf_reg = FeatureSelectionClassifier(sample_clf_reg, feat_sel)
        clf_reg.train(dat)
        _ = clf_reg.predict(dat.samples)
        self.failIf((np.array(clf_reg.ca.estimates)
                     - clf_reg.ca.predictions).sum()==0,
                    msg="Values were set to the predictions in %s." %
                    sample_clf_reg)


    @reseed_rng()
    def test_tree_classifier(self):
        """Basic tests for TreeClassifier
        """
        ds = datasets['uni4medium']
        # make it simple of the beast -- take only informative ones
        # because classifiers for the tree are selected randomly, so
        # performance varies a lot and we just need to check on
        # correct operation
        ds = ds[:, ds.fa.nonbogus_targets != [None]]

        clfs = clfswh['binary']         # pool of classifiers
        # Lets permute so each time we try some different combination
        # of the classifiers but exclude those operating on %s of
        # features since we might not have enough for that
        clfs = [clfs[i] for i in np.random.permutation(len(clfs))
                if not '%' in str(clfs[i])]

        # NB: It is necessary that the same classifier was not used at
        # different nodes, since it would be re-trained for a new set
        # of targets, thus leading to incorrect behavior/high error.
        #
        # Clone only those few leading ones which we will use
        # throughout the test
        clfs = [clf.clone() for clf in clfs[:4]]

        # Test conflicting definition
        tclf = TreeClassifier(clfs[0], {
            'L0+2' : (('L0', 'L2'), clfs[1]),
            'L2+3' : (('L2', 'L3'), clfs[2])})
        self.assertRaises(ValueError, tclf.train, ds)
        """Should raise exception since label 2 is in both"""

        # Test insufficient definition
        tclf = TreeClassifier(clfs[0], {
            'L0+5' : (('L0', 'L5'), clfs[1]),
            'L2+3' : (('L2', 'L3'),       clfs[2])})
        self.assertRaises(ValueError, tclf.train, ds)
        """Should raise exception since no group for L1"""

        # proper definition now
        tclf = TreeClassifier(clfs[0], {
            'L0+1' : (('L0', 'L1'), clfs[1]),
            'L2+3' : (('L2', 'L3'), clfs[2])})

        # Lets test train/test cycle using CVTE
        cv = CrossValidation(tclf, OddEvenPartitioner(), postproc=mean_sample(),
            enable_ca=['stats', 'training_stats'])
        cverror = cv(ds).samples.squeeze()
        try:
            rtclf = repr(tclf)
        except:
            self.fail(msg="Could not obtain repr for TreeClassifier")

        # Test accessibility of .clfs
        self.assertTrue(tclf.clfs['L0+1'] is clfs[1])
        self.assertTrue(tclf.clfs['L2+3'] is clfs[2])

        cvtrc = cv.ca.training_stats
        cvtc = cv.ca.stats

        if cfg.getboolean('tests', 'labile', default='yes'):
            # just a dummy check to make sure everything is working
            self.assertTrue(cvtrc != cvtc)
            self.assertTrue(cverror < 0.3,
                            msg="Got too high error = %s using %s"
                            % (cverror, tclf))

        # Test trailing nodes with no classifier

        # That is why we use separate pool of classifiers here
        # (that is probably old/not-needed since switched to use clones)
        clfs_mc = clfswh['multiclass']         # pool of classifiers
        clfs_mc = [clfs_mc[i] for i in np.random.permutation(len(clfs_mc))
                   if not '%' in str(clfs_mc[i])]
        clfs_mc = [clf.clone() for clf in clfs_mc[:4]] # and clones again

        tclf = TreeClassifier(clfs_mc[0], {
            'L0' : (('L0',), None),
            'L1+2+3' : (('L1', 'L2', 'L3'), clfs_mc[1])})

        cv = CrossValidation(tclf,
                             OddEvenPartitioner(),
                             postproc=mean_sample(),
                             enable_ca=['stats', 'training_stats'])
        cverror = np.asscalar(cv(ds))
        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(cverror < 0.3,
                            msg="Got too high error = %s using %s"
                            % (cverror, tclf))


    @sweepargs(clf=clfswh[:])
    def test_values(self, clf):
        if isinstance(clf, MulticlassClassifier):
            # TODO: handle those values correctly
            return
        ds = datasets['uni2small']
        clf.ca.change_temporarily(enable_ca = ['estimates'])
        cv = CrossValidation(clf, OddEvenPartitioner(),
            enable_ca=['stats', 'training_stats'])
        _ = cv(ds)
        #print clf.descr, clf.values[0]
        # basic test either we get 1 set of values per each sample
        self.assertEqual(len(clf.ca.estimates), ds.nsamples/2)

        clf.ca.reset_changed_temporarily()

    # TODO: PLR expects [0,1], not [-1, 1] and apparently we do not
    #       do remapping
    #@sweepargs(clf=clfswh['!plr', 'binary'])
    # For now just test on a representative SVM
    @sweepargs(clf=clfswh['linear', 'svm', 'libsvm', '!meta'])
    def test_multiclass_classifier(self, clf):
        # Force non-dataspecific C value.
        # Otherwise multiclass libsvm builtin and our MultiClass would differ
        # in results
        svm = clf.clone()                 # operate on clone to avoid side-effects
        if svm.params.has_key('C') and svm.params.C<0:
            svm.params.C = 1.0                 # reset C to be 1
        svm2 = svm.clone()
        svm2.ca.enable(['training_stats'])

        mclf = MulticlassClassifier(clf=svm, enable_ca=['training_stats'])

        # with explicit MaximalVote with the conditional attributes
        # enabled
        mclf_mv = MulticlassClassifier(clf=svm,
                                       combiner=MaximalVote(enable_ca=['estimates', 'predictions']),
                                       enable_ca=['training_stats'])

        ds_train = datasets['uni2small']
        for clf_ in svm2, mclf, mclf_mv:
            clf_.train(ds_train)
        s1 = str(mclf.ca.training_stats)
        s2 = str(svm2.ca.training_stats)
        s3 = str(mclf_mv.ca.training_stats)
        self.assertEqual(s1, s2,
            msg="Multiclass clf should provide same results as built-in "
                "libsvm's %s. Got %s and %s" % (svm2, s1, s2))
        self.assertEqual(s1, s3,
            msg="%s should have used maxvote resolver by default"
                "so results should have been identical. Got %s and %s"
                % (mclf, s1, s3))

        assert_equal(len(mclf_mv.combiner.ca.estimates),
                     len(mclf_mv.combiner.ca.predictions))

        # They should have came from assessing training_stats ca being
        # enabled
        # recompute accuracy on predictions for training_stats
        training_acc = np.sum(mclf_mv.combiner.ca.predictions ==
                              ds_train.targets) / float(len(ds_train))
        # should match
        assert_equal(mclf_mv.ca.training_stats.stats['ACC'], training_acc)

        svm2.untrain()

        self.assertTrue(svm2.trained == False,
            msg="Un-Trained SVM should be untrained")

        self.assertTrue(np.array([x.trained for x in mclf.clfs]).all(),
            msg="Trained Boosted classifier should have all primary classifiers trained")
        self.assertTrue(mclf.trained,
            msg="Trained Boosted classifier should be marked as trained")

        mclf.untrain()

        self.assertTrue(not mclf.trained,
                        msg="UnTrained Boosted classifier should not be trained")
        self.assertTrue(not np.array([x.trained for x in mclf.clfs]).any(),
            msg="UnTrained Boosted classifier should have no primary classifiers trained")

        # TODO: test combiners, e.g. MaximalVote and ca they store


    # XXX meta should also work but TODO
    @sweepargs(clf=clfswh['svm', '!meta'])
    def test_svms(self, clf):
        knows_probabilities = \
            'probabilities' in clf.ca.keys() and clf.params.probability
        enable_ca = ['estimates']
        if knows_probabilities:
            enable_ca += ['probabilities']

        clf.ca.change_temporarily(enable_ca = enable_ca)
        spl = Splitter('train', count=2)
        traindata, testdata = list(spl.generate(datasets['uni2small']))
        clf.train(traindata)
        predicts = clf.predict(testdata.samples)
        # values should be different from predictions for SVMs we have
        self.assertTrue(np.any(predicts != clf.ca.estimates))

        if knows_probabilities and clf.ca.is_set('probabilities'):
            # XXX test more thoroughly what we are getting here ;-)
            self.assertEqual( len(clf.ca.probabilities),
                                  len(testdata.samples)  )
        clf.ca.reset_changed_temporarily()


    @sweepargs(clf=clfswh['retrainable'])
    @reseed_rng()
    def test_retrainables(self, clf):
        # XXX we agreed to not worry about this for the initial 0.6 release
        raise SkipTest
        # we need a copy since will tune its internals later on
        clf = clf.clone()
        clf.ca.change_temporarily(enable_ca = ['estimates'],
                                      # ensure that it does do predictions
                                      # while training
                                      disable_ca=['training_stats'])
        clf_re = clf.clone()
        # TODO: .retrainable must have a callback to call smth like
        # _set_retrainable
        clf_re._set_retrainable(True)

        # need to have high snr so we don't 'cope' with problematic
        # datasets since otherwise unittests would fail.
        dsargs = {'perlabel':50, 'nlabels':2, 'nfeatures':5, 'nchunks':1,
                  'nonbogus_features':[2,4], 'snr': 5.0}

        ## !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        # NB datasets will be changed by the end of testing, so if
        # are to change to use generic datasets - make sure to copy
        # them here
        ds = deepcopy(datasets['uni2large'])
        clf.untrain()
        clf_re.untrain()
        trerr = TransferMeasure(clf, Splitter('train'),
                                postproc=BinaryFxNode(mean_mismatch_error,
                                                      'targets'))
        trerr_re =  TransferMeasure(clf_re, Splitter('train'),
                                    disable_ca=['training_stats'],
                                    postproc=BinaryFxNode(mean_mismatch_error,
                                                          'targets'))

        # Just check for correctness of retraining
        err_1 = np.asscalar(trerr(ds))
        self.assertTrue(err_1<0.3,
            msg="We should test here on easy dataset. Got error of %s" % err_1)
        values_1 = clf.ca.estimates[:]
        # some times retraining gets into deeper optimization ;-)
        eps = 0.05
        corrcoef_eps = 0.85        # just to get no failures... usually > 0.95


        def batch_test(retrain=True, retest=True, closer=True):
            err = np.asscalar(trerr(ds))
            err_re = np.asscalar(trerr_re(ds))
            corr = np.corrcoef(
                clf.ca.estimates, clf_re.ca.estimates)[0, 1]
            corr_old = np.corrcoef(values_1, clf_re.ca.estimates)[0, 1]
            if __debug__:
                debug('TEST', "Retraining stats: errors %g %g corr %g "
                      "with old error %g corr %g" %
                  (err, err_re, corr, err_1, corr_old))
            self.assertTrue(clf_re.ca.retrained == retrain,
                            ("Must fully train",
                             "Must retrain instead of full training")[retrain])
            self.assertTrue(clf_re.ca.repredicted == retest,
                            ("Must fully test",
                             "Must retest instead of full testing")[retest])
            self.assertTrue(corr > corrcoef_eps,
              msg="Result must be close to the one without retraining."
                  " Got corrcoef=%s" % (corr))
            if closer:
                self.assertTrue(
                    corr >= corr_old,
                    msg="Result must be closer to current without retraining"
                    " than to old one. Got corrcoef=%s" % (corr_old))

        # Check sequential retraining/retesting
        for i in xrange(3):
            flag = bool(i!=0)
            # ok - on 1st call we should train/test, then retrain/retest
            # and we can't compare for closinest to old result since
            # we are working on the same data/classifier
            batch_test(retrain=flag, retest=flag, closer=False)

        # should retrain nicely if we change a parameter
        if 'C' in clf.params:
            clf.params.C *= 0.1
            clf_re.params.C *= 0.1
            batch_test()
        elif 'sigma_noise' in clf.params:
            clf.params.sigma_noise *= 100
            clf_re.params.sigma_noise *= 100
            batch_test()
        else:
            raise RuntimeError, \
                  'Please implement testing while changing some of the ' \
                  'params for clf %s' % clf

        # should retrain nicely if we change kernel parameter
        if hasattr(clf, 'kernel_params') and len(clf.kernel_params):
            clf.kernel_params.gamma = 0.1
            clf_re.kernel_params.gamma = 0.1
            # retest is false since kernel got recomputed thus
            # can't expect to use the same kernel
            batch_test(retest=not('gamma' in clf.kernel_params))

        # should retrain nicely if we change labels
        permute = AttributePermutator('targets', assure=True)
        oldlabels = dstrain.targets[:]
        dstrain = permute(dstrain)
        self.assertTrue((oldlabels != dstrain.targets).any(),
            msg="We should succeed at permutting -- now got the same targets")
        ds = vstack((dstrain, dstest))
        batch_test()

        # Change labels in testing
        oldlabels = dstest.targets[:]
        dstest = permute(dstest)
        self.assertTrue((oldlabels != dstest.targets).any(),
            msg="We should succeed at permutting -- now got the same targets")
        ds = vstack((dstrain, dstest))
        batch_test()

        # should re-train if we change data
        # reuse trained SVM and its 'final' optimization point
        if not clf.__class__.__name__ in ['GPR']: # on GPR everything depends on the data ;-)
            oldsamples = dstrain.samples.copy()
            dstrain.samples[:] += dstrain.samples*0.05
            self.assertTrue((oldsamples != dstrain.samples).any())
            ds = vstack((dstrain, dstest))
            batch_test(retest=False)
        clf.ca.reset_changed_temporarily()

        # test retrain()
        # TODO XXX  -- check validity
        clf_re.retrain(dstrain);
        self.assertTrue(clf_re.ca.retrained)
        clf_re.retrain(dstrain, labels=True);
        self.assertTrue(clf_re.ca.retrained)
        clf_re.retrain(dstrain, traindataset=True);
        self.assertTrue(clf_re.ca.retrained)

        # test repredict()
        clf_re.repredict(dstest.samples);
        self.assertTrue(clf_re.ca.repredicted)
        self.assertRaises(RuntimeError, clf_re.repredict,
                              dstest.samples, labels=True)
        """for now retesting with anything changed makes no sense"""
        clf_re._set_retrainable(False)


    def test_generic_tests(self):
        """Test all classifiers for conformant behavior
        """
        for clf_, traindata in \
                [(clfswh['binary'], datasets['dumb2']),
                 (clfswh['multiclass'], datasets['dumb'])]:
            traindata_copy = deepcopy(traindata) # full copy of dataset
            for clf in clf_:
                clf.train(traindata)
                self.assertTrue(
                   (traindata.samples == traindata_copy.samples).all(),
                   "Training of a classifier shouldn't change original dataset")

            # TODO: enforce uniform return from predict??
            #predicted = clf.predict(traindata.samples)
            #self.assertTrue(isinstance(predicted, np.ndarray))

        # Just simple test that all of them are syntaxed correctly
        self.assertTrue(str(clf) != "")
        self.assertTrue(repr(clf) != "")

        # TODO: unify str and repr for all classifiers

    # XXX TODO: should work on smlr, knn, ridgereg, lars as well! but now
    #     they fail to train
    #    svmocas -- segfaults -- reported to mailing list
    #    GNB, LDA, QDA -- cannot train since 1 sample isn't sufficient
    #    to assess variance
    @sweepargs(clf=clfswh['!random', '!smlr', '!knn', '!gnb', '!lda', '!qda', '!lars', '!meta', '!ridge', '!needs_population'])
    def test_correct_dimensions_order(self, clf):
        """To check if known/present Classifiers are working properly
        with samples being first dimension. Started to worry about
        possible problems while looking at sg where samples are 2nd
        dimension
        """
        # specially crafted dataset -- if dimensions are flipped over
        # the same storage, problem becomes unseparable. Like in this case
        # incorrect order of dimensions lead to equal samples [0, 1, 0]
        traindatas = [
            dataset_wizard(samples=np.array([ [0, 0, 1.0],
                                        [1, 0, 0] ]), targets=[0, 1]),
            dataset_wizard(samples=np.array([ [0, 0.0],
                                      [1, 1] ]), targets=[0, 1])]

        clf.ca.change_temporarily(enable_ca = ['training_stats'])
        for traindata in traindatas:
            clf.train(traindata)
            self.assertEqual(clf.ca.training_stats.percent_correct, 100.0,
                "Classifier %s must have 100%% correct learning on %s. Has %f" %
                (`clf`, traindata.samples, clf.ca.training_stats.percent_correct))

            # and we must be able to predict every original sample thus
            for i in xrange(traindata.nsamples):
                sample = traindata.samples[i,:]
                predicted = clf.predict([sample])
                self.assertEqual([predicted], traindata.targets[i],
                    "We must be able to predict sample %s using " % sample +
                    "classifier %s" % `clf`)
        clf.ca.reset_changed_temporarily()


    @sweepargs(regr=regrswh[:])
    def test_regression_as_classifier(self, regr):
        """Basic tests of metaclass for using regressions as classifiers
        """
        for dsname in 'uni2small', 'uni4small':
            ds = datasets[dsname]

            clf = RegressionAsClassifier(regr, enable_ca=['distances'])
            cv = CrossValidation(clf, OddEvenPartitioner(),
                    postproc=mean_sample(),
                    enable_ca=['stats', 'training_stats'])

            error = cv(ds).samples.squeeze()

            nlabels = len(ds.uniquetargets)
            if nlabels == 2 \
               and cfg.getboolean('tests', 'labile', default='yes'):
                self.assertTrue(error < 0.3,
                                msg="Got error %.2f on %s dataset"
                                % (error, dsname))

            # Check if does not puke on repr and str
            self.assertTrue(str(clf) != "")
            self.assertTrue(repr(clf) != "")

            self.assertEqual(clf.ca.distances.shape,
                                 (ds.nsamples / 2, nlabels))

            #print "Using %s " % regr, error
            # Just validate that everything is ok
            #self.assertTrue(str(cv.ca.stats) != "")

    @reseed_rng()
    def test_gideon_weird_case(self):
        """Test if MappedClassifier could handle a mapper altering number of samples

        'The utter collapse' -- communicated by Peter J. Kohler

        Desire to collapse all samples per each category in training
        and testing sets, thus resulting only in a single
        sample/category per training and per testing.

        It is a peculiar scenario which pin points the problem that so
        far mappers assumed not to change number of samples
        """
        from mvpa2.mappers.fx import mean_group_sample
        from mvpa2.clfs.knn import kNN
        from mvpa2.mappers.base import ChainMapper
        ds = datasets['uni2large'].copy()
        #ds = ds[ds.sa.chunks < 9]
        accs = []
        k = 1                           # for kNN
        nf = 1                          # for NFoldPartitioner
        for i in xrange(1):          # # of random runs
            ds.samples = np.random.randn(*ds.shape)
            #
            # There are 3 ways to accomplish needed goal
            #

            # 0. Hard way: overcome the problem by manually
            #    pre-splitting/meaning in a loop
            from mvpa2.clfs.transerror import ConfusionMatrix
            partitioner = NFoldPartitioner(nf)
            meaner = mean_group_sample(['targets', 'partitions'])
            cm = ConfusionMatrix()
            te = TransferMeasure(kNN(k), Splitter('partitions'),
                                 postproc=BinaryFxNode(mean_mismatch_error,
                                                       'targets'),
                                 enable_ca = ['stats']
                                 )
            errors = []
            for part in partitioner.generate(ds):
                ds_meaned = meaner(part)
                errors.append(np.asscalar(te(ds_meaned)))
                cm += te.ca.stats
            #print i, cm.stats['ACC']
            accs.append(cm.stats['ACC'])


            if False: # not yet working -- see _tent/allow_ch_nsamples
                      # branch for attempt to make it work
                # 1. This is a "native way" IF we allow change of number
                #    of samples via _call to be done by MappedClassifier
                #    while operating solely on the mapped dataset
                clf2 = MappedClassifier(clf=kNN(k), #clf,
                                        mapper=mean_group_sample(['targets', 'partitions']))
                cv = CrossValidation(clf2, NFoldPartitioner(nf), postproc=None,
                                     enable_ca=['stats'])
                # meaning all should be ok since we should have ballanced
                # sets across all chunks here
                errors_native = cv(ds)

                self.assertEqual(np.max(np.abs(errors_native.samples[:,0] - errors)),
                                 0)

            # 2. Work without fixes to MappedClassifier allowing
            #    change of # of samples
            #
            # CrossValidation will operate on a chain mapper which
            # would perform necessary meaning first before dealing with
            # kNN cons: .stats would not be exposed since ChainMapper
            # doesn't expose them from ChainMapper (yet)
            if __debug__ and 'ENFORCE_CA_ENABLED' in debug.active:
                raise SkipTest("Known to fail while trying to enable "
                               "training_stats for the ChainMapper")
            cv2 = CrossValidation(ChainMapper([mean_group_sample(['targets', 'partitions']),
                                               kNN(k)],
                                              space='targets'),
                                  NFoldPartitioner(nf),
                                  postproc=None)
            errors_native2 = cv2(ds)

            self.assertEqual(np.max(np.abs(errors_native2.samples[:,0] - errors)),
                             0)

            # All of the ways should provide the same results
            #print i, np.max(np.abs(errors_native.samples[:,0] - errors)), \
            #      np.max(np.abs(errors_native2.samples[:,0] - errors))

        if False: # just to investigate the distribution if we have enough iterations
            import pylab as pl
            uaccs = np.unique(accs)
            step = np.asscalar(np.unique(np.round(uaccs[1:] - uaccs[:-1], 4)))
            bins = np.linspace(0., 1., np.round(1./step+1))
            xx = pl.hist(accs, bins=bins, align='left')
            pl.xlim((0. - step/2, 1.+step/2))


def suite():  # pragma: no cover
    return unittest.makeSuite(ClassifiersTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_clfcrossval
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA classifier cross-validation"""

from mvpa2.testing.tools import assert_equal, ok_, assert_array_equal

from mvpa2.base.node import ChainNode
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.measures.base import CrossValidation

from mvpa2.testing import *
from mvpa2.testing.datasets import pure_multivariate_signal, get_mv_pattern
from mvpa2.testing.clfs import *

class CrossValidationTests(unittest.TestCase):


    def test_simple_n_minus_one_cv(self):
        data = get_mv_pattern(3)
        data.init_origids('samples')

        self.assertTrue( data.nsamples == 120 )
        self.assertTrue( data.nfeatures == 2 )
        self.assertTrue(
            (data.sa.targets == \
                [0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0] * 6).all())
        self.assertTrue(
            (data.sa.chunks == \
                [k for k in range(1, 7) for i in range(20)]).all())
        assert_equal(len(np.unique(data.sa.origids)), data.nsamples)

        cv = CrossValidation(sample_clf_nl, NFoldPartitioner(),
                enable_ca=['stats', 'training_stats'])
#                               'samples_error'])

        results = cv(data)
        self.assertTrue((results.samples < 0.2).all() and (results.samples >= 0.0).all())

        # TODO: test accessibility of {training_,}stats{,s} of
        # CrossValidatedTransferError

        # not yet implemented, and no longer this way
        #self.assertTrue(isinstance(cv.ca.samples_error, dict))
        #self.assertTrue(len(cv.ca.samples_error) == data.nsamples)
        ## one value for each origid
        #assert_array_equal(sorted(cv.ca.samples_error.keys()),
        #                   sorted(data.sa.origids))
        #for k, v in cv.ca.samples_error.iteritems():
        #    self.assertTrue(len(v) == 1)


    def test_noise_classification(self):
        # get a dataset with a very high SNR
        data = get_mv_pattern(10)

        # do crossval with default errorfx and 'mean' combiner
        cv = CrossValidation(sample_clf_nl, NFoldPartitioner())

        # must return a scalar value
        result = cv(data)
        # must be perfect
        self.assertTrue((result.samples < 0.05).all())

        # do crossval with permuted regressors
        cv = CrossValidation(sample_clf_nl,
                        ChainNode([NFoldPartitioner(),
                            AttributePermutator('targets', count=10)],
                                  space='partitions'))
        results = cv(data)

        # results must not be the same
        self.assertTrue(len(np.unique(results.samples))>1)

        # must be at chance level
        pmean = np.array(results).mean()
        self.assertTrue( pmean < 0.58 and pmean > 0.42 )


    def test_unpartitioned_cv(self):
        data = get_mv_pattern(10)
        # only one big chunk
        data.sa.chunks[:] = 1
        cv = CrossValidation(sample_clf_nl, NFoldPartitioner())
        # need to fail, because it can't be split into training and testing
        assert_raises(ValueError, cv, data)


def suite():  # pragma: no cover
    return unittest.makeSuite(CrossValidationTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_cmdline
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA cmdline helpers"""

import unittest
from mvpa2.testing import *

from mvpa2.misc.cmdline import *

if __debug__:
    from mvpa2.base import debug

class CmdlineHelpersTest(unittest.TestCase):

    def test_basic(self):
        """Test if we are not missing basic parts"""
        globals_ = globals()
        for member in  [#'_verbose_callback',
                        'parser', 'opt', 'opts']:
            self.assertTrue(globals_.has_key(member),
                msg="We must have imported %s from mvpa2.misc.cmdline!" % member)

    @sweepargs(example=[
        ('targets:rest', None, [('targets', ['rest'])]),
        ('targets:rest;trial:bad,crap,shit', None,
         [('targets', ['rest']), ('trial', ['bad', 'crap', 'shit'])]),
        ])
    def test_split_comma_semicolon_lists(self, example):
        s, dtype, t = example
        v = split_comma_semicolon_lists(s, dtype=dtype)
        assert_equal(v, t)

def suite():  # pragma: no cover
    return unittest.makeSuite(CmdlineHelpersTest)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_collections
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for attribute collections and their collectables'''

import numpy as np
import copy
import sys

from mvpa2.testing.tools import assert_raises, assert_false, assert_equal, \
    assert_true,  assert_array_equal, assert_array_almost_equal, reseed_rng
from mvpa2.testing import sweepargs

from mvpa2.base.collections import Collectable, ArrayCollectable, \
        SampleAttributesCollection

from mvpa2.base.attributes import ConditionalAttribute
from mvpa2.base.node import Node
from mvpa2.measures.base import Measure, RepeatedMeasure
from mvpa2.clfs.transerror import ConfusionMatrix

def test_basic_collectable():
    c = Collectable()

    # empty by default
    assert_equal(c.name, None)
    assert_equal(c.value, None)
    assert_equal(c.__doc__, None)

    # late assignment
    c.name = 'somename'
    c.value = 12345
    assert_equal(c.name, 'somename')
    assert_equal(c.value, 12345)

    # immediate content
    c = Collectable('value', 'myname', "This is a test")
    assert_equal(c.name, 'myname')
    assert_equal(c.value, 'value')
    assert_equal(c.__doc__, "This is a test")
    assert_equal(str(c), 'myname')

    # repr
    e = eval(repr(c))
    assert_equal(e.name, 'myname')
    assert_equal(e.value, 'value')
    assert_equal(e.__doc__, "This is a test")

    # shallow copy does not create a view of value array
    c.value = np.arange(5)
    d = copy.copy(c)
    assert_false(d.value.base is c.value)

    # names starting with _ are not allowed
    assert_raises(ValueError, c._set_name, "_underscore")


@reseed_rng()
def test_array_collectable():
    c = ArrayCollectable()

    # empty by default
    assert_equal(c.name, None)
    assert_equal(c.value, None)

    # late assignment
    c.name = 'somename'
    assert_raises(ValueError, c._set, 12345)
    assert_equal(c.value, None)
    c.value = np.arange(5)
    assert_equal(c.name, 'somename')
    assert_array_equal(c.value, np.arange(5))

    # immediate content
    data = np.random.random(size=(3,10))
    c = ArrayCollectable(data.copy(), 'myname',
                         "This is a test", length=3)
    assert_equal(c.name, 'myname')
    assert_array_equal(c.value, data)
    assert_equal(c.__doc__, "This is a test")
    assert_equal(str(c), 'myname')

    # repr
    from numpy import array
    e = eval(repr(c))
    assert_equal(e.name, 'myname')
    assert_array_almost_equal(e.value, data)
    assert_equal(e.__doc__, "This is a test")

    # cannot assign array of wrong length
    assert_raises(ValueError, c._set, np.arange(5))
    assert_equal(len(c), 3)

    # shallow copy DOES create a view of value array
    c.value = np.arange(3)
    d = copy.copy(c)
    assert_true(d.value.base is c.value)

    # names starting with _ are not allowed
    assert_raises(ValueError, c._set_name, "_underscore")


@sweepargs(a=(
    np.arange(4),
    # note: numpy casts int(1) it into dtype=float due to presence of
    #       nan since there is no int('nan'), so float right away
    [1., np.nan],
    np.array((1, np.nan)),
    [1, None],
    [np.nan, None],
    [1, 2.0, np.nan, None, "string"],
    np.arange(6).reshape((2, -1)),      # 2d's unique
    np.array([(1, 'mom'), (2,)], dtype=object),       # elaborate object ndarray
    ))
def test_array_collectable_unique(a):
    c = ArrayCollectable(a)
    a_flat = np.asanyarray(a).ravel()
    # Since nan != nan, we better compare based on string
    # representation here
    # And sort since order of those is not guaranteed (failed test
    # on squeeze)
    def repr_(x):
        x_ = set(x)
        if sys.version_info[0] < 3:
            x_ = list(x_)
            return repr(sorted(x_))
        else:
            return repr(np.sort(x_))

    assert_equal(repr_(a_flat), repr_(c.unique))
    # even if we request it 2nd time ;)
    assert_equal(repr_(a_flat), repr_(c.unique))
    assert_equal(len(a_flat), len(c.unique))

    c2 = ArrayCollectable(list(a_flat) + [float('nan')])
    # and since nan != nan, we should get new element
    assert_equal(len(c2.unique), len(c.unique) + 1)


def test_collections():
    sa = SampleAttributesCollection()
    assert_equal(len(sa), 0)

    assert_raises(ValueError, sa.__setitem__, 'test', 0)
    l = range(5)
    sa['test'] = l
    # auto-wrapped
    assert_true(isinstance(sa['test'], ArrayCollectable))
    assert_equal(len(sa), 1)

    # names which are already present in dict interface
    assert_raises(ValueError, sa.__setitem__, 'values', range(5))

    sa_c = copy.deepcopy(sa)
    assert_equal(len(sa), len(sa_c))
    assert_array_equal(sa.test, sa_c.test)


class TestNodeOffDefault(Node):
   test = ConditionalAttribute(enabled=False, doc="OffTest")
   stats = ConditionalAttribute(enabled=False, doc="OffStats")

class TestNodeOnDefault(Node):
   test = ConditionalAttribute(enabled=True, doc="OnTest")
   stats = ConditionalAttribute(enabled=True, doc="OnStats")


def test_conditional_attr():
    import copy
    import cPickle
    for node in (TestNodeOnDefault(enable_ca=['test', 'stats']),
                 TestNodeOffDefault(enable_ca=['test', 'stats'])):
        node.ca.test = range(5)
        node.ca.stats = ConfusionMatrix(labels=['one', 'two'])
        node.ca.stats.add(('one', 'two', 'one', 'two'),
                    ('one', 'two', 'two', 'one'))
        node.ca.stats.compute()

        dc_node = copy.deepcopy(node)
        assert_equal(set(node.ca.enabled), set(dc_node.ca.enabled))
        assert(node.ca['test'].enabled)
        assert(node.ca['stats'].enabled)
        assert_array_equal(node.ca['test'].value, dc_node.ca['test'].value)
        assert_array_equal(node.ca['stats'].value.matrix, dc_node.ca['stats'].value.matrix)

        # check whether values survive pickling
        pickled = cPickle.dumps(node)
        up_node = cPickle.loads(pickled)
        assert_array_equal(up_node.ca['test'].value, range(5))
        assert_array_equal(up_node.ca['stats'].value.matrix, node.ca['stats'].value.matrix)


########NEW FILE########
__FILENAME__ = test_compound
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA sparse multinomial logistic regression classifier"""

import numpy as np

from mvpa2.testing import *
from mvpa2.base.learner import Learner, CompoundLearner, \
                ChainLearner, CombinedLearner
from mvpa2.base.node import Node, CompoundNode, \
                ChainNode, CombinedNode

from mvpa2.datasets.base import AttrDataset

class FxNode(Node):
    def __init__(self, f, space='targets',
                 pass_attr=None, postproc=None, **kwargs):
        super(FxNode, self).__init__(space, pass_attr, postproc, **kwargs)
        self.f = f

    def _call(self, ds):
        cp = ds.copy()
        cp.samples = self.f(ds.samples)
        return cp

class FxyLearner(Learner):
    def __init__(self, f):
        super(FxyLearner, self).__init__()
        self.f = f
        self.x = None

    def _train(self, ds):
        self.x = ds.samples

    def _call(self, ds):
        cp = ds.copy()
        cp.samples = self.f(self.x)(ds.samples)
        return cp


class CompoundTests(unittest.TestCase):
    def test_compound_node(self):
        data = np.asarray([[1, 2, 3, 4]], dtype=np.float_).T
        ds = AttrDataset(data, sa=dict(targets=[0, 0, 1, 1]))

        add = lambda x: lambda y: x + y
        mul = lambda x: lambda y: x * y

        add2 = FxNode(add(2))
        mul3 = FxNode(mul(3))

        assert_array_equal(add2(ds).samples, data + 2)

        add2mul3 = ChainNode([add2, mul3])
        assert_array_equal(add2mul3(ds), (data + 2) * 3)

        add2_mul3v = CombinedNode([add2, mul3], 'v')
        add2_mul3h = CombinedNode([add2, mul3], 'h')
        assert_array_equal(add2_mul3v(ds).samples,
                           np.vstack((data + 2, data * 3)))
        assert_array_equal(add2_mul3h(ds).samples,
                           np.hstack((data + 2, data * 3)))

    def test_compound_learner(self):
        data = np.asarray([[1, 2, 3, 4]], dtype=np.float_).T
        ds = AttrDataset(data, sa=dict(targets=[0, 0, 1, 1]))
        train = ds[ds.sa.targets == 0]
        test = ds[ds.sa.targets == 1]
        dtrain = train.samples
        dtest = test.samples

        sub = FxyLearner(lambda x: lambda y: x - y)
        assert_false(sub.is_trained)
        sub.train(train)
        assert_array_equal(sub(test).samples, dtrain - dtest)


        div = FxyLearner(lambda x: lambda y: x / y)
        div.train(train)
        assert_array_almost_equal(div(test).samples, dtrain / dtest)
        div.untrain()

        subdiv = ChainLearner((sub, div))
        assert_false(subdiv.is_trained)
        subdiv.train(train)
        assert_true(subdiv.is_trained)
        subdiv.untrain()
        assert_raises(RuntimeError, subdiv, test)
        subdiv.train(train)

        assert_array_almost_equal(subdiv(test).samples, dtrain / (dtrain - dtest))

        sub_div = CombinedLearner((sub, div), 'v')
        assert_true(sub_div.is_trained)
        sub_div.untrain()
        subdiv.train(train)
        assert_true(sub_div.is_trained)

        assert_array_almost_equal(sub_div(test).samples,
                                  np.vstack((dtrain - dtest, dtrain / dtest)))



def suite():  # pragma: no cover
    return unittest.makeSuite(CompoundTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_config
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA dense array mapper"""


import unittest
from mvpa2.base.config import ConfigManager

class ConfigTests(unittest.TestCase):

    def test_config(self):
        cfg = ConfigManager()

        # does nothing so far, but will be used to test the default
        # configuration from doc/examples/pymvpa2.cfg

        # query for some non-existing option and check if default is returned
        query = cfg.get('dasgibtsdochnicht', 'neegarnicht', default='required')
        self.assertTrue(query == 'required')



def suite():  # pragma: no cover
    return unittest.makeSuite(ConfigTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_constraints
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Unit tests for basic constraints functionality.'''


from mvpa2.testing import *
import unittest

from mvpa2.base.constraints import *
import sys

class SimpleConstraintsTests(unittest.TestCase):

    def test_int(self):
        c = EnsureInt()
        # this should always work
        assert_equal(c(7), 7)
        assert_equal(c(7.0), 7)
        assert_equal(c('7'), 7)
        assert_equal(c([7, 3]), [7, 3])
        # this should always fail
        assert_raises(ValueError, lambda: c('fail'))
        assert_raises(ValueError, lambda: c([3, 'fail']))
        # this will also fail
        assert_raises(ValueError, lambda: c('17.0'))

    def test_float(self):
        c = EnsureFloat()
        # this should always work
        assert_equal(c(7.0), 7.0)
        assert_equal(c(7), 7.0)
        assert_equal(c('7'), 7.0)
        assert_equal(c([7.0, '3.0']), [7.0, 3.0])
        # this should always fail
        assert_raises(ValueError, lambda: c('fail'))
        assert_raises(ValueError, lambda: c([3.0, 'fail']))

    def test_bool(self):
        c = EnsureBool()
        # this should always work
        assert_equal(c(True), True)
        assert_equal(c(False), False)
        # all that resuls in True
        assert_equal(c('True'), True)
        assert_equal(c('true'), True)
        assert_equal(c('1'), True)
        assert_equal(c('yes'), True)
        assert_equal(c('on'), True)
        assert_equal(c('enable'), True)
        # all that resuls in False
        assert_equal(c('false'), False)
        assert_equal(c('False'), False)
        assert_equal(c('0'), False)
        assert_equal(c('no'), False)
        assert_equal(c('off'), False)
        assert_equal(c('disable'), False)
        # this should always fail
        assert_raises(ValueError, c, 0)
        assert_raises(ValueError, c, 1)

    def test_str(self):
        c = EnsureStr()
        # this should always work
        assert_equal(c('hello'), 'hello')
        assert_equal(c('7.0'), '7.0')
        # this should always fail
        assert_raises(ValueError, lambda: c(['ab']))
        assert_raises(ValueError, lambda: c(['a', 'b']))
        assert_raises(ValueError, lambda: c(('a', 'b')))
        # no automatic conversion attempted
        assert_raises(ValueError, lambda: c(7.0))

    def test_none(self):
        c = EnsureNone()
        # this should always work
        assert_equal(c(None), None)
        # this should always fail
        assert_raises(ValueError, lambda: c('None'))
        assert_raises(ValueError, lambda: c([]))

    def test_choice(self):
        c = EnsureChoice('choice1', 'choice2', None)
        # this should always work
        assert_equal(c('choice1'), 'choice1')
        assert_equal(c(None), None)
        # this should always fail
        assert_raises(ValueError, lambda: c('fail'))
        assert_raises(ValueError, lambda: c('None'))

    def test_range(self):
        c = EnsureRange(min=3, max=7)
        # this should always work
        assert_equal(c(3.0), 3.0)

        # Python 3 raises an TypeError if incompatible types are compared,
        # whereas Python 2 raises a ValueError
        type_error = TypeError if sys.version_info[0] >= 3 else ValueError

        # this should always fail
        assert_raises(ValueError, lambda: c(2.9999999))
        assert_raises(ValueError, lambda: c(77))
        assert_raises(type_error, lambda: c('fail'))
        assert_raises(type_error, lambda: c((3, 4)))
        # since no type checks are performed
        assert_raises(type_error, lambda: c('7'))

        # Range doesn't have to be numeric
        c = EnsureRange(min="e", max="qqq")
        assert_equal(c('e'), 'e')
        assert_equal(c('fa'), 'fa')
        assert_equal(c('qq'), 'qq')
        assert_raises(ValueError, c, 'a')
        assert_raises(ValueError, c, 'qqqa')


    def test_listof(self):
        c = EnsureListOf(str)
        assert_equal(c(['a', 'b']), ['a', 'b'])
        assert_equal(c(['a1', 'b2']), ['a1', 'b2'])

    def test_tupleof(self):
        c = EnsureTupleOf(str)
        assert_equal(c(('a', 'b')), ('a', 'b'))
        assert_equal(c(('a1', 'b2')), ('a1', 'b2'))
        
        
class ComplexConstraintsTests(unittest.TestCase):

    def test_constraints(self):
        # this should always work
        c = Constraints(EnsureFloat())
        assert_equal(c(7.0), 7.0)
        c = Constraints(EnsureFloat(), EnsureRange(min=4.0))
        assert_equal(c(7.0), 7.0)
        # __and__ form
        c = EnsureFloat() & EnsureRange(min=4.0)
        assert_equal(c(7.0), 7.0)
        assert_raises(ValueError, c, 3.9)
        c = Constraints(EnsureFloat(), EnsureRange(min=4), EnsureRange(max=9))
        assert_equal(c(7.0), 7.0)
        assert_raises(ValueError, c, 3.9)
        assert_raises(ValueError, c, 9.01)
        # __and__ form
        c = EnsureFloat() & EnsureRange(min=4) & EnsureRange(max=9)
        assert_equal(c(7.0), 7.0)
        assert_raises(ValueError, c, 3.99)
        assert_raises(ValueError, c, 9.01)
        # and reordering should not have any effect
        c = Constraints(EnsureRange(max=4), EnsureRange(min=9), EnsureFloat())
        assert_raises(ValueError, c, 3.99)
        assert_raises(ValueError, c, 9.01)

    def test_altconstraints(self):
        # this should always work
        c = AltConstraints(EnsureFloat())
        assert_equal(c(7.0), 7.0)
        c = AltConstraints(EnsureFloat(), EnsureNone())
        assert_equal(c(7.0), 7.0)
        assert_equal(c(None), None)
        # __or__ form
        c = EnsureFloat() | EnsureNone()
        assert_equal(c(7.0), 7.0)
        assert_equal(c(None), None)

        # this should always fail
        c = Constraints(EnsureRange(min=0, max=4), EnsureRange(min=9, max=11))
        assert_raises(ValueError, c, 7.0)
        c = EnsureRange(min=0, max=4) | EnsureRange(min=9, max=11)
        assert_equal(c(3.0), 3.0)
        assert_equal(c(9.0), 9.0)
        assert_raises(ValueError, c, 7.0)
        assert_raises(ValueError, c, -1.0)

    def test_both(self):
        # this should always work
        c = AltConstraints(Constraints(EnsureFloat(), \
                                      EnsureRange(min=7.0, max=44.0)), \
                                      EnsureNone())
        assert_equal(c(7.0), 7.0)
        assert_equal(c(None), None)
        # this should always fail
        assert_raises(ValueError, lambda: c(77.0))




if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()



########NEW FILE########
__FILENAME__ = test_corrstability
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for basic mappers'''

import numpy as np

from mvpa2.testing.tools import ok_, assert_raises, assert_false, assert_equal, \
        assert_true, assert_array_equal, assert_array_less
from mvpa2.measures.corrstability import CorrStability
from mvpa2.testing.datasets import datasets
from mvpa2.testing import sweepargs

@sweepargs(ds=datasets.itervalues())
def test_corrstability_smoketest(ds):
    if not 'chunks' in ds.sa:
        return
    if len(ds.sa['targets'].unique) > 30:
        # was regression dataset
        return
    # very basic testing since
    cs = CorrStability()
    #ds = datasets['uni2small']
    out = cs(ds)
    assert_equal(out.shape, (ds.nfeatures,))
    ok_(np.all(out >= -1.001))  # it should be a correlation after all
    ok_(np.all(out <= 1.001))

    # and theoretically those nonbogus features should have higher values
    if 'nonbogus_targets' in ds.fa:
        bogus_features = np.array([x==None for x in  ds.fa.nonbogus_targets])
        assert_array_less(np.mean(out[bogus_features]), np.mean(out[~bogus_features]))
    # and if we move targets to alternative location
    ds = ds.copy(deep=True)
    ds.sa['alt'] = ds.T
    ds.sa.pop('targets')
    assert_raises(KeyError, cs, ds)
    cs = CorrStability('alt')
    out_ = cs(ds)
    assert_array_equal(out, out_)



########NEW FILE########
__FILENAME__ = test_datameasure
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA SplittingSensitivityAnalyzer"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.clfs import *
from mvpa2.testing.datasets import *

from mvpa2.base import externals, warning
from mvpa2.base.node import ChainNode, CombinedNode
from mvpa2.datasets.base import Dataset, AttrDataset
from mvpa2.featsel.base import SensitivityBasedFeatureSelection, \
        CombinedFeatureSelection
from mvpa2.featsel.helpers import FixedNElementTailSelector, \
                                 FractionTailSelector, RangeElementSelector

from mvpa2.featsel.rfe import RFE

from mvpa2.clfs.meta import SplitClassifier, MulticlassClassifier, \
     FeatureSelectionClassifier
from mvpa2.clfs.smlr import SMLR, SMLRWeights
from mvpa2.mappers.zscore import zscore
from mvpa2.mappers.fx import sumofabs_sample, absolute_features, FxMapper, \
     maxofabs_sample, BinaryFxNode, \
     mean_sample, mean_feature
from mvpa2.generators.splitters import Splitter
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.generators.resampling import Balancer

from mvpa2.misc.errorfx import mean_mismatch_error
from mvpa2.misc.transformers import Absolute, \
     DistPValue

from mvpa2.measures.base import Measure, \
        TransferMeasure, RepeatedMeasure, CrossValidation
from mvpa2.measures.anova import OneWayAnova, CompoundOneWayAnova
from mvpa2.measures.irelief import IterativeRelief, IterativeReliefOnline, \
     IterativeRelief_Devel, IterativeReliefOnline_Devel


_MEASURES_2_SWEEP = [ OneWayAnova(),
                      CompoundOneWayAnova(postproc=sumofabs_sample()),
                      IterativeRelief(), IterativeReliefOnline(),
                      IterativeRelief_Devel(), IterativeReliefOnline_Devel()
                      ]
if externals.exists('scipy'):
    from mvpa2.measures.corrcoef import CorrCoef
    _MEASURES_2_SWEEP += [ CorrCoef(),
                           # that one is good when small... handle later
                           #CorrCoef(pvalue=True)
                           ]
    from mvpa2.featsel.base import SplitSamplesProbabilityMapper

class SensitivityAnalysersTests(unittest.TestCase):

    def setUp(self):
        self.dataset = datasets['uni2large']


    @sweepargs(dsm=_MEASURES_2_SWEEP)
    def test_basic(self, dsm):
        data = datasets['dumbinv']
        datass = data.samples.copy()

        # compute scores
        f = dsm(data)
        # check if nothing evil is done to dataset
        self.assertTrue(np.all(data.samples == datass))
        self.assertTrue(f.shape == (1, data.nfeatures))
        self.assertTrue(abs(f.samples[0, 1]) <= 1e-12, # some small value
            msg="Failed test with value %g instead of != 0.0" % f.samples[0, 1])
        self.assertTrue(f.samples[0, 0] > 0.1)     # some reasonably large value

        # we should not have NaNs
        self.assertTrue(not np.any(np.isnan(f)))



    # NOTE: lars with stepwise used to segfault if all ca are enabled
    @sweepargs(clfds=
               [(c, datasets['uni2large'])
                for c in clfswh['has_sensitivity', 'binary']] +
               [(c, datasets['uni4large'])
                for c in clfswh['has_sensitivity', 'multiclass']]
               )
    def test_analyzer_with_split_classifier(self, clfds):
        """Test analyzers in split classifier
        """
        clf, ds = clfds             # unroll the tuple
        # We need to skip some LARSes here
        _sclf = str(clf)
        if 'LARS(' in _sclf and "type='stepwise'" in _sclf:
            # ADD KnownToFail thingie from NiPy
            return

        # To don't waste too much time testing lets limit to 3 splits
        nsplits = 3
        partitioner = NFoldPartitioner(count=nsplits)
        mclf = SplitClassifier(clf=clf,
                               partitioner=partitioner,
                               enable_ca=['training_stats',
                                              'stats'])
        sana = mclf.get_sensitivity_analyzer(# postproc=absolute_features(),
                                           pass_attr=['fa.nonbogus_targets'],
                                           enable_ca=["sensitivities"])

        ulabels = ds.uniquetargets
        nlabels = len(ulabels)
        # Can't rely on splitcfg since count-limit is done in __call__
        assert(nsplits == len(list(partitioner.generate(ds))))
        sens = sana(ds)
        assert('nonbogus_targets' in sens.fa) # were they passsed?
        # TODO: those few do not expose biases
        if not len(set(clf.__tags__).intersection(('lars', 'glmnet', 'gpr'))):
            assert('biases' in sens.sa)
            # print sens.sa.biases
        # It should return either ...
        #  nlabels * nsplits
        req_nsamples = [ nlabels * nsplits ]
        if nlabels == 2:
            # A single sensitivity in case of binary
            req_nsamples += [ nsplits ]
        else:
            # and for pairs in case of multiclass
            req_nsamples += [ (nlabels * (nlabels - 1) / 2) * nsplits ]
            # and for 1-vs-1 embedded within Multiclass operating on
            # pairs (e.g. SMLR)
            req_nsamples += [req_nsamples[-1] * 2]

            # Also for regression_based -- they can do multiclass
            # but only 1 sensitivity is provided
            if 'regression_based' in clf.__tags__:
                req_nsamples += [ nsplits ]

        # # of features should correspond
        self.assertEqual(sens.shape[1], ds.nfeatures)
        # # of samples/sensitivities should also be reasonable
        self.assertTrue(sens.shape[0] in req_nsamples)

        # Check if labels are present
        self.assertTrue('splits' in sens.sa)
        self.assertTrue('targets' in sens.sa)
        # should be 1D -- otherwise dtype object
        self.assertTrue(sens.sa.targets.ndim == 1)

        sens_ulabels = sens.sa['targets'].unique
        # Some labels might be pairs(tuples) so ndarray would be of
        # dtype object and we would need to get them all
        if sens_ulabels.dtype is np.dtype('object'):
            sens_ulabels = np.unique(
                reduce(lambda x, y: x + y, [list(x) for x in sens_ulabels]))

        assert_array_equal(sens_ulabels, ds.sa['targets'].unique)

        errors = [x.percent_correct
                    for x in sana.clf.ca.stats.matrices]

        # lets go through all sensitivities and see if we selected the right
        # features
        #if 'meta' in clf.__tags__ and len(sens.samples[0].nonzero()[0])<2:
        if '5%' in clf.descr \
               or (nlabels > 2 and 'regression_based' in clf.__tags__):
            # Some meta classifiers (5% of ANOVA) are too harsh ;-)
            # if we get less than 2 features with on-zero sensitivities we
            # cannot really test
            # Also -- regression based classifiers performance for multiclass
            # is expected to suck in general
            return

        if cfg.getboolean('tests', 'labile', default='yes'):
            for conf_matrix in [sana.clf.ca.training_stats] \
                              + sana.clf.ca.stats.matrices:
                self.assertTrue(
                    conf_matrix.percent_correct >= 70,
                    msg="We must have trained on each one more or " \
                    "less correctly. Got %f%% correct on %d labels" %
                    (conf_matrix.percent_correct,
                     nlabels))


        # Since  now we have per split and possibly per label -- lets just find
        # mean per each feature per label across splits
        sensm = FxMapper('samples', lambda x: np.sum(x),
                         uattrs=['targets']).forward(sens)
        sensgm = maxofabs_sample().forward(sensm)    # global max of abs of means

        assert_equal(sensgm.shape[0], 1)
        assert_equal(sensgm.shape[1], ds.nfeatures)

        selected = FixedNElementTailSelector(
            len(ds.a.bogus_features))(sensgm.samples[0])

        if cfg.getboolean('tests', 'labile', default='yes'):

            self.assertEqual(
                set(selected), set(ds.a.nonbogus_features),
                msg="At the end we should have selected the right features. "
                "Chose %s whenever nonbogus are %s"
                % (selected, ds.a.nonbogus_features))

            # Now test each one per label
            # TODO: collect all failures and spit them out at once --
            #       that would make it easy to see if the sensitivity
            #       just has incorrect order of labels assigned
            for sens1 in sensm:
                labels1 = sens1.targets  # labels (1) for this sensitivity
                lndim = labels1.ndim
                label = labels1[0]      # current label

                # XXX whole lndim comparison should be gone after
                #     things get fixed and we arrive here with a tuple!
                if lndim == 1: # just a single label
                    self.assertTrue(label in ulabels)

                    ilabel_all = np.where(ds.fa.nonbogus_targets == label)[0]
                    # should have just 1 feature for the label
                    self.assertEqual(len(ilabel_all), 1)
                    ilabel = ilabel_all[0]

                    maxsensi = np.argmax(sens1) # index of max sensitivity
                    self.assertEqual(maxsensi, ilabel,
                        "Maximal sensitivity for %s was found in %i whenever"
                        " original feature was %i for nonbogus features %s"
                        % (labels1, maxsensi, ilabel, ds.a.nonbogus_features))
                elif lndim == 2 and labels1.shape[1] == 2: # pair of labels
                    # we should have highest (in abs) coefficients in
                    # those two labels
                    maxsensi2 = np.argsort(np.abs(sens1))[0][-2:]
                    ilabel2 = [np.where(ds.fa.nonbogus_targets == l)[0][0]
                                    for l in label]
                    self.assertEqual(
                        set(maxsensi2), set(ilabel2),
                        "Maximal sensitivity for %s was found in %s whenever"
                        " original features were %s for nonbogus features %s"
                        % (labels1, maxsensi2, ilabel2, ds.a.nonbogus_features))
                    """
                    # Now test for the sign of each one in pair ;) in
                    # all binary problems L1 (-1) -> L2(+1), then
                    # weights for L2 should be positive.  to test for
                    # L1 -- invert the sign
                    # We already know (if we haven't failed in previous test),
                    # that those 2 were the strongest -- so check only signs
                    """
                    self.assertTrue(
                        sens1.samples[0, ilabel2[0]] < 0,
                        "With %i classes in pair %s got feature %i for %r >= 0"
                        % (nlabels, label, ilabel2[0], label[0]))
                    self.assertTrue(sens1.samples[0, ilabel2[1]] > 0,
                        "With %i classes in pair %s got feature %i for %r <= 0"
                        % (nlabels, label, ilabel2[1], label[1]))
                else:
                    # yoh could be wrong at this assumption... time will show
                    self.fail("Got unknown number labels per sensitivity: %s."
                              " Should be either a single label or a pair"
                              % labels1)


    @sweepargs(clf=clfswh['has_sensitivity'])
    def test_mapped_classifier_sensitivity_analyzer(self, clf):
        """Test sensitivity of the mapped classifier
        """
        # Assuming many defaults it is as simple as
        mclf = FeatureSelectionClassifier(
            clf,
            SensitivityBasedFeatureSelection(
                OneWayAnova(),
                FractionTailSelector(0.5, mode='select', tail='upper')),
            enable_ca=['training_stats'])

        sana = mclf.get_sensitivity_analyzer(postproc=sumofabs_sample(),
                                             enable_ca=["sensitivities"])
        # and lets look at all sensitivities
        dataset = datasets['uni2small']
        # and we get sensitivity analyzer which works on splits
        sens = sana(dataset)
        self.assertEqual(sens.shape, (1, dataset.nfeatures))



    @sweepargs(svm=clfswh['linear', 'svm'])
    def test_linear_svm_weights(self, svm):
        # assumming many defaults it is as simple as
        sana = svm.get_sensitivity_analyzer(enable_ca=["sensitivities"])
        # and lets look at all sensitivities
        sens = sana(self.dataset)
        # for now we can do only linear SVM, so lets check if we raise
        # a concern
        svmnl = clfswh['non-linear', 'svm'][0]
        self.assertRaises(NotImplementedError,
                              svmnl.get_sensitivity_analyzer)


    # XXX doesn't work easily with meta since it would need
    #     to be explicitely passed to the slave classifier's
    #     getSengetSensitivityAnalyzer
    # Note: only libsvm interface supports split_weights
    @sweepargs(svm=clfswh['linear', 'svm', 'libsvm', '!sg', '!meta'])
    def test_linear_svm_weights_per_class(self, svm):
        # assumming many defaults it is as simple as
        kwargs = dict(enable_ca=["sensitivities"])
        sana_split = svm.get_sensitivity_analyzer(
            split_weights=True, **kwargs)
        sana_full = svm.get_sensitivity_analyzer(
            force_train=False, **kwargs)

        # and lets look at all sensitivities
        ds2 = datasets['uni4large'].copy()
        zscore(ds2, param_est=('targets', ['L2', 'L3']))
        ds2 = ds2[np.logical_or(ds2.sa.targets == 'L0', ds2.sa.targets == 'L1')]

        senssplit = sana_split(ds2)
        sensfull = sana_full(ds2)

        self.assertEqual(senssplit.shape, (2, ds2.nfeatures))
        self.assertEqual(sensfull.shape, (1, ds2.nfeatures))

        # just to verify that we split properly and if we reconstruct
        # manually we obtain the same
        dmap = (-1 * senssplit.samples[1] + senssplit.samples[0]) \
               - sensfull.samples
        self.assertTrue((np.abs(dmap) <= 1e-10).all())
        #print "____"
        #print senssplit
        #print SMLR().get_sensitivity_analyzer(combiner=None)(ds2)

        # for now we can do split weights for binary tasks only, so
        # lets check if we raise a concern
        # we temporarily shutdown warning, since it is going to complain
        # otherwise, but we do it on purpose here
        handlers = warning.handlers
        warning.handlers = []
        self.assertRaises(NotImplementedError,
                              sana_split, datasets['uni3medium'])
        # reenable the warnings
        warning.handlers = handlers


    def test_split_featurewise_dataset_measure(self):
        ds = datasets['uni3small']
        sana = RepeatedMeasure(
            SMLR(fit_all_weights=True).get_sensitivity_analyzer(),
            ChainNode([NFoldPartitioner(),
                       Splitter('partitions', attr_values=[1])]))

        sens = sana(ds)
        # a sensitivity for each chunk and each label combination
        assert_equal(sens.shape,
                     (len(ds.sa['chunks'].unique) * len(ds.sa['targets'].unique),
                      ds.nfeatures))

        # Lets try more complex example with 'boosting'
        ds = datasets['uni3medium']
        ds.init_origids('samples')
        sana = RepeatedMeasure(
            SMLR(fit_all_weights=True).get_sensitivity_analyzer(),
            Balancer(amount=0.25, count=2, apply_selection=True),
            enable_ca=['datasets', 'repetition_results'])
        sens = sana(ds)

        assert_equal(sens.shape, (2 * len(ds.sa['targets'].unique),
                                  ds.nfeatures))
        splits = sana.ca.datasets
        self.assertEqual(len(splits), 2)
        self.assertTrue(np.all([s.nsamples == ds.nsamples // 4 for s in splits]))
        # should have used different samples
        self.assertTrue(np.any([splits[0].sa.origids != splits[1].sa.origids]))
        # and should have got different sensitivities
        self.assertTrue(np.any(sens[0] != sens[3]))


        #skip_if_no_external('scipy')
        # Let's disable this one for now until we are sure about the destiny of
        # DistPValue -- read the docstring of it!
        # Most evil example
        #ds = datasets['uni2medium']
        #plain_sana = SVM().get_sensitivity_analyzer(
        #       transformer=DistPValue())
        #boosted_sana = SplitFeaturewiseMeasure(
        #    analyzer=SVM().get_sensitivity_analyzer(
        #       transformer=DistPValue(fpp=0.05)),
        #    splitter=NoneSplitter(npertarget=0.8, mode='first', nrunspersplit=2),
        #    enable_ca=['splits', 'sensitivities'])
        ## lets create feature selector
        #fsel = RangeElementSelector(upper=0.1, lower=0.9, inclusive=True)

        #sanas = dict(plain=plain_sana, boosted=boosted_sana)
        #for k,sana in sanas.iteritems():
        #    clf = FeatureSelectionClassifier(SVM(),
        #                SensitivityBasedFeatureSelection(sana, fsel),
        #                descr='SVM on p=0.2(both tails) using %s' % k)
        #    ce = CrossValidatedTransferError(TransferError(clf),
        #                                     NFoldSplitter())
        #    error = ce(ds)

        #sens = boosted_sana(ds)
        #sens_plain = plain_sana(ds)

        ## TODO: make a really unittest out of it -- not just runtime
        ##       bugs catcher

    # TODO -- unittests for sensitivity analyzers which use combiners
    # (linsvmweights for multi-class SVMs and smlrweights for SMLR)


    @sweepargs(basic_clf=clfswh['has_sensitivity'])
    ##REF: Name was automagically refactored
    def __test_fspipeline_with_split_classifier(self, basic_clf):
        #basic_clf = LinearNuSVMC()
        multi_clf = MulticlassClassifier(clf=basic_clf)
        #svm_weigths = LinearSVMWeights(svm)

        # Proper RFE: aggregate sensitivities across multiple splits,
        # but also due to multi class those need to be aggregated
        # somehow. Transfer error here should be 'leave-1-out' error
        # of split classifier itself
        sclf = SplitClassifier(clf=basic_clf)
        rfe = RFE(sensitivity_analyzer=
                    sclf.get_sensitivity_analyzer(
                        enable_ca=["sensitivities"]),
                  transfer_error=trans_error,
                  feature_selector=FeatureSelectionPipeline(
                      [FractionTailSelector(0.5),
                       FixedNElementTailSelector(1)]),
                  train_pmeasure=True)

        # and we get sensitivity analyzer which works on splits and uses
        # sensitivity
        selected_features = rfe(self.dataset)

    def test_union_feature_selection(self):
        # two methods: 5% highes F-scores, non-zero SMLR weights
        fss = [SensitivityBasedFeatureSelection(
                    OneWayAnova(),
                    FractionTailSelector(0.05, mode='select', tail='upper')),
               SensitivityBasedFeatureSelection(
                    SMLRWeights(SMLR(lm=1, implementation="C"),
                                postproc=sumofabs_sample()),
                    RangeElementSelector(mode='select'))]

        fs = CombinedFeatureSelection(fss, method='union')

        od_union = fs(self.dataset)

        self.assertTrue(fs.method == 'union')
        # check output dataset
        self.assertTrue(od_union.nfeatures <= self.dataset.nfeatures)
        # again for intersection
        fs = CombinedFeatureSelection(fss, method='intersection')
        od_intersect = fs(self.dataset)
        assert_true(od_intersect.nfeatures < od_union.nfeatures)

    def test_anova(self):
        """Additional aspects of OnewayAnova
        """
        oa = OneWayAnova()
        oa_custom = OneWayAnova(space='custom')

        ds = datasets['uni4large']
        ds_custom = Dataset(ds.samples, sa={'custom' : ds.targets})

        r = oa(ds)
        self.assertRaises(KeyError, oa_custom, ds)
        r_custom = oa_custom(ds_custom)

        self.assertTrue(np.allclose(r.samples, r_custom.samples))

        # we should get the same results on subsequent runs
        r2 = oa(ds)
        r_custom2 = oa_custom(ds_custom)
        self.assertTrue(np.allclose(r.samples, r2.samples))
        self.assertTrue(np.allclose(r_custom.samples, r_custom2.samples))


    def test_transfer_measure(self):
        # come up with my own measure that only checks if training data
        # and test data are the same
        class MyMeasure(Measure):
            def _train(self, ds):
                self._tds = ds
            def _call(self, ds):
                return Dataset(ds.samples == self._tds.samples)

        tm = TransferMeasure(MyMeasure(), Splitter('chunks', count=2))
        # result should not be all True (== identical)
        assert_true((tm(self.dataset).samples == False).any())


    def test_clf_transfer_measure(self):
        # and now on a classifier
        clf = SMLR()
        enode = BinaryFxNode(mean_mismatch_error, 'targets')
        tm = TransferMeasure(clf, Splitter('chunks', count=2),
                             enable_ca=['stats'])
        res = tm(self.dataset)
        manual_error = np.mean(res.samples.squeeze() != res.sa.targets)
        postproc_error = enode(res)
        tm_err = TransferMeasure(clf, Splitter('chunks', count=2),
                                 postproc=enode)
        auto_error = tm_err(self.dataset)
        ok_(manual_error == postproc_error.samples[0, 0])


    def test_pseudo_cv_measure(self):
        clf = SMLR()
        enode = BinaryFxNode(mean_mismatch_error, 'targets')
        tm = TransferMeasure(clf, Splitter('partitions'), postproc=enode)
        cvgen = NFoldPartitioner()
        rm = RepeatedMeasure(tm, cvgen)
        res = rm(self.dataset)
        # one error per fold
        assert_equal(res.shape, (len(self.dataset.sa['chunks'].unique), 1))

        # we can do the same with Crossvalidation
        cv = CrossValidation(clf, cvgen, enable_ca=['stats', 'training_stats',
                                                    'datasets'])
        res = cv(self.dataset)
        assert_equal(res.shape, (len(self.dataset.sa['chunks'].unique), 1))


    def test_repeated_features(self):
        class CountFeatures(Measure):
            is_trained = True
            def _call(self, ds):
                return Dataset([ds.nfeatures],
                                fa={'nonbogus_targets': list(ds.fa['nonbogus_targets'].unique)})

        cf = CountFeatures()
        spl = Splitter('fa.nonbogus_targets')
        nsplits = len(list(spl.generate(self.dataset)))
        assert_equal(nsplits, 3)
        rm = RepeatedMeasure(cf, spl, concat_as='features')
        res = rm(self.dataset)
        assert_equal(res.shape, (1, nsplits))
        # due to https://github.com/numpy/numpy/issues/641 we are
        # using list(set(...)) construct and there order of
        # nonbogus_targets.unique can vary from run to run, thus there
        # is no guarantee that we would get 18 first, which is a
        # questionable assumption anyways, thus performing checks
        # which do not require any specific order.
        # And yet due to another issue
        # https://github.com/numpy/numpy/issues/3759
        # we can't just == None for the bool mask
        None_fa = np.array([x == None for x in  res.fa.nonbogus_targets])
        assert_array_equal(res.samples[0, None_fa], [18])
        assert_array_equal(res.samples[0, ~None_fa], [1, 1])

        if sys.version_info[0] < 3:
            # with python2 order seems to be consistent
            assert_array_equal(res.samples[0], [18, 1, 1])

    def test_custom_combined_selectors(self):
        """Test combination of the selectors in a single function
        """

        def custom_tail_selector(seq):
            seq1 = FractionTailSelector(0.01, mode='discard', tail='upper')(seq)
            seq2 = FractionTailSelector(0.05, mode='select', tail='upper')(seq)
            return list(set(seq1).intersection(seq2))

        seq = np.arange(100)
        seq_ = custom_tail_selector(seq)

        assert_array_equal(sorted(seq_), [95, 96, 97, 98])
        # verify that this function could be used in place of the selector
        fs = SensitivityBasedFeatureSelection(
                    OneWayAnova(),
                    custom_tail_selector)
        ds = datasets['3dsmall']
        fs.train(ds)          # XXX: why needs to be trained here explicitly?
        ds_ = fs(ds)
        assert_equal(ds_.nfeatures, int(ds.nfeatures * 0.04))

    def test_combined_node(self):
        ds = datasets['3dsmall']
        axis2nodes = dict(h=(mean_feature, mean_feature),
                          v=(mean_sample, mean_sample))

        for i, axis in enumerate('vh'):
            nodes = axis2nodes[axis]
            combined = CombinedNode([n() for n in nodes], axis, False)
            assert_true(combined(ds).shape[i] == 2)
            assert_true(combined(ds).shape[1 - i] == ds.shape[1 - i])

    def test_split_samples_probability_mapper(self):
        skip_if_no_external('scipy')
        nf = 10
        ns = 100
        nsubj = 5
        nchunks = 5
        data = np.random.normal(size=(ns, nf))
        ds = AttrDataset(data, sa=dict(sidx=np.arange(ns),
                                    targets=np.arange(ns) % nchunks,
                                    chunks=np.floor(np.arange(ns) * nchunks / ns),
                                    subjects=np.arange(ns) / (ns / nsubj / nchunks) % nsubj),
                            fa=dict(fidx=np.arange(nf)))
        analyzer = OneWayAnova()
        element_selector = FractionTailSelector(.4, mode='select', tail='upper')
        common = True
        m = SplitSamplesProbabilityMapper(analyzer, 'subjects', probability_label='fprob',
                            select_common_features=common,
                            selector=element_selector)

        m.train(ds)
        y = m(ds)
        z = m(ds.samples)

        assert_array_equal(z, y.samples)
        assert_equal(y.shape, (100, 4))


    def test_pass_attr(self):
        from mvpa2.base.node import Node
        from mvpa2.base.state import ConditionalAttribute

        ds = datasets['dumbinv']

        class MyNode(Node):
            some_sa = ConditionalAttribute(enabled=True)
            some_fa = ConditionalAttribute(enabled=True)
            some_complex = ConditionalAttribute(enabled=True)
            def _call(self, ds):
                return Dataset(np.zeros(ds.shape))
        node = MyNode(pass_attr=['ca.some_sa',
                                 ('ca.some_fa', 'fa'),
                                 ('ca.some_complex', 'fa', 1, 'transposed'),
                                 'sa.targets'])
        node.ca.some_sa = np.arange(len(ds))
        node.ca.some_fa = np.arange(ds.nfeatures)
        node.ca.some_complex = ds.samples
        res = node(ds)
        assert_true('some_sa' in res.sa)
        assert_true('some_fa' in res.fa)
        assert_true('transposed' in res.fa)
        assert_true('targets' in res.sa)
        # view on original array
        assert_true(res.fa.transposed.base is ds.samples)
        assert_array_equal(res.fa.transposed.T, ds.samples)


def suite():  # pragma: no cover
    return unittest.makeSuite(SensitivityAnalysersTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_datasetfx
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA miscelaneouse functions operating on datasets"""

import unittest
from mvpa2.testing.tools import ok_, assert_equal, assert_array_equal, reseed_rng

import numpy as np

from mvpa2.base import externals
from mvpa2.datasets.base import dataset_wizard
from mvpa2.datasets.miscfx import remove_invariant_features, coarsen_chunks, \
        aggregate_features, SequenceStats


from mvpa2.misc.data_generators import normal_feature_dataset

class MiscDatasetFxTests(unittest.TestCase):

    def test_aggregation(self):
        data = dataset_wizard(np.arange( 20 ).reshape((4, 5)), targets=1, chunks=1)

        ag_data = aggregate_features(data, np.mean)

        ok_(ag_data.nsamples == 4)
        ok_(ag_data.nfeatures == 1)
        assert_array_equal(ag_data.samples[:, 0], [2, 7, 12, 17])


    @reseed_rng()
    def test_invar_features_removal(self):
        r = np.random.normal(size=(3,1))
        ds = dataset_wizard(samples=np.hstack((np.zeros((3,2)), r)),
                     targets=1)

        self.assertTrue(ds.nfeatures == 3)

        dsc = remove_invariant_features(ds)

        self.assertTrue(dsc.nfeatures == 1)
        self.assertTrue((dsc.samples == r).all())


    def test_coarsen_chunks(self):
        """Just basic testing for now"""
        chunks = [1,1,2,2,3,3,4,4]
        ds = dataset_wizard(samples=np.arange(len(chunks)).reshape(
            (len(chunks),1)), targets=[1]*8, chunks=chunks)
        coarsen_chunks(ds, nchunks=2)
        chunks1 = coarsen_chunks(chunks, nchunks=2)
        self.assertTrue((chunks1 == ds.chunks).all())
        self.assertTrue((chunks1 == np.asarray([0,0,0,0,1,1,1,1])).all())

        ds2 = dataset_wizard(samples=np.arange(len(chunks)).reshape(
            (len(chunks),1)), targets=[1]*8, chunks=range(len(chunks)))
        coarsen_chunks(ds2, nchunks=2)
        self.assertTrue((chunks1 == ds.chunks).all())

    def test_binds(self):
        ds = normal_feature_dataset()
        ds_data = ds.samples.copy()
        ds_chunks = ds.chunks.copy()
        self.assertTrue(np.all(ds.samples == ds_data)) # sanity check

        funcs = ['coarsen_chunks']

        for f in funcs:
            eval('ds.%s()' % f)
            self.assertTrue(np.any(ds.samples != ds_data) or
                            np.any(ds.chunks != ds_chunks),
                msg="We should have modified original dataset with %s" % f)
            ds.samples = ds_data.copy()
            ds.sa['chunks'].value = ds_chunks.copy()

        # and some which should just return results
        for f in ['aggregate_features', 'remove_invariant_features',
                  'get_samples_per_chunk_target']:
            res = eval('ds.%s()' % f)
            self.assertTrue(res is not None,
                msg='We should have got result from function %s' % f)
            self.assertTrue(np.all(ds.samples == ds_data),
                msg="Function %s should have not modified original dataset" % f)

    @reseed_rng()
    def test_sequence_stat(self):
        """Test sequence statistics
        """
        order = 3
        # Close to perfectly balanced one
        sp = np.array([-1,  1,  1, -1,  1, -1, -1,  1, -1, -1, -1,
                      -1,  1, -1,  1, -1,  1,  1,  1, -1,  1,  1,
                      -1, -1, -1,  1,  1,  1,  1,  1, -1], dtype=int)
        rp = SequenceStats(sp, order=order)
        self.failUnlessAlmostEqual(rp['sumabscorr'], 1.0)
        self.failUnlessAlmostEqual(np.max(rp['corrcoef'] * (len(sp)-1) + 1.0), 0.0)

        # Now some random but loong one still binary (boolean)
        sb = (np.random.random_sample((1000,)) >= 0.5)
        rb = SequenceStats(sb, order=order)

        # Now lets do multiclass with literal targets
        s5 = np.random.permutation(['f', 'bu', 'd', 0, 'zz']*200)
        r5 = SequenceStats(s5, order=order)

        # Degenerate one but still should be valid
        s1 = ['aaa']*100
        r1 = SequenceStats(s1, order=order)

        # Generic conformance tests
        for r in (rp, rb, r5, r1):
            ulabels = r['utargets']
            nlabels = len(r['utargets'])
            cbcounts = r['cbcounts']
            self.assertEqual(len(cbcounts), order)
            for cb in cbcounts:
                self.assertEqual(np.asarray(cb).shape, (nlabels, nlabels))
            # Check if str works fine
            sr = str(r)
            # TODO: check the content



def suite():  # pragma: no cover
    return unittest.makeSuite(MiscDatasetFxTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_datasetng
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for the dataset implementation'''

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets

import numpy as np
import shutil
import tempfile
import os


from mvpa2.base import cfg
from mvpa2.base.externals import versions
from mvpa2.base.types import is_datasetlike
from mvpa2.base.dataset import DatasetError, vstack, hstack, all_equal, \
                                stack_by_unique_feature_attribute, \
                                stack_by_unique_sample_attribute
from mvpa2.datasets.base import dataset_wizard, Dataset, HollowSamples
from mvpa2.misc.data_generators import normal_feature_dataset
from mvpa2.testing import reseed_rng
import mvpa2.support.copy as copy
from mvpa2.base.collections import \
     SampleAttributesCollection, FeatureAttributesCollection, \
     DatasetAttributesCollection, ArrayCollectable, SampleAttribute, \
     Collectable


class myarray(np.ndarray):
    pass


# TODO Urgently need test to ensure that multidimensional samples and feature
#      attributes work and adjust docs to mention that we support such
def test_from_wizard():
    samples = np.arange(12).reshape((4, 3)).view(myarray)
    labels = range(4)
    chunks = [1, 1, 2, 2]

    ds = Dataset(samples, sa={'targets': labels, 'chunks': chunks})
    ds.init_origids('both')
    first = ds.sa.origids
    # now do again and check that they get regenerated
    ds.init_origids('both')
    assert_false(first is ds.sa.origids)
    assert_array_equal(first, ds.sa.origids)

    ok_(is_datasetlike(ds))
    ok_(not is_datasetlike(labels))

    # array subclass survives
    ok_(isinstance(ds.samples, myarray))

    ## XXX stuff that needs thought:

    # ds.sa (empty) has this in the public namespace:
    #   add, get, getvalue, has_key, is_set, items, listing, name, names
    #   owner, remove, reset, setvalue, which_set
    # maybe we need some form of leightweightCollection?

    assert_array_equal(ds.samples, samples)
    assert_array_equal(ds.sa.targets, labels)
    assert_array_equal(ds.sa.chunks, chunks)

    # same should work for shortcuts
    assert_array_equal(ds.targets, labels)
    assert_array_equal(ds.chunks, chunks)

    ok_(sorted(ds.sa.keys()) == ['chunks', 'origids', 'targets'])
    ok_(sorted(ds.fa.keys()) == ['origids'])
    # add some more
    ds.a['random'] = 'blurb'

    # check stripping attributes from a copy
    cds = ds.copy() # full copy
    ok_(sorted(cds.sa.keys()) == ['chunks', 'origids', 'targets'])
    ok_(sorted(cds.fa.keys()) == ['origids'])
    ok_(sorted(cds.a.keys()) == ['random'])
    cds = ds.copy(sa=[], fa=[], a=[]) # plain copy
    ok_(cds.sa.keys() == [])
    ok_(cds.fa.keys() == [])
    ok_(cds.a.keys() == [])
    cds = ds.copy(sa=['targets'], fa=None, a=['random']) # partial copy
    ok_(cds.sa.keys() == ['targets'])
    ok_(cds.fa.keys() == ['origids'])
    ok_(cds.a.keys() == ['random'])

    # there is not necessarily a mapper present
    ok_(not ds.a.has_key('mapper'))

    # has to complain about misshaped samples attributes
    assert_raises(ValueError, Dataset.from_wizard, samples, labels + labels)

    # check that we actually have attributes of the expected type
    ok_(isinstance(ds.sa['targets'], ArrayCollectable))

    # the dataset will take care of not adding stupid stuff
    assert_raises(ValueError, ds.sa.__setitem__, 'stupid', np.arange(3))
    assert_raises(ValueError, ds.fa.__setitem__, 'stupid', np.arange(4))
    # or change proper attributes to stupid shapes
    try:
        ds.sa.targets = np.arange(3)
    except ValueError:
        pass
    else:
        ok_(False, msg="Assigning value with improper shape to attribute "
                       "did not raise exception.")


def test_labelschunks_access():
    samples = np.arange(12).reshape((4, 3)).view(myarray)
    labels = range(4)
    chunks = [1, 1, 2, 2]
    ds = Dataset.from_wizard(samples, labels, chunks)

    # array subclass survives
    ok_(isinstance(ds.samples, myarray))

    assert_array_equal(ds.targets, labels)
    assert_array_equal(ds.chunks, chunks)

    # moreover they should point to the same thing
    ok_(ds.targets is ds.sa.targets)
    ok_(ds.targets is ds.sa['targets'].value)
    ok_(ds.chunks is ds.sa.chunks)
    ok_(ds.chunks is ds.sa['chunks'].value)

    # assignment should work at all levels including 1st
    ds.targets = chunks
    assert_array_equal(ds.targets, chunks)
    ok_(ds.targets is ds.sa.targets)
    ok_(ds.targets is ds.sa['targets'].value)

    # test broadcasting
    # but not for plain scalars
    assert_raises(ValueError, ds.set_attr, 'sa.bc', 5)
    # and not for plain plain str
    assert_raises(TypeError, ds.set_attr, 'sa.bc', "mike")
    # but for any iterable of len == 1
    ds.set_attr('sa.bc', (5,))
    ds.set_attr('sa.dc', ["mike"])
    assert_array_equal(ds.sa.bc, [5] * len(ds))
    assert_array_equal(ds.sa.dc, ["mike"] * len(ds))


@reseed_rng()
def test_ex_from_masked():
    ds = Dataset.from_wizard(samples=np.atleast_2d(np.arange(5)).view(myarray),
                             targets=1, chunks=1)
    # simple sequence has to be a single pattern
    assert_equal(ds.nsamples, 1)
    # array subclass survives
    ok_(isinstance(ds.samples, myarray))

    # check correct pattern layout (1x5)
    assert_array_equal(ds.samples, [[0, 1, 2, 3, 4]])

    # check for single label and origin
    assert_array_equal(ds.targets, [1])
    assert_array_equal(ds.chunks, [1])

    # now try adding pattern with wrong shape
    assert_raises(ValueError, vstack,
                  (ds, Dataset.from_wizard(np.ones((2, 3)), targets=1, chunks=1)))

    # now add two real patterns
    ds = vstack((ds, Dataset.from_wizard(np.random.standard_normal((2, 5)),
                                         targets=2, chunks=2)))
    assert_equal(ds.nsamples, 3)
    assert_array_equal(ds.targets, [1, 2, 2])
    assert_array_equal(ds.chunks, [1, 2, 2])

    # test unique class labels
    ds = vstack((ds, Dataset.from_wizard(np.random.standard_normal((2, 5)),
                                         targets=3, chunks=5)))
    assert_array_equal(ds.sa['targets'].unique, [1, 2, 3])

    # test wrong attributes length
    assert_raises(ValueError, Dataset.from_wizard,
                  np.random.standard_normal((4, 2, 3, 4)), targets=[1, 2, 3],
                  chunks=2)
    assert_raises(ValueError, Dataset.from_wizard,
                  np.random.standard_normal((4, 2, 3, 4)), targets=[1, 2, 3, 4],
                  chunks=[2, 2, 2])

    # no test one that is using from_masked
    ds = datasets['3dlarge']
    for a in ds.sa:
        assert_equal(len(ds.sa[a].value), len(ds))
    for a in ds.fa:
        assert_equal(len(ds.fa[a].value), ds.nfeatures)


def test_shape_conversion():
    ds = Dataset.from_wizard(np.arange(24).reshape((2, 3, 4)).view(myarray),
                             targets=1, chunks=1)
    # array subclass survives
    ok_(isinstance(ds.samples, myarray))

    assert_equal(ds.nsamples, 2)
    assert_equal(ds.samples.shape, (2, 12))
    assert_array_equal(ds.samples, [range(12), range(12, 24)])


@reseed_rng()
def test_multidim_attrs():
    samples = np.arange(24).reshape(2, 3, 4)
    # have a dataset with two samples -- mapped from 2d into 1d
    # but have 2d labels and 3d chunks -- whatever that is
    ds = Dataset.from_wizard(samples.copy(),
                             targets=samples.copy(),
                             chunks=np.random.normal(size=(2, 10, 4, 2)))
    assert_equal(ds.nsamples, 2)
    assert_equal(ds.nfeatures, 12)
    assert_equal(ds.sa.targets.shape, (2, 3, 4))
    assert_equal(ds.sa.chunks.shape, (2, 10, 4, 2))

    # try slicing
    subds = ds[0]
    assert_equal(subds.nsamples, 1)
    assert_equal(subds.nfeatures, 12)
    assert_equal(subds.sa.targets.shape, (1, 3, 4))
    assert_equal(subds.sa.chunks.shape, (1, 10, 4, 2))

    # add multidim feature attr
    fattr = ds.mapper.forward(samples)
    assert_equal(fattr.shape, (2, 12))
    # should puke -- first axis is #samples
    assert_raises(ValueError, ds.fa.__setitem__, 'moresamples', fattr)
    # but that should be fine
    ds.fa['moresamples'] = fattr.T
    assert_equal(ds.fa.moresamples.shape, (12, 2))



def test_samples_shape():
    ds = Dataset.from_wizard(np.ones((10, 2, 3, 4)), targets=1, chunks=1)
    ok_(ds.samples.shape == (10, 24))

    # what happens to 1D samples
    ds = Dataset(np.arange(5))
    assert_equal(ds.shape, (5, 1))
    assert_equal(ds.nfeatures, 1)


def test_basic_datamapping():
    samples = np.arange(24).reshape((4, 3, 2)).view(myarray)

    ds = Dataset.from_wizard(samples)

    # array subclass survives
    ok_(isinstance(ds.samples, myarray))

    # mapper should end up in the dataset
    ok_(ds.a.has_key('mapper'))

    # check correct mapping
    ok_(ds.nsamples == 4)
    ok_(ds.nfeatures == 6)


def test_ds_shallowcopy():
    # lets use some instance of somewhat evolved dataset
    ds = normal_feature_dataset()
    ds.samples = ds.samples.view(myarray)

    # SHALLOW copy the beast
    ds_ = copy.copy(ds)
    # verify that we have the same data
    assert_array_equal(ds.samples, ds_.samples)
    assert_array_equal(ds.targets, ds_.targets)
    assert_array_equal(ds.chunks, ds_.chunks)

    # array subclass survives
    ok_(isinstance(ds_.samples, myarray))


    # modify and see that we actually DO change the data in both
    ds_.samples[0, 0] = 1234
    assert_array_equal(ds.samples, ds_.samples)
    assert_array_equal(ds.targets, ds_.targets)
    assert_array_equal(ds.chunks, ds_.chunks)

    ds_.sa.targets[0] = 'ab'
    ds_.sa.chunks[0] = 234
    assert_array_equal(ds.samples, ds_.samples)
    assert_array_equal(ds.targets, ds_.targets)
    assert_array_equal(ds.chunks, ds_.chunks)
    ok_(ds.sa.targets[0] == 'ab')
    ok_(ds.sa.chunks[0] == 234)

    # XXX implement me
    #ok_(np.any(ds.uniquetargets != ds_.uniquetargets))
    #ok_(np.any(ds.uniquechunks != ds_.uniquechunks))


def test_ds_deepcopy():
    # lets use some instance of somewhat evolved dataset
    ds = normal_feature_dataset()
    ds.samples = ds.samples.view(myarray)
    # Clone the beast
    ds_ = ds.copy()
    # array subclass survives
    ok_(isinstance(ds_.samples, myarray))

    # verify that we have the same data
    assert_array_equal(ds.samples, ds_.samples)
    assert_array_equal(ds.targets, ds_.targets)
    assert_array_equal(ds.chunks, ds_.chunks)

    # modify and see if we don't change data in the original one
    ds_.samples[0, 0] = 1234
    ok_(np.any(ds.samples != ds_.samples))
    assert_array_equal(ds.targets, ds_.targets)
    assert_array_equal(ds.chunks, ds_.chunks)

    ds_.sa.targets = np.hstack(([123], ds_.targets[1:]))
    ok_(np.any(ds.samples != ds_.samples))
    ok_(np.any(ds.targets != ds_.targets))
    assert_array_equal(ds.chunks, ds_.chunks)

    ds_.sa.chunks = np.hstack(([1234], ds_.chunks[1:]))
    ok_(np.any(ds.samples != ds_.samples))
    ok_(np.any(ds.targets != ds_.targets))
    ok_(np.any(ds.chunks != ds_.chunks))

    # XXX implement me
    #ok_(np.any(ds.uniquetargets != ds_.uniquetargets))
    #ok_(np.any(ds.uniquechunks != ds_.uniquechunks))

@sweepargs(dsp=datasets.items())
def test_ds_array(dsp):
    # When dataset
    dsname, ds = dsp
    if dsname != 'hollow':
        ok_(np.asarray(ds) is ds.samples,
            msg="Must have been the same on %s=%s" % dsp)
    else:
        ok_(np.asarray(ds) is not ds.samples,
            msg="Should have not been the same on %s=%s" % dsp)
    ok_(np.array(ds) is not ds.samples,
        msg="Copy should have been created on array(), %s=%s" % dsp)


def test_mergeds():
    data0 = Dataset.from_wizard(np.ones((5, 5)), targets=1)
    data0.fa['one'] = np.ones(5)
    data1 = Dataset.from_wizard(np.ones((5, 5)), targets=1, chunks=1)
    data1.fa['one'] = np.zeros(5)
    data2 = Dataset.from_wizard(np.ones((3, 5)), targets=2, chunks=1)
    data3 = Dataset.from_wizard(np.ones((4, 5)), targets=2)
    data4 = Dataset.from_wizard(np.ones((2, 5)), targets=3, chunks=2)
    data4.fa['test'] = np.arange(5)

    merged = vstack((data1.copy(), data2))

    ok_(merged.nfeatures == 5)
    l12 = [1] * 5 + [2] * 3
    l1 = [1] * 8
    ok_((merged.targets == l12).all())
    ok_((merged.chunks == l1).all())

    data_append = vstack((data1.copy(), data2))

    ok_(data_append.nfeatures == 5)
    ok_((data_append.targets == l12).all())
    ok_((data_append.chunks == l1).all())

    #
    # vstacking
    #
    if __debug__:
        # we need the same samples attributes in both datasets
        assert_raises(ValueError, vstack, (data2, data3))

        # tested only in __debug__
        assert_raises(ValueError, vstack, (data0, data1, data2, data3))
    datasets = (data1, data2, data4)
    merged = vstack(datasets)
    assert_equal(merged.shape,
                 (np.sum([len(ds) for ds in datasets]), data1.nfeatures))
    assert_true('test' in merged.fa)
    assert_array_equal(merged.sa.targets, [1] * 5 + [2] * 3 + [3] * 2)

    #
    # hstacking
    #
    assert_raises(ValueError, hstack, datasets)
    datasets = (data0, data1)
    merged = hstack(datasets)
    assert_equal(merged.shape,
                 (len(data1), np.sum([ds.nfeatures for ds in datasets])))
    assert_true('chunks' in merged.sa)
    assert_array_equal(merged.fa.one, [1] * 5 + [0] * 5)

def test_hstack():
    """Additional tests for hstacking of datasets
    """
    ds3d = datasets['3dsmall']
    nf1 = ds3d.nfeatures
    nf3 = 3 * nf1
    ds3dstacked = hstack((ds3d, ds3d, ds3d))
    ok_(ds3dstacked.nfeatures == nf3)
    for fav in ds3dstacked.fa.itervalues():
        v = fav.value
        ok_(len(v) == nf3)
        assert_array_equal(v[:nf1], v[nf1:2 * nf1])
        assert_array_equal(v[2 * nf1:], v[nf1:2 * nf1])

def test_stack_add_dataset_attributes():
    data0 = Dataset.from_wizard(np.ones((5, 5)), targets=1)
    data0.a['one'] = np.ones(2)
    data0.a['two'] = 2
    data0.a['three'] = 'three'
    data0.a['common'] = range(10)
    data0.a['array'] = np.arange(10)
    data1 = Dataset.from_wizard(np.ones((5, 5)), targets=1)
    data1.a['one'] = np.ones(3)
    data1.a['two'] = 3
    data1.a['four'] = 'four'
    data1.a['common'] = range(10)
    data1.a['array'] = np.arange(10)


    vstacker = lambda x: vstack((data0, data1), a=x)
    hstacker = lambda x: hstack((data0, data1), a=x)

    add_params = (1, None, 'unique', 'uniques', 'all', 'drop_nonunique')

    for stacker in (vstacker, hstacker):
        for add_param in add_params:
            if add_param == 'unique':
                assert_raises(DatasetError, stacker, add_param)
                continue

            r = stacker(add_param)

            if add_param == 1:
                assert_array_equal(data1.a.one, r.a.one)
                assert_equal(r.a.two, 3)
                assert_equal(r.a.four, 'four')
                assert_true('three' not in r.a.keys())
                assert_true('array' in r.a.keys())
            elif add_param == 'uniques':
                assert_equal(set(r.a.keys()),
                             set(['one', 'two', 'three',
                                  'four', 'common', 'array']))
                assert_equal(r.a.two, (2, 3))
                assert_equal(r.a.four, ('four',))
            elif add_param == 'all':
                assert_equal(set(r.a.keys()),
                             set(['one', 'two', 'three',
                                  'four', 'common', 'array']))
                assert_equal(r.a.two, (2, 3))
                assert_equal(r.a.three, ('three', None))
            elif add_param == 'drop_nonunique':
                assert_equal(set(r.a.keys()),
                             set(['common', 'three', 'four', 'array']))
                assert_equal(r.a.three, 'three')
                assert_equal(r.a.four, 'four')
                assert_equal(r.a.common, range(10))
                assert_array_equal(r.a.array, np.arange(10))


def test_unique_stack():
    data = Dataset(np.reshape(np.arange(24), (4, 6)),
                        sa=dict(x=[0, 1, 0, 1]),
                        fa=dict(y=[x for x in 'abccba']))

    sa_stack = stack_by_unique_sample_attribute(data, 'x')
    assert_equal(sa_stack.shape, (2, 12))
    assert_array_equal(sa_stack.fa.x, [0] * 6 + [1] * 6)
    assert_array_equal(sa_stack.fa.y, [x for x in 'abccbaabccba'])

    fa_stack = stack_by_unique_feature_attribute(data, 'y')
    assert_equal(fa_stack.shape, (12, 2))
    assert_array_equal(fa_stack.sa.x, [0, 1] * 6)
    assert_array_equal(fa_stack.sa.y, [y for y in 'aaaabbbbcccc'])
    #assert_array_equal(fa_stack.fa.y,[''])

    # check values match the fa or sa
    for i in xrange(4):
        for j in xrange(6):
            d = data[i, j]
            for k, other in enumerate((sa_stack, fa_stack)):
                msk = other.samples == d.samples
                ii, jj = np.nonzero(msk) # find matching indices in other

                o = other[ii, jj]
                coll = [o.fa, o.sa][k]

                assert_equal(coll.x, d.sa.x)
                assert_equal(coll.y, d.fa.y)

    ystacker = lambda y: lambda x: stack_by_unique_feature_attribute(x, y)
    assert_raises(KeyError, ystacker('z'), data)

    data.fa['z'] = [z for z in '123451']
    assert_raises(ValueError, ystacker('z'), data)

def test_mergeds2():
    """Test composition of new datasets by addition of existing ones
    """
    data = dataset_wizard([range(5)], targets=1, chunks=1)

    assert_array_equal(data.UT, [1])

    # simple sequence has to be a single pattern
    assert_equal(data.nsamples, 1)
    # check correct pattern layout (1x5)
    assert_array_equal(data.samples, [[0, 1, 2, 3, 4]])

    # check for single labels and origin
    assert_array_equal(data.targets, [1])
    assert_array_equal(data.chunks, [1])

    # now try adding pattern with wrong shape
    assert_raises(ValueError,
                  vstack,
                  (data, dataset_wizard(np.ones((2, 3)), targets=1, chunks=1)))

    # now add two real patterns
    dss = datasets['uni2large'].samples
    data = vstack((data, dataset_wizard(dss[:2, :5], targets=2, chunks=2)))
    assert_equal(data.nfeatures, 5)
    assert_array_equal(data.targets, [1, 2, 2])
    assert_array_equal(data.chunks, [1, 2, 2])

    # test automatic origins
    data = vstack((data, (dataset_wizard(dss[3:5, :5], targets=3, chunks=[0, 1]))))
    assert_array_equal(data.chunks, [1, 2, 2, 0, 1])

    # test unique class labels
    assert_array_equal(data.UT, [1, 2, 3])

    # test wrong label length
    assert_raises(ValueError, dataset_wizard, dss[:4, :5], targets=[ 1, 2, 3 ],
                                         chunks=2)

    # test wrong origin length
    assert_raises(ValueError, dataset_wizard, dss[:4, :5],
                  targets=[ 1, 2, 3, 4 ], chunks=[ 2, 2, 2 ])


def test_combined_samplesfeature_selection():
    data = dataset_wizard(np.arange(20).reshape((4, 5)).view(myarray),
                   targets=[1, 2, 3, 4],
                   chunks=[5, 6, 7, 8])

    # array subclass survives
    ok_(isinstance(data.samples, myarray))

    ok_(data.nsamples == 4)
    ok_(data.nfeatures == 5)
    sel = data[[0, 3], [1, 2]]
    ok_(sel.nsamples == 2)
    ok_(sel.nfeatures == 2)
    assert_array_equal(sel.targets, [1, 4])
    assert_array_equal(sel.chunks, [5, 8])
    assert_array_equal(sel.samples, [[1, 2], [16, 17]])
    # array subclass survives
    ok_(isinstance(sel.samples, myarray))


    # should yield the same result if done sequentially
    sel2 = data[:, [1, 2]]
    sel2 = sel2[[0, 3]]
    assert_array_equal(sel.samples, sel2.samples)
    ok_(sel2.nsamples == 2)
    ok_(sel2.nfeatures == 2)
    # array subclass survives
    ok_(isinstance(sel.samples, myarray))


    assert_raises(ValueError, data.__getitem__, (1, 2, 3))

    # test correct behavior when selecting just single rows/columns
    single = data[0]
    ok_(single.nsamples == 1)
    ok_(single.nfeatures == 5)
    assert_array_equal(single.samples, [[0, 1, 2, 3, 4]])
    single = data[:, 0]
    ok_(single.nsamples == 4)
    ok_(single.nfeatures == 1)
    assert_array_equal(single.samples, [[0], [5], [10], [15]])
    single = data[1, 1]
    ok_(single.nsamples == 1)
    ok_(single.nfeatures == 1)
    assert_array_equal(single.samples, [[6]])
    # array subclass survives
    ok_(isinstance(single.samples, myarray))


@reseed_rng()
def test_labelpermutation_randomsampling():
    ds = vstack([Dataset.from_wizard(np.ones((5, 10)), targets=range(5), chunks=i)
                    for i in xrange(1, 6)])
    # assign some feature attributes
    ds.fa['roi'] = np.repeat(np.arange(5), 2)
    ds.fa['lucky'] = np.arange(10) % 2
    # use subclass for testing if it would survive
    ds.samples = ds.samples.view(myarray)

    ok_(ds.get_nsamples_per_attr('targets') == {0:5, 1:5, 2:5, 3:5, 4:5})
    sample = ds.random_samples(2)
    ok_(sample.get_nsamples_per_attr('targets').values() == [ 2, 2, 2, 2, 2 ])
    ok_((ds.sa['chunks'].unique == range(1, 6)).all())

@reseed_rng()
def test_masked_featureselection():
    origdata = np.random.standard_normal((10, 2, 4, 3, 5)).view(myarray)
    data = Dataset.from_wizard(origdata, targets=2, chunks=2)

    unmasked = data.samples.copy()
    # array subclass survives
    ok_(isinstance(data.samples, myarray))

    # default must be no mask
    ok_(data.nfeatures == 120)
    ok_(data.a.mapper.forward1(origdata[0]).shape == (120,))

    # check that full mask uses all features
    # this uses auto-mapping of selection arrays in __getitem__
    sel = data[:, np.ones((2, 4, 3, 5), dtype='bool')]
    ok_(sel.nfeatures == data.samples.shape[1])
    ok_(data.nfeatures == 120)

    # check partial array mask
    partial_mask = np.zeros((2, 4, 3, 5), dtype='bool')
    partial_mask[0, 0, 2, 2] = 1
    partial_mask[1, 2, 2, 0] = 1

    sel = data[:, partial_mask]
    ok_(sel.nfeatures == 2)

    # check that feature selection does not change source data
    ok_(data.nfeatures == 120)
    ok_(data.a.mapper.forward1(origdata[0]).shape == (120,))

    # check selection with feature list
    sel = data[:, [0, 37, 119]]
    ok_(sel.nfeatures == 3)

    # check size of the masked samples
    ok_(sel.samples.shape == (10, 3))

    # check that the right features are selected
    assert_array_equal(unmasked[:, [0, 37, 119]], sel.samples)


@reseed_rng()
def test_origmask_extraction():
    origdata = np.random.standard_normal((10, 2, 4, 3))
    data = Dataset.from_wizard(origdata, targets=2, chunks=2)

    # check with custom mask
    sel = data[:, 5]
    ok_(sel.samples.shape[1] == 1)


@reseed_rng()
def test_feature_masking():
    mask = np.zeros((5, 3), dtype='bool')
    mask[2, 1] = True
    mask[4, 0] = True
    data = Dataset.from_wizard(np.arange(60).reshape((4, 5, 3)),
                               targets=1, chunks=1, mask=mask)

    # check simple masking
    ok_(data.nfeatures == 2)

    # selection should be idempotent
    ok_(data[:, mask].nfeatures == data.nfeatures)
    # check that correct feature get selected
    assert_array_equal(data[:, 1].samples[:, 0], [12, 27, 42, 57])
    # XXX put back when coord -> fattr is implemented
    #ok_(tuple(data[:, 1].a.mapper.getInId(0)) == (4, 0))
    ok_(data[:, 1].a.mapper.forward1(mask).shape == (1,))

    # check sugarings
    # XXX put me back
    #self.assertTrue(np.all(data.I == data.origids))
    assert_array_equal(data.C, data.chunks)
    assert_array_equal(data.UC, np.unique(data.chunks))
    assert_array_equal(data.T, data.targets)
    assert_array_equal(data.UT, np.unique(data.targets))
    assert_array_equal(data.S, data.samples)
    assert_array_equal(data.O, data.mapper.reverse(data.samples))


def test_origid_handling():
    ds = dataset_wizard(np.atleast_2d(np.arange(35)).T)
    ds.init_origids('both')
    ok_(ds.nsamples == 35)
    assert_equal(len(np.unique(ds.sa.origids)), 35)
    assert_equal(len(np.unique(ds.fa.origids)), 1)
    selector = [3, 7, 10, 15]
    subds = ds[selector]
    assert_array_equal(subds.sa.origids, ds.sa.origids[selector])

    # Now if we request new origids if they are present we could
    # expect different behavior
    assert_raises(ValueError, subds.init_origids, 'both', mode='raises')
    sa_origids = subds.sa.origids.copy()
    fa_origids = subds.fa.origids.copy()
    for s in ('both', 'samples', 'features'):
        assert_raises(RuntimeError, subds.init_origids, s, mode='raise')
        subds.init_origids(s, mode='existing')
        # we should have the same origids as before
        assert_array_equal(subds.sa.origids, sa_origids)
        assert_array_equal(subds.fa.origids, fa_origids)

    # Lets now change, which should be default behavior
    subds.init_origids('both')
    assert_equal(len(sa_origids), len(subds.sa.origids))
    assert_equal(len(fa_origids), len(subds.fa.origids))
    # values should change though
    ok_((sa_origids != subds.sa.origids).any())
    ok_((fa_origids != subds.fa.origids).any())

def test_idhash():
    ds = dataset_wizard(np.arange(12).reshape((4, 3)),
                 targets=1, chunks=1)
    origid = ds.idhash
    #XXX BUG -- no assurance that labels would become an array... for now -- do manually
    ds.targets = np.array([3, 1, 2, 3])   # change all labels
    ok_(origid != ds.idhash,
                    msg="Changing all targets should alter dataset's idhash")

    origid = ds.idhash

    z = ds.targets[1]
    assert_equal(origid, ds.idhash,
                 msg="Accessing shouldn't change idhash")
    z = ds.chunks
    assert_equal(origid, ds.idhash,
                 msg="Accessing shouldn't change idhash")
    z[2] = 333
    ok_(origid != ds.idhash,
        msg="Changing value in attribute should change idhash")

    origid = ds.idhash
    ds.samples[1, 1] = 1000
    ok_(origid != ds.idhash,
        msg="Changing value in data should change idhash")

    origid = ds.idhash
    orig_labels = ds.targets #.copy()
    ds.sa.targets = range(len(ds))
    ok_(origid != ds.idhash,
        msg="Chaging attribute also changes idhash")

    ds.targets = orig_labels
    ok_(origid == ds.idhash,
        msg="idhash should be restored after reassigning orig targets")


def test_arrayattributes():
    samples = np.arange(12).reshape((4, 3))
    labels = range(4)
    chunks = [1, 1, 2, 2]
    ds = dataset_wizard(samples, labels, chunks)

    for a in (ds.samples, ds.targets, ds.chunks):
        ok_(isinstance(a, np.ndarray))

    ds.targets = labels
    ok_(isinstance(ds.targets, np.ndarray))

    ds.chunks = chunks
    ok_(isinstance(ds.chunks, np.ndarray))

    # we should allow assigning somewhat more complex
    # iterables -- use ndarray of dtype object then
    # and possibly spit out a warning
    ds.sa['complex_list'] = [[], [1], [1, 2], []]
    ok_(ds.sa.complex_list.dtype == object)

    # but incorrect length should still fail
    assert_raises(ValueError, ds.sa.__setitem__,
                  'complex_list2', [[], [1], [1, 2]])


def test_repr():
    attr_repr = "SampleAttribute(name='TestAttr', doc='my own test', " \
                                "value=array([0, 1, 2, 3, 4]), length=None)"
    sattr = SampleAttribute(name='TestAttr', doc='my own test',
                            value=np.arange(5))
    # check precise formal representation
    ok_(repr(sattr) == attr_repr)
    # check that it actually works as a Python expression
    from numpy import array
    eattr = eval(repr(sattr))
    ok_(repr(eattr), attr_repr)

    # should also work for a simple dataset
    # Does not work due to bug in numpy:
    #  python -c "from numpy import *; print __version__; r=repr(array(['s', None])); print r; eval(r)"
    # would give "array([s, None], dtype=object)" without '' around s
    #ds = datasets['uni2small']
    ds = Dataset([[0, 1]],
                 a={'dsa1': 'v1'},
                 sa={'targets': [0]},
                 fa={'targets': ['b', 'n']})
    ds_repr = repr(ds)
    cfg_repr = cfg.get('datasets', 'repr', 'full')
    if cfg_repr == 'full':
        try:
            ok_(repr(eval(ds_repr)) == ds_repr)
        except SyntaxError, e:
            raise AssertionError, "%r cannot be evaluated" % ds_repr
    elif cfg_repr == 'str':
        ok_(str(ds) == ds_repr)
    else:
        raise AssertionError('Unknown kind of datasets.repr configuration %r'
                             % cfg_repr)

def test_str():
    args = (np.arange(12, dtype=np.int8).reshape((4, 3)),
             range(4),
             [1, 1, 2, 2])
    for iargs in range(1, len(args)):
        ds = dataset_wizard(*(args[:iargs]))
        ds_s = str(ds)
        ok_(ds_s.startswith('<Dataset: 4x3@int8'))
        ok_(ds_s.endswith('>'))

def is_bsr(x):
    """Helper function to check if instance is bsr_matrix if such is
    avail at all
    """
    import scipy.sparse as sparse
    return hasattr(sparse, 'bsr_matrix') and isinstance(x, sparse.bsr_matrix)

def test_other_samples_dtypes():
    skip_if_no_external('scipy')
    import scipy.sparse as sparse
    dshape = (4, 3)
    # test for ndarray, custom ndarray-subclass, matrix,
    # and all sparse matrix types we know
    stypes = [np.arange(np.prod(dshape)).reshape(dshape),
              np.arange(np.prod(dshape)).reshape(dshape).view(myarray),
              np.matrix(np.arange(np.prod(dshape)).reshape(dshape)),
              sparse.csc_matrix(np.arange(np.prod(dshape)).reshape(dshape)),
              sparse.csr_matrix(np.arange(np.prod(dshape)).reshape(dshape))]
    if hasattr(sparse, 'bsr_matrix'):
        stypes += [
              # BSR cannot be sliced, but is more efficient for sparse
              # arithmetic operations than CSC pr CSR
              sparse.bsr_matrix(np.arange(np.prod(dshape)).reshape(dshape))]
              # LIL and COO are best for constructing matrices, not for
              # doing something with them
              #sparse.lil_matrix(np.arange(np.prod(dshape)).reshape(dshape)),
              #sparse.coo_matrix(np.arange(np.prod(dshape)).reshape(dshape)),
              # DOK doesn't allow duplicates and is bad at array-like slicing
              #sparse.dok_matrix(np.arange(np.prod(dshape)).reshape(dshape)),
              # DIA only has diagonal storage and cannot be sliced
              #sparse.dia_matrix(np.arange(np.prod(dshape)).reshape(dshape))]

    # it needs to have .shape (the only way to get len(sparse))
    for s in stypes:
        ds = Dataset(s)
        # nothing happended to the original dtype
        assert_equal(type(s), type(ds.samples))
        # no shape change
        assert_equal(ds.shape, dshape)
        assert_equal(ds.nsamples, dshape[0])
        assert_equal(ds.nfeatures, dshape[1])

        # sparse doesn't work like an array
        if sparse.isspmatrix(ds.samples):
            assert_raises(RuntimeError, np.mean, ds)
        else:
            # need to convert results, since matrices return matrices
            assert_array_equal(np.mean(ds, axis=0),
                               np.array(np.mean(ds.samples, axis=0)).squeeze())

        # select subset and see what happens
        # bsr type doesn't support first axis slicing
        if is_bsr(s):
            assert_raises(NotImplementedError, ds.__getitem__, [0])
        elif versions['scipy'] <= '0.6.0' and sparse.isspmatrix(ds.samples):
            assert_raises(IndexError, ds.__getitem__, [0])
        else:
            sel = ds[1:3]
            assert_equal(sel.shape, (2, dshape[1]))
            assert_equal(type(s), type(sel.samples))
            if sparse.isspmatrix(sel.samples):
                assert_array_equal(sel.samples[1].todense(),
                                   ds.samples[2].todense())
            else:
                assert_array_equal(sel.samples[1],
                                   ds.samples[2])

        # feature selection
        if is_bsr(s):
            assert_raises(NotImplementedError, ds.__getitem__, (slice(None), 0))
        elif versions['scipy'] <= '0.6.0' and sparse.isspmatrix(ds.samples):
            assert_raises(IndexError, ds.__getitem__, (slice(None), 0))
        else:
            sel = ds[:, 1:3]
            assert_equal(sel.shape, (dshape[0], 2))
            assert_equal(type(s), type(sel.samples))
            if sparse.isspmatrix(sel.samples):
                assert_array_equal(sel.samples[:, 1].todense(),
                        ds.samples[:, 2].todense())
            else:
                assert_array_equal(sel.samples[:, 1],
                        ds.samples[:, 2])


        # what we don't do
        class voodoo:
            dtype = 'fancy'
        # voodoo
        assert_raises(ValueError, Dataset, voodoo())
        # crippled
        assert_raises(ValueError, Dataset, np.array(5))

        # things that might behave in surprising ways
        # lists -- first axis is samples, hence single feature
        ds = Dataset(range(5))
        assert_equal(ds.nfeatures, 1)
        assert_equal(ds.shape, (5, 1))
        # arrays of objects
        data = np.array([{}, {}])
        ds = Dataset(data)
        assert_equal(ds.shape, (2, 1))
        assert_equal(ds.nsamples, 2)
        # Nothing to index, hence no features
        assert_equal(ds.nfeatures, 1)


@sweepargs(ds=datasets.values() + [
    Dataset(np.array([None], dtype=object)),
    dataset_wizard(np.arange(3), targets=['a', 'bc', 'd'], chunks=np.arange(3)),
    dataset_wizard(np.arange(4), targets=['a', 'bc', 'a', 'bc'], chunks=[1, 1, 2, 2]),
    ])
def test_dataset_summary(ds):
    s = ds.summary()
    ok_(s.startswith(str(ds)[1:-1])) # we strip surrounding '<...>'
    # TODO: actual test of what was returned; to do that properly
    #       RF the summary() so it is a dictionary

    summaries = []
    if 'targets' in ds.sa:
        summaries += ['Sequence statistics']
        if 'chunks' in ds.sa:
            summaries += ['Summary for targets', 'Summary for chunks']

    # By default we should get all kinds of summaries
    if not 'Number of unique targets >' in s:
        for summary in summaries:
            ok_(summary in s)

    # If we give "wrong" targets_attr we should see none of summaries
    s2 = ds.summary(targets_attr='bogus')
    for summary in summaries:
        ok_(not summary in s2)

@nodebug(['ID_IN_REPR', 'MODULE_IN_REPR'])
@with_tempfile(suffix='.hdf5')
def test_h5py_io(dsfile):
    skip_if_no_external('h5py')

    # store random dataset to file
    ds = datasets['3dlarge']
    ds.save(dsfile)

    # reload and check for identity
    ds2 = Dataset.from_hdf5(dsfile)
    assert_array_equal(ds.samples, ds2.samples)
    for attr in ds.sa:
        assert_array_equal(ds.sa[attr].value, ds2.sa[attr].value)
    for attr in ds.fa:
        assert_array_equal(ds.fa[attr].value, ds2.fa[attr].value)
    assert_true(len(ds.a.mapper), 2)

    # since we have no __equal__ do at least some comparison
    assert_equal(repr(ds.a.mapper), repr(ds2.a.mapper))

    if __debug__:
        # debug mode needs special test as it enhances the repr output
        # with module info and id() appendix for objects
        #
        # INCORRECT slicing (:-1) since without any hash it results in
        # empty list -- moreover we seems of not reporting ids with #
        # any longer
        #
        #assert_equal('#'.join(repr(ds.a.mapper).split('#')[:-1]),
        #             '#'.join(repr(ds2.a.mapper).split('#')[:-1]))
        pass


def test_all_equal():
    # all these values are supposed to be different from each other
    # but equal to themselves
    a = np.random.normal(size=(10, 10)) + 1000.
    b = np.zeros((10, 10))
    c = np.zeros(10)
    d = np.zeros(11)
    e = 0
    f = None
    g = True
    h = ''
    i = 'a'

    values = [a, b, c, d, e, f, g, h, i]
    for ii, v in enumerate(values):
        for jj, w in enumerate(values):
            assert_equal(all_equal(v, w), ii == jj)

    # ensure that this function behaves like the 
    # standard python '==' comparator for singulars
    singulars = [0, None, True, False, '', 1, 'a']
    for v in singulars:
        for w in singulars:
            assert_equal(all_equal(v, w), v == w)


def test_hollow_samples():
    sshape = (10, 5)
    ds = Dataset(HollowSamples(sshape, dtype=int),
                 sa={'targets': np.tile(['one', 'two'], sshape[0] / 2)})
    assert_equal(ds.shape, sshape)
    assert_equal(ds.samples.dtype, int)
    # should give us features [1,3] and samples [2,3,5]
    mds = ds[[2, 3, 5], 1::2]
    assert_array_equal(mds.samples.sid, [2, 3, 5])
    assert_array_equal(mds.samples.fid, [1, 3])
    assert_equal(mds.shape, (3, 2))
    assert_equal(ds.samples.dtype, mds.samples.dtype)
    # orig should stay pristine
    assert_equal(ds.samples.dtype, int)
    assert_equal(ds.shape, sshape)

def test_assign_sa():
    # https://github.com/PyMVPA/PyMVPA/issues/149
    ds = Dataset(np.arange(6).reshape((2,-1)), sa=dict(targets=range(2)))
    ds.sa['task'] = ds.sa['targets']
    # so it should be a new collectable now
    assert_equal(ds.sa['task'].name, 'task')
    assert_equal(ds.sa['targets'].name, 'targets') # this lead to issue reported in 149
    assert('task' in ds.sa.keys())
    assert('targets' in ds.sa.keys())
    ds1 = ds[:, 1]
    assert('task' in ds1.sa.keys())
    assert('targets' in ds1.sa.keys()) # issue reported in 149
    assert_equal(ds1.sa['task'].name, 'task')
    assert_equal(ds1.sa['targets'].name,'targets')

########NEW FILE########
__FILENAME__ = test_dataset_formats
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for the dataset implementation'''

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets

from mvpa2.datasets.formats import *

import tempfile
import os

def test_format_lightsvm_basic():
    # Just doing basic testing for the silliest of usages -- just
    # dumping / loading data back without any customization via
    # arguments
    for dsname in ['uni2small', 'uni3small', 'chirp_linear']:
        ds = datasets[dsname]
        f = tempfile.NamedTemporaryFile(delete=False)
        am = to_lightsvm_format(ds, f)
        f.close()
        f_ = open(f.name, 'r')
        ds_ = from_lightsvm_format(f_, am=am)
        f_.close()
        os.unlink(f.name)
        # Lets do checks now
        ok_(ds.targets.dtype == ds_.targets.dtype)
        if ds.targets.dtype.char in ['i', 'S', 'U']:
            assert_array_equal(ds.targets, ds_.targets)
        else:
            assert_array_almost_equal(ds.targets, ds_.targets, decimal=3)
        assert_array_almost_equal(ds.samples, ds_.samples)

########NEW FILE########
__FILENAME__ = test_datasrcs
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA's data sources"""

from mvpa2.testing import *

def test_sklearn_data_wrappers():
    skip_if_no_external('skl')
    import mvpa2.datasets.sources as mvpads
    if externals.versions['skl'] >= '0.9':
        from sklearn import datasets as skldata
    else:
        from scikits.learn import datasets as skldata
    import inspect
    found_fx = 0
    for fx in skldata.__dict__:
        if not (fx.startswith('make_') or fx.startswith('load_')) \
                or fx in ['load_filenames', 'load_files',
                          'load_sample_image', 'load_sample_images',
                          'load_svmlight_files', 'load_svmlight_file']:
            continue
        found_fx += 1
        # fx() signatures must be the same
        assert_equal(inspect.getargspec(getattr(skldata, fx)),
                     inspect.getargspec(getattr(mvpads, 'skl_%s' % fx[5:])))
        if fx in ('load_iris',):
            # add this one if sklearn issue #2865 is resolved
            # 'load_boston'):
            assert_array_equal(getattr(skldata, fx)()['data'],
                               getattr(mvpads, 'skl_%s' % fx[5:])().samples)
    # if we do not get a whole bunch, something changed
    assert_true(found_fx > 15)

########NEW FILE########
__FILENAME__ = test_dcov
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for dCOV and associated functions"""

from mvpa2.testing import *

# For testing
from nose.tools import ok_
from numpy.testing import assert_array_almost_equal
from mvpa2.testing.datasets import get_random_rotation

from mvpa2.misc.dcov import _euclidean_distances, dCOV, dcorcoef

from mvpa2.base import externals
if externals.exists('cran-energy'):
    from mvpa2.misc.dcov import dCOV_R


@reseed_rng()
def test_euclidean_distances():
    x = np.random.normal(size=(4, 10)) + np.random.normal() * 10
    d = _euclidean_distances(x, uv=True)
    # trust no one!
    distances = np.zeros((4, 10, 10))
    for ix, x_ in enumerate(x.T):
        for iy, y_ in enumerate(x.T):
            distances[:, ix, iy] = np.sqrt((x_ - y_) ** 2)
    assert_array_equal(d, distances)


def test_dCOV_against_R_energy():
    skip_if_no_external('cran-energy')

    for N in xrange(1, 10): # sweep through size of the first data
        # We will compare to R implementation
        M, T = 4, 30
        x = np.random.normal(size=(N, T)) + np.random.normal() * 10
        R = np.random.normal(size=(N, M))
        y = 10 * np.dot(R.T, x) + np.random.normal(size=(M, T)) \
            + np.random.normal(size=(M,))[:, None] # offset

        # To assure that works for not all_est
        pdCovs = dCOV(x, y, all_est=False)
        dCovs = dCOV_R(x, y, all_est=False)
        assert_array_almost_equal(pdCovs, dCovs)

        for uv in True, False:
            for out, outp in zip(dCOV_R(x, y, uv=uv),
                                 dCOV(x, y, uv=uv)):
                assert_array_almost_equal(out, outp)

@labile(5, 1)
def test_dCOV():
    # Few simple tests to verify that the measure seems to be ok
    for N in xrange(1, 10): # sweep through size of the first data
        # We will compare to R implementation
        M, T = 4, 100
        x = np.random.normal(size=(N, T)) + np.random.normal() * 10
        R = np.random.normal(size=(N, M))

        # linearly dependent variable after rotation
        dCov, dCor, _, _ = dCOV(x, 10 * np.dot(R.T, x))
        ok_(dCor > 0.7)           # should be really high but might fluctuate

        # completely independent variable
        dCov, dCor, _, _ = dCOV(x, np.random.normal(size=x.shape))
        # more dimension in x -- more uncertainty that they are
        # independent below is a heuristic (for T=100) and we should
        # just implement proper bootstrap significance estimation for
        # dCor
        ok_(dCor < 0.2 + N / 2.0)           # should be really high but might fluctuate

        # the same variable -- things should match for dCov and dVar's
        dCov, dCor, dVarx, dVary = dCOV(x, x)
        assert_equal(dCov, dVarx)
        assert_equal(dCov, dVary)
        assert_equal(dCor, 1.)
        assert_equal(dcorcoef(x, x), 1)
        #+ np.random.normal(size=(M, T)) \
        #    + np.random.normal(size=(M,))[:, None] # offset

        # Test that would work on vectors
        dCov, dCor, dVarx, dVary = dCOV(np.arange(N), np.sin(np.arange(N) / 3.))
        if N > 1:
            ok_(dCor > 0.6)           # should be really high but might fluctuate
        assert_equal(dcorcoef(np.arange(N), np.sin(np.arange(N) / 3.)), dCor)

########NEW FILE########
__FILENAME__ = test_dochelpers
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA dochelpers"""

from mvpa2.base.dochelpers import single_or_plural, borrowdoc, borrowkwargs

import unittest

if __debug__:
    from mvpa2.base import debug

from mvpa2.testing.tools import SkipTest

class DochelpersTests(unittest.TestCase):

    def test_basic(self):
        self.assertEqual(single_or_plural('a', 'b', 1), 'a')
        self.assertEqual(single_or_plural('a', 'b', 0), 'b')
        self.assertEqual(single_or_plural('a', 'b', 123), 'b')

    def test_borrow_doc(self):

        class A(object):
            def met1(self):
                """met1doc"""
                pass
            def met2(self):
                """met2doc"""
                pass

        class B(object):
            @borrowdoc(A)
            def met1(self):
                pass
            @borrowdoc(A, 'met1')
            def met2(self):
                pass

        self.assertEqual(B.met1.__doc__, A.met1.__doc__)
        self.assertEqual(B.met2.__doc__, A.met1.__doc__)


    def test_borrow_kwargs(self):

        class A(object):
            def met1(self, kp1=None, kp2=1):
                """met1 doc

                Parameters
                ----------
                kp1 : None or int
                  keyword parameter 1
                kp2 : int, optional
                  something
                """
                pass

            def met2(self):
                """met2doc"""
                pass

        class B(object):

            @borrowkwargs(A)
            def met1(self, bu, **kwargs):
                """B.met1 doc

                Parameters
                ----------
                bu
                  description
                **kwargs
                  Same as in A.met1

                Some postamble
                """
                pass

            @borrowkwargs(A, 'met1')
            def met_nodoc(self, **kwargs):
                pass

            @borrowkwargs(A, 'met1')
            def met_nodockwargs(self, bogus=None, **kwargs):
                """B.met_nodockwargs

                Parameters
                ----------
                bogus
                  something
                """
                pass

            if True:
                # Just so we get different indentation level
                @borrowkwargs(A, 'met1', ['kp1'])
                def met_excludes(self, boguse=None, **kwargs):
                    """B.met_excludes

                    Parameters
                    ----------
                    boguse
                      something
                    """
                    pass

        self.assertTrue('B.met1 doc' in B.met1.__doc__)
        for m in (B.met1,
                  B.met_nodoc,
                  B.met_nodockwargs,
                  B.met_excludes):
            docstring = m.__doc__
            self.assertTrue('Parameters' in docstring)
            self.assertTrue(not '*kwargs' in docstring,
                msg="We shouldn't carry kwargs in docstring now,"
                    "Got %r for %s" % (docstring, m))
            self.assertTrue('kp2 ' in docstring)
            self.assertTrue((('kp1 ' in docstring)
                                 ^ (m == B.met_excludes)))
            # indentation should have been squashed properly
            self.assertTrue(not '   ' in docstring)

        # some additional checks to see if we are not loosing anything
        self.assertTrue('Some postamble' in B.met1.__doc__)
        self.assertTrue('B.met_nodockwargs' in B.met_nodockwargs.__doc__)
        self.assertTrue('boguse' in B.met_excludes.__doc__)

    def test_searchlight_doc(self):
        # Searchlight __doc__ revealed issue of multiple enable_ca
        from mvpa2.measures.searchlight import Searchlight
        sldoc = Searchlight.__init__.__doc__
        self.assertEqual(sldoc.count('enable_ca'), 1)
        self.assertEqual(sldoc.count('disable_ca'), 1)


    def test_recursive_reprs(self):
        # https://github.com/PyMVPA/PyMVPA/issues/122

        from mvpa2.base.param import Parameter
        from mvpa2.base.state import ClassWithCollections

        class C1(ClassWithCollections):
            f = Parameter(None)

        class C2(ClassWithCollections):
            p = Parameter(None)
            def trouble(self, results):
                return results

        # provide non-default value of sl
        c1 = C1()
        c2 = C2(p=c1)
        # bind sl's results_fx to hsl's instance method
        c2.params.p.params.f = c2.trouble
        c1id = c2id = mod = ''
        # kaboom -- this should not crash now
        if __debug__:
            if 'ID_IN_REPR' in debug.active:
                from mvpa2.base.dochelpers import _strid
                c1id = _strid(c1)
                c2id = _strid(c2)

            if 'MODULE_IN_REPR' in debug.active:
                mod = 'mvpa2.tests.test_dochelpers.'
                raise SkipTest("TODO: needs similar handling in _saferepr")

        self.assertEqual(
            repr(c2), '%(mod)sC2(p=%(mod)sC1(f=<bound %(mod)sC2%(c2id)s.trouble>)%(c1id)s)%(c2id)s' % locals())

# TODO: more unittests
def suite():  # pragma: no cover
    return unittest.makeSuite(DochelpersTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_eeglab
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA EEGLAB stuff"""

import os.path

from mvpa2.testing import *
from mvpa2 import pymvpa_dataroot
from mvpa2.datasets.eeglab import eeglab_dataset

import tempfile

class MEGTests(unittest.TestCase):

    def test_eeglab_dataset(self):
        data = '''    Fpz Cz Pz
0 30.2 20.3 20.2
2 1.5 1.6 1.72
0 1.1 1.2 1.3
2 2.5 2.6 -0.2
0 -2 -3 1
2 1 2 2.234'''

        # Use this whenever we fully switch to nose to run tests
        #skip_if_no_external('gzip')
        fd, fn = tempfile.mkstemp('eeglab.txt', 'eeglab'); os.close(fd)
        with open(fn, 'w') as f:
            f.write(data)

        eeg = eeglab_dataset(fn)
        os.remove(fn)

        assert_array_equal(set(eeg.channelids), set(['Fpz', 'Cz', 'Pz']))
        assert_array_equal(eeg.timepoints, np.asarray([0., 2.]))

        assert_equal(eeg.nchannels, 3)
        assert_equal(eeg.ntimepoints, 2)

        assert_equal(eeg.nsamples, 3)
        assert_equal(eeg.nfeatures, 6)

        assert_equal(eeg.dt, 2)
        assert_equal(eeg.t0, 0)

        assert_array_equal(eeg.samples[0, 3], 1.5)

        sel_time = eeg[:, eeg.get_features_by_timepoints(lambda x:x > 0)]
        assert_equal(sel_time.ntimepoints, 1)
        assert_equal(sel_time.t0, 2)

        sel_chan = eeg[:, eeg.get_features_by_channelids(['Fpz', 'Pz'])]
        assert_equal(sel_chan.nchannels, 2)
        assert_array_equal(sel_chan.channelids, ['Fpz', 'Pz'])


def suite():  # pragma: no cover
    return unittest.makeSuite(MEGTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_eepdataset
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA EEP dataset"""

import os.path
import numpy as np

from mvpa2 import pymvpa_dataroot
from mvpa2.base import externals
from mvpa2.datasets.eep import eep_dataset, EEPBin

from mvpa2.testing.tools import assert_equal, assert_true, \
     assert_array_almost_equal

def test_eep_load():
    eb = EEPBin(os.path.join(pymvpa_dataroot, 'eep.bin'))

    ds = [ eep_dataset(source, targets=[1, 2]) for source in
            (eb, os.path.join(pymvpa_dataroot, 'eep.bin')) ]

    for d in ds:
        assert_equal(d.nsamples, 2)
        assert_equal(d.nfeatures, 128)
        assert_equal(np.unique(d.fa.channels[4*23:4*23+4]), 'Pz')
        assert_array_almost_equal([np.arange(-0.002, 0.005, 0.002)] * 32,
                                  d.a.mapper.reverse1(d.fa.timepoints))


def test_eep_bin():
    eb = EEPBin(os.path.join(pymvpa_dataroot, 'eep.bin'))

    assert_equal(eb.nchannels, 32)
    assert_equal(eb.nsamples, 2)
    assert_equal(eb.ntimepoints, 4)
    assert_true(eb.t0 - eb.dt < 0.00000001)
    assert_equal(len(eb.channels), 32)
    assert_equal(eb.data.shape, (2, 32, 4))


    # XXX put me back whenever there is a proper resamples()
#     def test_resampling(self):
#         ds = eep_dataset(os.path.join(pymvpa_dataroot, 'eep.bin'),
#                          targets=[1, 2])
#         channelids = np.array(ds.a.channelids).copy()
#         self.assertTrue(np.round(ds.samplingrate) == 500.0)
# 
#         if not externals.exists('scipy'):
#             return
# 
#         # should puke when called with nothing
#         self.assertRaises(ValueError, ds.resample)
# 
#         # now for real -- should divide nsamples into half
#         rds = ds.resample(sr=250, inplace=False)
#         # We should have not changed anything
#         self.assertTrue(np.round(ds.samplingrate) == 500.0)
# 
#         # by default do 'inplace' resampling
#         ds.resample(sr=250)
#         for d in [rds, ds]:
#             self.assertTrue(np.round(d.samplingrate) == 250)
#             self.assertTrue(d.nsamples == 2)
#             self.assertTrue(np.abs((d.a.dt - 1.0/250)/d.a.dt)<1e-5)
#             self.assertTrue(np.all(d.a.channelids == channelids))
#             # lets now see if we still have a mapper
#             self.assertTrue(d.O.shape == (2, len(channelids), 2))

########NEW FILE########
__FILENAME__ = test_emp_null
"""
Test the empirical null estimator.

Borrowed from NiPy -- see COPYING distributed with PyMVPA for the
copyright/license information.
"""
import warnings

import numpy as np

from mvpa2.base import cfg
from mvpa2.testing import *
skip_if_no_external('scipy')
from mvpa2.support.nipy import emp_null
#from emp_null import ENN

def setup():
    # Suppress warnings during tests to reduce noise
    warnings.simplefilter("ignore")

def teardown():
    # Clear list of warning filters
    warnings.resetwarnings()

@reseed_rng()
def test_efdr():
    # generate the data
    n = 100000
    x = np.random.randn(n)
    x[:3000] += 3
    #
    # make the tests
    efdr = emp_null.ENN(x)
    if cfg.getboolean('tests', 'labile', default='yes'):
        # 2.9 instead of stricter 3.0 for tolerance
        np.testing.assert_array_less(efdr.fdr(2.9), 0.15)
        np.testing.assert_array_less(-efdr.threshold(alpha=0.05), -3)
        np.testing.assert_array_less(-efdr.uncorrected_threshold(alpha=0.001), -3)

########NEW FILE########
__FILENAME__ = test_enet
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA least angle regression (ENET) classifier"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.datasets import *

skip_if_no_external('elasticnet')

from mvpa2 import cfg
from mvpa2.clfs.enet import ENET
from scipy.stats import pearsonr
from mvpa2.misc.data_generators import normal_feature_dataset

class ENETTests(unittest.TestCase):

    def test_enet(self):
        # not the perfect dataset with which to test, but
        # it will do for now.
        #data = datasets['dumb2']
        # for some reason the R code fails with the dumb data
        data = datasets['chirp_linear']

        clf = ENET()

        clf.train(data)

        # prediction has to be almost perfect
        # test with a correlation
        pre = clf.predict(data.samples)
        cor = pearsonr(pre, data.targets)
        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(cor[0] > .8)

    def test_enet_state(self):
        #data = datasets['dumb2']
        # for some reason the R code fails with the dumb data
        data = datasets['chirp_linear']

        clf = ENET()

        clf.train(data)

        clf.ca.enable('predictions')

        p = clf.predict(data.samples)

        self.assertTrue((p == clf.ca.predictions).all())


    def test_enet_sensitivities(self):
        data = datasets['chirp_linear']

        # use ENET on binary problem
        clf = ENET()
        clf.train(data)

        # now ask for the sensitivities WITHOUT having to pass the dataset
        # again
        sens = clf.get_sensitivity_analyzer(force_train=False)(None)

        self.assertTrue(sens.shape == (data.nfeatures,))


def suite():  # pragma: no cover
    return unittest.makeSuite(ENETTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_erdataset
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for the event-related dataset'''

from mvpa2.testing import *
from mvpa2.datasets import dataset_wizard
from mvpa2.mappers.flatten import FlattenMapper
from mvpa2.mappers.boxcar import BoxcarMapper
from mvpa2.mappers.fx import FxMapper
from mvpa2.datasets.eventrelated import find_events, eventrelated_dataset
from mvpa2.misc.data_generators import load_example_fmri_dataset
from mvpa2.mappers.zscore import zscore


def test_erdataset():
    # 3 chunks, 5 targets, blocks of 5 samples each
    nchunks = 3
    ntargets = 5
    blocklength = 5
    nfeatures = 10
    targets = np.tile(np.repeat(range(ntargets), blocklength), nchunks)
    chunks = np.repeat(np.arange(nchunks), ntargets * blocklength)
    samples = np.repeat(
                np.arange(nchunks * ntargets * blocklength),
                nfeatures).reshape(-1, nfeatures)
    ds = dataset_wizard(samples, targets=targets, chunks=chunks)
    # check if events are determined properly
    evs = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)
    for ev in evs:
        assert_equal(ev['duration'], blocklength)
    assert_equal(ntargets * nchunks, len(evs))
    for t in range(ntargets):
        assert_equal(len([ev for ev in evs if ev['targets'] == t]),
                     nchunks)
    # now turn `ds` into an eventreleated dataset
    erds = eventrelated_dataset(ds, evs)
    # the only unprefixed sample attributes are 
    assert_equal(sorted([a for a in ds.sa if not a.startswith('event')]),
                 ['chunks', 'targets'])
    # samples as expected?
    assert_array_equal(erds.samples[0],
                       np.repeat(np.arange(blocklength), nfeatures))
    # that should also be the temporal feature offset
    assert_array_equal(erds.samples[0], erds.fa.event_offsetidx)
    assert_array_equal(erds.sa.event_onsetidx, np.arange(0,71,5))
    # finally we should see two mappers
    assert_equal(len(erds.a.mapper), 2)
    assert_true(isinstance(erds.a.mapper[0], BoxcarMapper))
    assert_true(isinstance(erds.a.mapper[1], FlattenMapper))
    # check alternative event mapper
    # this one does temporal compression by averaging
    erds_compress = eventrelated_dataset(
                        ds, evs, event_mapper=FxMapper('features', np.mean))
    assert_equal(len(erds), len(erds_compress))
    assert_array_equal(erds_compress.samples[:,0], np.arange(2,73,5))
    #
    # now check the same dataset with event descretization
    tr = 2.5
    ds.sa['time'] = np.arange(nchunks * ntargets * blocklength) * tr
    evs = [{'onset': 4.9, 'duration': 6.2}]
    # doesn't work without conversion
    assert_raises(ValueError, eventrelated_dataset, ds, evs)
    erds = eventrelated_dataset(ds, evs, time_attr='time')
    assert_equal(len(erds), 1)
    assert_array_equal(erds.samples[0], np.repeat(np.arange(1,5), nfeatures))
    assert_array_equal(erds.sa.orig_onset, [evs[0]['onset']])
    assert_array_equal(erds.sa.orig_duration, [evs[0]['duration']])
    assert_array_almost_equal(erds.sa.orig_offset, [2.4])
    assert_array_equal(erds.sa.time, [np.arange(2.5, 11, 2.5)])
    # now with closest match
    erds = eventrelated_dataset(ds, evs, time_attr='time', match='closest')
    expected_nsamples = 3
    assert_equal(len(erds), 1)
    assert_array_equal(erds.samples[0],
                       np.repeat(np.arange(2,2+expected_nsamples),
                                nfeatures))
    assert_array_equal(erds.sa.orig_onset, [evs[0]['onset']])
    assert_array_equal(erds.sa.orig_duration, [evs[0]['duration']])
    assert_array_almost_equal(erds.sa.orig_offset, [-0.1])
    assert_array_equal(erds.sa.time, [np.arange(5.0, 11, 2.5)])
    # now test the way back
    results = np.arange(erds.nfeatures)
    assert_array_equal(erds.a.mapper.reverse1(results),
                       results.reshape(expected_nsamples, nfeatures))
    # what about multiple results?
    nresults = 5
    results = dataset_wizard([results] * nresults)
    # and let's have an attribute to make it more difficult
    results.sa['myattr'] = np.arange(5)
    rds = erds.a.mapper.reverse(results)
    assert_array_equal(rds,
                       results.samples.reshape(nresults * expected_nsamples,
                                               nfeatures))
    assert_array_equal(rds.sa.myattr, np.repeat(results.sa.myattr,
                                               expected_nsamples))

def test_hrf_modeling():
    skip_if_no_external('nibabel')
    skip_if_no_external('nipy') # ATM relies on NiPy's GLM implementation
    ds = load_example_fmri_dataset('25mm') #literal=True)
    # TODO: simulate short dataset with known properties and use it
    # for testing
    events = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)
    tr = ds.a.imghdr['pixdim'][4]
    for ev in events:
        for a in ('onset', 'duration'):
            ev[a] = ev[a] * tr
    evds = eventrelated_dataset(ds, events, time_attr='time_coords',
                                condition_attr='targets',
                                design_kwargs=dict(drift_model='blank'),
                                glmfit_kwargs=dict(model='ols'),
                                model='hrf')
    # same voxels
    assert_equal(ds.nfeatures, evds.nfeatures)
    assert_array_equal(ds.fa.voxel_indices, evds.fa.voxel_indices)
    # one sample for each condition, plus constant
    assert_equal(sorted(ds.sa['targets'].unique), sorted(evds.sa.targets))
    assert_equal(evds.a.add_regs.sa.regressor_names[0], 'constant')
    # with centered data
    zscore(ds)
    evds_demean = eventrelated_dataset(ds, events, time_attr='time_coords',
                                condition_attr='targets',
                                design_kwargs=dict(drift_model='blank'),
                                glmfit_kwargs=dict(model='ols'),
                                model='hrf')
    # after demeaning the constant should consume a lot less
    assert(evds.a.add_regs[0].samples.mean()
           > evds_demean.a.add_regs[0].samples.mean())
    # from eyeballing the sensitivity example -- would be better to test this on
    # the tutorial data
    assert(evds_demean[evds.sa.targets == 'shoe'].samples.max() \
           > evds_demean[evds.sa.targets == 'bottle'].samples.max())
    # HRF models
    assert('regressors' in evds.sa)
    assert('regressors' in evds.a.add_regs.sa)
    assert_equal(evds.sa.regressors.shape[1], len(ds))

    # custom regressors
    evds_regrs = eventrelated_dataset(ds, events, time_attr='time_coords',
                                condition_attr='targets',
                                regr_attrs=['time_indices'],
                                design_kwargs=dict(drift_model='blank'),
                                glmfit_kwargs=dict(model='ols'),
                                model='hrf')
    # verify that nothing screwed up time_coords
    assert_equal(ds.sa.time_coords[0], 0)
    assert_equal(len(evds_regrs), len(evds))
    # one more output sample in .a.add_regs
    assert_equal(len(evds_regrs.a.add_regs) - 1, len(evds.a.add_regs))
    # comes last before constant
    assert_equal('time_indices', evds_regrs.a.add_regs.sa.regressor_names[-2])
    # order of main regressors is unchanged
    assert_array_equal(evds.sa.targets, evds_regrs.sa.targets)

    # custom regressors from external sources
    evds_regrs = eventrelated_dataset(ds, events, time_attr='time_coords',
                                condition_attr='targets',
                                regr_attrs=['time_coords'],
                                design_kwargs=dict(drift_model='blank',
                                                   add_regs=np.linspace(1, -1, len(ds))[None].T,
                                                   add_reg_names=['negative_trend']),
                                glmfit_kwargs=dict(model='ols'),
                                model='hrf')
    assert_equal(len(evds_regrs), len(evds))
    # But we got one more in additional regressors
    assert_equal(len(evds_regrs.a.add_regs) - 2, len(evds.a.add_regs))
    # comes last before constant
    assert_array_equal(['negative_trend', 'time_coords', 'constant'],
                       evds_regrs.a.add_regs.sa.regressor_names)
    # order is otherwise unchanged
    assert_array_equal(evds.sa.targets, evds_regrs.sa.targets)

    # HRF models with estimating per each chunk
    assert_equal(ds.sa.time_coords[0], 0)
    evds_regrs = eventrelated_dataset(ds, events, time_attr='time_coords',
                                condition_attr=['targets', 'chunks'],
                                regr_attrs=['time_indices'],
                                design_kwargs=dict(drift_model='blank'),
                                glmfit_kwargs=dict(model='ols'),
                                model='hrf')
    assert_true('add_regs' in evds_regrs.a)
    assert_true('time_indices' in evds_regrs.a.add_regs.sa.regressor_names)

    assert_equal(len(ds.UC) * len(ds.UT), len(evds_regrs))
    assert_equal(len(evds_regrs.UC) * len(evds_regrs.UT), len(evds_regrs))

    from mvpa2.mappers.fx import mean_group_sample
    evds_regrs_meaned = mean_group_sample(['targets'])(evds_regrs)
    assert_array_equal(evds_regrs_meaned.T, evds.T) # targets should be the same

    #corr = np.corrcoef(np.vstack((evds.samples, evds_regrs_meaned)))
    #import pydb; pydb.debugger()
    #pass
    #i = 1


########NEW FILE########
__FILENAME__ = test_errorfx
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for various use cases users reported mis-behaving"""

import unittest
import numpy as np

from mvpa2.testing.tools import ok_, assert_array_equal, assert_true, \
        assert_false, assert_equal, assert_not_equal, reseed_rng

from mvpa2.misc.errorfx import auc_error

def test_auc_error():
    # two basic cases
    # perfect
    assert_equal(auc_error([-1, -1, 1, 1], [0, 0, 1, 1]), 1)
    # anti-perfect
    assert_equal(auc_error([-1, -1, 1, 1], [1, 1, 0, 0]), 0)

    # chance -- we aren't taking care ATM about randomly broken
    # ties, e.g. if both labels have the same estimate :-/
    # TODO:
    #assert_equal(auc_error([-1, 1, -1, 1], [0, 0, 1, 1]), 0.5)

########NEW FILE########
__FILENAME__ = test_externals
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Test externals checking"""

import unittest

from mvpa2 import cfg
from mvpa2.base import externals
from mvpa2.support import copy

class TestExternals(unittest.TestCase):

    def setUp(self):
        self.backup = []
        # paranoid check
        self.cfgstr = str(cfg)
        # clean up externals cfg for proper testing
        if cfg.has_section('externals'):
            self.backup = copy.deepcopy(cfg.items('externals'))
        cfg.remove_section('externals')


    def tearDown(self):
        if len(self.backup):
            # wipe existing one completely
            if cfg.has_section('externals'):
                cfg.remove_section('externals')
            cfg.add_section('externals')
            for o,v in self.backup:
                cfg.set('externals', o,v)
        # paranoid check
        # since order can't be guaranteed, lets check
        # each item after sorting
        self.assertEqual(sorted(self.cfgstr.split('\n')),
                             sorted(str(cfg).split('\n')))


    def test_externals(self):
        self.assertRaises(ValueError, externals.exists, 'BoGuS')


    def test_externals_no_double_invocation(self):
        # no external should be checking twice (unless specified
        # explicitely)

        class Checker(object):
            """Helper class to increment count of actual checks"""
            def __init__(self): self.checked = 0
            def check(self): self.checked += 1

        checker = Checker()

        externals._KNOWN['checker'] = 'checker.check()'
        externals.__dict__['checker'] = checker
        externals.exists('checker')
        self.assertEqual(checker.checked, 1)
        externals.exists('checker')
        self.assertEqual(checker.checked, 1)
        externals.exists('checker', force=True)
        self.assertEqual(checker.checked, 2)
        externals.exists('checker')
        self.assertEqual(checker.checked, 2)

        # restore original externals
        externals.__dict__.pop('checker')
        externals._KNOWN.pop('checker')


    def test_externals_correct2nd_invocation(self):
        # always fails
        externals._KNOWN['checker2'] = 'raise ImportError'

        self.assertTrue(not externals.exists('checker2'),
                        msg="Should be False on 1st invocation")

        self.assertTrue(not externals.exists('checker2'),
                        msg="Should be False on 2nd invocation as well")

        externals._KNOWN.pop('checker2')



def suite():  # pragma: no cover
    return unittest.makeSuite(TestExternals)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_filters
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for filter mappers'''


from mvpa2.testing import *
skip_if_no_external('scipy')

import numpy as np

from mvpa2.datasets import Dataset, vstack
from mvpa2.mappers.filters import FFTResampleMapper, iir_filter

def test_resample():
    time = np.linspace(0, 2*np.pi, 100)
    ds = Dataset(np.vstack((np.sin(time), np.cos(time))).T,
                 sa = {'time': time,
                       'section': np.repeat(range(10), 10)})
    assert_equal(ds.shape, (100, 2))

    # downsample
    num = 10
    rm = FFTResampleMapper(num, window=('gauss', 50),
                           position_attr='time',
                           attr_strategy='sample')
    mds = rm.forward(ds)
    assert_equal(mds.shape, (num, ds.nfeatures))
    # didn't change the orig
    assert_equal(len(ds), 100)

    # check position-based resampling
    ds_partial = ds[0::10]
    mds_partial = rm.forward(ds_partial)
    # despite different input sampling should yield the same output timepoints
    assert_array_almost_equal(mds.sa.time, mds_partial.sa.time)
    # exclude the first points to prevent edge effects, but the data should be
    # very similar too
    assert_array_almost_equal(mds.samples[2:], mds_partial.samples[2:], decimal=2)
    # simple sample of sa's should give meaningful stuff
    assert_array_equal(mds.sa.section, range(10))

    # and now for a dataset with chunks
    cds = vstack([ds.copy(), ds.copy()])
    cds.sa['chunks'] = np.repeat([0,1], len(ds))
    rm = FFTResampleMapper(num, attr_strategy='sample', chunks_attr='chunks',
                           window=('gauss', 50))
    mcds = rm.forward(cds)
    assert_equal(mcds.shape, (20, 2))
    assert_array_equal(mcds.sa.section, np.tile(range(10),2))
    # each individual chunks should be identical to previous dataset
    assert_array_almost_equal(mds.samples, mcds.samples[:10])
    assert_array_almost_equal(mds.samples, mcds.samples[10:])


def test_iirfilter():
    # dataset with one feature from two waves
    t = np.linspace(0, 1.0, 2001)
    xlow = np.sin(2 * np.pi * 5 * t)
    xhigh = np.sin(2 * np.pi * 250 * t)
    x = xlow + xhigh
    ds = Dataset(x, sa={'sid': np.arange(len(x))}, fa={'fid':['theone']})

    # butterworth filter with a cutoff between the waves
    from scipy import signal
    b, a = signal.butter(8, 0.125)
    mds = iir_filter(ds, b, a, padlen=150)
    # check we get just the slow wave out (compensate for edge artifacts)
    assert_false(np.sum(np.abs(mds.samples[100:-100,0] - xlow[100:-100]) > 0.001))
    assert_equal(len(ds.sa), len(mds.sa))
    assert_equal(len(ds.fa), len(mds.fa))
    assert_array_equal(ds.fa.fid, mds.fa.fid)
    assert_array_equal(ds.sa.sid, mds.sa.sid)

########NEW FILE########
__FILENAME__ = test_fxmapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA SampleGroup mapper"""

from mvpa2.testing import sweepargs
from mvpa2.testing.datasets import datasets
from mvpa2.measures.anova import OneWayAnova

import numpy as np
from mvpa2.mappers.fx import *
from mvpa2.datasets.base import dataset_wizard, Dataset

from mvpa2.testing.tools import *

def test_samplesgroup_mapper():
    data = np.arange(24).reshape(8, 3)
    labels = [0, 1] * 4
    chunks = np.repeat(np.array((0, 1)), 4)

    # correct results
    csamples = [[3, 4, 5], [6, 7, 8], [15, 16, 17], [18, 19, 20]]
    clabels = [0, 1, 0, 1]
    cchunks = [0, 0, 1, 1]

    ds = dataset_wizard(samples=data, targets=labels, chunks=chunks)
    # add some feature attribute -- just to check
    ds.fa['checker'] = np.arange(3)
    ds.init_origids('samples')

    m = mean_group_sample(['targets', 'chunks'])
    mds = m.forward(ds)
    assert_array_equal(mds.samples, csamples)
    # FAs should simply remain the same
    assert_array_equal(mds.fa.checker, np.arange(3))

    # now without grouping
    m = mean_sample()
    # forwarding just the samples should yield the same result
    assert_array_equal(m.forward(ds.samples),
                       m.forward(ds).samples)

    # directly apply to dataset
    # using untrained mapper
    m = mean_group_sample(['targets', 'chunks'])
    mapped = ds.get_mapped(m)

    assert_equal(mapped.nsamples, 4)
    assert_equal(mapped.nfeatures, 3)
    assert_array_equal(mapped.samples, csamples)
    assert_array_equal(mapped.targets, clabels)
    assert_array_equal(mapped.chunks, cchunks)
    # make sure origids get regenerated
    assert_array_equal([s.count('+') for s in mapped.sa.origids], [1] * 4)

    # disbalanced dataset -- lets remove 0th sample so there is no target
    # 0 in 0th chunk
    ds_ = ds[[0, 1, 3, 5]]
    mapped = ds_.get_mapped(m)
    ok_(len(mapped) == 3)
    ok_(not None in mapped.sa.origids)

    # with such a dataset we should get identical results if we order groups
    # by their occurence
    mo = mean_group_sample(['targets', 'chunks'], order='occurrence')
    mappedo = ds.get_mapped(mo)
    assert_array_equal(mappedo.samples, csamples)
    assert_array_equal(mappedo.targets, clabels)
    assert_array_equal(mappedo.chunks, cchunks)

    # but if we would get different result if we swap order
    # of specified uattrs: now first targets would be groupped
    # and only then chunks:
    mr = mean_group_sample(['chunks', 'targets'])
    mappedr = ds.get_mapped(mr)
    # which effectively swaps two comparison targets in this fake dataset
    assert_array_equal(mappedr.targets, cchunks)
    assert_array_equal(mappedr.chunks, clabels)

def test_samplesgroup_mapper_test_order_occurrence():
    data = np.arange(8)[:, None]
    ds = dataset_wizard(samples=data,
                        targets=[1, 0]*4,
                        chunks=[0]*4 + [1]*4)

    m = mean_group_sample(['targets', 'chunks'], order='occurrence')
    assert_true('order=' in repr(m))

    mds = ds.get_mapped(m)

    assert_array_equal(mds.sa.targets, [1, 0] * 2)
    assert_array_equal(mds.sa.chunks, [0]*2 + [1]*2)
    assert_array_equal(mds.samples[:, 0], [1, 2, 5, 6])

    # and if we ordered as 'uattrs' (default)
    m = mean_group_sample(['targets', 'chunks'])
    assert_false('order=' in repr(m))
    mds = ds.get_mapped(m)

    assert_array_equal(mds.sa.targets, [0, 1] * 2)
    assert_array_equal(mds.sa.chunks, [0]*2 + [1]*2)
    assert_array_equal(mds.samples[:, 0], [2, 1, 6, 5])

def test_featuregroup_mapper():
    ds = Dataset(np.arange(24).reshape(3,8))
    ds.fa['roi'] = [0, 1] * 4
    # just to check
    ds.sa['chunks'] = np.arange(3)

    # correct results
    csamples = [[3, 4], [11, 12], [19, 20]]
    croi = [0, 1]
    cchunks = np.arange(3)

    m = mean_group_feature(['roi'])
    mds = m.forward(ds)
    assert_equal(mds.shape, (3, 2))
    assert_array_equal(mds.samples, csamples)
    assert_array_equal(mds.fa.roi, np.unique([0, 1] * 4))
    # FAs should simply remain the same
    assert_array_equal(mds.sa.chunks, np.arange(3))

    # now without grouping
    m = mean_feature()
    # forwarding just the samples should yield the same result
    assert_array_equal(m.forward(ds.samples),
                       m.forward(ds).samples)

    # And when operating on a dataset with >1D samples, then operate
    # only across "features", i.e. 1st dimension
    ds = Dataset(np.arange(24).reshape(3,2,2,2))
    mapped = ds.get_mapped(m)
    assert_array_equal(m.forward(ds.samples),
                       mapped.samples)
    assert_array_equal(mapped.samples.shape, (3, 2, 2))
    assert_array_equal(mapped.samples, np.mean(ds.samples, axis=1))
    # and still could map back? ;) not ATM, so just to ensure consistency
    assert_raises(NotImplementedError,
                  mapped.a.mapper.reverse, mapped.samples)
    # but it should also work with standard 2d sample arrays
    ds = Dataset(np.arange(24).reshape(3,8))
    mapped = ds.get_mapped(m)
    assert_array_equal(mapped.samples.shape, (3, 1))


def test_fxmapper():
    origdata = np.arange(24).reshape(3,8)
    ds = Dataset(origdata.copy())
    ds.samples *= -1

    # test a mapper that doesn't change the shape
    # it shouldn't mapper along with axis it is applied
    m_s = FxMapper('samples', np.absolute)
    m_f = FxMapper('features', np.absolute)
    a_m = absolute_features()
    assert_array_equal(m_s.forward(ds), origdata)
    assert_array_equal(a_m.forward(ds), origdata)
    assert_array_equal(m_s.forward(ds), m_f.forward(ds))


def test_features01():
    # TODO: might be worth creating appropriate factory
    #       help in mappers/fx
    aov = OneWayAnova(
        postproc=FxMapper('features',
                          lambda x: x / x.max(),
                          attrfx=None))
    f = aov(datasets['uni2small'])
    ok_((f.samples != 1.0).any())
    ok_(f.samples.max() == 1.0)

@sweepargs(f=dir(np))
def test_fx_native_calls(f):
    import inspect

    ds = datasets['uni2small']
    if f in ['size', 'rollaxis']:
        # really not appropriate ones here to test
        return
    try:
        f_ = getattr(np, f)
        if 'axis' != inspect.getargs(f_.__code__).args[1]:
            # if 'axis' is not the 2nd arg -- skip
            return
    except:
        return

    # so we got a function which has 'axis' arugment
    for naxis in (0, 1): # check on both axes
        for do_group in (False, True): # test with
                                       # groupping and without
            kwargs = dict(attrfx='merge')
            if do_group:
                if naxis == 0:
                    kwargs['uattrs'] = ('targets', 'chunks')
                else:
                    kwargs['uattrs'] = ('nonbogus_targets',)

            axis = ('samples', 'features')[naxis]
            def custom(data):
                """So we could enforce apply_along_axis
                """
                # always 0 since it would be the job for apply_along_axis
                return f_(data, axis=0)
            try:
                m2 = FxMapper(axis, custom, **kwargs)
                dsm2 = ds.get_mapped(m2)
            except Exception, e:
                # We assume that our previous implementation should work ;-)
                continue

            m1 = FxMapper(axis, f_, **kwargs)
            dsm1 = ds.get_mapped(m1)

            assert_objectarray_equal(dsm1.samples, dsm2.samples)
            assert_objectarray_equal(dsm1.targets, dsm2.targets)
            assert_objectarray_equal(dsm1.chunks, dsm2.chunks)
            assert_objectarray_equal(dsm1.fa.nonbogus_targets, dsm2.fa.nonbogus_targets)

def test_uniquemerge2literal():
    from mvpa2.mappers.fx import _uniquemerge2literal
    assert_equal(_uniquemerge2literal(range(3)), ['0+1+2'])
    assert_equal(_uniquemerge2literal(np.arange(6).reshape(2,3)), ['[0 1 2]+[3 4 5]'])
    assert_array_equal(_uniquemerge2literal([[2,3,4]]), [[2, 3, 4]])
    assert_array_equal(_uniquemerge2literal([[2,3,4],[2,3,4]]), [[2, 3, 4]])
    assert_equal(_uniquemerge2literal([2,2,2]), [2])
    assert_array_equal(_uniquemerge2literal(['L1', 'L1']), ['L1'])

########NEW FILE########
__FILENAME__ = test_generators
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for generators."""

import itertools
import numpy as np

from mvpa2.testing.tools import ok_, assert_array_equal, assert_true, \
        assert_false, assert_equal, assert_raises, assert_almost_equal, \
        reseed_rng, assert_not_equal

from mvpa2.datasets import dataset_wizard, Dataset
from mvpa2.generators.splitters import Splitter
from mvpa2.base.node import ChainNode
from mvpa2.generators.partition import OddEvenPartitioner, NFoldPartitioner, \
     ExcludeTargetsCombinationsPartitioner
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.generators.base import  Repeater, Sifter
from mvpa2.generators.resampling import Balancer
from mvpa2.misc.support import get_nelements_per_value


def give_data():
    # 100x10, 10 chunks, 4 targets
    return dataset_wizard(np.random.normal(size=(100,10)),
                          targets=[ i%4 for i in range(100) ],
                          chunks=[ i//10 for i in range(100)])


@reseed_rng()
def test_splitter():
    ds = give_data()
    # split with defaults
    spl1 = Splitter('chunks')
    assert_raises(NotImplementedError, spl1, ds)

    splits = list(spl1.generate(ds))
    assert_equal(len(splits), len(ds.sa['chunks'].unique))

    for split in splits:
        # it should have perform basic slicing!
        assert_true(split.samples.base is ds.samples)
        assert_equal(len(split.sa['chunks'].unique), 1)
        assert_true('lastsplit' in split.a)
    assert_true(splits[-1].a.lastsplit)

    # now again, more customized
    spl2 = Splitter('targets', attr_values = [0,1,1,2,3,3,3], count=4,
                   noslicing=True)
    splits = list(spl2.generate(ds))
    assert_equal(len(splits), 4)
    for split in splits:
        # it should NOT have perform basic slicing!
        assert_false(split.samples.base is ds.samples)
        assert_equal(len(split.sa['targets'].unique), 1)
        assert_equal(len(split.sa['chunks'].unique), 10)
    assert_true(splits[-1].a.lastsplit)

    # two should be identical
    assert_array_equal(splits[1].samples, splits[2].samples)

    # now go wild and split by feature attribute
    ds.fa['roi'] = np.repeat([0,1], 5)
    # splitter should auto-detect that this is a feature attribute
    spl3 = Splitter('roi')
    splits = list(spl3.generate(ds))
    assert_equal(len(splits), 2)
    for split in splits:
        assert_true(split.samples.base is ds.samples)
        assert_equal(len(split.fa['roi'].unique), 1)
        assert_equal(split.shape, (100, 5))

    # and finally test chained splitters
    cspl = ChainNode([spl2, spl3, spl1])
    splits = list(cspl.generate(ds))
    # 4 target splits and 2 roi splits each and 10 chunks each
    assert_equal(len(splits), 80)


@reseed_rng()
def test_partitionmapper():
    ds = give_data()
    oep = OddEvenPartitioner()
    parts = list(oep.generate(ds))
    assert_equal(len(parts), 2)
    for i, p in enumerate(parts):
        assert_array_equal(p.sa['partitions'].unique, [1, 2])
        assert_equal(p.a.partitions_set, i)
        assert_equal(len(p), len(ds))


@reseed_rng()
def test_attrpermute():
    ds = give_data()
    ds.sa['ids'] = range(len(ds))
    pristine_data = ds.samples.copy()
    permutation = AttributePermutator(['targets', 'ids'], assure=True)
    pds = permutation(ds)
    # should not touch the data
    assert_array_equal(pristine_data, pds.samples)
    # even keep the very same array
    assert_true(pds.samples.base is ds.samples)
    # there is no way that it can be the same attribute
    assert_false(np.all(pds.sa.ids == ds.sa.ids))
    # ids should reflect permutation setup
    assert_array_equal(pds.sa.targets, ds.sa.targets[pds.sa.ids])
    # other attribute should remain intact
    assert_array_equal(pds.sa.chunks, ds.sa.chunks)

    # now chunk-wise permutation
    permutation = AttributePermutator('ids', limit='chunks')
    pds = permutation(ds)
    # first ten should remain first ten
    assert_false(np.any(pds.sa.ids[:10] > 9))

    # verify that implausible assure=True would not work
    permutation = AttributePermutator('targets', limit='ids', assure=True)
    assert_raises(RuntimeError, permutation, ds)

    # same thing, but only permute single chunk
    permutation = AttributePermutator('ids', limit={'chunks': 3})
    pds = permutation(ds)
    # one chunk should change
    assert_false(np.any(pds.sa.ids[30:40] > 39))
    assert_false(np.any(pds.sa.ids[30:40] < 30))
    # the rest not
    assert_array_equal(pds.sa.ids[:30], range(30))

    # or a list of chunks
    permutation = AttributePermutator('ids', limit={'chunks': [3,4]})
    pds = permutation(ds)
    # two chunks should change
    assert_false(np.any(pds.sa.ids[30:50] > 49))
    assert_false(np.any(pds.sa.ids[30:50] < 30))
    # the rest not
    assert_array_equal(pds.sa.ids[:30], range(30))

    # and now try generating more permutations
    nruns = 2
    permutation = AttributePermutator(['targets', 'ids'],
                                      assure=True, count=nruns)
    pds = list(permutation.generate(ds))
    assert_equal(len(pds), nruns)
    for p in pds:
        assert_false(np.all(p.sa.ids == ds.sa.ids))

    # permute feature attrs
    ds.fa['ids'] = range(ds.shape[1])
    permutation = AttributePermutator('fa.ids', assure=True)
    pds = permutation(ds)
    assert_false(np.all(pds.fa.ids == ds.fa.ids))

    # now chunk-wise uattrs strategy (reassignment)
    permutation = AttributePermutator('targets', limit='chunks',
                                      strategy='uattrs', assure=True)
    pds = permutation(ds)
    # Due to assure above -- we should have changed things
    assert_not_equal(zip(ds.targets), zip(pds.targets))
    # in each chunk we should have unique remappings
    for c in ds.UC:
        chunk_idx = ds.C == c
        otargets, ptargets = ds.targets[chunk_idx], pds.sa.targets[chunk_idx]
        # we still have the same targets
        assert_equal(set(ptargets), set(otargets))
        # we have only 1-to-1 mappings
        assert_true(len(set(zip(otargets, ptargets))), len(set(otargets)))

    ds.sa['odds'] = ds.sa.ids % 2
    # test combinations
    permutation = AttributePermutator(['targets', 'odds'], limit='chunks',
                                       strategy='uattrs', assure=True)
    pds = permutation(ds)
    # Due to assure above -- we should have changed things
    assert_not_equal(zip(ds.targets,   ds.sa.odds),
                     zip(pds.targets, pds.sa.odds))
    # In each chunk we should have unique remappings
    for c in ds.UC:
        chunk_idx = ds.C == c
        otargets, ptargets = ds.targets[chunk_idx], pds.sa.targets[chunk_idx]
        oodds, podds = ds.sa.odds[chunk_idx], pds.sa.odds[chunk_idx]
        # we still have the same targets
        assert_equal(set(ptargets), set(otargets))
        assert_equal(set(oodds), set(podds))
        # at the end we have the same mapping
        assert_equal(set(zip(otargets, oodds)), set(zip(ptargets, podds)))

@reseed_rng()
def test_balancer():
    ds = give_data()
    # only mark the selection in an attribute
    bal = Balancer()
    res = bal(ds)
    # we get a new dataset, with shared samples
    assert_false(ds is res)
    assert_true(ds.samples is res.samples.base)
    # should kick out 2 samples in each chunk of 10
    assert_almost_equal(np.mean(res.sa.balanced_set), 0.8)
    # same as above, but actually apply the selection
    bal = Balancer(apply_selection=True, count=5)
    # just run it once
    res = bal(ds)
    # we get a new dataset, with shared samples
    assert_false(ds is res)
    # should kick out 2 samples in each chunk of 10
    assert_equal(len(res), int(0.8 * len(ds)))
    # now use it as a generator
    dses = list(bal.generate(ds))
    assert_equal(len(dses), 5)
    # with limit
    bal = Balancer(limit={'chunks': 3}, apply_selection=True)
    res = bal(ds)
    assert_equal(res.sa['chunks'].unique, (3,))
    assert_equal(get_nelements_per_value(res.sa.targets).values(),
                 [2] * 4)
    # same but include all offlimit samples
    bal = Balancer(limit={'chunks': 3}, include_offlimit=True,
                   apply_selection=True)
    res = bal(ds)
    assert_array_equal(res.sa['chunks'].unique, range(10))
    # chunk three still balanced, but the rest is not, i.e. all samples included
    assert_equal(get_nelements_per_value(res[res.sa.chunks == 3].sa.targets).values(),
                 [2] * 4)
    assert_equal(get_nelements_per_value(res.sa.chunks).values(),
                 [10, 10, 10, 8, 10, 10, 10, 10, 10, 10])
    # fixed amount
    bal = Balancer(amount=1, limit={'chunks': 3}, apply_selection=True)
    res = bal(ds)
    assert_equal(get_nelements_per_value(res.sa.targets).values(),
                 [1] * 4)
    # fraction
    bal = Balancer(amount=0.499, limit=None, apply_selection=True)
    res = bal(ds)
    assert_array_equal(
            np.round(np.array(get_nelements_per_value(ds.sa.targets).values()) * 0.5),
            np.array(get_nelements_per_value(res.sa.targets).values()))
    # check on feature attribute
    ds.fa['one'] = np.tile([1,2], 5)
    ds.fa['chk'] = np.repeat([1,2], 5)
    bal = Balancer(attr='one', amount=2, limit='chk', apply_selection=True)
    res = bal(ds)
    assert_equal(get_nelements_per_value(res.fa.one).values(),
                 [4] * 2)


def test_repeater():
    reps = 4
    r = Repeater(reps, space='OMG')
    dsl = [ds for ds in r.generate(Dataset([0,1]))]
    assert_equal(len(dsl), reps)
    for i, ds in enumerate(dsl):
        assert_equal(ds.a.OMG, i)

def test_sifter():
    # somewhat duplicating the doctest
    ds = Dataset(samples=np.arange(8).reshape((4,2)),
                 sa={'chunks':   [ 0 ,  1 ,  2 ,  3 ],
                     'targets':  ['c', 'c', 'p', 'p']})
    for sift_targets_definition in (['c', 'p'],
                                    dict(uvalues=['c', 'p'])):
        par = ChainNode([NFoldPartitioner(cvtype=2, attr='chunks'),
                         Sifter([('partitions', 2),
                                 ('targets', sift_targets_definition)])
                         ])
        dss = list(par.generate(ds))
        assert_equal(len(dss), 4)
        for ds_ in dss:
            testing = ds[ds_.sa.partitions == 2]
            assert_array_equal(np.unique(testing.sa.targets), ['c', 'p'])
            # and we still have both targets  present in training
            training = ds[ds_.sa.partitions == 1]
            assert_array_equal(np.unique(training.sa.targets), ['c', 'p'])

def test_sifter_with_balancing():
    # extended previous test which was already
    # "... somewhat duplicating the doctest"
    ds = Dataset(samples=np.arange(12).reshape((-1, 2)),
                 sa={'chunks':   [ 0 ,  1 ,  2 ,  3 ,  4,   5 ],
                     'targets':  ['c', 'c', 'c', 'p', 'p', 'p']})

    # Without sifter -- just to assure that we do get all of them
    # i.e. 6*5*4*3/(4!) = 15
    par = ChainNode([NFoldPartitioner(cvtype=4, attr='chunks')])
    assert_equal(len(list(par.generate(ds))), 15)

    # so we will take 4 chunks out of available 7, but would care only
    # about those partitions where we have balanced number of 'c' and 'p'
    # entries
    assert_raises(ValueError,
                  lambda x: list(Sifter([('targets', dict(wrong=1))]).generate(x)),
                  ds)

    par = ChainNode([NFoldPartitioner(cvtype=4, attr='chunks'),
                     Sifter([('partitions', 2),
                             ('targets',
                              dict(uvalues=['c', 'p'],
                                   balanced=True))])
                     ])
    dss = list(par.generate(ds))
    # print [ x[x.sa.partitions==2].sa.targets for x in dss ]
    assert_equal(len(dss), 9)
    for ds_ in dss:
        testing = ds[ds_.sa.partitions == 2]
        assert_array_equal(np.unique(testing.sa.targets), ['c', 'p'])
        # and we still have both targets  present in training
        training = ds[ds_.sa.partitions == 1]
        assert_array_equal(np.unique(training.sa.targets), ['c', 'p'])

def test_exclude_targets_combinations():
    partitioner = ChainNode([NFoldPartitioner(),
                             ExcludeTargetsCombinationsPartitioner(
                                 k=2,
                                 targets_attr='targets',
                                 space='partitions')],
                            space='partitions')
    from mvpa2.misc.data_generators import normal_feature_dataset
    ds = normal_feature_dataset(snr=0., nlabels=4, perlabel=3, nchunks=3,
                                nonbogus_features=[0,1,2,3], nfeatures=4)
    partitions = list(partitioner.generate(ds))
    assert_equal(len(partitions), 3 * 6)
    splitter = Splitter('partitions')
    combs = []
    comb_chunks = []
    for p in partitions:
        trds, teds = list(splitter.generate(p))[:2]
        comb = tuple(np.unique(teds.targets))
        combs.append(comb)
        comb_chunks.append(comb + tuple(np.unique(teds.chunks)))
    assert_equal(len(set(combs)), 6)         # just 6 possible combinations of 2 out of 4
    assert_equal(len(set(comb_chunks)), 3*6) # all unique


def test_exclude_targets_combinations_subjectchunks():
    partitioner = ChainNode([NFoldPartitioner(attr='subjects'),
                             ExcludeTargetsCombinationsPartitioner(
                                 k=1,
                                 targets_attr='chunks',
                                 space='partitions')],
                            space='partitions')
    # targets do not need even to be defined!
    ds = Dataset(np.arange(18).reshape(9, 2),
                 sa={'chunks': np.arange(9) // 3,
                     'subjects': np.arange(9) % 3})
    dss = list(partitioner.generate(ds))
    assert_equal(len(dss), 9)

    testing_subjs, testing_chunks = [], []
    for ds_ in dss:
        testing_partition = ds_.sa.partitions == 2
        training_partition = ds_.sa.partitions == 1
        # must be scalars -- so implicit test here
        # if not -- would be error
        testing_subj = np.asscalar(np.unique(ds_.sa.subjects[testing_partition]))
        testing_subjs.append(testing_subj)
        testing_chunk = np.asscalar(np.unique(ds_.sa.chunks[testing_partition]))
        testing_chunks.append(testing_chunk)
        # and those must not appear for training
        ok_(not testing_subj in ds_.sa.subjects[training_partition])
        ok_(not testing_chunk in ds_.sa.chunks[training_partition])
    # and we should have gone through all chunks/subjs pairs
    testing_pairs = set(zip(testing_subjs, testing_chunks))
    assert_equal(len(testing_pairs), 9)
    # yoh: equivalent to set(itertools.product(range(3), range(3))))
    #      but .product is N/A for python2.5
    assert_equal(testing_pairs, set(zip(*np.where(np.ones((3,3))))))

########NEW FILE########
__FILENAME__ = test_glmmapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for the NiPy GLM mapper (requiring NiPy)."""

import numpy as np

from mvpa2.testing.tools import *

skip_if_no_external('scipy')

from scipy import signal
from mvpa2.datasets import Dataset

from mvpa2.mappers.glm import *
from mvpa2.misc.fx import double_gamma_hrf, single_gamma_hrf

def get_bold():
    # TODO add second model
    hrf_x = np.linspace(0,25,250)
    hrf = double_gamma_hrf(hrf_x) - single_gamma_hrf(hrf_x, 0.8, 1, 0.05)

    samples = 1200
    exp_time = np.linspace(0, 120, samples)

    fast_er_onsets = np.array([50, 240, 340, 590, 640, 940, 960])
    fast_er = np.zeros(samples)
    fast_er[fast_er_onsets] = 1

    model_hr = np.convolve(fast_er, hrf)[:samples]

    tr = 2.0
    model_lr = signal.resample(model_hr, int(samples / tr / 10), window='ham')

    ## moderate noise level
    baseline = 800
    wsignal = baseline + 8.0 \
              * model_lr + np.random.randn(int(samples / tr / 10)) * 4.0
    nsignal = baseline \
              + np.random.randn(int(samples / tr / 10)) * 4.0

    ds = Dataset(samples=np.array([wsignal, nsignal]).T,
                 sa={'model': model_lr})

    return ds

def test_glm_mapper():
    bold = get_bold()
    assert_equal(bold.nfeatures, 2)
    assert('model' in bold.sa)
    reg_names = ['model']
    implementations = []
    if externals.exists('nipy'):
        implementations.append(NiPyGLMMapper)
    if externals.exists('statsmodels'):
        implementations.append(StatsmodelsGLMMapper)
    results = []
    if not len(implementations):
        raise SkipTest
    for klass in implementations:
        pest = klass(reg_names)(bold)
        assert_equal(pest.shape, (len(reg_names), bold.nfeatures))
        assert_array_equal(pest.sa.regressor_names, reg_names)
        pest = klass(reg_names, add_constant=True)(bold)
        assert_equal(pest.shape, (len(reg_names) + 1, bold.nfeatures))
        # nothing at all
        noglm = klass([])
        assert_raises(ValueError, noglm.__call__, bold)
        # no reg from ds at all
        pest = klass([], add_constant=True)(bold)
        assert_equal(pest.shape, (1, bold.nfeatures))
        assert_array_equal(pest.sa.regressor_names, ['constant'])
        # only reg from mapper
        pest = klass([],
                     add_regs=(('trend',
                               (np.linspace(-1,1,len(bold)))),))(bold)
        assert_equal(pest.shape, (1, bold.nfeatures))
        assert_array_equal(pest.sa.regressor_names, ['trend'])
        # full monty
        pest = klass(['model'],
                     add_regs=(('trend',
                               (np.linspace(-1,1,len(bold)))),),
                     add_constant=True,
                     space='conditions',
                     return_design=True,
                     return_model=True)(bold)
        results.append(pest)
        assert_equal(pest.shape, (len(reg_names) + 2, bold.nfeatures))
        assert_array_equal(pest.sa.conditions, ['model', 'trend', 'constant'])
        assert('model' in pest.a)
        assert('regressors' in pest.sa)
        assert_array_equal(pest.sa.regressors[0], bold.sa.model)
        assert_array_equal(pest.sa.regressors[-1], np.ones(len(bold)))
    if len(results) < 2:
        return
    ds1, ds2 = results[0], results[1]
    # should really have very similar results, independent of actual model fit details
    assert(np.corrcoef(ds1.samples.ravel(), ds2.samples.ravel())[0,1] > 0.99)


########NEW FILE########
__FILENAME__ = test_glmnet
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA least angle regression (ENET) classifier"""

from mvpa2.testing import *
skip_if_no_external('glmnet')

from mvpa2.testing.datasets import *

from mvpa2 import cfg
from mvpa2.clfs.glmnet import GLMNET_R,GLMNET_C

#from scipy.stats import pearsonr
# Lets use our corr_error which would be available even without scipy
from mvpa2.misc.errorfx import corr_error
from mvpa2.misc.data_generators import normal_feature_dataset

from mvpa2.testing.tools import assert_true, assert_equal, assert_array_equal

def test_glmnet_r():
    # not the perfect dataset with which to test, but
    # it will do for now.
    #data = datasets['dumb2']
    # for some reason the R code fails with the dumb data
    data = datasets['chirp_linear']

    clf = GLMNET_R()

    clf.train(data)

    # prediction has to be almost perfect
    # test with a correlation
    pre = clf.predict(data.samples)
    corerr = corr_error(pre, data.targets)
    if cfg.getboolean('tests', 'labile', default='yes'):
        assert_true(corerr < .2)

def test_glmnet_c():
    # define binary prob
    data = datasets['dumb2']

    # use GLMNET on binary problem
    clf = GLMNET_C()
    clf.ca.enable('estimates')

    clf.train(data)

    # test predictions
    pre = clf.predict(data.samples)

    assert_array_equal(pre, data.targets)

def test_glmnet_state():
    #data = datasets['dumb2']
    # for some reason the R code fails with the dumb data
    data = datasets['chirp_linear']

    clf = GLMNET_R()

    clf.train(data)

    clf.ca.enable('predictions')

    p = clf.predict(data.samples)

    assert_array_equal(p, clf.ca.predictions)


def test_glmnet_c_sensitivities():
    data = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=4)

    # use GLMNET on binary problem
    clf = GLMNET_C()
    clf.train(data)

    # now ask for the sensitivities WITHOUT having to pass the dataset
    # again
    sens = clf.get_sensitivity_analyzer(force_train=False)(None)

    #failUnless(sens.shape == (data.nfeatures,))
    assert_equal(sens.shape, (len(data.UT), data.nfeatures))

def test_glmnet_r_sensitivities():
    data = datasets['chirp_linear']

    clf = GLMNET_R()

    clf.train(data)

    # now ask for the sensitivities WITHOUT having to pass the dataset
    # again
    sens = clf.get_sensitivity_analyzer(force_train=False)(None)

    assert_equal(sens.shape, (1, data.nfeatures))

########NEW FILE########
__FILENAME__ = test_gnb
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA GNB classifier"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.datasets import *

from mvpa2.clfs.gnb import GNB
from mvpa2.measures.base import TransferMeasure
from mvpa2.generators.splitters import Splitter

class GNBTests(unittest.TestCase):

    def test_gnb(self):
        gnb = GNB()
        gnb_nc = GNB(common_variance=False)
        gnb_n = GNB(normalize=True)
        gnb_n_nc = GNB(normalize=True, common_variance=False)

        ds = datasets['uni2medium']

        # Generic silly coverage just to assure that it works in all
        # possible scenarios:
        bools = (True, False)
        # There should be better way... heh
        for cv in bools:                # common_variance?
          for prior in ('uniform', 'laplacian_smoothing', 'ratio'):
            tp = None                   # predictions -- all above should
                                        # result in the same predictions
            for n in bools:             # normalized?
              for ls in bools:          # logspace?
                for es in ((), ('estimates')):
                    gnb_ = GNB(common_variance=cv,
                               prior=prior,
                               normalize=n,
                               logprob=ls,
                               enable_ca=es)
                    tm = TransferMeasure(gnb_, Splitter('train'))
                    predictions = tm(ds).samples[:,0]
                    if tp is None:
                        tp = predictions
                    assert_array_equal(predictions, tp)
                    # if normalized -- check if estimates are such
                    if n and 'estimates' in es:
                        v = gnb_.ca.estimates
                        if ls:          # in log space -- take exp ;)
                            v = np.exp(v)
                        d1 = np.sum(v, axis=1) - 1.0
                        self.assertTrue(np.max(np.abs(d1)) < 1e-5)

def suite():  # pragma: no cover
    return unittest.makeSuite(GNBTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_gpr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA GPR."""

from mvpa2.base import externals
from mvpa2.misc import data_generators
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.kernels.np import GeneralizedLinearKernel
from mvpa2.clfs.gpr import GPR

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets
from mvpa2.testing.tools import assert_array_equal, assert_array_almost_equal

if __debug__:
    from mvpa2.base import debug


class GPRTests(unittest.TestCase):

    def test_basic(self):
        skip_if_no_external('scipy') # needed by GPR code
        dataset = data_generators.linear1d_gaussian_noise()
        k = GeneralizedLinearKernel()
        clf = GPR(k)
        clf.train(dataset)
        y = clf.predict(dataset.samples)
        assert_array_equal(y.shape, dataset.targets.shape)

    def test_linear(self):
        pass

    def _test_gpr_model_selection(self):  # pragma: no cover
        """Smoke test for running model selection while getting GPRWeights

        TODO: DISABLED because setting of hyperparameters was not adopted for 0.6 (yet)
        """
        if not externals.exists('openopt'):
            return
        amap = AttributeMap()           # we would need to pass numbers into the GPR
        dataset = datasets['uni2small'].copy() #data_generators.linear1d_gaussian_noise()
        dataset.targets = amap.to_numeric(dataset.targets).astype(float)
        k = GeneralizedLinearKernel()
        clf = GPR(k, enable_ca=['log_marginal_likelihood'])
        sa = clf.get_sensitivity_analyzer() # should be regular weights
        sa_ms = clf.get_sensitivity_analyzer(flavor='model_select') # with model selection
        def prints():
            print clf.ca.log_marginal_likelihood, clf.kernel.Sigma_p, clf.kernel.sigma_0

        sa(dataset)
        lml = clf.ca.log_marginal_likelihood

        sa_ms(dataset)
        lml_ms = clf.ca.log_marginal_likelihood

        self.assertTrue(lml_ms > lml)



def suite():  # pragma: no cover
    return unittest.makeSuite(GPRTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_hamster
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Hamster"""

import os
import unittest
import numpy as np

from mvpa2.testing.tools import *
from mvpa2.misc.io.hamster import *

class HamsterHelperTests(unittest.TestCase):

    def test_specification(self):

        # more than 1 positional
        self.assertRaises(ValueError, Hamster, "1", 2)
        # do not mix positional
        self.assertRaises(ValueError, Hamster, "1", bu=123)
        # need to be a string
        self.assertRaises(ValueError, Hamster, 1)
        # dump cannot be assigned
        self.assertRaises(ValueError, Hamster, dump=123)
        # need to be an existing file
        self.assertRaises(IOError, Hamster, "/dev/ZUMBARGAN123")

        hh=Hamster(budda=1, z=[123], fuga="123"); hh.h1=123;
        delattr(hh, 'budda')
        self.assertTrue(`hh` == "Hamster(fuga='123', h1=123, z=[123])")


    @with_tempfile()
    @reseed_rng()
    def test_simple_storage(self, filename):
        ex1 = """eins zwei drei
        0 1 2
        3 4 5
        """
        skip_if_no_external('cPickle')
        skip_if_no_external('gzip')

        ex2 = {'d1': np.random.normal(size=(4,4))}

        hamster = Hamster(ex1=ex1)
        hamster.d = ex2
        hamster.boo = HamsterHelperTests

        total_dict = {'ex1' : ex1,
                      'd'   : ex2,
                      'boo' : HamsterHelperTests}
        self.assertTrue(hamster.asdict() == total_dict)
        self.assertTrue(set(hamster.registered) == set(['ex1', 'd', 'boo']))

        filename_gz = filename + '.gz'
        filename_bogusgz = filename + '_bogus.gz'

        # dump
        hamster.dump(filename)
        hamster.dump(filename_gz)
        # allow to shoot yourself in the head
        hamster.dump(filename_bogusgz, compresslevel=0)
        self.assertTrue(hamster.asdict() == total_dict)

        # We should have stored plain and gzipped versions
        gzplain = gzip.open(filename)
        self.assertRaises(IOError, gzplain.readlines)
        gzipped = gzip.open(filename_gz)
        discard = gzipped.readlines()
        gzbogus = gzip.open(filename_bogusgz)
        self.assertRaises(IOError, gzbogus.readlines)

        # load plain
        hamster2 = Hamster(filename)

        # check if we re-stored all the keys
        k =  hamster.__dict__.keys();
        k2 = hamster2.__dict__.keys();
        self.assertTrue(set(k) == set(k2))

        # identity should be lost
        self.assertTrue(hamster.ex1 is hamster.ex1)
        self.assertTrue(not (hamster.ex1 is hamster2.ex1))

        # lets compare
        self.assertTrue(hamster.ex1 == hamster2.ex1)

        self.assertTrue(hamster.d.keys() == hamster2.d.keys())
        self.assertTrue((hamster.d['d1'] == hamster2.d['d1']).all())


        self.assertTrue(hamster.boo == hamster2.boo)
        # not sure if that is a feature or a bug
        self.assertTrue(hamster.boo is hamster2.boo)

        # cleanup
        gzipped.close()
        os.remove(filename_gz)
        gzbogus.close()
        os.remove(filename_bogusgz)

    @reseed_rng()
    def test_assignment(self):
        ex1 = """eins zwei drei
        0 1 2
        3 4 5
        """
        ex2 = {'d1': np.random.normal(size=(4,4))}

        h = Hamster(ex1=ex1)
        h.ex2 = ex2
        self.assertTrue(hasattr(h, 'ex2'))
        h.ex2 = None
        self.assertTrue(h.ex2 is None)
        h.ex2 = 123
        self.assertTrue(h.ex2 == 123)
        h.has_key  = 123



def suite():  # pragma: no cover
    return unittest.makeSuite(HamsterHelperTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_hdf5
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for HDF5 converter'''

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets, saveload_warehouse

skip_if_no_external('h5py')
import h5py

import os
import tempfile

from mvpa2.base.dataset import AttrDataset, save
from mvpa2.base.hdf5 import h5save, h5load, obj2hdf, HDF5ConversionError
from mvpa2.misc.data_generators import load_example_fmri_dataset
from mvpa2.mappers.fx import mean_sample
from mvpa2.mappers.boxcar import BoxcarMapper

class HDFDemo(object):
    pass

class CustomOldStyle:
    pass

@nodebug(['ID_IN_REPR', 'MODULE_IN_REPR'])
def test_h5py_datasets():
    # this one stores and reloads all datasets in the warehouse
    rc_ds = saveload_warehouse()

    # global checks
    assert_equal(len(datasets), len(rc_ds))
    assert_equal(sorted(datasets.keys()), sorted(rc_ds.keys()))
    # check each one
    for d in datasets:
        ds = datasets[d]
        ds2 = rc_ds[d]
        assert_array_equal(ds.samples, ds2.samples)
        # we can check all sa and fa attrs
        for attr in ds.sa:
            assert_array_equal(ds.sa[attr].value, ds2.sa[attr].value)
        for attr in ds.fa:
            assert_array_equal(ds.fa[attr].value, ds2.fa[attr].value)
        # with datasets attributes it is more difficult, but we'll do some
        assert_equal(len(ds.a), len(ds2.a))
        assert_equal(sorted(ds.a.keys()), sorted(ds2.a.keys()))
        if 'mapper' in ds.a:
            # since we have no __equal__ do at least some comparison
            if __debug__:
                # debug mode needs special test as it enhances the repr output
                # with module info and id() appendix for objects
                assert_equal('#'.join(repr(ds.a.mapper).split('#')[:-1]),
                             '#'.join(repr(ds2.a.mapper).split('#')[:-1]))
            else:
                assert_equal(repr(ds.a.mapper), repr(ds2.a.mapper))


def test_h5py_dataset_typecheck():
    ds = datasets['uni2small']

    fd, fpath = tempfile.mkstemp('mvpa', 'test'); os.close(fd)
    fd, fpath2 = tempfile.mkstemp('mvpa', 'test'); os.close(fd)

    h5save(fpath2, [[1, 2, 3]])
    assert_raises(ValueError, AttrDataset.from_hdf5, fpath2)
    # this one just catches if there is such a group
    assert_raises(ValueError, AttrDataset.from_hdf5, fpath2, name='bogus')

    hdf = h5py.File(fpath, 'w')
    ds = AttrDataset([1, 2, 3])
    obj2hdf(hdf, ds, name='non-bogus')
    obj2hdf(hdf, [1, 2, 3], name='bogus')
    hdf.close()

    assert_raises(ValueError, AttrDataset.from_hdf5, fpath, name='bogus')
    ds_loaded = AttrDataset.from_hdf5(fpath, name='non-bogus')
    assert_array_equal(ds, ds_loaded)   # just to do smth useful with ds ;)

    # cleanup and ignore stupidity
    os.remove(fpath)
    os.remove(fpath2)


def test_matfile_v73_compat():
    mat = h5load(os.path.join(pymvpa_dataroot, 'v73.mat'))
    assert_equal(len(mat), 2)
    assert_equal(sorted(mat.keys()), ['x', 'y'])
    assert_array_equal(mat['x'], np.arange(6)[None].T)
    assert_array_equal(mat['y'], np.array([(1,0,1)], dtype='uint8').T)


def test_directaccess():
    f = tempfile.NamedTemporaryFile()
    h5save(f.name, 'test')
    assert_equal(h5load(f.name), 'test')
    f.close()
    f = tempfile.NamedTemporaryFile()
    h5save(f.name, datasets['uni4medium'])
    assert_array_equal(h5load(f.name).samples,
                       datasets['uni4medium'].samples)


def test_function_ptrs():
    if not externals.exists('nibabel'):
        raise SkipTest
    ds = load_example_fmri_dataset()
    # add a mapper with a function ptr inside
    ds = ds.get_mapped(mean_sample())
    f = tempfile.NamedTemporaryFile()
    h5save(f.name, ds)
    ds_loaded = h5load(f.name)
    fresh = load_example_fmri_dataset().O
    # check that the reconstruction function pointer in the FxMapper points
    # to the right one
    assert_array_equal(ds_loaded.a.mapper.forward(fresh),
                        ds.samples)

def test_various_special_cases():
    # 0d object ndarray
    f = tempfile.NamedTemporaryFile()
    a = np.array(0, dtype=object)
    h5save(f.name, a)
    a_ = h5load(f.name)
    ok_(a == a_)
    # slice
    h5save(f.name, slice(2,5,3))
    sl = h5load(f.name)
    ok_(sl == slice(2,5,3))

def test_class_oldstyle():
    # AttributeError: CustomOld instance has no attribute '__reduce__'

    # old style classes do not define reduce -- sure thing we might
    # not need to support them at all, but then some meaningful
    # exception should be thrown
    co = CustomOldStyle()
    co.v = 1
    f = tempfile.NamedTemporaryFile()
    assert_raises(HDF5ConversionError, save, co, f.name, compression='gzip')

def test_locally_defined_class():
    # cannot store locally defined classes
    class Custom(object):
        pass
    c = Custom()
    f = tempfile.NamedTemporaryFile()
    assert_raises(HDF5ConversionError, h5save, f.name, c, compression='gzip')

def test_dataset_without_chunks():
    #  ValueError: All chunk dimensions must be positive (Invalid arguments to routine: Out of range)
    # MH: This is not about Dataset chunks, but about an empty samples array
    f = tempfile.NamedTemporaryFile()
    ds = AttrDataset([8], a=dict(custom=1))
    save(ds, f.name, compression='gzip')
    ds_loaded = h5load(f.name)
    ok_(ds_loaded.a.custom == ds.a.custom)

def test_recursion():
    obj = range(2)
    obj.append(HDFDemo())
    obj.append(obj)
    f = tempfile.NamedTemporaryFile()
    h5save(f.name, obj)
    lobj = h5load(f.name)
    assert_equal(obj[:2], lobj[:2])
    assert_equal(type(obj[2]), type(lobj[2]))
    ok_(obj[3] is obj)
    ok_(lobj[3] is lobj)

@with_tempfile()
def test_h5save_mkdir(dirname):
    # create deeper directory name
    filename = os.path.join(dirname, 'a', 'b', 'c', 'test_file.hdf5')
    assert_raises(IOError, h5save, filename, {}, mkdir=False)

    # And create by default
    h5save(filename, {})
    ok_(os.path.exists(filename))
    d = h5load(filename)
    assert_equal(d, {})

    # And that we can still just save into a file in current directory
    # Let's be safe and assure going back to the original directory
    cwd = os.getcwd()
    try:
        os.chdir(dirname)
        h5save("TEST.hdf5", [1,2,3])
    finally:
        os.chdir(cwd)

def test_state_cycle_with_custom_reduce():
    # BoxcarMapper has a custom __reduce__ implementation . The 'space'
    # setting will only survive a svae/load cycle if the state is correctly
    # handle for custom reduce iplementations.
    bm = BoxcarMapper([0], 1, space='boxy')
    f = tempfile.NamedTemporaryFile()
    h5save(f.name, bm)
    bm_rl = h5load(f.name)
    assert_equal(bm_rl.get_space(), 'boxy')

def test_store_metaclass_types():
    f = tempfile.NamedTemporaryFile()
    from mvpa2.kernels.base import Kernel
    allowedtype=Kernel
    h5save(f.name, allowedtype)
    lkrn = h5load(f.name)
    assert_equal(lkrn, Kernel)
    assert_equal(lkrn.__metaclass__, Kernel.__metaclass__)

def test_state_setter_getter():
    # make sure the presence of custom __setstate__, __getstate__ methods
    # is honored -- numpy's RNGs have it
    from numpy.random.mtrand import RandomState
    f = tempfile.NamedTemporaryFile()
    r = RandomState()
    h5save(f.name, r)
    rl = h5load(f.name)
    rl_state = rl.get_state()
    for i, v in enumerate(r.get_state()):
        assert_array_equal(v, rl_state[i])


@sweepargs(obj=(
    # simple 1d -- would have worked before as well
    np.array([{'d': np.empty(shape=(2,3))}], dtype=object),
    # 2d -- before fix would be reconstructed incorrectly
    np.array([[{'d': np.empty(shape=(2,3))}]], dtype=object),
    # a bit more elaborate storage
    np.array([[{'d': np.empty(shape=(2,3)),
                'k': 33}]*2]*3, dtype=object),
    # Swaroop's use-case
    AttrDataset(np.array([{'d': np.empty(shape=(2,3))}], dtype=object)),
    # as it would be reconstructed before the fix -- obj array of obj arrays
    np.array([np.array([{'d': np.empty(shape=(2,3))}], dtype=object)],
             dtype=object),
    np.array([],dtype='int64'),
    ))
def test_save_load_object_dtype_ds(obj=None):
    """Test saving of custom object ndarray (GH #84)
    """
    aobjf = np.asanyarray(obj).flatten()

    if not aobjf.size and externals.versions['hdf5'] < '1.8.7':
        raise SkipTest("Versions of hdf5 before 1.8.7 have problems with empty arrays")

    # print obj, obj.shape
    f = tempfile.NamedTemporaryFile()

    # save/reload
    obj_ = saveload(obj, f.name)

    # and compare
    # neh -- not versatile enough
    #assert_objectarray_equal(np.asanyarray(obj), np.asanyarray(obj_))

    assert_array_equal(obj.shape, obj_.shape)
    assert_equal(type(obj), type(obj_))
    # so we could test both ds and arrays
    aobjf_ = np.asanyarray(obj_).flatten()
    # checks if having just array above
    if aobjf.size:
        assert_equal(type(aobjf[0]), type(aobjf_[0]))
        assert_array_equal(aobjf[0]['d'], aobjf_[0]['d'])


_python_objs = [
    # lists
    [1, 2], [],
    # tuples
    (1, 2), tuple(),
    # pure Python sets
    set([1,2]), set(), set([None]), set([tuple()]),
    ]
import collections
_python_objs.append([collections.deque([1,2])])
if hasattr(collections, 'OrderedDict'):
    _python_objs.append([collections.OrderedDict(),
                         collections.OrderedDict(a9=1, a0=2)])
if hasattr(collections, 'Counter'):
    _python_objs.append([collections.Counter({'red': 4, 'blue': 2})])

@sweepargs(obj=_python_objs)
def test_save_load_python_objs(obj):
    """Test saving objects of various types
    """
    # print obj, obj.shape
    f = tempfile.NamedTemporaryFile()

    # save/reload
    h5save(f.name, obj)
    obj_ = h5load(f.name)

    assert_equal(type(obj), type(obj_))
    assert_equal(obj, obj_)

def saveload(obj, f, backend='hdf5'):
    """Helper to save/load using some of tested backends
    """
    if backend == 'hdf5':
        h5save(f, obj)
        #import os; os.system('h5dump %s' % f)
        obj_ = h5load(f)
    else:
        #check pickle -- does it correctly
        import cPickle
        with open(f, 'w') as f_:
            cPickle.dump(obj, f_)
        with open(f) as f_:
            obj_ = cPickle.load(f_)
    return obj_

# Test some nasty nested constructs of mutable beasts
_nested_d = {0: 2}
_nested_d[1] = {
    0: {3: 4}, # to ease comprehension of the dump
    1: _nested_d}
_nested_d[1][2] = ['crap', _nested_d]   # 3rd level of nastiness

_nested_l = [2, None]
_nested_l[1] = [{3: 4}, _nested_l, None]
_nested_l[1][2] = ['crap', _nested_l]   # 3rd level of nastiness

@sweepargs(obj=[_nested_d, _nested_l])
@sweepargs(backend=['hdf5', 'pickle'])
@with_tempfile()
def test_nested_obj(f, backend, obj):
    ok_(obj[1][1] is obj)
    obj_ = saveload(obj, f, backend=backend)
    assert_equal(obj_[0], 2)
    assert_equal(obj_[1][0], {3: 4})
    ok_(obj_[1][1] is obj_)
    ok_(obj_[1][1] is not obj)  # nobody does teleportation

    # 3rd level
    ok_(obj_[1][2][1] is obj_)

_nested_a = np.array([1, 2], dtype=object)
_nested_a[1] = {1: 0, 2: _nested_a}

@sweepargs(a=[_nested_a])
@sweepargs(backend=['hdf5', 'pickle'])
@with_tempfile()
def test_nested_obj_arrays(f, backend, a):
    assert_equal(a.dtype, np.object)
    a_ = saveload(a, f, backend=backend)
    # import pydb; pydb.debugger()
    ok_(a_[1][2] is a_)

@sweepargs(backend=['hdf5','pickle'])
@with_tempfile()
def test_ca_col(f, backend):
    from mvpa2.base.state import ConditionalAttributesCollection, ConditionalAttribute
    c1 = ConditionalAttribute(name='ca1', enabled=True)
    #c2 = ConditionalAttribute(name='test2', enabled=True)
    col = ConditionalAttributesCollection([c1], name='whoknows')
    col.ca1 = col # {0: c1, 1: [None, col]}  # nest badly
    assert_true(col.ca1 is col)
    col_ = saveload(col, f, backend=backend)
    # seems to work niceish with pickle
    #print col_, col_.ca1, col_.ca1.ca1, col_.ca1.ca1.ca1
    assert_true(col_.ca1.ca1 is col_.ca1)
    # but even there top-level assignment test fails, which means it creates two
    # instances
    if backend != 'pickle':
        assert_true(col_.ca1 is col_)

# regression tests for datasets which have been previously saved

def test_reg_load_hyperalignment_example_hdf5():
    from mvpa2 import pymvpa_datadbroot
    filepath = os.path.join(pymvpa_datadbroot,
                        'hyperalignment_tutorial_data',
                        'hyperalignment_tutorial_data.hdf5.gz')
    if not os.path.exists(filepath):
        raise SkipTest("No hyperalignment tutorial data available under %s" %
                       filepath)
    ds_all = h5load(filepath)

    ds = ds_all[0]
    # First mapper was a FlattenMapper
    flat_mapper = ds.a.mapper[0]
    assert_equal(flat_mapper.shape, (61, 73, 61))
    assert_equal(flat_mapper.pass_attr, None)
    assert_false('ERROR' in str(flat_mapper))
    ds_reversed = ds.a.mapper.reverse(ds)
    assert_equal(ds_reversed.shape, (len(ds),) + flat_mapper.shape)

@with_tempfile()
def test_save_load_FlattenMapper(f):
    from mvpa2.mappers.flatten import FlattenMapper
    fm = FlattenMapper()
    ds = datasets['3dsmall']
    ds_ = fm(ds)
    ds_r = fm.reverse(ds_)
    fm_ = saveload(fm, f)
    assert_equal(fm_.shape, fm.shape)

@with_tempfile()
def test_versions(f):
    h5save(f, [])
    hdf = h5py.File(f, 'r')
    assert_equal(hdf.attrs.get('__pymvpa_hdf5_version__'), '2')
    assert_equal(hdf.attrs.get('__pymvpa_version__'), mvpa2.__version__)

########NEW FILE########
__FILENAME__ = test_hdf5_clf
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for storage of classifiers in HDF5'''


from mvpa2.testing import *
skip_if_no_external('h5py')
skip_if_no_external('scipy')

import numpy as np
from mvpa2.base import cfg
from mvpa2.testing.datasets import datasets
from mvpa2.clfs.base import Classifier
from mvpa2.generators.splitters import Splitter
from mvpa2.measures.base import TransferMeasure
from mvpa2.misc.errorfx import corr_error, mean_mismatch_error
from mvpa2.mappers.fx import BinaryFxNode

from mvpa2.clfs.warehouse import clfswh, regrswh

import tempfile

from mvpa2.base.hdf5 import h5save, h5load, obj2hdf


@sweepargs(lrn=clfswh[:] + regrswh[:])
def test_h5py_clfs(lrn):
    # lets simply clone it so we could make its all states on
    lrn = lrn.clone()
    # Lets enable all the states
    lrn.ca.enable('all')

    f = tempfile.NamedTemporaryFile()

    # Store/reload untrained learner
    try:
        h5save(f.name, lrn)
    except Exception, e:
        raise AssertionError, \
              "Failed to store due to %r" % (e,)

    try:
        lrn_ = h5load(f.name)
        pass
    except Exception, e:
        raise AssertionError, \
              "Failed to load due to %r" % (e,)

    ok_(isinstance(lrn_, Classifier))
    # Verify that we have the same ca enabled
    # XXX FAILS atm!
    #ok_(set(lrn.ca.enabled) == set(lrn_.ca.enabled))

    # lets choose a dataset
    dsname, errorfx = \
            {False: ('uni2large', mean_mismatch_error),
             True: ('sin_modulated', corr_error)}\
            ['regression' in lrn.__tags__]
    ds = datasets[dsname]
    splitter = Splitter('train')
    postproc = BinaryFxNode(errorfx, 'targets')
    te = TransferMeasure(lrn, splitter, postproc=postproc)
    te_ = TransferMeasure(lrn_, splitter, postproc=postproc)

    error = te(ds)
    error_ = te_(ds)


    if len(set(['swig', 'rpy2']).intersection(lrn.__tags__)):
        raise SkipTest("Trained swigged and R-interfaced classifiers can't "
                       "be stored/reloaded yet")

    # now lets store/reload the trained one
    try:
        h5save(f.name, lrn_)
    except Exception, e:
        raise AssertionError, \
              "Failed to store trained lrn due to %r" % (e,)

    # This lrn__ is doubly stored/loaded ;-)
    try:
        lrn__ = h5load(f.name)
    except Exception, e:
        raise AssertionError, \
              "Failed to load trained lrn due to %r" % (e,)

    # Verify that we have the same ca enabled
    # TODO
    #ok_(set(lrn.ca.enabled) == set(lrn__.ca.enabled))
    # and having the same values?
    # TODO

    # now lets do predict and manually compute error
    predictions = lrn__.predict(ds[ds.sa.train == 2].samples)
    error__ = errorfx(predictions, ds[ds.sa.train == 2].sa.targets)

    if 'non-deterministic' in lrn_.__tags__:
        # might be different... let's allow to vary quite a bit
        # and new error should be no more than twice the old one
        # (better than no check at all)
        # TODO: smarter check, since 'twice' is quite coarse
        #       especially if original error happens to be 0 ;)
        if cfg.getboolean('tests', 'labile', default='yes'):
            ok_(np.asscalar(error_) <= 2*np.asscalar(error))
            ok_(np.asscalar(error__) <= 2*np.asscalar(error))
    else:
        # must match precisely
        assert_array_equal(error, error_)
        assert_array_equal(error, error__)

    # TODO: verify ca's

    #print "I PASSED!!!! %s" % lrn

########NEW FILE########
__FILENAME__ = test_hyperalignment
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA ..."""

import unittest
import numpy as np

from mvpa2.base import cfg
from mvpa2.datasets.base import Dataset

# See other tests and test_procrust.py for some example on what to do ;)
from mvpa2.algorithms.hyperalignment import Hyperalignment
from mvpa2.mappers.zscore import zscore
from mvpa2.misc.support import idhash
from mvpa2.misc.data_generators import random_affine_transformation
from mvpa2.misc.fx import get_random_rotation

# Somewhat slow but provides all needed ;)
from mvpa2.testing import sweepargs, reseed_rng
from mvpa2.testing.datasets import datasets

from mvpa2.generators.partition import NFoldPartitioner

# if you need some classifiers
#from mvpa2.testing.clfs import *

class HyperAlignmentTests(unittest.TestCase):


    @sweepargs(zscore_all=(False, True))
    @sweepargs(zscore_common=(False, True))
    @sweepargs(ref_ds=(None, 2))
    @reseed_rng()
    def test_basic_functioning(self, ref_ds, zscore_common, zscore_all):
        ha = Hyperalignment(ref_ds=ref_ds,
                            zscore_all=zscore_all,
                            zscore_common=zscore_common)
        if ref_ds is None:
            ref_ds = 0                      # by default should be this one

        # get a dataset with some prominent trends in it
        ds4l = datasets['uni4large']
        # lets select for now only meaningful features
        ds_orig = ds4l[:, ds4l.a.nonbogus_features]
        nf = ds_orig.nfeatures
        n = 4 # # of datasets to generate
        Rs, dss_rotated, dss_rotated_clean, random_shifts, random_scales \
            = [], [], [], [], []

        # now lets compose derived datasets by using some random
        # rotation(s)
        for i in xrange(n):
            ## if False: # i == ref_ds:
            #     # Do not rotate the target space so we could check later on
            #     # if we transform back nicely
            #     R = np.eye(ds_orig.nfeatures)
            ## else:
            ds_ = random_affine_transformation(ds_orig, scale_fac=100, shift_fac=10)
            Rs.append(ds_.a.random_rotation)
            # reusing random data from dataset itself
            random_scales += [ds_.a.random_scale]
            random_shifts += [ds_.a.random_shift]
            random_noise = ds4l.samples[:, ds4l.a.bogus_features[:4]]

            ## if (zscore_common or zscore_all):
            ##     # for later on testing of "precise" reconstruction
            ##     zscore(ds_, chunks_attr=None)

            dss_rotated_clean.append(ds_)

            ds_ = ds_.copy()
            ds_.samples = ds_.samples + 0.1 * random_noise
            dss_rotated.append(ds_)

        # Lets test two scenarios -- in one with no noise -- we should get
        # close to perfect reconstruction.  If noise was added -- not so good
        for noisy, dss in ((False, dss_rotated_clean),
                           (True, dss_rotated)):
            # to verify that original datasets didn't get changed by
            # Hyperalignment store their idhashes of samples
            idhashes = [idhash(ds.samples) for ds in dss]
            idhashes_targets = [idhash(ds.targets) for ds in dss]

            mappers = ha(dss)

            idhashes_ = [idhash(ds.samples) for ds in dss]
            idhashes_targets_ = [idhash(ds.targets) for ds in dss]
            self.assertEqual(idhashes, idhashes_,
                msg="Hyperalignment must not change original data.")
            self.assertEqual(idhashes_targets, idhashes_targets_,
                msg="Hyperalignment must not change original data targets.")

            self.assertEqual(ref_ds, ha.ca.chosen_ref_ds)

            # Map data back

            dss_clean_back = [m.forward(ds_)
                              for m, ds_ in zip(mappers, dss_rotated_clean)]

            ds_norm = np.linalg.norm(dss[ref_ds].samples)
            nddss = []
            ndcss = []
            ds_orig_Rref = np.dot(ds_orig.samples, Rs[ref_ds]) \
                           * random_scales[ref_ds] \
                           + random_shifts[ref_ds]
            if zscore_common or zscore_all:
                zscore(Dataset(ds_orig_Rref), chunks_attr=None)
            for ds_back in dss_clean_back:
                # if we used zscoring of common, we cannot rely
                # that range/offset could be matched, so lets use
                # corrcoef
                ndcs = np.diag(np.corrcoef(ds_back.samples.T,
                                           ds_orig_Rref.T)[nf:, :nf], k=0)
                ndcss += [ndcs]
                dds = ds_back.samples - ds_orig_Rref
                ndds = np.linalg.norm(dds) / ds_norm
                nddss += [ndds]
            snoisy = ('clean', 'noisy')[int(noisy)]
            do_labile = cfg.getboolean('tests', 'labile', default='yes')
            if not noisy or do_labile:
                # First compare correlations
                self.assertTrue(np.all(np.array(ndcss)
                                       >= (0.9, 0.85)[int(noisy)]),
                        msg="Should have reconstructed original dataset more or"
                        " less. Got correlations %s in %s case."
                        % (ndcss, snoisy))
                if not (zscore_all or zscore_common):
                    # if we didn't zscore -- all of them should be really close
                    self.assertTrue(np.all(np.array(nddss)
                                       <= (1e-10, 1e-1)[int(noisy)]),
                        msg="Should have reconstructed original dataset well "
                        "without zscoring. Got normed differences %s in %s case."
                        % (nddss, snoisy))
                elif do_labile:
                    # otherwise they all should be somewhat close
                    self.assertTrue(np.all(np.array(nddss)
                                           <= (.2, 3)[int(noisy)]),
                        msg="Should have reconstructed original dataset more or"
                        " less for all. Got normed differences %s in %s case."
                        % (nddss, snoisy))
                    self.assertTrue(np.all(nddss[ref_ds] <= .09),
                        msg="Should have reconstructed original dataset quite "
                        "well even with zscoring. Got normed differences %s "
                        "in %s case." % (nddss, snoisy))
                    # yoh: and leave 5% of difference for a chance and numerical
                    #      fluctuations ;)
                    self.assertTrue(np.all(np.array(nddss) >= 0.95*nddss[ref_ds]),
                        msg="Should have reconstructed orig_ds best of all. "
                        "Got normed differences %s in %s case with ref_ds=%d."
                        % (nddss, snoisy, ref_ds))

        # Lets see how well we do if asked to compute residuals
        ha = Hyperalignment(ref_ds=ref_ds, level2_niter=2,
                            enable_ca=['training_residual_errors',
                                       'residual_errors'])
        mappers = ha(dss_rotated_clean)
        self.assertTrue(np.all(ha.ca.training_residual_errors.sa.levels ==
                              ['1', '2:0', '2:1']))
        rterrors = ha.ca.training_residual_errors.samples
        # just basic tests:
        self.assertEqual(rterrors[0, ref_ds], 0)
        self.assertEqual(rterrors.shape, (3, n))
        rerrors = ha.ca.residual_errors.samples
        self.assertEqual(rerrors.shape, (1, n))


    def _test_on_swaroop_data(self):  # pragma: no cover
        #
        print "Running swaroops test on data we don't have"
        #from mvpa2.datasets.miscfx import zscore
        #from mvpa2.featsel.helpers import FixedNElementTailSelector
        #   or just for lazy ones like yarik atm
        #enable to test from mvpa2.suite import *
        subj = ['cb', 'dm', 'hj', 'kd', 'kl', 'mh', 'ph', 'rb', 'se', 'sm']
        ds = []
        for sub in subj:
            ds.append(fmri_dataset(samples=sub+'_movie.nii.gz',
                                   mask=sub+'_mask_vt.nii.gz'))

        '''
        Compute feature ranks in each dataset
        based on correlation with other datasets
        '''
        feature_scores = [ np.zeros(ds[i].nfeatures) for i in range(len(subj)) ]
        '''
        for i in range(len(subj)):
            ds_temp = ds[i].samples - np.mean(ds[i].samples, axis=0)
            ds_temp = ds_temp / np.sqrt( np.sum( np.square(ds_temp), axis=0) )
            for j in range(i+1,len(subj)):
            ds_temp2 = ds[j].samples - np.mean(ds[j].samples, axis=0)
            ds_temp2 = ds_temp2 / np.sqrt( np.sum( np.square(ds_temp2), axis=0) )
            corr_temp= np.asarray(np.mat(np.transpose(ds_temp))*np.mat(ds_temp2))
            feature_scores[i] = feature_scores[i] + np.max(corr_temp, axis = 1)
            feature_scores[j] = feature_scores[j] + np.max(corr_temp, axis = 0)
        '''
        for i, sd in enumerate(ds):
            ds_temp = sd.copy()
            zscore(ds_temp, chunks_attr=None)
            for j, sd2 in enumerate(ds[i+1:]):
                ds_temp2 = sd2.copy()
                zscore(ds_temp2, chunks_attr=None)
                corr_temp = np.dot(ds_temp.samples.T, ds_temp2.samples)
                feature_scores[i] = feature_scores[i] + \
                                    np.max(corr_temp, axis = 1)
                feature_scores[j+i+1] = feature_scores[j+i+1] + \
                                        np.max(corr_temp, axis = 0)

        for i, sd in enumerate(ds):
            sd.fa['bsc_scores'] = feature_scores[i]

        fselector = FixedNElementTailSelector(2000,
                                              tail='upper', mode='select')

        ds_fs = [ sd[:, fselector(sd.fa.bsc_scores)] for sd in ds]

        hyper = Hyperalignment()
        mapper_results = hyper(ds_fs)

        md_cd = ColumnData('labels.txt', header=['label'])
        md_labels = [int(x) for x in md_cd['label']]
        for run in range(8):
            md_labels[192*run:192*run+3] = [-1]*3

        mkdg_ds = []
        for sub in subj:
            mkdg_ds.append(fmri_dataset(
                samples=sub+'_mkdg.nii.gz', targets=md_labels,
                chunks=np.repeat(range(8), 192), mask=sub+'_mask_vt.nii.gz'))

        m = mean_group_sample(['targets', 'chunks'])

        mkdg_ds = [ds_.get_mapped(m) for ds_ in mkdg_ds]
        mkdg_ds = [ds_[ds_.sa.targets != -1] for ds_ in mkdg_ds]
        [zscore(ds_, param_est=('targets', [0])) for ds_ in mkdg_ds]
        mkdg_ds = [ds_[ds_.sa.targets != 0] for ds_ in mkdg_ds]

        for i, sd in enumerate(mkdg_ds):
            sd.fa['bsc_scores'] = feature_scores[i]

        mkdg_ds_fs = [ sd[:, fselector(sd.fa.bsc_scores)] for sd in mkdg_ds]
        mkdg_ds_mapped = [ sd.get_mapped(mapper_results[i])
                           for i, sd in enumerate(mkdg_ds_fs)]

        # within-subject classification
        within_acc = []
        clf = clfswh['multiclass', 'linear', 'NU_SVC'][0]
        cvterr = CrossValidation(clf, NFoldPartitioner(),
                                 enable_ca=['confusion'])
        for sd in mkdg_ds_fs:
            wsc = cvterr(sd)
            within_acc.append(1-np.mean(wsc))

        within_acc_mapped = []
        for sd in mkdg_ds_mapped:
            wsc = cvterr(sd)
            within_acc_mapped.append(1-np.mean(wsc))

        print np.mean(within_acc)
        print np.mean(within_acc_mapped)

        mkdg_ds_all = vstack(mkdg_ds_mapped)
        mkdg_ds_all.sa['subject'] = np.repeat(range(10), 56)
        mkdg_ds_all.sa['chunks'] = mkdg_ds_all.sa['subject']

        bsc = cvterr(mkdg_ds_all)
        print 1-np.mean(bsc)
        mkdg_all = vstack(mkdg_ds_fs)
        mkdg_all.sa['chunks'] = np.repeat(range(10), 56)
        bsc_orig = cvterr(mkdg_all)
        print 1-np.mean(bsc_orig)
        pass



def suite():  # pragma: no cover
    return unittest.makeSuite(HyperAlignmentTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_ifs
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA incremental feature search."""

from mvpa2.testing import *
from mvpa2.testing.clfs import *
from mvpa2.testing.datasets import datasets


from mvpa2.base.dataset import vstack
from mvpa2.datasets.base import Dataset
from mvpa2.featsel.ifs import IFS
from mvpa2.measures.base import CrossValidation, ProxyMeasure
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.generators.splitters import Splitter
from mvpa2.featsel.helpers import FixedNElementTailSelector
from mvpa2.mappers.fx import mean_sample, BinaryFxNode
from mvpa2.misc.errorfx import mean_mismatch_error



class IFSTests(unittest.TestCase):

    ##REF: Name was automagically refactored
    def get_data(self):
        data = np.random.standard_normal(( 100, 2, 2, 2 ))
        labels = np.concatenate( ( np.repeat( 0, 50 ),
                                  np.repeat( 1, 50 ) ) )
        chunks = np.repeat( range(5), 10 )
        chunks = np.concatenate( (chunks, chunks) )
        return Dataset.from_wizard(samples=data, targets=labels, chunks=chunks)


    # XXX just testing based on a single classifier. Not sure if
    # should test for every known classifier since we are simply
    # testing IFS algorithm - not sensitivities
    @sweepargs(svm=clfswh['has_sensitivity', '!meta'][:1])
    @reseed_rng()
    def test_ifs(self, svm):

        # measure for feature selection criterion and performance assesment
        # use the SAME clf!
        errorfx = mean_mismatch_error
        fmeasure = CrossValidation(svm, NFoldPartitioner(), postproc=mean_sample())
        pmeasure = ProxyMeasure(svm, postproc=BinaryFxNode(errorfx, 'targets'))

        ifs = IFS(fmeasure,
                  pmeasure,
                  Splitter('purpose', attr_values=['train', 'test']),
                  fselector=\
                    # go for lower tail selection as data_measure will return
                    # errors -> low is good
                    FixedNElementTailSelector(1, tail='lower', mode='select'),
                  )
        wdata = self.get_data()
        wdata.sa['purpose'] = np.repeat('train', len(wdata))
        tdata = self.get_data()
        tdata.sa['purpose'] = np.repeat('test', len(tdata))
        ds = vstack((wdata, tdata))
        orig_nfeatures = ds.nfeatures

        ifs.train(ds)
        resds = ifs(ds)

        # fail if orig datasets are changed
        self.assertTrue(ds.nfeatures == orig_nfeatures)

        # check that the features set with the least error is selected
        self.assertTrue(len(ifs.ca.errors))
        e = np.array(ifs.ca.errors)
        self.assertTrue(resds.nfeatures == e.argmin() + 1)


        # repeat with dataset where selection order is known
        wsignal = datasets['dumb2'].copy()
        wsignal.sa['purpose'] = np.repeat('train', len(wsignal))
        tsignal = datasets['dumb2'].copy()
        tsignal.sa['purpose'] = np.repeat('test', len(tsignal))
        signal = vstack((wsignal, tsignal))
        ifs.train(signal)
        resds = ifs(signal)
        self.assertTrue((resds.samples[:,0] == signal.samples[:,0]).all())


def suite():  # pragma: no cover
    return unittest.makeSuite(IFSTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_iohelpers
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA IO helpers"""

import re
import os
import unittest
from tempfile import mkstemp
import numpy as np

from mvpa2.testing.tools import ok_, assert_equal, with_tempfile

from mvpa2 import pymvpa_dataroot
from mvpa2.datasets.eventrelated import find_events
from mvpa2.misc.io import *
from mvpa2.misc.fsl import *
from mvpa2.misc.bv import BrainVoyagerRTC


class IOHelperTests(unittest.TestCase):

    def test_column_data_from_file(self):
        ex1 = """eins zwei drei
        0 1 2
        3 4 5
        """
        fd, fpath = mkstemp('mvpa', 'test'); os.close(fd)
        file = open(fpath, 'w')
        file.write(ex1)
        file.close()

        # intentionally rely on defaults
        d = ColumnData(fpath, header=True)

        # check header (sort because order in dict is unpredictable)
        self.assertTrue(sorted(d.keys()) == ['drei', 'eins', 'zwei'])

        self.assertTrue(d['eins'] == [0, 3])
        self.assertTrue(d['zwei'] == [1, 4])
        self.assertTrue(d['drei'] == [2, 5])

        # make a copy
        d2 = ColumnData(d)

        # check if identical
        self.assertTrue(sorted(d2.keys()) == ['drei', 'eins', 'zwei'])
        self.assertTrue(d2['eins'] == [0, 3])
        self.assertTrue(d2['zwei'] == [1, 4])
        self.assertTrue(d2['drei'] == [2, 5])

        # now merge back
        d += d2

        # same columns?
        self.assertTrue(sorted(d.keys()) == ['drei', 'eins', 'zwei'])

        # but more data
        self.assertEqual(d['eins'], [0, 3, 0, 3])
        self.assertEqual(d['zwei'], [1, 4, 1, 4])
        self.assertEqual(d['drei'], [2, 5, 2, 5])

        # test file write
        # TODO: check if correct
        header_order = ['drei', 'zwei', 'eins']
        d.tofile(fpath, header_order=header_order)

        # test sample selection
        dsel = d.select_samples([0, 2])
        self.assertEqual(dsel['eins'], [0, 0])
        self.assertEqual(dsel['zwei'], [1, 1])
        self.assertEqual(dsel['drei'], [2, 2])

        # test if order is read from file when available
        d3 = ColumnData(fpath)
        self.assertEqual(d3._header_order, header_order)

        # add another column -- should be appended as the last column
        # while storing
        d3['four'] = [0.1] * len(d3['eins'])
        d3.tofile(fpath)

        d4 = ColumnData(fpath)
        self.assertEqual(d4._header_order, header_order + ['four'])

        # cleanup and ignore stupidity
        try:
            os.remove(fpath)
        except WindowsError:
            pass


    def test_samples_attributes(self):
        sa = SampleAttributes(os.path.join(pymvpa_dataroot,
                                           'attributes_literal.txt'),
                              literallabels=True)

        ok_(sa.nrows == 1452, msg='There should be 1452 samples')

        # convert to event list, with some custom attr
        ev = find_events(**sa)
        ok_(len(ev) == 17 * (max(sa.chunks) + 1),
            msg='Not all events got detected.')

        ok_(ev[0]['targets'] == ev[-1]['targets'] == 'rest',
            msg='First and last event are rest condition.')

        ok_(ev[-1]['onset'] + ev[-1]['duration'] == sa.nrows,
            msg='Something is wrong with the timiing of the events')


    @with_tempfile('mvpa', 'sampleattr')
    def test_samples_attributes_autodtype(self, fn):
        payload = '''a b c
1 1.1 a
2 2.2 b
3 3.3 c
4 4.4 d'''

        with open(fn, 'w') as f:
            f.write(payload)

        attr = SampleAttributes(fn, header=True)

        assert_equal(set(attr.keys()), set(['a', 'b', 'c']))
        assert_equal(attr['a'], [1, 2, 3, 4])
        assert_equal(attr['b'], [1.1, 2.2, 3.3, 4.4])
        assert_equal(attr['c'], ['a', 'b', 'c', 'd'])


    def test_fsl_ev(self):
        ex1 = """0.0 2.0 1
        13.89 2 1
        16 2.0 0.5
        """
        fd, fpath = mkstemp('mvpa', 'test'); os.close(fd)
        file = open(fpath, 'w')
        file.write(ex1)
        file.close()

        # intentionally rely on defaults
        d = FslEV3(fpath)

        # check header (sort because order in dict is unpredictable)
        self.assertTrue(sorted(d.keys()) == \
            ['durations', 'intensities', 'onsets'])

        self.assertTrue(d['onsets'] == [0.0, 13.89, 16.0])
        self.assertTrue(d['durations'] == [2.0, 2.0, 2.0])
        self.assertTrue(d['intensities'] == [1.0, 1.0, 0.5])

        self.assertTrue(d.nevs == 3)
        self.assertTrue(d.get_ev(1) == (13.89, 2.0, 1.0))
        # cleanup and ignore stupidity
        try:
            os.remove(fpath)
        except WindowsError:
            pass

        d = FslEV3(os.path.join(pymvpa_dataroot, 'fslev3.txt'))
        ev = d.to_events()
        self.assertTrue(len(ev) == 3)
        self.assertTrue([e['duration'] for e in ev] == [9] * 3)
        self.assertTrue([e['onset'] for e in ev] == [6, 21, 35])
        self.assertTrue([e['features'] for e in ev] == [[1], [1], [1]])

        ev = d.to_events(label='face', chunk=0, crap=True)
        ev[0]['label'] = 'house'
        self.assertTrue(len(ev) == 3)
        self.assertTrue([e['duration'] for e in ev] == [9] * 3)
        self.assertTrue([e['onset'] for e in ev] == [6, 21, 35])
        self.assertTrue([e['features'] for e in ev] == [[1], [1], [1]])
        self.assertTrue([e['label'] for e in ev] == ['house', 'face', 'face'])
        self.assertTrue([e['chunk'] for e in ev] == [0] * 3)
        self.assertTrue([e['crap'] for e in ev] == [True] * 3)


    def test_fsl_ev2(self):
        attr = SampleAttributes(os.path.join(pymvpa_dataroot, 'smpl_attr.txt'))

        # check header (sort because order in dict is unpredictable)
        self.assertTrue(sorted(attr.keys()) == \
            ['chunks', 'targets'])

        self.assertTrue(attr.nsamples == 3)

    def test_bv_rtc(self):
        """Simple testing of reading RTC files from BrainVoyager"""

        attr = BrainVoyagerRTC(os.path.join(pymvpa_dataroot, 'bv', 'smpl_model.rtc'))
        self.assertEqual(attr.ncolumns, 4, "We must have 4 colums")
        self.assertEqual(attr.nrows, 147, "We must have 147 rows")

        self.assertEqual(attr._header_order,
                ['l_60 B', 'r_60 B', 'l_80 B', 'r_80 B'],
                "We must got column names correctly")
        self.assertTrue(len(attr.r_60_B) == attr.nrows,
                "We must have got access to column by property")
        self.assertTrue(attr.toarray() != None,
                "We must have got access to column by property")

    def testdesign2labels(self):
        """Simple testing of helper Design2Labels"""

        attr = BrainVoyagerRTC(os.path.join(pymvpa_dataroot, 'bv', 'smpl_model.rtc'))
        labels0 = design2labels(attr, baseline_label='silence')
        labels = design2labels(attr, baseline_label='silence',
                                func=lambda x:x > 0.5)
        Nsilence = lambda x:len(np.where(np.array(x) == 'silence')[0])

        nsilence0 = Nsilence(labels0)
        nsilence = Nsilence(labels)
        self.assertTrue(nsilence0 < nsilence,
                        "We must have more silence if thr is higher")
        self.assertEqual(len(labels), attr.nrows,
                        "We must have the same number of labels as rows")
        self.assertRaises(ValueError, design2labels, attr,
                        baseline_label='silence', func=lambda x:x > -1.0)


    def testlabels2chunks(self):
        attr = BrainVoyagerRTC(os.path.join(pymvpa_dataroot, 'bv', 'smpl_model.rtc'))
        labels = design2labels(attr, baseline_label='silence')
        self.assertRaises(ValueError, labels2chunks, labels, 'bugga')
        chunks = labels2chunks(labels)
        self.assertEqual(len(labels), len(chunks))
        # we must got them in sorted order
        chunks_sorted = np.sort(chunks)
        self.assertTrue((chunks == chunks_sorted).all())
        # for this specific one we must have just 4 chunks
        self.assertTrue((np.unique(chunks) == range(4)).all())


    def test_sensor_locations(self):
        sl = XAVRSensorLocations(os.path.join(pymvpa_dataroot, 'xavr1010.dat'))

        for var in ['names', 'pos_x', 'pos_y', 'pos_z']:
            self.assertTrue(len(eval('sl.' + var)) == 31)


    def test_fsl_glm_design(self):
        glm = FslGLMDesign(os.path.join(pymvpa_dataroot, 'glm.mat'))

        self.assertTrue(glm.mat.shape == (850, 6))
        self.assertTrue(len(glm.ppheights) == 6)

    def test_read_fsl_design(self):
        fname = os.path.join(pymvpa_dataroot,
                             'sample_design.fsf')
        # use our function
        design = read_fsl_design(fname)
        # and just load manually to see either we match fine
        set_lines = [x for x in open(fname).readlines()
                     if x.startswith('set ')]
        assert_equal(len(set_lines), len(design))

        # figure out which one is missing
        """TODO: would require the same special treatment for _files fields
        re_set = re.compile("set ([^)]*\)).*")
        for line in set_lines:
            key = re_set.search(line).groups()[0]
            if not key in design:
                raise AssertionError(
                    "Key %s was not found in read FSL design" % key)
        key_list = [' '.join(l.split(None,2)[1:2]) for l in set_lines]
        for k in set(key_list):
            if len([key for key in key_list if key == k]) == 2:
                raise AssertionError(
                    "Got the non-unique beast %s" % k)
                    """

def suite():  # pragma: no cover
    return unittest.makeSuite(IOHelperTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_kernel
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA kernels"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets

from mvpa2.base.externals import exists
from mvpa2.datasets import Dataset
from mvpa2.clfs.distance import squared_euclidean_distance, \
     pnorm_w, pnorm_w_python

import mvpa2.kernels.np as npK
from mvpa2.kernels.base import PrecomputedKernel, CachedKernel
try:
    import mvpa2.kernels.sg as sgK
    _has_sg = exists('shogun')
except RuntimeError:
    _has_sg = False


class KernelTests(unittest.TestCase):
    """Test bloody kernels
    """

    # mvpa2.kernel stuff

    def kernel_equiv(self, k1, k2, accuracy=None, relative_precision=0.6):
        """Test how accurately two kernels agree

        Parameters
        ----------
        k1 : kernel
        k2 : kernel
        accuracy : None or float
          To what accuracy to operate.  If None, length of mantissa
          (precision) is taken into account together with
          relative_precision to provide the `accuracy`.
        relative_precision : float, optional
          What proportion of leading digits in mantissa should match
          between k1 and k2 (effective only if `precision` is None).
        """
        k1m = k1.as_np()._k.copy() #; k1.as_np()._k.setflags(write=0)
        k2m = k2.as_np()._k.copy() #; k2.as_np()._k.setflags(write=0)

        # We should operate on mantissas (given exponents are the same) since
        # pure difference makes no sense to compare and we care about
        # digits in mantissa but there is no convenient way to compare
        # by mantissa:
        #      unfortunately there is no assert_array_approx_equal so
        #      we could specify number of significant digits to use.
        #      assert_array_almost_equal relies on number of decimals AFTER
        #      comma, so both
        #       assert_array_almost_equal([11111.001], [11111.002], decimal=4)
        #       and
        #       assert_array_almost_equal([0.001], [0.002], decimal=4)
        #      would fail, whenever
        #       assert_approx_equal(11111.001, 11111.002, significant=3)
        #      would be ok

        # assert_array_almost_equal(k1m, k2m, decimal=6)
        # assert_approx_equal(k1m, k2m, significant=12)

        if accuracy is None:
            # What precision should be operate at given relative_precision
            # and current dtype

            # first check if dtypes are the same
            ok_(k1m.dtype is k2m.dtype)

            k12mean = 0.5 * (np.abs(k1m) + np.abs(k2m))
            scales = np.ones(k12mean.shape)

            # don't bother dealing with values which would be within
            # resolution -- ** operation would lead to NaNs or 0s
            k12mean_nz = k12mean >= np.finfo(k1m.dtype).resolution * 1e+1
            scales[k12mean_nz] = 10 ** np.floor(np.log10(k12mean[k12mean_nz]))
            for a in (k1m, k2m):
                # lets normalize by exponent first
                anz = a != 0
                # "remove" exponent
                a[anz] /= scales[anz]

            accuracy = 10 ** -(np.finfo(k1m.dtype).precision * relative_precision)

        diff = np.abs(k1m - k2m)
        dmax = diff.max()               # and maximal difference
        dmax_index = np.unravel_index(np.argmax(diff), diff.shape)

        self.assertTrue(dmax <= accuracy,
                        '\n%s\nand\n%s\ndiffer by %s at %s:\n  %.15e\n  %.15e'
                        % (k1, k2, dmax, dmax_index,
                           k1.as_np()._k.__getitem__(dmax_index),
                           k2.as_np()._k.__getitem__(dmax_index)))

        self.assertTrue(np.all(k1m.astype('float32') == \
                              k2m.astype('float32')),
                        '\n%s\nand\n%s\nare unequal as float32' % (k1, k2))


    def test_linear_kernel(self):
        """Simplistic testing of linear kernel"""
        d1 = Dataset(np.asarray([range(5)] * 10, dtype=float))
        lk = npK.LinearKernel()
        lk.compute(d1)
        self.assertTrue(lk._k.shape == (10, 10),
                        "Failure computing LinearKernel (Size mismatch)")
        self.assertTrue((lk._k == 30).all(),
                        "Failure computing LinearKernel")

    @reseed_rng()
    def test_precomputed_kernel(self):
        """Statistic Kernels"""
        d = np.random.randn(50, 50)
        nk = PrecomputedKernel(matrix=d)
        nk.compute()
        self.assertTrue((d == nk._k).all(),
                        'Failure setting and retrieving PrecomputedKernel data')

    @reseed_rng()
    def test_cached_kernel(self):
        nchunks = 5
        n = 50 * nchunks
        d = Dataset(np.random.randn(n, 132))
        d.sa.chunks = np.random.randint(nchunks, size=n)

        # We'll compare against an Rbf just because it has a parameter to change
        rk = npK.RbfKernel(sigma=1.5)

        # Assure two kernels are independent for this test
        ck = CachedKernel(kernel=npK.RbfKernel(sigma=1.5))
        ck.compute(d) # Initial cache of all data

        self.assertTrue(ck._recomputed,
                        'CachedKernel was not initially computed')

        # Try some splitting
        for chunk in [d[d.sa.chunks == i] for i in range(nchunks)]:
            rk.compute(chunk)
            ck.compute(chunk)
            self.kernel_equiv(rk, ck) #, accuracy=1e-12)
            self.failIf(ck._recomputed,
                        "CachedKernel incorrectly recomputed it's kernel")

        # Test what happens when a parameter changes
        ck.params.sigma = 3.5
        ck.compute(d)
        self.assertTrue(ck._recomputed,
                        "CachedKernel doesn't recompute on kernel change")
        rk.params.sigma = 3.5
        rk.compute(d)
        self.assertTrue(np.all(rk._k == ck._k),
                        'Cached and rbf kernels disagree after kernel change')

        # Now test handling new data
        d2 = Dataset(np.random.randn(32, 43))
        ck.compute(d2)
        self.assertTrue(ck._recomputed,
                        "CachedKernel did not automatically recompute new data")
        ck.compute(d)
        self.assertTrue(ck._recomputed,
                        "CachedKernel did not recompute old data which had\n" + \
                        "previously been computed, but had the cache overriden")

    if _has_sg:
        # Unit tests which require shogun kernels
        # Note - there is a loss of precision from double to float32 in SG
        # Not clear if this is just for CustomKernels as there are some
        # remaining innaccuracies in others, but this might be due to other
        # sources of noise.  In all cases float32 should be identical

        @reseed_rng()
        def test_sg_conversions(self):
            nk = PrecomputedKernel(matrix=np.random.randn(50, 50))
            nk.compute()

            skip_if_no_external('shogun',
                                ver_dep='shogun:rev', min_version=4455)
            sk = nk.as_sg()
            sk.compute()
            # CustomKernels interally store as float32 ??
            self.assertTrue((nk._k.astype('float32') == \
                             sk.as_raw_np().astype('float32')).all(),
                            'Failure converting arrays between NP as SG')

        @reseed_rng()
        def test_linear_sg(self):
            d1 = np.random.randn(105, 32)
            d2 = np.random.randn(41, 32)

            nk = npK.LinearKernel()
            sk = sgK.LinearSGKernel()
            nk.compute(d1, d2)
            sk.compute(d1, d2)

            self.kernel_equiv(nk, sk)

        @reseed_rng()
        @labile(5, 1)
        def test_poly_sg(self):
            d1 = np.random.randn(105, 32)
            d2 = np.random.randn(41, 32)
            sk = sgK.PolySGKernel()
            nk = npK.PolyKernel(coef0=1)
            ordervals = [1, 2, 3, 5, 7]
            for p in ordervals:
                sk.params.degree = p
                nk.params.degree = p
                sk.compute(d1, d2)
                nk.compute(d1, d2)

                self.kernel_equiv(nk, sk)

        @reseed_rng()
        def test_rbf_sg(self):
            d1 = np.random.randn(105, 32)
            d2 = np.random.randn(41, 32)
            sk = sgK.RbfSGKernel()
            nk = npK.RbfKernel()
            sigmavals = np.logspace(-2, 5, num=10)
            for s in sigmavals:
                sk.params.sigma = s
                nk.params.sigma = s
                sk.compute(d1, d2)
                nk.compute(d1, d2)

                self.kernel_equiv(nk, sk)

        @reseed_rng()
        def test_custom_sg(self):
            skip_if_no_external('shogun')
            lk = sgK.LinearSGKernel()
            cl = sgK.CustomSGKernel(sgK.sgk.LinearKernel)
            poly = sgK.PolySGKernel()
            poly_params = [('order', 2),
                           ('inhomogenous', True)]
            if not exists('sg ge 0.6.5'):
                poly_params += [ ('use_normalization', False) ]

            custom = sgK.CustomSGKernel(sgK.sgk.PolyKernel,
                                        kernel_params=poly_params)

            d = np.random.randn(253, 52)
            lk.compute(d)
            cl.compute(d)
            poly.compute(d)
            custom.compute(d)

            self.assertTrue(np.all(lk.as_np()._k == cl.as_np()._k),
                            'CustomSGKernel does not agree with Linear')
            self.assertTrue(np.all(poly.as_np()._k == custom.as_np()._k),
                            'CustomSGKernel does not agree with Poly')

    # Older kernel stuff (ie not mvpa2.kernel) - perhaps refactor?
    def test_euclid_dist(self):
        """Euclidean distance kernel testing"""

        # select some block of data from already generated
        data = datasets['uni4large'].samples[:5, :8]

        ed = squared_euclidean_distance(data)

        # XXX not sure if that is right: 'weight' seems to be given by
        # feature (i.e. column), but distance is between samples (i.e. rows)
        # current behavior is:
        true_size = (5, 5)
        self.assertTrue(ed.shape == true_size)

        # slow version to compute distance matrix
        ed_manual = np.zeros(true_size, 'd')
        for i in range(true_size[0]):
            for j in range(true_size[1]):
                #ed_manual[i,j] = np.sqrt(((data[i,:] - data[j,:] )** 2).sum())
                ed_manual[i, j] = ((data[i, :] - data[j, :]) ** 2).sum()
        ed_manual[ed_manual < 0] = 0

        self.assertTrue(np.diag(ed_manual).sum() < 0.0000000001)
        self.assertTrue(np.diag(ed).sum() < 0.0000000001)

        # let see whether Kernel does the same
        self.assertTrue((ed - ed_manual).sum() < 0.0000001)


    def test_pnorm_w(self):
        data0 = datasets['uni4large'].samples.T
        weight = np.abs(data0[11, :60])

        self.assertRaises(ValueError, pnorm_w_python,
                              data0[:10, :2], p=1.2, heuristic='buga')
        self.assertRaises(ValueError, pnorm_w_python,
                              data0[:10, :2], weight=weight)

        self.assertRaises(ValueError, pnorm_w_python,
                              data0[:10, :2], data0[:10, :3],
                              weight=weight)
        self.assertRaises(ValueError, pnorm_w,
                              data0[:10, :2], data0[:10, :3],
                              weight=weight)

        self.assertRaises(ValueError, pnorm_w,
                              data0[:10, :2], weight=weight)

        # some sanity checks
        for did, (data1, data2, w) in enumerate(
            [ (data0[:2, :60], None, None),
              (data0[:2, :60], data0[3:4, 1:61], None),
              (data0[:2, :60], None, weight),
              (data0[:2, :60], data0[3:4, 1:61], weight),
              ]):
            # test different norms
            for p in [1, 2, 1.2]:
                kwargs = {'data1': data1,
                          'data2': data2,
                          'weight' : w,
                          'p' : p}
                d = pnorm_w(**kwargs)    # default one
                # to assess how far we are
                kwargs0 = kwargs.copy()
                kwargs0['data2'] = np.zeros(data1.shape)
                d0 = pnorm_w(**kwargs0)
                d0norm = np.linalg.norm(d - d0, 'fro')
                # test different implementations
                for iid, d2 in enumerate(
                    [pnorm_w_python(**kwargs),
                     pnorm_w_python(use_sq_euclidean=True, **kwargs),
                     pnorm_w_python(heuristic='auto', **kwargs),
                     pnorm_w_python(use_sq_euclidean=False, **kwargs)]
                    +
                    [pnorm_w_python(heuristic=h,
                                    use_sq_euclidean=False, **kwargs)
                     for h in ('auto', 'samples', 'features')]):
                    dnorm = np.linalg.norm(d2 - d, 'fro')
                    self.assertTrue(dnorm / d0norm < 1e-7,
                        msg="Failed comparison of different implementations on "
                            "data #%d, implementation #%d, p=%s. "
                            "Norm of the difference is %g"
                            % (did, iid, p, dnorm))


def suite():  # pragma: no cover
    return unittest.makeSuite(KernelTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_knn
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA kNN classifier"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.datasets import pure_multivariate_signal

from mvpa2.clfs.knn import kNN
from mvpa2.clfs.distance import one_minus_correlation

class KNNTests(unittest.TestCase):

    def test_multivariate(self):

        mv_perf = []
        uv_perf = []

        clf = kNN(k=10)
        for i in xrange(20):
            train = pure_multivariate_signal( 20, 3 )
            test = pure_multivariate_signal( 20, 3 )
            clf.train(train)
            p_mv = clf.predict( test.samples )
            mv_perf.append( np.mean(p_mv==test.targets) )

            clf.train(train[:, 0])
            p_uv = clf.predict(test[:, 0].samples)
            uv_perf.append( np.mean(p_uv==test.targets) )

        mean_mv_perf = np.mean(mv_perf)
        mean_uv_perf = np.mean(uv_perf)

        self.assertTrue( mean_mv_perf > 0.9 )
        self.assertTrue( mean_uv_perf < mean_mv_perf )


    def test_knn_state(self):
        train = pure_multivariate_signal( 40, 3 )
        test = pure_multivariate_signal( 20, 3 )

        clf = kNN(k=10)
        clf.train(train)

        clf.ca.enable(['estimates', 'predictions', 'distances'])

        p = clf.predict(test.samples)

        self.assertTrue(p == clf.ca.predictions)
        self.assertTrue(len(clf.ca.estimates) == 80)
        self.assertTrue(set(clf.ca.estimates[0].keys()) == set(test.targets))
        self.assertTrue(clf.ca.distances.shape == (80,160))

        self.assertTrue(not clf.ca.distances.fa is train.sa)
        # Those are deep-copied now by default so they should not be the same
        self.assertTrue(not (clf.ca.distances.fa['chunks'] is train.sa['chunks']))
        self.assertTrue(not (clf.ca.distances.fa.chunks is train.sa.chunks))

def suite():  # pragma: no cover
    return unittest.makeSuite(KNNTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_lars
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA least angle regression (LARS) classifier"""

from mvpa2.testing import *
skip_if_no_external('lars')

from mvpa2.testing.datasets import datasets

from mvpa2 import cfg
from mvpa2.clfs.lars import LARS
from scipy.stats import pearsonr
from mvpa2.misc.data_generators import normal_feature_dataset

class LARSTests(unittest.TestCase):

    def test_lars(self):
        # not the perfect dataset with which to test, but
        # it will do for now.
        #data = datasets['dumb2']
        # for some reason the R code fails with the dumb data
        data = datasets['chirp_linear']


        clf = LARS()

        clf.train(data)

        # prediction has to be almost perfect
        # test with a correlation
        pre = clf.predict(data.samples)
        cor = pearsonr(pre, data.targets)
        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(cor[0] > .8)

    def test_lars_state(self):
        #data = datasets['dumb2']
        # for some reason the R code fails with the dumb data
        data = datasets['chirp_linear']


        clf = LARS()

        clf.train(data)

        clf.ca.enable('predictions')

        p = clf.predict(data.samples)

        self.assertTrue((p == clf.ca.predictions).all())


    def test_lars_sensitivities(self):
        data = datasets['chirp_linear']

        # use LARS on binary problem
        clf = LARS()
        clf.train(data)

        # now ask for the sensitivities WITHOUT having to pass the dataset
        # again
        sens = clf.get_sensitivity_analyzer(force_train=False)(None)

        self.assertTrue(sens.shape == (1, data.nfeatures))


def suite():  # pragma: no cover
    return unittest.makeSuite(LARSTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_mapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for basic mappers'''

import numpy as np
# for repr
from numpy import array

from mvpa2.testing.tools import ok_, assert_raises, assert_false, assert_equal, \
        assert_true, assert_array_equal, nodebug

from mvpa2.testing.datasets import datasets
from mvpa2.mappers.flatten import FlattenMapper
from mvpa2.mappers.base import ChainMapper
from mvpa2.featsel.base import StaticFeatureSelection
from mvpa2.mappers.slicing import SampleSliceMapper, StripBoundariesSamples
from mvpa2.support.copy import copy
from mvpa2.datasets.base import Dataset
from mvpa2.base.collections import ArrayCollectable
from mvpa2.datasets.base import dataset_wizard
from mvpa2.mappers.flatten import ProductFlattenMapper

import itertools
import operator

from mvpa2.base import externals

# arbitrary ndarray subclass for testing
class myarray(np.ndarray):
    pass

def test_flatten():
    samples_shape = (2, 2, 4)
    data_shape = (4,) + samples_shape
    data = np.arange(np.prod(data_shape)).reshape(data_shape).view(myarray)
    pristinedata = data.copy()
    target = [[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
              [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],
              [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],
              [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
    target = np.array(target).view(myarray)
    index_target = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 0, 3],
                            [0, 1, 0], [0, 1, 1], [0, 1, 2], [0, 1, 3],
                            [1, 0, 0], [1, 0, 1], [1, 0, 2], [1, 0, 3],
                            [1, 1, 0], [1, 1, 1], [1, 1, 2], [1, 1, 3]])

    # test only flattening the first two dimensions
    fm_max = FlattenMapper(maxdims=2)
    fm_max.train(data)
    assert_equal(fm_max(data).shape, (4, 4, 4))

    # array subclass survives
    ok_(isinstance(data, myarray))

    # actually, there should be no difference between a plain FlattenMapper and
    # a chain that only has a FlattenMapper as the one element
    for fm in [FlattenMapper(space='voxel'),
               ChainMapper([FlattenMapper(space='voxel'),
                            StaticFeatureSelection(slice(None))])]:
        # not working if untrained
        assert_raises(RuntimeError,
                      fm.forward1,
                      np.arange(np.sum(samples_shape) + 1))

        fm.train(data)

        ok_(isinstance(fm.forward(data), myarray))
        ok_(isinstance(fm.forward1(data[2]), myarray))
        assert_array_equal(fm.forward(data), target)
        assert_array_equal(fm.forward1(data[2]), target[2])
        assert_raises(ValueError, fm.forward, np.arange(4))

        # all of that leaves that data unmodified
        assert_array_equal(data, pristinedata)

        # reverse mapping
        ok_(isinstance(fm.reverse(target), myarray))
        ok_(isinstance(fm.reverse1(target[0]), myarray))
        ok_(isinstance(fm.reverse(target[1:2]), myarray))
        assert_array_equal(fm.reverse(target), data)
        assert_array_equal(fm.reverse1(target[0]), data[0])
        assert_array_equal(fm.reverse(target[1:2]), data[1:2])
        assert_raises(ValueError, fm.reverse, np.arange(14))

        # check one dimensional data, treated as scalar samples
        oned = np.arange(5)
        fm.train(Dataset(oned))
        # needs 2D
        assert_raises(ValueError, fm.forward, oned)
        # doesn't match mapper, since Dataset turns `oned` into (5,1)
        assert_raises(ValueError, fm.forward, oned)
        assert_equal(Dataset(oned).nfeatures, 1)

        # try dataset mode, with some feature attribute
        fattr = np.arange(np.prod(samples_shape)).reshape(samples_shape)
        ds = Dataset(data, fa={'awesome': fattr.copy()})
        assert_equal(ds.samples.shape, data_shape)
        fm.train(ds)
        dsflat = fm.forward(ds)
        ok_(isinstance(dsflat, Dataset))
        ok_(isinstance(dsflat.samples, myarray))
        assert_array_equal(dsflat.samples, target)
        assert_array_equal(dsflat.fa.awesome, np.arange(np.prod(samples_shape)))
        assert_true(isinstance(dsflat.fa['awesome'], ArrayCollectable))
        # test index creation
        assert_array_equal(index_target, dsflat.fa.voxel)

        # and back
        revds = fm.reverse(dsflat)
        ok_(isinstance(revds, Dataset))
        ok_(isinstance(revds.samples, myarray))
        assert_array_equal(revds.samples, data)
        assert_array_equal(revds.fa.awesome, fattr)
        assert_true(isinstance(revds.fa['awesome'], ArrayCollectable))
        assert_false('voxel' in revds.fa)


def test_product_flatten():
    nsamples = 17
    product_name_values = [('chan', ['C1', 'C2']),
                         ('freq', np.arange(4, 20, 6)),
                         ('time', np.arange(-200, 800, 200))]

    shape = (nsamples,) + tuple(len(v) for _, v in product_name_values)

    sample_names = ['samp%d' % i for i in xrange(nsamples)]

    # generate random data in four dimensions
    data = np.random.normal(size=shape)
    ds = Dataset(data, sa=dict(sample_names=sample_names))
    for n, v in product_name_values:
        ds.a[n] = v

    # apply flattening to ds
    names, values = zip(*(product_name_values))

    flattened_ds = None

    # test both with explicit values for factor_values and without
    for with_values in (False, True):
        # the order of False and True is critical.
        # In the first iteration flattened_ds is set and used in the second
        # iteration
        args = {}
        if with_values:
            factor_values = [v for n, v in product_name_values]
            args['factor_values'] = factor_values

        flattener = ProductFlattenMapper(names, **args)

    # test I/O (only if h5py is available)
    if externals.exists('h5py'):
        from mvpa2.base.hdf5 import h5save, h5load
        import tempfile
        import os

        fd, testfn = tempfile.mkstemp('mapper.h5py', 'test_product'); os.close(fd)
        h5save(testfn, flattener)
        flattener = h5load(testfn)
        os.unlink(testfn)

    if flattened_ds is None:
        assert_raises(ValueError, flattener.reverse, ds)
    else:
        ds_ = flattener.reverse(flattened_ds)
        assert_equal(ds.samples, ds_.samples)

    mds = flattener(ds)

    prod = lambda x:reduce(operator.mul, x)

    # ensure the size is ok
    assert_equal(mds.shape, (nsamples,) + (prod(shape[1:]),))

    ndim = len(product_name_values)

    idxs = [range(len(v)) for v in values]
    for si in xrange(nsamples):
        for fi, p in enumerate(itertools.product(*idxs)):
            data_tup = (si,) + p

            x = mds[si, fi]

            # value should match
            assert_equal(data[data_tup], x.samples[0, 0])

            # indices should match as well
            all_idxs = tuple(x.fa['chan_freq_time_indices'].value.ravel())
            assert_equal(p, all_idxs)

            # values and indices in each dimension should match
            for i, (name, value) in enumerate(product_name_values):
                assert_equal(x.fa[name].value, value[p[i]])
                assert_equal(x.fa[name + '_indices'].value, p[i])

    dsr = flattener.reverse(mds)
    assert_equal(dsr.shape, ds.shape)


    names += ('foo',)

    flattener = ProductFlattenMapper(names)
    assert_raises(KeyError, flattener, ds)

    # for next iterations
    flattened_ds = mds


def test_subset():
    data = np.array(
            [[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
            [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],
            [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],
            [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]])
    # float array doesn't work
    sm = StaticFeatureSelection(np.ones(16))
    assert_raises(IndexError, sm.forward, data)

    # full mask
    sm = StaticFeatureSelection(slice(None))
    # should not change single samples
    assert_array_equal(sm.forward(data[0:1].copy()), data[0:1])
    # or multi-samples
    assert_array_equal(sm.forward(data.copy()), data)
    sm.train(data)
    # same on reverse
    assert_array_equal(sm.reverse(data[0:1].copy()), data[0:1])
    # or multi-samples
    assert_array_equal(sm.reverse(data.copy()), data)

    # identical mappers
    sm_none = StaticFeatureSelection(slice(None))
    sm_int = StaticFeatureSelection(np.arange(16))
    sm_bool = StaticFeatureSelection(np.ones(16, dtype='bool'))
    sms = [sm_none, sm_int, sm_bool]

    # test subsets
    sids = [3, 4, 5, 6]
    bsubset = np.zeros(16, dtype='bool')
    bsubset[sids] = True
    subsets = [sids, slice(3, 7), bsubset, [3, 3, 4, 4, 6, 6, 6, 5]]
    # all test subset result in equivalent masks, hence should do the same to
    # the mapper and result in identical behavior
    for st in sms:
        for i, sub in enumerate(subsets):
            # shallow copy
            orig = copy(st)
            subsm = StaticFeatureSelection(sub)
            # should do copy-on-write for all important stuff!!
            orig += subsm
            # test if selection did its job
            if i == 3:
                # special case of multiplying features
                assert_array_equal(orig.forward1(data[0].copy()), subsets[i])
            else:
                assert_array_equal(orig.forward1(data[0].copy()), sids)

    ## all of the above shouldn't change the original mapper
    #assert_array_equal(sm.get_mask(), np.arange(16))

    # check for some bug catcher
    # no 3D input
    #assert_raises(IndexError, sm.forward, np.ones((3,2,1)))
    # no input of wrong length
    if __debug__:
        # checked only in __debug__
        assert_raises(ValueError, sm.forward, np.ones(4))
    # same on reverse
    #assert_raises(ValueError, sm.reverse, np.ones(16))
    # invalid ids
    #assert_false(subsm.is_valid_inid(-1))
    #assert_false(subsm.is_valid_inid(16))

    # intended merge failures
    fsm = StaticFeatureSelection(np.arange(16))
    assert_equal(fsm.__iadd__(None), NotImplemented)
    assert_equal(fsm.__iadd__(Dataset([2, 3, 4])), NotImplemented)


def test_subset_filler():
    sm = StaticFeatureSelection(np.arange(3))
    sm_f0 = StaticFeatureSelection(np.arange(3), filler=0)
    sm_fm1 = StaticFeatureSelection(np.arange(3), filler=-1)
    sm_fnan = StaticFeatureSelection(np.arange(3), filler=np.nan)
    data = np.arange(12).astype(float).reshape((2, -1))

    sm.train(data)
    data_forwarded = sm.forward(data)

    for m in (sm, sm_f0, sm_fm1, sm_fnan):
        m.train(data)
        assert_array_equal(data_forwarded, m.forward(data))

    data_back_fm1 = sm_fm1.reverse(data_forwarded)
    ok_(np.all(data_back_fm1[:, 3:] == -1))
    data_back_fnan = sm_fnan.reverse(data_forwarded)
    ok_(np.all(np.isnan(data_back_fnan[:, 3:])))

@nodebug(['ID_IN_REPR', 'MODULE_IN_REPR'])
def test_repr():
    # this time give mask only by its target length
    sm = StaticFeatureSelection(slice(None), space='myspace')

    # check reproduction
    sm_clone = eval(repr(sm))
    assert_equal(repr(sm_clone), repr(sm))

@nodebug(['ID_IN_REPR', 'MODULE_IN_REPR'])
def test_chainmapper():
    # the chain needs at lest one mapper
    assert_raises(ValueError, ChainMapper, [])
    # a typical first mapper is to flatten
    cm = ChainMapper([FlattenMapper()])

    # few container checks
    assert_equal(len(cm), 1)
    assert_true(isinstance(cm[0], FlattenMapper))

    # now training
    # come up with data
    samples_shape = (2, 2, 4)
    data_shape = (4,) + samples_shape
    data = np.arange(np.prod(data_shape)).reshape(data_shape)
    pristinedata = data.copy()
    target = [[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
              [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],
              [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],
              [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
    target = np.array(target)

    # if it is not trained it knows nothing
    cm.train(data)

    # a new mapper should appear when doing feature selection
    cm.append(StaticFeatureSelection(range(1, 16)))
    assert_equal(cm.forward1(data[0]).shape, (15,))
    assert_equal(len(cm), 2)
    # multiple slicing
    cm.append(StaticFeatureSelection([9, 14]))
    assert_equal(cm.forward1(data[0]).shape, (2,))
    assert_equal(len(cm), 3)

    # check reproduction
    if __debug__:
        # debug mode needs special test as it enhances the repr output
        # with module info and id() appendix for objects
        import mvpa2
        cm_clone = eval(repr(cm))
        assert_equal('#'.join(repr(cm_clone).split('#')[:-1]),
                     '#'.join(repr(cm).split('#')[:-1]))
    else:
        cm_clone = eval(repr(cm))
        assert_equal(repr(cm_clone), repr(cm))

    # what happens if we retrain the whole beast an same data as before
    cm.train(data)
    assert_equal(cm.forward1(data[0]).shape, (2,))
    assert_equal(len(cm), 3)

    # let's map something
    mdata = cm.forward(data)
    assert_array_equal(mdata, target[:, [10, 15]])
    # and back
    rdata = cm.reverse(mdata)
    # original shape
    assert_equal(rdata.shape, data.shape)
    # content as far it could be restored
    assert_array_equal(rdata[rdata > 0], data[rdata > 0])
    assert_equal(np.sum(rdata > 0), 8)

    # Lets construct a dataset with mapper assigned and see
    # if sub-selecting a feature adjusts trailing StaticFeatureSelection
    # appropriately
    ds_subsel = Dataset.from_wizard(data, mapper=cm)[:, 1]
    tail_sfs = ds_subsel.a.mapper[-1]
    assert_equal(repr(tail_sfs), 'StaticFeatureSelection(slicearg=array([14]))')

def test_sampleslicemapper():
    # this does nothing but Dataset.__getitem__ which is tested elsewhere -- but
    # at least we run it
    ds = datasets['uni2small']
    ssm = SampleSliceMapper(slice(3, 8, 2))
    sds = ssm(ds)
    assert_equal(len(sds), 3)


def test_strip_boundary():
    ds = datasets['hollow']
    ds.sa['btest'] = np.repeat([0, 1], 20)
    sn = StripBoundariesSamples('btest', 1, 2)
    sds = sn(ds)
    assert_equal(len(sds), len(ds) - 3)
    for i in [19, 20, 21]:
        assert_false(i in sds.samples.sid)

def test_transpose():
    from mvpa2.mappers.shape import TransposeMapper
    ds = Dataset(np.arange(24).reshape(2, 3, 4),
                 sa={'testsa': np.arange(2)},
                 fa={'testfa': np.arange(3)})
    tp = TransposeMapper()
    tds = tp(ds)
    assert_equal(tds.shape, (3, 2, 4))
    assert_true('testfa' in tds.sa)
    assert_true('testsa' in tds.fa)
    assert_false(tds.fa is tds.sa)
    # and back
    ttds = tp(tds)
    assert_array_equal(ttds.samples, ds.samples)
    assert_equal(ttds.sa, ds.sa)
    assert_equal(ttds.fa, ds.fa)
    # or this way
    rds = tp.reverse(tds)
    assert_array_equal(rds.samples, ds.samples)
    assert_equal(rds.sa, ds.sa)
    assert_equal(rds.fa, ds.fa)
    assert_array_equal(rds.samples, ttds.samples)
    assert_equal(rds.sa, ttds.sa)
    assert_equal(rds.fa, ttds.fa)

def test_addaxis():
    from mvpa2.mappers.shape import AddAxisMapper
    ds = Dataset(np.arange(24).reshape(2, 3, 4),
                 sa={'testsa': np.arange(2)},
                 fa={'testfa': np.arange(3)})
    ds0 = AddAxisMapper(pos=0)(ds)
    assert_array_equal(ds0.shape, (1,) + ds.shape)
    # sas have extra dimension
    assert_array_equal(ds0.sa.testsa[0], ds.sa.testsa)
    # fas are duplicated
    assert_array_equal(ds0.fa.testfa[0], ds0.fa.testfa[1])
    ds1 = AddAxisMapper(pos=1)(ds)
    assert_array_equal(ds1.shape, (2, 1, 3, 4))
    # same sample attribute
    assert_equal(ds1.sa, ds.sa)
    # fas have extra dimension
    assert_array_equal(ds1.fa.testfa[0], ds.fa.testfa)
    ds2 = AddAxisMapper(pos=2)(ds)
    assert_array_equal(ds2.shape, (2, 3, 1, 4))
    # no change to attribute collections
    assert_equal(ds2.sa, ds.sa)
    assert_equal(ds2.fa, ds.fa)
    # append an axis
    ds3 = AddAxisMapper(pos=3)(ds)
    assert_array_equal(ds3.shape, ds.shape + (1,))
    # reverse indexing
    ds_1 = AddAxisMapper(pos=-1)(ds)
    assert_array_equal(ds3.samples, ds_1.samples)
    assert_equal(ds3.sa, ds_1.sa)
    assert_equal(ds3.fa, ds_1.fa)
    # add multiple axes
    ds4 = AddAxisMapper(pos=4)(ds)
    assert_array_equal(ds4.shape, ds.shape + (1, 1))

########NEW FILE########
__FILENAME__ = test_mapper_sp
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for detrending mapper (requiring SciPy)."""

import numpy as np

from mvpa2.testing.tools import *

skip_if_no_external('scipy')

from mvpa2.datasets import Dataset, dataset_wizard
from mvpa2.mappers.detrend import PolyDetrendMapper, poly_detrend

def test_polydetrend():
    samples_forwhole = np.array( [[1.0, 2, 3, 4, 5, 6],
                                 [-2.0, -4, -6, -8, -10, -12]], ndmin=2 ).T
    samples_forchunks = np.array( [[1.0, 2, 3, 3, 2, 1],
                                  [-2.0, -4, -6, -6, -4, -2]], ndmin=2 ).T
    chunks = [0, 0, 0, 1, 1, 1]
    chunks_bad = [ 0, 0, 1, 1, 1, 0]
    target_whole = np.array( [[-3.0, -2, -1, 1, 2, 3],
                             [-6, -4, -2,  2, 4, 6]], ndmin=2 ).T
    target_chunked = np.array( [[-1.0, 0, 1, 1, 0, -1],
                               [2, 0, -2, -2, 0, 2]], ndmin=2 ).T


    ds = Dataset(samples_forwhole)

    # this one will auto-train the mapper on first use
    dm = PolyDetrendMapper(polyord=1, space='police')
    mds = dm.forward(ds)
    # features are linear trends, so detrending should remove all
    assert_array_almost_equal(mds.samples, np.zeros(mds.shape))
    # we get the information where each sample is assumed to be in the
    # space spanned by the polynomials
    assert_array_equal(mds.sa.police, np.arange(len(ds)))

    # hackish way to get the previous regressors into a dataset
    ds.sa['opt_reg_const'] = dm._regs[:,0]
    ds.sa['opt_reg_lin'] = dm._regs[:,1]
    # using these precomputed regressors, we should get the same result as
    # before even if we do not generate a regressor for linear
    dm_optreg = PolyDetrendMapper(polyord=0,
                                  opt_regs=['opt_reg_const', 'opt_reg_lin'])
    mds_optreg = dm_optreg.forward(ds)
    assert_array_almost_equal(mds_optreg, np.zeros(mds.shape))


    ds = Dataset(samples_forchunks)
    # 'constant' detrending removes the mean
    mds = PolyDetrendMapper(polyord=0).forward(ds)
    assert_array_almost_equal(
            mds.samples,
            samples_forchunks - np.mean(samples_forchunks, axis=0))
    # if there is no GLOBAL linear trend it should be identical to mean removal
    # even if trying to remove linear
    mds2 = PolyDetrendMapper(polyord=1).forward(ds)
    assert_array_almost_equal(mds, mds2)

    # chunk-wise detrending
    ds = dataset_wizard(samples_forchunks, chunks=chunks)
    dm = PolyDetrendMapper(chunks_attr='chunks', polyord=1, space='police')
    mds = dm.forward(ds)
    # features are chunkswise linear trends, so detrending should remove all
    assert_array_almost_equal(mds.samples, np.zeros(mds.shape))
    # we get the information where each sample is assumed to be in the
    # space spanned by the polynomials, which is the identical linspace in both
    # chunks
    assert_array_equal(mds.sa.police, range(3) * 2)
    # non-matching number of samples cannot be mapped
    assert_raises(ValueError, dm.forward, ds[:-1])
    # however, if the dataset knows about the space it is possible
    ds.sa['police'] = mds.sa.police
    # XXX this should be
    #mds2 = dm(ds[1:-1])
    #assert_array_equal(mds[1:-1], mds2)
    # XXX but right now is
    assert_raises(NotImplementedError, dm.forward, ds[1:-1])

    # Detrend must preserve the size of dataset
    assert_equal(mds.shape, ds.shape)

    # small additional test for break points
    # although they are no longer there
    ds = dataset_wizard(np.array([[1.0, 2, 3, 1, 2, 3]], ndmin=2).T,
                 targets=chunks, chunks=chunks)
    mds = PolyDetrendMapper(chunks_attr='chunks', polyord=1).forward(ds)
    assert_array_almost_equal(mds.samples, np.zeros(mds.shape))

    # test of different polyord on each chunk
    target_mixed = np.array( [[-1.0, 0, 1, 0, 0, 0],
                             [2.0, 0, -2, 0, 0, 0]], ndmin=2 ).T
    ds = dataset_wizard(samples_forchunks.copy(), targets=chunks, chunks=chunks)
    mds = PolyDetrendMapper(chunks_attr='chunks', polyord=[0,1]).forward(ds)
    assert_array_almost_equal(mds, target_mixed)

    # test irregluar spacing of samples, but with corrective time info
    samples_forwhole = np.array( [[1.0, 4, 6, 8, 2, 9],
                                 [-2.0, -8, -12, -16, -4, -18]], ndmin=2 ).T
    ds = Dataset(samples_forwhole, sa={'time': samples_forwhole[:,0]})
    # linear detrending that makes use of temporal info from dataset
    dm = PolyDetrendMapper(polyord=1, space='time')
    mds = dm.forward(ds)
    assert_array_almost_equal(mds.samples, np.zeros(mds.shape))

    # and now the same stuff, but with chunking and ordered by time
    samples_forchunks = np.array( [[1.0, 3, 3, 2, 2, 1],
                                  [-2.0, -6, -6, -4, -4, -2]], ndmin=2 ).T
    chunks = [0, 1, 0, 1, 0, 1]
    time = [4, 4, 12, 8, 8, 12]
    ds = Dataset(samples_forchunks.copy(), sa={'chunks': chunks, 'time': time})
    mds = PolyDetrendMapper(chunks_attr='chunks', polyord=1, space='time').forward(ds)

    # the whole thing must not affect the source data
    assert_array_equal(ds, samples_forchunks)
    # but if done inplace that is no longer true
    poly_detrend(ds, chunks_attr='chunks', polyord=1, space='time')
    assert_array_equal(ds, mds)

########NEW FILE########
__FILENAME__ = test_mdp
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for basic mappers'''

import numpy as np

from mvpa2.testing import *
skip_if_no_external('mdp')

from mvpa2.base import externals
import mdp

from mvpa2.mappers.mdp_adaptor import MDPNodeMapper, MDPFlowMapper, PCAMapper, \
        ICAMapper
from mvpa2.mappers.lle import LLEMapper
from mvpa2.datasets.base import Dataset
from mvpa2.base.dataset import DAE
from mvpa2.misc.data_generators import normal_feature_dataset

@reseed_rng()
def test_mdpnodemapper():
    ds = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=4)

    node = mdp.nodes.PCANode()
    mm = MDPNodeMapper(node, nodeargs={'stoptrain': ((), {'debug': True})})

    mm.train(ds)

    fds = mm.forward(ds)
    if externals.versions['mdp'] >= '2.5':
        assert_true(hasattr(mm.node, 'cov_mtx'))

    assert_true(isinstance(fds, Dataset))
    assert_equal(fds.samples.shape, ds.samples.shape)

    # set projection onto first 2 components
    mm.nodeargs['exec'] = ((), {'n': 2})
    #should be different from above
    lfds = mm.forward(ds.samples)
    # output shape changes although the node still claim otherwise
    assert_equal(mm.node.output_dim, 4)
    assert_equal(lfds.shape[0], fds.samples.shape[0])
    assert_equal(lfds.shape[1], 2)
    assert_array_equal(lfds, fds.samples[:, :2])

    # reverse
    rfds = mm.reverse(fds)

    # even smaller size works
    rlfds = mm.reverse(lfds)
    assert_equal(rfds.samples.shape, ds.samples.shape)

    # retraining has to work on a new dataset too, since we copy the node
    # internally
    dsbig = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=10)
    mm.train(dsbig)


def test_mdpflowmapper():
    flow = mdp.nodes.PCANode() + mdp.nodes.SFANode()
    fm = MDPFlowMapper(flow)
    ds = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=4)

    fm.train(ds)
    assert_false(fm.flow[0].is_training())
    assert_false(fm.flow[1].is_training())

    fds = fm.forward(ds)
    assert_true(isinstance(fds, Dataset))
    assert_equal(fds.samples.shape, ds.samples.shape)


def test_mdpflow_additional_arguments():
    skip_if_no_external('mdp', min_version='2.5')
    # we have no IdentityNode yet... is there analog?

    ds = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=4)
    flow = mdp.nodes.PCANode() + mdp.nodes.IdentityNode() + mdp.nodes.FDANode()
    # this is what it would look like in MDP itself
    #flow.train([[ds.samples],
    #            [[ds.samples, ds.sa.targets]]])
    assert_raises(ValueError, MDPFlowMapper, flow, node_arguments=[[],[]])
    fm = MDPFlowMapper(flow, node_arguments = ([], [], [DAE('sa', 'targets')]))
    fm.train(ds)
    fds = fm.forward(ds)
    assert_equal(ds.samples.shape, fds.samples.shape)
    rds = fm.reverse(fds)
    assert_array_almost_equal(ds.samples, rds.samples)

def test_mdpflow_additional_arguments_nones():
    skip_if_no_external('mdp', min_version='2.5')
    # we have no IdentityNode yet... is there analog?

    ds = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=4)
    flow = mdp.nodes.PCANode() + mdp.nodes.IdentityNode() + mdp.nodes.FDANode()
    # this is what it would look like in MDP itself
    #flow.train([[ds.samples],
    #            [[ds.samples, ds.sa.targets]]])
    assert_raises(ValueError, MDPFlowMapper, flow, node_arguments=[[],[]])
    fm = MDPFlowMapper(flow, node_arguments = (None, None, [ds.sa.targets]))
    fm.train(ds)
    fds = fm.forward(ds)
    assert_equal(ds.samples.shape, fds.samples.shape)
    rds = fm.reverse(fds)
    assert_array_almost_equal(ds.samples, rds.samples)

@reseed_rng()
def test_pcamapper():
    # data: 40 sample feature line in 20d space (40x20; samples x features)
    ndlin = Dataset(np.concatenate([np.arange(40)
                               for i in range(20)]).reshape(20,-1).T)

    pm = PCAMapper()
    # train PCA
    assert_raises(mdp.NodeException, pm.train, ndlin)
    ndlin.samples = ndlin.samples.astype('float')
    ndlin_noise = ndlin.copy()
    ndlin_noise.samples += np.random.random(size=ndlin.samples.shape)
    # we have no variance for more than one PCA component, hence just one
    # actual non-zero eigenvalue
    assert_raises(mdp.NodeException, pm.train, ndlin)
    pm.train(ndlin_noise)
    assert_equal(pm.proj.shape, (20, 20))
    # now project data into PCA space
    p = pm.forward(ndlin.samples)
    assert_equal(p.shape, (40, 20))
    # check that the mapped data can be fully recovered by 'reverse()'
    assert_array_almost_equal(pm.reverse(p), ndlin)


@reseed_rng()
def test_icamapper():
    # data: 40 sample feature line in 2d space (40x2; samples x features)
    samples = np.vstack([np.arange(40.) for i in range(2)]).T
    samples -= samples.mean()
    samples +=  np.random.normal(size=samples.shape, scale=0.1)
    ndlin = Dataset(samples)

    pm = ICAMapper()
    try:
        pm.train(ndlin.copy())
        assert_equal(pm.proj.shape, (2, 2))
        p = pm.forward(ndlin.copy())
        assert_equal(p.shape, (40, 2))
        # check that the mapped data can be fully recovered by 'reverse()'
        assert_array_almost_equal(pm.reverse(p), ndlin)
    except mdp.NodeException:
        # do not puke if the ICA did not converge at all -- that is not our
        # fault but MDP's
        pass


def test_llemapper():
    skip_if_no_external('mdp', min_version='2.4')

    ds = Dataset(np.array([[0., 0., 0.], [0., 0., 1.], [0., 1., 0.],
                          [1., 0., 0.], [0., 1., 1.], [1., 0., 1.],
                          [1., 1., 0.], [1., 1., 1.]]))
    pm = LLEMapper(3, output_dim=2)
    pm.train(ds)
    fmapped = pm.forward(ds)
    assert_equal(fmapped.shape, (8, 2))

@reseed_rng()
def test_nodeargs():
    skip_if_no_external('mdp', min_version='2.4')
    ds = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=4)

    for svd_val in [True, False]:
        pcm = PCAMapper(alg='PCA', svd=svd_val)
        assert_equal(pcm.node.svd, svd_val)
        pcm.train(ds)
        assert_equal(pcm.node.svd, svd_val)
    for output_dim in [0.5, 0.95, 0.99, 10, 50, 100]:
        pcm = PCAMapper(alg='PCA', output_dim=output_dim)
        for i in range(2):              # so we also test on trained one
            if isinstance(output_dim, float):
                assert_equal(pcm.node.desired_variance, output_dim)
            else:
                assert_equal(pcm.node.output_dim, output_dim)
            pcm.train(ds)
            if isinstance(output_dim, float):
                assert_not_equal(pcm.node.output_dim, output_dim)
                # some dimensions are chosen
                assert_true(pcm.node.output_dim > 0)


########NEW FILE########
__FILENAME__ = test_meg
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA MEG stuff"""

import os.path

from mvpa2.testing import *
from mvpa2 import pymvpa_dataroot
from mvpa2.misc.io.meg import TuebingenMEG

class MEGTests(unittest.TestCase):

    def test_tuebingen_meg(self):
        # Use this whenever we fully switch to nose to run tests
        #skip_if_no_external('gzip')
        if not externals.exists('gzip'):
            return

        meg = TuebingenMEG(os.path.join(pymvpa_dataroot, 'tueb_meg.dat.gz'))

        # check basics
        self.assertTrue(meg.channelids == ['BG1', 'MLC11', 'EEG02'])
        self.assertTrue(meg.ntimepoints == 814)
        self.assertTrue(meg.nsamples == 4)
        # check correct axis order (samples x channels x timepoints)
        self.assertTrue(meg.data.shape == (4, 3, 814))

        # check few values
        self.assertTrue(meg.data[0, 1, 4] == -2.318207982e-14)
        self.assertTrue(meg.data[3, 0, 808] == -4.30692876e-12)


def suite():  # pragma: no cover
    return unittest.makeSuite(MEGTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_misc
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA misc stuff"""

from mvpa2.testing import *

from mvpa2.datasets import Dataset
from mvpa2.misc.fx import dual_gaussian, dual_positive_gaussian, fit2histogram
from mvpa2.misc.data_generators import random_affine_transformation

@reseed_rng()
@sweepargs(f=(dual_gaussian, dual_positive_gaussian))
def test_dual_gaussian(f):
    skip_if_no_external('scipy')
    data = np.random.normal(size=(100, 1))

    histfit = fit2histogram(np.repeat(data[None, :], 2, axis=0),
                            f,
                            (1000, 0.5, 0.1, 1000, 0.8, 0.05),
                            nbins=20)
    H, bin_left, bin_width, fit = histfit
    params = fit[0]
    # both variances must be positive
    ok_(params[2] > 0)
    ok_(params[5] > 0)

    if f is dual_positive_gaussian:
        # both amplitudes must be positive
        ok_(params[0] > 0)
        ok_(params[3] > 0)


def test_random_affine_transformation():
    ds = Dataset.from_wizard(np.random.randn(8,3,2))
    ds_d = random_affine_transformation(ds)
    # compare original to the inverse of the distortion using reported
    # parameters
    assert_array_almost_equal(
        np.dot((ds_d.samples - ds_d.a.random_shift) / ds_d.a.random_scale,
               ds_d.a.random_rotation.T),
        ds.samples)


@reseed_rng()
def test_ttest_1samp_masked():
    skip_if_no_external('scipy')
    import numpy as np
    from mvpa2.misc.stats import ttest_1samp as ttest_1samp_masked

    # old scipy's ttest_1samp need to be conditioned since they
    # return 1's and 0's for when should be NaNs
    if externals.versions['scipy'] < '0.10.1':
        from scipy.stats import ttest_1samp as scipy_ttest_1samp

        def ttest_1samp(*args, **kwargs):
            t, p = scipy_ttest_1samp(*args, **kwargs)
            p_isnan = np.isnan(p)
            if np.any(p_isnan):
                if t.ndim == 0:
                    t = np.nan
                else:
                    t[p_isnan] = np.nan
            return t, p
    else:
        from scipy.stats import ttest_1samp

    if externals.versions['numpy'] < '1.6.2':
        # yoh: there is a bug in old (e.g. 1.4.1) numpy's while operating on
        #      masked arrays -- for some reason refuses to compute var
        #      correctly whenever only 2 elements are available and it is
        #      multi-dimensional:
        # (Pydb) print np.var(a[:, 9:11], axis, ddof=1)
        # [540.0 --]
        # (Pydb) print np.var(a[:, 10:11], axis, ddof=1)
        # [--]
        # (Pydb) print np.var(a[:, 10], axis, ddof=1)
        # 648.0
        # To overcome -- assure masks with without 2 elements in any
        # dimension and allow for NaN t-test results in such anyway
        # degenerate cases
        def random_mask(shape):
            # screw it -- let's generate quite primitive mask with
            return (np.arange(np.prod(shape))%2).astype(bool).reshape(shape)
        ndshape = (5, 6, 1, 7)          # we need larger structure with this XOR mask
    else:
        def random_mask(shape):
            # otherwise all simple:
            return np.random.normal(size=shape) > -0.5
        ndshape = (4, 3, 2, 1)

    _assert_array_equal = assert_array_almost_equal

    # test on some random data to match results of ttest_1samp
    d = np.random.normal(size=(5, 3))
    for null in 0, 0.5:
        # 1D case
        _assert_array_equal(ttest_1samp       (d[0], null),
                            ttest_1samp_masked(d[0], null))

        for axis in 0, 1, None:
            _assert_array_equal(ttest_1samp       (d, null, axis=axis),
                                ttest_1samp_masked(d, null, axis=axis))
    # we do not yet support >2D
    ##assert_raises(AssertionError, ttest_1samp_masked, d[None,...], 0)

    # basic test different alternatives
    d = range(10)
    tl, pl = ttest_1samp_masked(d, 0, alternative='greater')

    tr, pr = ttest_1samp_masked(d, 0, alternative='less')
    tb, pb = ttest_1samp_masked(d, 0, alternative='two-sided')

    assert_equal(tl, tr)
    assert_equal(tl, tb)
    assert_equal(pl + pr, 1.0)
    assert_equal(pb, pl*2)
    assert(pl < 0.05)               # clearly we should be able to reject

    # finally let's get to masking
    # 1D
    d = np.arange(10)
    _assert_array_equal(ttest_1samp       (d[3:], 0),
                        ttest_1samp_masked(d,     0,
                                           mask=[False]*3 + [True]*7))

    # random mask
    m = random_mask(d.shape)
    _assert_array_equal(ttest_1samp       (d[m], 0),
                        ttest_1samp_masked(d,    0, mask=m))

    # 2D masking
    d = np.arange(30).reshape((5,-1))
    m = random_mask(d.shape)

    # axis=1
    ts, ps = ttest_1samp_masked(d, 0, mask=m, axis=1)
    for d_, m_, t_, p_ in zip(d, m, ts, ps):
        _assert_array_equal(ttest_1samp (d_[m_], 0), (t_, p_))

    # axis=0
    ts, ps = ttest_1samp_masked(d, 0, mask=m, axis=0)
    for d_, m_, t_, p_ in zip(d.T, m.T, ts, ps):
        _assert_array_equal(ttest_1samp (d_[m_], 0), (t_, p_))

    #5D masking
    d = np.random.normal(size=ndshape)
    m = random_mask(d.shape)

    for axis in range(d.ndim):
        for t0 in (0, 1.0):             # test for different targets
            ts, ps = ttest_1samp_masked(d, t0, mask=m, axis=axis)
            target_shape = list(d.shape)
            n = target_shape.pop(axis)
            assert_equal(ts.shape,  tuple(target_shape))

            def iterflat_view(a):
                return np.rollaxis(a, axis, 0).reshape((n, -1)).T

            # now compare to t-test with masking if done manually on
            for d_, m_, t_, p_ in zip(iterflat_view(d),
                                      iterflat_view(m),
                                      ts.flatten(),
                                      ps.flatten()):
                _assert_array_equal(ttest_1samp (d_[m_], t0), (t_, p_))


########NEW FILE########
__FILENAME__ = test_multiclf
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Multiclass Classifiers

Pulled into a separate tests file for efficiency
"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.datasets import *
from mvpa2.testing.clfs import *

from mvpa2.base.dataset import vstack

from mvpa2.generators.partition import NFoldPartitioner, OddEvenPartitioner
from mvpa2.generators.splitters import Splitter

from mvpa2.clfs.meta import CombinedClassifier, \
     BinaryClassifier, MulticlassClassifier, \
     MaximalVote
from mvpa2.measures.base import TransferMeasure, CrossValidation
from mvpa2.mappers.fx import mean_sample, BinaryFxNode
from mvpa2.misc.errorfx import mean_mismatch_error



# Generate test data for testing ties
#mvpa2._random_seed = 2#982220910
@reseed_rng()
def get_dsties1():
    ds = datasets['uni2small'].copy()
    dtarget = ds.targets[0]             # duplicate target
    tied_samples = ds.targets == dtarget
    ds2 = ds[tied_samples].copy(deep=True)
    # add similar noise to both ties
    noise_level = 0.2
    ds2.samples += \
                  np.random.normal(size=ds2.shape)*noise_level
    ds[tied_samples].samples += \
                  np.random.normal(size=ds2.shape)*noise_level
    ds2.targets[:] = 'TI' # 'E' would have been swallowed since it is S2 here
    ds = vstack((ds, ds2))
    ds.a.ties = [dtarget, 'TI']
    ds.a.ties_idx = [ds.targets == t for t in ds.a.ties]
    return ds
_dsties1 = get_dsties1()

#from mvpa2.clfs.smlr import SMLR
#clf=SMLR(lm=1.0, fit_all_weights=True, enable_ca=['estimates'])
#if True:
@sweepargs(clf=clfswh['multiclass'])
def test_multiclass_ties(clf):
    if 'lars' in clf.__tags__:
        raise SkipTest("Known to crash while running this test")
    ds = _dsties1

    # reassign data between ties, so we know that decision is data, not order driven
    ds_ = ds.copy(deep=True)
    ds_.samples[ds.a.ties_idx[1]] = ds.samples[ds.a.ties_idx[0]]
    ds_.samples[ds.a.ties_idx[0]] = ds.samples[ds.a.ties_idx[1]]
    ok_(np.any(ds_.samples != ds.samples))

    clf_ = clf.clone()
    clf = clf.clone()
    clf.ca.enable(['estimates', 'predictions'])
    clf_.ca.enable(['estimates', 'predictions'])
    te = TransferMeasure(clf, Splitter('train'),
                            postproc=BinaryFxNode(mean_mismatch_error,
                                                  'targets'),
                            enable_ca=['stats'])
    te_ = TransferMeasure(clf_, Splitter('train'),
                            postproc=BinaryFxNode(mean_mismatch_error,
                                                  'targets'),
                            enable_ca=['stats'])

    te = CrossValidation(clf, NFoldPartitioner(), postproc=mean_sample(),
                        enable_ca=['stats'])
    te_ = CrossValidation(clf_, NFoldPartitioner(), postproc=mean_sample(),
                        enable_ca=['stats'])

    error = te(ds)
    matrix = te.ca.stats.matrix

    # if ties were broken randomly we should have got nearly the same
    # number of hits for tied targets
    ties_indices = [te.ca.stats.labels.index(c) for c in ds.a.ties]
    hits = np.diag(te.ca.stats.matrix)[ties_indices]

    # First check is to see if we swap data between tied labels we
    # are getting the same results if we permute labels accordingly,
    # i.e. that tie resolution is not dependent on the labels order
    # but rather on the data
    te_(ds_)
    matrix_swapped = te_.ca.stats.matrix

    if False: #0 in hits:
        print clf, matrix, matrix_swapped
        print clf.ca.estimates[:, 2] - clf.ca.estimates[:,0]
        #print clf.ca.estimates

    # TODO: for now disabled all the non-compliant ones to pass the
    #       tests. For visibility decided to skip them instead of just
    #       exclusion and skipping only here to possibly catch crashes
    #       which might happen before
    if len(set(('libsvm', 'sg', 'skl', 'gpr', 'blr')
               ).intersection(clf.__tags__)):
        raise SkipTest("Skipped %s because it is known to fail")

    ok_(not (np.array_equal(matrix, matrix_swapped) and 0 in hits))

    # this check is valid only if ties are not broken randomly
    # like it is the case with SMLR
    if not ('random_tie_breaking' in clf.__tags__
            or  # since __tags__ would not go that high up e.g. in
                # <knn on SMLR non-0>
            'SMLR' in str(clf)):
        assert_array_equal(hits,
                           np.diag(matrix_swapped)[ties_indices[::-1]])

    # Second check is to just see if we didn't get an obvious bias and
    # got 0 in one of the hits, although it is labile
    if cfg.getboolean('tests', 'labile', default='yes'):
        ok_(not 0 in hits)
    # this is old test... even more cumbersome/unreliable
    #hits_ndiff = abs(float(hits[1]-hits[0]))/max(hits)
    #thr = 0.9   # let's be generous and pretty much just request absent 0s
    #ok_(hits_ndiff < thr)

@sweepargs(clf=clfswh['linear', 'svm', 'libsvm', '!meta', 'multiclass'])
@sweepargs(ds=[datasets['uni%dsmall' % i] for i in 2,3,4])
def test_multiclass_classifier_cv(clf, ds):
    # Extending test_clf.py:ClassifiersTests.test_multiclass_classifier
    # Compare performance with our MaximalVote to the one done natively
    # by e.g. LIBSVM
    clf = clf.clone()
    clf.params.C = 1                      # so it doesn't auto-adjust
    mclf = MulticlassClassifier(clf=clf.clone())
    part = NFoldPartitioner()
    cv  = CrossValidation(clf , part, enable_ca=['stats', 'training_stats'])
    mcv = CrossValidation(mclf, part, enable_ca=['stats', 'training_stats'])

    er  =  cv(ds)
    mer = mcv(ds)

    # errors should be the same
    assert_array_equal(er, mer)
    assert_equal(str(cv.ca.training_stats), str(mcv.ca.training_stats))
    # if it was a binary task, cv.ca.stats would also have AUC column
    # while mcv would not  :-/  TODO
    if len(ds.UT) == 2:
        # so just compare the matrix and ACC
        assert_array_equal(cv.ca.stats.matrix, mcv.ca.stats.matrix)
        assert_equal(cv.ca.stats.stats['ACC'], mcv.ca.stats.stats['ACC'])
    else:
        assert_equal(str(cv.ca.stats), str(mcv.ca.stats))
    

def test_multiclass_classifier_pass_ds_attributes():
    # TODO: replicate/extend basic testing of pass_attr
    #       in some more "basic" test_*
    clf = LinearCSVMC(C=1)
    ds = datasets['uni3small'].copy()
    ds.sa['ids'] = np.arange(len(ds))
    mclf = MulticlassClassifier(
        clf,
        pass_attr=['ids', 'sa.chunks', 'a.bogus_features',
                  # 'ca.raw_estimates' # this one is binary_clf x samples list ATM
                  # that is why raw_predictions_ds was born
                  'ca.raw_predictions_ds',
                  'ca.estimates', # this one is ok
                  'ca.predictions',
                  ],
        enable_ca=['all'])
    mcv  = CrossValidation(mclf, NFoldPartitioner(), errorfx=None)
    res = mcv(ds)
    assert_array_equal(sorted(res.sa.ids), ds.sa.ids)
    assert_array_equal(res.chunks, ds.chunks[res.sa.ids])
    assert_array_equal(res.sa.predictions, res.samples[:, 0])
    assert_array_equal(res.sa.cvfolds,
                       np.repeat(range(len(ds.UC)), len(ds)/len(ds.UC)))


def test_multiclass_without_combiner():
    # The goal is to obtain all pairwise results as the resultant dataset
    # avoiding even calling any combiner
    clf = LinearCSVMC(C=1)
    ds = datasets['uni3small'].copy()
    ds.sa['ids'] = np.arange(len(ds))
    mclf = MulticlassClassifier(clf, combiner=None)
    # without combining results at all
    mcv = CrossValidation(mclf, NFoldPartitioner(), errorfx=None)
    res = mcv(ds)
    assert_equal(len(res), len(ds))
    assert_equal(res.nfeatures, 3)        # 3 pairs for 3 classes
    assert_array_equal(res.UT, ds.UT)
    assert_array_equal(np.unique(np.array(res.fa.targets.tolist())), ds.UT)
    # TODO -- check that we have all the pairs?
    assert_array_equal(res.sa['cvfolds'].unique, np.arange(len(ds.UC)))
    if mcv.ca.is_enabled('training_stats'):
        # we must have received a dictionary per each pair
        training_stats = mcv.ca.training_stats
        assert_equal(set(training_stats.keys()),
                     set([('L1', 'L0'), ('L2', 'L1'), ('L2', 'L0')]))
        for pair, cm in training_stats.iteritems():
            assert_array_equal(cm.labels, ds.UT)
            # we should have no predictions for absent label
            assert_array_equal(cm.matrix[~np.in1d(ds.UT, pair)], 0)
            # while altogether all samples were processed once
            assert_array_equal(cm.stats['P'], len(ds))
            # and number of sets should be equal number of chunks here
            assert_equal(len(cm.sets), len(ds.UC))

########NEW FILE########
__FILENAME__ = test_neighborhood
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##

import os

import numpy as np
from numpy import array

from mvpa2.datasets.base import Dataset
import mvpa2.misc.neighborhood as ne
from mvpa2.clfs.distance import *

from mvpa2.testing.tools import ok_, assert_raises, assert_false, assert_equal, \
        assert_array_equal
from mvpa2.testing.datasets import datasets

def test_distances():
    a = np.array([3,8])
    b = np.array([6,4])
    # test distances or yarik recalls unit testing ;)
    assert_equal(cartesian_distance(a, b), 5.0)
    assert_equal(manhatten_distance(a, b), 7)
    assert_equal(absmin_distance(a, b), 4)


def test_sphere():
    # test sphere initialization
    s = ne.Sphere(1)
    center0 = (0, 0, 0)
    center1 = (1, 1, 1)
    assert_equal(len(s(center0)), 7)
    target = array([array([-1,  0,  0]),
              array([ 0, -1,  0]),
              array([ 0,  0, -1]),
              array([0, 0, 0]),
              array([0, 0, 1]),
              array([0, 1, 0]),
              array([1, 0, 0])])
    # test of internals -- no recomputation of increments should be done
    prev_increments = s._increments
    assert_array_equal(s(center0), target)
    ok_(prev_increments is s._increments)
    # query lower dimensionality
    _ = s((0, 0))
    ok_(not prev_increments is s._increments)

    # test Sphere call
    target = [array([0, 1, 1]),
              array([1, 0, 1]),
              array([1, 1, 0]),
              array([1, 1, 1]),
              array([1, 1, 2]),
              array([1, 2, 1]),
              array([2, 1, 1])]
    res = s(center1)
    assert_array_equal(array(res), target)
    # They all should be tuples
    ok_(np.all([isinstance(x, tuple) for x in res]))

    # test for larger diameter
    s = ne.Sphere(4)
    assert_equal(len(s(center1)), 257)

    # test extent keyword
    #s = ne.Sphere(4,extent=(1,1,1))
    #assert_array_equal(array(s((0,0,0))), array([[0,0,0]]))

    # test Errors during initialisation and call
    #assert_raises(ValueError, ne.Sphere, 2)
    #assert_raises(ValueError, ne.Sphere, 1.0)

    # no longer extent available
    assert_raises(TypeError, ne.Sphere, 1, extent=(1))
    assert_raises(TypeError, ne.Sphere, 1, extent=(1.0, 1.0, 1.0))

    s = ne.Sphere(1)
    #assert_raises(ValueError, s, (1))
    if __debug__:
        # No float coordinates allowed for now...
        # XXX might like to change that ;)
        # 
        assert_raises(ValueError, s, (1.0, 1.0, 1.0))


def test_sphere_distance_func():
    # Test some other distance
    se = ne.Sphere(3)
    sm = ne.Sphere(3, distance_func=manhatten_distance)
    rese = se((10, 5))
    resm = sm((10, 5))
    for res in rese, resm:
        # basic test for duplicates (I think we forgotten to test for them)
        ok_(len(res) == len(set(res)))

    # in manhatten distance we should all be no further than 3 "steps" away
    ok_(np.all([np.sum(np.abs(np.array(x) - (10, 5))) <= 3 for x in resm]))
    # in euclidean we are taking shortcuts ;)
    ok_(np.any([np.sum(np.abs(np.array(x) - (10, 5))) > 3 for x in rese]))


def test_sphere_scaled():
    s1 = ne.Sphere(3)
    s = ne.Sphere(3, element_sizes=(1, 1))

    # Should give exactly the same results since element_sizes are 1s
    for p in ((0, 0), (-23, 1)):
        assert_array_equal(s1(p), s(p))
        ok_(len(s(p)) == len(set(s(p))))

    # Raise exception if query dimensionality does not match element_sizes
    assert_raises(ValueError, s, (1,))

    s = ne.Sphere(3, element_sizes=(1.5, 2))
    assert_array_equal(s((0, 0)),
                       [(-2, 0), (-1, -1), (-1, 0), (-1, 1),
                        (0, -1), (0, 0), (0, 1),
                        (1, -1), (1, 0), (1, 1), (2, 0)])

    s = ne.Sphere(1.5, element_sizes=(1.5, 1.5, 1.5))
    res = s((0, 0, 0))
    ok_(np.all([np.sqrt(np.sum(np.array(x)**2)) <= 1.5 for x in res]))
    ok_(len(res) == 7)

    # all neighbors so no more than 1 voxel away -- just a cube, for
    # some "sphere" effect radius had to be 3.0 ;)
    td = np.sqrt(3*1.5**2)
    s = ne.Sphere(td, element_sizes=(1.5, 1.5, 1.5))
    res = s((0, 0, 0))
    ok_(np.all([np.sqrt(np.sum(np.array(x)**2)) <= td for x in res]))
    ok_(np.all([np.sum(np.abs(x) > 1) == 0 for x in res]))
    ok_(len(res) == 27)


def test_hollowsphere_basic():
    hs = ne.HollowSphere(1, 0)
    assert_array_equal(hs((2, 1)),  [(1, 1), (2, 0), (2, 2), (3, 1)])
    assert_array_equal(hs((1, )),   [(0,), (2,)])
    assert_equal(len(hs((1,1,1))), 6)


def test_hollowsphere_degenerate_neighborhood():
    """Test either we sustain empty neighborhoods
    """
    hs = ne.HollowSphere(1, inner_radius=0, element_sizes=(3,3,3))
    assert_equal(len(hs((1,1,1))), 0)


def test_query_engine():
    data = np.arange(54)
    # indices in 3D
    ind = np.transpose((np.ones((3, 3, 3)).nonzero()))
    # sphere generator for 3 elements diameter
    sphere = ne.Sphere(1)
    # dataset with just one "space"
    ds = Dataset([data, data], fa={'s_ind': np.concatenate((ind, ind))})
    # and the query engine attaching the generator to the "index-space"
    qe = ne.IndexQueryEngine(s_ind=sphere)
    # cannot train since the engine does not know about the second space
    assert_raises(ValueError, qe.train, ds)
    # now do it again with a full spec
    ds = Dataset([data, data], fa={'s_ind': np.concatenate((ind, ind)),
                                   't_ind': np.repeat([0,1], 27)})
    qe = ne.IndexQueryEngine(s_ind=sphere, t_ind=None)
    qe.train(ds)
    # internal representation check
    # YOH: invalid for new implementation with lookup tables (dictionaries)
    #assert_array_equal(qe._searcharray,
    #                   np.arange(54).reshape(qe._searcharray.shape) + 1)
    # should give us one corner, collapsing the 't_ind'
    assert_array_equal(qe(s_ind=(0, 0, 0)),
                       [0, 1, 3, 9, 27, 28, 30, 36])
    # directly specifying an index for 't_ind' without having an ROI
    # generator, should give the same corner, but just once
    assert_array_equal(qe(s_ind=(0, 0, 0), t_ind=0), [0, 1, 3, 9])
    # just out of the mask -- no match
    assert_array_equal(qe(s_ind=(3, 3, 3)), [])
    # also out of the mask -- but single match
    assert_array_equal(qe(s_ind=(2, 2, 3), t_ind=1), [53])
    # query by id
    assert_array_equal(qe(s_ind=(0, 0, 0), t_ind=0), qe[0])
    assert_array_equal(qe(s_ind=(0, 0, 0), t_ind=[0, 1]),
                       qe(s_ind=(0, 0, 0)))
    # should not fail if t_ind is outside
    assert_array_equal(qe(s_ind=(0, 0, 0), t_ind=[0, 1, 10]),
                       qe(s_ind=(0, 0, 0)))

    # should fail if asked about some unknown thing
    assert_raises(ValueError, qe.__call__, s_ind=(0, 0, 0), buga=0)

    # Test by using some literal feature atttribute
    ds.fa['lit'] =  ['roi1', 'ro2', 'r3']*18
    # should work as well as before
    assert_array_equal(qe(s_ind=(0, 0, 0)), [0, 1, 3, 9, 27, 28, 30, 36])
    # should fail if asked about some unknown (yet) thing
    assert_raises(ValueError, qe.__call__, s_ind=(0,0,0), lit='roi1')

    # Create qe which can query literals as well
    qe_lit = ne.IndexQueryEngine(s_ind=sphere, t_ind=None, lit=None)
    qe_lit.train(ds)
    # should work as well as before
    assert_array_equal(qe_lit(s_ind=(0, 0, 0)), [0, 1, 3, 9, 27, 28, 30, 36])
    # and subselect nicely -- only /3 ones
    assert_array_equal(qe_lit(s_ind=(0, 0, 0), lit='roi1'),
                       [0, 3, 9, 27, 30, 36])
    assert_array_equal(qe_lit(s_ind=(0, 0, 0), lit=['roi1', 'ro2']),
                       [0, 1, 3, 9, 27, 28, 30, 36])


def test_cached_query_engine():
    """Test cached query engine
    """
    sphere = ne.Sphere(1)
    # dataset with just one "space"
    ds = datasets['3dlarge']
    qe0 = ne.IndexQueryEngine(myspace=sphere)
    qec = ne.CachedQueryEngine(qe0)

    # and ground truth one
    qe = ne.IndexQueryEngine(myspace=sphere)
    results_ind = []
    results_kw = []

    def cmp_res(res1, res2):
        comp = [x == y for x, y in zip(res1, res2)]
        ok_(np.all(comp))

    for iq, q in enumerate((qe, qec)):
        q.train(ds)
        # sequential train on the same should be ok in both cases
        q.train(ds)
        res_ind = [q[fid] for fid in xrange(ds.nfeatures)]
        res_kw = [q(myspace=x) for x in ds.fa.myspace]
        # test if results match
        cmp_res(res_ind, res_kw)

        results_ind.append(res_ind)
        results_kw.append(res_kw)

    # now check if results of cached were the same as of regular run
    cmp_res(results_ind[0], results_ind[1])

    # Now do sanity checks
    assert_raises(ValueError, qec.train, ds[:, :-1])
    assert_raises(ValueError, qec.train, ds.copy())
    ds2 = ds.copy()
    qec.untrain()
    qec.train(ds2)
    # should be the same results on the copy
    cmp_res(results_ind[0], [qec[fid] for fid in xrange(ds.nfeatures)])
    cmp_res(results_kw[0], [qec(myspace=x) for x in ds.fa.myspace])
    ok_(qec.train(ds2) is None)
    # unfortunately we are not catching those
    #ds2.fa.myspace = ds2.fa.myspace*3
    #assert_raises(ValueError, qec.train, ds2)

def test_scattered_neighborhoods():
    radius = 1
    sphere = ne.Sphere(radius)
    coords = range(50)

    scoords, sidx = ne.scatter_neighborhoods(sphere, coords,
                                             deterministic=False)
    # for this specific case of 1d coordinates the coords and idx should be
    # identical
    assert_array_equal(scoords, sidx)
    # minimal difference of successive coordinates must be larger than the
    # radius of the spheres. Test only works for 1d coords and sorted return
    # values
    assert(np.diff(scoords).min() > radius)

    # now the same for the case where a particular coordinate appears multiple
    # times
    coords = range(10) + range(10)
    scoords, sidx = ne.scatter_neighborhoods(sphere, coords,
                                             deterministic=False)
    sidx = sorted(sidx)
    assert_array_equal(scoords, sidx[:5])
    assert_array_equal(scoords, [i - 10 for i in sidx[5:]])

########NEW FILE########
__FILENAME__ = test_niftidataset
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA nifti dataset"""

import os
import numpy as np

from mvpa2 import cfg
from mvpa2.testing import *

if not externals.exists('nibabel'):
    raise SkipTest

from mvpa2.base.dataset import vstack
from mvpa2 import pymvpa_dataroot
from mvpa2.datasets.mri import fmri_dataset, _load_anyimg, map2nifti
from mvpa2.datasets.eventrelated import eventrelated_dataset
from mvpa2.misc.fsl import FslEV3
from mvpa2.misc.support import Event, value2idx
from mvpa2.misc.io.base import SampleAttributes


def test_nifti_dataset():
    """Basic testing of NiftiDataset
    """
    ds = fmri_dataset(samples=os.path.join(pymvpa_dataroot, 'example4d.nii.gz'),
                       targets=[1,2], sprefix='voxel')
    assert_equal(ds.nfeatures, 294912)
    assert_equal(ds.nsamples, 2)

    assert_array_equal(ds.a.voxel_eldim, ds.a.imghdr['pixdim'][1:4])
    assert_true(ds.a['voxel_dim'].value == (128,96,24))


    # XXX move elsewhere
    #check that mapper honours elementsize
    #nb22 = np.array([i for i in data.a.mapper.getNeighborIn((1, 1, 1), 2.2)])
    #nb20 = np.array([i for i in data.a.mapper.getNeighborIn((1, 1, 1), 2.0)])
    #self.assertTrue(nb22.shape[0] == 7)
    #self.assertTrue(nb20.shape[0] == 5)

    merged = vstack((ds.copy(), ds), a=0)
    assert_equal(merged.nfeatures, 294912)
    assert_equal(merged.nsamples, 4)

    # check that the header survives
    for k in merged.a.imghdr.keys():
        assert_array_equal(merged.a.imghdr[k], ds.a.imghdr[k])

    # throw away old dataset and see if new one survives
    del ds
    assert_array_equal(merged.samples[3], merged.samples[1])

    # check whether we can use a plain ndarray as mask
    mask = np.zeros((128, 96, 24), dtype='bool')
    mask[40, 20, 12] = True
    nddata = fmri_dataset(samples=os.path.join(pymvpa_dataroot,'example4d.nii.gz'),
                          targets=[1,2],
                          mask=mask)
    assert_equal(nddata.nfeatures, 1)
    rmap = nddata.a.mapper.reverse1(np.array([44]))
    assert_equal(rmap.shape, (128, 96, 24))
    assert_equal(np.sum(rmap), 44)
    assert_equal(rmap[40, 20, 12], 44)


def test_fmridataset():
    # full-blown fmri dataset testing
    import nibabel
    maskimg = nibabel.load(os.path.join(pymvpa_dataroot, 'mask.nii.gz'))
    data = maskimg.get_data().copy()
    data[data>0] = np.arange(1, np.sum(data) + 1)
    maskimg = nibabel.Nifti1Image(data, None, maskimg.get_header())
    attr = SampleAttributes(os.path.join(pymvpa_dataroot, 'attributes.txt'))
    ds = fmri_dataset(samples=os.path.join(pymvpa_dataroot,'bold.nii.gz'),
                      targets=attr.targets, chunks=attr.chunks,
                      mask=maskimg,
                      sprefix='subj1',
                      add_fa={'myintmask': maskimg})
    # content
    assert_equal(len(ds), 1452)
    assert_true(ds.nfeatures, 530)
    assert_array_equal(sorted(ds.sa.keys()),
            ['chunks', 'targets', 'time_coords', 'time_indices'])
    assert_array_equal(sorted(ds.fa.keys()),
            ['myintmask', 'subj1_indices'])
    assert_array_equal(sorted(ds.a.keys()),
            ['imghdr', 'imgtype', 'mapper', 'subj1_dim', 'subj1_eldim'])
    # vol extent
    assert_equal(ds.a.subj1_dim, (40, 20, 1))
    # check time
    assert_equal(ds.sa.time_coords[-1], 3627.5)
    # non-zero mask values
    assert_array_equal(ds.fa.myintmask, np.arange(1, ds.nfeatures + 1))
    # we know that imgtype must be:
    ok_(ds.a.imgtype is nibabel.Nifti1Image)

@with_tempfile(suffix='.img')
def test_nifti_mapper(filename):
    """Basic testing of map2Nifti
    """
    skip_if_no_external('scipy')

    import nibabel
    data = fmri_dataset(samples=os.path.join(pymvpa_dataroot,'example4d.nii.gz'),
                        targets=[1,2])

    # test mapping of ndarray
    vol = map2nifti(data, np.ones((294912,), dtype='int16'))
    if externals.versions['nibabel'] >= '1.2': 
        vol_shape = vol.shape
    else:
        vol_shape = vol.get_shape()
    assert_equal(vol_shape, (128, 96, 24))
    assert_true((vol.get_data() == 1).all())
    # test mapping of the dataset
    vol = map2nifti(data)
    if externals.versions['nibabel'] >= '1.2':
        vol_shape = vol.shape
    else:
        vol_shape = vol.get_shape()
    assert_equal(vol_shape, (128, 96, 24, 2))
    ok_(isinstance(vol, data.a.imgtype))

    # test providing custom imgtypes
    vol = map2nifti(data, imgtype=nibabel.Nifti1Pair)
    if externals.versions['nibabel'] >= '1.2':
        vol_shape = vol.shape
    else:
        vol_shape = vol.get_shape()
    ok_(isinstance(vol, nibabel.Nifti1Pair))

    # Lets generate a dataset using an alternative format (MINC)
    # and see if type persists
    volminc = nibabel.MincImage(vol.get_data(),
                                vol.get_affine(),
                                vol.get_header())
    ok_(isinstance(volminc, nibabel.MincImage))
    dsminc = fmri_dataset(volminc, targets=1)
    ok_(dsminc.a.imgtype is nibabel.MincImage)
    ok_(isinstance(dsminc.a.imghdr, nibabel.minc.MincImage.header_class))

    # Lets test if we could save/load now into Analyze volume/dataset
    if externals.versions['nibabel'] < '1.1.0':
        raise SkipTest('nibabel prior 1.1.0 had an issue with types comprehension')
    volanal = map2nifti(dsminc, imgtype=nibabel.AnalyzeImage) # MINC has no 'save' capability
    ok_(isinstance(volanal, nibabel.AnalyzeImage))
    volanal.to_filename(filename)
    dsanal = fmri_dataset(filename, targets=1)
    # this one is tricky since it might become Spm2AnalyzeImage
    ok_('AnalyzeImage' in str(dsanal.a.imgtype))
    ok_('AnalyzeHeader' in str(dsanal.a.imghdr.__class__))
    volanal_ = map2nifti(dsanal)
    ok_(isinstance(volanal_, dsanal.a.imgtype)) # type got preserved


def test_multiple_calls():
    """Test if doing exactly the same operation twice yields the same result
    """
    data = fmri_dataset(samples=os.path.join(pymvpa_dataroot,'example4d.nii.gz'),
                        targets=1, sprefix='abc')
    data2 = fmri_dataset(samples=os.path.join(pymvpa_dataroot,'example4d.nii.gz'),
                         targets=1, sprefix='abc')
    assert_array_equal(data.a.abc_eldim, data2.a.abc_eldim)


def test_er_nifti_dataset():
    # setup data sources
    tssrc = os.path.join(pymvpa_dataroot, u'bold.nii.gz')
    evsrc = os.path.join(pymvpa_dataroot, 'fslev3.txt')
    masrc = os.path.join(pymvpa_dataroot, 'mask.nii.gz')
    evs = FslEV3(evsrc).to_events()
    # load timeseries
    ds_orig = fmri_dataset(tssrc)
    # segment into events
    ds = eventrelated_dataset(ds_orig, evs, time_attr='time_coords')

    # we ask for boxcars of 9s length, and the tr in the file header says 2.5s
    # hence we should get round(9.0/2.4) * np.prod((1,20,40) == 3200 features
    assert_equal(ds.nfeatures, 3200)
    assert_equal(len(ds), len(evs))
    # the voxel indices are reflattened after boxcaring , but still 3D
    assert_equal(ds.fa.voxel_indices.shape, (ds.nfeatures, 3))
    # and they have been broadcasted through all boxcars
    assert_array_equal(ds.fa.voxel_indices[:800], ds.fa.voxel_indices[800:1600])
    # each feature got an event offset value
    assert_array_equal(ds.fa.event_offsetidx, np.repeat([0,1,2,3], 800))
    # check for all event attributes
    assert_true('onset' in ds.sa)
    assert_true('duration' in ds.sa)
    assert_true('features' in ds.sa)
    # check samples
    origsamples = _load_anyimg(tssrc)[0]
    for i, onset in \
        enumerate([value2idx(e['onset'], ds_orig.sa.time_coords, 'floor')
                        for e in evs]):
        assert_array_equal(ds.samples[i], origsamples[onset:onset+4].ravel())
        assert_array_equal(ds.sa.time_indices[i], np.arange(onset, onset + 4))
        assert_array_equal(ds.sa.time_coords[i],
                           np.arange(onset, onset + 4) * 2.5)
        for evattr in [a for a in ds.sa
                        if a.count("event_attrs")
                           and not a.count('event_attrs_event')]:
            assert_array_equal(evs[i]['_'.join(evattr.split('_')[2:])],
                               ds.sa[evattr].value[i])
    # check offset: only the last one exactly matches the tr
    assert_array_equal(ds.sa.orig_offset, [1, 1, 0])

    # map back into voxel space, should ignore addtional features
    nim = map2nifti(ds)
    # origsamples has t,x,y,z
    if externals.versions['nibabel'] >= '1.2':
        vol_shape = nim.shape
    else:
        vol_shape = nim.get_shape()
    assert_equal(vol_shape, origsamples.shape[1:] + (len(ds) * 4,))
    # check shape of a single sample
    nim = map2nifti(ds, ds.samples[0])
    if externals.versions['nibabel'] >= '1.2':
        vol_shape = nim.shape
    else:
        vol_shape = nim.get_shape()
    # pynifti image has [t,]z,y,x
    assert_equal(vol_shape, (40, 20, 1, 4))

    # and now with masking
    ds = fmri_dataset(tssrc, mask=masrc)
    ds = eventrelated_dataset(ds, evs, time_attr='time_coords')
    nnonzero = len(_load_anyimg(masrc)[0].nonzero()[0])
    assert_equal(nnonzero, 530)
    # we ask for boxcars of 9s length, and the tr in the file header says 2.5s
    # hence we should get round(9.0/2.4) * np.prod((1,20,40) == 3200 features
    assert_equal(ds.nfeatures, 4 * 530)
    assert_equal(len(ds), len(evs))
    # and they have been broadcasted through all boxcars
    assert_array_equal(ds.fa.voxel_indices[:nnonzero],
                       ds.fa.voxel_indices[nnonzero:2*nnonzero])



def test_er_nifti_dataset_mapping():
    """Some mapping testing -- more tests is better
    """
    # z,y,x
    sample_size = (4, 3, 2)
    # t,z,y,x
    samples = np.arange(120).reshape((5,) + sample_size)
    dsmask = np.arange(24).reshape(sample_size) % 2
    import nibabel
    tds = fmri_dataset(nibabel.Nifti1Image(samples.T, None),
                       mask=nibabel.Nifti1Image(dsmask.T, None))
    ds = eventrelated_dataset(
            tds,
            events=[Event(onset=0, duration=2, label=1,
                          chunk=1, features=[1000, 1001]),
                    Event(onset=1, duration=2, label=2,
                          chunk=1, features=[2000, 2001])])
    nfeatures = tds.nfeatures
    mask = np.zeros(dsmask.shape, dtype='bool')
    mask[0, 0, 0] = mask[1, 0, 1] = mask[0, 0, 1] = 1
    fmask = ds.a.mapper.forward1(mask.T)
    # select using mask in volume and all features in the other part
    ds_sel = ds[:, fmask]

    # now tests
    assert_array_equal(mask.reshape(24).nonzero()[0], [0, 1, 7])
    # two events, 2 orig features at 2 timepoints
    assert_equal(ds_sel.samples.shape, (2, 4))
    assert_array_equal(ds_sel.sa.features,
                       [[1000, 1001], [2000, 2001]])
    assert_array_equal(ds_sel.samples,
                       [[   1,    7,   25,   31],
                        [  25,   31,   49,   55]])
    # reproducability
    assert_array_equal(ds_sel.samples,
                       ds_sel.a.mapper.forward(np.rollaxis(samples.T, -1)))

    # reverse-mapping
    rmapped = ds_sel.a.mapper.reverse1(np.arange(10, 14))
    assert_equal(np.rollaxis(rmapped, 0, 4).T.shape, (2,) + sample_size)
    expected = np.zeros((2,)+sample_size, dtype='int')
    expected[0,0,0,1] = 10
    expected[0,1,0,1] = 11
    expected[1,0,0,1] = 12
    expected[1,1,0,1] = 13
    assert_array_equal(np.rollaxis(rmapped, 0, 4).T, expected)


def test_nifti_dataset_from3_d():
    """Test NiftiDataset based on 3D volume(s)
    """
    tssrc = os.path.join(pymvpa_dataroot, 'bold.nii.gz')
    masrc = os.path.join(pymvpa_dataroot, 'mask.nii.gz')

    # Test loading of 3D volumes
    # by default we are enforcing 4D, testing here with the demo 3d mask
    ds = fmri_dataset(masrc, mask=masrc, targets=1)
    assert_equal(len(ds), 1)

    import nibabel
    plain_data = nibabel.load(masrc).get_data()
    # Lets check if mapping back works as well
    assert_array_equal(plain_data,
                       map2nifti(ds).get_data().reshape(plain_data.shape))

    # test loading from a list of filenames

    # for now we should fail if trying to load a mix of 4D and 3D volumes
    # TODO: nope -- it should work and we should test here if correctly
    dsfull_plusone = fmri_dataset((masrc, tssrc), mask=masrc, targets=1)

    # Lets prepare some custom NiftiImage
    dsfull = fmri_dataset(tssrc, mask=masrc, targets=1)
    assert_equal(len(dsfull)+1, len(dsfull_plusone))
    assert_equal(dsfull.nfeatures, dsfull_plusone.nfeatures)
    # skip 3d mask in 0th sample
    
    assert_array_equal(dsfull.samples, dsfull_plusone[1:].samples)
    ds_selected = dsfull[3]
    nifti_selected = map2nifti(ds_selected)

    # Load dataset from a mix of 3D volumes
    # (given by filenames and NiftiImages)
    labels = [123, 2, 123]
    ds2 = fmri_dataset((masrc, masrc, nifti_selected),
                       mask=masrc, targets=labels)
    assert_equal(ds2.nsamples, 3)
    assert_array_equal(ds2.samples[0], ds2.samples[1])
    assert_array_equal(ds2.samples[2], dsfull.samples[3])
    assert_array_equal(ds2.targets, labels)


#def test_nifti_dataset_roi_mask_neighbors(self):
#    """Test if we could request neighbors within spherical ROI whenever
#       center is outside of the mask
#       """
#
#    # check whether we can use a plain ndarray as mask
#    mask_roi = np.zeros((24, 96, 128), dtype='bool')
#    mask_roi[12, 20, 38:42] = True
#    mask_roi[23, 20, 38:42] = True  # far away
#    ds_full = nifti_dataset(samples=os.path.join(pymvpa_dataroot,'example4d'),
#                           targets=[1,2])
#    ds_roi = nifti_dataset(samples=os.path.join(pymvpa_dataroot,'example4d'),
#                           targets=[1,2], mask=mask_roi)
#    # Should just work since we are in the mask
#    ids_roi = ds_roi.a.mapper.getNeighbors(
#                    ds_roi.a.mapper.getOutId((12, 20, 40)),
#                    radius=20)
#    self.assertTrue(len(ids_roi) == 4)
#
#    # Trying to request feature outside of the mask
#    self.assertRaises(ValueError,
#                          ds_roi.a.mapper.getOutId,
#                          (12, 20, 37))
#
#    # Lets work around using full (non-masked) volume
#    ids_out = []
#    for id_in in ds_full.a.mapper.getNeighborIn( (12, 20, 37), radius=20):
#        try:
#            ids_out.append(ds_roi.a.mapper.getOutId(id_in))
#        except ValueError:
#            pass
#    self.assertTrue(ids_out == ids_roi)

@with_tempfile(suffix='.nii.gz')
def test_assumptions_on_nibabel_behavior(filename):
    if not externals.exists('nibabel'):
        raise SkipTest('No nibabel available')

    import nibabel as nb
    masrc = os.path.join(pymvpa_dataroot, 'mask.nii.gz')
    ni = nb.load(masrc)
    hdr = ni.get_header()
    data = ni.get_data()
    # operate in the native endianness so that symbolic type names (e.g. 'int16')
    # remain the same across platforms
    if hdr.endianness == nb.volumeutils.swapped_code:
        hdr = hdr.as_byteswapped()
    assert_equal(hdr.get_data_dtype(), 'int16') # we deal with int file

    dataf = data.astype(float)
    dataf_dtype = dataf.dtype
    dataf[1,1,0] = 123 + 1./3

    # and if we specify float64 as the datatype we should be in better
    # position
    hdr64 = hdr.copy()
    hdr64.set_data_dtype('float64')

    for h,t,d in ((hdr, 'int16', 2),
                  (hdr64, 'float64', 166)):
        # we can only guarantee 2-digits precision while converting
        # into int16? weird
        # but infinite precision for float64 since data and file
        # formats match
        nif = nb.Nifti1Image(dataf, None, h)
        # Header takes over and instructs to keep it int despite dtype
        assert_equal(nif.get_header().get_data_dtype(), t)
        # but does not cast the data (yet?) into int16 (in case of t==int16)
        assert_equal(nif.get_data().dtype, dataf_dtype)
        # nor changes somehow within dataf
        assert_equal(dataf.dtype, dataf_dtype)

        # save it back to the file and load
        nif.to_filename(filename)
        nif_ = nb.load(filename)
        dataf_ = nif_.get_data()
        assert_equal(nif_.get_header().get_data_dtype(), t)
        assert_equal(dataf_.dtype, dataf_dtype)
        assert_array_almost_equal(dataf_, dataf, decimal=d)
        # TEST scale/intercept to be changed
        slope, inter = nif_.get_header().get_slope_inter()
        if t == 'int16':
            # it should have rescaled the data
            assert_not_equal(slope, 1.0)
            assert_not_equal(inter, 0)
        else:
            assert_equal(slope, 1.0)
            assert_equal(inter, 0)


########NEW FILE########
__FILENAME__ = test_params
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Parameter class."""

import unittest, copy

import numpy as np

from mvpa2.datasets.base import dataset_wizard
from mvpa2.base.state import ClassWithCollections, ConditionalAttribute
from mvpa2.base.param import Parameter, KernelParameter
from mvpa2.base.constraints import *
from mvpa2.testing.clfs import *
from mvpa2.testing import assert_warnings

class ParametrizedClassifier(SameSignClassifier):
    p1 = Parameter(1.0, constraints='float')
    kp1 = KernelParameter(100.0)

class ParametrizedClassifierExtended(ParametrizedClassifier):
    def __init__(self):
        ParametrizedClassifier.__init__(self)
        self.kernel_params['kp2'] = \
            KernelParameter(200.0, doc="Very useful param")


class ChoiceClass(ClassWithCollections):
    C = Parameter('choice1',
                  constraints=EnsureChoice('choice1', 'choice2'),
                  doc="documentation")

class BlankClass(ClassWithCollections):
    pass

class SimpleClass(ClassWithCollections):
    C = Parameter(1.0, 
                  constraints=Constraints(EnsureFloat(),
                                          EnsureRange(min=0.0, max=10.0)),
                  doc="C parameter")

class MixedClass(ClassWithCollections):
    C = Parameter(1.0, constraints=EnsureRange(min=0), doc="C parameter")
    D = Parameter(3.0, constraints=EnsureRange(min=0), doc="D parameter")
    state1 = ConditionalAttribute(doc="bogus")

class ParamsTests(unittest.TestCase):

    def test_blank(self):
        blank  = BlankClass()

        self.assertRaises(AttributeError, blank.__getattribute__, 'ca')
        self.assertRaises(AttributeError, blank.__getattribute__, '')

    def test_deprecated_allowedtype(self):
        with assert_warnings(
                [(DeprecationWarning,
                  "allowedtype option was deprecated in favor of constraints. "
                  "Adjust your code, provided value 'str' was ignored")]):
            p = Parameter(1.0, allowedtype="str")
            self.assertRaises(AttributeError, lambda p: p.allowedtype, p)
            self.assertEqual(p.constraints, None)

    def test_choice(self):
        c = ChoiceClass()
        self.assertRaises(ValueError, c.params.__setattr__, 'C', 'bu')

    def test_simple(self):
        simple  = SimpleClass()

        self.assertEqual(len(simple.params.items()), 1)
        self.assertRaises(AttributeError, simple.__getattribute__, 'dummy')
        self.assertRaises(AttributeError, simple.__getattribute__, '')

        self.assertEqual(simple.params.C, 1.0)
        self.assertEqual(simple.params.is_set("C"), False)
        self.assertEqual(simple.params.is_set(), False)
        self.assertEqual(simple.params["C"].is_default, True)
        self.assertEqual(simple.params["C"].equal_default, True)

        simple.params.C = 1.0
        # we are not actually setting the value if == default
        self.assertEqual(simple.params["C"].is_default, True)
        self.assertEqual(simple.params["C"].equal_default, True)

        simple.params.C = 10.0
        self.assertEqual(simple.params.is_set("C"), True)
        self.assertEqual(simple.params.is_set(), True)
        self.assertEqual(simple.params["C"].is_default, False)
        self.assertEqual(simple.params["C"].equal_default, False)

        self.assertEqual(simple.params.C, 10.0)
        simple.params["C"].reset_value()
        self.assertEqual(simple.params.is_set("C"), True)
        # TODO: Test if we 'train' a classifier f we get is_set to false
        self.assertEqual(simple.params.C, 1.0)
        self.assertRaises(AttributeError, simple.params.__getattribute__, 'B')

        # set int but get float
        simple.params.C = 10
        self.assertTrue(isinstance(simple.params.C, float))
        # wrong type causes exception
        self.assertRaises(ValueError, simple.params.__setattr__, 'C', 'somestr')
        # value < min causes exception
        self.assertRaises(ValueError, simple.params.__setattr__, 'C', -123.4)
        # value > max causes exception
        self.assertRaises(ValueError, simple.params.__setattr__, 'C', 123.4)


        # check for presence of the constraints description
        self.assertTrue(simple._paramsdoc[0][1].find('Constraints: ') > 0)

    def test_mixed(self):
        mixed  = MixedClass()

        self.assertEqual(len(mixed.params.items()), 2)
        self.assertEqual(len(mixed.ca.items()), 1)
        self.assertRaises(AttributeError, mixed.__getattribute__, 'kernel_params')

        self.assertEqual(mixed.params.C, 1.0)
        self.assertEqual(mixed.params.is_set("C"), False)
        self.assertEqual(mixed.params.is_set(), False)
        mixed.params.C = 10.0
        self.assertEqual(mixed.params.is_set("C"), True)
        self.assertEqual(mixed.params.is_set("D"), False)
        self.assertEqual(mixed.params.is_set(), True)
        self.assertEqual(mixed.params.D, 3.0)


    def test_classifier(self):
        clf  = ParametrizedClassifier()
        self.assertEqual(len(clf.params.items()), 2) # + retrainable
        self.assertEqual(len(clf.kernel_params.items()), 1)

        clfe  = ParametrizedClassifierExtended()
        self.assertEqual(len(clfe.params.items()), 2)
        self.assertEqual(len(clfe.kernel_params.items()), 2)
        self.assertEqual(len(clfe.kernel_params.listing), 2)

        # check assignment once again
        self.assertEqual(clfe.kernel_params.kp2, 200.0)
        clfe.kernel_params.kp2 = 201.0
        self.assertEqual(clfe.kernel_params.kp2, 201.0)
        self.assertEqual(clfe.kernel_params.is_set("kp2"), True)
        clfe.train(dataset_wizard(samples=[[0,0]], targets=[1], chunks=[1]))
        self.assertEqual(clfe.kernel_params.is_set("kp2"), False)
        self.assertEqual(clfe.kernel_params.is_set(), False)
        self.assertEqual(clfe.params.is_set(), False)

    def test_incorrect_parameter_error(self):
        # Just a sample class
        from mvpa2.generators.partition import NFoldPartitioner
        try:
            spl = NFoldPartitioner(1, incorrect=None)
            raise AssertionError("Must have failed with an exception here "
                                 "due to incorrect parameter")
        except Exception, e:
            estr = str(e)
        self.assertTrue(not "calling_time" in estr,
             msg="must give valid parameters for partitioner, "
                 "not .ca's. Got: \n\t%r" % estr)
        # sample parameters which should be present
        for p in 'count', 'disable_ca', 'postproc':
            self.assertTrue(p in estr)

    def test_choices(self):
        # Test doc strings for parameters with choices
        class WithChoices(ClassWithCollections):
            C = Parameter('choice1',
                  constraints=EnsureChoice('choice1', 'choice2'),
                  doc="documentation")
            # We need __init__ to get 'custom' docstring
            def __init__(self, **kwargs):
                super(type(self), self).__init__(**kwargs)
  
        c = WithChoices()
        self.assertRaises(ValueError, c.params.__setattr__, 'C', 'bu')
        c__doc__ = c.__init__.__doc__.replace('"', "'")
        # Will currently fail due to unfixed _paramdoc of Parameter class 
        #self.assertTrue('choice2' in c__doc__)
        #self.assertTrue("(Default: 'choice1')" in c__doc__)

        # But we will not (at least for now) list choices if there are
        # non-strings
        class WithFuncChoices(ClassWithCollections):
            C = Parameter('choice1',
                          constraints=EnsureChoice('choice1', np.sum),
                          doc="documentation")
            # We need __init__ to get 'custom' docstring
            def __init__(self, **kwargs):
                super(type(self), self).__init__(**kwargs)

        cf = WithFuncChoices()
        self.assertRaises(ValueError, cf.params.__setattr__, 'C', 'bu')
        cf.params.C = np.sum
        cf__doc__ = cf.__init__.__doc__.replace('"', "'")
        # Will currently fail due to unfixed _paramdoc of Parameter class 
        #self.assertTrue('choice2' in c__doc__)
        #self.assertTrue("(Default: 'choice1')" in c__doc__)        
        #self.assertTrue("(Default: 'choice1')" in cf__doc__)

    def test_simple_specs(self):
        p = Parameter(1.0, constraints='int')
        self.assertTrue(p.value is 1)
        self.assertTrue(p.constraints is constraint_spec_map['int'])
        self.assertRaises(ValueError, Parameter, 'a', constraints='int')
        self.assertRaises(ValueError, Parameter, 1.0, constraints='str')


def suite():  # pragma: no cover
    return unittest.makeSuite(ParamsTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_perturbsensana
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA perturbation sensitivity analyzer."""

import numpy as np
from mvpa2.testing import *
from mvpa2.testing.clfs import *

from mvpa2.datasets.base import Dataset
from mvpa2.measures.noiseperturbation import NoisePerturbationSensitivity
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.measures.base import CrossValidation


class PerturbationSensitivityAnalyzerTests(unittest.TestCase):

    @reseed_rng()
    def setUp(self):
        data = np.random.standard_normal(( 100, 3, 4, 2 ))
        labels = np.concatenate( ( np.repeat( 0, 50 ),
                                  np.repeat( 1, 50 ) ) )
        chunks = np.repeat( range(5), 10 )
        chunks = np.concatenate( (chunks, chunks) )
        mask = np.ones( (3, 4, 2), dtype='bool')
        mask[0,0,0] = 0
        mask[1,3,1] = 0
        self.dataset = Dataset.from_wizard(samples=data, targets=labels,
                                           chunks=chunks, mask=mask)


    def test_perturbation_sensitivity_analyzer(self):
        # compute N-1 cross-validation as datameasure
        cv = CrossValidation(sample_clf_lin, NFoldPartitioner())
        # do perturbation analysis using gaussian noise
        pa = NoisePerturbationSensitivity(cv, noise=np.random.normal)

        # run analysis
        map = pa(self.dataset)

        # check for correct size of map
        self.assertTrue(map.nfeatures == self.dataset.nfeatures)

        # dataset is noise -> mean sensitivity should be zero
        self.assertTrue(-0.2 < np.mean(map) < 0.2)


def suite():  # pragma: no cover
    return unittest.makeSuite(PerturbationSensitivityAnalyzerTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_plr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA logistic regression classifier"""

import numpy as np

from mvpa2.clfs.plr import PLR
from mvpa2.testing import *
from mvpa2.testing.datasets import datasets


class PLRTests(unittest.TestCase):

    def test_plr(self):
        data = datasets['dumb2']

        clf = PLR()

        clf.train(data)

        # prediction has to be perfect
        self.assertTrue((clf.predict(data.samples) == data.targets).all())

    def test_plr_state(self):
        data = datasets['dumb2']

        clf = PLR()

        clf.train(data)
        # Also get "sensitivity".  Was introduced to check a bug with
        # processing dataset with numeric labels
        sa = clf.get_sensitivity_analyzer()
        sens = sa(data)

        clf.ca.enable('estimates')
        clf.ca.enable('predictions')

        p = clf.predict(data.samples)

        self.assertTrue((p == clf.ca.predictions).all())
        self.assertTrue(np.array(clf.ca.estimates).shape == np.array(p).shape)


def suite():  # pragma: no cover
    return unittest.makeSuite(PLRTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_procrust
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Procrustean mapper"""


import unittest
import numpy as np
from numpy.linalg import norm
from mvpa2.base import externals
from mvpa2.datasets.base import dataset_wizard
from mvpa2.testing import *
from mvpa2.testing.datasets import *
from mvpa2.mappers.procrustean import ProcrusteanMapper

svds = ['numpy']
if externals.exists('liblapack.so'):
    svds += ['dgesvd']
if externals.exists('scipy'):
    svds += ['scipy']

class ProcrusteanMapperTests(unittest.TestCase):

    @sweepargs(oblique=(False,True))
    @sweepargs(svd=svds)
    @reseed_rng()
    def test_simple(self, svd, oblique):
        d_orig = datasets['uni2large'].samples
        d_orig2 = datasets['uni4large'].samples
        for sdim, nf_s, nf_t, full_test \
                in (('Same 2D',  2,  2,  True),
                    ('Same 10D', 10, 10, True),
                    ('2D -> 3D', 2,  3,  True),
                    ('3D -> 2D', 3,  2,  False)):
            # figure out some "random" rotation
            d = max(nf_s, nf_t)
            R = get_random_rotation(nf_s, nf_t, d_orig)
            if nf_s == nf_t:
                adR = np.abs(1.0 - np.linalg.det(R))
                self.assertTrue(adR < 1e-10,
                                "Determinant of rotation matrix should "
                                "be 1. Got it 1+%g" % adR)
                self.assertTrue(norm(np.dot(R, R.T)
                                     - np.eye(R.shape[0])) < 1e-10)

            for s, scaling in ((0.3, True), (1.0, False)):
                pm = ProcrusteanMapper(scaling=scaling, oblique=oblique, svd=svd)
                # pm2 = ProcrusteanMapper(scaling=scaling, oblique=oblique)

                t1, t2 = d_orig[23, 1], d_orig[22, 1]

                # Create source/target data
                d = d_orig[:, :nf_s]
                d_s = d + t1
                d_t = np.dot(s * d, R) + t2

                # train bloody mapper(s)
                ds = dataset_wizard(samples=d_s, targets=d_t)
                pm.train(ds)
                ## not possible with new interface
                #pm2.train(d_s, d_t)

                ## verify that both created the same transformation
                #npm2proj = norm(pm.proj - pm2.proj)
                #self.assertTrue(npm2proj <= 1e-10,
                #                msg="Got transformation different by norm %g."
                #                " Had to be less than 1e-10" % npm2proj)
                #self.assertTrue(norm(pm._offset_in - pm2._offset_in) <= 1e-10)
                #self.assertTrue(norm(pm._offset_out - pm2._offset_out) <= 1e-10)

                # do forward transformation on the same source data
                d_s_f = pm.forward(d_s)

                self.assertEqual(d_s_f.shape, d_t.shape,
                    msg="Mapped shape should be identical to the d_t")

                dsf = d_s_f - d_t
                ndsf = norm(dsf)/norm(d_t)
                if full_test:
                    dsR = norm(s*R - pm.proj)

                    if not oblique:
                        self.assertTrue(dsR <= 1e-12,
                            msg="We should have got reconstructed rotation+scaling "
                                "perfectly. Now got d scale*R=%g" % dsR)

                        self.assertTrue(np.abs(s - pm._scale) < 1e-12,
                            msg="We should have got reconstructed scale "
                                "perfectly. Now got %g for %g" % (pm._scale, s))

                    self.assertTrue(ndsf <= 1e-12,
                      msg="%s: Failed to get to the target space correctly."
                        " normed error=%g" % (sdim, ndsf))

                # Test if we get back
                d_s_f_r = pm.reverse(d_s_f)

                dsfr = d_s_f_r - d_s
                ndsfr = norm(dsfr)/norm(d_s)
                if full_test:
                    self.assertTrue(ndsfr <= 1e-12,
                      msg="%s: Failed to reconstruct into source space correctly."
                        " normed error=%g" % (sdim, ndsfr))



def suite():  # pragma: no cover
    return unittest.makeSuite(ProcrusteanMapperTests)

if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_progress
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA SplittingSensitivityAnalyzer"""

import numpy as np
import time

from mvpa2.testing import *
from mvpa2.base.progress import ProgressBar


class ProgressTests(unittest.TestCase):
    def test_progress(self):

        pre = '+0:00:02 ===='
        post = '===________ -0:00:01  <msg>'

        for show in [True, False]:
            p = ProgressBar(progress_bar_width=20,
                            show_percentage=show)
            t = time.time()
            p.start(t)

            while time.time() < t + 2:
                pass

            s = p(.6, '<msg>')

            infix = '[60%]' if show else '====='
            assert_equal(s, pre + infix + post)


def suite():  # pragma: no cover
    return unittest.makeSuite(SensitivityAnalysersTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_prototypemapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA prototype mapper."""


import unittest
import numpy as np
from mvpa2.mappers.prototype import PrototypeMapper
from mvpa2.kernels.np import ExponentialKernel, SquaredExponentialKernel

from mvpa2.datasets import Dataset
from mvpa2.clfs.similarity import StreamlineSimilarity
from mvpa2.clfs.distance import corouge

from mvpa2.testing.tools import assert_array_equal, assert_array_almost_equal, \
     reseed_rng

import random

if __debug__:
    from mvpa2.base import debug

class PrototypeMapperTests(unittest.TestCase):

    def setUp(self):
        pass

    ##REF: Name was automagically refactored
    @reseed_rng()
    def build_vector_based_pm(self):
        # samples: 40 samples in 20d space (40x20; samples x features)
        self.samples = np.random.rand(40,20)

        # initial prototypes are samples itself:
        self.prototypes = self.samples.copy()

        # using just two similarities for now:
        self.similarities = [ExponentialKernel(), SquaredExponentialKernel()]
        # set up prototype mapper with prototypes identical to samples.
        self.pm = PrototypeMapper(similarities=self.similarities,
                                  prototypes=self.prototypes)
        # train Prototype
        self.pm.train(self.samples)


    def test_size(self):
        self.build_vector_based_pm()
        assert_array_equal(self.pm.proj.shape,
                           (self.samples.shape[0],
                            self.prototypes.shape[0] * len(self.similarities)))


    def test_symmetry(self):
        self.build_vector_based_pm()
        assert_array_almost_equal(self.pm.proj[:,self.samples.shape[0]],
                                  self.pm.proj.T[self.samples.shape[0],:])
        assert_array_equal(self.pm.proj[:,self.samples.shape[0]],
                           self.pm.proj.T[self.samples.shape[0],:])


    def test_size_random_prototypes(self):
        self.build_vector_based_pm()
        fraction = 0.5
        prototype_number = max(int(len(self.samples)*fraction),1)
        ## debug("MAP","Generating "+str(prototype_number)+" random prototypes.")
        self.prototypes2 = np.array(random.sample(list(self.samples), prototype_number))
        self.pm2 = PrototypeMapper(similarities=self.similarities, prototypes=self.prototypes2)
        self.pm2.train(self.samples)
        assert_array_equal(self.pm2.proj.shape, (self.samples.shape[0], self.pm2.prototypes.shape[0]*len(self.similarities)))

    # 2-nd portion of tests using a Dataset of streamlines

    @reseed_rng()
    def build_streamline_things(self):
        # Build a dataset having samples of different lengths. This is
        # trying to mimic a possible interface for streamlines
        # datasets, i.e., an iterable container of Mx3 points, where M
        # depends on each single streamline.

        # trying to pack it into an 'object' array to prevent conversion in the
        # Dataset
        self.streamline_samples = np.array([
                                   np.random.rand(3,3),
                                   np.random.rand(5,3),
                                   np.random.rand(7,3)],
                                   dtype='object')
        self.dataset = Dataset(self.streamline_samples)
        self.similarities = [StreamlineSimilarity(distance=corouge)]


    def test_streamline_equal_mapper(self):
        self.build_streamline_things()

        self.prototypes_equal = self.dataset.samples
        self.pm = PrototypeMapper(similarities=self.similarities,
                                  prototypes=self.prototypes_equal,
                                  demean=False)
        self.pm.train(self.dataset.samples)
        ## debug("MAP","projected data: "+str(self.pm.proj))
        # check size:
        assert_array_equal(self.pm.proj.shape, (len(self.dataset.samples), len(self.prototypes_equal)*len(self.similarities)))
        # test symmetry
        assert_array_almost_equal(self.pm.proj, self.pm.proj.T)


    def test_streamline_random_mapper(self):
        self.build_streamline_things()

        # Adding one more similarity to test multiple similarities in the streamline case:
        self.similarities.append(StreamlineSimilarity(distance=corouge))

        fraction = 0.5
        prototype_number = max(int(len(self.dataset.samples)*fraction),1)
        ## debug("MAP","Generating "+str(prototype_number)+" random prototypes.")
        self.prototypes_random = self.dataset.samples[np.random.permutation(self.dataset.samples.size)][:prototype_number]
        ## debug("MAP","prototypes: "+str(self.prototypes_random))

        self.pm = PrototypeMapper(similarities=self.similarities, prototypes=self.prototypes_random, demean=False)
        self.pm.train(self.dataset.samples) # , fraction=1.0)
        # test size:
        assert_array_equal(self.pm.proj.shape, (len(self.dataset.samples), len(self.prototypes_random)*len(self.similarities)))


def suite():  # pragma: no cover
    return unittest.makeSuite(PrototypeMapperTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_regr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Regressions"""

from mvpa2.testing import *
from mvpa2.testing.clfs import *
from mvpa2.testing.datasets import dataset_wizard, datasets
from mvpa2.base import externals

from mvpa2.generators.partition import NFoldPartitioner, OddEvenPartitioner
from mvpa2.measures.base import CrossValidation

from mvpa2.clfs.meta import SplitClassifier
from mvpa2.misc.exceptions import UnknownStateError
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.mappers.fx import mean_sample


class RegressionsTests(unittest.TestCase):

    @sweepargs(ml=clfswh['regression_based'] + regrswh[:])
    def test_non_regressions(self, ml):
        """Test If binary regression-based  classifiers have proper tag
        """
        self.assertTrue(('binary' in ml.__tags__) != ml.__is_regression__,
            msg="Inconsistent tagging with binary and regression features"
                " detected in %s having %r" % (ml, ml.__tags__))

    @sweepargs(regr=regrswh['regression'])
    def test_regressions(self, regr):
        """Simple tests on regressions
        """
        if not externals.exists('scipy'):
            raise SkipTest
        else:
            from mvpa2.misc.errorfx import corr_error
        ds = datasets['chirp_linear']
        # we want numeric labels to maintain the previous behavior, especially
        # since we deal with regressions here
        ds.sa.targets = AttributeMap().to_numeric(ds.targets)

        cve = CrossValidation(regr, NFoldPartitioner(), postproc=mean_sample(),
            errorfx=corr_error, enable_ca=['training_stats', 'stats'])
        # check the default
        #self.assertTrue(cve.transerror.errorfx is corr_error)

        corr = np.asscalar(cve(ds).samples)

        # Our CorrErrorFx should never return NaN
        self.assertTrue(not np.isnan(corr))
        self.assertTrue(corr == cve.ca.stats.stats['CCe'])

        splitregr = SplitClassifier(
            regr, partitioner=OddEvenPartitioner(),
            enable_ca=['training_stats', 'stats'])
        splitregr.train(ds)
        split_corr = splitregr.ca.stats.stats['CCe']
        split_corr_tr = splitregr.ca.training_stats.stats['CCe']

        for confusion, error in (
            (cve.ca.stats, corr),
            (splitregr.ca.stats, split_corr),
            (splitregr.ca.training_stats, split_corr_tr),
            ):
            #TODO: test confusion statistics
            # Part of it for now -- CCe
            for conf in confusion.summaries:
                stats = conf.stats
                if cfg.getboolean('tests', 'labile', default='yes'):
                    self.assertTrue(stats['CCe'] < 0.5)
                self.assertEqual(stats['CCe'], stats['Summary CCe'])

            s0 = confusion.as_string(short=True)
            s1 = confusion.as_string(short=False)

            for s in [s0, s1]:
                self.assertTrue(len(s) > 10,
                                msg="We should get some string representation "
                                "of regression summary. Got %s" % s)
            if cfg.getboolean('tests', 'labile', default='yes'):
                self.assertTrue(error < 0.2,
                            msg="Regressions should perform well on a simple "
                            "dataset. Got correlation error of %s " % error)

            # Test access to summary statistics
            # YOH: lets start making testing more reliable.
            #      p-value for such accident to have is verrrry tiny,
            #      so if regression works -- it better has at least 0.5 ;)
            #      otherwise fix it! ;)
            # YOH: not now -- issues with libsvr in SG and linear kernel
            if cfg.getboolean('tests', 'labile', default='yes'):
                self.assertTrue(confusion.stats['CCe'] < 0.5)

        # just to check if it works fine
        split_predictions = splitregr.predict(ds.samples)

        # To test basic plotting
        #import pylab as pl
        #cve.confusion.plot()
        #pl.show()

    @sweepargs(clf=clfswh['regression'])
    def test_regressions_classifiers(self, clf):
        """Simple tests on regressions being used as classifiers
        """
        # check if we get values set correctly
        clf.ca.change_temporarily(enable_ca=['estimates'])
        self.assertRaises(UnknownStateError, clf.ca['estimates']._get)
        cv = CrossValidation(clf, NFoldPartitioner(),
            enable_ca=['stats', 'training_stats'])
        ds = datasets['uni2small'].copy()
        # we want numeric labels to maintain the previous behavior, especially
        # since we deal with regressions here
        ds.sa.targets = AttributeMap().to_numeric(ds.targets)
        cverror = cv(ds)

        self.assertTrue(len(clf.ca.estimates) == ds[ds.chunks == 1].nsamples)
        clf.ca.reset_changed_temporarily()


    # yoh: Here LARS pukes on some seeds, e.g. MVPA_SEED=665157742
    # conditioned that test_regressions was ran.  I could not spot
    # anything "stateful" on our Python side, and I guess it is
    # a problem of R's implementation
    @sweepargs(regr=regrswh['regression', 'has_sensitivity', '!gpr', '!lars'])
    @reseed_rng()
    def test_sensitivities(self, regr):
        """Test "sensitivities" provided by regressions

        Inspired by a snippet leading to segfault from Daniel Kimberg

        lead to segfaults due to inappropriate access of SVs thinking
        that it is a classification problem (libsvm keeps SVs at None
        for those, although reports nr_class to be 2.
        """
        myds = dataset_wizard(samples=np.random.normal(size=(10,5)),
                              targets=np.random.normal(size=10))
        sa = regr.get_sensitivity_analyzer()
        #try:
        if True:
            res = sa(myds)
        #except Exception, e:
        #    self.fail('Failed to obtain a sensitivity due to %r' % (e,))
        self.assertTrue(res.shape == (1, myds.nfeatures))
        # TODO: extend the test -- checking for validity of sensitivities etc


def suite():  # pragma: no cover
    return unittest.makeSuite(RegressionsTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_report
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA simple report facility"""

import unittest, os, shutil

from mvpa2.base import verbose, externals

from mvpa2.base.report_dummy import Report as DummyReport
_test_classes = [ DummyReport ]

from mvpa2.testing import sweepargs, with_tempfile

if externals.exists('reportlab', raise_=False):
    from mvpa2.base.report import Report
    _test_classes += [ Report ]

if __debug__:
    from mvpa2.base import debug

class ReportTest(unittest.TestCase):
    """Just basic testing of reports -- pretty much that nothing fails
    """

    def setUp(self):
        # preserve handlers/level for verbose
        self.__oldverbosehandlers = verbose.handlers
        self.__oldverbose_level = verbose.level


    def tearDown(self):
        verbose.handlers = self.__oldverbosehandlers
        verbose.level = self.__oldverbose_level

    ##REF: Name was automagically refactored
    def aux_basic(self, dirname, rc):
        """Helper function -- to assure that all filehandlers
           get closed so we could remove trash directory.

           Otherwise -- .nfs* files on NFS-mounted drives cause problems
           """
        report = rc('UnitTest report',
                    title="Sample report for testing",
                    path=dirname)
        isdummy = isinstance(report, DummyReport)

        verbose.handlers = [report]
        verbose.level = 3
        verbose(1, "Starting")
        verbose(2, "Level 2")

        if not isdummy:
            self.assertTrue(len(report._story) == 2,
                            msg="We should have got some lines from verbose")

        if __debug__:
            odhandlers = debug.handlers
            debug.handlers = [report]
            oactive = debug.active
            debug.active = ['TEST'] + debug.active
            debug('TEST', "Testing report as handler for debug")
            if not isdummy:
                self.assertTrue(len(report._story) == 4,
                            msg="We should have got some lines from debug")
            debug.active = oactive
            debug.handlers = odhandlers

        os.makedirs(dirname)

        if externals.exists('pylab plottable'):
            if not isdummy:
                clen = len(report._story)
            import pylab as pl
            pl.ioff()
            pl.close('all')
            pl.figure()
            pl.plot([1, 2], [3, 2])

            pl.figure()
            pl.plot([2, 10], [3, 2])
            pl.title("Figure 2 must be it")
            report.figures()

            if not isdummy:
                self.assertTrue(
                    len(report._story) == clen+2,
                    msg="We should have got some lines from figures")

        report.text("Dugi bugi")
        # make sure we don't puke on xml like text with crap
        report.text("<kaj>$lkj&*()^$%#%</kaj>")
        report.text("locals:\n%s globals:\n%s" % (`locals()`, `globals()`))
        # bloody XML - just to check that there is no puke
        report.xml("<b>Dugi bugi</b>")
        report.save()

        if externals.exists('pylab'):
            import pylab as pl
            pl.close('all')
            pl.ion()

        pass


    @with_tempfile()
    @sweepargs(rc=_test_classes)
    def test_basic(self, dirname, rc):
        """Test all available reports, real or dummy for just working
        """
        self.aux_basic(dirname, rc)
        # cleanup
        shutil.rmtree(dirname, ignore_errors=True)


def suite():  # pragma: no cover
    return unittest.makeSuite(ReportTest)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_rfe
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA recursive feature elimination"""

import numpy as np

from mvpa2.generators.base import Repeater
from mvpa2.generators.partition import NFoldPartitioner, OddEvenPartitioner
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.generators.splitters import Splitter
from mvpa2.datasets.base import Dataset
from mvpa2.mappers.base import ChainMapper
from mvpa2.mappers.fx import maxofabs_sample, mean_sample, BinaryFxNode, FxMapper
from mvpa2.misc.errorfx import mean_mismatch_error
from mvpa2.misc.transformers import l2_normed
from mvpa2.misc.data_generators import normal_feature_dataset
from mvpa2.featsel.rfe import RFE
from mvpa2.featsel.base import \
     SensitivityBasedFeatureSelection
from mvpa2.featsel.helpers import \
     NBackHistoryStopCrit, FractionTailSelector, FixedErrorThresholdStopCrit, \
     MultiStopCrit, NStepsStopCrit, \
     FixedNElementTailSelector, BestDetector, RangeElementSelector

from mvpa2.clfs.meta import FeatureSelectionClassifier, SplitClassifier
from mvpa2.clfs.transerror import ConfusionBasedError
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.clfs.stats import MCNullDist
from mvpa2.measures.base import ProxyMeasure, CrossValidation

from mvpa2.base.state import UnknownStateError

from mvpa2.testing import *
from mvpa2.testing.clfs import *
from mvpa2.testing.tools import reseed_rng
from mvpa2.testing.datasets import datasets


class RFETests(unittest.TestCase):
    def get_data(self):
        return datasets['uni2medium']


    def test_best_detector(self):
        bd = BestDetector()

        # for empty history -- no best
        self.assertTrue(bd([]) == False)
        # we got the best if we have just 1
        self.assertTrue(bd([1]) == True)
        # we got the best if we have the last minimal
        self.assertTrue(bd([1, 0.9, 0.8]) == True)

        # test for alternative func
        bd = BestDetector(func=max)
        self.assertTrue(bd([0.8, 0.9, 1.0]) == True)
        self.assertTrue(bd([0.8, 0.9, 1.0]+[0.9]*9) == False)
        self.assertTrue(bd([0.8, 0.9, 1.0]+[0.9]*10) == False)

        # test to detect earliest and latest minimum
        bd = BestDetector(lastminimum=True)
        self.assertTrue(bd([3, 2, 1, 1, 1, 2, 1]) == True)
        bd = BestDetector()
        self.assertTrue(bd([3, 2, 1, 1, 1, 2, 1]) == False)


    def test_n_back_history_stop_crit(self):
        """Test stopping criterion"""
        stopcrit = NBackHistoryStopCrit()
        # for empty history -- no best but just go
        self.assertTrue(stopcrit([]) == False)
        # should not stop if we got 10 more after minimal
        self.assertTrue(stopcrit(
            [1, 0.9, 0.8]+[0.9]*(stopcrit.steps-1)) == False)
        # should stop if we got 10 more after minimal
        self.assertTrue(stopcrit(
            [1, 0.9, 0.8]+[0.9]*stopcrit.steps) == True)

        # test for alternative func
        stopcrit = NBackHistoryStopCrit(BestDetector(func=max))
        self.assertTrue(stopcrit([0.8, 0.9, 1.0]+[0.9]*9) == False)
        self.assertTrue(stopcrit([0.8, 0.9, 1.0]+[0.9]*10) == True)

        # test to detect earliest and latest minimum
        stopcrit = NBackHistoryStopCrit(BestDetector(lastminimum=True))
        self.assertTrue(stopcrit([3, 2, 1, 1, 1, 2, 1]) == False)
        stopcrit = NBackHistoryStopCrit(steps=4)
        self.assertTrue(stopcrit([3, 2, 1, 1, 1, 2, 1]) == True)


    def test_fixed_error_threshold_stop_crit(self):
        """Test stopping criterion"""
        stopcrit = FixedErrorThresholdStopCrit(0.5)

        self.assertTrue(stopcrit([]) == False)
        self.assertTrue(stopcrit([0.8, 0.9, 0.5]) == False)
        self.assertTrue(stopcrit([0.8, 0.9, 0.4]) == True)
        # only last error has to be below to stop
        self.assertTrue(stopcrit([0.8, 0.4, 0.6]) == False)


    def test_n_steps_stop_crit(self):
        """Test stopping criterion"""
        stopcrit = NStepsStopCrit(2)

        self.assertTrue(stopcrit([]) == False)
        self.assertTrue(stopcrit([0.8, 0.9]) == True)
        self.assertTrue(stopcrit([0.8]) == False)


    def test_multi_stop_crit(self):
        """Test multiple stop criteria"""
        stopcrit = MultiStopCrit([FixedErrorThresholdStopCrit(0.5),
                                  NBackHistoryStopCrit(steps=4)])

        # default 'or' mode
        # nback triggers
        self.assertTrue(stopcrit([1, 0.9, 0.8]+[0.9]*4) == True)
        # threshold triggers
        self.assertTrue(stopcrit([1, 0.9, 0.2]) == True)

        # alternative 'and' mode
        stopcrit = MultiStopCrit([FixedErrorThresholdStopCrit(0.5),
                                  NBackHistoryStopCrit(steps=4)],
                                 mode = 'and')
        # nback triggers not
        self.assertTrue(stopcrit([1, 0.9, 0.8]+[0.9]*4) == False)
        # threshold triggers not
        self.assertTrue(stopcrit([1, 0.9, 0.2]) == False)
        # only both satisfy
        self.assertTrue(stopcrit([1, 0.9, 0.4]+[0.4]*4) == True)


    def test_feature_selector(self):
        """Test feature selector"""
        # remove 10% weekest
        selector = FractionTailSelector(0.1)
        data = np.array([3.5, 10, 7, 5, -0.4, 0, 0, 2, 10, 9])
        # == rank [4, 5, 6, 7, 0, 3, 2, 9, 1, 8]
        target10 = np.array([0, 1, 2, 3, 5, 6, 7, 8, 9])
        target30 = np.array([0, 1, 2, 3, 7, 8, 9])

        self.assertRaises(UnknownStateError,
                              selector.ca.__getattribute__, 'ndiscarded')
        self.assertTrue((selector(data) == target10).all())
        selector.felements = 0.30      # discard 30%
        self.assertTrue(selector.felements == 0.3)
        self.assertTrue((selector(data) == target30).all())
        self.assertTrue(selector.ca.ndiscarded == 3) # se 3 were discarded

        selector = FixedNElementTailSelector(1)
        #                   0   1   2  3   4    5  6  7  8   9
        data = np.array([3.5, 10, 7, 5, -0.4, 0, 0, 2, 10, 9])
        self.assertTrue((selector(data) == target10).all())

        selector.nelements = 3
        self.assertTrue(selector.nelements == 3)
        self.assertTrue((selector(data) == target30).all())
        self.assertTrue(selector.ca.ndiscarded == 3)

        # test range selector
        # simple range 'above'
        self.assertTrue((RangeElementSelector(lower=0)(data) == \
                         np.array([0,1,2,3,7,8,9])).all())

        self.assertTrue((RangeElementSelector(lower=0,
                                              inclusive=True)(data) == \
                         np.array([0,1,2,3,5,6,7,8,9])).all())

        self.assertTrue((RangeElementSelector(lower=0, mode='discard',
                                              inclusive=True)(data) == \
                         np.array([4])).all())

        # simple range 'below'
        self.assertTrue((RangeElementSelector(upper=2)(data) == \
                         np.array([4,5,6])).all())

        self.assertTrue((RangeElementSelector(upper=2,
                                              inclusive=True)(data) == \
                         np.array([4,5,6,7])).all())

        self.assertTrue((RangeElementSelector(upper=2, mode='discard',
                                              inclusive=True)(data) == \
                         np.array([0,1,2,3,8,9])).all())


        # ranges
        self.assertTrue((RangeElementSelector(lower=2, upper=9)(data) == \
                         np.array([0,2,3])).all())

        self.assertTrue((RangeElementSelector(lower=2, upper=9,
                                              inclusive=True)(data) == \
                         np.array([0,2,3,7,9])).all())

        self.assertTrue((RangeElementSelector(upper=2, lower=9, mode='discard',
                                              inclusive=True)(data) ==
                         RangeElementSelector(lower=2, upper=9,
                                              inclusive=False)(data)).all())

        # non-0 elements -- should be equivalent to np.nonzero()[0]
        self.assertTrue((RangeElementSelector()(data) == \
                         np.nonzero(data)[0]).all())


    # XXX put GPR back in after it gets fixed up
    @sweepargs(clf=clfswh['has_sensitivity', '!meta', '!gpr'])
    def test_sensitivity_based_feature_selection(self, clf):

        # sensitivity analyser and transfer error quantifier use the SAME clf!
        sens_ana = clf.get_sensitivity_analyzer(postproc=maxofabs_sample())

        # of features to remove
        Nremove = 2

        # because the clf is already trained when computing the sensitivity
        # map, prevent retraining for transfer error calculation
        # Use absolute of the svm weights as sensitivity
        fe = SensitivityBasedFeatureSelection(sens_ana,
                feature_selector=FixedNElementTailSelector(2),
                enable_ca=["sensitivity", "selected_ids"])

        data = self.get_data()

        data_nfeatures = data.nfeatures

        fe.train(data)
        resds = fe(data)

        # fail if orig datasets are changed
        self.assertTrue(data.nfeatures == data_nfeatures)

        # silly check if nfeatures got a single one removed
        self.assertEqual(data.nfeatures, resds.nfeatures+Nremove,
            msg="We had to remove just a single feature")

        self.assertEqual(fe.ca.sensitivity.nfeatures, data_nfeatures,
            msg="Sensitivity have to have # of features equal to original")



    def test_feature_selection_pipeline(self):
        sens_ana = SillySensitivityAnalyzer()

        data = self.get_data()
        data_nfeatures = data.nfeatures

        # test silly one first ;-)
        self.assertEqual(sens_ana(data).samples[0,0], -int(data_nfeatures/2))

        # OLD: first remove 25% == 6, and then 4, total removing 10
        # NOW: test should be independent of the numerical number of features
        feature_selections = [SensitivityBasedFeatureSelection(
                                sens_ana,
                                FractionTailSelector(0.25)),
                              SensitivityBasedFeatureSelection(
                                sens_ana,
                                FixedNElementTailSelector(4))
                              ]

        # create a FeatureSelection pipeline
        feat_sel_pipeline = ChainMapper(feature_selections)

        feat_sel_pipeline.train(data)
        resds = feat_sel_pipeline(data)

        self.assertEqual(len(feat_sel_pipeline),
                             len(feature_selections),
                             msg="Test the property feature_selections")

        desired_nfeatures = int(np.ceil(data_nfeatures*0.75))
        self.assertEqual([fe._oshape[0] for fe in feat_sel_pipeline],
                             [desired_nfeatures, desired_nfeatures - 4])


    # TODO: should later on work for any clfs_with_sens
    @sweepargs(clf=clfswh['has_sensitivity', '!meta'][:1])
    @reseed_rng()
    def test_rfe(self, clf):

        # sensitivity analyser and transfer error quantifier use the SAME clf!
        sens_ana = clf.get_sensitivity_analyzer(postproc=maxofabs_sample())
        pmeasure = ProxyMeasure(clf, postproc=BinaryFxNode(mean_mismatch_error,
                                                           'targets'))
        cvmeasure = CrossValidation(clf, NFoldPartitioner(),
                                    errorfx=mean_mismatch_error,
                                    postproc=mean_sample())

        rfesvm_split = SplitClassifier(clf, OddEvenPartitioner())

        # explore few recipes
        for rfe, data in [
            # because the clf is already trained when computing the sensitivity
            # map, prevent retraining for transfer error calculation
            # Use absolute of the svm weights as sensitivity
            (RFE(sens_ana,
                pmeasure,
                Splitter('train'),
                fselector=FixedNElementTailSelector(1),
                train_pmeasure=False),
             self.get_data()),
            # use cross-validation within training to get error for the stopping point
            # but use full training data to derive sensitivity
            (RFE(sens_ana,
                 cvmeasure,
                 Repeater(2),            # give the same full dataset to sens_ana and cvmeasure
                 fselector=FractionTailSelector(
                     0.70,
                     mode='select', tail='upper'),
                train_pmeasure=True),
             normal_feature_dataset(perlabel=20, nchunks=5, nfeatures=200,
                                    nonbogus_features=[0, 1], snr=1.5)),
            # use cross-validation (via SplitClassifier) and get mean
            # of normed sensitivities across those splits
            (RFE(rfesvm_split.get_sensitivity_analyzer(
                    postproc=ChainMapper([ FxMapper('features', l2_normed),
                                           FxMapper('samples', np.mean),
                                           FxMapper('samples', np.abs)])),
                 ConfusionBasedError(rfesvm_split, confusion_state='stats'),
                 Repeater(2),             #  we will use the same full cv-training dataset
                 fselector=FractionTailSelector(
                     0.50,
                     mode='select', tail='upper'),
                 stopping_criterion=NBackHistoryStopCrit(BestDetector(), 10),
                 train_pmeasure=False,    # we just extract it from existing confusion
                 update_sensitivity=True),
             normal_feature_dataset(perlabel=28, nchunks=7, nfeatures=200,
                                    nonbogus_features=[0, 1], snr=1.5))
            ]:
            # prep data
            # data = datasets['uni2medium']
            data_nfeatures = data.nfeatures

            rfe.train(data)
            resds = rfe(data)

            # fail if orig datasets are changed
            self.assertTrue(data.nfeatures == data_nfeatures)

            # check that the features set with the least error is selected
            if len(rfe.ca.errors):
                e = np.array(rfe.ca.errors)
                if isinstance(rfe._fselector, FixedNElementTailSelector):
                    self.assertTrue(resds.nfeatures == data_nfeatures - e.argmin())
                else:
                    imin = np.argmin(e)
                    if 'does_feature_selection' in clf.__tags__:
                        # if clf is smart it might figure it out right away
                        assert_array_less( imin, len(e) )
                    else:
                        # in this case we can even check if we had actual
                        # going down/up trend... although -- why up???
                        self.assertTrue( 1 < imin < len(e) - 1 )
            else:
                self.assertTrue(resds.nfeatures == data_nfeatures)

            # silly check if nfeatures is in decreasing order
            nfeatures = np.array(rfe.ca.nfeatures).copy()
            nfeatures.sort()
            self.assertTrue( (nfeatures[::-1] == rfe.ca.nfeatures).all() )

            # check if history has elements for every step
            self.assertTrue(set(rfe.ca.history)
                            == set(range(len(np.array(rfe.ca.errors)))))

            # Last (the largest number) can be present multiple times even
            # if we remove 1 feature at a time -- just need to stop well
            # in advance when we have more than 1 feature left ;)
            self.assertTrue(rfe.ca.nfeatures[-1]
                            == len(np.where(rfe.ca.history
                                           ==max(rfe.ca.history))[0]))

            # XXX add a test where sensitivity analyser and transfer error do not
            # use the same classifier


    def test_james_problem(self):
        percent = 80
        dataset = datasets['uni2small']
        rfesvm_split = LinearCSVMC()
        fs = \
            RFE(rfesvm_split.get_sensitivity_analyzer(),
                ProxyMeasure(rfesvm_split,
                             postproc=BinaryFxNode(mean_mismatch_error,
                                                   'targets')),
                Splitter('train'),
                fselector=FractionTailSelector(
                    percent / 100.0,
                    mode='select', tail='upper'), update_sensitivity=True)

        clf = FeatureSelectionClassifier(
            LinearCSVMC(),
            # on features selected via RFE
            fs)
             # update sensitivity at each step (since we're not using the
             # same CLF as sensitivity analyzer)

        class StoreResults(object):
            def __init__(self):
                self.storage = []
            def __call__(self, data, node, result):
                self.storage.append((node.measure.mapper.ca.history,
                                     node.measure.mapper.ca.errors)),

        cv_storage = StoreResults()
        cv = CrossValidation(clf, NFoldPartitioner(), postproc=mean_sample(),
                             callback=cv_storage,
                             enable_ca=['confusion']) # TODO -- it is stats
        #cv = SplitClassifier(clf)
        try:
            error = cv(dataset).samples.squeeze()
        except Exception, e:
            self.fail('CrossValidation cannot handle classifier with RFE '
                      'feature selection. Got exception: %s' % (e,))

        assert(len(cv_storage.storage) == len(dataset.sa['chunks'].unique))
        assert(len(cv_storage.storage[0]) == 2)
        assert(len(cv_storage.storage[0][0]) == dataset.nfeatures)

        self.assertTrue(error < 0.2)


    def test_james_problem_multiclass(self):
        percent = 80
        dataset = datasets['uni4large']
        #dataset = dataset[:, dataset.a.nonbogus_features]

        rfesvm_split = LinearCSVMC()
        fs = \
            RFE(rfesvm_split.get_sensitivity_analyzer(
            postproc=ChainMapper([
                #FxMapper('features', l2_normed),
                #FxMapper('samples', np.mean),
                #FxMapper('samples', np.abs)
                FxMapper('features', lambda x: np.argsort(np.abs(x))),
                #maxofabs_sample()
                mean_sample()
                ])),
                ProxyMeasure(rfesvm_split,
                             postproc=BinaryFxNode(mean_mismatch_error,
                                                   'targets')),
                Splitter('train'),
                fselector=FractionTailSelector(
                    percent / 100.0,
                    mode='select', tail='upper'), update_sensitivity=True)

        clf = FeatureSelectionClassifier(
            LinearCSVMC(),
            # on features selected via RFE
            fs)
             # update sensitivity at each step (since we're not using the
             # same CLF as sensitivity analyzer)

        class StoreResults(object):
            def __init__(self):
                self.storage = []
            def __call__(self, data, node, result):
                self.storage.append((node.measure.mapper.ca.history,
                                     node.measure.mapper.ca.errors)),

        cv_storage = StoreResults()
        cv = CrossValidation(clf, NFoldPartitioner(), postproc=mean_sample(),
                             callback=cv_storage,
                             enable_ca=['stats'])
        #cv = SplitClassifier(clf)
        try:
            error = cv(dataset).samples.squeeze()
        except Exception, e:
            self.fail('CrossValidation cannot handle classifier with RFE '
                      'feature selection. Got exception: %s' % (e,))
        #print "ERROR: ", error
        #print cv.ca.stats
        assert(len(cv_storage.storage) == len(dataset.sa['chunks'].unique))
        assert(len(cv_storage.storage[0]) == 2)
        assert(len(cv_storage.storage[0][0]) == dataset.nfeatures)
        #print "non bogus features",  dataset.a.nonbogus_features
        #print cv_storage.storage

        self.assertTrue(error < 0.2)


    ##REF: Name was automagically refactored
    def __test_matthias_question(self):
        rfe_clf = LinearCSVMC(C=1)

        rfesvm_split = SplitClassifier(rfe_clf)
        clf = \
            FeatureSelectionClassifier(
            clf = LinearCSVMC(C=1),
            feature_selection = RFE(
                sensitivity_analyzer = rfesvm_split.get_sensitivity_analyzer(
                    combiner=first_axis_mean,
                    transformer=np.abs),
                transfer_error=ConfusionBasedError(
                    rfesvm_split,
                    confusion_state="confusion"),
                stopping_criterion=FixedErrorThresholdStopCrit(0.20),
                feature_selector=FractionTailSelector(
                    0.2, mode='discard', tail='lower'),
                update_sensitivity=True))

        no_permutations = 1000
        permutator = AttributePermutator('targets', count=no_permutations)
        cv = CrossValidation(clf, NFoldPartitioner(),
            null_dist=MCNullDist(permutator, tail='left'),
            enable_ca=['stats'])
        error = cv(datasets['uni2small'])
        self.assertTrue(error < 0.4)
        self.assertTrue(cv.ca.null_prob < 0.05)

    @reseed_rng()
    @labile(3, 1)
    def test_SplitRFE(self):
        # just a smoke test ATM
        from mvpa2.clfs.svm import LinearCSVMC
        from mvpa2.clfs.meta import MappedClassifier
        from mvpa2.misc.data_generators import normal_feature_dataset
        #import mvpa2.featsel.rfe
        #reload(mvpa2.featsel.rfe)
        from mvpa2.featsel.rfe import RFE, SplitRFE
        from mvpa2.generators.partition import NFoldPartitioner
        from mvpa2.featsel.helpers import FractionTailSelector
        from mvpa2.testing import ok_, assert_equal

        clf = LinearCSVMC(C=1)
        dataset = normal_feature_dataset(perlabel=20, nlabels=2, nfeatures=30,
                                         snr=1., nonbogus_features=[1,5])
        # flip one of the meaningful features around to see
        # if we are still getting proper selection
        dataset.samples[:, dataset.a.nonbogus_features[1]] *= -1
        # 4 partitions should be enough for testing
        partitioner = NFoldPartitioner(count=4)

        rfeclf = MappedClassifier(
            clf, SplitRFE(clf,
                          partitioner,
                          fselector=FractionTailSelector(
                              0.2, mode='discard', tail='lower')))
        r0 = repr(rfeclf)

        ok_(rfeclf.mapper.nfeatures_min == 0)
        rfeclf.train(dataset)
        ok_(rfeclf.mapper.nfeatures_min > 0)
        predictions = rfeclf(dataset).samples

        # at least 1 of the nonbogus-features should be chosen
        ok_(len(set(dataset.a.nonbogus_features).intersection(
                rfeclf.mapper.slicearg)) > 0)
        # check repr to have all needed pieces
        r = repr(rfeclf)
        s = str(rfeclf)
        ok_(('partitioner=NFoldP' in r) or
            ('partitioner=mvpa2.generators.partition.NFoldPartitioner' in r))
        ok_('lrn=' in r)
        ok_(not 'slicearg=' in r)
        assert_equal(r, r0)

def suite():  # pragma: no cover
    return unittest.makeSuite(RFETests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_ridge
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA ridge regression classifier"""

from mvpa2.clfs.ridge import RidgeReg
from mvpa2.testing import *
from mvpa2.testing.datasets import datasets

skip_if_no_external('scipy')

from scipy.stats import pearsonr

class RidgeRegTests(unittest.TestCase):

    def test_ridge_reg(self):
        # not the perfect dataset with which to test, but
        # it will do for now.
        data = datasets['dumb']

        clf = RidgeReg()

        clf.train(data)

        # prediction has to be almost perfect
        # test with a correlation
        pre = clf.predict(data.samples)
        cor = pearsonr(pre,data.targets)
        self.assertTrue(cor[0] > .8)

        # do again for fortran implementation
        # DISABLE for now, at it is known to be broken
#        clf = RidgeReg(implementation='gradient')
#        clf.train(data)
#        cor = pearsonr(clf.predict(data.samples), data.targets)
#        print cor
#        self.assertTrue(cor[0] > .8)



    def test_ridge_reg_state(self):
        data = datasets['dumb']

        clf = RidgeReg()

        clf.train(data)

        clf.ca.enable('predictions')

        p = clf.predict(data.samples)

        self.assertTrue((p == clf.ca.predictions).all())


def suite():  # pragma: no cover
    return unittest.makeSuite(RidgeRegTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_rsa
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for rsa measures"""

from mvpa2.testing import *
skip_if_no_external('scipy')

from mvpa2.testing.datasets import datasets
from mvpa2.measures.anova import OneWayAnova

import numpy as np
from mvpa2.mappers.fx import *
from mvpa2.datasets.base import dataset_wizard, Dataset

from mvpa2.testing.tools import *

from mvpa2.measures.rsa import *
from mvpa2.base import externals
import scipy.stats as stats
from scipy.spatial.distance import pdist, squareform
from scipy.stats import rankdata, pearsonr

data = np.array([[ 0.22366105, 0.51562476, 0.62623543, 0.28081652, 0.56513533],
                [ 0.22077129, 0.63013374, 0.19641318, 0.38466208, 0.60788347],
                [ 0.64273055, 0.60455658, 0.71368501, 0.36652763, 0.51720253],
                [ 0.40148338, 0.34188668, 0.09174233, 0.33906488, 0.17804584],
                [ 0.60728718, 0.6110304 , 0.84817742, 0.33830628, 0.7123945 ],
                [ 0.32113428, 0.16916899, 0.53471886, 0.93321617, 0.22531679]])


def test_PDistConsistency():
    targets = np.tile(xrange(3),2)
    chunks = np.repeat(np.array((0,1)),3)
    # correct results
    cres1 = 0.41894348
    cres2 = np.array([[ 0.73062639, 0.16137995, 0.59441713]]).T
    dc1 = data[0:3,:] - np.mean(data[0:3,:],0)
    dc2 = data[3:6,:] - np.mean(data[3:6,:],0)
    center = squareform(np.corrcoef(pdist(dc1,'correlation'),pdist(dc2,'correlation')), 
                        checks=False).reshape((1,-1))
    dsm1 = stats.rankdata(pdist(data[0:3,:],'correlation').reshape((1,-1)))
    dsm2 = stats.rankdata(pdist(data[3:6,:],'correlation').reshape((1,-1)))

    spearman = squareform(np.corrcoef(np.vstack((dsm1,dsm2))), 
                        checks=False).reshape((1,-1))
    
    ds = dataset_wizard(samples=data, targets=targets, chunks=chunks)
    dscm = PDistConsistency()
    res1 = dscm(ds)
    dscm_c = PDistConsistency(center_data=True)
    res2 = dscm_c(ds)
    dscm_sp = PDistConsistency(consistency_metric='spearman')
    res3 = dscm_sp(ds)
    ds.append(ds)
    chunks = np.repeat(['one', 'two', 'three'], 4)
    ds.sa['chunks'] = chunks
    res4 = dscm(ds)
    dscm_sq = PDistConsistency(square=True)
    res4_sq = dscm_sq(ds)
    for i, p in enumerate(res4.sa.pairs):
        sqval =  np.asscalar(res4_sq[res4_sq.sa.chunks == p[0],
                                     res4_sq.sa.chunks == p[1]])
        assert_equal(sqval, res4.samples[i, 0])
    assert_almost_equal(np.mean(res1.samples),cres1)
    assert_array_almost_equal(res2.samples, center)
    assert_array_almost_equal(res3.samples, spearman)
    assert_array_almost_equal(res4.samples,cres2)



def test_PDist():
    targets = np.tile(xrange(3),2)
    chunks = np.repeat(np.array((0,1)),3)
    ds = dataset_wizard(samples=data, targets=targets, chunks=chunks)
    data_c = data - np.mean(data,0)
    # DSM matrix elements should come out as samples of one feature
    # to be in line with what e.g. a classifier returns -- facilitates
    # collection in a searchlight ...
    euc = pdist(data, 'euclidean')[None].T
    pear = pdist(data, 'correlation')[None].T
    city = pdist(data, 'cityblock')[None].T
    center_sq = squareform(pdist(data_c,'correlation'))

    # Now center each chunk separately
    dsm1 = PDist()
    dsm2 = PDist(pairwise_metric='euclidean')
    dsm3 = PDist(pairwise_metric='cityblock')
    dsm4 = PDist(center_data=True,square=True)
    assert_array_almost_equal(dsm1(ds).samples,pear)
    assert_array_almost_equal(dsm2(ds).samples,euc)
    dsm_res = dsm3(ds)
    assert_array_almost_equal(dsm_res.samples,city)
    # length correspondings to a single triangular matrix
    assert_equal(len(dsm_res.sa.pairs), len(ds) * (len(ds) - 1) / 2)
    # generate label pairs actually reflect the vectorform generated by
    # squareform()
    dsm_res_square = squareform(dsm_res.samples.T[0])
    for i, p in enumerate(dsm_res.sa.pairs):
        assert_equal(dsm_res_square[p[0], p[1]], dsm_res.samples[i, 0])
    dsm_res = dsm4(ds)
    assert_array_almost_equal(dsm_res.samples,center_sq)
    # sample attributes are carried over
    assert_almost_equal(ds.sa.targets, dsm_res.sa.targets)

def test_PDistTargetSimilarity():
    ds = Dataset(data)
    tdsm = range(15)
    ans1 = np.array([0.30956920104253222, 0.26152022709856804])
    ans2 = np.array([0.53882710751962437, 0.038217527859375197])
    ans3 = np.array([0.33571428571428574, 0.22121153763932569])
    tdcm1 = PDistTargetSimilarity(tdsm)
    tdcm2 = PDistTargetSimilarity(tdsm,
                                            pairwise_metric='euclidean')
    tdcm3 = PDistTargetSimilarity(tdsm,
                                comparison_metric = 'spearman')
    tdcm4 = PDistTargetSimilarity(tdsm,
                                    corrcoef_only=True)
    a1 = tdcm1(ds)
    a2 = tdcm2(ds)
    a3 = tdcm3(ds)
    a4 = tdcm4(ds)
    assert_array_almost_equal(a1.samples.squeeze(),ans1)
    assert_array_equal(a1.fa.metrics, ['rho', 'p'])
    assert_array_almost_equal(a2.samples.squeeze(),ans2)
    assert_array_equal(a2.fa.metrics, ['rho', 'p'])
    assert_array_almost_equal(a3.samples.squeeze(),ans3)
    assert_array_equal(a3.fa.metrics, ['rho', 'p'])
    assert_array_almost_equal(a4.samples.squeeze(),ans1[0])
    assert_array_equal(a4.fa.metrics, ['rho'])





########NEW FILE########
__FILENAME__ = test_searchlight
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA searchlight algorithm"""

import tempfile, time
import numpy.random as rnd

from math import ceil

import mvpa2
from mvpa2.testing import *
from mvpa2.testing.clfs import *
from mvpa2.testing.datasets import *

from mvpa2.datasets import Dataset, hstack
from mvpa2.base.types import is_datasetlike
from mvpa2.base import externals
from mvpa2.mappers.base import ChainMapper
from mvpa2.mappers.fx import mean_group_sample
from mvpa2.clfs.transerror import ConfusionMatrix
from mvpa2.measures.searchlight import sphere_searchlight, Searchlight
from mvpa2.measures.gnbsearchlight import sphere_gnbsearchlight, \
     GNBSearchlight
from mvpa2.clfs.gnb import GNB

from mvpa2.measures.nnsearchlight import sphere_m1nnsearchlight, \
     M1NNSearchlight
from mvpa2.clfs.knn import kNN

from mvpa2.misc.neighborhood import IndexQueryEngine, Sphere
from mvpa2.misc.errorfx import corr_error
from mvpa2.generators.partition import NFoldPartitioner, OddEvenPartitioner
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.measures.base import CrossValidation


class SearchlightTests(unittest.TestCase):

    def setUp(self):
        self.dataset = datasets['3dlarge']
        # give the feature coord a more common name, matching the default of
        # the searchlight
        self.dataset.fa['voxel_indices'] = self.dataset.fa.myspace
        self._tested_pprocess = False


    # https://github.com/PyMVPA/PyMVPA/issues/67
    # https://github.com/PyMVPA/PyMVPA/issues/69
    def test_gnbsearchlight_doc(self):
        # Test either we excluded nproc from the docstrings
        ok_(not 'nproc' in GNBSearchlight.__init__.__doc__)
        ok_(not 'nproc' in GNBSearchlight.__doc__)
        ok_(not 'nproc' in sphere_gnbsearchlight.__doc__)
        # but present elsewhere
        ok_('nproc' in sphere_searchlight.__doc__)
        ok_('nproc' in Searchlight.__init__.__doc__)

    # https://github.com/PyMVPA/PyMVPA/issues/106
    def test_searchlights_doc_qe(self):
        # queryengine should not be provided to sphere_* helpers
        for sl in (sphere_searchlight,
                   sphere_gnbsearchlight,
                   sphere_m1nnsearchlight):
            for kw in ('queryengine', 'qe'):
                ok_(not kw in sl.__doc__,
                    msg='There should be no %r in %s.__doc__' % (kw, sl))

        # queryengine should be provided in corresponding classes __doc__s
        for sl in (Searchlight,
                   GNBSearchlight,
                   M1NNSearchlight):
            for kw in ('queryengine',):
                ok_(kw in sl.__init__.__doc__,
                    msg='There should be %r in %s.__init__.__doc__' % (kw, sl))
            for kw in ('qe',):
                ok_(not kw in sl.__init__.__doc__,
                    msg='There should be no %r in %s.__init__.__doc__' % (kw, sl))



    #def _test_searchlights(self, ds, sls, roi_ids, result_all):  # pragma: no cover

    @sweepargs(lrn_sllrn_SL_partitioner=
               [(GNB(common_variance=v, descr='GNB'), None,
                 sphere_gnbsearchlight,
                 NFoldPartitioner(cvtype=1),
                 0.                       # correction for the error range
                 )
                 for v in (True, False)] +
               # Mean 1 NN searchlights
               [(ChainMapper(
                   [mean_group_sample(['targets', 'partitions']),
                    kNN(1)], space='targets', descr='M1NN'),
                 kNN(1),
                 sphere_m1nnsearchlight,
                 NFoldPartitioner(0.5, selection_strategy='random', count=20),
                 0.05),
                # the same but with NFold(1) partitioner since it still should work
                (ChainMapper(
                   [mean_group_sample(['targets', 'partitions']),
                    kNN(1)], space='targets', descr='NF-M1NN'),
                 kNN(1),
                 sphere_m1nnsearchlight,
                 NFoldPartitioner(1),
                 0.05),
                ]
               )
    @sweepargs(do_roi=(False, True))
    @sweepargs(results_backend=('native', 'hdf5'))
    @reseed_rng()
    def test_spatial_searchlight(self, lrn_sllrn_SL_partitioner, do_roi=False,
                                 results_backend='native'):
        """Tests both generic and ad-hoc searchlights (e.g. GNBSearchlight)
        Test of and adhoc searchlight anyways requires a ground-truth
        comparison to the generic version, so we are doing sweepargs here
        """
        lrn, sllrn, SL, partitioner, correction = lrn_sllrn_SL_partitioner
        ## if results_backend == 'hdf5' and not common_variance:
        ##     # no need for full combination of all possible arguments here
        ##     return

        if __debug__ and 'ENFORCE_CA_ENABLED' in debug.active \
           and  isinstance(lrn, ChainMapper):
            raise SkipTest("Known to fail while trying to enable "
                           "training_stats for the ChainMapper (M1NN here)")


        # e.g. for M1NN we need plain kNN(1) for m1nnsl, but to imitate m1nn
        #      "learner" we must use a chainmapper atm
        if sllrn is None:
            sllrn = lrn
        ds = datasets['3dsmall'].copy()
        # Let's test multiclass here, so boost # of labels
        ds[6:18].T += 2
        ds.fa['voxel_indices'] = ds.fa.myspace

        # To assure that users do not run into incorrect operation due to overflows
        ds.samples += 5000
        ds.samples *= 1000
        ds.samples = ds.samples.astype(np.int16)

        # compute N-1 cross-validation for each sphere
        # YOH: unfortunately sample_clf_lin is not guaranteed
        #      to provide exactly the same results due to inherent
        #      iterative process.  Therefore lets use something quick
        #      and pure Python
        cv = CrossValidation(lrn, partitioner)

        skwargs = dict(radius=1, enable_ca=['roi_sizes', 'raw_results',
                                            'roi_feature_ids'])

        if do_roi:
            # select some random set of features
            nroi = rnd.randint(1, ds.nfeatures)
            # and lets compute the full one as well once again so we have a reference
            # which will be excluded itself from comparisons but values will be compared
            # for selected roi_id
            sl_all = SL(sllrn, partitioner, **skwargs)
            result_all = sl_all(ds)
            # select random features
            roi_ids = rnd.permutation(range(ds.nfeatures))[:nroi]
            skwargs['center_ids'] = roi_ids
        else:
            nroi = ds.nfeatures
            roi_ids = np.arange(nroi)
            result_all = None

        if results_backend == 'hdf5':
            skip_if_no_external('h5py')

        sls = [sphere_searchlight(cv, results_backend=results_backend,
                                  **skwargs),
               #GNBSearchlight(gnb, NFoldPartitioner(cvtype=1))
               SL(sllrn, partitioner, indexsum='fancy', **skwargs)
               ]

        if externals.exists('scipy'):
            sls += [ SL(sllrn, partitioner, indexsum='sparse', **skwargs)]

        # Test nproc just once
        if externals.exists('pprocess') and not self._tested_pprocess:
            sls += [sphere_searchlight(cv, nproc=2, **skwargs)]
            self._tested_pprocess = True

        # Provide the dataset and all those searchlights for testing
        #self._test_searchlights(ds, sls, roi_ids, result_all)
        #nroi = len(roi_ids)
        #do_roi = nroi != ds.nfeatures
        all_results = []
        for sl in sls:
            # run searchlight
            mvpa2.seed()                # reseed rng again for m1nnsl
            results = sl(ds)
            all_results.append(results)
            #print `sl`
            # check for correct number of spheres
            self.assertTrue(results.nfeatures == nroi)
            # and measures (one per xfold)
            if partitioner.cvtype == 1:
                self.assertTrue(len(results) == len(ds.UC))
            elif partitioner.cvtype == 0.5:
                # here we had 4 unique chunks, so 6 combinations
                # even though 20 max was specified for NFold
                self.assertTrue(len(results) == 6)
            else:
                raise RuntimeError("Unknown yet type of partitioner to check")
            # check for chance-level performance across all spheres
            # makes sense only if number of features was big enough
            # to get some stable estimate of mean
            if not do_roi or nroi > 20:
                # correction here is for M1NN class which has wider distribution
                self.assertTrue(
                    0.67 - correction < results.samples.mean() < 0.85 + correction,
                    msg="Out of range mean result: "
                    "lrn: %s  sllrn: %s  NROI: %d  MEAN: %.3f"
                    % (lrn, sllrn, nroi, results.samples.mean(),))

            mean_errors = results.samples.mean(axis=0)
            # that we do get different errors ;)
            self.assertTrue(len(np.unique(mean_errors) > 3))

            # check resonable sphere sizes
            self.assertTrue(len(sl.ca.roi_sizes) == nroi)
            self.assertTrue(len(sl.ca.roi_feature_ids) == nroi)
            for i, fids in enumerate(sl.ca.roi_feature_ids):
                self.assertTrue(len(fids) == sl.ca.roi_sizes[i])
            if do_roi:
                # for roi we should relax conditions a bit
                self.assertTrue(max(sl.ca.roi_sizes) <= 7)
                self.assertTrue(min(sl.ca.roi_sizes) >= 4)
            else:
                self.assertTrue(max(sl.ca.roi_sizes) == 7)
                self.assertTrue(min(sl.ca.roi_sizes) == 4)

            # check base-class state
            self.assertEqual(sl.ca.raw_results.nfeatures, nroi)

            # Test if we got results correctly for 'selected' roi ids
            if do_roi:
                assert_array_equal(result_all[:, roi_ids], results)

        if len(all_results) > 1:
            # if we had multiple searchlights, we can check either they all
            # gave the same result (they should have)
            aresults = np.array([a.samples for a in all_results])
            dresults = np.abs(aresults - aresults.mean(axis=0))
            dmax = np.max(dresults)
            self.assertTrue(dmax <= 1e-13)

        # Test the searchlight's reuse of neighbors
        for indexsum in ['fancy'] + (
            externals.exists('scipy') and ['sparse'] or []):
            sl = SL(sllrn, partitioner, indexsum='fancy',
                    reuse_neighbors=True, **skwargs)
            mvpa2.seed()
            result1 = sl(ds)
            mvpa2.seed()
            result2 = sl(ds)                # must be faster
            assert_array_equal(result1, result2)

    def test_adhocsearchlight_perm_testing(self):
        # just a smoke test pretty much
        ds = datasets['3dmedium'].copy()
        #ds.samples += np.random.normal(size=ds.samples.shape)*10
        mvpa2.seed()
        ds.fa['voxel_indices'] = ds.fa.myspace
        from mvpa2.mappers.fx import mean_sample
        from mvpa2.clfs.stats import MCNullDist
        permutator = AttributePermutator('targets', count=8,
                                         limit='chunks')
        distr_est = MCNullDist(permutator, tail='left',
                               enable_ca=['dist_samples'])
        slargs = (kNN(1),
                  NFoldPartitioner(0.5,
                                   selection_strategy='random',
                                   count=9))
        slkwargs = dict(radius=1, postproc=mean_sample())

        sl_nodistr = sphere_m1nnsearchlight(*slargs, **slkwargs)
        skip_if_no_external('scipy')    # needed for null_t
        sl = sphere_m1nnsearchlight(
            *slargs,
            null_dist=distr_est,
            enable_ca=['null_t'],
            reuse_neighbors=True,
            **slkwargs
            )
        mvpa2.seed()
        res_nodistr = sl_nodistr(ds)
        mvpa2.seed()
        res = sl(ds)
        # verify that we at least got the same main result
        # ah (yoh) -- null dist is estimated before the main
        # estimate so we can't guarantee correspondence :-/
        # assert_array_equal(res_nodistr, res)
        # only resemblance (TODO, may be we want to get/setstate
        # for rng before null_dist.fit?)

        # and dimensions correspond
        assert_array_equal(distr_est.ca.dist_samples.shape,
                           (1, ds.nfeatures, 8))
        assert_array_equal(sl.ca.null_t.samples.shape,
                           (1, ds.nfeatures))

    def test_partial_searchlight_with_full_report(self):
        ds = self.dataset.copy()
        center_ids = np.zeros(ds.nfeatures, dtype='bool')
        center_ids[[3, 50]] = True
        ds.fa['center_ids'] = center_ids
        # compute N-1 cross-validation for each sphere
        cv = CrossValidation(GNB(), NFoldPartitioner())
        # contruct diameter 1 (or just radius 0) searchlight
        # one time give center ids as a list, the other one takes it from the
        # dataset itself
        sls = (sphere_searchlight(cv, radius=0, center_ids=[3, 50]),
               sphere_searchlight(None, radius=0, center_ids=[3, 50]),
               sphere_searchlight(cv, radius=0, center_ids='center_ids'),
               )
        for sl in sls:
            # assure that we could set cv post constructor
            if sl.datameasure is None:
                sl.datameasure = cv
            # run searchlight
            results = sl(ds)
            # only two spheres but error for all CV-folds
            self.assertEqual(results.shape, (len(self.dataset.UC), 2))
            # Test if results hold if we "set" a "new" datameasure
            sl.datameasure = CrossValidation(GNB(), NFoldPartitioner())
            results2 = sl(ds)
            assert_array_almost_equal(results, results2)

        # test if we graciously puke if center_ids are out of bounds
        dataset0 = ds[:, :50] # so we have no 50th feature
        self.assertRaises(IndexError, sls[0], dataset0)
        # but it should be fine on the one that gets the ids from the dataset
        # itself
        results = sl(dataset0)
        assert_equal(results.nfeatures, 1)
        # check whether roi_seeds are correct
        sl = sphere_searchlight(lambda x: np.vstack((x.fa.roi_seed, x.samples)),
                                radius=1, add_center_fa=True, center_ids=[12])
        res = sl(ds)
        assert_array_equal(res.samples[1:, res.samples[0].astype('bool')].squeeze(),
                           ds.samples[:, 12])


    def test_partial_searchlight_with_confusion_matrix(self):
        ds = self.dataset
        from mvpa2.clfs.stats import MCNullDist
        from mvpa2.mappers.fx import mean_sample, sum_sample

        # compute N-1 cross-validation for each sphere
        cm = ConfusionMatrix(labels=ds.UT)
        cv = CrossValidation(
            sample_clf_lin, NFoldPartitioner(),
            # we have to assure that matrix does not get flatted by
            # first vstack in cv and then hstack in searchlight --
            # thus 2 leading dimensions
            # TODO: RF? make searchlight/crossval smarter?
            errorfx=lambda *a: cm(*a)[None, None, :])
        # contruct diameter 2 (or just radius 1) searchlight
        sl = sphere_searchlight(cv, radius=1, center_ids=[3, 5, 50])

        # our regular searchlight -- to compare results
        cv_gross = CrossValidation(sample_clf_lin, NFoldPartitioner())
        sl_gross = sphere_searchlight(cv_gross, radius=1, center_ids=[3, 5, 50])

        # run searchlights
        res = sl(ds)
        res_gross = sl_gross(ds)

        # only two spheres but error for all CV-folds and complete confusion matrix
        assert_equal(res.shape, (len(ds.UC), 3, len(ds.UT), len(ds.UT)))
        assert_equal(res_gross.shape, (len(ds.UC), 3))

        # briefly inspect the confusion matrices
        mat = res.samples
        # since input dataset is probably balanced (otherwise adjust
        # to be per label): sum within columns (thus axis=-2) should
        # be identical to per-class/chunk number of samples
        samples_per_classchunk = len(ds) / (len(ds.UT) * len(ds.UC))
        ok_(np.all(np.sum(mat, axis= -2) == samples_per_classchunk))
        # and if we compute accuracies manually -- they should
        # correspond to the one from sl_gross
        assert_array_almost_equal(res_gross.samples,
                           # from accuracies to errors
                           1 - (mat[..., 0, 0] + mat[..., 1, 1]).astype(float)
                           / (2 * samples_per_classchunk))

        # and now for those who remained sited -- lets perform H0 MC
        # testing of this searchlight... just a silly one with minimal
        # number of permutations
        no_permutations = 10
        permutator = AttributePermutator('targets', count=no_permutations)

        # once again -- need explicit leading dimension to avoid
        # vstacking during cross-validation
        cv.postproc = lambda x: sum_sample()(x)[None, :]

        sl = sphere_searchlight(cv, radius=1, center_ids=[3, 5, 50],
                                null_dist=MCNullDist(permutator, tail='right',
                                                     enable_ca=['dist_samples']))
        res_perm = sl(ds)
        # XXX all of the res_perm, sl.ca.null_prob and
        #     sl.null_dist.ca.dist_samples carry a degenerate leading
        #     dimension which was probably due to introduced new axis
        #     above within cv.postproc
        assert_equal(res_perm.shape, (1, 3, 2, 2))
        assert_equal(sl.null_dist.ca.dist_samples.shape,
                     res_perm.shape + (no_permutations,))
        assert_equal(sl.ca.null_prob.shape, res_perm.shape)
        # just to make sure ;)
        ok_(np.all(sl.ca.null_prob.samples >= 0))
        ok_(np.all(sl.ca.null_prob.samples <= 1))

        # we should have got sums of hits across the splits
        assert_array_equal(np.sum(mat, axis=0), res_perm.samples[0])


    def test_chi_square_searchlight(self):
        # only do partial to save time

        # Can't yet do this since test_searchlight isn't yet "under nose"
        #skip_if_no_external('scipy')
        if not externals.exists('scipy'):
            return

        from mvpa2.misc.stats import chisquare

        cv = CrossValidation(sample_clf_lin, NFoldPartitioner(),
                enable_ca=['stats'])


        def getconfusion(data):
            cv(data)
            return chisquare(cv.ca.stats.matrix)[0]

        sl = sphere_searchlight(getconfusion, radius=0,
                         center_ids=[3, 50])

        # run searchlight
        results = sl(self.dataset)
        self.assertTrue(results.nfeatures == 2)


    def test_1d_multispace_searchlight(self):
        ds = Dataset([np.arange(6)])
        ds.fa['coord1'] = np.repeat(np.arange(3), 2)
        # add a second space to the dataset
        ds.fa['coord2'] = np.tile(np.arange(2), 3)
        measure = lambda x: "+".join([str(x) for x in x.samples[0]])
        # simply select each feature once
        res = Searchlight(measure,
                          IndexQueryEngine(coord1=Sphere(0),
                                           coord2=Sphere(0)),
                          nproc=1)(ds)
        assert_array_equal(res.samples, [['0', '1', '2', '3', '4', '5']])
        res = Searchlight(measure,
                          IndexQueryEngine(coord1=Sphere(0),
                                           coord2=Sphere(1)),
                          nproc=1)(ds)
        assert_array_equal(res.samples,
                           [['0+1', '0+1', '2+3', '2+3', '4+5', '4+5']])
        res = Searchlight(measure,
                          IndexQueryEngine(coord1=Sphere(1),
                                           coord2=Sphere(0)),
                          nproc=1)(ds)
        assert_array_equal(res.samples,
                           [['0+2', '1+3', '0+2+4', '1+3+5', '2+4', '3+5']])

    #@sweepargs(regr=regrswh[:])
    @reseed_rng()
    def test_regression_with_additional_sa(self):
        regr = regrswh[:][0]
        ds = datasets['3dsmall'].copy()
        ds.fa['voxel_indices'] = ds.fa.myspace

        # Create a new sample attribute which will be used along with
        # every searchlight
        ds.sa['beh'] = np.random.normal(size=(ds.nsamples, 2))

        # and now for fun -- lets create custom linar regression
        # targets out of some random feature and beh linearly combined
        rfeature = np.random.randint(ds.nfeatures)
        ds.sa.targets = np.dot(
            np.hstack((ds.sa.beh,
                       ds.samples[:, rfeature:rfeature + 1])),
            np.array([0.3, 0.2, 0.3]))

        class CrossValidationWithBeh(CrossValidation):
            """An adapter for regular CV which would hstack
               sa.beh to the searchlighting ds"""
            def _call(self, ds):
                return CrossValidation._call(
                    self,
                    Dataset(np.hstack((ds, ds.sa.beh)),
                            sa=ds.sa))
        cvbeh = CrossValidationWithBeh(regr, OddEvenPartitioner(),
                                       errorfx=corr_error)
        # regular cv
        cv = CrossValidation(regr, OddEvenPartitioner(),
                             errorfx=corr_error)

        slbeh = sphere_searchlight(cvbeh, radius=1)
        slmapbeh = slbeh(ds)
        sl = sphere_searchlight(cv, radius=1)
        slmap = sl(ds)

        assert_equal(slmap.shape, (2, ds.nfeatures))
        # SL which had access to beh should have got for sure better
        # results especially in the vicinity of the chosen feature...
        features = sl.queryengine.query_byid(rfeature)
        assert_array_lequal(slmapbeh.samples[:, features],
                            slmap.samples[:, features])

        # elsewhere they should tend to be better but not guaranteed

    @labile(5, 1)
    def test_usecase_concordancesl(self):
        import numpy as np
        from mvpa2.base.dataset import vstack
        from mvpa2.mappers.fx import mean_sample

        # Take our sample 3d dataset
        ds1 = datasets['3dsmall'].copy(deep=True)
        ds1.fa['voxel_indices'] = ds1.fa.myspace
        ds1.sa['subject'] = [1]  # not really necessary -- but let's for clarity
        ds1 = mean_sample()(ds1) # so we get just a single representative sample

        def corr12(ds):
            corr = np.corrcoef(ds.samples)
            assert(corr.shape == (2, 2)) # for paranoid ones
            return corr[0, 1]

        for nsc, thr, thr_mean in (
            (0, 1.0, 1.0),
            (0.1, 0.3, 0.8)):   # just a bit of noise
            ds2 = ds1.copy(deep=True)    # make a copy for the 2nd subject
            ds2.sa['subject'] = [2]
            ds2.samples += nsc * np.random.normal(size=ds1.shape)

            # make sure that both have the same voxel indices
            assert(np.all(ds1.fa.voxel_indices == ds2.fa.voxel_indices))
            ds_both = vstack((ds1, ds2))# join 2 images into a single dataset
                                        # with .sa.subject distinguishing both

            sl = sphere_searchlight(corr12, radius=2)
            slmap = sl(ds_both)
            ok_(np.all(slmap.samples >= thr))
            ok_(np.mean(slmap.samples) >= thr)

    def test_swaroop_case(self):
        """Test hdf5 backend to pass results on Swaroop's usecase
        """
        skip_if_no_external('h5py')
        from mvpa2.measures.base import Measure
        class sw_measure(Measure):
            def __init__(self):
                Measure.__init__(self, auto_train=True)
            def _call(self, dataset):
                # For performance measures -- increase to 50-200
                # np.sum here is just to get some meaningful value in
                # them
                #return np.ones(shape=(2, 2))*np.sum(dataset)
                return Dataset(
                    np.array([{'d': np.ones(shape=(5, 5)) * np.sum(dataset)}],
                             dtype=object))
        results = []
        ds = datasets['3dsmall'].copy(deep=True)
        ds.fa['voxel_indices'] = ds.fa.myspace

        our_custom_prefix = tempfile.mktemp()
        for backend in ['native'] + \
                (externals.exists('h5py') and ['hdf5'] or []):
            sl = sphere_searchlight(sw_measure(),
                                    radius=1,
                                    tmp_prefix=our_custom_prefix,
                                    results_backend=backend)
            t0 = time.time()
            results.append(np.asanyarray(sl(ds)))
            # print "Done for backend %s in %d sec" % (backend, time.time() - t0)
        # because of swaroop's ad-hoc (who only could recommend such
        # a construct?) use case, and absent fancy working assert_objectarray_equal
        # let's compare manually
        #assert_objectarray_equal(*results)
        if not externals.exists('h5py'):
            self.assertRaises(RuntimeError,
                              sphere_searchlight,
                              sw_measure(),
                              results_backend='hdf5')
            raise SkipTest('h5py required for test of backend="hdf5"')
        assert_equal(results[0].shape, results[1].shape)
        results = [r.flatten() for r in results]
        for x, y in zip(*results):
            assert_equal(x.keys(), y.keys())
            assert_array_equal(x['d'], y['d'])
        # verify that no junk is left behind
        tempfiles = glob.glob(our_custom_prefix + '*')
        assert_equal(len(tempfiles), 0)


    def test_nblocks(self):
        skip_if_no_external('pprocess')
        # just a basic test to see that we are getting the same
        # results with different nblocks
        ds = datasets['3dsmall'].copy(deep=True)[:, :13]
        ds.fa['voxel_indices'] = ds.fa.myspace
        cv = CrossValidation(GNB(), OddEvenPartitioner())
        res1 = sphere_searchlight(cv, radius=1, nproc=2)(ds)
        res2 = sphere_searchlight(cv, radius=1, nproc=2, nblocks=5)(ds)
        assert_array_equal(res1, res2)


    def test_custom_results_fx_logic(self):
        # results_fx was introduced for the blow-up-the-memory-Swaroop
        # where keeping all intermediate results of the dark-magic SL
        # hyperalignment is not feasible.  So it is desired to split
        # searchlight computation in more blocks while composing the
        # target result "on-the-fly" from available so far results.
        #
        # Implementation relies on using generators feeding the
        # results_fx with fresh results whenever those become
        # available.
        #
        # This test/example's "measure" creates files which should be
        # handled by the results_fx function and removed in this case
        # to check if we indeed have desired high number of blocks while
        # only limited nproc.
        skip_if_no_external('pprocess')

        tfile = tempfile.mktemp('mvpa', 'test-sl')

        ds = datasets['3dsmall'].copy()[:, :25] # smaller copy
        ds.fa['voxel_indices'] = ds.fa.myspace
        ds.fa['feature_id'] = np.arange(ds.nfeatures)

        nproc = 3 # it is not about computing -- so we will can
                  # start more processes than possibly having CPUs just to test
        nblocks = nproc * 7
        # figure out max number of features to be given to any proc_block
        # yoh: not sure why I had to +1 here... but now it became more robust and
        # still seems to be doing what was demanded so be it
        max_block = int(ceil(ds.nfeatures / float(nblocks)) + 1)

        def print_(s, *args):
            """For local debugging"""
            #print s, args
            pass

        def results_fx(sl=None, dataset=None, roi_ids=None, results=None):
            """It will "process" the results by removing those files
               generated inside the measure
            """
            res = []
            print_("READY")
            for x in results:
                ok_(isinstance(x, list))
                res.append(x)
                print_("R: ", x)
                for r in x:
                    # Can happen if we requested those .ca's enabled
                    # -- then automagically _proc_block would wrap
                    # results in a dataset... Originally detected by
                    # running with MVPA_DEBUG=.* which triggered
                    # enabling all ca's
                    if is_datasetlike(r):
                        r = np.asscalar(r.samples)
                    os.unlink(r)         # remove generated file
                print_("WAITING")

            results_ds = hstack(sum(res, []))

            # store the center ids as a feature attribute since we use
            # them for testing
            results_ds.fa['center_ids'] = roi_ids
            return results_ds

        def results_postproc_fx(results):
            for ds in results:
                ds.fa['test_postproc'] = np.atleast_1d(ds.a.roi_center_ids**2)
            return results

        def measure(ds):
            """The "measure" will check if a run with the same "index" from
               previous block has been processed by now
            """
            f = '%s+%03d' % (tfile, ds.fa.feature_id[0] % (max_block * nproc))
            print_("FID:%d f:%s" % (ds.fa.feature_id[0], f))

            # allow for up to few seconds to wait for the file to
            # disappear -- i.e. its result from previous "block" was
            # processed
            t0 = time.time()
            while os.path.exists(f) and time.time() - t0 < 4.:
                time.sleep(0.5) # so it does take time to compute the measure
                pass
            if os.path.exists(f):
                print_("ERROR: ", f)
                raise AssertionError("File %s must have been processed by now"
                                     % f)
            open(f, 'w').write('XXX')   # signal that we have computing this measure
            print_("RES: %s" % f)
            return f

        sl = sphere_searchlight(measure,
                                radius=0,
                                nproc=nproc,
                                nblocks=nblocks,
                                results_postproc_fx=results_postproc_fx,
                                results_fx=results_fx,
                                center_ids=np.arange(ds.nfeatures)
                                )

        assert_equal(len(glob.glob(tfile + '*')), 0) # so no junk around
        try:
            res = sl(ds)
            assert_equal(res.nfeatures, ds.nfeatures)
            # verify that we did have results_postproc_fx called
            assert_array_equal(res.fa.test_postproc, np.power(res.fa.center_ids, 2))
        finally:
            # remove those generated left-over files
            for f in glob.glob(tfile + '*'):
                os.unlink(f)

def suite():  # pragma: no cover
    return unittest.makeSuite(SearchlightTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_senses
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Sensitivity Analyzers"""

import numpy as np

from mvpa2.testing import *
from mvpa2.misc.data_generators import normal_feature_dataset
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.clfs.meta import SplitClassifier
from mvpa2.clfs.smlr import SMLR

@reseed_rng()
@labile(5, 1)
def test_splitclf_sensitivities():
    datasets = [normal_feature_dataset(perlabel=100, nlabels=2,
                                       nfeatures=4,
                                       nonbogus_features=[0, i + 1],
                                       snr=1, nchunks=2)
                for i in xrange(2)]

    sclf = SplitClassifier(SMLR(),
                           NFoldPartitioner())
    analyzer = sclf.get_sensitivity_analyzer()

    senses1 = analyzer(datasets[0])
    senses2 = analyzer(datasets[1])

    for senses in senses1, senses2:
        # This should be False when comparing two folds
        assert_false(np.allclose(senses.samples[0],
                                 senses.samples[2]))
        assert_false(np.allclose(senses.samples[1],
                                 senses.samples[3]))
    # Moreover with new data we should have got different results
    # (i.e. it must retrained correctly)
    for s1, s2 in zip(senses1, senses2):
        assert_false(np.allclose(s1, s2))

    # and we should have "selected" "correct" voxels
    for i, senses in enumerate((senses1, senses2)):
        assert_equal(set(np.argsort(np.max(np.abs(senses), axis=0))[-2:]),
                     set((0, i + 1)))


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_smlr
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA sparse multinomial logistic regression classifier"""

import numpy as np

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets

from mvpa2.clfs.smlr import SMLR
from mvpa2.misc.data_generators import normal_feature_dataset


class SMLRTests(unittest.TestCase):

    def test_smlr(self):
        data = datasets['dumb']

        clf = SMLR()

        clf.train(data)

        # prediction has to be perfect
        #
        # XXX yoh: whos said that?? ;-)
        #
        # There is always a tradeoff between learning and
        # generalization errors so...  but in this case the problem is
        # more interesting: absent bias disallows to learn data you
        # have here -- there is no solution which would pass through
        # (0,0)
        predictions = clf.predict(data.samples)
        self.assertTrue((predictions == data.targets).all())


    def test_smlr_state(self):
        data = datasets['dumb']

        clf = SMLR()

        clf.train(data)

        clf.ca.enable('estimates')
        clf.ca.enable('predictions')

        p = np.asarray(clf.predict(data.samples))

        self.assertTrue((p == clf.ca.predictions).all())
        self.assertTrue(np.array(clf.ca.estimates).shape[0] == np.array(p).shape[0])


    def test_smlr_sensitivities(self):
        data = normal_feature_dataset(perlabel=10, nlabels=2, nfeatures=4)

        # use SMLR on binary problem, but not fitting all weights
        clf = SMLR(fit_all_weights=False)
        clf.train(data)

        # now ask for the sensitivities WITHOUT having to pass the dataset
        # again
        sens = clf.get_sensitivity_analyzer(force_train=False)(None)
        self.assertTrue(sens.shape == (len(data.UT) - 1, data.nfeatures))


def suite():  # pragma: no cover
    return unittest.makeSuite(SMLRTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_som
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA SOM mapper"""


import unittest
import numpy as np
from mvpa2 import cfg
from mvpa2.mappers.som import SimpleSOMMapper
from mvpa2.datasets.base import dataset_wizard

class SOMMapperTests(unittest.TestCase):

    def test_simple_som(self):
        colors = np.array([[0., 0., 0.], [0., 0., 1.], [0., 1., 0.],
                          [1., 0., 0.], [0., 1., 1.], [1., 0., 1.],
                          [1., 1., 0.], [1., 1., 1.]])

        distance_measures = (None, lambda x, y:(x ** 3 + y ** 3) ** (1. / 3))

        for distance_measure in distance_measures:
            # only small SOM for speed reasons
            som = SimpleSOMMapper((10, 5), 200, learning_rate=0.05)

            # no acces when nothing is there
            self.assertRaises(RuntimeError, som._access_kohonen)

            som.train(colors)

            fmapped = som.forward(colors)
            self.assertTrue(fmapped.shape == (8, 2))

            # reverse mapping
            rmapped = som.reverse(fmapped)

            if cfg.getboolean('tests', 'labile', default='yes'):
                # should approximately restore the input, but could fail
                # with bad initialisation
                self.assertTrue((np.round(rmapped) == colors).all())


def suite():  # pragma: no cover
    return unittest.makeSuite(SOMMapperTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_spam
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA SpAM."""

from mvpa2.misc import data_generators

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets


def test_basic():
    """Disabled the test since we have no SpAM yet
    """
    raise SkipTest, "skipped SPaM since no SPaM"
    from mvpa2.clfs.spam import SpAM

    dataset = datasets['sin_modulated']
    clf = SpAM()
    clf.train(dataset)
    y = clf.predict(dataset.samples)
    assert_array_equal(y.shape, dataset.targets.shape)


########NEW FILE########
__FILENAME__ = test_splitter
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA pattern handling"""

import unittest
import numpy as np

from mvpa2.testing.datasets import datasets
from mvpa2.base.node import ChainNode
from mvpa2.datasets.base import dataset_wizard, Dataset
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.mappers.slicing import StripBoundariesSamples
from mvpa2.generators.splitters import Splitter
from mvpa2.generators.partition import *

from mvpa2.testing.tools import ok_, assert_array_equal, assert_true, \
        assert_false, assert_equal, assert_not_equal, reseed_rng

class SplitterTests(unittest.TestCase):

    @reseed_rng()
    def setUp(self):
        self.data = dataset_wizard(np.random.normal(size=(100,10)),
                            targets=[ i%4 for i in range(100) ],
                            chunks=[ i//10 for i in range(100)])

    def test_splitattr_deprecation(self):
        # Just a smoke test -- remove for 2.1 release
        nfs = NFoldPartitioner()
        _ = nfs.splitattr

    def test_reprs(self):
        # very very basic test to see that there is no errors in reprs
        # of partitioners
        import mvpa2.generators.partition as mgp
        for sclass in (x for x in dir(mgp) if x.endswith('Partitioner')):
            args = (1,)
            if sclass == 'ExcludeTargetsCombinationsPartitioner':
                args += (1,1)
            pclass = getattr(mgp, sclass)
            r = repr(pclass(*args))
            assert_false('ERROR' in r)

    def test_simplest_cv_pat_gen(self):
        # create the generator
        nfs = NFoldPartitioner(cvtype=1)
        spl = Splitter(attr='partitions')
        # now get the xval pattern sets One-Fold CV)
        xvpat = [ list(spl.generate(p)) for p in nfs.generate(self.data) ]

        self.assertTrue( len(xvpat) == 10 )

        for i,p in enumerate(xvpat):
            self.assertTrue( len(p) == 2 )
            self.assertTrue( p[0].nsamples == 90 )
            self.assertTrue( p[1].nsamples == 10 )
            self.assertTrue( p[1].chunks[0] == i )


    def test_odd_even_split(self):
        oes = OddEvenPartitioner()
        spl = Splitter(attr='partitions')

        splits = [ list(spl.generate(p)) for p in oes.generate(self.data) ]

        self.assertTrue(len(splits) == 2)

        for i,p in enumerate(splits):
            self.assertTrue( len(p) == 2 )
            self.assertTrue( p[0].nsamples == 50 )
            self.assertTrue( p[1].nsamples == 50 )

        assert_array_equal(splits[0][1].sa['chunks'].unique, [1, 3, 5, 7, 9])
        assert_array_equal(splits[0][0].sa['chunks'].unique, [0, 2, 4, 6, 8])
        assert_array_equal(splits[1][0].sa['chunks'].unique, [1, 3, 5, 7, 9])
        assert_array_equal(splits[1][1].sa['chunks'].unique, [0, 2, 4, 6, 8])

        # check if it works on pure odd and even chunk ids
        moresplits = [ list(spl.generate(p)) for p in oes.generate(splits[0][0])]

        for split in moresplits:
            self.assertTrue(split[0] != None)
            self.assertTrue(split[1] != None)


    def test_half_split(self):
        hs = HalfPartitioner()
        spl = Splitter(attr='partitions')

        splits = [ list(spl.generate(p)) for p in hs.generate(self.data) ]

        self.assertTrue(len(splits) == 2)

        for i,p in enumerate(splits):
            self.assertTrue( len(p) == 2 )
            self.assertTrue( p[0].nsamples == 50 )
            self.assertTrue( p[1].nsamples == 50 )

        assert_array_equal(splits[0][1].sa['chunks'].unique, [0, 1, 2, 3, 4])
        assert_array_equal(splits[0][0].sa['chunks'].unique, [5, 6, 7, 8, 9])
        assert_array_equal(splits[1][1].sa['chunks'].unique, [5, 6, 7, 8, 9])
        assert_array_equal(splits[1][0].sa['chunks'].unique, [0, 1, 2, 3, 4])

        # check if it works on pure odd and even chunk ids
        moresplits = [ list(spl.generate(p)) for p in hs.generate(splits[0][0])]

        for split in moresplits:
            self.assertTrue(split[0] != None)
            self.assertTrue(split[1] != None)

    def test_n_group_split(self):
        """Test NGroupSplitter alongside with the reversal of the
        order of spit out datasets
        """
        # Test 2 groups like HalfSplitter first
        hs = NGroupPartitioner(2)

        for isreversed, splitter in enumerate((hs, hs)):
            if isreversed:
                spl = Splitter(attr='partitions', reverse=True)
            else:
                spl = Splitter(attr='partitions')
            splits = [ list(spl.generate(p)) for p in hs.generate(self.data) ]
            self.assertTrue(len(splits) == 2)

            for i, p in enumerate(splits):
                self.assertTrue( len(p) == 2 )
                self.assertTrue( p[0].nsamples == 50 )
                self.assertTrue( p[1].nsamples == 50 )

            assert_array_equal(splits[0][1-isreversed].sa['chunks'].unique,
                               [0, 1, 2, 3, 4])
            assert_array_equal(splits[0][isreversed].sa['chunks'].unique,
                               [5, 6, 7, 8, 9])
            assert_array_equal(splits[1][1-isreversed].sa['chunks'].unique,
                               [5, 6, 7, 8, 9])
            assert_array_equal(splits[1][isreversed].sa['chunks'].unique,
                               [0, 1, 2, 3, 4])

        # check if it works on pure odd and even chunk ids
        moresplits = [ list(spl.generate(p)) for p in hs.generate(splits[0][0])]

        for split in moresplits:
            self.assertTrue(split[0] != None)
            self.assertTrue(split[1] != None)

        # now test more groups
        s5 = NGroupPartitioner(5)

        # get the splits
        for isreversed, s5splitter in enumerate((s5, s5)):
            if isreversed:
                spl = Splitter(attr='partitions', reverse=True)
            else:
                spl = Splitter(attr='partitions')
            splits = [ list(spl.generate(p)) for p in s5splitter.generate(self.data) ]

            # must have 10 splits
            self.assertTrue(len(splits) == 5)

            # check split content
            assert_array_equal(splits[0][1-isreversed].sa['chunks'].unique,
                               [0, 1])
            assert_array_equal(splits[0][isreversed].sa['chunks'].unique,
                               [2, 3, 4, 5, 6, 7, 8, 9])
            assert_array_equal(splits[1][1-isreversed].sa['chunks'].unique,
                               [2, 3])
            assert_array_equal(splits[1][isreversed].sa['chunks'].unique,
                               [0, 1, 4, 5, 6, 7, 8, 9])
            # ...
            assert_array_equal(splits[4][1-isreversed].sa['chunks'].unique,
                               [8, 9])
            assert_array_equal(splits[4][isreversed].sa['chunks'].unique,
                               [0, 1, 2, 3, 4, 5, 6, 7])


        # Test for too many groups
        def splitcall(spl, dat):
            return list(spl.generate(dat))
        s20 = NGroupPartitioner(20)
        self.assertRaises(ValueError,splitcall,s20,self.data)

    @reseed_rng()
    def test_nfold_random_counted_selection_partitioner(self):
        return
        # Lets get somewhat extensive but complete one and see if
        # everything is legit. 0.5 must correspond to 50%, in our case
        # 5 out of 10 unique chunks
        split_partitions = [
            tuple(x.sa.partitions)
            for x in NFoldPartitioner(0.5).generate(self.data)]
        # 252 is # of combinations of 5 from 10
        assert_equal(len(split_partitions), 252)

        # verify that all of them are unique
        assert_equal(len(set(split_partitions)), 252)

        # now let's limit our query
        kwargs = dict(count=10, selection_strategy='random')
        split10_partitions = [
            tuple(x.sa.partitions)
            for x in NFoldPartitioner(5, **kwargs).generate(self.data)]
        split10_partitions_ = [
            tuple(x.sa.partitions)
            for x in NFoldPartitioner(0.5, **kwargs).generate(self.data)]
        # to make sure that I deal with sets of tuples correctly:
        assert_equal(len(set(split10_partitions)), 10)
        assert_equal(len(split10_partitions), 10)
        assert_equal(len(split10_partitions_), 10)
        # and they must differ (same ones are possible but very very unlikely)
        assert_not_equal(split10_partitions, split10_partitions_)
        # but every one of them must be within known exhaustive set
        assert_equal(set(split_partitions).intersection(split10_partitions),
                     set(split10_partitions))
        assert_equal(set(split_partitions).intersection(split10_partitions_),
                     set(split10_partitions_))

    @reseed_rng()
    def test_nfold_random_counted_selection_partitioner_huge(self):
        # Just test that it completes in a reasonable time and does
        # not blow up as if would do if it was not limited by count
        kwargs = dict(count=10)
        ds = dataset_wizard(np.arange(1000).reshape((-1, 1)),
                            targets=range(1000),
                            chunks=range(500)*2)
        split_partitions_random = [
            tuple(x.sa.partitions)
            for x in NFoldPartitioner(100,  selection_strategy='random',
                                      **kwargs).generate(ds)]
        assert_equal(len(split_partitions_random), 10) # we get just 10


    def test_custom_split(self):
        #simulate half splitter
        hs = CustomPartitioner([(None,[0,1,2,3,4]),(None,[5,6,7,8,9])])
        spl = Splitter(attr='partitions')
        splits = [ list(spl.generate(p)) for p in hs.generate(self.data) ]
        self.assertTrue(len(splits) == 2)

        for i,p in enumerate(splits):
            self.assertTrue( len(p) == 2 )
            self.assertTrue( p[0].nsamples == 50 )
            self.assertTrue( p[1].nsamples == 50 )

        assert_array_equal(splits[0][1].sa['chunks'].unique, [0, 1, 2, 3, 4])
        assert_array_equal(splits[0][0].sa['chunks'].unique, [5, 6, 7, 8, 9])
        assert_array_equal(splits[1][1].sa['chunks'].unique, [5, 6, 7, 8, 9])
        assert_array_equal(splits[1][0].sa['chunks'].unique, [0, 1, 2, 3, 4])


        # check fully customized split with working and validation set specified
        cs = CustomPartitioner([([0,3,4],[5,9])])
        # we want to discared the unselected partition of the data, hence attr_value
        # these two splitters should do exactly the same thing
        splitters = (Splitter(attr='partitions', attr_values=[1,2]),
                     Splitter(attr='partitions', ignore_values=(0,)))
        for spl in splitters:
            splits = [ list(spl.generate(p)) for p in cs.generate(self.data) ]
            self.assertTrue(len(splits) == 1)

            for i,p in enumerate(splits):
                self.assertTrue( len(p) == 2 )
                self.assertTrue( p[0].nsamples == 30 )
                self.assertTrue( p[1].nsamples == 20 )

            self.assertTrue((splits[0][1].sa['chunks'].unique == [5, 9]).all())
            self.assertTrue((splits[0][0].sa['chunks'].unique == [0, 3, 4]).all())


    def test_label_splitter(self):
        oes = OddEvenPartitioner(attr='targets')
        spl = Splitter(attr='partitions')

        splits = [ list(spl.generate(p)) for p in oes.generate(self.data) ]

        assert_array_equal(splits[0][0].sa['targets'].unique, [0,2])
        assert_array_equal(splits[0][1].sa['targets'].unique, [1,3])
        assert_array_equal(splits[1][0].sa['targets'].unique, [1,3])
        assert_array_equal(splits[1][1].sa['targets'].unique, [0,2])


    def test_counted_splitting(self):
        spl = Splitter(attr='partitions')
        # count > #chunks, should result in 10 splits
        nchunks = len(self.data.sa['chunks'].unique)
        for strategy in Partitioner._STRATEGIES:
            for count, target in [ (nchunks*2, nchunks),
                                   (nchunks, nchunks),
                                   (nchunks-1, nchunks-1),
                                   (3, 3),
                                   (0, 0),
                                   (1, 1)
                                   ]:
                nfs = NFoldPartitioner(cvtype=1, count=count,
                                       selection_strategy=strategy)
                splits = [ list(spl.generate(p)) for p in nfs.generate(self.data) ]
                self.assertTrue(len(splits) == target)
                chosenchunks = [int(s[1].uniquechunks) for s in splits]

                # Test if configuration matches as well
                nsplits_cfg = len(nfs.get_partition_specs(self.data))
                self.assertEqual(nsplits_cfg, target)

                # Check if "lastsplit" dsattr was assigned appropriately
                nsplits = len(splits)
                if nsplits > 0:
                    # dummy-proof testing of last split
                    for ds_ in splits[-1]:
                        self.assertTrue(ds_.a.lastpartitionset)
                    # test all now
                    for isplit,split in enumerate(splits):
                        for ds_ in split:
                            ds_.a.lastpartitionset == isplit==nsplits-1

                # Check results of different strategies
                if strategy == 'first':
                    self.assertEqual(chosenchunks, range(target))
                elif strategy == 'equidistant':
                    if target == 3:
                        self.assertEqual(chosenchunks, [0, 3, 7])
                elif strategy == 'random':
                    # none is selected twice
                    self.assertTrue(len(set(chosenchunks)) == len(chosenchunks))
                    self.assertTrue(target == len(chosenchunks))
                else:
                    raise RuntimeError, "Add unittest for strategy %s" \
                          % strategy


    def test_discarded_boundaries(self):
        ds = datasets['hollow']
        # four runs
        ds.sa['chunks'] = np.repeat(np.arange(4), 10)
        # do odd even splitting for lots of boundaries in few splits
        part = ChainNode([OddEvenPartitioner(),
                          StripBoundariesSamples('chunks', 1, 2)])

        parts = [d.samples.sid for d in part.generate(ds)]

        # both dataset should have the same samples, because the boundaries are
        # identical and the same sample should be stripped
        assert_array_equal(parts[0], parts[1])

        # we strip 3 samples per boundary
        assert_equal(len(parts[0]), len(ds) - (3 * 3))

        for i in [9, 10, 11, 19, 20, 21, 29, 30, 31]:
            assert_false(i in parts[0])


    def test_slicing(self):
        hs = HalfPartitioner()
        spl = Splitter(attr='partitions')
        splits = list(hs.generate(self.data))
        for s in splits:
            # partitioned dataset shared the data
            assert_true(s.samples.base is self.data.samples)
        splits = [ list(spl.generate(p)) for p in hs.generate(self.data) ]

        # with numpy 1.7.0b1 "chaining" was deprecated so let's create
        # check function appropriate for the given numpy version
        _a = np.arange(5)
        __a = _a[:4][:3]
        if __a.base is _a:
            # 1.7.0b1
            def is_the_same_base(x, base=self.data.samples):
                return x.base is base
        elif __a.base.base is _a:
            # prior 1.7.0b1
            def is_the_same_base(x, base=self.data.samples):
                return x.base.base is base
        else:
            raise RuntimeError("Uknown handling of .base by numpy")

        for s in splits:
            # we get slicing all the time
            assert_true(is_the_same_base(s[0].samples))
            assert_true(is_the_same_base(s[1].samples))
        spl = Splitter(attr='partitions', noslicing=True)
        splits = [ list(spl.generate(p)) for p in hs.generate(self.data) ]
        for s in splits:
            # we no slicing at all
            assert_false(s[0].samples.base is self.data.samples)
            assert_false(s[1].samples.base is self.data.samples)
        nfs = NFoldPartitioner()
        spl = Splitter(attr='partitions')
        splits = [ list(spl.generate(p)) for p in nfs.generate(self.data) ]
        for i, s in enumerate(splits):
            # training only first and last split
            if i == 0 or i == len(splits) - 1:
                assert_true(is_the_same_base(s[0].samples))
            else:
                assert_true(s[0].samples.base is None)
            # we get slicing all the time
            assert_true(is_the_same_base(s[1].samples))
        step_ds = Dataset(np.random.randn(20,2),
                          sa={'chunks': np.tile([0,1], 10)})
        oes = OddEvenPartitioner()
        spl = Splitter(attr='partitions')
        splits = list(oes.generate(step_ds))
        for s in splits:
            # partitioned dataset shared the data
            assert_true(s.samples.base is step_ds.samples)
        splits = [ list(spl.generate(p)) for p in oes.generate(step_ds) ]
        assert_equal(len(splits), 2)
        for s in splits:
            # we get slicing all the time
            assert_true(is_the_same_base(s[0].samples, step_ds.samples))
            assert_true(is_the_same_base(s[1].samples, step_ds.samples))


def suite():  # pragma: no cover
    return unittest.makeSuite(SplitterTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_state
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA State parent class"""

import unittest
import mvpa2.support.copy as copy

import numpy as np

from mvpa2.base import externals

from mvpa2.base.state import ConditionalAttribute, ClassWithCollections, \
     ParameterCollection, _def_sep
from mvpa2.base.param import *
from mvpa2.misc.exceptions import UnknownStateError

if __debug__:
    from mvpa2.base import debug

class TestClassEmpty(ClassWithCollections):
    pass

class TestClassBlank(ClassWithCollections):
    # We can force to have 'ca' present even though we don't have
    # any ConditionalAttribute defined here -- it might be added later on at run time
    _ATTRIBUTE_COLLECTIONS = ['ca']
    pass

class TestClassBlankNoExplicitStates(ClassWithCollections):
    pass

class TestClassProper(ClassWithCollections):

    state1 = ConditionalAttribute(enabled=False, doc="state1 doc")
    state2 = ConditionalAttribute(enabled=True, doc="state2 doc")


class TestClassProperChild(TestClassProper):

    state4 = ConditionalAttribute(enabled=False, doc="state4 doc")

class TestClassReadOnlyParameter(ClassWithCollections):
    paramro = Parameter(0, doc="state4 doc", ro=True)


class TestClassParametrized(TestClassProper, ClassWithCollections):
    p1 = Parameter(0)
    state0 = ConditionalAttribute(enabled=False)

    def __init__(self, **kwargs):
        # XXX make such example when we actually need to invoke
        # constructor
        # TestClassProper.__init__(self, **kwargs)
        ClassWithCollections.__init__(self, **kwargs)


class StateTests(unittest.TestCase):

    def test_blank_state(self):
        empty  = TestClassEmpty()
        blank  = TestClassBlank()
        blank2 = TestClassBlank()

        self.assertRaises(AttributeError, empty.__getattribute__, 'ca')

        self.assertEqual(list(blank.ca.items()), [])
        self.assertEqual(len(blank.ca), 0)
        self.assertTrue(blank.ca.enabled == [])
        self.assertRaises(AttributeError, blank.__getattribute__, 'dummy')
        self.assertRaises(AttributeError, blank.__getattribute__, '_')

        # we shouldn't use _registerState now since metaclass statecollector wouldn't
        # update the ca... may be will be implemented in the future if necessity comes
        return

        # add some conditional attribute
        blank._registerState('state1', False)
        self.assertTrue(blank.ca == ['state1'])

        self.assertTrue(blank.ca.is_enabled('state1') == False)
        self.assertTrue(blank.ca.enabled == [])
        self.assertRaises(UnknownStateError, blank.__getattribute__, 'state1')

        # assign value now
        blank.state1 = 123
        # should have no effect since the conditional attribute wasn't enabled
        self.assertRaises(UnknownStateError, blank.__getattribute__, 'state1')

        # lets enable and assign
        blank.ca.enable('state1')
        blank.state1 = 123
        self.assertTrue(blank.state1 == 123)

        # we should not share ca across instances at the moment, so an arbitrary
        # object could carry some custom ca
        self.assertTrue(blank2.ca == [])
        self.assertRaises(AttributeError, blank2.__getattribute__, 'state1')


    def test_proper_state(self):
        proper   = TestClassProper()
        proper2  = TestClassProper(enable_ca=['state1'], disable_ca=['state2'])

        # disable_ca should override anything in enable_ca
        proper3 = TestClassProper(enable_ca=['all'], disable_ca='all')

        self.assertEqual(len(proper3.ca.enabled), 0,
            msg="disable_ca should override anything in enable_ca")

        proper.ca.state2 = 1000
        value = proper.ca.state2
        self.assertEqual(proper.ca.state2, 1000, msg="Simple assignment/retrieval")

        proper.ca.disable('state2')
        proper.ca.state2 = 10000
        self.assertEqual(proper.ca.state2, 1000, msg="Simple assignment after being disabled")

        proper4 = copy.deepcopy(proper)

        proper.ca.reset('state2')
        self.assertRaises(UnknownStateError, proper.ca.__getattribute__, 'state2')
        """Must be blank after being reset"""

        self.assertEqual(proper4.ca.state2, 1000,
            msg="Simple assignment after being reset in original instance")


        proper.ca.enable(['state2'])
        self.assertEqual(set(proper.ca.keys()), set(['state1', 'state2']))
        if __debug__ and 'ENFORCE_CA_ENABLED' in debug.active:
            # skip testing since all ca are on now
            return
        self.assertTrue(proper.ca.enabled == ['state2'])

        self.assertTrue(set(proper2.ca.enabled) == set(['state1']))

        self.assertRaises(AttributeError, proper.__getattribute__, 'state12')

        # if documentary on the state is appropriate
        self.assertEqual(proper2.ca.listing,
                             ['%sstate1+%s: state1 doc' % (_def_sep, _def_sep),
                              '%sstate2%s: state2 doc' % (_def_sep, _def_sep)])

        # if __str__ lists correct number of ca
        str_ = str(proper2)
        self.assertTrue(str_.find('2 ca:') != -1)

        # check if disable works
        self.assertTrue(set(proper2.ca.enabled), set(['state1']))

        proper2.ca.disable("all")
        self.assertEqual(set(proper2.ca.enabled), set())

        proper2.ca.enable("all")
        self.assertEqual(len(proper2.ca.enabled), 2)

        proper2.ca.state1, proper2.ca.state2 = 1,2
        self.assertEqual(proper2.ca.state1, 1)
        self.assertEqual(proper2.ca.state2, 2)

        # now reset them
        proper2.ca.reset('all')
        self.assertRaises(UnknownStateError, proper2.ca.__getattribute__, 'state1')
        self.assertRaises(UnknownStateError, proper2.ca.__getattribute__, 'state2')


    def test_get_save_enabled(self):
        """Check if we can store/restore set of enabled ca"""

        if __debug__ and 'ENFORCE_CA_ENABLED' in debug.active:
            # skip testing since all ca are on now
            return

        proper  = TestClassProper()
        enabled_ca = proper.ca.enabled
        proper.ca.enable('state1')

        self.assertTrue(enabled_ca != proper.ca.enabled,
                        msg="New enabled ca should differ from previous")

        self.assertTrue(set(proper.ca.enabled) == set(['state1', 'state2']),
                        msg="Making sure that we enabled all ca of interest")

        proper.ca.enabled = enabled_ca
        self.assertTrue(enabled_ca == proper.ca.enabled,
                        msg="List of enabled ca should return to original one")


    # TODO: make test for _copy_ca_ or whatever comes as an alternative

    def test_stored_temporarily(self):
        proper   = TestClassProper()
        properch = TestClassProperChild(enable_ca=["state1"])

        if __debug__ and 'ENFORCE_CA_ENABLED' in debug.active:
            # skip testing since all ca are on now
            return

        self.assertEqual(proper.ca.enabled, ["state2"])
        proper.ca.change_temporarily(
            enable_ca=["state1"], other=properch)
        self.assertEqual(set(proper.ca.enabled),
                             set(["state1", "state2"]))
        proper.ca.reset_changed_temporarily()
        self.assertEqual(proper.ca.enabled, ["state2"])

        # allow to enable disable without other instance
        proper.ca.change_temporarily(
            enable_ca=["state1", "state2"])
        self.assertEqual(set(proper.ca.enabled),
                             set(["state1", "state2"]))
        proper.ca.reset_changed_temporarily()
        self.assertEqual(proper.ca.enabled, ["state2"])


    def test_proper_state_child(self):
        """
        Simple test if child gets conditional attributes from the parent as well
        """
        proper = TestClassProperChild()
        self.assertEqual(set(proper.ca.keys()),
                             set(['state1', 'state2', 'state4']))


    def test_state_variables(self):
        """To test new ca"""

        class S1(ClassWithCollections):
            v1 = ConditionalAttribute(enabled=True, doc="values1 is ...")
            v1XXX = ConditionalAttribute(enabled=False, doc="values1 is ...")


        class S2(ClassWithCollections):
            v2 = ConditionalAttribute(enabled=True, doc="values12 is ...")

        class S1_(S1):
            pass

        class S1__(S1_):
            v1__ = ConditionalAttribute(enabled=False)

        class S12(S1__, S2):
            v12 = ConditionalAttribute()

        s1, s2, s1_, s1__, s12 = S1(), S2(), S1_(), S1__(), S12()

        self.assertEqual(s1.ca.is_enabled("v1"), True)
        s1.ca.v1 = 12
        s12.ca.v1 = 120
        s2.ca.v2 = 100

        self.assertEqual(len(s2.ca.listing), 1)

        self.assertEqual(s1.ca.v1, 12)
        try:
            tempvalue = s1__.ca.v1__
            self.fail("Should have puked since values were not enabled yet")
        except:
            pass


    def test_parametrized(self):

        self.assertRaises(TypeError, TestClassParametrized,
            p2=34, enable_ca=['state1'],
            msg="Should raise an exception if argument doesn't correspond to"
                "any parameter")
        a = TestClassParametrized(p1=123, enable_ca=['state1'])
        self.assertEqual(a.params.p1, 123, msg="We must have assigned value to instance")
        self.assertTrue('state1' in a.ca.enabled,
                        msg="state1 must have been enabled")

        if (__debug__ and 'ID_IN_REPR' in debug.active):
            # next tests would fail due to ID in the tails
            return

        # validate that string representation of the object is valid and consistent
        a_str = repr(a)
        try:
            import mvpa2.tests.test_state as test_state
            exec("a2=%s" % a_str)
        except Exception as e:
            self.fail(msg="Failed to generate an instance out of "
                      "representation %s. Got exception: %s" % (a_str, e))

        # For specifics of difference in exec keyword from exec() function in
        # python3 see
        # http://stackoverflow.com/questions/6561482/why-did-python-3-changes-to-exec-break-this-code
        # which mandates us to use exec here around repr so it gets access to
        # above a2 placed into locals()
        exec('a2_str_=repr(a2)')
        a2_str = locals()['a2_str_']       # crazy ha?  it must not be a2_str either
        self.assertTrue(a2_str == a_str,
            msg="Generated object must have the same repr. Got %s and %s" %
            (a_str, a2_str))

        # Test at least that repr of collection is of correct syntax
        aparams_str = repr(a.params)
        try:
            import mvpa2.tests.test_state as test_state
            exec("aparams2=%s" % aparams_str)
        except Exception as e:
            self.fail(msg="Failed to generate an instance out of "
                      "representation  %s of params. Got exception: %s" % (aparams_str, e))

    def test_read_only(self):
        # Should be able to assign in constructor
        cro = TestClassReadOnlyParameter(paramro=12)
        # but not run time
        self.assertRaises(RuntimeError, cro.params['paramro']._set, 13)
        # Test if value wasn't actually changed
        self.assertEqual(cro.params.paramro, 12)

    def test_value_in_constructor(self):
        param = Parameter(0, value=True)
        self.assertTrue(param.value)

    def test_deep_copying_state_variable(self):
        for v in (True, False):
            sv = ConditionalAttribute(enabled=v,
                               doc="Testing")
            sv.enabled = not v
            sv_dc = copy.deepcopy(sv)
            if not (__debug__ and 'ENFORCE_CA_ENABLED' in debug.active):
                self.assertEqual(sv.enabled, sv_dc.enabled)
            self.assertEqual(sv.name, sv_dc.name)
            self.assertEqual(sv._instance_index, sv_dc._instance_index)

def suite():  # pragma: no cover
    return unittest.makeSuite(StateTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_staticprojection
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA StaticProjectionMapper"""

import numpy as np
from mvpa2.testing import *
from mvpa2.testing.datasets import *
from mvpa2.mappers.staticprojection import StaticProjectionMapper


def test_staticprojection_reverse_fa():
    ds = datasets['uni2small']
    proj = np.eye(ds.nfeatures)
    spm = StaticProjectionMapper(proj=proj[:,:3], recon=proj[:,:3].T)

    ok_(len(ds.fa) > 0)                   # we have some fa
    dsf = spm.forward(ds)
    ok_(len(dsf.fa) == 0)                 # no fa were left
    assert_equal(dsf.nfeatures, 3)        # correct # of features
    assert_equal(dsf.fa.attr_length, 3)   # and .fa knows about that 
    dsf.fa['new3'] = np.arange(3)

    dsfr = spm.reverse(dsf)
    ok_(len(dsfr.fa) == 0)                 # no fa were left
    assert_equal(dsfr.nfeatures, 6)
    assert_equal(dsfr.fa.attr_length, 6)   # .fa knows about them again
    dsfr.fa['new'] = np.arange(6)

########NEW FILE########
__FILENAME__ = test_stats
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA stats helpers"""

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets

from mvpa2 import cfg
from mvpa2.base import externals
from mvpa2.clfs.stats import MCNullDist, FixedNullDist, NullDist
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.datasets import Dataset
from mvpa2.measures.anova import OneWayAnova, CompoundOneWayAnova
from mvpa2.misc.fx import double_gamma_hrf, single_gamma_hrf
from mvpa2.measures.corrcoef import pearson_correlation

# Prepare few distributions to test
#kwargs = {'permutations':10, 'tail':'any'}
permutator = AttributePermutator('targets', count=30)
nulldist_sweep = [ MCNullDist(permutator, tail='any'),
                   MCNullDist(permutator, tail='right')]

if externals.exists('scipy'):
    from mvpa2.support.scipy.stats import scipy
    from scipy.stats import f_oneway
    from mvpa2.clfs.stats import rv_semifrozen
    nulldist_sweep += [ MCNullDist(permutator, scipy.stats.norm,
                                   tail='any'),
                        MCNullDist(permutator, scipy.stats.norm,
                                   tail='right'),
                        MCNullDist(permutator,
                                   rv_semifrozen(scipy.stats.norm, loc=0),
                                   tail='right'),
                        MCNullDist(permutator, scipy.stats.expon,
                                   tail='right'),
                        FixedNullDist(scipy.stats.norm(0, 10.0), tail='any'),
                        FixedNullDist(scipy.stats.norm(0, 10.0), tail='right'),
                        scipy.stats.norm(0, 0.1)
                        ]

class StatsTests(unittest.TestCase):
    """Unittests for various statistics"""


    @sweepargs(null=nulldist_sweep[1:])
    def test_null_dist_prob(self, null):
        """Testing null dist probability"""
        if not isinstance(null, NullDist):
            return
        ds = datasets['uni2small']

        null.fit(OneWayAnova(), ds)

        # check reasonable output.
        # p-values for non-bogus features should significantly different,
        # while bogus (0) not
        prob = null.p([20, 0, 0, 0, 0, np.nan])
        # XXX this is labile! it also needs checking since the F-scores
        # of the MCNullDists using normal distribution are apparently not
        # distributed that way, hence the test often (if not always) fails.
        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(np.abs(prob[0]) < 0.05,
                            msg="Expected small p, got %g" % prob[0])
        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue((np.abs(prob[1:]) > 0.05).all(),
                            msg="Bogus features should have insignificant p."
                            " Got %s" % (np.abs(prob[1:]),))

        # has to have matching shape
        if not isinstance(null, FixedNullDist):
            # Fixed dist is univariate ATM so it doesn't care
            # about dimensionality and gives 1 output value
            self.assertRaises(ValueError, null.p, [5, 3, 4])


    def test_anova(self):
        """Do some extended testing of OneWayAnova

        in particular -- compound estimation
        """

        m = OneWayAnova()               # default must be not compound ?
        mc = CompoundOneWayAnova()
        ds = datasets['uni2medium']

        # For 2 labels it must be identical for both and equal to
        # simple OneWayAnova
        a, ac = m(ds), mc(ds)

        self.assertTrue(a.shape == (1, ds.nfeatures))
        self.assertTrue(ac.shape == (len(ds.UT), ds.nfeatures))

        assert_array_equal(ac[0], ac[1])
        assert_array_equal(a, ac[1])

        # check for p-value attrs
        if externals.exists('scipy'):
            assert_true('fprob' in a.fa.keys())
            assert_equal(len(ac.fa), len(ac))

        ds = datasets['uni4large']
        ac = mc(ds)
        if cfg.getboolean('tests', 'labile', default='yes'):
            # All non-bogus features must be high for a corresponding feature
            self.assertTrue((ac.samples[np.arange(4),
                                        np.array(ds.a.nonbogus_features)] >= 1
                                        ).all())
        # All features should have slightly but different CompoundAnova
        # values. I really doubt that there will be a case when this
        # test would fail just to being 'labile'
        self.assertTrue(np.max(np.std(ac, axis=1)) > 0,
                        msg='In compound anova, we should get different'
                        ' results for different labels. Got %s' % ac)

    def test_pearson_correlation(self):
        sh = (3, -1)
        x = np.reshape(np.asarray([5, 3, 6, 5, 5, 4]), sh)
        y = np.reshape(np.asarray([3, 4, 5, 6, 3, 2, 6, 5, 4, 6, 6, 3]), sh)

        # compute in the traditional way
        nx = x.shape[1]
        ny = y.shape[1]

        c_np = np.zeros((nx, ny))
        for k in xrange(nx):
            for j in xrange(ny):
                c_np[k, j] = np.corrcoef(x[:, k], y[:, j])[0, 1]

        c = pearson_correlation(x, y)

        assert_array_almost_equal(c, c_np)


def suite():  # pragma: no cover
    """Create the suite"""
    return unittest.makeSuite(StatsTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_stats_sp
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA stats helpers -- those requiring scipy"""

from mvpa2.testing import *
skip_if_no_external('scipy')

from mvpa2.testing.datasets import datasets
from mvpa2.tests.test_stats import *

from scipy import signal
from mvpa2.clfs.stats import match_distribution, rv_semifrozen
from mvpa2.misc.stats import chisquare
from mvpa2.misc.attrmap import AttributeMap
from mvpa2.datasets.base import dataset_wizard
from mvpa2.generators.permutation import AttributePermutator

class StatsTestsScipy(unittest.TestCase):
    """Unittests for various statistics which use scipy"""

    @sweepargs(exp=('uniform', 'indep_rows', 'indep_cols'))
    def test_chi_square(self, exp):
        """Test chi-square distribution"""
        # test equal distribution
        tbl = np.array([[5, 5], [5, 5]])
        chi, p = chisquare(tbl, exp=exp)
        self.assertTrue(chi == 0.0)
        self.assertTrue(p == 1.0)

        # test perfect "generalization"
        tbl = np.array([[4, 0], [0, 4]])
        chi, p = chisquare(tbl, exp=exp)
        self.assertTrue(chi == 8.0)
        self.assertTrue(p < 0.05)

    def test_chi_square_disbalanced(self):
        # test perfect "generalization"
        tbl = np.array([[1, 100], [1, 100]])
        chi, p = chisquare(tbl, exp='indep_rows')
        self.assertTrue(chi == 0)
        self.assertTrue(p == 1)

        chi, p = chisquare(tbl, exp='uniform')
        self.assertTrue(chi > 194)
        self.assertTrue(p < 1e-10)

        # by default lets do uniform
        chi_, p_ = chisquare(tbl)
        self.assertTrue(chi == chi_)
        self.assertTrue(p == p_)


    def test_null_dist_prob_any(self):
        """Test 'any' tail statistics estimation"""
        skip_if_no_external('scipy')

        # test 'any' mode
        from mvpa2.measures.corrcoef import CorrCoef
        # we will reassign targets later on, so let's operate on a
        # copy
        ds = datasets['uni2medium'].copy()

        permutator = AttributePermutator('targets', count=20)
        null = MCNullDist(permutator, tail='any')

        assert_raises(ValueError, null.fit, CorrCoef(), ds)
        # cheat and map to numeric for this test
        ds.sa.targets = AttributeMap().to_numeric(ds.targets)
        null.fit(CorrCoef(), ds)

        # 100 and -100 should both have zero probability on their respective
        # tails
        pm100 = null.p([-100] + [0] * (ds.nfeatures - 1))
        p100 = null.p([100] + [0] * (ds.nfeatures - 1))
        assert_array_almost_equal(pm100, p100)

        # With 20 samples it isn't that easy to get a reliable sampling for
        # non-parametric, so we can allow somewhat low significance
        self.assertTrue(pm100[0] <= 0.1)
        self.assertTrue(p100[0] <= 0.1)

        self.assertTrue(np.all(pm100[1:] > 0.05))
        self.assertTrue(np.all(p100[1:] > 0.05))
        # same test with just scalar measure/feature
        null.fit(CorrCoef(), ds[:, 0])
        p_100 = null.p(100)
        self.failUnlessAlmostEqual(null.p(-100), p_100)
        self.failUnlessAlmostEqual(p100[0], p_100)


    @sweepargs(nd=nulldist_sweep)
    def test_dataset_measure_prob(self, nd):
        """Test estimation of measures statistics"""
        skip_if_no_external('scipy')

        ds = datasets['uni2medium']

        m = OneWayAnova(null_dist=nd, enable_ca=['null_t'])
        score = m(ds)

        score_nonbogus = np.mean(score.samples[:, ds.a.nonbogus_features])
        score_bogus = np.mean(score.samples[:, ds.a.bogus_features])
        # plausability check
        self.assertTrue(score_bogus < score_nonbogus)

        # [0] because the first axis is len == 0
        null_prob_nonbogus = m.ca.null_prob[0, ds.a.nonbogus_features]
        null_prob_bogus = m.ca.null_prob[0, ds.a.bogus_features]

        self.assertTrue((null_prob_nonbogus.samples < 0.05).all(),
            msg="Nonbogus features should have a very unlikely value. Got %s"
                % null_prob_nonbogus)

        # the others should be a lot larger
        self.assertTrue(np.mean(np.abs(null_prob_bogus)) >
                        np.mean(np.abs(null_prob_nonbogus)))

        if isinstance(nd, MCNullDist):
            # MCs are not stable with just 10 samples... so lets skip them
            return

        if cfg.getboolean('tests', 'labile', default='yes'):
            # Failed on c94ec26eb593687f25d8c27e5cfdc5917e352a69
            # with MVPA_SEED=833393575
            self.assertTrue(
                (np.abs(m.ca.null_t[0, ds.a.nonbogus_features]) >= 5).all(),
                msg="Nonbogus features should have high t-score. Got %s"
                % (m.ca.null_t[0, ds.a.nonbogus_features]))

            bogus_min = min(np.abs(m.ca.null_t.samples[0][ds.a.bogus_features]))
            self.assertTrue(bogus_min < 4,
                msg="Some bogus features should have low t-score of %g."
                    "Got (t,p,sens):%s"
                    % (bogus_min,
                        zip(m.ca.null_t[0, ds.a.bogus_features],
                            m.ca.null_prob[0, ds.a.bogus_features],
                            score.samples[0][ds.a.bogus_features])))

    @reseed_rng()
    def test_negative_t(self):
        """Basic testing of the sign in p and t scores
        """
        from mvpa2.measures.base import FeaturewiseMeasure

        class BogusMeasure(FeaturewiseMeasure):
            """Just put high positive into first 2 features, and high
            negative into 2nd two
            """
            is_trained = True
            def _call(self, dataset):
                """just a little helper... pylint shut up!"""
                res = np.random.normal(size=(dataset.nfeatures,))
                res[0] = res[1] = 100
                res[2] = res[3] = -100
                return Dataset([res])

        nd = FixedNullDist(scipy.stats.norm(0, 0.1), tail='any')
        m = BogusMeasure(null_dist=nd, enable_ca=['null_t'])
        ds = datasets['uni2small']
        _ = m(ds)
        t, p = m.ca.null_t, m.ca.null_prob
        assert_array_less(-1e-30, p.samples) # just that all >= 0
        assert_array_less(0, t.samples[0, :2])
        assert_array_less(t.samples[0, 2:4], 0)


    def test_match_distribution(self):
        """Some really basic testing for match_distribution
        """
        ds = datasets['uni2medium']      # large to get stable stats
        data = ds.samples[:, ds.a.bogus_features[0]]
        # choose bogus feature, which
        # should have close to normal distribution

        # Lets test ad-hoc rv_semifrozen
        floc = rv_semifrozen(scipy.stats.norm, loc=0).fit(data)
        self.assertTrue(floc[0] == 0)

        fscale = rv_semifrozen(scipy.stats.norm, scale=1.0).fit(data)
        self.assertTrue(fscale[1] == 1)

        flocscale = rv_semifrozen(scipy.stats.norm, loc=0, scale=1.0).fit(data)
        self.assertTrue(flocscale[1] == 1 and flocscale[0] == 0)

        full = scipy.stats.norm.fit(data)
        for res in [floc, fscale, flocscale, full]:
            self.assertTrue(len(res) == 2)

        data_mean = np.mean(data)
        for loc in [None, data_mean]:
            for test in ['p-roc', 'kstest']:
                # some really basic testing
                matched = match_distribution(
                    data=data,
                    distributions=['scipy',
                                     ('norm',
                                      {'name': 'norm_fixed',
                                       'loc': 0.2,
                                       'scale': 0.3})],
                    test=test, loc=loc, p=0.05)
                # at least norm should be in there
                names = [m[2] for m in matched]
                if test == 'p-roc':
                    if cfg.getboolean('tests', 'labile', default='yes'):
                        # we can guarantee that only for norm_fixed
                        self.assertTrue('norm' in names)
                        self.assertTrue('norm_fixed' in names)
                        inorm = names.index('norm_fixed')
                        # and it should be at least in the first
                        # 30 best matching ;-)
                        self.assertTrue(inorm <= 30)
                    # Test plotting only once
                    if loc is None and externals.exists("pylab plottable"):
                        import pylab as pl
                        from mvpa2.clfs.stats import plot_distribution_matches
                        fig = pl.figure()
                        plot_distribution_matches(data, matched, legend=1, nbest=5)
                        #pl.show()
                        pl.close(fig)

    def test_match_distribution_semifrozen(self):
        """Handle frozen params in match_distribution
        """
        matches = match_distribution(np.arange(10),
                                     distributions=[
                                         'uniform',
                                         ('uniform', {'loc': 0})
                                         ],
                                     p= -1 # so we get all matches
                                     )
        self.assertEqual(len(matches), 2) # we must get some match

        self.assertTrue(abs(matches[0][-1][0]) < 4e-1) # full fit should get close to true loc
        self.assertEqual(matches[1][-1][0], 0) # frozen should maintain the loc

        if externals.versions['scipy'] >= '0.10':
            # known to work on 0.10 and fail on 0.7.3
            self.assertTrue(abs(matches[0][-1][1] - 9) < 1e-1) # full fit should get close to true scale
        else:
            raise SkipTest("KnownFailure to fit uniform on older scipy")

        # actually it fails ATM to fit uniform with frozen loc=0
        # nicely -- sets scale = 1 :-/   TODO
        raise SkipTest("TODO: Known failure to fit uniform with frozen loc")
        self.assertTrue(abs(matches[1][-1][1] - 9) < 1e-1) # frozen fit of scale

    def test_r_dist_stability(self):
        """Test either rdist distribution performs nicely
        """
        try:
            # actually I haven't managed to cause this error
            scipy.stats.rdist(1.32, 0, 1).pdf(-1.0 + np.finfo(float).eps)
        except Exception, e:
            self.fail('Failed to compute rdist.pdf due to numeric'
                      ' loss of precision. Exception was %s' % e)

        try:
            # this one should fail with recent scipy with error
            # ZeroDivisionError: 0.0 cannot be raised to a negative power

            # XXX: There is 1 more bug in etch's scipy.stats or numpy
            #      (vectorize), so I have to put 2 elements in the
            #      queried x's, otherwise it
            #      would puke. But for now that fix is not here
            #
            # value = scipy.stats.rdist(1.32, 0, 1).cdf(
            #      [-1.0+np.finfo(float).eps, 0])
            #
            # to cause it now just run this unittest only with
            #  nosetests -s test_stats:StatsTests.testRDistStability

            # NB: very cool way to store the trace of the execution
            #import pydb
            #pydb.debugger(dbg_cmds=['bt', 'l', 's']*300 + ['c'])
            scipy.stats.rdist(1.32, 0, 1).cdf(-1.0 + np.finfo(float).eps)
        except IndexError, e:
            self.fail('Failed due to bug which leads to InvalidIndex if only'
                      ' scalar is provided to cdf')
        except Exception, e:
            self.fail('Failed to compute rdist.cdf due to numeric'
                      ' loss of precision. Exception was %s' % e)

        v = scipy.stats.rdist(10000, 0, 1).cdf([-0.1])
        self.assertTrue(v >= 0, v <= 1)

    @reseed_rng()
    def test_scipy_fit_2fparams(self):
        # fixing parameters was not before this version
        skip_if_no_external('scipy', min_version='0.8.1')
        t = scipy.stats.t
        d = t(10, 1, 10).rvs(10)
        params = t.fit(d, floc=1, fscale=10.)
        # assured result to be a tuple due to failure on
        # travis-ci (ubuntu precise)
        # http://travis-ci.org/#!/PyMVPA/PyMVPA/builds/2459017
        self.assertEqual(tuple(params[1:]), (1, 10.))
        # df's are apparently quite difficult to assess unless plenty
        # of samples
        #self.assertTrue(abs(params[0] - 10) < 7) # estimate df at least in the right ball park

    def test_anova_compliance(self):
        ds = datasets['uni2large']

        fwm = OneWayAnova()
        f = fwm(ds)
        f_sp = f_oneway(ds[ds.targets == 'L1'].samples,
                        ds[ds.targets == 'L0'].samples)

        # SciPy needs to compute the same F-scores
        assert_array_almost_equal(f, f_sp[0:1])


    @reseed_rng()
    def test_statsmodels(self):
        """Test GLM
        """
        skip_if_no_external('statsmodels')
        from mvpa2.measures.statsmodels_adaptor import GLM
        # play fmri
        # full-blown HRF with initial dip and undershoot ;-)
        hrf_x = np.linspace(0, 25, 250)
        hrf = double_gamma_hrf(hrf_x) - single_gamma_hrf(hrf_x, 0.8, 1, 0.05)

        # come up with an experimental design
        samples = 1800
        fast_er_onsets = np.array([10, 200, 250, 500, 600, 900, 920, 1400])
        fast_er = np.zeros(samples)
        fast_er[fast_er_onsets] = 1

        # high resolution model of the convolved regressor
        model_hr = np.convolve(fast_er, hrf)[:samples]

        # downsample the regressor to fMRI resolution
        tr = 2.0
        model_lr = signal.resample(model_hr,
                                   int(samples / tr / 10),
                                   window='ham')

        # generate artifical fMRI data: two voxels one is noise, one has
        # something
        baseline = 800.0
        wsignal = baseline + 2 * model_lr + \
                  np.random.randn(int(samples / tr / 10)) * 0.2
        nsignal = baseline + np.random.randn(int(samples / tr / 10)) * 0.5

        # build design matrix: bold-regressor and constant
        X = np.array([model_lr, np.repeat(1, len(model_lr))]).T

        # two 'voxel' dataset
        data = dataset_wizard(samples=np.array((wsignal, nsignal, nsignal)).T, targets=1)

        # check GLM betas
        glm = GLM(X)
        betas = glm(data)

        # betas for each feature and each regressor
        self.assertTrue(betas.shape == (X.shape[1], data.nfeatures))

        self.assertTrue(np.absolute(betas.samples[1] - baseline < 10).all(),
            msg="baseline betas should be huge and around 800")

        self.assertTrue(betas.samples[0, 0] > betas[0, 1],
            msg="feature (with signal) beta should be larger than for noise")

        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(np.absolute(betas[0, 1]) < 0.5)
            self.assertTrue(np.absolute(betas[0, 0]) > 1.0)


        # check GLM t values
        glm = GLM(X, voi='tvalues')
        tstats = glm(data)

        self.assertTrue(tstats.shape == betas.shape)

        self.assertTrue((tstats.samples[1] > 1000).all(),
                msg='constant tvalues should be huge')

        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(np.absolute(betas[0, 0]) > betas[0, 1],
                msg='with signal should have higher tvalues')

        # check t-contrast -- should do the same as tvalues for the first
        # parameter
        glm = GLM(X, voi=[1, 0])
        contrast = glm(data)
        assert_array_almost_equal(contrast.samples[0], tstats.samples[0])
        assert_equals(len(contrast), 6)
        # we should be able to recover the approximate effect size of the signal
        # which is constructed with a baseline offset of 2 (see above)
        if cfg.getboolean('tests', 'labile', default='yes'):
            assert_true(1.5 < contrast.samples[2, 0] < 2.5)

        # check F-test
        glm = GLM(X, voi=[[1, 0]])
        ftest = glm(data)
        assert_equals(len(ftest), 4)
        assert_true(ftest.samples[0, 0] > ftest.samples[0, 1])


    def test_binomdist_ppf(self):
        """Test if binomial distribution works ok

        after possibly a monkey patch
        """
        bdist = scipy.stats.binom(100, 0.5)
        self.assertTrue(bdist.ppf(1.0) == 100)
        self.assertTrue(bdist.ppf(0.9) <= 60)
        self.assertTrue(bdist.ppf(0.5) == 50)
        self.assertTrue(bdist.ppf(0) == -1)

    def test_right_tail(self):
        # Test if we are getting "correct" p-value for right tail in
        # "interesting" cases
        # TODO:  some of it is done in
        #   test_transerror.py:ErrorsTests.test_confusionmatrix_nulldist
        pass


def suite():  # pragma: no cover
    """Create the suite"""
    return unittest.makeSuite(StatsTestsScipy)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_suite
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit test for PyMVPA mvpa2.suite() of being loading ok"""

import inspect
import re
import sys
import unittest

from mvpa2.base.dochelpers import get_docstring_split

class SuiteTest(unittest.TestCase):

    def test_suite_load(self):
        """Test if we are loading fine
        """
        try:
            exec "from mvpa2.suite import *"
        except Exception, e: # pragma: no cover - should not be hit if ok_
            self.fail(msg="Cannot import everything from mvpa2.suite: %s" % e)

    def test_docstrings(self):
        #import mvpa2.suite as mv
        from mvpa2.suite import suite_stats
        # Lets do compliance checks
        # Get gross information on what we have in general
        #mv_scope = dict((x, getattr(mv, x)) for x in dir(mv))
        gs = suite_stats()#mv_scope)

        # all functions/classes/types should have some docstring
        missing = []
        # We should not have both :Parameters: and new style Parameters
        conflicting = []
        con_re1 = re.compile(':Parameters?:')
        con_re2 = re.compile('(?::Parameters?:.*Parameters?\s*\n\s*-------'
                             '|Parameters?\s*\n\s*-------.*:Parameters?:)',
                             flags=re.DOTALL)
        for c in ('functions', 'modules', 'objects', 'types') \
          + ('classes',) if sys.version_info[0] < 3 else ():
            missing1 = []
            conflicting1 = []
            self.assertTrue(gs[c])
            for k, i in gs[c].iteritems():
                try:
                    s = i.__doc__.strip()
                except: # pragma: no cover - should not be hit if ok_
                    s = ""
                if s == "":
                    missing1.append(k)

                if hasattr(i, '__init__') and not c in ['objects']:
                    # Smoke test get_docstring_split which would be used
                    # if someone specifies incorrect keyword argument
                    _ = get_docstring_split(i.__init__)
                    #if not None in _:
                    #    print [x[0] for x in _[1]]
                    si = i.__init__.__doc__
                    k += '.__init__'
                    if si is None or si == "":
                        try:
                            i_file = inspect.getfile(i)
                            if i_file == inspect.getfile(i.__init__) \
                               and 'mvpa' in i_file:
                                # if __init__ wasn't simply taken from some parent
                                # which is not within MVPA
                                missing1.append(k)
                        except TypeError:
                            # for some things like 'debug' inspect can't figure path
                            # just skip for now
                            pass
                else:
                    si = s

                if si is not None \
                       and  con_re1.search(si) and con_re2.search(si):
                    conflicting1.append(k)
            if len(missing1): # pragma: no cover - should not be hit if ok_
                missing.append("%s: " % c + ', '.join(missing1))
            if len(conflicting1): # pragma: no cover - should not be hit if ok_
                conflicting.append("%s: " % c + ', '.join(conflicting1))

        sfailures = []
        if len(missing): # pragma: no cover - should not be hit if ok_
            sfailures += ["Some items have missing docstrings:\n "
                          + '\n '.join(missing)]
        if len(conflicting): # pragma: no cover - should not be hit if ok_
            sfailures += ["Some items have conflicting formats of docstrings:\n "
                      + '\n '.join(conflicting)]
        if len(sfailures): # pragma: no cover - should not be hit if ok_
            self.fail('\n'.join(sfailures))


def suite():  # pragma: no cover
    return unittest.makeSuite(SuiteTest)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_support
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA serial feature inclusion algorithm"""

from mvpa2.testing import *
from mvpa2.misc.support import *
from mvpa2.base.types import asobjarray
from mvpa2.testing import *
from mvpa2.testing.datasets import get_mv_pattern, datasets
from mvpa2.testing.clfs import *
from mvpa2.clfs.distance import one_minus_correlation

from mvpa2.support.copy import deepcopy

class SupportFxTests(unittest.TestCase):

    def test_transform_with_boxcar(self):
        data = np.arange(10)
        sp = np.arange(10)

        # check if stupid thing don't work
        self.assertRaises(ValueError,
                              transform_with_boxcar,
                              data,
                              sp,
                              0)

        # now do an identity transformation
        trans = transform_with_boxcar(data, sp, 1)
        self.assertTrue((trans == data).all())

        # now check for illegal boxes
        self.assertRaises(ValueError,
                              transform_with_boxcar,
                              data,
                              sp,
                              2)

        # now something that should work
        sp = np.arange(9)
        trans = transform_with_boxcar(data, sp, 2)
        self.assertTrue((trans == \
                           [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5]).all())


        # now test for proper data shape
        data = np.ones((10, 3, 4, 2))
        sp = [ 2, 4, 3, 5 ]
        trans = transform_with_boxcar(data, sp, 4)
        self.assertTrue(trans.shape == (4, 3, 4, 2))



    def test_event(self):
        self.assertRaises(ValueError, Event)
        ev = Event(onset=2.5)

        # all there?
        self.assertTrue(ev.items() == [('onset', 2.5)])

        # conversion
        self.assertTrue(ev.as_descrete_time(dt=2).items() == [('onset', 1)])
        evc = ev.as_descrete_time(dt=2, storeoffset=True)
        self.assertTrue(evc.has_key('offset'))
        self.assertTrue(evc['offset'] == 0.5)

        # same with duration included
        evc = Event(onset=2.5, duration=3.55).as_descrete_time(dt=2)
        self.assertTrue(evc['duration'] == 3)


    def test_mof_n_combinations(self):
        self.assertEqual(
            unique_combinations(range(3), 1), [[0], [1], [2]])
        self.assertEqual(
            unique_combinations(
                        range(4), 2),
                        [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]
                        )
        self.assertEqual(
            unique_combinations(
                        range(4), 3),
                        [[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])


    @reseed_rng()
    def test_xrandom_unique_combinations(self):
        for n in [4, 5, 10]:
            limit = 4
            limited = list(xrandom_unique_combinations(range(n), 3, limit))
            self.assertEqual(len(limited), limit)
            # See if we would obtain the same
            for k in [1, 2, 3, int(n / 2), n]:
                all_random = list(xrandom_unique_combinations(range(n), k))
                all_ = list(xunique_combinations(range(n), k))
                self.assertEqual(sorted(all_random), sorted(all_))

        # test that we are not sampling the same space -- two
        # consecutive samples within large number very unlikely not
        # have more than few overlapping samples
        iter_count = 100
        overlapping_count = 0
        for k in xrange(iter_count):
            c1, c2 = xrandom_unique_combinations(range(1000), 10, 2)
            if len(set(c1).intersection(c2)) == 2:
                overlapping_count += 1

        # assume this happens less than 10 percent of the time
        self.assertTrue(overlapping_count * 10 < iter_count)


    def test_break_points(self):
        items_cont = [0, 0, 0, 1, 1, 1, 3, 3, 2]
        items_noncont = [0, 0, 1, 1, 0, 3, 2]
        self.assertRaises(ValueError, get_break_points, items_noncont)
        self.assertEqual(get_break_points(items_noncont, contiguous=False),
                             [0, 2, 4, 5, 6])
        self.assertEqual(get_break_points(items_cont), [0, 3, 6, 8])
        self.assertEqual(get_break_points(items_cont, contiguous=False),
                             [0, 3, 6, 8])


    def test_map_overlap(self):
        mo = MapOverlap()

        maps = [[1, 0, 1, 0],
                [1, 0, 0, 1],
                [1, 0, 1, 0]]

        overlap = mo(maps)

        self.assertEqual(overlap, 1. / len(maps[0]))
        self.assertTrue((mo.overlap_map == [1, 0, 0, 0]).all())
        self.assertTrue((mo.spread_map == [0, 0, 1, 1]).all())
        self.assertTrue((mo.ovstats_map == [1, 0, 2. / 3, 1. / 3]).all())

        mo = MapOverlap(overlap_threshold=0.5)
        overlap = mo(maps)
        self.assertEqual(overlap, 2. / len(maps[0]))
        self.assertTrue((mo.overlap_map == [1, 0, 1, 0]).all())
        self.assertTrue((mo.spread_map == [0, 0, 0, 1]).all())
        self.assertTrue((mo.ovstats_map == [1, 0, 2. / 3, 1. / 3]).all())


    @reseed_rng()
    @sweepargs(pair=[(np.random.normal(size=(10, 20)), np.random.normal(size=(10, 20))),
                     ([1, 2, 3, 0], [1, 3, 2, 0]),
                     ((1, 2, 3, 1), (1, 3, 2, 1))])
    def test_id_hash(self, pair):
        a, b = pair
        a1 = deepcopy(a)
        a_1 = idhash(a)
        self.assertTrue(a_1 == idhash(a), msg="Must be of the same idhash")
        self.assertTrue(a_1 != idhash(b), msg="Must be of different idhash")
        if isinstance(a, np.ndarray):
            self.assertTrue(a_1 != idhash(a.T), msg=".T must be of different idhash")
        if not isinstance(a, tuple):
            self.assertTrue(a_1 != idhash(a1), msg="Must be of different idhash")
            a[2] += 1; a_2 = idhash(a)
            self.assertTrue(a_1 != a_2, msg="Idhash must change")
        else:
            a_2 = a_1
        a = a[2:]; a_3 = idhash(a)
        self.assertTrue(a_2 != a_3, msg="Idhash must change after slicing")


    def test_asobjarray(self):
        for i in ([1, 2, 3], ['a', 2, '3'],
                  ('asd')):
            i_con = asobjarray(i)
            self.assertTrue(i_con.dtype is np.dtype('object'))
            self.assertEqual(len(i), len(i_con))

            # Note: in Python3 the ['a' , 2, '3'] list is converted to
            # an array with elements 'a', '2',' and '3' (i.e. string representation
            # for the second element), and thus np.all(i==i_con) fails.
            # Instead here each element is tested for equality seperately
            # XXX is this an issue?
            self.assertTrue(np.all((i[j] == i_con[j]) for j in xrange(len(i))))

    @reseed_rng()
    def test_correlation(self):
        # data: 20 samples, 80 features
        X = np.random.rand(20, 80)

        C = 1 - one_minus_correlation(X, X)

        # get nsample x nssample correlation matrix
        self.assertTrue(C.shape == (20, 20))
        # diagonal is 1
        self.assertTrue((np.abs(np.diag(C) - 1).mean() < 0.00001).all())

        # now two different
        Y = np.random.rand(5, 80)
        C2 = 1 - one_minus_correlation(X, Y)
        # get nsample x nssample correlation matrix
        self.assertTrue(C2.shape == (20, 5))
        # external validity check -- we are dealing with correlations
        self.assertTrue(C2[10, 2] - np.corrcoef(X[10], Y[2])[0, 1] < 0.000001)

    def test_version_to_tuple(self):
        """Test conversion of versions from strings
        """

        self.assertTrue(version_to_tuple('0.0.01') == (0, 0, 1))
        self.assertTrue(version_to_tuple('0.7.1rc3') == (0, 7, 1, 'rc', 3))


    def test_smart_version(self):
        """Test our ad-hoc SmartVersion
        """
        SV = SmartVersion

        for v1, v2 in (
            ('0.0.1', '0.0.2'),
            ('0.0.1', '0.1'),
            ('0.0.1', '0.1.0'),
            ('0.0.1', '0.0.1a'), # this might be a bit unconventional?
            ('0.0.1', '0.0.1+svn234'),
            ('0.0.1+svn234', '0.0.1+svn235'),
            ('0.0.1dev1', '0.0.1'),
            ('0.0.1dev1', '0.0.1rc3'),
            ('0.7.1rc3', '0.7.1'),
            ('0.0.1-dev1', '0.0.1'),
            ('0.0.1-svn1', '0.0.1'),
            ('0.0.1~p', '0.0.1'),
            ('0.0.1~prior.1.2', '0.0.1'),
            ):
            self.assertTrue(SV(v1) < SV(v2),
                            msg="Failed to compare %s to %s" % (v1, v2))
            self.assertTrue(SV(v2) > SV(v1),
                            msg="Failed to reverse compare %s to %s" % (v2, v1))
            # comparison to strings
            self.assertTrue(SV(v1) < v2,
                            msg="Failed to compare %s to string %s" % (v1, v2))
            self.assertTrue(v1 < SV(v2),
                            msg="Failed to compare string %s to %s" % (v1, v2))
            # to tuples
            self.assertTrue(SV(v1) < version_to_tuple(v2),
                            msg="Failed to compare %s to tuple of %s"
                            % (v1, v2))
            self.assertTrue(version_to_tuple(v1) < SV(v2),
                            msg="Failed to compare tuple of %s to %s"
                            % (v1, v2))


def test_value2idx():
    times = [1.2, 1.3, 2., 4., 0., 2., 1.1]
    assert_equal(value2idx(0, times), 4)
    assert_equal(value2idx(100, times), 3)
    assert_equal(value2idx(1.5, times), 1)
    assert_equal(value2idx(1.5, times, 'ceil'), 2)
    assert_equal(value2idx(1.2, times, 'floor'), 0)
    assert_equal(value2idx(1.14, times, 'round'), 6)
    assert_equal(value2idx(1.14, times, 'floor'), 6)
    assert_equal(value2idx(1.14, times, 'ceil'), 0)
    assert_equal(value2idx(-100, times, 'ceil'), 4)


def test_limit_filter():
    ds = datasets['uni2small']
    assert_array_equal(get_limit_filter(None, ds.sa),
                       np.ones(len(ds), dtype=np.bool))
    assert_array_equal(get_limit_filter('chunks', ds.sa),
                       ds.sa.chunks)
    assert_array_equal(get_limit_filter({'chunks': 3}, ds.sa),
                       ds.sa.chunks == 3)
    assert_array_equal(get_limit_filter({'chunks': [3, 1]}, ds.sa),
                       np.logical_or(ds.sa.chunks == 3,
                                     ds.sa.chunks == 1))

def test_mask2slice():
    slc = np.repeat(False, 5)
    assert_equal(mask2slice(slc), slice(None, 0, None))


def suite():  # pragma: no cover
    return unittest.makeSuite(SupportFxTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_surfing
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA surface searchlight and related utilities"""

from mvpa2.testing import *
skip_if_no_external('nibabel')

import numpy as np
from numpy.testing.utils import assert_array_almost_equal

import nibabel as nb

import os
import tempfile

from mvpa2.testing.datasets import datasets

from mvpa2 import cfg
from mvpa2.base import externals
from mvpa2.datasets import Dataset, hstack
from mvpa2.measures.base import Measure
from mvpa2.datasets.mri import fmri_dataset

from mvpa2.misc.surfing import volgeom, volsurf, \
                                volume_mask_dict, surf_voxel_selection, \
                                queryengine

from mvpa2.support.nibabel import surf, surf_fs_asc, surf_gifti

from mvpa2.measures.searchlight import sphere_searchlight, Searchlight
from mvpa2.misc.neighborhood import Sphere

if externals.exists('h5py'):
    from mvpa2.base.hdf5 import h5save, h5load


class SurfTests(unittest.TestCase):
    """Test for surfaces

    NNO Aug 2012

    'Ground truth' is whatever output is returned by the implementation
    as of mid-Aug 2012"""

    @with_tempfile('.asc', 'test_surf')
    def test_surf(self, temp_fn):
        """Some simple testing with surfaces
        """

        s = surf.generate_sphere(10)

        assert_true(s.nvertices == 102)
        assert_true(s.nfaces == 200)

        v = s.vertices
        f = s.faces

        assert_true(v.shape == (102, 3))
        assert_true(f.shape == (200, 3))

        # another surface
        t = s * 10 + 2
        assert_true(t.same_topology(s))
        assert_array_equal(f, t.faces)

        assert_array_equal(v * 10 + 2, t.vertices)

        # allow updating, but should not affect original array
        # CHECKME: maybe we want to throw an exception instead
        assert_true((v * 10 + 2 == t.vertices).all().all())
        assert_true((s.vertices * 10 + 2 == t.vertices).all().all())

        # a few checks on vertices and nodes
        v_check = {40:(0.86511144 , -0.28109175, -0.41541501),
                   10:(0.08706015, -0.26794358, -0.95949297)}
        f_check = {10:(7, 8, 1), 40:(30, 31, 21)}


        vf_checks = [(v_check, lambda x:x.vertices),
                     (f_check, lambda x:x.faces)]

        eps = .0001
        for cmap, f in vf_checks:
            for k, v in cmap.iteritems():
                surfval = f(s)[k, :]
                assert_true((abs(surfval - v) < eps).all())

        # make sure same topology fails with different topology
        u = surf.generate_cube()
        assert_false(u.same_topology(s))

        # check that neighbours are computed correctly
        # even if we nuke the topology afterwards
        for _ in [0, 1]:
            nbrs = s.neighbors
            n_check = [(0, 96, 0.284629),
                       (40, 39, 0.56218349),
                       (100, 99, 0.1741202)]
            for i, j, k in n_check:
                assert_true(abs(nbrs[i][j] - k) < eps)


        def assign_zero(x):
            x.faces[:, :] = 0
            return None

        assert_raises((ValueError, RuntimeError), assign_zero, s)

        # see if mapping to high res works
        h = surf.generate_sphere(40)

        low2high = s.map_to_high_resolution_surf(h, .1)
        partmap = {7: 141, 8: 144, 9: 148, 10: 153, 11: 157, 12: 281}
        for k, v in partmap.iteritems():
            assert_true(low2high[k] == v)

        # ensure that slow implementation gives same results as fast one
        low2high_slow = s.map_to_high_resolution_surf(h, .1)
        for k, v in low2high.iteritems():
            assert_true(low2high_slow[k] == v)

        #  should fail if epsilon is too small
        assert_raises(ValueError,
                      lambda x:x.map_to_high_resolution_surf(h, .01), s)

        n2f = s.node2faces
        for i in xrange(s.nvertices):
            nf = [10] if i < 2 else [5, 6] # number of faces expected

            assert_true(len(n2f[i]) in nf)


        # test dijkstra distances
        ds2 = s.dijkstra_distance(2)
        some_ds = {0: 3.613173280799, 1: 0.2846296765, 2: 0.,
                 52: 1.87458018, 53: 2.0487004817, 54: 2.222820777,
                 99: 3.32854360, 100: 3.328543604, 101: 3.3285436042}

        eps = np.finfo('f').eps
        for k, v in some_ds.iteritems():
            assert_true(abs(v - ds2[k]) < eps)

        # test I/O (through ascii files)
        surf.write(temp_fn, s, overwrite=True)
        s2 = surf.read(temp_fn)

        # test i/o and ensure that the loaded instance is trained
        if externals.exists('h5py'):
            h5save(temp_fn, s2)
            s2 = h5load(temp_fn)


        assert_array_almost_equal(s.vertices, s2.vertices, 4)
        assert_array_almost_equal(s.faces, s2.faces, 4)

        # test plane (new feature end of August 2012)
        s3 = surf.generate_plane((0, 0, 0), (2, 0, 0), (0, 1, 0), 10, 20)
        assert_equal(s3.nvertices, 200)
        assert_equal(s3.nfaces, 342)
        assert_array_almost_equal(s3.vertices[-1, :], np.array([18., 19, 0.]))
        assert_array_almost_equal(s3.faces[-1, :], np.array([199, 198, 179]))

        # test bar
        p, q = (0, 0, 0), (100, 0, 0)
        s4 = surf.generate_bar(p, q, 10, 12)
        assert_equal(s4.nvertices, 26)
        assert_equal(s4.nfaces, 48)


    def test_surf_border(self):
        s = surf.generate_sphere(3)
        assert_array_equal(s.nodes_on_border(), [False] * 11)

        s = surf.generate_plane((0, 0, 0), (0, 1, 0), (1, 0, 0), 10, 10)
        b = s.nodes_on_border()
        v = s.vertices

        vb = reduce(np.logical_or, [v[:, 0] == 0, v[:, 1] == 0,
                                    v[:, 0] == 9, v[:, 1] == 9])

        assert_array_equal(b, vb)

        assert_true(s.nodes_on_border(0))

    @with_tempfile('.asc', 'test_surf')
    def test_surf_fs_asc(self, temp_fn):
        s = surf.generate_sphere(5) * 100

        surf_fs_asc.write(temp_fn, s, overwrite=True)
        t = surf_fs_asc.read(temp_fn)

        assert_array_almost_equal(s.vertices, t.vertices)
        assert_array_almost_equal(s.vertices, t.vertices)

        theta = np.asarray([0, 0., 180.])

        r = s.rotate(theta, unit='deg')

        l2r = surf.get_sphere_left_right_mapping(s, r)
        l2r_expected = [0, 1, 2, 6, 5, 4, 3, 11, 10, 9, 8, 7, 15, 14, 13, 12,
                       16, 19, 18, 17, 21, 20, 23, 22, 26, 25, 24]

        assert_array_equal(l2r, np.asarray(l2r_expected))


        sides_facing = 'apism'
        for side_facing in sides_facing:
            l, r = surf.reposition_hemisphere_pairs(s + 10., t + (-10.),
                                              side_facing)

            m = surf.merge(l, r)

            # not sure at the moment why medial rotation
            # messes up - but leave for now
            eps = 666 if side_facing == 'm' else .001
            assert_true((abs(m.center_of_mass) < eps).all())


    @with_tempfile('.nii', 'test_vol')
    def test_volgeom(self, temp_fn):
        sz = (17, 71, 37, 73) # size of 4-D 'brain volume'
        d = 2. # voxel size
        xo, yo, zo = -6., -12., -20. # origin
        mx = np.identity(4, np.float) * d # affine transformation matrix
        mx[3, 3] = 1
        mx[0, 3] = xo
        mx[1, 3] = yo
        mx[2, 3] = zo

        vg = volgeom.VolGeom(sz, mx) # initialize volgeom

        eq_shape_nvoxels = {(17, 71, 37): (True, True),
                           (71, 17, 37, 1): (False, True),
                           (17, 71, 37, 2): (True, True),
                            (17, 71, 37, 73): (True, True),
                           (2, 2, 2): (False, False)}

        for other_sz, (eq_shape, eq_nvoxels) in eq_shape_nvoxels.iteritems():
            other_vg = volgeom.VolGeom(other_sz, mx)
            assert_equal(other_vg.same_shape(vg), eq_shape)
            assert_equal(other_vg.nvoxels_mask == vg.nvoxels_mask, eq_nvoxels)

        nv = sz[0] * sz[1] * sz[2] # number of voxels
        nt = sz[3] # number of time points
        assert_equal(vg.nvoxels, nv)

        # a couple of hard-coded test cases
        # last two are outside the volume
        linidxs = [0, 1, sz[2], sz[1] * sz[2], nv - 1, -1 , nv]
        subidxs = ([(0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0),
                    (sz[0] - 1, sz[1] - 1, sz[2] - 1)]
                   + [(sz[0], sz[1], sz[2])] * 2)

        xyzs = ([(xo, yo, zo), (xo, yo, zo + d), (xo, yo + d, zo),
                 (xo + d, yo, zo),
                 (xo + d * (sz[0] - 1), yo + d * (sz[1] - 1), zo + d * (sz[2] - 1))]
                + [(np.nan, np.nan, np.nan)] * 2)

        for i, linidx in enumerate(linidxs):
            lin = np.asarray([linidx])
            ijk = vg.lin2ijk(lin)


            ijk_expected = np.reshape(np.asarray(subidxs[i]), (1, 3))
            assert_array_almost_equal(ijk, ijk_expected)

            xyz = vg.lin2xyz(lin)

            xyz_expected = np.reshape(np.asarray(xyzs[i]), (1, 3))
            assert_array_almost_equal(xyz, xyz_expected)


        # check that some identities hold
        ab, bc, ac = vg.lin2ijk, vg.ijk2xyz, vg.lin2xyz
        ba, cb, ca = vg.ijk2lin, vg.xyz2ijk, vg.xyz2lin
        identities = [lambda x:ab(ba(x)),
                      lambda x:bc(cb(x)),
                      lambda x:ac(ca(x)),
                      lambda x:ba(ab(x)),
                      lambda x:cb(bc(x)),
                      lambda x:ca(ac(x)),
                      lambda x:bc(ab(ca(x))),
                      lambda x:ba(cb(ac(x)))]

        # 0=lin, 1=ijk, 2=xyz
        identities_input = [1, 2, 2, 0, 1, 0, 2, 0]

        # voxel indices to test
        linrange = [0, 1, sz[2], sz[1] * sz[2]] + range(0, nv, nv // 100)

        lin = np.reshape(np.asarray(linrange), (-1,))
        ijk = vg.lin2ijk(lin)
        xyz = vg.ijk2xyz(ijk)

        for j, identity in enumerate(identities):
            inp = identities_input[j]
            x = {0: lin,
                 1: ijk,
                 2: xyz}[inp]

            assert_array_equal(x, identity(x))

        # check that masking works
        assert_true(vg.contains_lin(lin).all())
        assert_false(vg.contains_lin(-lin - 1).any())

        assert_true(vg.contains_ijk(ijk).all())
        assert_false(vg.contains_ijk(-ijk - 1).any())


        # ensure that we have no rounding issues
        deltas = [-.51, -.49, 0., .49, .51]
        should_raise = [True, False, False, False, True]

        for delta, r in zip(deltas, should_raise):
            xyz_d = xyz + delta * d
            lin_d = vg.xyz2lin(xyz_d)

            if r:
                assert_raises(AssertionError,
                              assert_array_almost_equal, lin_d, lin)
            else:
                assert_array_almost_equal(lin_d, lin)


        # some I/O testing

        img = vg.get_empty_nifti_image()
        img.to_filename(temp_fn)

        assert_true(os.path.exists(temp_fn))

        vg2 = volgeom.from_any(img)
        vg3 = volgeom.from_any(temp_fn)

        assert_array_equal(vg.affine, vg2.affine)
        assert_array_equal(vg.affine, vg3.affine)

        assert_equal(vg.shape[:3], vg2.shape[:3], 0)
        assert_equal(vg.shape[:3], vg3.shape[:3], 0)

        assert_true(len('%s%r' % (vg, vg)) > 0)


    def test_volgeom_masking(self):
        maskstep = 5
        vg = volgeom.VolGeom((2 * maskstep, 2 * maskstep, 2 * maskstep), np.identity(4))

        mask = vg.get_empty_array()
        sh = vg.shape

        # mask a subset of the voxels
        rng = range(0, sh[0], maskstep)
        for i in rng:
            for j in rng:
                for k in rng:
                    mask[i, j, k] = 1

        # make a new volgeom instance
        vg = volgeom.VolGeom(vg.shape, vg.affine, mask)

        data = vg.get_masked_nifti_image(nt=1)
        msk = vg.get_masked_nifti_image()
        dset = fmri_dataset(data, mask=msk)
        vg_dset = volgeom.from_any(dset)

        # ensure that the mask is set properly and
        assert_equal(vg.nvoxels, vg.nvoxels_mask * maskstep ** 3)
        assert_equal(vg_dset, vg)

        dilates = range(0, 8, 2)
        nvoxels_masks = [] # keep track of number of voxels for each size
        for dilate in dilates:
            covers_full_volume = dilate * 2 >= maskstep * 3 ** .5 + 1

            # constr gets values: None, Sphere(0), 2, Sphere(2), ...
            for i, constr in enumerate([Sphere, lambda x:x if x else None]):
                dilater = constr(dilate)

                img_dilated = vg.get_masked_nifti_image(dilate=dilater)
                data = img_dilated.get_data()

                assert_array_equal(data, vg.get_masked_array(dilate=dilater))
                n = np.sum(data)

                # number of voxels in mask is increasing
                assert_true(all(n >= p for p in nvoxels_masks))

                # results should be identical irrespective of constr
                if i == 0:
                    # - first call with this value of dilate: has to be more
                    #   voxels than very previous dilation value, unless the
                    #   full volume is covered - then it can be equal too
                    # - every next call: ensure size matches
                    cmp = lambda x, y:(x >= y if covers_full_volume else x > y)
                    assert_true(all(cmp(n, p) for p in nvoxels_masks))
                    nvoxels_masks.append(n)
                else:
                    # same size as previous call
                    assert_equal(n, nvoxels_masks[-1])

                # if dilate is not None or zero, then it should
                # have selected all the voxels if the radius is big enough
                assert_equal(np.sum(data) == vg.nvoxels, covers_full_volume)



    def test_volsurf(self):
        vg = volgeom.VolGeom((50, 50, 50), np.identity(4))

        density = 40
        outer = surf.generate_sphere(density) * 25. + 25
        inner = surf.generate_sphere(density) * 20. + 25


        # increasingly select more voxels in 'grey matter'
        steps_start_stop = [(1, .5, .5), (5, .5, .5), (3, .3, .7),
                          (5, .3, .7), (5, 0., 1.), (10, 0., 1.)]


        mp = None
        expected_keys = set(range(density ** 2 + 2))
        selection_counter = []
        voxel_counter = []
        for sp, sa, so in steps_start_stop:
            vs = volsurf.VolSurfMaximalMapping(vg, outer, inner, (outer + inner) * .5, sp, sa, so)

            n2v = vs.get_node2voxels_mapping()

            if mp is None:
                mp = n2v

            assert_equal(expected_keys, set(n2v.keys()))

            counter = 0
            for k, v2pos in n2v.iteritems():
                for v, pos in v2pos.iteritems():
                    # should be close to grey matter

                    assert_true(-1. <= pos and pos <= 2.)
                    counter += 1

            selection_counter.append(counter)
            img = vs.voxel_count_nifti_image()

            voxel_counter.append(np.sum(img.get_data() > 0))

        # hard coded number of expected voxels
        selection_expected = [1602, 1602, 4618, 5298, 7867, 10801]
        assert_equal(selection_counter, selection_expected)

        voxel_expected = [1498, 1498, 4322, 4986, 7391, 10141]
        assert_equal(voxel_counter, voxel_expected)

        # check that string building works
        assert_true(len('%s%r' % (vs, vs)) > 0)


    def test_volsurf_surf_from_volume(self):
        aff = np.eye(4)
        aff[0, 0] = aff[1, 1] = aff[2, 2] = 3

        sh = (40, 40, 40)

        vg = volgeom.VolGeom(sh, aff)

        p = volsurf.from_volume(vg).intermediate_surface
        q = volsurf.VolumeBasedSurface(vg)

        centers = [0, 10, 10000, (-1, -1, -1), (5, 5, 5)]
        radii = [0, 10, 20, 100]

        for center in centers:
            for radius in radii:
                x = p.circlearound_n2d(center, radius)
                y = q.circlearound_n2d(center, radius)
                assert_equal(x, y)


    def test_volume_mask_dict(self):
        # also tests the outside_node_margin feature
        sh = (10, 10, 10)
        msk = np.zeros(sh)
        for i in xrange(0, sh[0], 2):
            msk[i, :, :] = 1

        vol_affine = np.identity(4)
        vol_affine[0, 0] = vol_affine[1, 1] = vol_affine[2, 2] = 2

        vg = volgeom.VolGeom(sh, vol_affine, mask=msk)

        density = 10

        outer = surf.generate_sphere(density) * 10. + 5
        inner = surf.generate_sphere(density) * 5. + 5

        intermediate = outer * .5 + inner * .5
        xyz = intermediate.vertices

        radius = 50

        outside_node_margins = [None, 0, 100., np.inf, True]
        expected_center_count = [87] * 2 + [intermediate.nvertices] * 3
        for k, outside_node_margin in enumerate(outside_node_margins):

            sel = surf_voxel_selection.run_voxel_selection(radius, vg, inner,
                                outer, outside_node_margin=outside_node_margin)
            assert_equal(intermediate, sel.source)
            assert_equal(len(sel.keys()), expected_center_count[k])
            assert_true(set(sel.aux_keys()).issubset(set(['center_distances',
                                                    'grey_matter_position'])))

            msk_lin = msk.ravel()
            sel_msk_lin = sel.get_mask().ravel()
            for i in xrange(vg.nvoxels):
                if msk_lin[i]:
                    src = sel.target2nearest_source(i)
                    assert_false((src is None) ^ (sel_msk_lin[i] == 0))

                    if src is None:
                        continue

                    # index of node nearest to voxel i
                    src_anywhere = sel.target2nearest_source(i,
                                            fallback_euclidean_distance=True)

                    # coordinates of node nearest to voxel i
                    xyz_src = xyz[src_anywhere]

                    # coordinates of voxel i
                    xyz_trg = vg.lin2xyz(np.asarray([i]))

                    # distance between node nearest to voxel i, and voxel i
                    # this should be the smallest distancer
                    d = volgeom.distance(np.reshape(xyz_src, (1, 3)), xyz_trg)


                    # distances between all nodes and voxel i
                    ds = volgeom.distance(xyz, xyz_trg)

                    # order of the distances
                    is_ds = np.argsort(ds.ravel())

                    # go over all the nodes
                    # require that the node is in the volume
                    # mask

                    # index of node nearest to voxel i
                    ii = np.argmin(ds)

                    xyz_min = xyz[ii]
                    lin_min = vg.xyz2lin([xyz_min])




                    # linear index of voxel that contains xyz_src
                    lin_src = vg.xyz2lin(np.reshape(xyz_src, (1, 3)))

                    # when using multi-core support,
                    # pickling and unpickling can reduce the precision
                    # a little bit, causing rounding errors
                    eps = 1e-14

                    delta = np.abs(ds[ii] - d)
                    assert_false(delta > eps and ii in sel and
                                 i in sel[ii] and
                                 vg.contains_lin(lin_min))


    def test_surf_voxel_selection(self):
        vol_shape = (10, 10, 10)
        vol_affine = np.identity(4)
        vol_affine[0, 0] = vol_affine[1, 1] = vol_affine[2, 2] = 5

        vg = volgeom.VolGeom(vol_shape, vol_affine)

        density = 10

        outer = surf.generate_sphere(density) * 25. + 15
        inner = surf.generate_sphere(density) * 20. + 15

        vs = volsurf.VolSurfMaximalMapping(vg, outer, inner)

        nv = outer.nvertices

        # select under variety of parameters
        # parameters are distance metric (dijkstra or euclidean),
        # radius, and number of searchlight  centers
        params = [('d', 1., 10), ('d', 1., 50), ('d', 1., 100), ('d', 2., 100),
                  ('e', 2., 100), ('d', 2., 100), ('d', 20, 100),
                  ('euclidean', 5, None), ('dijkstra', 10, None)]

        # function that indicates for which parameters the full test is run
        test_full = lambda x:len(x[0]) > 1 or x[2] == 100

        expected_labs = ['grey_matter_position',
                         'center_distances']

        voxcount = []
        tested_double_features = False
        for param in params:
            distance_metric, radius, ncenters = param
            srcs = range(0, nv, nv // (ncenters or nv))
            sel = surf_voxel_selection.voxel_selection(vs, radius,
                                            source_surf_nodes=srcs,
                                            distance_metric=distance_metric)

            # see how many voxels were selected
            vg = sel.volgeom
            datalin = np.zeros((vg.nvoxels, 1))

            mp = sel
            for k, idxs in mp.iteritems():
                if idxs is not None:
                    datalin[idxs] = 1

            voxcount.append(np.sum(datalin))

            if test_full(param):
                assert_equal(np.sum(datalin), np.sum(sel.get_mask()))

                assert_true(len('%s%r' % (sel, sel)) > 0)

                # see if voxels containing inner and outer
                # nodes were selected
                for sf in [inner, outer]:
                    for k, idxs in mp.iteritems():
                        xyz = np.reshape(sf.vertices[k, :], (1, 3))
                        linidx = vg.xyz2lin(xyz)

                        # only required if xyz is actually within the volume
                        assert_equal(linidx in idxs, vg.contains_lin(linidx))

                # check that it has all the attributes
                labs = sel.aux_keys()

                assert_true(all([lab in labs for lab in expected_labs]))


                if externals.exists('h5py'):
                    # some I/O testing
                    fd, fn = tempfile.mkstemp('.h5py', 'test'); os.close(fd)
                    h5save(fn, sel)

                    sel2 = h5load(fn)
                    os.remove(fn)

                    assert_equal(sel, sel2)
                else:
                    sel2 = sel

                # check that mask is OK even after I/O
                assert_array_equal(sel.get_mask(), sel2.get_mask())


                # test I/O with surfaces
                # XXX the @tempfile decorator only supports a single filename
                #     hence this method does not use it
                fd, outerfn = tempfile.mkstemp('outer.asc', 'test'); os.close(fd)
                fd, innerfn = tempfile.mkstemp('inner.asc', 'test'); os.close(fd)
                fd, volfn = tempfile.mkstemp('vol.nii', 'test'); os.close(fd)

                surf.write(outerfn, outer, overwrite=True)
                surf.write(innerfn, inner, overwrite=True)

                img = sel.volgeom.get_empty_nifti_image()
                img.to_filename(volfn)

                sel3 = surf_voxel_selection.run_voxel_selection(radius, volfn, innerfn,
                                outerfn, source_surf_nodes=srcs,
                                distance_metric=distance_metric)

                outer4 = surf.read(outerfn)
                inner4 = surf.read(innerfn)
                vsm4 = vs = volsurf.VolSurfMaximalMapping(vg, inner4, outer4)

                # check that two ways of voxel selection match
                sel4 = surf_voxel_selection.voxel_selection(vsm4, radius,
                                                    source_surf_nodes=srcs,
                                                    distance_metric=distance_metric)

                assert_equal(sel3, sel4)

                os.remove(outerfn)
                os.remove(innerfn)
                os.remove(volfn)


                # compare sel3 with other selection results
                # NOTE: which voxels are precisely selected by sel can be quite
                #       off from those in sel3, as writing the surfaces imposes
                #       rounding errors and the sphere is very symmetric, which
                #       means that different neighboring nodes are selected
                #       to select a certain number of voxels.
                sel3cmp_difference_ratio = [(sel, .2), (sel4, 0.)]
                for selcmp, ratio in sel3cmp_difference_ratio:
                    nunion = ndiff = 0

                    for k in selcmp.keys():
                        p = set(sel3.get(k))
                        q = set(selcmp.get(k))
                        nunion += len(p.union(q))
                        ndiff += len(p.symmetric_difference(q))

                    assert_true(float(ndiff) / float(nunion) <= ratio)

                # check searchlight call
                # as of late Aug 2012, this is with the fancy query engine
                # as implemented by Yarik

                mask = sel.get_mask()
                keys = None if ncenters is None else sel.keys()

                dset_data = np.reshape(np.arange(vg.nvoxels), vg.shape)
                dset_img = nb.Nifti1Image(dset_data, vg.affine)
                dset = fmri_dataset(samples=dset_img, mask=mask)

                qe = queryengine.SurfaceVerticesQueryEngine(sel,
                                    # you can optionally add additional
                                    # information about each near-disk-voxels
                                    add_fa=['center_distances',
                                            'grey_matter_position'])

                # test i/o ensuring that when loading it is still trained
                if externals.exists('h5py'):
                    fd, qefn = tempfile.mkstemp('qe.hdf5', 'test'); os.close(fd)
                    h5save(qefn, qe)
                    qe = h5load(qefn)
                    os.remove(qefn)


                assert_false('ERROR' in repr(qe))   #  to check if repr works
                voxelcounter = _Voxel_Count_Measure()
                searchlight = Searchlight(voxelcounter, queryengine=qe, roi_ids=keys, nproc=1,
                                          enable_ca=['roi_feature_ids', 'roi_center_ids'])
                sl_dset = searchlight(dset)

                selected_count = sl_dset.samples[0, :]
                mp = sel
                for i, k in enumerate(sel.keys()):
                    # check that number of selected voxels matches
                    assert_equal(selected_count[i], len(mp[k]))


                assert_equal(searchlight.ca.roi_center_ids, sel.keys())

                assert_array_equal(sl_dset.fa['center_ids'], qe.ids)

                # check nearest node is *really* the nearest node

                allvx = sel.get_targets()
                intermediate = outer * .5 + inner * .5

                for vx in allvx:
                    nearest = sel.target2nearest_source(vx)

                    xyz = intermediate.vertices[nearest, :]
                    sqsum = np.sum((xyz - intermediate.vertices) ** 2, 1)

                    idx = np.argmin(sqsum)
                    assert_equal(idx, nearest)

                if not tested_double_features:           # test only once
                    # see if we have multiple features for the same voxel, we would get them all
                    dset1 = dset.copy()
                    dset1.fa['dset'] = [1]
                    dset2 = dset.copy()
                    dset2.fa['dset'] = [2]
                    dset_ = hstack((dset1, dset2), 'drop_nonunique')
                    dset_.sa = dset1.sa
                    #dset_.a.imghdr = dset1.a.imghdr
                    assert_true('imghdr' in dset_.a.keys())
                    assert_equal(dset_.a['imghdr'].value, dset1.a['imghdr'].value)
                    roi_feature_ids = searchlight.ca.roi_feature_ids
                    sl_dset_ = searchlight(dset_)
                    # and we should get twice the counts
                    assert_array_equal(sl_dset_.samples, sl_dset.samples * 2)

                    # compare old and new roi_feature_ids
                    assert(len(roi_feature_ids) == len(searchlight.ca.roi_feature_ids))
                    nfeatures = dset.nfeatures
                    for old, new in zip(roi_feature_ids,
                                        searchlight.ca.roi_feature_ids):
                        # each new ids should comprise of old ones + (old + nfeatures)
                        # since we hstack'ed two datasets
                        assert_array_equal(np.hstack([(x, x + nfeatures) for x in old]),
                                           new)
                    tested_double_features = True

        # check whether number of voxels were selected is as expected
        expected_voxcount = [22, 93, 183, 183, 183, 183, 183, 183, 183]

        assert_equal(voxcount, expected_voxcount)

    def test_h5support(self):
        sh = (20, 20, 20)
        msk = np.zeros(sh)
        for i in xrange(0, sh[0], 2):
            msk[i, :, :] = 1
        vg = volgeom.VolGeom(sh, np.identity(4), mask=msk)

        density = 20

        outer = surf.generate_sphere(density) * 10. + 5
        inner = surf.generate_sphere(density) * 5. + 5

        intermediate = outer * .5 + inner * .5
        xyz = intermediate.vertices

        radius = 50

        backends = ['native', 'hdf5']

        for i, backend in enumerate(backends):
            if backend == 'hdf5' and not externals.exists('h5py'):
                continue

            sel = surf_voxel_selection.run_voxel_selection(radius, vg, inner,
                            outer, results_backend=backend)

            if i == 0:
                sel0 = sel
            else:
                assert_equal(sel0, sel)

    def test_agreement_surface_volume(self):
        '''test agreement between volume-based and surface-based
        searchlights when using euclidean measure'''

        #import runner
        def sum_ds(ds):
            return np.sum(ds)

        radius = 3

        # make a small dataset with a mask
        sh = (10, 10, 10)
        msk = np.zeros(sh)
        for i in xrange(0, sh[0], 2):
            msk[i, :, :] = 1
        vg = volgeom.VolGeom(sh, np.identity(4), mask=msk)

        # make an image
        nt = 6
        img = vg.get_masked_nifti_image(6)
        ds = fmri_dataset(img, mask=msk)


        # run the searchlight
        sl = sphere_searchlight(sum_ds, radius=radius)
        m = sl(ds)

        # now use surface-based searchlight
        v = volsurf.from_volume(ds)
        source_surf = v.intermediate_surface
        node_msk = np.logical_not(np.isnan(source_surf.vertices[:, 0]))

        # check that the mask matches with what we used earlier
        assert_array_equal(msk.ravel() + 0., node_msk.ravel() + 0.)

        source_surf_nodes = np.nonzero(node_msk)[0]

        sel = surf_voxel_selection.voxel_selection(v, float(radius),
                                        source_surf=source_surf,
                                        source_surf_nodes=source_surf_nodes,
                                        distance_metric='euclidean')

        qe = queryengine.SurfaceVerticesQueryEngine(sel)
        sl = Searchlight(sum_ds, queryengine=qe)
        r = sl(ds)

        # check whether they give the same results
        assert_array_equal(r.samples, m.samples)


    @with_tempfile('.h5py', 'qe')
    def test_surf_queryengine(self, qefn):
        s = surf.generate_plane((0, 0, 0), (0, 1, 0), (0, 0, 1), 4, 5)

        # add scond layer
        s2 = surf.merge(s, (s + (.01, 0, 0)))

        ds = Dataset(samples=np.arange(20)[np.newaxis],
                    fa=dict(node_indices=np.arange(39, 0, -2)))

        # add more features (with shared node indices)
        ds3 = hstack((ds, ds, ds))

        radius = 2.5

        # Note: sweepargs it not used to avoid re-generating the same
        #       surface and dataset multiple times.
        for distance_metric in ('euclidean', 'dijkstra', '<illegal>', None):
            builder = lambda: queryengine.SurfaceQueryEngine(s2, radius,
                                                             distance_metric)
            if distance_metric in ('<illegal>', None):
                assert_raises(ValueError, builder)
                continue

            qe = builder()

            # test i/o and ensure that the untrained instance is not trained
            if externals.exists('h5py'):
                fd, qefn = tempfile.mkstemp('qe.hdf5', 'test'); os.close(fd)
                h5save(qefn, qe)
                qe = h5load(qefn)
                os.remove(qefn)


            # untrained qe should give errors
            assert_raises(ValueError, lambda:qe.ids)
            assert_raises(ValueError, lambda:qe.query_byid(0))

            # node index out of bounds should give error
            ds_ = ds.copy()
            ds_.fa.node_indices[0] = 100
            assert_raises(ValueError, lambda: qe.train(ds_))

            # lack of node indices should give error
            ds_.fa.pop('node_indices')
            assert_raises(ValueError, lambda: qe.train(ds_))


            # train the qe
            qe.train(ds3)

            # test i/o and ensure that the loaded instance is trained
            if externals.exists('h5py'):
                h5save(qefn, qe)
                qe = h5load(qefn)

            for node in np.arange(-1, s2.nvertices + 1):
                if node < 0 or node >= s2.nvertices:
                    assert_raises(KeyError, lambda: qe.query_byid(node))
                    continue

                feature_ids = np.asarray(qe.query_byid(node))

                # node indices relative to ds
                base_ids = feature_ids[feature_ids < 20]

                # should have multiples of 20
                assert_equal(set(feature_ids),
                             set((base_ids[np.newaxis].T + \
                                            [0, 20, 40]).ravel()))



                node_indices = list(s2.circlearound_n2d(node,
                                    radius, distance_metric or 'dijkstra'))

                fa_indices = [fa_index for fa_index, node in
                                    enumerate(ds3.fa.node_indices)
                                    if node in node_indices]


                assert_equal(set(feature_ids), set(fa_indices))

            # smoke tests
            assert_true('SurfaceQueryEngine' in '%s' % qe)
            assert_true('SurfaceQueryEngine' in '%r' % qe)


    def test_surf_pairs(self):
        o, x, y = map(np.asarray, [(0, 0, 0), (0, 1, 0), (1, 0, 0)])
        d = np.asarray((0, 0, .1))
        n = 10
        s1 = surf.generate_plane(o, x, y, n, n)
        s2 = surf.generate_plane(o + d, x, y, n, n)
        s = surf.merge(s1, s2)

        # try for small surface
        eps = .0000001
        pw = s.pairwise_near_nodes(.5)
        for i in xrange(n ** 2):
            d = pw.pop((i, i + 100))
            assert_array_almost_equal(d, .1)

        assert_true(len(pw) == 0)

        pw = s.pairwise_near_nodes(.5)
        for i in xrange(n ** 2):
            d = pw.pop((i, i + 100))
            assert_array_almost_equal(d, .1)

        assert_true(len(pw) == 0)

        # bigger one
        pw = s.pairwise_near_nodes(1.4)
        for i in xrange(n ** 2):
            p, q = i // n, i % n
            offsets = sum(([] if q == 0 else [-1],
                         [] if q == n - 1 else [+1],
                         [] if p == 0 else [-n],
                         [] if p == n - 1 else [n],
                         [0]), [])
            for offset in offsets:
                ii = i + offset + n ** 2
                d = pw.pop((i, ii))

            assert_true((d < .5) ^ (offset > 0))

        assert_true(len(pw) == 0)

    @with_tempfile('surf.surf.gii', 'surftest')
    def test_surf_gifti(self, fn):
            # From section 14.4 in GIFTI Surface Data Format Version 1.0
            # (with some adoptions)

            test_data = '''<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE GIFTI SYSTEM "http://www.nitrc.org/frs/download.php/1594/gifti.dtd">
<GIFTI
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:noNamespaceSchemaLocation="http://www.nitrc.org/frs/download.php/1303/GIFTI_Caret.xsd"
  Version="1.0"
  NumberOfDataArrays="2">
<MetaData>
  <MD>
    <Name><![CDATA[date]]></Name>
    <Value><![CDATA[Thu Nov 15 09:05:22 2007]]></Value>
  </MD>
</MetaData>
<LabelTable/>
<DataArray Intent="NIFTI_INTENT_POINTSET"
  DataType="NIFTI_TYPE_FLOAT32"
  ArrayIndexingOrder="RowMajorOrder"
  Dimensionality="2"
  Dim0="4"
  Dim1="3"
  Encoding="ASCII"
  Endian="LittleEndian"
  ExternalFileName=""
  ExternalFileOffset="">
<CoordinateSystemTransformMatrix>
  <DataSpace><![CDATA[NIFTI_XFORM_TALAIRACH]]></DataSpace>
  <TransformedSpace><![CDATA[NIFTI_XFORM_TALAIRACH]]></TransformedSpace>
  <MatrixData>
    1.000000 0.000000 0.000000 0.000000
    0.000000 1.000000 0.000000 0.000000
    0.000000 0.000000 1.000000 0.000000
    0.000000 0.000000 0.000000 1.000000
  </MatrixData>
</CoordinateSystemTransformMatrix>
<Data>
  10.5 0 0
  0 20.5 0
  0 0 30.5
  0 0 0
</Data>
</DataArray>
<DataArray Intent="NIFTI_INTENT_TRIANGLE"
  DataType="NIFTI_TYPE_INT32"
  ArrayIndexingOrder="RowMajorOrder"
  Dimensionality="2"
  Dim0="4"
  Dim1="3"
  Encoding="ASCII"
  Endian="LittleEndian"
  ExternalFileName="" ExternalFileOffset="">
<Data>
0 1 2
1 2 3
0 1 3
0 2 3
</Data>
</DataArray>
</GIFTI>'''

            with open(fn, 'w') as f:
                f.write(test_data)

            # test I/O
            s = surf.read(fn)
            surf.write(fn, s)
            s = surf.read(fn)

            v = np.zeros((4, 3))
            v[0, 0] = 10.5
            v[1, 1] = 20.5
            v[2, 2] = 30.5

            f = np.asarray([[0, 1, 2], [1, 2, 3], [0, 1, 3], [0, 2, 3]],
                            dtype=np.int32)

            assert_array_equal(s.vertices, v)
            assert_array_equal(s.faces, f)




class _Voxel_Count_Measure(Measure):
    # used to check voxel selection results
    is_trained = True
    def __init__(self, **kwargs):
        Measure.__init__(self, **kwargs)

    def _call(self, dset):
        return dset.nfeatures


def suite():  # pragma: no cover
    """Create the suite"""
    return unittest.makeSuite(SurfTests)

if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_surfing_afni
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA surface searchlight functions specific for
handling AFNI datasets"""


import numpy as np

import os
import tempfile

from mvpa2.testing import *

from mvpa2.support.nibabel import afni_niml, afni_niml_dset, afni_niml_roi, \
                                                surf, afni_suma_spec
from mvpa2.datasets import niml
from mvpa2.datasets.base import Dataset


class SurfTests(unittest.TestCase):
    """Test for AFNI I/O together with surface-based stuff

    NNO Aug 2012

    'Ground truth' is whatever output is returned by the implementation
    as of mid-Aug 2012"""

    def _get_rng(self):
        keys = [(17 * i ** 5 + 78234745 * i + 8934) % (2 ** 32 - 1)
                        for i in xrange(624)]
        keys = np.asanyarray(keys, dtype=np.uint32)
        rng = np.random.RandomState()
        rng.set_state(('MT19937', keys, 0))
        return rng

    def test_afni_niml(self):
        # just a bunch of tests

        ps = afni_niml._partial_string

        assert_equal(ps("", 0, 0), "")
        assert_equal(ps("ab", 0, 0), "")
        assert_equal(ps("abcdefghij", 0, 0), "")
        assert_equal(ps("", 2, 0), "")
        assert_equal(ps("ab", 2, 0), "")
        assert_equal(ps("abcdefghij", 2, 0), "")
        assert_equal(ps("", 0, 1), "")
        assert_equal(ps("ab", 0, 1), " ... b")
        assert_equal(ps("abcdefghij", 0, 1), " ... j")
        assert_equal(ps("", 2, 1), "")
        assert_equal(ps("ab", 2, 1), "")
        assert_equal(ps("abcdefghij", 2, 1), " ... j")
        assert_equal(ps("", 0, 100), "")
        assert_equal(ps("ab", 0, 100), "ab")
        assert_equal(ps("abcdefghij", 0, 100), "abcdefghij")
        assert_equal(ps("", 2, 100), "")
        assert_equal(ps("ab", 2, 100), "")
        assert_equal(ps("abcdefghij", 2, 100), "cdefghij")



        data = np.asarray([[1347506771, 1347506772],
                       [1347506773, 1347506774]],
                      dtype=np.int32)

        fmt_data_reprs = dict(text='1347506771 1347506772\n1347506773 1347506774',
                         binary='SRQPTRQPURQPVRQP',
                         base64='U1JRUFRSUVBVUlFQVlJRUA==')

        minimal_niml_struct = [{'dset_type': 'Node_Bucket',
                               'name': 'AFNI_dataset',
                               'ni_form': 'ni_group',
                               'nodes': [{'data': data,
                                          'data_type': 'Node_Bucket_data',
                                          'name': 'SPARSE_DATA',
                                          'ni_dimen': '2',
                                          'ni_type': '2*int32'},
                                         {'atr_name': 'COLMS_LABS',
                                          'data': 'col_0;col_1',
                                          'name': 'AFNI_atr',
                                          'ni_dimen': '1',
                                          'ni_type': 'String'}]}]


        def _eq(p, q):
            # helper function: equality for both arrays and other things
            return np.all(p == q) if type(p) is np.ndarray else p == q

        for fmt, data_repr in fmt_data_reprs.iteritems():
            s = afni_niml.rawniml2string(minimal_niml_struct, fmt)
            d = afni_niml.string2rawniml(s)

            # ensure data was converted properly

            for k, v in minimal_niml_struct[0].iteritems():
                if k == 'nodes':
                    # at least in one of the data
                    for node in v:
                        for kk, vv in node.iteritems():
                            # at least one of the data fields should have a value matching
                            # that from the expected converted value
                            dvals = [d[0]['nodes'][i].get(kk, None) for i in xrange(len(v))]
                            assert_true(any([_eq(vv, dval) for dval in dvals]))

                elif k != 'name':
                    # check header was properly converted
                    assert_true(('%s="%s"' % (k, v)).encode() in s)


            # check that if we remove some important information, then parsing fails
            important_keys = ['ni_form', 'ni_dimen', 'ni_type']

            for k in important_keys:
                s_bad = s.replace(k.encode(), b'foo')
                assert_raises((KeyError, ValueError), afni_niml.string2rawniml, s_bad)

            # adding garbage at the beginning or end should fail the parse
            garbage = "GARBAGE".encode()
            assert_raises((KeyError, ValueError), afni_niml.string2rawniml, s + garbage)
            assert_raises((KeyError, ValueError), afni_niml.string2rawniml, garbage + s)


    @with_tempfile('.niml.dset', 'dset')
    def test_afni_niml_dset_with_2d_strings(self, fn):
        # test for 2D arrays with strings. These are possibly SUMA-incompatible
        # but should still be handled properly for i/o.
        # Addresses https://github.com/PyMVPA/PyMVPA/issues/163 (#163)
        samples = np.asarray([[1, 2, 3], [4, 5, 6]])
        labels = np.asarray(map(list, ['abcd', 'efgh']))
        idxs = np.asarray([np.arange(10, 14), np.arange(20, 24)])

        ds = Dataset(samples, sa=dict(labels=labels, idxs=idxs))

        for fmt in ('binary', 'text', 'base64'):
            niml.write(fn, ds, fmt)

            ds_ = niml.read(fn)

            assert_array_equal(ds.samples, ds_.samples)

            for sa_key in ds.sa.keys():
                v = ds.sa[sa_key].value
                v_ = ds_.sa[sa_key].value
                assert_array_equal(v, v_)


    @with_tempfile('.niml.dset', 'dset')
    def test_afni_niml_dset(self, fn):
        sz = (100, 45) # dataset size
        rng = self._get_rng() # generate random data

        expected_vals = {(0, 0):-2.13856 , (sz[0] - 1, sz[1] - 1):-1.92434,
                         (sz[0], sz[1] - 1):None, (sz[0] - 1, sz[1]):None,
                         sz:None}

        # test for different formats in which the data is stored
        fmts = ['text', 'binary', 'base64']

        # also test for different datatypes
        tps = [np.int32, np.int64, np.float32, np.float64]

        # generated random data
        data = rng.normal(size=sz)

        # set labels for samples, and set node indices
        labels = ['lab_%d' % round(rng.uniform() * 1000)
                        for _ in xrange(sz[1])]
        node_indices = np.argsort(rng.uniform(size=(sz[0],)))
        node_indices = np.reshape(node_indices, (sz[0], 1))


        eps = .00001

        # test I/O
        # depending on the mode we do different tests (but on the same data)
        modes = ['normal', 'skipio', 'sparse2full']

        for fmt in fmts:
            for tp in tps:
                for mode in modes:
                    # make a dataset
                    dset = dict(data=np.asarray(data, tp),
                                labels=labels,
                                node_indices=node_indices)
                    dset_keys = dset.keys()

                    if mode == 'skipio':
                        # try conversion to/from raw NIML
                        # do not write to disk
                        r = afni_niml_dset.dset2rawniml(dset)
                        s = afni_niml.rawniml2string(r)
                        r2 = afni_niml.string2rawniml(s)
                        dset2 = afni_niml_dset.rawniml2dset(r2)[0]

                    else:
                        # write and read from disk
                        afni_niml_dset.write(fn, dset, fmt)
                        dset2 = afni_niml_dset.read(fn)
                        os.remove(fn)

                    # data in dset and dset2 should be identical
                    for k in dset_keys:
                        # general idea is to test whether v is equal to v2
                        v = dset[k]
                        v2 = dset2[k]

                        if k == 'data':
                            if mode == 'sparse2full':
                                # test the sparse2full feature
                                # this changes the order of the data over columns
                                # so we skip testing whether dset2 is equal to dset
                                nfull = 2 * sz[0]

                                dset3 = afni_niml_dset.sparse2full(dset2,
                                                            pad_to_node=nfull)

                                assert_equal(dset3['data'].shape[0], nfull)

                                idxs = dset['node_indices'][:, 0]
                                idxs3 = dset3['node_indices'][:, 0]
                                vbig = np.zeros((nfull, sz[1]))
                                vbig[idxs, :] = v[np.arange(sz[0]), :]
                                v = vbig
                                v2 = dset3['data'][idxs3, :]
                            else:
                                # check that data is as expected
                                for pos, val in expected_vals.iteritems():
                                    if val is None:
                                        assert_raises(IndexError, lambda x:x[pos], v2)
                                    else:
                                        val2 = np.asarray(val, tp)
                                        assert_true(abs(v2[pos] - val2) < eps)
                        if type(v) is list:
                            assert_equal(v, v2)
                        else:
                            eps_dec = 4
                            if mode != 'sparse2full' or k == 'data':
                                assert_array_almost_equal(v, v2, eps_dec)

    @with_tempfile('.niml.dset', 'dset')
    def test_niml(self, fn):
        d = dict(data=np.random.normal(size=(10, 2)),
              node_indices=np.arange(10),
              stats=['none', 'Tstat(2)'],
              labels=['foo', 'bar'])
        a = niml.from_niml(d)
        b = niml.to_niml(a)

        afni_niml_dset.write(fn, b)
        bb = afni_niml_dset.read(fn)
        cc = niml.from_niml(bb)

        os.remove(fn)

        for dset in (a, cc):
            assert_equal(list(dset.sa['labels']), d['labels'])
            assert_equal(list(dset.sa['stats']), d['stats'])
            assert_array_equal(np.asarray(dset.fa['node_indices']).ravel(),
                               d['node_indices'])

            eps_dec = 4
            assert_array_almost_equal(dset.samples, d['data'].transpose(),
                                                                    eps_dec)

        # some more tests to ensure that the order of elements is ok
        # (row first or column first)

        d = np.arange(10).reshape((5, -1)) + .5
        ds = Dataset(d)

        writers = [niml.write, afni_niml_dset.write]
        for i, writer in enumerate(writers):
            for form in ('text', 'binary', 'base64'):
                if i == 0:
                    writer(fn, ds, form=form)
                else:
                    writer(fn, dict(data=d.transpose()), form=form)

                x = afni_niml_dset.read(fn)
                assert_array_equal(x['data'], d.transpose())


    @with_tempfile('.niml.dset', 'dset')
    def test_niml_dset_voxsel(self, fn):
        if not externals.exists('nibabel'):
            return

        # This is actually a bit of an integration test.
        # It tests storing and retrieving searchlight results.
        # Imports are inline here so that it does not mess up the header
        # and makes the other unit tests more modular
        # XXX put this in a separate file?
        from mvpa2.misc.surfing import volgeom, surf_voxel_selection, queryengine
        from mvpa2.measures.searchlight import Searchlight
        from mvpa2.support.nibabel import surf
        from mvpa2.measures.base import Measure
        from mvpa2.datasets.mri import fmri_dataset

        class _Voxel_Count_Measure(Measure):
            # used to check voxel selection results
            is_trained = True
            def __init__(self, dtype, **kwargs):
                Measure.__init__(self, **kwargs)
                self.dtype = dtype

            def _call(self, dset):
                return self.dtype(dset.nfeatures)

        sh = (20, 20, 20)
        vg = volgeom.VolGeom(sh, np.identity(4))

        density = 20

        outer = surf.generate_sphere(density) * 10. + 5
        inner = surf.generate_sphere(density) * 5. + 5

        intermediate = outer * .5 + inner * .5
        xyz = intermediate.vertices

        radius = 50

        sel = surf_voxel_selection.run_voxel_selection(radius, vg, inner, outer)
        qe = queryengine.SurfaceVerticesQueryEngine(sel)

        for dtype in (int, float):
            sl = Searchlight(_Voxel_Count_Measure(dtype), queryengine=qe)

            ds = fmri_dataset(vg.get_empty_nifti_image(1))
            r = sl(ds)

            niml.write(fn, r)
            rr = niml.read(fn)

            os.remove(fn)

            assert_array_equal(r.samples, rr.samples)


    def test_niml_dset_stack(self):
        values = map(lambda x:np.random.normal(size=x), [(10, 3), (10, 4), (10, 5)])
        indices = [[0, 1, 2], [3, 2, 1, 0], None]

        dsets = []
        for v, i in zip(values, indices):
            dset = Dataset(v)
            if not i is None:
                dset.fa['node_indices'] = i
            dsets.append(dset)


        dset = niml.hstack(dsets)
        assert_equal(dset.nfeatures, 12)
        assert_equal(dset.nsamples, 10)
        indices = np.asarray([ 0, 1, 2, 6, 5, 4, 3, 7, 8, 9, 10, 11])
        assert_array_equal(dset.fa['node_indices'], indices)

        dset = niml.hstack(dsets, 10)
        dset = niml.hstack(dsets, 10) # twice to ensure not overwriting
        assert_equal(dset.nfeatures, 30)
        indices = np.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
                              13, 12, 11, 10, 14, 15, 16, 17, 18, 19,
                              20, 21, 22, 23, 24, 25, 26, 27, 28, 29])
        assert_array_equal(dset.fa['node_indices'], indices)

        assert_true(np.all(dset[:, 4].samples == 0))
        assert_array_equal(dset[:, 10:14].samples, dsets[1].samples)

        # If not enough space it should raise an error
        stacker = (lambda x: niml.hstack(dsets, x))
        assert_raises(ValueError, stacker, 2)

        # If sparse then with no padding it should fail
        dsets[0].fa.node_indices[0] = 3
        assert_raises(ValueError, stacker, None)

        # Using an illegal node index should raise an error
        dsets[1].fa.node_indices[0] = 666
        assert_raises(ValueError, stacker, 10)

    @with_tempfile('.niml.roi', 'dset')
    def test_afni_niml_roi(self, fn):
        payload = """# <Node_ROI
#  ni_type = "SUMA_NIML_ROI_DATUM"
#  ni_dimen = "5"
#  self_idcode = "XYZ_QlRYtdSyHmNr39qZWxD0wQ"
#  domain_parent_idcode = "XYZ_V_Ug6er2LCNoLy_OzxPsZg"
#  Parent_side = "no_side"
#  Label = "myroi"
#  iLabel = "12"
#  Type = "2"
#  ColPlaneName = "ROI.-.CoMminfl"
#  FillColor = "0.525490 0.043137 0.231373 1.000000"
#  EdgeColor = "0.000000 0.000000 1.000000 1.000000"
#  EdgeThickness = "2"
# >
 1 4 1 42946
 1 4 10 42946 42947 43062 43176 43289 43401 43512 43513 43623 43732
 1 4 8 43732 43623 43514 43404 43293 43181 43068 42954
 3 4 9 42954 42953 42952 42951 42950 42949 42948 42947 42946
 4 1 14 43063 43064 43065 43066 43067 43177 43178 43179 43180 43290 43291 43292 43402 43403
# </Node_ROI>"""

        with open(fn, 'w') as f:
            f.write(payload)

        rois = afni_niml_roi.read(fn)

        assert_equal(len(rois), 1)
        roi = rois[0]

        expected_keys = ['ni_type', 'ColPlaneName', 'iLabel', 'Parent_side',
                       'EdgeColor', 'Label', 'edges', 'ni_dimen',
                       'self_idcode', 'EdgeThickness', 'Type', 'areas',
                       'domain_parent_idcode', 'FillColor']

        assert_equal(set(roi.keys()), set(expected_keys))

        assert_equal(roi['Label'], 'myroi')
        assert_equal(roi['iLabel'], 12)

        # check edges
        arr = np.asarray
        expected_edges = [arr([42946]),
                          arr([42946, 42947, 43062, 43176, 43289, 43401,
                               43512, 43513, 43623, 43732]),
                          arr([43732, 43623, 43514, 43404, 43293, 43181,
                               43068, 42954]),
                          arr([42954, 42953, 42952, 42951, 42950, 42949,
                               42948, 42947, 42946])]

        for i in xrange(4):
            assert_array_equal(roi['edges'][i], expected_edges[i])

        # check nodes
        expected_nodes = [arr([43063, 43064, 43065, 43066, 43067, 43177, 43178,
                            43179, 43180, 43290, 43291, 43292, 43402, 43403])]

        assert_equal(len(roi['areas']), 1)
        assert_array_equal(roi['areas'][0], expected_nodes[0])


        # check mapping
        m = afni_niml_roi.read_mapping(rois)
        assert_equal(m.keys(), ['myroi'])

        unique_nodes = np.unique(expected_nodes[0])
        assert_array_equal(m['myroi'], unique_nodes)


    @with_tempfile()
    def test_afni_suma_spec(self, temp_dir):

        # XXX this function generates quite a few temporary files,
        #     which are removed at the end.
        #     the decorator @with_tempfile seems unsuitable as it only
        #     supports a single temporary file

        # make temporary directory
        os.mkdir(temp_dir)

        # generate surfaces
        inflated_surf = surf.generate_plane((0, 0, 0), (0, 1, 0), (0, 0, 1),
                                                    10, 10)
        white_surf = inflated_surf + 1.

        # helper function
        _tmp = lambda x:os.path.join(temp_dir, x)


        # filenames for surfaces and spec file
        inflated_fn = _tmp('_lh_inflated.asc')
        white_fn = _tmp('_lh_white.asc')
        spec_fn = _tmp('lh.spec')

        spec_dir = os.path.split(spec_fn)[0]

        # generate SUMA-like spec dictionary
        white = dict(SurfaceFormat='ASCII',
            EmbedDimension='3',
            SurfaceType='FreeSurfer',
            SurfaceName=white_fn,
            Anatomical='Y',
            LocalCurvatureParent='SAME',
            LocalDomainParent='SAME',
            SurfaceState='smoothwm')

        inflated = dict(SurfaceFormat='ASCII',
            EmbedDimension='3',
            SurfaceType='FreeSurfer',
            SurfaceName=inflated_fn,
            Anatomical='N',
            LocalCurvatureParent=white_fn,
            LocalDomainParent=white_fn,
            SurfaceState='inflated')

        # make SurfaceSpec object
        spec = afni_suma_spec.SurfaceSpec([white], directory=spec_dir)
        spec.add_surface(inflated)

        # test __str__ and __repr__
        assert_true('SurfaceSpec instance with 2 surfaces'
                        ', 2 states ' in '%s' % spec)
        assert_true(('%r' % spec).startswith('SurfaceSpec'))

        # test finding surfaces
        inflated_ = spec.find_surface_from_state('inflated')
        assert_equal([(1, inflated)], inflated_)

        empty = spec.find_surface_from_state('unknown')
        assert_equal(empty, [])

        # test .same_states
        minimal = afni_suma_spec.SurfaceSpec([dict(SurfaceState=s)
                                            for s in ('smoothwm', 'inflated')])
        assert_true(spec.same_states(minimal))
        assert_false(spec.same_states(afni_suma_spec.SurfaceSpec(dict())))

        # test 'smart' surface file matching
        assert_equal(spec.get_surface_file('smo'), white_fn)
        assert_equal(spec.get_surface_file('inflated'), inflated_fn)
        assert_equal(spec.get_surface_file('this should be None'), None)

        # test i/o
        spec.write(spec_fn)
        spec_ = afni_suma_spec.from_any(spec_fn)

        # prepare for another (right-hemisphere) spec file
        lh_spec = spec
        rh_spec_fn = spec_fn.replace('lh', 'rh')

        rh_inflated_fn = _tmp(os.path.split(inflated_fn)[1].replace('_lh',
                                                                    '_rh'))
        rh_white_fn = _tmp(os.path.split(white_fn)[1].replace('_lh',
                                                              '_rh'))
        rh_spec_fn = _tmp('rh.spec')

        rh_white = dict(SurfaceFormat='ASCII',
            EmbedDimension='3',
            SurfaceType='FreeSurfer',
            SurfaceName=rh_white_fn,
            Anatomical='Y',
            LocalCurvatureParent='SAME',
            LocalDomainParent='SAME',
            SurfaceState='smoothwm')

        rh_inflated = dict(SurfaceFormat='ASCII',
            EmbedDimension='3',
            SurfaceType='FreeSurfer',
            SurfaceName=rh_inflated_fn,
            Anatomical='N',
            LocalCurvatureParent=rh_white_fn,
            LocalDomainParent=rh_white_fn,
            SurfaceState='inflated')

        rh_spec = afni_suma_spec.SurfaceSpec([rh_white], directory=spec_dir)
        rh_spec.add_surface(rh_inflated)

        # write files
        all_temp_fns = [spec_fn, rh_spec_fn]
        for fn, s in [(rh_inflated_fn, inflated_surf),
                      (rh_white_fn, white_surf),
                      (inflated_fn, inflated_surf),
                      (white_fn, white_surf)]:
            surf.write(fn, s)
            all_temp_fns.append(fn)

        # test adding views
        added_specs = afni_suma_spec.hemi_pairs_add_views((lh_spec, rh_spec),
                                                          'inflated', '.asc')

        for hemi, added_spec in zip(('l', 'r'), added_specs):
            states = ['smoothwm', 'inflated'] + ['CoM%sinflated' % i
                                                    for i in 'msiap']
            assert_equal(states, [s['SurfaceState']
                                  for s in added_specs[0].surfaces])
            all_temp_fns.extend([s['SurfaceName']
                                 for s in added_spec.surfaces])

        # test combining specs (bh=both hemispheres)
        bh_spec = afni_suma_spec.combine_left_right(added_specs)

        # test merging specs (mh=merged hemispheres)
        mh_spec, mh_surfs = afni_suma_spec.merge_left_right(bh_spec)

        assert_equal([s['SurfaceState'] for s in mh_spec.surfaces],
                    ['smoothwm'] + ['CoM%sinflated' % i for i in 'msiap'])




def suite():  # pragma: no cover
    """Create the suite"""
    return unittest.makeSuite(SurfTests)

if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_surfing_voxelselection
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA surface searchlight voxel selection"""

from mvpa2.testing import *
skip_if_no_external('nibabel')

import numpy as np
from numpy.testing.utils import assert_array_almost_equal, assert_array_equal, \
    assert_raises

import nibabel as nb

import os
import tempfile

from mvpa2.testing import  reseed_rng
from mvpa2.testing.datasets import datasets

from mvpa2 import cfg
from mvpa2.base import externals
from mvpa2.datasets import Dataset
from mvpa2.measures.base import Measure
from mvpa2.datasets.mri import fmri_dataset

from mvpa2.support.nibabel import surf
from mvpa2.misc.surfing import surf_voxel_selection, queryengine, volgeom, \
                                volsurf
from mvpa2.misc.surfing.volume_mask_dict import VolumeMaskDictionary
from mvpa2.misc.surfing import volume_mask_dict

from mvpa2.measures.searchlight import Searchlight
from mvpa2.misc.surfing.queryengine import SurfaceVerticesQueryEngine, \
                                           SurfaceVoxelsQueryEngine, \
                                           disc_surface_queryengine

from mvpa2.measures.base import Measure, \
        TransferMeasure, RepeatedMeasure, CrossValidation
from mvpa2.clfs.smlr import SMLR
from mvpa2.generators.partition import OddEvenPartitioner
from mvpa2.mappers.fx import mean_sample
from mvpa2.misc.io.base import SampleAttributes
from mvpa2.mappers.detrend import poly_detrend
from mvpa2.mappers.zscore import zscore
from mvpa2.misc.neighborhood import Sphere, IndexQueryEngine
from mvpa2.clfs.gnb import GNB

if externals.exists('h5py'):
    from mvpa2.base.hdf5 import h5save, h5load


class SurfVoxelSelectionTests(unittest.TestCase):

    def test_voxel_selection(self):
        '''Compare surface and volume based searchlight'''

        '''
        Tests to see whether results are identical for surface-based
        searchlight (just one plane; Euclidean distnace) and volume-based
        searchlight.

        Note that the current value is a float; if it were int, it would
        specify the number of voxels in each searchlight'''

        radius = 10.

        '''Define input filenames'''
        epi_fn = os.path.join(pymvpa_dataroot, 'bold.nii.gz')
        maskfn = os.path.join(pymvpa_dataroot, 'mask.nii.gz')

        '''
        Use the EPI datafile to define a surface.
        The surface has as many nodes as there are voxels
        and is parallel to the volume 'slice'
        '''
        vg = volgeom.from_any(maskfn, mask_volume=True)

        aff = vg.affine
        nx, ny, nz = vg.shape[:3]

        '''Plane goes in x and y direction, so we take these vectors
        from the affine transformation matrix of the volume'''
        plane = surf.generate_plane(aff[:3, 3], aff[:3, 0], aff[:3, 1],
                                    nx, ny)



        '''
        Simulate pial and white matter as just above and below
        the central plane
        '''
        normal_vec = aff[:3, 2]
        outer = plane + normal_vec
        inner = plane + -normal_vec

        '''
        Combine volume and surface information
        '''
        vsm = volsurf.VolSurfMaximalMapping(vg, outer, inner)

        '''
        Run voxel selection with specified radius (in mm), using
        Euclidean distance measure
        '''
        surf_voxsel = surf_voxel_selection.voxel_selection(vsm, radius,
                                                    distance_metric='e')

        '''Define the measure'''

        # run_slow=True would give an actual cross-validation with meaningful
        # accuracies. Because this is a unit-test only the number of voxels
        # in each searchlight is tested.
        run_slow = False

        if run_slow:
            meas = CrossValidation(GNB(), OddEvenPartitioner(),
                                   errorfx=lambda p, t: np.mean(p == t))
            postproc = mean_sample
        else:
            meas = _Voxel_Count_Measure()
            postproc = lambda x:x

        '''
        Surface analysis: define the query engine, cross validation,
        and searchlight
        '''
        surf_qe = SurfaceVerticesQueryEngine(surf_voxsel)
        surf_sl = Searchlight(meas, queryengine=surf_qe, postproc=postproc)


        '''
        new (Sep 2012): also test 'simple' queryengine wrapper function
        '''

        surf_qe2 = disc_surface_queryengine(radius, maskfn, inner, outer,
                                            plane, volume_mask=True,
                                            distance_metric='euclidean')
        surf_sl2 = Searchlight(meas, queryengine=surf_qe2,
                               postproc=postproc)


        '''
        Same for the volume analysis
        '''
        element_sizes = tuple(map(abs, (aff[0, 0], aff[1, 1], aff[2, 2])))
        sph = Sphere(radius, element_sizes=element_sizes)
        kwa = {'voxel_indices': sph}

        vol_qe = IndexQueryEngine(**kwa)
        vol_sl = Searchlight(meas, queryengine=vol_qe, postproc=postproc)


        '''The following steps are similar to start_easy.py'''
        attr = SampleAttributes(os.path.join(pymvpa_dataroot,
                                'attributes_literal.txt'))

        mask = surf_voxsel.get_mask()

        dataset = fmri_dataset(samples=os.path.join(pymvpa_dataroot,
                                                    'bold.nii.gz'),
                               targets=attr.targets,
                               chunks=attr.chunks,
                               mask=mask)

        if run_slow:
            # do chunkswise linear detrending on dataset

            poly_detrend(dataset, polyord=1, chunks_attr='chunks')

            # zscore dataset relative to baseline ('rest') mean
            zscore(dataset, chunks_attr='chunks', param_est=('targets', ['rest']))

        # select class face and house for this demo analysis
        # would work with full datasets (just a little slower)
        dataset = dataset[np.array([l in ['face', 'house']
                                    for l in dataset.sa.targets],
                                    dtype='bool')]

        '''Apply searchlight to datasets'''
        surf_dset = surf_sl(dataset)
        surf_dset2 = surf_sl2(dataset)
        vol_dset = vol_sl(dataset)

        surf_data = surf_dset.samples
        surf_data2 = surf_dset2.samples
        vol_data = vol_dset.samples

        assert_array_equal(surf_data, surf_data2)
        assert_array_equal(surf_data, vol_data)

    def test_voxel_selection_alternative_calls(self):
        # Tests a multitude of different searchlight calls
        # that all should yield exactly the same results.
        #
        # Calls differ by whether the arguments are filenames
        # or data objects, whether values are specified explicityly
        # or set to the default implicitly (using None).
        # and by different calls to run the voxel selection.
        #
        # This method does not test for mask functionality.

        # define the volume
        vol_shape = (10, 10, 10, 3)
        vol_affine = np.identity(4)
        vol_affine[0, 0] = vol_affine[1, 1] = vol_affine[2, 2] = 5



        # four versions: array, nifti image, file name, fmri dataset
        volarr = np.ones(vol_shape)
        volimg = nb.Nifti1Image(volarr, vol_affine)
        # There is a detected problem with elderly NumPy's (e.g. 1.6.1
        # on precise on travis) leading to segfaults while operating
        # on memmapped volumes being forwarded to pprocess.
        # Thus just making it compressed volume for those cases
        suf = '.gz' \
            if externals.exists('pprocess') and externals.versions['numpy'] < '1.6.2' \
            else ''
        fd, volfn = tempfile.mkstemp('vol.nii' + suf, 'test'); os.close(fd)
        volimg.to_filename(volfn)
        volds = fmri_dataset(volfn)

        fd, volfngz = tempfile.mkstemp('vol.nii.gz', 'test'); os.close(fd)
        volimg.to_filename(volfngz)
        voldsgz = fmri_dataset(volfngz)


        # make the surfaces
        sphere_density = 10

        # two versions: Surface and file name
        outer = surf.generate_sphere(sphere_density) * 25. + 15
        inner = surf.generate_sphere(sphere_density) * 20. + 15
        intermediate = inner * .5 + outer * .5
        nv = outer.nvertices

        fd, outerfn = tempfile.mkstemp('outer.asc', 'test'); os.close(fd)
        fd, innerfn = tempfile.mkstemp('inner.asc', 'test'); os.close(fd)
        fd, intermediatefn = tempfile.mkstemp('intermediate.asc', 'test'); os.close(fd)

        for s, fn in zip([outer, inner, intermediate],
                         [outerfn, innerfn, intermediatefn]):
            surf.write(fn, s, overwrite=True)

        # searchlight radius (in mm)
        radius = 10.

        # dataset used to run searchlight on
        ds = fmri_dataset(volfn)

        # simple voxel counter (run for each searchlight position)
        m = _Voxel_Count_Measure()

        # number of voxels expected in each searchlight
        r_expected = np.array([[18, 9, 10, 9, 9, 9, 9, 10, 9,
                                 9, 9, 9, 11, 11, 11, 11, 10,
                                10, 10, 9, 10, 11, 9, 10, 10,
                                8, 7, 8, 8, 8, 9, 10, 12, 12,
                                11, 7, 7, 8, 5, 9, 11, 11, 12,
                                12, 9, 5, 8, 7, 7, 12, 12, 13,
                                12, 12, 7, 7, 8, 5, 9, 12, 12,
                                13, 11, 9, 5, 8, 7, 7, 11, 12,
                                12, 11, 12, 10, 10, 11, 9, 11,
                                12, 12, 12, 12, 16, 13, 16, 16,
                                16, 17, 15, 17, 17, 17, 16, 16,
                                16, 18, 16, 16, 16, 16, 18, 16]])



        params = dict(intermediate_=(intermediate, intermediatefn, None),
                      center_nodes_=(None, range(nv)),
                      volume_=(volimg, volfn, volds, volfngz, voldsgz),
                      surf_src_=('filename', 'surf'),
                      volume_mask_=(None, True, 0, 2),
                      call_method_=("qe", "rvs", "gam"))

        combis = _cartprod(params) # compute all possible combinations
        combistep = 17  #173
                        # some fine prime number to speed things up
                        # if this value becomes too big then not all
                        # cases are covered
                        # the unit test tests itself whether all values
                        # occur at least once

        tested_params = dict()
        def val2str(x):
            return '%r:%r' % (type(x), x)

        for i in xrange(0, len(combis), combistep):
            combi = combis[i]

            intermediate_ = combi['intermediate_']
            center_nodes_ = combi['center_nodes_']
            volume_ = combi['volume_']
            surf_src_ = combi['surf_src_']
            volume_mask_ = combi['volume_mask_']
            call_method_ = combi['call_method_']


            # keep track of which values were used -
            # so that this unit test tests itself

            for k in combi.keys():
                if not k in tested_params:
                    tested_params[k] = set()
                tested_params[k].add(val2str(combi[k]))

            if surf_src_ == 'filename':
                s_i, s_m, s_o = inner, intermediate, outer
            elif surf_src_ == 'surf':
                s_i, s_m, s_o = innerfn, intermediatefn, outerfn
            else:
                raise ValueError('this should not happen')

            if call_method_ == "qe":
                # use the fancy query engine wrapper
                qe = disc_surface_queryengine(radius,
                        volume_, s_i, s_o, s_m,
                        source_surf_nodes=center_nodes_,
                        volume_mask=volume_mask_)
                sl = Searchlight(m, queryengine=qe)
                r = sl(ds).samples

            elif call_method_ == 'rvs':
                # use query-engine but build the
                # ingredients by hand
                vg = volgeom.from_any(volume_,
                                      volume_mask_)
                vs = volsurf.VolSurfMaximalMapping(vg, s_i, s_o)
                sel = surf_voxel_selection.voxel_selection(
                        vs, radius, source_surf=s_m,
                        source_surf_nodes=center_nodes_)
                qe = SurfaceVerticesQueryEngine(sel)
                sl = Searchlight(m, queryengine=qe)
                r = sl(ds).samples

            elif call_method_ == 'gam':
                # build everything from the ground up
                vg = volgeom.from_any(volume_,
                                      volume_mask_)
                vs = volsurf.VolSurfMaximalMapping(vg, s_i, s_o)
                sel = surf_voxel_selection.voxel_selection(
                        vs, radius, source_surf=s_m,
                        source_surf_nodes=center_nodes_)
                mp = sel

                ks = sel.keys()
                nk = len(ks)
                r = np.zeros((1, nk))
                for i, k in enumerate(ks):
                    r[0, i] = len(mp[k])

            # check if result is as expected
            assert_array_equal(r_expected, r)

        # clean up
        all_fns = [volfn, volfngz, outerfn, innerfn, intermediatefn]
        map(os.remove, all_fns)

        for k, vs in params.iteritems():
            if not k in tested_params:
                raise ValueError("Missing key: %r" % k)
            for v in vs:
                vstr = val2str(v)
                if not vstr in tested_params[k]:
                    raise ValueError("Missing value %r for %s" %
                                        (tested_params[k], k))


    def test_volsurf_projections(self):
        white = surf.generate_plane((0, 0, 0), (0, 1, 0), (0, 0, 1), 10, 10)
        pial = white + np.asarray([[1, 0, 0]])

        above = pial + np.asarray([[3, 0, 0]])
        vg = volgeom.VolGeom((10, 10, 10), np.eye(4))
        vs = volsurf.VolSurfMaximalMapping(vg, white, pial)

        dx = pial.vertices - white.vertices

        for s, w in ((white, 0), (pial, 1), (above, 4)):
            xyz = s.vertices
            ws = vs.surf_project_weights(True, xyz)
            delta = vs.surf_unproject_weights_nodewise(ws) - xyz
            assert_array_equal(delta, np.zeros((100, 3)))
            assert_true(np.all(w == ws))


        vs = volsurf.VolSurfMaximalMapping(vg, white, pial, nsteps=2)
        n2vs = vs.get_node2voxels_mapping()
        assert_equal(n2vs, dict((i, {i:0., i + 100:1.}) for i in xrange(100)))

        nd = 17
        ds_mm_expected = np.sum((above.vertices - pial.vertices[nd, :]) ** 2,
                                                                    1) ** .5
        ds_mm = vs.coordinates_to_grey_distance_mm(nd, above.vertices)
        assert_array_almost_equal(ds_mm_expected, ds_mm)

        ds_mm_nodewise = vs.coordinates_to_grey_distance_mm(True,
                                                            above.vertices)

        assert_array_equal(ds_mm_nodewise, np.ones((100,)) * 3)


    @with_tempfile('.h5py', 'voxsel')
    def test_surface_outside_volume_voxel_selection(self, fn):
        skip_if_no_external('h5py')
        from mvpa2.base.hdf5 import h5save, h5load
        vol_shape = (10, 10, 10, 1)
        vol_affine = np.identity(4)
        vg = volgeom.VolGeom(vol_shape, vol_affine)

        # make surfaces that are far away from all voxels
        # in the volume
        sphere_density = 4
        far = 10000.
        outer = surf.generate_sphere(sphere_density) * 10 + far
        inner = surf.generate_sphere(sphere_density) * 5 + far

        vs = volsurf.VolSurfMaximalMapping(vg, inner, outer)
        radii = [10., 10] # fixed and variable radii

        outside_node_margins = [0, far, True]
        for outside_node_margin in outside_node_margins:
            for radius in radii:
                selector = lambda:surf_voxel_selection.voxel_selection(vs,
                                            radius,
                                            outside_node_margin=outside_node_margin)

                if type(radius) is int and outside_node_margin is True:
                    assert_raises(ValueError, selector)
                else:
                    sel = selector()
                    if outside_node_margin is True:
                        # it should have all the keys, but they should
                        # all be empty
                        assert_array_equal(sel.keys(), range(inner.nvertices))
                        for k, v in sel.iteritems():
                            assert_equal(v, [])
                    else:
                        assert_array_equal(sel.keys(), [])

                    if outside_node_margin is True and \
                                 externals.versions['hdf5'] < '1.8.7':
                        raise SkipTest("Versions of hdf5 before 1.8.7 have "
                                                    "problems with empty arrays")

                    h5save(fn, sel)
                    sel_copy = h5load(fn)

                    assert_array_equal(sel.keys(), sel_copy.keys())
                    for k in sel.keys():
                        assert_equal(sel[k], sel_copy[k])

                    assert_equal(sel, sel_copy)



    def test_surface_voxel_query_engine(self):
        vol_shape = (10, 10, 10, 1)
        vol_affine = np.identity(4)
        vol_affine[0, 0] = vol_affine[1, 1] = vol_affine[2, 2] = 5
        vg = volgeom.VolGeom(vol_shape, vol_affine)

        # make the surfaces
        sphere_density = 10

        outer = surf.generate_sphere(sphere_density) * 25. + 15
        inner = surf.generate_sphere(sphere_density) * 20. + 15

        vs = volsurf.VolSurfMaximalMapping(vg, inner, outer)

        radius = 10

        for fallback, expected_nfeatures in ((True, 1000), (False, 183)):
            voxsel = surf_voxel_selection.voxel_selection(vs, radius)
            qe = SurfaceVoxelsQueryEngine(voxsel, fallback_euclidean_distance=fallback)

            # test i/o and ensure that the loaded instance is trained
            if externals.exists('h5py'):
                fd, qefn = tempfile.mkstemp('qe.hdf5', 'test'); os.close(fd)
                h5save(qefn, qe)
                qe = h5load(qefn)
                os.remove(qefn)


            m = _Voxel_Count_Measure()

            sl = Searchlight(m, queryengine=qe)

            data = np.random.normal(size=vol_shape)
            img = nb.Nifti1Image(data, vol_affine)
            ds = fmri_dataset(img)

            sl_map = sl(ds)

            counts = sl_map.samples

            assert_true(np.all(np.logical_and(5 <= counts, counts <= 18)))
            assert_equal(sl_map.nfeatures, expected_nfeatures)



    @reseed_rng()
    def test_surface_minimal_voxel_selection(self):
        # Tests 'minimal' voxel selection.
        # It assumes that 'maximal' voxel selection works (which is tested
        # in other unit tests)
        vol_shape = (10, 10, 10, 1)
        vol_affine = np.identity(4)
        vg = volgeom.VolGeom(vol_shape, vol_affine)

        # generate some surfaces,
        # and add some noise to them
        sphere_density = 10
        nvertices = sphere_density ** 2 + 2
        noise = np.random.uniform(size=(nvertices, 3))
        outer = surf.generate_sphere(sphere_density) * 5 + 8 + noise
        inner = surf.generate_sphere(sphere_density) * 3 + 8 + noise

        radii = [5., 20., 10] # note: no fixed radii at the moment

        # Note: a little outside margin is necessary
        # as otherwise there are nodes in the minimal case
        # that have no voxels associated with them

        for radius in radii:
            for output_modality in ('surface', 'volume'):
                for i, nvm in enumerate(('minimal', 'maximal')):
                    qe = disc_surface_queryengine(radius, vg, inner,
                                        outer, node_voxel_mapping=nvm,
                                        output_modality=output_modality)
                    voxsel = qe.voxsel

                    if i == 0:
                        keys_ = voxsel.keys()
                        voxsel_ = voxsel
                    else:
                        keys = voxsel.keys()
                        # minimal one has a subset
                        assert_equal(keys, keys_)

                        # and the subset is quite overlapping
                        assert_true(len(keys) * .90 < len(keys_))

                        for k in keys_:
                            x = set(voxsel_[k])
                            y = set(voxsel[k])

                            d = set.symmetric_difference(x, y)
                            r = float(len(d)) / 2 / len(x)
                            if type(radius) is float:
                                assert_equal(x - y, set())

                            # decent agreement in any case between the two sets
                            assert_true(r < .6)

    @reseed_rng()
    @with_tempfile('.h5py', 'voxsel')
    def test_queryengine_io(self, fn):
        skip_if_no_external('h5py')
        from mvpa2.base.hdf5 import h5save, h5load

        vol_shape = (10, 10, 10, 1)
        vol_affine = np.identity(4)
        vg = volgeom.VolGeom(vol_shape, vol_affine)

        # generate some surfaces,
        # and add some noise to them
        sphere_density = 10
        outer = surf.generate_sphere(sphere_density) * 5 + 8
        inner = surf.generate_sphere(sphere_density) * 3 + 8
        radius = 5.

        add_fa = ['center_distances', 'grey_matter_position']
        qe = disc_surface_queryengine(radius, vg, inner, outer,
                            add_fa=add_fa)
        ds = fmri_dataset(vg.get_masked_nifti_image())

        # the following is not really a strong requirement. XXX remove?
        assert_raises(ValueError, lambda: qe[qe.ids[0]])

        # check that after training it behaves well
        qe.train(ds)
        i = qe.ids[0]
        try:
            m = qe[i]
        except ValueError, e:
            raise AssertionError(
                'Failed to query %r from %r after training on %r. Exception was: %r'
                 % (i, qe, ds, e))

        assert_equal(qe[qe.ids[0]].samples[0, 0], 883)

        voxsel = qe.voxsel

        # store the original methods
        setstate_current = VolumeMaskDictionary.__dict__['__setstate__']
        reduce_current = VolumeMaskDictionary.__dict__['__reduce__']

        # try all combinations.
        # end with both set to False so that VolumeMaskDictionary is back
        # in its original state
        # XXX is manipulating class methods this way too dangerous?
        true_false_combis = [(i % 2 == 1, i // 2 == 0) for i in xrange(3, 7)]

        # try different ways to load volume mask dictionaries
        # first argument is filename, second argument is volume mask dictionary
        vmd_load_methods = [lambda f, vmd: h5load(f),
                            lambda f, vmd: volume_mask_dict.from_any(vmd),
                            lambda f, vmd: volume_mask_dict.from_any(f),
                            lambda f, vmd: vmd]
        for setstate_use_legacy, reduce_use_legacy in true_false_combis:
            reducer = VolumeMaskDictionary._reduce_legacy \
                            if reduce_use_legacy  \
                                else reduce_current
            VolumeMaskDictionary.__reduce__ = reducer

            setstater = VolumeMaskDictionary._setstate_legacy \
                            if setstate_use_legacy \
                            else setstate_current
            VolumeMaskDictionary.__setstate__ = setstater

            indices_stored = voxsel.__reduce__()[2][3]

            if reduce_use_legacy:
                assert_equal(type(indices_stored), dict)
                assert_equal(len(indices_stored), len(qe.ids))
            else:
                assert_equal(type(indices_stored), tuple)
                assert_equal(len(indices_stored), 3)
                for ix in indices_stored:
                    assert_equal(type(ix), np.ndarray)
            h5save(fn, qe)

            qe_copy = h5load(fn)

            if setstate_use_legacy and not reduce_use_legacy:
                assert_raises(AttributeError, lambda: qe_copy.ids)
                continue

            # ensure keys are the same
            assert_equal(qe.ids, qe_copy.ids)

            # ensure values are the same and that qe_copy is trained
            for id in qe.ids:
                assert_array_equal(qe[id].samples, qe_copy[id].samples)

            sel = qe.voxsel
            h5save(fn, sel)
            for vmd_load_method in vmd_load_methods:
                sel_copy = vmd_load_method(fn, sel)
                assert_equal(sel.aux_keys(), add_fa)
                expected_values = [1.13851869106, 1.03270423412] # smoke test
                for key, v in zip(add_fa, expected_values):
                    for id in qe.ids:
                        assert_array_equal(sel.get_aux(id, key), sel_copy.get_aux(id, key))

                    assert_array_almost_equal(sel.get_aux(qe.ids[0], key)[3], v)


    @with_tempfile('.h5py', 'voxsel')
    def test_surface_minimal_lowres_voxel_selection(self, fn):
        vol_shape = (4, 10, 10, 1)
        vol_affine = np.identity(4)
        vg = volgeom.VolGeom(vol_shape, vol_affine)


        # make surfaces that are far away from all voxels
        # in the volume
        sphere_density = 10
        radius = 10

        outer = surf.generate_plane((0, 0, 4), (0, .4, 0), (0, 0, .4), 14, 14)
        inner = outer + 2

        source = surf.generate_plane((0, 0, 4), (0, .8, 0), (0, 0, .8), 7, 7) + 1

        for i, nvm in enumerate(('minimal', 'minimal_lowres')):
            qe = disc_surface_queryengine(radius, vg, inner,
                                      outer, source,
                                      node_voxel_mapping=nvm)

            voxsel = qe.voxsel
            if i == 0:
                voxsel0 = voxsel
            else:
                assert_equal(voxsel.keys(), voxsel0.keys())
                for k in voxsel.keys():
                    p = voxsel[k]
                    q = voxsel0[k]

                    # require at least 60% agreement
                    delta = set.symmetric_difference(set(p), set(q))
                    assert_true(len(delta) < .8 * (len(p) + len(q)))

            if externals.exists('h5py'):
                from mvpa2.base.hdf5 import h5save, h5load

                h5save(fn, voxsel)
                voxsel_copy = h5load(fn)
                assert_equal(voxsel.keys(), voxsel_copy.keys())

                for id in qe.ids:
                    assert_array_equal(voxsel.get(id), voxsel_copy.get(id))




    @reseed_rng()
    def test_minimal_dataset(self):
        vol_shape = (10, 10, 10, 3)
        vol_affine = np.identity(4)
        vg = volgeom.VolGeom(vol_shape, vol_affine)

        data = np.random.normal(size=vol_shape)
        msk = np.ones(vol_shape[:3])
        msk[:, 1:-1:2, :] = 0

        ni_data = nb.Nifti1Image(data, vol_affine)
        ni_msk = nb.Nifti1Image(msk, vol_affine)

        ds = fmri_dataset(ni_data, mask=ni_msk)

        sphere_density = 20
        outer = surf.generate_sphere(sphere_density) * 10. + 5
        inner = surf.generate_sphere(sphere_density) * 7. + 5


        radius = 10
        sel = surf_voxel_selection.run_voxel_selection(radius, ds, inner, outer)


        sel_fids = set.union(*(set(sel[k]) for k in sel.keys()))

        ds_vox = map(tuple, ds.fa.voxel_indices)

        vg = sel.volgeom
        sel_vox = map(tuple, vg.lin2ijk(np.asarray(list(sel_fids))))


        fid_mask = np.asarray([v in sel_vox for v in ds_vox])
        assert_array_equal(fid_mask, sel.get_dataset_feature_mask(ds))

        # check if it raises errors
        ni_neg_msk = nb.Nifti1Image(1 - msk, vol_affine)
        neg_ds = fmri_dataset(ni_data, mask=ni_neg_msk) # inverted mask

        assert_raises(ValueError, sel.get_dataset_feature_mask, neg_ds)

        min_ds = sel.get_minimal_dataset(ds)
        assert_array_equal(min_ds.samples, ds[:, fid_mask].samples)


def _cartprod(d):
    '''makes a combinatorial explosion from a dictionary
    only combinations are made from values in tuples'''
    if not d:
        return [dict()]

    r = []
    k, vs = d.popitem()
    itervals = vs if type(vs) is tuple else [vs]
    for v in itervals:
        cps = _cartprod(d)
        for cp in cps:
            kv = {k:v}
            kv.update(cp)
            r.append(kv)
    d[k] = vs
    return r

class _Voxel_Count_Measure(Measure):
    # used to check voxel selection results
    is_trained = True
    def __init__(self, **kwargs):
        Measure.__init__(self, **kwargs)

    def _call(self, dset):
        return dset.nfeatures

def suite():  # pragma: no cover
    """Create the suite"""
    return unittest.makeSuite(SurfVoxelSelectionTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()

########NEW FILE########
__FILENAME__ = test_svdmapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA SVD mapper"""


import unittest
import numpy as np

from mvpa2.mappers.svd import SVDMapper
from mvpa2.testing import reseed_rng
from mvpa2.support.copy import deepcopy


class SVDMapperTests(unittest.TestCase):

    def setUp(self):
        # data: 40 sample feature line in 20d space (40x20; samples x features)
        self.ndlin = np.concatenate([np.arange(40)
                                        for i in range(20)]).reshape(20,-1).T

        # data: 10 sample feature line in 40d space
        #       (10x40; samples x features)
        self.largefeat = np.concatenate([np.arange(10)
                                        for i in range(40)]).reshape(40,-1).T


    def test_simple_svd(self):
        pm = SVDMapper()
        # train SVD
        pm.train(self.ndlin)

        self.assertEqual(pm.proj.shape, (20, 20))

        # now project data into PCA space
        p = pm.forward(self.ndlin)

        # only first eigenvalue significant
        self.assertTrue(pm.sv[:1] > 1.0)
        self.assertTrue((pm.sv[1:] < 0.0001).all())

        # only variance of first component significant
        var = p.var(axis=0)

       # test that only one component has variance
        self.assertTrue(var[:1] > 1.0)
        self.assertTrue((var[1:] < 0.0001).all())

        # check that the mapped data can be fully recovered by 'reverse()'
        pr = pm.reverse(p)

        self.assertEqual(pr.shape, (40,20))
        self.assertTrue(np.abs(pm.reverse(p) - self.ndlin).sum() < 0.0001)


    @reseed_rng()
    def test_more_svd(self):
        pm = SVDMapper()
        # train SVD
        pm.train(self.largefeat)

        # mixing matrix cannot be square
        self.assertEqual(pm.proj.shape, (40, 10))

        # only first singular value significant
        self.assertTrue(pm.sv[:1] > 10)
        self.assertTrue((pm.sv[1:] < 10).all())

        # now project data into SVD space
        p = pm.forward(self.largefeat)

        # only variance of first component significant
        var = p.var(axis=0)

        # test that only one component has variance
        self.assertTrue(var[:1] > 1.0)
        self.assertTrue((var[1:] < 0.0001).all())

        # check that the mapped data can be fully recovered by 'reverse()'
        rp = pm.reverse(p)
        self.assertEqual(rp.shape, self.largefeat.shape)
        self.assertTrue((np.round(rp) == self.largefeat).all())

        # copy mapper
        pm2 = deepcopy(pm)

        # now make new random data and do forward->reverse check
        data = np.random.normal(size=(98,40))
        data_f = pm.forward(data)

        self.assertEqual(data_f.shape, (98,10))

        data_r = pm.reverse(data_f)
        self.assertEqual(data_r.shape, (98,40))



def suite():  # pragma: no cover
    return unittest.makeSuite(SVDMapperTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_svm
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for SVM classifier"""

import numpy as np

from mvpa2.datasets import dataset_wizard

from mvpa2.testing import *
from mvpa2.testing.clfs import *
from mvpa2.testing.datasets import *

from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.datasets.miscfx import get_nsamples_per_attr
from mvpa2.clfs.meta import ProxyClassifier
from mvpa2.measures.base import CrossValidation

class SVMTests(unittest.TestCase):

#    @sweepargs(nl_clf=clfswh['non-linear', 'svm'] )
#    @sweepargs(nl_clf=clfswh['non-linear', 'svm'] )
    def test_multivariate(self):
        mv_perf = []
        mv_lin_perf = []
        uv_perf = []

        l_clf = clfswh['linear', 'svm'][0]
        nl_clf = clfswh['non-linear', 'svm'][0]

        #orig_keys = nl_clf.param._params.keys()
        #nl_param_orig = nl_clf.param._params.copy()

        # l_clf = LinearNuSVMC()

        # XXX ??? not sure what below meant and it is obsolete if
        # using SG... commenting out for now
        # for some reason order is not preserved thus dictionaries are not
        # the same any longer -- lets compare values
        #self.assertEqual([nl_clf.param._params[k] for k in orig_keys],
        #                     [nl_param_orig[k] for k in orig_keys],
        #   msg="New instance mustn't override values in previously created")
        ## and keys separately
        #self.assertEqual(set(nl_clf.param._params.keys()),
        #                     set(orig_keys),
        #   msg="New instance doesn't change set of parameters in original")

        # We must be able to deepcopy not yet trained SVMs now
        import mvpa2.support.copy as copy
        try:
            nl_clf.untrain()
            nl_clf_copy = copy.deepcopy(nl_clf)
        except:
            self.fail(msg="Failed to deepcopy not-yet trained SVM %s" % nl_clf)

        for i in xrange(20):
            train = pure_multivariate_signal( 20, 3 )
            test = pure_multivariate_signal( 20, 3 )

            # use non-linear CLF on 2d data
            nl_clf.train(train)
            p_mv = nl_clf.predict(test.samples)
            mv_perf.append(np.mean(p_mv==test.targets))

            # use linear CLF on 2d data
            l_clf.train(train)
            p_lin_mv = l_clf.predict(test.samples)
            mv_lin_perf.append(np.mean(p_lin_mv==test.targets))

            # use non-linear CLF on 1d data
            nl_clf.train(train[:, 0])
            p_uv = nl_clf.predict(test[:, 0].samples)
            uv_perf.append(np.mean(p_uv==test.targets))

        mean_mv_perf = np.mean(mv_perf)
        mean_mv_lin_perf = np.mean(mv_lin_perf)
        mean_uv_perf = np.mean(uv_perf)

        # non-linear CLF has to be close to perfect
        self.assertTrue( mean_mv_perf > 0.9 )
        # linear CLF cannot learn this problem!
        self.assertTrue( mean_mv_perf > mean_mv_lin_perf )
        # univariate has insufficient information
        self.assertTrue( mean_uv_perf < mean_mv_perf )


    # XXX for now works only with linear... think it through -- should
    #     work non-linear, shouldn't it?
    @sweepargs(clf=clfswh['svm', 'linear', '!regression', '!gnpp', '!meta'])
    @reseed_rng()
    def test_cper_class(self, clf):
        if not (clf.params.has_key('C')):
            # skip those without C
            return

        ds = datasets['uni2medium'].copy()
        ds__ = datasets['uni2medium'].copy()
        #
        # ballanced set
        # Lets add a bit of noise to drive classifier nuts. same
        # should be done for disballanced set
        ds__.samples = ds__.samples + \
                       0.5 * np.random.normal(size=(ds__.samples.shape))
        #
        # disballanced set
        # lets overpopulate label 0
        times = 20
        ds_ = ds[(range(ds.nsamples) + range(ds.nsamples//2) * times)]
        ds_.samples = ds_.samples + \
                      0.5 * np.random.normal(size=(ds_.samples.shape))
        spl = get_nsamples_per_attr(ds_, 'targets') #_.samplesperlabel
        #print ds_.targets, ds_.chunks

        cve = CrossValidation(clf, NFoldPartitioner(), enable_ca='stats')
        # on balanced
        e = cve(ds__)
        tpr_1 = cve.ca.stats.stats["TPR"][1]

        # on disbalanced
        e = cve(ds_)
        tpr_2 =  cve.ca.stats.stats["TPR"][1]

        # Set '1 C per label'
        # recreate cvte since previous might have operated on copies
        cve = CrossValidation(clf, NFoldPartitioner(),
                                          enable_ca='stats')
        oldC = clf.params.C
        # TODO: provide clf.params.C not with a tuple but dictionary
        #       with C per label (now order is deduced in a cruel way)
        ratio = np.sqrt(float(spl[ds_.UT[0]])/spl[ds_.UT[1]])
        clf.params.C = (-1/ratio, -1*ratio)
        try:
            # on disbalanced but with balanced C
            e_ = cve(ds_)
            # reassign C
            clf.params.C = oldC
        except:
            clf.params.C = oldC
            raise
        tpr_3 = cve.ca.stats.stats["TPR"][1]

        # Actual tests
        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(tpr_1 > 0.25,
                            msg="Without disballance we should have some "
                            "hits, but got TPR=%.3f" % tpr_1)

            self.assertTrue(tpr_2 < 0.25,
                            msg="With disballance we should have almost no "
                            "hits for minor, but got TPR=%.3f" % tpr_2)

            self.assertTrue(tpr_3 > 0.25,
                            msg="With disballanced data but ratio-based Cs "
                            "we should have some hits for minor, but got "
                            "TPR=%.3f" % tpr_3)



    def test_sillyness(self):
        """Test if we raise exceptions on incorrect specifications
        """

        if externals.exists('libsvm'):
            self.assertRaises(TypeError, libsvm.SVM, C=1.0, nu=2.3)
            self.assertRaises(TypeError, libsvm.SVM,  C=1.0, nu=2.3)
            self.assertRaises(TypeError, LinearNuSVMC, C=2.3)
            self.assertRaises(TypeError, LinearCSVMC, nu=2.3)

        if externals.exists('shogun'):
            self.assertRaises(TypeError, sg.SVM, C=1.0, nu=2.3)
            self.assertRaises(TypeError, sg.SVM, C=10, kernel_type='RBF',
                                  coef0=3)

    @sweepargs(clf=clfswh['svm', 'linear', '!meta', 'C_SVC'][:1])
    def test_C_on_int_dataset(self, clf):
        a = np.arange(8, dtype=np.int16).reshape(4,-1)
        a[0,0] = 322           # the value which would overflow
        self.assertTrue(np.isfinite(clf._get_default_c(a)))

def suite():  # pragma: no cover
    return unittest.makeSuite(SVMTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_svmkernels
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for new Kernel-based SVMs"""

import numpy as np
from time import time

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets
skip_if_no_external('shogun')

from mvpa2.kernels.base import CachedKernel
from mvpa2.kernels.sg import RbfSGKernel, LinearSGKernel

from mvpa2.misc.data_generators import normal_feature_dataset

from mvpa2.clfs.libsvmc.svm import SVM as lsSVM
from mvpa2.clfs.sg.svm import SVM as sgSVM

from mvpa2.generators.splitters import Splitter
from mvpa2.generators.partition import NFoldPartitioner
from mvpa2.measures.base import CrossValidation, TransferMeasure, ProxyMeasure
from mvpa2.mappers.fx import BinaryFxNode
from mvpa2.misc.errorfx import mean_mismatch_error


class SVMKernelTests(unittest.TestCase):

    @sweepargs(clf=[lsSVM(), sgSVM()])
    def test_basic_clf_train_predict(self, clf):
        d = datasets['uni4medium']
        clf.train(d)
        clf.predict(d)
        pass

    @reseed_rng()
    def test_cache_speedup(self):
        skip_if_no_external('shogun', ver_dep='shogun:rev', min_version=4455)

        ck = sgSVM(kernel=CachedKernel(kernel=RbfSGKernel(sigma=2)), C=1)
        sk = sgSVM(kernel=RbfSGKernel(sigma=2), C=1)

        cv_c = CrossValidation(ck, NFoldPartitioner())
        cv_s = CrossValidation(sk, NFoldPartitioner())

        #data = datasets['uni4large']
        P = 5000
        data = normal_feature_dataset(snr=2, perlabel=200, nchunks=10,
                                    means=np.random.randn(2, P), nfeatures=P)

        t0 = time()
        ck.params.kernel.compute(data)
        cachetime = time()-t0

        t0 = time()
        cached_err = cv_c(data)
        ccv_time = time()-t0

        t0 = time()
        norm_err = cv_s(data)
        ncv_time = time()-t0

        assert_almost_equal(np.asanyarray(cached_err),
                            np.asanyarray(norm_err))
        ok_(cachetime<ncv_time)
        ok_(ccv_time<ncv_time)
        #print 'Regular CV time: %s seconds'%ncv_time
        #print 'Caching time: %s seconds'%cachetime
        #print 'Cached CV time: %s seconds'%ccv_time

        speedup = ncv_time/(ccv_time+cachetime)
        #print 'Speedup factor: %s'%speedup

        # Speedup ideally should be 10, though it's not purely linear
        self.failIf(speedup < 2, 'Problem caching data - too slow!')

    def test_cached_kernel_different_datasets(self):
        skip_if_no_external('shogun', ver_dep='shogun:rev', min_version=4455)

        # Inspired by the problem Swaroop ran into
        k  = LinearSGKernel(normalizer_cls=False)
        k_ = LinearSGKernel(normalizer_cls=False)   # to be cached
        ck = CachedKernel(k_)

        clf = sgSVM(svm_impl='libsvm', kernel=k, C=-1)
        clf_ = sgSVM(svm_impl='libsvm', kernel=ck, C=-1)

        cvte = CrossValidation(clf, NFoldPartitioner())
        cvte_ = CrossValidation(clf_, NFoldPartitioner())

        postproc=BinaryFxNode(mean_mismatch_error, 'targets')
        te = ProxyMeasure(clf, postproc=postproc)
        te_ = ProxyMeasure(clf_, postproc=postproc)

        for r in xrange(2):
            ds1 = datasets['uni2medium']
            errs1 = cvte(ds1)
            ck.compute(ds1)
            ok_(ck._recomputed)
            errs1_ = cvte_(ds1)
            ok_(~ck._recomputed)
            assert_array_equal(errs1, errs1_)

            ds2 = datasets['uni3small']
            errs2 = cvte(ds2)
            ck.compute(ds2)
            ok_(ck._recomputed)
            errs2_ = cvte_(ds2)
            ok_(~ck._recomputed)
            assert_array_equal(errs2, errs2_)

            ssel = np.round(datasets['uni2large'].samples[:5, 0]).astype(int)
            te.train(datasets['uni3small'][::2])
            terr = np.asscalar(te(datasets['uni3small'][ssel]))
            te_.train(datasets['uni3small'][::2])
            terr_ = np.asscalar(te_(datasets['uni3small'][ssel]))
            ok_(~ck._recomputed)
            ok_(terr == terr_)

    def test_vstack_and_origids_issue(self):
        # That is actually what swaroop hit
        skip_if_no_external('shogun', ver_dep='shogun:rev', min_version=4455)

        # Inspired by the problem Swaroop ran into
        k  = LinearSGKernel(normalizer_cls=False)
        k_ = LinearSGKernel(normalizer_cls=False)   # to be cached
        ck = CachedKernel(k_)

        clf = sgSVM(svm_impl='libsvm', kernel=k, C=-1)
        clf_ = sgSVM(svm_impl='libsvm', kernel=ck, C=-1)

        cvte = CrossValidation(clf, NFoldPartitioner())
        cvte_ = CrossValidation(clf_, NFoldPartitioner())

        ds = datasets['uni2large'].copy(deep=True)
        ok_(~('orig_ids' in ds.sa))     # assure that there are None
        ck.compute(ds)                  # so we initialize origids
        ok_('origids' in ds.sa)
        ds2 = ds.copy(deep=True)
        ds2.samples = np.zeros(ds2.shape)
        from mvpa2.base.dataset import vstack
        ds_vstacked = vstack((ds2, ds))
        # should complaint now since there would not be unique
        # samples' origids
        if __debug__:
            assert_raises(ValueError, ck.compute, ds_vstacked)

        ds_vstacked.init_origids('samples')      # reset origids
        ck.compute(ds_vstacked)

        errs = cvte(ds_vstacked)
        errs_ = cvte_(ds_vstacked)
        # Following test would have failed since origids
        # were just ints, and then non-unique after vstack
        assert_array_equal(errs.samples, errs_.samples)

def suite():  # pragma: no cover
    return unittest.makeSuite(SVMKernelTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_testing
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for complementary unittest-ing tools"""

import numpy as np

from mvpa2.base.externals import versions
from mvpa2.testing.tools import *
from mvpa2.testing.sweep import *

import mvpa2.tests as mvtests

def test_assert_objectarray_equal():
    if versions['numpy'] < '1.4':
        raise SkipTest("Skipping because of known segfaults with numpy < 1.4")
    # explicit dtype so we could test with numpy < 1.6
    a = np.array([np.array([0, 1]), np.array(1)], dtype=object)
    b = np.array([np.array([0, 1]), np.array(1)], dtype=object)

    # they should be ok for both types of comparison
    for strict in True, False:
        # good with self
        assert_objectarray_equal(a, a, strict=strict)
        # good with a copy
        assert_objectarray_equal(a, a.copy(), strict=strict)
        # good while operating with an identical one
        # see http://projects.scipy.org/numpy/ticket/2117
        assert_objectarray_equal(a, b, strict=strict)

    # now check if we still fail for a good reason
    for value_equal, b in (
            (False, np.array(1)),
            (False, np.array([1])),
            (False, np.array([np.array([0, 1]), np.array((1, 2))], dtype=object)),
            (False, np.array([np.array([0, 1]), np.array(1.1)], dtype=object)),
            (True, np.array([np.array([0, 1]), np.array(1.0)], dtype=object)),
            (True, np.array([np.array([0, 1]), np.array(1, dtype=object)], dtype=object)),
            ):
        assert_raises(AssertionError, assert_objectarray_equal, a, b)
        if value_equal:
            # but should not raise for non-default strict=False
            assert_objectarray_equal(a, b, strict=False)
        else:
            assert_raises(AssertionError, assert_objectarray_equal, a, b, strict=False)

# Set of basic smoke tests for tests collectors/runners
def test_tests_run():
    ok_(len(mvtests.collect_unit_tests()) > 10)
    ok_(len(mvtests.collect_nose_tests()) > 10)
    ok_(len(mvtests.collect_test_suites(instantiate=False)) > 10)
    mvtests.run(limit=[])

@sweepargs(suffix=['', 'customsuffix'])
@sweepargs(prefix=['', 'customprefix'])
#@sweepargs(mkdir=(True, False))
def test_with_tempfile(suffix, prefix): #, mkdir):
    files = []

    @with_tempfile(suffix, prefix) #, mkdir=mkdir)
    def testf(f):
        assert_false(os.path.exists(f)) # not yet
        if suffix:
            assert_true(f.endswith(suffix))
        if prefix:
            assert_true(os.path.basename(f).startswith(prefix))
        #assert_true(os.path.isdir(f) == dir_)
        # make sure it is writable
        with open(f, 'w') as f_:
            f_.write('load')
            files.append(f)
        assert_true(os.path.exists(f)) # should be there
        # and we should be able to create a bunch of those with other suffixes
        with open(f + '1', 'w') as f_:
            f_.write('load')
            files.append(f + '1')

    testf()
    # now we need to figure out what file was actually
    assert_equal(len(files), 2)
    assert_false(os.path.exists(files[0]))
    assert_false(os.path.exists(files[1]))
########NEW FILE########
__FILENAME__ = test_transerror
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA classifier cross-validation"""

import unittest
import numpy as np

from mvpa2.support.copy import copy

from mvpa2.base.dataset import vstack
from mvpa2.datasets import Dataset
from mvpa2.base import externals, warning
from mvpa2.generators.partition import OddEvenPartitioner, NFoldPartitioner
from mvpa2.generators.base import Repeater
from mvpa2.generators.permutation import AttributePermutator
from mvpa2.generators.splitters import Splitter

from mvpa2.clfs.meta import MulticlassClassifier
from mvpa2.clfs.transerror import ConfusionMatrix, ConfusionBasedError
from mvpa2.measures.base import CrossValidation, TransferMeasure

from mvpa2.clfs.stats import MCNullDist

from mvpa2.misc.exceptions import UnknownStateError
from mvpa2.misc.errorfx import mean_mismatch_error
from mvpa2.mappers.fx import mean_sample, BinaryFxNode

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets
from mvpa2.testing.clfs import *

class ErrorsTests(unittest.TestCase):

    def test_confusion_matrix(self):
        data = np.array([1,2,1,2,2,2,3,2,1], ndmin=2).T
        reg = [1,1,1,2,2,2,3,3,3]
        regl = [1,2,1,2,2,2,3,2,1]
        correct_cm = [[2,0,1],[1,3,1],[0,0,1]]
        # Check if we are ok with any input type - either list, or np.array, or tuple
        for t in [reg, tuple(reg), list(reg), np.array(reg)]:
            for p in [regl, tuple(regl), list(regl), np.array(regl)]:
                cm = ConfusionMatrix(targets=t, predictions=p)
                # check table content
                self.assertTrue((cm.matrix == correct_cm).all())


        # Do a bit more thorough checking
        cm = ConfusionMatrix()
        self.assertRaises(ZeroDivisionError, lambda x:x.percent_correct, cm)
        """No samples -- raise exception"""

        cm.add(reg, regl)

        self.assertEqual(len(cm.sets), 1,
            msg="Should have a single set so far")
        self.assertEqual(cm.matrix.shape, (3,3),
            msg="should be square matrix (len(reglabels) x len(reglabels)")

        self.assertRaises(ValueError, cm.add, reg, np.array([1]))
        """ConfusionMatrix must complaint if number of samples different"""

        # check table content
        self.assertTrue((cm.matrix == correct_cm).all())

        # lets add with new labels (not yet known)
        cm.add(reg, np.array([1,4,1,2,2,2,4,2,1]))

        self.assertEqual(cm.labels, [1,2,3,4],
                             msg="We should have gotten 4th label")

        matrices = cm.matrices          # separate CM per each given set
        self.assertEqual(len(matrices), 2,
                             msg="Have gotten two splits")

        self.assertTrue((matrices[0].matrix + matrices[1].matrix == cm.matrix).all(),
                        msg="Total votes should match the sum across split CMs")

        # check pretty print
        # just a silly test to make sure that printing works
        self.assertTrue(len(cm.as_string(
            header=True, summary=True,
            description=True))>100)
        self.assertTrue(len(str(cm))>100)
        # and that it knows some parameters for printing
        self.assertTrue(len(cm.as_string(summary=True,
                                       header=False))>100)

        # lets check iadd -- just itself to itself
        cm += cm
        self.assertEqual(len(cm.matrices), 4, msg="Must be 4 sets now")

        # lets check add -- just itself to itself
        cm2 = cm + cm
        self.assertEqual(len(cm2.matrices), 8, msg="Must be 8 sets now")
        self.assertEqual(cm2.percent_correct, cm.percent_correct,
                             msg="Percent of corrrect should remain the same ;-)")

        self.assertEqual(cm2.error, 1.0-cm.percent_correct/100.0,
                             msg="Test if we get proper error value")


    def test_confusion_matrix_addition(self):
        """Test confusions addition inconsistent results (GH #51)

        Was fixed by deepcopying instead of copying in __add__
        """
        cm1 = ConfusionMatrix(sets=[[np.array((1,2)), np.array((1,2))]])
        cm2 = ConfusionMatrix(sets=[[np.array((3,2)), np.array((3,2))]])
        assert_array_equal(cm1.stats['P'], [1, 1])
        assert_array_equal(cm2.stats['P'], [1, 1])

        # actual bug scenario -- results would be different
        r1 = (cm1 + cm2).stats['P']
        r2 = (cm1 + cm2).stats['P']
        assert_array_equal(r1, r2)
        assert_array_equal(r1, [1, 2, 1])


    def test_degenerate_confusion(self):
        # We must not just puke -- some testing splits might
        # have just a single target label

        for orig in ([1], [1, 1], [0], [0, 0]):
            cm = ConfusionMatrix(targets=orig, predictions=orig, estimates=orig)

            scm = str(cm)
            self.assertTrue(cm.stats['ACC%'] == 100)


    def test_confusion_matrix_acc(self):
        reg  = [0,0,1,1]
        regl = [1,0,1,0]
        cm = ConfusionMatrix(targets=reg, predictions=regl)
        self.assertTrue('ACC%         50' in str(cm))
        skip_if_no_external('scipy')
        self.assertTrue(cm.stats['CHI^2'] == (0., 1.))


    def test_confusion_matrix_with_mappings(self):
        data = np.array([1,2,1,2,2,2,3,2,1], ndmin=2).T
        reg = [1,1,1,2,2,2,3,3,3]
        regl = [1,2,1,2,2,2,3,2,1]
        correct_cm = [[2,0,1], [1,3,1], [0,0,1]]
        lm = {'apple':1, 'orange':2, 'shitty apple':1, 'candy':3}
        cm = ConfusionMatrix(targets=reg, predictions=regl,
                             labels_map=lm)
        # check table content
        self.assertTrue((cm.matrix == correct_cm).all())
        # assure that all labels are somewhere listed ;-)
        s = str(cm)
        for l in lm.keys():
            self.assertTrue(l in s)

    def test_confusion_call(self):
        # Also tests for the consistency of the labels as
        # either provided or collected by ConfusionMatrix through its lifetime
        self.assertRaises(RuntimeError, ConfusionMatrix(), [1], [1])
        self.assertRaises(ValueError, ConfusionMatrix(labels=[2]), [1], [1])
        # Now lets test proper matrix and either we obtain the same
        t = ['ho', 'ho', 'ho', 'fa', 'fa', 'ho', 'ho']
        p = ['ho','ho', 'ho', 'ho', 'fa', 'fa', 'fa']
        cm1 = ConfusionMatrix(labels=['ho', 'fa'])
        cm2 = ConfusionMatrix(labels=['fa', 'ho'])
        assert_array_equal(cm1(p, t), [[3, 1], [2, 1]])
        assert_array_equal(cm2(p, t), [[1, 2], [1, 3]]) # reverse order of labels

        cm1_ = ConfusionMatrix(labels=['ho', 'fa'], sets=[(t,p)])
        assert_array_equal(cm1(p, t), cm1_.matrix) # both should be identical
        # Lets provoke "mother" CM to get to know more labels which could get ahead
        # of the known ones
        cm1.add(['ho', 'aa'], ['ho', 'aa'])
        # compare and cause recomputation so .__labels get reassigned
        assert_equal(cm1.labels, ['ho', 'fa', 'aa'])
        assert_array_equal(cm1(p, t), [[3, 1, 0], [2, 1, 0], [0, 0, 0]])
        assert_equal(len(cm1.sets), 1)  # just 1 must be known atm from above add
        assert_array_equal(cm1(p, t, store=True), [[3, 1, 0], [2, 1, 0], [0, 0, 0]])
        assert_equal(len(cm1.sets), 2)  # and now 2
        assert_array_equal(cm1(p + ['ho', 'aa'], t + ['ho', 'aa']), cm1.matrix)

    @sweepargs(l_clf=clfswh['linear', 'svm'])
    def test_confusion_based_error(self, l_clf):
        train = datasets['uni2medium']
        train = train[train.sa.train == 1]
        # to check if we fail to classify for 3 labels
        test3 = datasets['uni3medium']
        test3 = test3[test3.sa.train == 1]
        err = ConfusionBasedError(clf=l_clf)
        terr = TransferMeasure(l_clf, Splitter('train', attr_values=[1,1]),
                               postproc=BinaryFxNode(mean_mismatch_error,
                                                     'targets'))

        self.assertRaises(UnknownStateError, err, None)
        """Shouldn't be able to access the state yet"""

        l_clf.train(train)
        e, te = err(None), terr(train)
        te = np.asscalar(te)
        self.assertTrue(abs(e-te) < 1e-10,
            msg="ConfusionBasedError (%.2g) should be equal to TransferError "
                "(%.2g) on traindataset" % (e, te))

        # this will print nasty WARNING but it is ok -- it is just checking code
        # NB warnings are not printed while doing whole testing
        warning("Don't worry about the following warning.")
        if 'multiclass' in l_clf.__tags__:
            self.assertFalse(terr(test3) is None)

        # try copying the beast
        terr_copy = copy(terr)


    @sweepargs(l_clf=clfswh['linear', 'svm'])
    def test_null_dist_prob(self, l_clf):
        train = datasets['uni2medium']

        num_perm = 10
        permutator = AttributePermutator('targets', count=num_perm,
                                         limit='chunks')
        # define class to estimate NULL distribution of errors
        # use left tail of the distribution since we use MeanMatchFx as error
        # function and lower is better
        terr = TransferMeasure(
            l_clf,
            Repeater(count=2),
            postproc=BinaryFxNode(mean_mismatch_error, 'targets'),
            null_dist=MCNullDist(permutator,
                                 tail='left'))

        # check reasonable error range
        err = terr(train)
        self.assertTrue(np.mean(err) < 0.4)

        # Lets do the same for CVTE
        cvte = CrossValidation(l_clf, OddEvenPartitioner(),
            null_dist=MCNullDist(permutator,
                                 tail='left',
                                 enable_ca=['dist_samples']),
            postproc=mean_sample())
        cv_err = cvte(train)

        # check that the result is highly significant since we know that the
        # data has signal
        null_prob = np.asscalar(terr.ca.null_prob)

        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(null_prob <= 0.1,
                msg="Failed to check that the result is highly significant "
                    "(got %f) since we know that the data has signal"
                    % null_prob)

            self.assertTrue(np.asscalar(cvte.ca.null_prob) <= 0.1,
                msg="Failed to check that the result is highly significant "
                    "(got p(cvte)=%f) since we know that the data has signal"
                    % np.asscalar(cvte.ca.null_prob))

        # we should be able to access the actual samples of the distribution
        # yoh: why it is 3D really?
        # mih: because these are the distribution samples for the ONE error
        #      collapsed into ONE value across all folds. It will also be
        #      3d if the return value of the measure isn't a scalar and it is
        #      not collapsed across folds. it simply corresponds to the shape
        #      of the output dataset of the respective measure (+1 axis)
        # Some permutations could have been skipped since classifier failed
        # to train due to degenerate situation etc, thus accounting for them
        self.assertEqual(cvte.null_dist.ca.dist_samples.shape[2],
                             num_perm - cvte.null_dist.ca.skipped)



    @sweepargs(clf=clfswh['multiclass'])
    def test_auc(self, clf):
        """Test AUC computation
        """
        if isinstance(clf, MulticlassClassifier):
            raise SkipTest, \
                  "TODO: handle values correctly in MulticlassClassifier"
        clf.ca.change_temporarily(enable_ca = ['estimates'])
        if 'qda' in clf.__tags__:
            # for reliable estimation of covariances, need sufficient
            # sample size
            ds_size = 'large'
        else:
            ds_size = 'small'
        # uni2 dataset with reordered labels
        ds2 = datasets['uni2' + ds_size].copy()
        # revert labels
        ds2.sa['targets'].value = ds2.targets[::-1].copy()
        # same with uni3
        ds3 = datasets['uni3' + ds_size].copy()
        ul = ds3.sa['targets'].unique
        nl = ds3.targets.copy()
        for l in xrange(3):
            nl[ds3.targets == ul[l]] = ul[(l+1)%3]
        ds3.sa.targets = nl
        for ds in [datasets['uni2' + ds_size], ds2,
                   datasets['uni3' + ds_size], ds3]:
            cv = CrossValidation(clf, OddEvenPartitioner(),
                enable_ca=['stats', 'training_stats'])
            cverror = cv(ds)
            stats = cv.ca.stats.stats
            Nlabels = len(ds.uniquetargets)
            # so we at least do slightly above chance
            # But LARS manages to screw up there as well ATM from time to time, so making
            # its testing labile
            if (('lars' in clf.__tags__) and cfg.getboolean('tests', 'labile', default='yes')) \
                or (not 'lars' in clf.__tags__):
                self.assertTrue(stats['ACC'] > 1.2 / Nlabels)
            auc = stats['AUC']
            if (Nlabels == 2) or (Nlabels > 2 and auc[0] is not np.nan):
                mauc = np.min(stats['AUC'])
                if cfg.getboolean('tests', 'labile', default='yes'):
                    self.assertTrue(mauc > 0.55,
                         msg='All AUCs must be above chance. Got minimal '
                             'AUC=%.2g among %s' % (mauc, stats['AUC']))
        clf.ca.reset_changed_temporarily()




    def test_confusion_plot(self):
        """Basic test of confusion plot

        Based on existing cell dataset results.

        Let in for possible future testing, but is not a part of the
        unittests suite
        """
        #from matplotlib import rc as rcmpl
        #rcmpl('font',**{'family':'sans-serif','sans-serif':['DejaVu Sans']})
        ##rcmpl('text', usetex=True)
        ##rcmpl('font',  family='sans', style='normal', variant='normal',
        ##   weight='bold',  stretch='normal', size='large')
        #import numpy as np
        #from mvpa2.clfs.transerror import \
        #     TransferError, ConfusionMatrix, ConfusionBasedError

        array = np.array
        uint8 = np.uint8
        sets = [
           (array([47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44], dtype=uint8),
            array([40, 39, 47, 43, 45, 41, 44, 41, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 38, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 46,
                 45, 38, 44, 39, 46, 38, 39, 39, 38, 43, 45, 41, 44, 40, 46, 42, 38,
                 40, 47, 43, 45, 41, 44, 40, 46, 42, 38, 39, 40, 43, 45, 41, 44, 39,
                 46, 42, 47, 38, 38, 43, 45, 41, 44, 38, 46, 42, 47, 38, 39, 43, 45,
                 41, 44, 40, 46, 42, 47, 38, 38, 43, 45, 41, 44, 40, 46, 42, 47, 38,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 47, 43, 45, 41, 44, 40, 46,
                 42, 47, 38, 38, 43, 45, 41, 44, 40, 46, 42, 39, 39, 38, 43, 45, 41,
                 44, 47, 46, 42, 47, 38, 39, 43, 45, 40, 44, 40, 46, 42, 47, 39, 40,
                 43, 45, 41, 44, 38, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 41,
                 47, 39, 38, 46, 45, 41, 44, 40, 46, 42, 40, 38, 38, 43, 45, 41, 44,
                 40, 45, 42, 47, 39, 39, 43, 45, 41, 44, 38, 46, 42, 47, 38, 42, 43,
                 45, 41, 44, 39, 46, 42, 39, 39, 39, 47, 45, 41, 44], dtype=uint8)),
           (array([40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43], dtype=uint8),
            array([40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 47, 46, 42, 47, 39, 40, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 39, 46, 42, 47, 47, 47, 43, 45, 41, 44, 40,
                 46, 42, 43, 39, 38, 43, 45, 41, 44, 38, 38, 42, 38, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 40, 38, 43, 45, 41, 44, 40, 40, 42, 47, 40,
                 40, 43, 45, 41, 44, 38, 38, 42, 47, 38, 38, 47, 45, 41, 44, 40, 46,
                 42, 47, 39, 40, 43, 45, 41, 44, 40, 46, 42, 47, 47, 39, 43, 45, 41,
                 44, 40, 46, 42, 39, 39, 42, 43, 45, 41, 44, 40, 46, 42, 47, 39, 39,
                 43, 45, 41, 44, 47, 46, 42, 40, 39, 39, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 40, 44, 40, 46, 42, 47, 39, 39, 43, 45, 41, 44,
                 38, 46, 42, 47, 39, 39, 43, 45, 41, 44, 40, 46, 46, 47, 38, 39, 43,
                 45, 41, 44, 40, 46, 42, 47, 38, 39, 43, 45, 41, 44, 40, 46, 42, 39,
                 39, 38, 47, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43], dtype=uint8)),
           (array([45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47], dtype=uint8),
            array([45, 41, 44, 40, 46, 42, 47, 39, 46, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 39, 43, 45, 41, 44, 38, 46, 42, 47, 38, 39, 43, 45, 41, 44, 40,
                 46, 42, 47, 38, 39, 43, 45, 41, 44, 40, 46, 42, 47, 39, 43, 43, 45,
                 40, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 47,
                 40, 43, 45, 41, 44, 40, 47, 42, 38, 47, 38, 43, 45, 41, 44, 40, 40,
                 42, 47, 39, 39, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 38, 46, 42, 47, 39, 39, 43, 45, 41, 44, 40, 46, 42, 47, 40, 38,
                 43, 45, 41, 44, 40, 46, 38, 38, 39, 38, 43, 45, 41, 44, 39, 46, 42,
                 47, 40, 39, 43, 45, 38, 44, 38, 46, 42, 47, 47, 40, 43, 45, 41, 44,
                 40, 40, 42, 47, 40, 38, 43, 39, 41, 44, 41, 46, 42, 39, 39, 38, 38,
                 45, 41, 44, 38, 46, 40, 46, 46, 46, 43, 45, 38, 44, 40, 46, 42, 39,
                 39, 45, 43, 45, 41, 44, 38, 46, 42, 38, 39, 39, 43, 45, 41, 38, 40,
                 46, 42, 47, 38, 39, 43, 45, 41, 44, 40, 46, 42, 40], dtype=uint8)),
           (array([39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40], dtype=uint8),
            array([39, 38, 43, 45, 41, 44, 40, 46, 38, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 41, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 38, 43, 47, 38, 38, 43, 45, 41, 44, 39, 46, 42, 39, 39,
                 38, 43, 45, 41, 44, 43, 46, 42, 47, 39, 39, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 40, 43, 45, 41, 44, 40, 46, 42, 39, 38, 38, 43, 45, 40,
                 44, 47, 46, 38, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 38, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 38, 39, 38, 43, 45, 47, 44, 45, 46, 42,
                 38, 39, 41, 43, 45, 41, 44, 38, 38, 42, 39, 40, 40, 43, 45, 41, 39,
                 40, 46, 42, 47, 39, 40, 43, 45, 41, 44, 40, 47, 42, 47, 38, 38, 43,
                 45, 41, 44, 47, 46, 42, 47, 40, 47, 43, 45, 41, 44, 40, 46, 42, 47,
                 38, 39, 43, 45, 41, 44, 40, 46, 42, 39, 38, 43, 45, 46, 44, 38, 46,
                 42, 47, 38, 44, 43, 45, 42, 44, 41, 46, 42, 47, 47, 38, 43, 45, 41,
                 44, 38, 46, 42, 39, 39, 38, 43, 45, 41, 44, 40], dtype=uint8)),
           (array([46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45], dtype=uint8),
            array([46, 42, 39, 38, 38, 43, 45, 41, 44, 40, 46, 42, 47, 47, 42, 43, 45,
                 42, 44, 40, 46, 42, 38, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 47,
                 40, 43, 45, 41, 44, 41, 46, 42, 38, 39, 38, 43, 45, 41, 44, 38, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 46, 38, 38, 43, 45, 41,
                 44, 39, 46, 42, 47, 39, 40, 43, 45, 41, 44, 40, 46, 42, 47, 39, 39,
                 43, 45, 41, 44, 40, 47, 42, 47, 38, 39, 43, 45, 41, 44, 39, 46, 42,
                 47, 39, 46, 43, 45, 41, 44, 39, 46, 42, 39, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 38, 38, 43, 45, 41, 44, 40, 46, 42, 39, 39, 38, 43,
                 45, 41, 44, 40, 38, 42, 46, 39, 38, 43, 45, 41, 44, 38, 46, 42, 46,
                 46, 38, 43, 45, 41, 44, 40, 46, 42, 47, 47, 38, 38, 45, 41, 44, 38,
                 38, 42, 43, 39, 40, 43, 45, 41, 44, 38, 46, 42, 47, 38, 39, 47, 45,
                 46, 44, 40, 46, 42, 47, 40, 38, 43, 45, 41, 44, 40, 46, 42, 47, 40,
                 38, 43, 45, 41, 44, 38, 46, 42, 38, 39, 38, 47, 45], dtype=uint8)),
           (array([41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39], dtype=uint8),
            array([41, 44, 38, 46, 42, 47, 39, 47, 40, 45, 41, 44, 40, 46, 42, 38, 40,
                 38, 43, 45, 41, 44, 40, 46, 42, 38, 38, 38, 43, 45, 41, 44, 46, 38,
                 42, 40, 38, 39, 43, 45, 41, 44, 41, 46, 42, 47, 47, 38, 43, 45, 41,
                 44, 40, 46, 42, 38, 39, 39, 43, 45, 41, 44, 38, 46, 42, 47, 43, 39,
                 43, 45, 41, 44, 40, 46, 42, 38, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 40, 39, 38, 43, 45, 41, 44, 38, 46, 42, 39, 39, 39, 43, 45, 41, 44,
                 40, 46, 42, 39, 38, 47, 43, 45, 38, 44, 40, 38, 42, 47, 38, 38, 43,
                 45, 41, 44, 40, 38, 46, 47, 38, 38, 43, 45, 41, 44, 41, 46, 42, 40,
                 38, 38, 40, 45, 41, 44, 40, 40, 42, 43, 38, 40, 43, 39, 41, 44, 40,
                 40, 42, 47, 38, 46, 43, 45, 41, 44, 47, 41, 42, 43, 40, 47, 43, 45,
                 41, 44, 41, 38, 42, 40, 39, 40, 43, 45, 41, 44, 39, 43, 42, 47, 39,
                 40, 43, 45, 41, 44, 42, 46, 42, 47, 40, 46, 43, 45, 41, 44, 38, 46,
                 42, 47, 47, 38, 43, 45, 41, 44, 40, 38, 39, 47, 38], dtype=uint8)),
           (array([38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46], dtype=uint8),
            array([39, 43, 45, 41, 44, 40, 46, 42, 47, 38, 38, 43, 45, 41, 44, 41, 46,
                 42, 47, 47, 39, 43, 45, 41, 44, 40, 46, 42, 47, 38, 39, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 40, 43, 45, 41, 44, 40, 46, 42, 47, 45, 38,
                 43, 45, 41, 44, 38, 46, 42, 47, 38, 39, 43, 45, 41, 44, 40, 46, 42,
                 39, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 40, 39, 43, 45, 41, 44, 40, 39, 42, 40, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 38, 46, 42, 39,
                 39, 47, 43, 45, 41, 44, 40, 46, 42, 47, 39, 39, 43, 45, 41, 44, 40,
                 46, 42, 46, 47, 39, 47, 45, 41, 44, 40, 46, 42, 47, 39, 39, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 38, 46, 42, 47, 39,
                 38, 43, 45, 42, 44, 39, 47, 42, 39, 39, 47, 43, 47, 40, 44, 40, 46,
                 42, 39, 39, 38, 39, 45, 41, 44, 40, 46, 42, 47, 38, 38, 43, 45, 41,
                 44, 46, 38, 42, 47, 39, 43, 43, 45, 41, 44, 40, 46], dtype=uint8)),
           (array([42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42,
                 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44,
                 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43,
                 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47,
                 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40,
                 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45,
                 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39,
                 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46,
                 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 40, 46, 42, 47, 39, 38, 43, 45, 41, 44, 40, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 40, 46, 42, 47, 39, 38, 43, 45], dtype=uint8),
            array([42, 38, 38, 40, 43, 45, 41, 44, 39, 46, 42, 47, 39, 38, 43, 45, 41,
                 44, 39, 38, 42, 47, 41, 40, 43, 45, 41, 44, 40, 41, 42, 47, 38, 46,
                 43, 45, 41, 44, 41, 41, 42, 40, 39, 39, 43, 45, 41, 44, 46, 45, 42,
                 39, 39, 40, 43, 45, 41, 44, 40, 46, 42, 40, 44, 38, 43, 41, 41, 44,
                 39, 46, 42, 39, 39, 39, 43, 45, 41, 44, 40, 43, 42, 47, 39, 39, 43,
                 45, 41, 44, 40, 47, 42, 38, 46, 39, 47, 45, 41, 44, 39, 46, 42, 47,
                 41, 38, 43, 45, 41, 44, 42, 46, 42, 46, 39, 38, 43, 45, 41, 44, 41,
                 46, 42, 46, 39, 38, 43, 45, 41, 44, 40, 46, 42, 38, 38, 38, 43, 45,
                 41, 44, 38, 46, 42, 39, 40, 43, 43, 45, 41, 44, 39, 38, 40, 40, 38,
                 38, 43, 45, 41, 44, 41, 40, 42, 39, 39, 39, 43, 45, 41, 44, 40, 46,
                 42, 47, 40, 40, 43, 45, 41, 44, 40, 46, 42, 41, 39, 39, 43, 45, 41,
                 44, 40, 38, 42, 40, 39, 46, 43, 45, 41, 44, 47, 46, 42, 47, 39, 38,
                 43, 45, 41, 44, 41, 46, 42, 43, 39, 39, 43, 45], dtype=uint8))]
        labels_map = {'12kHz': 40,
                      '20kHz': 41,
                      '30kHz': 42,
                      '3kHz': 38,
                      '7kHz': 39,
                      'song1': 43,
                      'song2': 44,
                      'song3': 45,
                      'song4': 46,
                      'song5': 47}
        try:
            cm = ConfusionMatrix(sets=sets, labels_map=labels_map)
        except:
            self.fail()

        cms = str(cm)
        self.assertTrue('3kHz / 38' in cms)
        if externals.exists("scipy"):
            self.assertTrue('ACC(i) = 0.82-0.012*i p=0.12 r=-0.59 r^2=0.35' in cms)

        if externals.exists("pylab plottable"):
            import pylab as pl
            pl.figure()
            labels_order = ("3kHz", "7kHz", "12kHz", "20kHz","30kHz", None,
                            "song1","song2","song3","song4","song5")
            #print cm
            #fig, im, cb = cm.plot(origin='lower', labels=labels_order)
            fig, im, cb = cm.plot(labels=labels_order[1:2] + labels_order[:1]
                                         + labels_order[2:], numbers=True)
            self.assertTrue(cm._plotted_confusionmatrix[0,0] == cm.matrix[1,1])
            self.assertTrue(cm._plotted_confusionmatrix[0,1] == cm.matrix[1,0])
            self.assertTrue(cm._plotted_confusionmatrix[1,1] == cm.matrix[0,0])
            self.assertTrue(cm._plotted_confusionmatrix[1,0] == cm.matrix[0,1])
            pl.close(fig)
            fig, im, cb = cm.plot(labels=labels_order, numbers=True)
            pl.close(fig)
            # pl.show()

    def test_confusion_plot2(self):

        array = np.array
        uint8 = np.uint8
        sets = [(array([1, 2]), array([1, 1]),
                 array([[ 0.54343765,  0.45656235],
                        [ 0.92395853,  0.07604147]])),
                (array([1, 2]), array([1, 1]),
                 array([[ 0.98030832,  0.01969168],
                        [ 0.78998763,  0.21001237]])),
                (array([1, 2]), array([1, 1]),
                 array([[ 0.86125263,  0.13874737],
                        [ 0.83674113,  0.16325887]])),
                (array([1, 2]), array([1, 1]),
                 array([[ 0.57870383,  0.42129617],
                        [ 0.59702509,  0.40297491]])),
                (array([1, 2]), array([1, 1]),
                 array([[ 0.89530255,  0.10469745],
                        [ 0.69373919,  0.30626081]])),
                (array([1, 2]), array([1, 1]),
                 array([[ 0.75015218,  0.24984782],
                        [ 0.9339767 ,  0.0660233 ]])),
                (array([1, 2]), array([1, 2]),
                 array([[ 0.97826616,  0.02173384],
                        [ 0.38620638,  0.61379362]])),
                (array([2]), array([2]),
                 array([[ 0.46893776,  0.53106224]]))]
        try:
            cm = ConfusionMatrix(sets=sets)
        except:
            self.fail()

        if externals.exists("pylab plottable"):
            import pylab as pl
            #pl.figure()
            #print cm
            fig, im, cb = cm.plot(origin='lower', numbers=True)
            #pl.plot()
            self.assertTrue((cm._plotted_confusionmatrix == cm.matrix).all())
            pl.close(fig)
            #fig, im, cb = cm.plot(labels=labels_order, numbers=True)
            #pl.close(fig)
            #pl.show()

    @reseed_rng()
    @labile(3, 1)
    def test_confusionmatrix_nulldist(self):
        from mvpa2.clfs.gnb import GNB
        from mvpa2.clfs.transerror import ConfusionMatrixError
        from mvpa2.misc.data_generators import normal_feature_dataset
        for snr in [0., 2.,]:
            ds = normal_feature_dataset(snr=snr, perlabel=42, nchunks=3,
                                        nonbogus_features=[0,1], nfeatures=2)

            clf = GNB()
            num_perm = 50
            permutator = AttributePermutator('targets',
                                             limit='chunks',
                                             count=num_perm)
            cv = CrossValidation(
                clf, NFoldPartitioner(),
                errorfx=ConfusionMatrixError(labels=ds.sa['targets'].unique),
                postproc=mean_sample(),
                null_dist=MCNullDist(permutator,
                                     tail='right', # because we now look at accuracy not error
                                     enable_ca=['dist_samples']),
                enable_ca=['stats'])
            cmatrix = cv(ds)
            #print "Result:\n", cmatrix.samples
            cvnp = cv.ca.null_prob.samples
            #print cvnp
            self.assertTrue(cvnp.shape, (2, 2))
            if cfg.getboolean('tests', 'labile', default='yes'):
                if snr == 0.:
                    # all p should be high since no signal
                    assert_array_less(0.05, cvnp)
                else:
                    # diagonal p is low -- we have signal after all
                    assert_array_less(np.diag(cvnp), 0.05)
                    # off diagonals are high p since for them we would
                    # need to look at the other tail
                    assert_array_less(0.9,
                                      cvnp[(np.array([0,1]), np.array([1,0]))])


def test_confusion_as_node():
    from mvpa2.misc.data_generators import normal_feature_dataset
    from mvpa2.clfs.gnb import GNB
    from mvpa2.clfs.transerror import Confusion
    ds = normal_feature_dataset(snr=2.0, perlabel=42, nchunks=3,
                                nonbogus_features=[0,1], nfeatures=2)
    clf = GNB()
    cv = CrossValidation(
        clf, NFoldPartitioner(),
        errorfx=None,
        postproc=Confusion(labels=ds.UT),
        enable_ca=['stats'])
    res = cv(ds)
    # needs to be identical to CA
    assert_array_equal(res.samples, cv.ca.stats.matrix)
    assert_array_equal(res.sa.predictions, ds.UT)
    assert_array_equal(res.fa.targets, ds.UT)

    skip_if_no_external('scipy')

    from mvpa2.clfs.transerror import BayesConfusionHypothesis
    from mvpa2.base.node import ChainNode
    # same again, but this time with Bayesian hypothesis testing at the end
    cv = CrossValidation(
        clf, NFoldPartitioner(),
        errorfx=None,
        postproc=ChainNode([Confusion(labels=ds.UT),
                            BayesConfusionHypothesis()]))
    res = cv(ds)
    # only two possible hypothesis with two classes
    assert_equals(len(res), 2)
    # the first hypothesis is the can't discriminate anything
    assert_equal(len(res.sa.hypothesis[0]), 1)
    assert_equal(len(res.sa.hypothesis[0][0]), 2)
    # and the hypothesis is actually less likely than the other one
    # (both classes can be distinguished)
    assert(np.e**res.samples[0,0] < np.e**res.samples[1,0])

    # Let's see how well it would work within the searchlight when we also
    # would like to store the hypotheses per each voxel
    # Somewhat an ad-hoc solution for the answer posted on the ML
    #
    # run 1d searchlight of radii 0, for that just provide a .fa with coordinates
    ds.fa['voxel_indices'] = [[0], [1]]
    # and a custom Node which would collect .sa.hypothesis to place together along
    # with the posterior probabilities
    from mvpa2.base.node import Node
    from mvpa2.measures.searchlight import sphere_searchlight

    class KeepBothPosteriorAndHypothesis(Node):
        def _call(self, ds):
            out = np.zeros(1, dtype=object)
            out[0] = (ds.samples, ds.sa.hypothesis)
            return out
    cv.postproc.append(KeepBothPosteriorAndHypothesis())
    sl = sphere_searchlight(cv, radius=0, nproc=1)
    res = sl(ds)

    assert_equal(res.shape, (1, 2))
    assert_equal(len(res.samples[0,0]), 2)
    assert_equal(res.samples[0,0][0].shape, (2, 2))   # posteriors per 1st SL
    assert_equal(len(res.samples[0,0][1]), 2)   # 2 of hypotheses


def test_bayes_confusion_hyp():
    from mvpa2.clfs.transerror import BayesConfusionHypothesis
    conf = np.array([
            [ 10,  0,  5,  5],
            [  0, 10,  5,  5],
            [  5,  5, 10,  0],
            [  5,  5,  0, 10]
            ])
    conf = Dataset(conf, sa={'labels': ['A', 'B', 'C', 'D']})
    bayes = BayesConfusionHypothesis(labels_attr='labels')
    skip_if_no_external('scipy')        # uses factorial from scipy.misc
    hyptest = bayes(conf)
    # by default comes with all hypothesis and posterior probs
    assert_equal(hyptest.shape, (15,2))
    assert_array_equal(hyptest.fa.stat, ['log(p(C|H))', 'log(p(H|C))'])
    # check order of hypothesis (coarse)
    assert_array_equal(hyptest.sa.hypothesis[0], [['A', 'B', 'C', 'D']])
    assert_array_equal(hyptest.sa.hypothesis[-1], [['A'], ['B'], ['C'], ['D']])
    # now with limited hypothesis (given with literal labels), set and in
    # non-log scale
    bayes = BayesConfusionHypothesis(labels_attr='labels', log=False,
                hypotheses=[[['A', 'B', 'C', 'D']],
                            [['A', 'C',], ['B', 'D']],
                            [['A', 'D',], ['B', 'C']],
                            [['A'], ['B'], ['C'], ['D']]])
    hyptest = bayes(conf)
    # also with custom hyp the post-probs must add up to 1
    post_prob = hyptest.samples[:,1]
    assert_almost_equal(np.sum(post_prob), 1)
    # in this particular case ...
    assert(post_prob[3] - np.sum(post_prob[1:3]) < 0.02)


def suite():  # pragma: no cover
    return unittest.makeSuite(ErrorsTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_transformers
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA transformers."""

import unittest
import numpy as np

from mvpa2.base import externals

from mvpa2.misc.transformers import Absolute, one_minus, rank_order, \
     reverse_rank_order, l1_normed, l2_normed, OverAxis, \
     DistPValue, first_axis_sum_not_zero

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets

from mvpa2.base import cfg

class TransformerTests(unittest.TestCase):

    def setUp(self):
        self.d1 = np.array([ 1,  0, -1, -2, -3])
        self.d2 = np.array([ 2.3,  0, -1, 2, -30, 1])

    @reseed_rng()
    def test_absolute(self):
        # generate 100 values (gaussian noise mean -1000 -> all negative)
        out = Absolute(np.random.normal(-1000, size=100))

        self.assertTrue(out.min() >= 0)
        self.assertTrue(len(out) == 100)

    def test_absolute2(self):
        target = self.d1
        out = one_minus(np.arange(5))
        self.assertTrue((out == target).all())

    def test_first_axis_sum_not_zero(self):
        src = [[ 1, -22.9, 6.8, 0],
               [ -.8, 7, 0, 0.0],
               [88, 0, 0.0, 0],
               [0, 0, 0, 0.0]]
        target = np.array([ 3, 2, 1, 0])
        out = first_axis_sum_not_zero(src)
        self.assertTrue((out == target).all())
        
    def test_rank_order(self):
        nelements = len(self.d2)
        out = rank_order(self.d2)
        outr = reverse_rank_order(self.d2)
        uout = np.unique(out)
        uoutr = np.unique(outr)
        self.assertTrue((uout == np.arange(nelements)).all(),
                        msg="We should get all indexes. Got just %s" % uout)
        self.assertTrue((uoutr == np.arange(nelements)).all(),
                        msg="We should get all indexes. Got just %s" % uoutr)
        self.assertTrue((out+outr+1 == nelements).all())
        self.assertTrue((out == [ 0,  3,  4,  1,  5,  2]).all())

    def test_l2_norm(self):
        out = l2_normed(self.d2)
        self.assertTrue(np.abs(np.sum(out*out)-1.0) < 1e-10)

    def test_l1_norm(self):
        out = l1_normed(self.d2)
        self.assertTrue(np.abs(np.sum(np.abs(out))-1.0) < 1e-10)


    def test_over_axis(self):
        data = datasets['uni4large'].samples[:120,0].reshape((2,3,4,5))
        # Simple transformer/combiner which collapses across given
        # dimension, e.g. sum
        for axis in [None, 0, 1, 2]:
            oversum = OverAxis(np.sum, axis=axis)(data)
            sum_ = np.sum(data, axis=axis)
            assert_array_almost_equal(sum_, oversum)

        # Transformer which doesn't modify dimensionality of the data
        data = data.reshape((6, -1))
        overnorm = OverAxis(l2_normed, axis=1)(data)
        self.assertTrue(np.linalg.norm(overnorm)!=1.0)
        for d in overnorm:
            self.assertTrue(np.abs(np.linalg.norm(d) - 1.0)<0.00001)

        overnorm = OverAxis(l2_normed, axis=0)(data)
        self.assertTrue(np.linalg.norm(overnorm)!=1.0)
        for d in overnorm.T:
            self.assertTrue(np.abs(np.linalg.norm(d) - 1.0)<0.00001)

    @reseed_rng()
    def test_dist_p_value(self):
        """Basic testing of DistPValue"""
        if not externals.exists('scipy'):
            return
        ndb = 200
        ndu = 20
        nperd = 2
        pthr = 0.05
        Nbins = 400

        # Lets generate already normed data (on sphere) and add some nonbogus features
        datau = (np.random.normal(size=(nperd, ndb)))
        dist = np.sqrt((datau * datau).sum(axis=1))

        datas = (datau.T / dist.T).T
        tn = datax = datas[0, :]
        dataxmax = np.max(np.abs(datax))

        # now lets add true positive features
        tp = [-dataxmax * 1.1] * (ndu//2) + [dataxmax * 1.1] * (ndu//2)
        x = np.hstack((datax, tp))

        # lets add just pure normal to it
        x = np.vstack((x, np.random.normal(size=x.shape))).T
        for distPValue in (DistPValue(), DistPValue(fpp=0.05)):
            result = distPValue(x)
            self.assertTrue((result>=0).all)
            self.assertTrue((result<=1).all)

        if cfg.getboolean('tests', 'labile', default='yes'):
            self.assertTrue(distPValue.ca.positives_recovered[0] > 10)
            self.assertTrue((np.array(distPValue.ca.positives_recovered) +
                             np.array(distPValue.ca.nulldist_number) == ndb + ndu).all())
            self.assertEqual(distPValue.ca.positives_recovered[1], 0)


def suite():  # pragma: no cover
    return unittest.makeSuite(TransformerTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_usecases
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for various use cases users reported mis-behaving"""

import unittest
import numpy as np

from mvpa2.testing.tools import ok_, assert_array_equal, assert_true, \
        assert_false, assert_equal, assert_not_equal, reseed_rng, assert_raises, \
        assert_array_almost_equal, SkipTest

@reseed_rng()
def _test_mcasey20120222():  # pragma: no cover
    # http://lists.alioth.debian.org/pipermail/pkg-exppsy-pymvpa/2012q1/002034.html

    # This one is conditioned on allowing # of samples to be changed
    # by the mapper provided to MappedClassifier.  See
    # https://github.com/yarikoptic/PyMVPA/tree/_tent/allow_ch_nsamples

    import numpy as np
    from mvpa2.datasets.base import dataset_wizard
    from mvpa2.generators.partition import NFoldPartitioner
    from mvpa2.mappers.base import ChainMapper
    from mvpa2.mappers.svd import SVDMapper
    from mvpa2.mappers.fx import mean_group_sample
    from mvpa2.clfs.svm import LinearCSVMC
    from mvpa2.clfs.meta import MappedClassifier
    from mvpa2.measures.base import CrossValidation

    mapper = ChainMapper([mean_group_sample(['targets','chunks']),
                          SVDMapper()])
    clf = MappedClassifier(LinearCSVMC(), mapper)
    cvte = CrossValidation(clf, NFoldPartitioner(),
                           enable_ca=['repetition_results', 'stats'])

    ds = dataset_wizard(
        samples=np.arange(32).reshape((8, -1)),
        targets=[1, 1, 2, 2, 1, 1, 2, 2],
        chunks=[1, 1, 1, 1, 2, 2, 2, 2])

    errors = cvte(ds)


@reseed_rng()
def test_sifter_superord_usecase():
    from mvpa2.misc.data_generators import normal_feature_dataset
    from mvpa2.clfs.svm import LinearCSVMC            # fast one to use for tests
    from mvpa2.measures.base import CrossValidation

    from mvpa2.base.node import ChainNode
    from mvpa2.generators.partition import NFoldPartitioner
    from mvpa2.generators.base import  Sifter

    # Let's simulate the beast -- 6 categories total groupped into 3
    # super-ordinate, and actually without any 'superordinate' effect
    # since subordinate categories independent
    ds = normal_feature_dataset(nlabels=6,
                                snr=100,   # pure signal! ;)
                                perlabel=30,
                                nfeatures=6,
                                nonbogus_features=range(6),
                                nchunks=5)
    ds.sa['subord'] = ds.sa.targets.copy()
    ds.sa['superord'] = ['super%d' % (int(i[1])%3,)
                         for i in ds.targets]   # 3 superord categories
    # let's override original targets just to be sure that we aren't relying on them
    ds.targets[:] = 0

    npart = ChainNode([
    ## so we split based on superord
        NFoldPartitioner(len(ds.sa['superord'].unique),
                         attr='subord'),
        ## so it should select only those splits where we took 1 from
        ## each of the superord categories leaving things in balance
        Sifter([('partitions', 2),
                ('superord',
                 { 'uvalues': ds.sa['superord'].unique,
                   'balanced': True})
                 ]),
                   ], space='partitions')

    # and then do your normal where clf is space='superord'
    clf = LinearCSVMC(space='superord')
    cvte_regular = CrossValidation(clf, NFoldPartitioner(),
                                   errorfx=lambda p,t: np.mean(p==t))
    cvte_super = CrossValidation(clf, npart, errorfx=lambda p,t: np.mean(p==t))

    accs_regular = cvte_regular(ds)
    accs_super = cvte_super(ds)

    # With sifting we should get only 2^3 = 8 splits
    assert(len(accs_super) == 8)
    # I don't think that this would ever fail, so not marking it labile
    assert(np.mean(accs_regular) > .8)
    assert(np.mean(accs_super)   < .6)

def _test_edmund_chong_20120907():  # pragma: no cover
    # commented out to avoid syntax warnings while compiling
    # from mvpa2.suite import *
    from mvpa2.testing.datasets import datasets
    repeater = Repeater(count=20)

    partitioner = ChainNode([NFoldPartitioner(cvtype=1),
                             Balancer(attr='targets',
                                      count=1, # for real data > 1
                                      limit='partitions',
                                      apply_selection=True
                                      )],
                            space='partitions')

    clf = LinearCSVMC() #choice of classifier
    permutator = AttributePermutator('targets', limit={'partitions': 1},
                                     count=1)
    null_cv = CrossValidation(
        clf,
        ChainNode([partitioner, permutator], space=partitioner.get_space()),
        errorfx=mean_mismatch_error)
    distr_est = MCNullDist(repeater, tail='left', measure=null_cv,
                           enable_ca=['dist_samples'])
    cvte = CrossValidation(clf, partitioner,
                           errorfx=mean_mismatch_error,
                           null_dist=distr_est,
                           enable_ca=['stats'])
    errors = cvte(datasets['uni2small'])


def test_chained_crossvalidation_searchlight():
    from mvpa2.clfs.gnb import GNB
    from mvpa2.clfs.meta import MappedClassifier
    from mvpa2.generators.partition import NFoldPartitioner
    from mvpa2.mappers.base import ChainMapper
    from mvpa2.mappers.base import Mapper
    from mvpa2.measures.base import CrossValidation
    from mvpa2.measures.searchlight import sphere_searchlight
    from mvpa2.testing.datasets import datasets

    dataset = datasets['3dlarge'].copy()
    dataset.fa['voxel_indices'] = dataset.fa.myspace
    sample_clf = GNB()              # fast and deterministic

    class ZScoreFeaturesMapper(Mapper):
        """Very basic mapper which would take care about standardizing
        all features within each sample separately
        """
        def _forward_data(self, data):
            return (data - np.mean(data, axis=1)[:, None])/np.std(data, axis=1)[:, None]

    # only do partial to save time
    sl_kwargs = dict(radius=2, center_ids=[3, 50])
    clf_mapped = MappedClassifier(sample_clf, ZScoreFeaturesMapper())
    cv = CrossValidation(clf_mapped, NFoldPartitioner())
    sl = sphere_searchlight(cv, **sl_kwargs)
    results_mapped = sl(dataset)

    cv_chained = ChainMapper([ZScoreFeaturesMapper(auto_train=True),
                              CrossValidation(sample_clf, NFoldPartitioner())])
    sl_chained = sphere_searchlight(cv_chained, **sl_kwargs)
    results_chained = sl_chained(dataset)

    assert_array_equal(results_mapped, results_chained)

def test_gnbsearchlight_permutations():
    import mvpa2
    from mvpa2.base.node import ChainNode
    from mvpa2.clfs.gnb import GNB
    from mvpa2.generators.base import  Repeater
    from mvpa2.generators.partition import NFoldPartitioner, OddEvenPartitioner
    #import mvpa2.generators.permutation
    #reload(mvpa2.generators.permutation)
    from mvpa2.generators.permutation import AttributePermutator
    from mvpa2.testing.datasets import datasets
    from mvpa2.measures.base import CrossValidation
    from mvpa2.measures.gnbsearchlight import sphere_gnbsearchlight
    from mvpa2.measures.searchlight import sphere_searchlight
    from mvpa2.mappers.fx import mean_sample
    from mvpa2.misc.errorfx import mean_mismatch_error
    from mvpa2.clfs.stats import MCNullDist
    from mvpa2.testing.tools import assert_raises, ok_, assert_array_less

    # mvpa2.debug.active = ['APERM', 'SLC'] #, 'REPM']
    # mvpa2.debug.metrics += ['pid']
    count = 10
    nproc = 1 + int(mvpa2.externals.exists('pprocess'))
    ds = datasets['3dsmall'].copy()
    ds.fa['voxel_indices'] = ds.fa.myspace

    slkwargs = dict(radius=3, space='voxel_indices',  enable_ca=['roi_sizes'],
                    center_ids=[1, 10, 70, 100])

    mvpa2.seed(mvpa2._random_seed)
    clf  = GNB()
    splt = NFoldPartitioner(cvtype=2, attr='chunks')

    repeater   = Repeater(count=count)
    permutator = AttributePermutator('targets', limit={'partitions': 1}, count=1)

    null_sl = sphere_gnbsearchlight(clf, ChainNode([splt, permutator], space=splt.get_space()),
                                    postproc=mean_sample(), errorfx=mean_mismatch_error,
                                    **slkwargs)

    distr_est = MCNullDist(repeater, tail='left', measure=null_sl,
                           enable_ca=['dist_samples'])
    sl = sphere_gnbsearchlight(clf, splt,
                               reuse_neighbors=True,
                               null_dist=distr_est, postproc=mean_sample(),
                               errorfx=mean_mismatch_error,
                               **slkwargs)
    if __debug__:                         # assert is done only without -O mode
        assert_raises(NotImplementedError, sl, ds)

    # "ad-hoc searchlights can't handle yet varying targets across partitions"
    if False:
        # after above limitation is removed -- enable
        sl_map = sl(ds)
        sl_null_prob = sl.ca.null_prob.samples.copy()

    mvpa2.seed(mvpa2._random_seed)
    ### 'normal' Searchlight
    clf  = GNB()
    splt = NFoldPartitioner(cvtype=2, attr='chunks')
    repeater   = Repeater(count=count)
    permutator = AttributePermutator('targets', limit={'partitions': 1}, count=1)
    # rng=np.random.RandomState(0)) # to trigger failure since the same np.random state
    # would be reused across all pprocesses
    null_cv = CrossValidation(clf, ChainNode([splt, permutator], space=splt.get_space()),
                              postproc=mean_sample())
    null_sl_normal = sphere_searchlight(null_cv, nproc=nproc, **slkwargs)
    distr_est_normal = MCNullDist(repeater, tail='left', measure=null_sl_normal,
                           enable_ca=['dist_samples'])

    cv = CrossValidation(clf, splt, errorfx=mean_mismatch_error,
                         enable_ca=['stats'], postproc=mean_sample() )
    sl = sphere_searchlight(cv, nproc=nproc, null_dist=distr_est_normal, **slkwargs)
    sl_map_normal = sl(ds)
    sl_null_prob_normal = sl.ca.null_prob.samples.copy()

    # For every feature -- we should get some variance in estimates In
    # case of failure they are all really close to each other (up to
    # numerical precision), so variance will be close to 0
    assert_array_less(-np.var(distr_est_normal.ca.dist_samples.samples[0],
                              axis=1), -1e-5)
    for s in distr_est_normal.ca.dist_samples.samples[0]:
        ok_(len(np.unique(s)) > 1)

    # TODO: compare two results, although might become tricky with
    #       nproc=2 and absent way to control RNG across child processes

def test_multiclass_pairs_svm_searchlight():
    from mvpa2.measures.searchlight import sphere_searchlight
    import mvpa2.clfs.meta
    #reload(mvpa2.clfs.meta)
    from mvpa2.clfs.meta import MulticlassClassifier

    from mvpa2.datasets import Dataset
    from mvpa2.clfs.svm import LinearCSVMC
    #import mvpa2.testing.datasets
    #reload(mvpa2.testing.datasets)
    from mvpa2.testing.datasets import datasets
    from mvpa2.generators.partition import NFoldPartitioner, OddEvenPartitioner
    from mvpa2.measures.base import CrossValidation

    from mvpa2.testing import ok_, assert_equal, assert_array_equal
    from mvpa2.sandbox.multiclass import get_pairwise_accuracies

    # Some parameters used in the test below
    nproc = 1 + int(mvpa2.externals.exists('pprocess'))
    ntargets = 4                                # number of targets
    npairs = ntargets*(ntargets-1)/2
    center_ids = [35, 55, 1]
    ds = datasets['3dsmall'].copy()

    # redefine C,T so we have a multiclass task
    nsamples = len(ds)
    ds.sa.targets = range(ntargets) * (nsamples//ntargets)
    ds.sa.chunks = np.arange(nsamples) // ntargets
    # and add some obvious signal where it is due
    ds.samples[:, 55] += 15*ds.sa.targets   # for all 4 targets
    ds.samples[:, 35] += 15*(ds.sa.targets % 2) # so we have conflicting labels
    # while 35 would still be just for 2 categories which would conflict

    mclf = MulticlassClassifier(LinearCSVMC(),
                                pass_attr=['sa.chunks', 'ca.raw_predictions_ds'],
                                enable_ca=['raw_predictions_ds'])

    label_pairs = mclf._get_binary_pairs(ds)

    def place_sa_as_samples(ds):
        # add a degenerate dimension for the hstacking in the searchlight
        ds.samples = ds.sa.raw_predictions_ds[:, None]
        ds.sa.pop('raw_predictions_ds')   # no need to drag the copy
        return ds

    mcv = CrossValidation(mclf, OddEvenPartitioner(), errorfx=None,
                          postproc=place_sa_as_samples)
    sl = sphere_searchlight(mcv, nproc=nproc, radius=2, space='myspace',
                            center_ids=center_ids)
    slmap = sl(ds)


    ok_('chunks' in slmap.sa)
    ok_('cvfolds' in slmap.sa)
    ok_('targets' in slmap.sa)
    # so for each SL we got all pairwise tests
    assert_equal(slmap.shape, (nsamples, len(center_ids), npairs))
    assert_array_equal(np.unique(slmap.sa.cvfolds), [0, 1])

    # Verify that we got right labels in each 'pair'
    # all searchlights should have the same set of labels for a given
    # pair of targets
    label_pairs_ = np.apply_along_axis(
        np.unique, 0,
        ## reshape slmap so we have only simple pairs in the columns
        np.reshape(slmap, (-1, npairs))).T

    # need to prep that list of pairs obtained from MulticlassClassifier
    # and since it is 1-vs-1, they all should be just pairs of lists of
    # 1 element so should work
    assert_equal(len(label_pairs_), npairs)
    assert_array_equal(np.squeeze(np.array(label_pairs)), label_pairs_)
    assert_equal(label_pairs_.shape, (npairs, 2))   # for this particular case


    out    = get_pairwise_accuracies(slmap)
    out123 = get_pairwise_accuracies(slmap, select=[1, 2, 3])

    assert_array_equal(np.unique(out123.T), np.arange(1, 4))   # so we got at least correct targets
    # test that we extracted correct accuracies
    # First 3 in out.T should have category 0, so skip them and compare otherwise
    assert_array_equal(out.samples[3:], out123.samples)

    ok_(np.all(out.samples[:, 1] == 1.), "This was with super-strong result")

@reseed_rng()
def test_rfe_sensmap():
    # http://lists.alioth.debian.org/pipermail/pkg-exppsy-pymvpa/2013q3/002538.html
    # just a smoke test. fails with
    from mvpa2.clfs.svm import LinearCSVMC
    from mvpa2.clfs.meta import FeatureSelectionClassifier
    from mvpa2.measures.base import CrossValidation, RepeatedMeasure
    from mvpa2.generators.splitters import Splitter
    from mvpa2.generators.partition import NFoldPartitioner
    from mvpa2.misc.errorfx import mean_mismatch_error
    from mvpa2.mappers.fx import mean_sample
    from mvpa2.mappers.fx import maxofabs_sample
    from mvpa2.generators.base import Repeater
    from mvpa2.featsel.rfe import RFE
    from mvpa2.featsel.helpers import FractionTailSelector, BestDetector
    from mvpa2.featsel.helpers import NBackHistoryStopCrit
    from mvpa2.datasets import vstack

    from mvpa2.misc.data_generators import normal_feature_dataset

    # Let's simulate the beast -- 6 categories total groupped into 3
    # super-ordinate, and actually without any 'superordinate' effect
    # since subordinate categories independent
    fds = normal_feature_dataset(nlabels=3,
                                 snr=1, # 100,   # pure signal! ;)
                                 perlabel=9,
                                 nfeatures=6,
                                 nonbogus_features=range(3),
                                 nchunks=3)
    clfsvm = LinearCSVMC()

    rfesvm = RFE(clfsvm.get_sensitivity_analyzer(postproc=maxofabs_sample()),
                 CrossValidation(
                     clfsvm,
                     NFoldPartitioner(),
                     errorfx=mean_mismatch_error, postproc=mean_sample()),
                 Repeater(2),
                 fselector=FractionTailSelector(0.70, mode='select', tail='upper'),
                 stopping_criterion=NBackHistoryStopCrit(BestDetector(), 10),
                 update_sensitivity=True)

    fclfsvm = FeatureSelectionClassifier(clfsvm, rfesvm)

    sensanasvm = fclfsvm.get_sensitivity_analyzer(postproc=maxofabs_sample())


    # manually repeating/splitting so we do both RFE sensitivity and classification
    senses, errors = [], []
    for i, pset in enumerate(NFoldPartitioner().generate(fds)):
        # split partitioned dataset
        split = [d for d in Splitter('partitions').generate(pset)]
        senses.append(sensanasvm(split[0])) # and it also should train the classifier so we would ask it about error
        errors.append(mean_mismatch_error(fclfsvm.predict(split[1]), split[1].targets))

    senses = vstack(senses)
    errors = vstack(errors)

    # Let's compare against rerunning the beast simply for classification with CV
    errors_cv = CrossValidation(fclfsvm, NFoldPartitioner(), errorfx=mean_mismatch_error)(fds)
    # and they should match
    assert_array_equal(errors, errors_cv)

    # buggy!
    cv_sensana_svm = RepeatedMeasure(sensanasvm, NFoldPartitioner())
    senses_rm = cv_sensana_svm(fds)

    #print senses.samples, senses_rm.samples
    #print errors, errors_cv.samples
    assert_raises(AssertionError,
                  assert_array_almost_equal,
                  senses.samples, senses_rm.samples)
    raise SkipTest("Known failure for repeated measures: https://github.com/PyMVPA/PyMVPA/issues/117")

def test_remove_invariant_as_a_mapper():
    from mvpa2.featsel.helpers import RangeElementSelector
    from mvpa2.featsel.base import StaticFeatureSelection, SensitivityBasedFeatureSelection
    from mvpa2.testing.datasets import datasets
    from mvpa2.datasets.miscfx import remove_invariant_features

    mapper = SensitivityBasedFeatureSelection(
              lambda x: np.std(x, axis=0),
              RangeElementSelector(lower=0, inclusive=False),
              train_analyzer=False,
              auto_train=True)

    ds = datasets['uni2large'].copy()

    ds.a['mapper'] = StaticFeatureSelection(np.arange(ds.nfeatures))
    ds.fa['index'] = np.arange(ds.nfeatures)
    ds.samples[:, [1, 8]] = 10

    ds_out = mapper(ds)

    # Validate that we are getting the same results as remove_invariant_features
    ds_rifs = remove_invariant_features(ds)
    assert_array_equal(ds_out.samples, ds_rifs.samples)
    assert_array_equal(ds_out.fa.index, ds_rifs.fa.index)

    assert_equal(ds_out.fa.index[1], 2)
    assert_equal(ds_out.fa.index[8], 10)

########NEW FILE########
__FILENAME__ = test_verbosity
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA verbose and debug output"""

import unittest, re
from StringIO import StringIO

from mvpa2.base.verbosity import OnceLogger

from mvpa2.base import verbose, error

if __debug__:
    from mvpa2.base import debug
    debug.register('1', 'id 1')           # needed for testing
    debug.register('2', 'id 2')


## XXX There must be smth analogous in python... don't know it yet
# And it is StringIO
#class StringStream(object):
#    def __init__(self):
#        self.__str = ""
#
#    def __repr__(self):
#        return self.__str
#
#    def write(self, s):
#        self.__str += s
#
#    def clean(self):
#        self.__str = ""
#
class VerboseOutputTest(unittest.TestCase):

    def setUp(self):
        self.msg = "Test level 2"
        # output stream
        self.sout = StringIO()

        self.once = OnceLogger(handlers=[self.sout])

        # set verbose to 4th level
        self.__oldverbosehandlers = verbose.handlers
        self.__oldverbose_level = verbose.level
        verbose.handlers = []           # so debug doesn't spoil it
        verbose.level = 4
        if __debug__:
            self.__olddebughandlers = debug.handlers
            self.__olddebugactive = debug.active
            self.__olddebugmetrics = debug.metrics
            debug.active = ['1', '2', 'SLC']
            debug.handlers = [self.sout]
            debug.offsetbydepth = False
        verbose.handlers = [self.sout]

    def tearDown(self):
        if __debug__:
            debug.active = self.__olddebugactive
            debug.handlers = self.__olddebughandlers
            debug.metrics = self.__olddebugmetrics
            debug.offsetbydepth = True
        verbose.handlers = self.__oldverbosehandlers
        verbose.level = self.__oldverbose_level
        self.sout.close()


    def test_verbose_above(self):
        """Test if it doesn't output at higher levels"""
        verbose(5, self.msg)
        self.assertEqual(self.sout.getvalue(), "")


    def test_verbose_below(self):
        """Test if outputs at lower levels and indents
        by default with spaces
        """
        verbose(2, self.msg)
        self.assertEqual(self.sout.getvalue(),
                             "  %s\n" % self.msg)

    def test_verbose_indent(self):
        """Test indent symbol
        """
        verbose.indent = "."
        verbose(2, self.msg)
        self.assertEqual(self.sout.getvalue(), "..%s\n" % self.msg)
        verbose.indent = " "            # restore

    def test_verbose_negative(self):
        """Test if chokes on negative level"""
        self.assertRaises( ValueError,
                               verbose._set_level, -10 )

    def test_no_lf(self):
        """Test if it works fine with no newline (LF) symbol"""
        verbose(2, self.msg, lf=False)
        verbose(2, " continue ", lf=False)
        verbose(2, "end")
        verbose(0, "new %s" % self.msg)
        self.assertEqual(self.sout.getvalue(),
                             "  %s continue end\nnew %s\n" % \
                             (self.msg, self.msg))

    def test_cr(self):
        """Test if works fine with carriage return (cr) symbol"""
        verbose(2, self.msg, cr=True)
        verbose(2, "rewrite", cr=True)
        verbose(1, "rewrite 2", cr=True)
        verbose(1, " add", cr=False, lf=False)
        verbose(1, " finish")
        target = '\r  %s\r              \rrewrite' % self.msg + \
                 '\r       \rrewrite 2 add finish\n'
        self.assertEqual(self.sout.getvalue(), target)

    def test_once_logger(self):
        """Test once logger"""
        self.once("X", self.msg)
        self.once("X", self.msg)
        self.assertEqual(self.sout.getvalue(), self.msg+"\n")

        self.once("Y", "XXX", 2)
        self.once("Y", "XXX", 2)
        self.once("Y", "XXX", 2)
        self.assertEqual(self.sout.getvalue(), self.msg+"\nXXX\nXXX\n")


    def test_error(self):
        """Test error message"""
        error(self.msg, critical=False) # should not exit
        self.assertTrue(self.sout.getvalue().startswith("ERROR"))


    if __debug__:
        def test_debug(self):
            verbose.handlers = []           # so debug doesn't spoil it
            debug.active = ['1', '2', 'SLC']
            debug.metrics = debug._known_metrics.keys()
            # do not offset for this test
            debug('SLC', self.msg, lf=False)
            self.assertRaises(ValueError, debug, 3, 'bugga')
            #Should complain about unknown debug id
            svalue = self.sout.getvalue()
            regexp = "\[SLC\] DBG(?:{.*})?: %s" % self.msg
            rematch = re.match(regexp, svalue)
            self.assertTrue(rematch, msg="Cannot match %s with regexp %s" %
                            (svalue, regexp))
            # find metrics
            self.assertTrue('RSS/VMS:' in svalue,
                            msg="Cannot find vmem metric in " + svalue)
            self.assertTrue('>test_verbosity:' in svalue,
                            msg="Cannot find tbc metric in " + svalue)
            self.assertTrue(' sec' in svalue,
                            msg="Cannot find tbc metric in " + svalue)


        def test_debug_rgexp(self):
            verbose.handlers = []           # so debug doesn't spoil it
            debug.active = ['.*']
            # we should have enabled all of them
            self.assertEqual(set(debug.active),
                                 set(debug.registered.keys()))
            debug.active = ['S.*', 'CLF']
            self.assertEqual(set(debug.active),
                                 set(filter(lambda x:x.startswith('S'),
                                            debug.registered.keys())+['CLF']))
            debug.active = ['SG', 'CLF']
            self.assertEqual(set(debug.active), set(['SG', 'CLF']),
                                 msg="debug should do full line matching")

            debug.offsetbydepth = True


        # TODO: More tests needed for debug output testing

def suite():  # pragma: no cover
    return unittest.makeSuite(VerboseOutputTest)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_viz
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
'''Tests for visualizations'''


from mvpa2 import pymvpa_dataroot
from mvpa2.testing import *
from nose.tools import *

skip_if_no_external('pylab')

import numpy as np

def test_get_lim():
    from mvpa2.viz import _get_lim
    d = np.arange(10)
    assert_equal(_get_lim(d, 'same'), (0, 9))
    assert_raises(ValueError, _get_lim, d, 'wrong')
    assert_equal(_get_lim(d,  None), None)
    assert_equal(_get_lim(d,  (1, 3)), (1, 3))

def test_hist():
    from mvpa2.viz import hist
    from mvpa2.misc.data_generators import normal_feature_dataset
    from matplotlib.axes import Subplot
    ds = normal_feature_dataset(10, 3, 10, 5)
    plots = hist(ds, ygroup_attr='targets', xgroup_attr='chunks',
                 noticks=None, xlim=(-.5, .5), normed=True)
    assert_equal(len(plots), 15)
    for sp in plots:
        assert_is_instance(sp, Subplot)
    # simple case
    plots = hist(ds)
    assert_equal(len(plots), 1)
    assert_is_instance(plots[0], Subplot)
    # make sure it works with plan arrays too
    plots = hist(ds.samples)
    assert_equal(len(plots), 1)
    assert_is_instance(plots[0], Subplot)

def test_imshow():
    from mvpa2.viz import matshow
    from mvpa2.misc.data_generators import normal_feature_dataset
    from matplotlib.colorbar import Colorbar
    ds = normal_feature_dataset(10, 2, 18, 5)
    im = matshow(ds)
    # old mpl returns a tuple of Colorbar which is anyways available as its .ax
    if isinstance(im.colorbar, tuple):
        assert_is_instance(im.colorbar[0], Colorbar)
        assert_true(im.colorbar[1] is im.colorbar[0].ax)
    else:
        # new mpls do it withough unnecessary duplication
        assert_is_instance(im.colorbar, Colorbar)

def test_lightbox():
    skip_if_no_external('nibabel') # used for loading the niftis here
    # smoketest for lightbox - moved from its .py __main__
    from mvpa2.misc.plot.lightbox import plot_lightbox
    fig = plot_lightbox(
        #background = NiftiImage('%s/anat.nii.gz' % impath),
        background = os.path.join(pymvpa_dataroot, 'bold.nii.gz'),
        background_mask = None,
        overlay = os.path.join(pymvpa_dataroot, 'bold.nii.gz'),
        overlay_mask = os.path.join(pymvpa_dataroot, 'mask.nii.gz'),
        #
        do_stretch_colors = False,
        add_colorbar = True,
        cmap_bg = 'gray',
        cmap_overlay = 'hot', # YlOrRd_r # pl.cm.autumn
        #
        fig = None,
        # vlim describes value limits
        # clim color limits (same by default)
        vlim = [1500, None],
        #vlim_type = 'symneg_z',
        interactive = True,
        #
        #nrows = 2,
        #ncolumns = 3,
        add_info = (1, 2),
        add_hist = (0, 2),
        #
        slices = [0]
        )
    assert_true(fig)
########NEW FILE########
__FILENAME__ = test_waveletmapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA Wavelet mappers"""

from mvpa2.testing import *
from mvpa2.testing.datasets import datasets
skip_if_no_external('pywt')

from mvpa2.base import externals

import unittest
from mvpa2.support.copy import deepcopy
import numpy as np

from mvpa2.mappers.boxcar import BoxcarMapper
from mvpa2.mappers.wavelet import *
from mvpa2.datasets import Dataset


class WaveletMappersTests(unittest.TestCase):

    def test_simple_wdm(self):
        """
        """
        ds = datasets['uni2medium']
        d2d = ds.samples
        ws = 15                          # size of timeline for wavelet
        sp = np.arange(ds.nsamples-ws*2) + ws

        # create 3D instance (samples x timepoints x channels)
        bcm = BoxcarMapper(sp, ws)
        d3d = bcm.forward(d2d)

        # use wavelet mapper
        wdm = WaveletTransformationMapper()
        d3d_wd = wdm.forward(d3d)
        d3d_swap = d3d.swapaxes(1,2)

        self.assertRaises(ValueError, WaveletTransformationMapper,
                              wavelet='bogus')
        self.assertRaises(ValueError, WaveletTransformationMapper,
                              mode='bogus')

        # use wavelet mapper
        for wdm, wdm_swap in ((WaveletTransformationMapper(),
                               WaveletTransformationMapper(dim=2)),
                              (WaveletPacketMapper(),
                               WaveletPacketMapper(dim=2))):
          for dd, dd_swap in ((d3d, d3d_swap),
                              (d2d, None)):
            dd_wd = wdm.forward(dd)
            if dd_swap is not None:
                dd_wd_swap = wdm_swap.forward(dd_swap)

                self.assertTrue((dd_wd == dd_wd_swap.swapaxes(1,2)).all(),
                                msg="We should have got same result with swapped "
                                "dimensions and explicit mentioining of it. "
                                "Got %s and %s" % (dd_wd, dd_wd_swap))

            # some sanity checks
            self.assertTrue(dd_wd.shape[0] == dd.shape[0])

            if not isinstance(wdm, WaveletPacketMapper):
                # we can do reverse only for DWT
                dd_rev = wdm.reverse(dd_wd)
                # inverse transform might be not exactly as the
                # input... but should be very close ;-)
                self.assertEqual(dd_rev.shape, dd.shape,
                                     msg="Shape should be the same after iDWT")

                diff = np.linalg.norm(dd - dd_rev)
                ornorm = np.linalg.norm(dd)
                self.assertTrue(diff/ornorm < 1e-10)


    def test_simple_wp1_level(self):
        """
        """

        ds = datasets['uni2large']
        d2d = ds.samples
        ws = 50                          # size of timeline for wavelet
        sp = (np.arange(ds.nsamples - ws*2) + ws)[:4]

        # create 3D instance (samples x timepoints x channels)
        bcm = BoxcarMapper(sp, ws)
        d3d = bcm.forward(d2d)

        # use wavelet mapper
        wdm = WaveletPacketMapper(level=2, wavelet='sym2')
        d3d_wd = wdm.forward(d3d)

        # Check dimensionality
        d3d_wds, d3ds = d3d_wd.shape, d3d.shape
        self.assertTrue(len(d3d_wds) == len(d3ds)+1)
        self.assertTrue(d3d_wds[1] * d3d_wds[2] >= d3ds[1])
        self.assertTrue(d3d_wds[0] == d3ds[0])
        self.assertTrue(d3d_wds[-1] == d3ds[-1])
        #print d2d.shape, d3d.shape, d3d_wd.shape

        if externals.exists('pywt wp reconstruct'):
            # Test reverse -- should be identical
            # we can do reverse only for DWT
            d3d_rev = wdm.reverse(d3d_wd)

            # inverse transform might be not exactly as the
            # input... but should be very close ;-)
            self.assertEqual(d3d_rev.shape, d3d.shape,
                                 msg="Shape should be the same after iDWT")

            diff = np.linalg.norm(d3d - d3d_rev)
            ornorm = np.linalg.norm(d3d)

            skip_if_no_external('pywt wp reconstruct fixed')
            self.assertTrue(diff/ornorm < 1e-10)
        else:
            self.assertRaises(NotImplementedError, wdm.reverse, d3d_wd)


    ##REF: Name was automagically refactored
    def _test_compare_to_old(self):  # pragma: no cover
        """Good just to compare if I didn't screw up anything... treat
        it as a regression test
        """
        import mvpa2.mappers.wavelet_ as wavelet_

        ds = datasets['uni2medium']
        d2d = ds.samples
        ws = 16                          # size of timeline for wavelet
        sp = np.arange(ds.nsamples-ws*2) + ws

        # create 3D instance (samples x timepoints x channels)
        bcm = BoxcarMapper(sp, ws)
        d3d = bcm.forward(d2d)

        # use wavelet mapper
        for wdm, wdm_ in ((WaveletTransformationMapper(),
                           wavelet_.WaveletTransformationMapper()),
                          (WaveletPacketMapper(),
                           wavelet_.WaveletPacketMapper()),):
            d3d_wd = wdm(d3d)
            d3d_wd_ = wdm_(d3d)

            self.assertTrue((d3d_wd == d3d_wd_).all(),
                msg="We should have got same result with old and new code. "
                    "Got %s and %s" % (d3d_wd, d3d_wd_))


def suite():  # pragma: no cover
    return unittest.makeSuite(WaveletMappersTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_winner
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA searchlight algorithm"""

import unittest
import numpy as np

from mvpa2.base import cfg
from mvpa2.datasets.base import Dataset
from mvpa2.measures.winner import feature_winner_measure, \
                                  feature_loser_measure, \
                                  sample_winner_measure, \
                                  sample_loser_measure, \
                                  group_sample_winner_measure, \
                                  group_sample_loser_measure

from mvpa2.testing import assert_array_equal, assert_true

# if you need some classifiers
#from mvpa2.testing.clfs import *

class WinnerTests(unittest.TestCase):
    def test_winner(self):
        ns = 4
        nf = 3
        n = ns * nf
        ds = Dataset(np.reshape(np.mod(np.arange(0, n * 5, 5) + .5 * n, n), (ns, nf)),
                     sa=dict(targets=[0, 0, 1, 1], x=[3, 2, 1, 0]),
                     fa=dict(v=[3, 2, 1], w=['a', 'b', 'c']))

        measures2out = {feature_winner_measure : [1, 0, 2, 1],
                        feature_loser_measure: [2, 1, 0, 2],
                        sample_winner_measure: [1, 0, 2],
                        sample_loser_measure:[2, 1, 3],
                        group_sample_winner_measure:[0, 0, 0],
                        group_sample_loser_measure: [1, 0, 0]}

        for m, out in measures2out.iteritems():
            assert_array_equal(m()(ds).samples.ravel(), np.asarray(out))

def suite():  # pragma: no cover
    return unittest.makeSuite(WinnerTests)


if __name__ == '__main__':  # pragma: no cover
    import runner
    runner.run()


########NEW FILE########
__FILENAME__ = test_zscoremapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Unit tests for PyMVPA ZScore mapper"""


from mvpa2.base import externals

from mvpa2.support.copy import deepcopy
import numpy as np

from mvpa2.datasets.base import dataset_wizard
from mvpa2.mappers.zscore import ZScoreMapper, zscore
from mvpa2.testing.tools import assert_array_almost_equal, assert_array_equal, \
        assert_equal, assert_raises, ok_, nodebug
from mvpa2.misc.support import idhash

from mvpa2.testing.datasets import datasets

def test_mapper_vs_zscore():
    """Test by comparing to results of elderly z-score function
    """
    # data: 40 sample feature line in 20d space (40x20; samples x features)
    dss = [
        dataset_wizard(np.concatenate(
            [np.arange(40) for i in range(20)]).reshape(20,-1).T,
                targets=1, chunks=1),
        ] + datasets.values()

    for ds in dss:
        ds1 = deepcopy(ds)
        ds2 = deepcopy(ds)

        zsm = ZScoreMapper(chunks_attr=None)
        assert_raises(RuntimeError, zsm.forward, ds1.samples)
        idhashes = (idhash(ds1), idhash(ds1.samples))
        zsm.train(ds1)
        idhashes_train = (idhash(ds1), idhash(ds1.samples))
        assert_equal(idhashes, idhashes_train)

        # forward dataset
        ds1z_ds = zsm.forward(ds1)
        idhashes_forwardds = (idhash(ds1), idhash(ds1.samples))
        # must not modify samples in place!
        assert_equal(idhashes, idhashes_forwardds)

        # forward samples explicitly
        ds1z = zsm.forward(ds1.samples)
        idhashes_forward = (idhash(ds1), idhash(ds1.samples))
        assert_equal(idhashes, idhashes_forward)

        zscore(ds2, chunks_attr=None)
        assert_array_almost_equal(ds1z, ds2.samples)
        assert_array_equal(ds1.samples, ds.samples)

@nodebug(['ID_IN_REPR', 'MODULE_IN_REPR'])
def test_zcore_repr():
    # Just basic test if everything is sane... no proper comparison
    for m in (ZScoreMapper(chunks_attr=None),
              ZScoreMapper(params=(3, 1)),
              ZScoreMapper()):
        mr = eval(repr(m))
        ok_(isinstance(mr, ZScoreMapper))

def test_zscore():
    """Test z-scoring transformation
    """
    # dataset: mean=2, std=1
    samples = np.array((0, 1, 3, 4, 2, 2, 3, 1, 1, 3, 3, 1, 2, 2, 2, 2)).\
        reshape((16, 1))
    data = dataset_wizard(samples.copy(), targets=range(16), chunks=[0] * 16)
    assert_equal(data.samples.mean(), 2.0)
    assert_equal(data.samples.std(), 1.0)
    data_samples = data.samples.copy()
    zscore(data, chunks_attr='chunks')

    # copy should stay intact
    assert_equal(data_samples.mean(), 2.0)
    assert_equal(data_samples.std(), 1.0)
    # we should be able to operate on ndarrays
    # But we can't change type inplace for an array, can't we?
    assert_raises(TypeError, zscore, data_samples, chunks_attr=None)
    # so lets do manually
    data_samples = data_samples.astype(float)
    zscore(data_samples, chunks_attr=None)
    assert_array_equal(data.samples, data_samples)

    # check z-scoring
    check = np.array([-2, -1, 1, 2, 0, 0, 1, -1, -1, 1, 1, -1, 0, 0, 0, 0],
                    dtype='float64').reshape(16, 1)
    assert_array_equal(data.samples, check)

    data = dataset_wizard(samples.copy(), targets=range(16), chunks=[0] * 16)
    zscore(data, chunks_attr=None)
    assert_array_equal(data.samples, check)

    # check z-scoring taking set of labels as a baseline
    data = dataset_wizard(samples.copy(),
                   targets=[0, 2, 2, 2, 1] + [2] * 11,
                   chunks=[0] * 16)
    zscore(data, param_est=('targets', [0, 1]))
    assert_array_equal(samples, data.samples + 1.0)

    # check that zscore modifies in-place; only guaranteed if no upcasting is
    # necessary
    samples = samples.astype('float')
    data = dataset_wizard(samples,
                   targets=[0, 2, 2, 2, 1] + [2] * 11,
                   chunks=[0] * 16)
    zscore(data, param_est=('targets', [0, 1]))
    assert_array_equal(samples, data.samples)

    # these might be duplicating code above -- but twice is better than nothing

    # dataset: mean=2, std=1
    raw = np.array((0, 1, 3, 4, 2, 2, 3, 1, 1, 3, 3, 1, 2, 2, 2, 2))
    # dataset: mean=12, std=1
    raw2 = np.array((0, 1, 3, 4, 2, 2, 3, 1, 1, 3, 3, 1, 2, 2, 2, 2)) + 10
    # zscore target
    check = [-2, -1, 1, 2, 0, 0, 1, -1, -1, 1, 1, -1, 0, 0, 0, 0]

    ds = dataset_wizard(raw.copy(), targets=range(16), chunks=[0] * 16)
    pristine = dataset_wizard(raw.copy(), targets=range(16), chunks=[0] * 16)

    zm = ZScoreMapper()
    # should do global zscore by default
    zm.train(ds)                        # train
    assert_array_almost_equal(zm.forward(ds), np.transpose([check]))
    # should not modify the source
    assert_array_equal(pristine, ds)

    # if we tell it a different mean it should obey the order
    zm = ZScoreMapper(params=(3,1))
    zm.train(ds)
    assert_array_almost_equal(zm.forward(ds), np.transpose([check]) - 1 )
    assert_array_equal(pristine, ds)

    # let's look at chunk-wise z-scoring
    ds = dataset_wizard(np.hstack((raw.copy(), raw2.copy())),
                        targets=range(32),
                        chunks=[0] * 16 + [1] * 16)
    # by default chunk-wise
    zm = ZScoreMapper()
    zm.train(ds)                        # train
    assert_array_almost_equal(zm.forward(ds), np.transpose([check + check]))
    # we should be able to do that same manually
    zm = ZScoreMapper(params={0: (2,1), 1: (12,1)})
    zm.train(ds)                        # train
    assert_array_almost_equal(zm.forward(ds), np.transpose([check + check]))

    # And just a smoke test for warnings reporting whenever # of
    # samples per chunk is low.
    # on 1 sample per chunk
    zds1 = ZScoreMapper(chunks_attr='chunks', auto_train=True)(
        ds[[0, -1]])
    ok_(np.all(zds1.samples == 0))   # they all should be 0
    # on 2 samples per chunk
    zds2 = ZScoreMapper(chunks_attr='chunks', auto_train=True)(
        ds[[0, 1, -10, -1]])
    assert_array_equal(np.unique(zds2.samples), [-1., 1]) # they all should be -1 or 1
    # on 3 samples per chunk -- different warning
    ZScoreMapper(chunks_attr='chunks', auto_train=True)(
        ds[[0, 1, 2, -3, -2, -1]])

    # test if std provided as a list not as an array is handled
    # properly -- should zscore all features (not just first/none
    # as it was before)
    ds = dataset_wizard(np.arange(32).reshape((8,-1)),
                        targets=range(8), chunks=[0] * 8)
    means = [0, 1, -10, 10]
    std0 = np.std(ds[:, 0])             # std deviation of first one
    stds = [std0, 10, .1, 1]

    zm = ZScoreMapper(params=(means, stds),
                      auto_train=True)
    dsz = zm(ds)

    assert_array_almost_equal((np.mean(ds, axis=0) - np.asanyarray(means))/np.array(stds),
                              np.mean(dsz, axis=0))

    assert_array_almost_equal(np.std(ds, axis=0)/np.array(stds),
                              np.std(dsz, axis=0))

def test_zscore_withoutchunks():
    # just a smoke test to see if all issues of
    # https://github.com/PyMVPA/PyMVPA/issues/26
    # are fixed
    from mvpa2.datasets import Dataset
    ds = Dataset(np.arange(32).reshape((8,-1)), sa=dict(targets=range(8)))
    zscore(ds, chunks_attr=None)
    assert(np.any(ds.samples != np.arange(32).reshape((8,-1))))
    ds_summary = ds.summary()
    assert(ds_summary is not None)
########NEW FILE########
__FILENAME__ = tutorial_suite
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##

import os
import numpy as np

# later replace with
from mvpa2.suite import *

tutorial_data_path = mvpa2.cfg.get('location', 'tutorial data', default=os.path.curdir)

def get_raw_haxby2001_data(path=os.path.join(tutorial_data_path, 'data'),
                           roi='vt'):
    if roi is 0:
        # this means something special in the searchlight tutorial
        nimg = nb.load(os.path.join(path, 'mask_hoc.nii.gz'))
        nimg_brain = nb.load(os.path.join(path, 'mask_brain.nii.gz'))
        tmpmask = nimg.get_data() == roi
        # trim it down to the lower anterior quadrant
        tmpmask[:, :, tmpmask.shape[-1]/2:] = False
        tmpmask[:, :tmpmask.shape[1]/2] = False
        tmpmask[nimg_brain.get_data() > 0] = False
        mask = nb.Nifti1Image(tmpmask.astype(int), None, nimg.get_header())
        attr = SampleAttributes(os.path.join(path, 'attributes.txt'))
        ds = fmri_dataset(samples=os.path.join(path, 'bold.nii.gz'),
                          targets=attr.targets, chunks=attr.chunks,
                          mask=mask)
        return ds
    else:
        return load_datadb_tutorial_data(path=path, roi=roi)


def get_haxby2001_data(path=None, roi='vt'):
    if path is None:
        ds = get_raw_haxby2001_data(roi=roi)
    else:
        ds = get_raw_haxby2001_data(path, roi=roi)

    # do chunkswise linear detrending on dataset
    poly_detrend(ds, polyord=1, chunks_attr='chunks', space='time_coords')

    # mark the odd and even runs
    rnames = {0: 'even', 1: 'odd'}
    ds.sa['runtype'] = [rnames[c % 2] for c in ds.sa.chunks]

    # compute the mean sample per condition and odd vs. even runs
    # aka "constructive interference"
    ds = ds.get_mapped(mean_group_sample(['targets', 'runtype']))

    # XXX suboptimal order: should be zscore->avg
    # but then: where is the difference between this and _alternative()?
    # zscore dataset relative to baseline ('rest') mean
    zscore(ds, param_est=('targets', ['rest']))

    # exclude the rest condition from the dataset
    ds = ds[ds.sa.targets != 'rest']

    return ds


def get_haxby2001_data_alternative(path=None, roi='vt', grp_avg=True):
    if path is None:
        ds = get_raw_haxby2001_data(roi=roi)
    else:
        ds = get_raw_haxby2001_data(path, roi=roi)

    # do chunkswise linear detrending on dataset
    poly_detrend(ds, polyord=1, chunks_attr='chunks', space='time_coords')

    # zscore dataset relative to baseline ('rest') mean
    zscore(ds, param_est=('targets', ['rest']))

    # exclude the rest condition from the dataset
    ds = ds[ds.sa.targets != 'rest']

    # mark the odd and even runs
    rnames = {0: 'even', 1: 'odd'}
    ds.sa['runtype'] = [rnames[c % 2] for c in ds.sa.chunks]

    if grp_avg:
        # compute the mean sample per condition and odd vs. even runs
        # aka "constructive interference"
        ds = ds.get_mapped(mean_group_sample(['targets', 'runtype']))

    return ds


def get_haxby2001_clf():
    clf = kNN(k=1, dfx=one_minus_correlation, voting='majority')
    return clf


def load_tutorial_results(name, path=os.path.join(tutorial_data_path,
                                                  'results')):
    return h5load(os.path.join(path, name + '.hdf5'))

########NEW FILE########
__FILENAME__ = _random
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Helper module for control of RNGs (numpy and stock python)"""

import random
import numpy as np

from mvpa2.base import cfg
if __debug__:
    from mvpa2.base import debug

#
# RNG seeding
#

def get_random_seed():
    """Generate a random int good for seeding RNG via `seed` function"""
    return int(np.random.uniform()*(2**31-1))

if cfg.has_option('general', 'seed'):
    _random_seed = cfg.getint('general', 'seed')
else:
    _random_seed = get_random_seed()

def seed(random_seed=_random_seed):
    """Uniform and combined seeding of all relevant random number
    generators.
    """
    if __debug__:
        debug('RANDOM', 'Reseeding RNGs with %s' % random_seed)
    np.random.seed(random_seed)
    random.seed(random_seed)

seed(_random_seed)

########NEW FILE########
__FILENAME__ = py3tool
#!/usr/bin/env python3
"""
Convert *py files with lib2to3.
Taken from MDP and numpy.
"""

import shutil
import os
import sys
import fnmatch
import lib2to3.main
from io import StringIO


EXTRA_2TO3_FLAGS = {'*': '-x import'}

BASE = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
TEMP = os.path.normpath(os.path.join(BASE, '_py3k'))

def custom_mangling(filename):
    pass

def walk_sync(dir1, dir2, _seen=None):
    if _seen is None:
        seen = {}
    else:
        seen = _seen

    if not dir1.endswith(os.path.sep):
        dir1 = dir1 + os.path.sep

    # Walk through stuff (which we haven't yet gone through) in dir1
    for root, dirs, files in os.walk(dir1):
        sub = root[len(dir1):]
        if sub in seen:
            dirs = [x for x in dirs if x not in seen[sub][0]]
            files = [x for x in files if x not in seen[sub][1]]
            seen[sub][0].extend(dirs)
            seen[sub][1].extend(files)
        else:
            seen[sub] = (dirs, files)
        if not dirs and not files:
            continue
        yield os.path.join(dir1, sub), os.path.join(dir2, sub), dirs, files

    if _seen is None:
        # Walk through stuff (which we haven't yet gone through) in dir2
        for root2, root1, dirs, files in walk_sync(dir2, dir1, _seen=seen):
            yield root1, root2, dirs, files


def sync_2to3(src, dst, clean=False):

    to_convert = []

    for src_dir, dst_dir, dirs, files in walk_sync(src, dst):
        for fn in dirs + files:
            src_fn = os.path.join(src_dir, fn)
            dst_fn = os.path.join(dst_dir, fn)

            # skip temporary etc. files
            if fn.startswith('.#') or fn.endswith('~'):
                continue

            # remove non-existing
            if os.path.exists(dst_fn) and not os.path.exists(src_fn):
                if clean:
                    if os.path.isdir(dst_fn):
                        shutil.rmtree(dst_fn)
                    else:
                        os.unlink(dst_fn)
                continue

            # make directories
            if os.path.isdir(src_fn):
                if not os.path.isdir(dst_fn):
                    os.makedirs(dst_fn)
                continue

            dst_dir = os.path.dirname(dst_fn)
            if os.path.isfile(dst_fn) and not os.path.isdir(dst_dir):
                os.makedirs(dst_dir)

            # don't replace up-to-date files
            try:
                if os.path.isfile(dst_fn) and \
                       os.stat(dst_fn).st_mtime >= os.stat(src_fn).st_mtime:
                    continue
            except OSError:
                pass

            # copy file
            shutil.copyfile(src_fn, dst_fn)

            # add .py files to 2to3 list
            if dst_fn.endswith('.py'):
                to_convert.append((src_fn, dst_fn))

    # run 2to3
    flag_sets = {}
    for fn, dst_fn in to_convert:
        flag = ''
        for pat, opt in EXTRA_2TO3_FLAGS.items():
            if fnmatch.fnmatch(fn, pat):
                flag = opt
                break
        flag_sets.setdefault(flag, []).append(dst_fn)

    for flags, filenames in flag_sets.items():
        if flags == 'skip':
            continue

        _old_stdout = sys.stdout
        try:
            sys.stdout = StringIO()
            lib2to3.main.main("lib2to3.fixes", ['-w'] + flags.split()+filenames)
        finally:
            sys.stdout = _old_stdout

    for fn, dst_fn in to_convert:
        # perform custom mangling
        custom_mangling(dst_fn)

########NEW FILE########
__FILENAME__ = apigen
"""Attempt to generate templates for module reference with Sphinx

XXX - we exclude extension modules

To include extension modules, first identify them as valid in the
``_uri2path`` method, then handle them in the ``_parse_module`` script.

We get functions and classes by parsing the text of .py files.
Alternatively we could import the modules for discovery, and we'd have
to do that for extension modules.  This would involve changing the
``_parse_module`` method to work via import and introspection, and
might involve changing ``discover_modules`` (which determines which
files are modules, and therefore which module URIs will be passed to
``_parse_module``).

NOTE: this is a modified version of a script originally shipped with
the PyMVPA project, which we've adapted for NIPY use.  PyMVPA and NIPY
are both BSD-licensed projects.
"""

# Stdlib imports
import os
import re

# Functions and classes
class ApiDocWriter(object):
    ''' Class for automatic detection and parsing of API docs
    to Sphinx-parsable reST format'''

    # only separating first two levels
    rst_section_levels = ['*', '=', '-', '~', '^']

    def __init__(self,
                 package_name,
                 rst_extension='.rst',
                 package_skip_patterns=None,
                 module_skip_patterns=None,
                 ):
        ''' Initialize package for parsing

        Parameters
        ----------
        package_name : string
            Name of the top-level package.  *package_name* must be the
            name of an importable package
        rst_extension : string, optional
            Extension for reST files, default '.rst'
        package_skip_patterns : None or sequence of {strings, regexps}
            Sequence of strings giving URIs of packages to be excluded
            Operates on the package path, starting at (including) the
            first dot in the package path, after *package_name* - so,
            if *package_name* is ``sphinx``, then ``sphinx.util`` will
            result in ``.util`` being passed for earching by these
            regexps.  If is None, gives default. Default is:
            ['\.tests$']
        module_skip_patterns : None or sequence
            Sequence of strings giving URIs of modules to be excluded
            Operates on the module name including preceding URI path,
            back to the first dot after *package_name*.  For example
            ``sphinx.util.console`` results in the string to search of
            ``.util.console``
            If is None, gives default. Default is:
            ['\.setup$', '\._']
        '''
        if package_skip_patterns is None:
            package_skip_patterns = ['\\.tests$']
        if module_skip_patterns is None:
            module_skip_patterns = ['\\.setup$', '\\._']
        self.package_name = package_name
        self.rst_extension = rst_extension
        self.package_skip_patterns = package_skip_patterns
        self.module_skip_patterns = module_skip_patterns

    def get_package_name(self):
        return self._package_name

    def set_package_name(self, package_name):
        ''' Set package_name

        >>> docwriter = ApiDocWriter('sphinx')
        >>> import sphinx
        >>> docwriter.root_path == sphinx.__path__[0]
        True
        >>> docwriter.package_name = 'docutils'
        >>> import docutils
        >>> docwriter.root_path == docutils.__path__[0]
        True
        '''
        # It's also possible to imagine caching the module parsing here
        self._package_name = package_name
        self.root_module = __import__(package_name)
        self.root_path = self.root_module.__path__[0]
        self.written_modules = None

    package_name = property(get_package_name, set_package_name, None,
                            'get/set package_name')

    def _get_object_name(self, line):
        ''' Get second token in line
        >>> docwriter = ApiDocWriter('sphinx')
        >>> docwriter._get_object_name("  def func():  ")
        'func'
        >>> docwriter._get_object_name("  class Klass(object):  ")
        'Klass'
        >>> docwriter._get_object_name("  class Klass:  ")
        'Klass'
        '''
        name = line.split()[1].split('(')[0].strip()
        # in case we have classes which are not derived from object
        # ie. old style classes
        return name.rstrip(':')

    def _uri2path(self, uri):
        ''' Convert uri to absolute filepath

        Parameters
        ----------
        uri : string
            URI of python module to return path for

        Returns
        -------
        path : None or string
            Returns None if there is no valid path for this URI
            Otherwise returns absolute file system path for URI

        Examples
        --------
        >>> docwriter = ApiDocWriter('sphinx')
        >>> import sphinx
        >>> modpath = sphinx.__path__[0]
        >>> res = docwriter._uri2path('sphinx.builder')
        >>> res == os.path.join(modpath, 'builder.py')
        True
        >>> res = docwriter._uri2path('sphinx')
        >>> res == os.path.join(modpath, '__init__.py')
        True
        >>> docwriter._uri2path('sphinx.does_not_exist')

        '''
        if uri == self.package_name:
            return os.path.join(self.root_path, '__init__.py')
        path = uri.replace('.', os.path.sep)
        path = path.replace(self.package_name + os.path.sep, '')
        path = os.path.join(self.root_path, path)
        # XXX maybe check for extensions as well?
        if os.path.exists(path + '.py'): # file
            path += '.py'
        elif os.path.exists(os.path.join(path, '__init__.py')):
            path = os.path.join(path, '__init__.py')
        else:
            return None
        return path

    def _path2uri(self, dirpath):
        ''' Convert directory path to uri '''
        relpath = dirpath.replace(self.root_path, self.package_name)
        if relpath.startswith(os.path.sep):
            relpath = relpath[1:]
        return relpath.replace(os.path.sep, '.')

    def _parse_module(self, uri):
        ''' Parse module defined in *uri* '''
        filename = self._uri2path(uri)
        if filename is None:
            # nothing that we could handle here.
            return ([],[])
        f = open(filename, 'rt')
        functions, classes = self._parse_lines(f)
        f.close()
        return functions, classes

    def _parse_lines(self, linesource):
        ''' Parse lines of text for functions and classes '''
        functions = []
        classes = []
        for line in linesource:
            if line.startswith('def ') and line.count('('):
                # exclude private stuff
                name = self._get_object_name(line)
                if not name.startswith('_'):
                    functions.append(name)
            elif line.startswith('class '):
                # exclude private stuff
                name = self._get_object_name(line)
                if not name.startswith('_'):
                    classes.append(name)
            else:
                pass
        functions.sort()
        classes.sort()
        return functions, classes

    def generate_api_doc(self, uri):
        '''Make autodoc documentation template string for a module

        Parameters
        ----------
        uri : string
            python location of module - e.g 'sphinx.builder'

        Returns
        -------
        S : string
            Contents of API doc
        '''
        # get the names of all classes and functions
        functions, classes = self._parse_module(uri)
        if not len(functions) and not len(classes):
            print 'WARNING: Empty -',uri  # dbg
            return ''

        # Make a shorter version of the uri that omits the package name for
        # titles 
        uri_short = re.sub(r'^%s\.' % self.package_name,'',uri)

        ad = '.. AUTO-GENERATED FILE -- DO NOT EDIT!\n\n'

        chap_title = uri_short
        ad += (chap_title+'\n'+ self.rst_section_levels[1] * len(chap_title)
               + '\n\n')

        # Set the chapter title to read 'module' for all modules except for the
        # main packages
        if '.' in uri:
            title = 'Module: :mod:`' + uri_short + '`'
        else:
            title = ':mod:`' + uri_short + '`'
        ad += title + '\n' + self.rst_section_levels[2] * len(title)

        #if len(classes):
        #    ad += '\nInheritance diagram for ``%s``:\n\n' % uri
        #    ad += '.. inheritance-diagram:: %s \n' % uri
        #    ad += '   :parts: 3\n'

        ad += '\n.. automodule:: ' + uri + '\n'
        ad += '\n.. currentmodule:: ' + uri + '\n'
        multi_class = len(classes) > 1
        multi_fx = len(functions) > 1
        if multi_class:
            ad += '\n' + 'Classes' + '\n' + \
                  self.rst_section_levels[2] * 7 + '\n'
        elif len(classes) and multi_fx:
            ad += '\n' + 'Class' + '\n' + \
                  self.rst_section_levels[2] * 5 + '\n'
        for c in classes:
            ad += '\n:class:`' + c + '`\n' \
                  + self.rst_section_levels[multi_class + 2 ] * \
                  (len(c)+9) + '\n\n'
            ad += '\n.. autoclass:: ' + c + '\n'
            # must NOT exclude from index to keep cross-refs working
            ad += '  :members:\n' \
                  '  :undoc-members:\n'# \
                  #'  :show-inheritance:\n'
        if multi_fx:
            ad += '\n' + 'Functions' + '\n' + \
                  self.rst_section_levels[2] * 9 + '\n\n'
        elif len(functions) and multi_class:
            ad += '\n' + 'Function' + '\n' + \
                  self.rst_section_levels[2] * 8 + '\n\n'
        for f in functions:
            # must NOT exclude from index to keep cross-refs working
            ad += '\n.. autofunction:: ' + uri + '.' + f + '\n\n'
        return ad

    def _survives_exclude(self, matchstr, match_type):
        ''' Returns True if *matchstr* does not match patterns

        ``self.package_name`` removed from front of string if present

        Examples
        --------
        >>> dw = ApiDocWriter('sphinx')
        >>> dw._survives_exclude('sphinx.okpkg', 'package')
        True
        >>> dw.package_skip_patterns.append('^\\.badpkg$')
        >>> dw._survives_exclude('sphinx.badpkg', 'package')
        False
        >>> dw._survives_exclude('sphinx.badpkg', 'module')
        True
        >>> dw._survives_exclude('sphinx.badmod', 'module')
        True
        >>> dw.module_skip_patterns.append('^\\.badmod$')
        >>> dw._survives_exclude('sphinx.badmod', 'module')
        False
        '''
        if match_type == 'module':
            patterns = self.module_skip_patterns
        elif match_type == 'package':
            patterns = self.package_skip_patterns
        else:
            raise ValueError('Cannot interpret match type "%s"' 
                             % match_type)
        # Match to URI without package name
        L = len(self.package_name)
        if matchstr[:L] == self.package_name:
            matchstr = matchstr[L:]
        for pat in patterns:
            try:
                pat.search
            except AttributeError:
                pat = re.compile(pat)
            if pat.search(matchstr):
                return False
        return True

    def discover_modules(self):
        ''' Return module sequence discovered from ``self.package_name`` 


        Parameters
        ----------
        None

        Returns
        -------
        mods : sequence
            Sequence of module names within ``self.package_name``

        Examples
        --------
        >>> dw = ApiDocWriter('sphinx')
        >>> mods = dw.discover_modules()
        >>> 'sphinx.util' in mods
        True
        >>> dw.package_skip_patterns.append('\.util$')
        >>> 'sphinx.util' in dw.discover_modules()
        False
        >>> 
        '''
        modules = [self.package_name]
        # raw directory parsing
        for dirpath, dirnames, filenames in os.walk(self.root_path):
            # Check directory names for packages
            root_uri = self._path2uri(os.path.join(self.root_path,
                                                   dirpath))
            for dirname in dirnames[:]: # copy list - we modify inplace
                package_uri = '.'.join((root_uri, dirname))
                if (self._uri2path(package_uri) and
                    self._survives_exclude(package_uri, 'package')):
                    modules.append(package_uri)
                else:
                    dirnames.remove(dirname)
            # Check filenames for modules
            for filename in filenames:
                module_name = filename[:-3]
                module_uri = '.'.join((root_uri, module_name))
                if (self._uri2path(module_uri) and
                    self._survives_exclude(module_uri, 'module')):
                    modules.append(module_uri)
        return sorted(modules)

    def write_modules_api(self, modules,outdir):
        # write the list
        written_modules = []
        for m in modules:
            api_str = self.generate_api_doc(m)
            if not api_str:
                continue
            # write out to file
            outfile = os.path.join(outdir,
                                   m + self.rst_extension)
            fileobj = open(outfile, 'wt')
            fileobj.write(api_str)
            fileobj.close()
            written_modules.append(m)
        self.written_modules = written_modules

    def write_api_docs(self, outdir):
        """Generate API reST files.

        Parameters
        ----------
        outdir : string
            Directory name in which to store files
            We create automatic filenames for each module

        Returns
        -------
        None

        Notes
        -----
        Sets self.written_modules to list of written modules
        """
        if not os.path.exists(outdir):
            os.mkdir(outdir)
        # compose list of modules
        modules = self.discover_modules()
        self.write_modules_api(modules,outdir)

    def write_index(self, outdir, froot='gen', relative_to=None):
        """Make a reST API index file from written files

        Parameters
        ----------
        path : string
            Filename to write index to
        outdir : string
            Directory to which to write generated index file
        froot : string, optional
            root (filename without extension) of filename to write to
            Defaults to 'gen'.  We add ``self.rst_extension``.
        relative_to : string
            path to which written filenames are relative.  This
            component of the written file path will be removed from
            outdir, in the generated index.  Default is None, meaning,
            leave path as it is.
        """
        if self.written_modules is None:
            raise ValueError('No modules written')
        # Get full filename path
        path = os.path.join(outdir, froot+self.rst_extension)
        # Path written into index is relative to rootpath
        if relative_to is not None:
            relpath = outdir.replace(relative_to + os.path.sep, '')
        else:
            relpath = outdir
        idx = open(path,'wt')
        w = idx.write
        w('.. AUTO-GENERATED FILE -- DO NOT EDIT!\n\n')
        w('.. toctree::\n\n')
        for f in self.written_modules:
            w('   %s\n' % os.path.join(relpath,f))
        idx.close()

########NEW FILE########
__FILENAME__ = bib2rst_ref
#!/usr/bin/env python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
import _bibtex
import re

##REF: Name was automagically refactored
def compare_bib_by_date(a, b):
    """Sorting helper."""
    x = a[1][1]
    y = b[1][1]

    if x.has_key('year'):
        if y.has_key('year'):
            if x['year'].isdigit():
                if y['year'].isdigit():
                    # x and y have dates
                    xyear = int( x['year'] )
                    yyear = int( y['year'] )

                    comp =  cmp(xyear, yyear)

                    if comp == 0:
                        return compare_bib_by_author(a,b)
                    else:
                        return (-1)*comp
                else:
                    # x has date, y not -> y is first
                    return 1
            else:
                if y['year'][0].isdigit():
                    return -1
                else:
                    return compare_bib_by_author(a,b)
        else:
            # only x has date
            return 1
    else:
        if y.has_key('year'):
            return -1
        else:
            # neither nor y have dates
            return compare_bib_by_author(a, b)


##REF: Name was automagically refactored
def compare_bib_by_author(a,b):
    """Sorting helper."""
    x = a[1][1]
    y = b[1][1]

    if x.has_key('author'):
        if y.has_key('author'):
            return cmp(join_author_list(x['author']), join_author_list(y['author']))
        else:
            # only x has author
            return 1
    else:
        if y.has_key('author'):
            return -1
        else:
            # neither nor y have authors
            return 0

##REF: Name was automagically refactored
def format_surname(s, keep_full = False):
    """Recieves a string with surname(s) and returns a string with nicely
    concatenated surnames or initals (with dots).
    """
    # clean spaces
    s = s.strip()

    # go home if empty
    if not len(s):
        return ''

    if not keep_full:
        # only keep initial
        s = s[0]

    if len(s) == 1:
        # add final dot
        s += '.'

    return s


##REF: Name was automagically refactored
def format_author(s, full_surname = False):
    """ Takes a string as argument an tries to determine the lastname and 
    surname(s) of a single author.

    Returns a string with 'lastname, surname(s)'.

    The function takes care of 'von's and other funny prefixes.
    """
    s = s.strip()

    # nothing? take ball, go home
    if not len(s):
        return s

    if s.count(','):
        # assume we have 'lastname, surname(s)'
        slist = s.split(',')
        # take lastname verbatim
        lastname = slist[0].strip()
        # remerge possible surnames with spaces if any
        surnames = u' '.join(slist[1:])

        # get nicely formated surnames concat with spaces
        surname = u' '.join( [ format_surname(i, full_surname) for i in surnames.split() ] )


    else:
        # assume last entity is lastname the rest is surnames
        # check for lastname prefixes
        slist = s.split()
        if len(slist) < 2:
            # only lastname -> finished
            return slist[0]

        # check for order
        if len(slist[-1]) == 1 or slist[-1].endswith('.'):
            # seems like we have lastname->surname order
            if slist[0] in ('von', 'van'):
                lastname = slist[0] + ' ' + slist[1]
                surnames = u' '.join(slist[2:])
            else:
                lastname = slist[0]
                surnames = u' '.join(slist[1:])

        else:
            # the lastname is last
            lastname = slist[-1]

            if slist[-2] in ('von', 'van'):
                lastname = slist[-2] + u' ' + lastname
                surnames = u' '.join(slist[:-2])
            else:
                surnames = u' '.join(slist[:-1])

        surname = u' '.join( [ format_surname(i, full_surname) for i in surnames.split() ] )

    return lastname + u', ' + surname


##REF: Name was automagically refactored
def join_author_list(alist):
    """ Nicely concatenate a list of author with ', ' and a final ' & '.

    Each author is passed to format_author() internally.
    """
    if not len(alist) > 1:
        return format_author(alist[0])

    ret = u', '.join( [ format_author(a) for a in alist[:-1] ] )

    ret += u' & ' + format_author( alist[-1] )

    return ret


##REF: Name was automagically refactored
def format_property(string, indent, max_length = 80):
    """ Helper function to place linebreaks and indentation for
    pretty printing.
    """
    length = len(string)

    lines = []
    pos = 0

    while pos < length:
        if not pos == 0:
            justify = ''.ljust(indent)
            line_length = max_length - indent
        else:
            justify = ''
            line_length = max_length

        if length - pos > line_length:
            lastspace = string.rfind(' ', pos + 1, pos + line_length)
        else:
            lastspace = length

        if lastspace == -1 or lastspace < indent + 1:
            lastspace = string.find(' ', pos + line_length)
            # if no space in the whole string
            if lastspace == -1:
                lastspace = length

        lines.append(justify + string[pos:lastspace])

        pos = lastspace + 1

    return '\n'.join(lines)


class BibTeX(dict):
    """Read bibtex file as dictionary.

    Each entry is accessible by its bibtex ID. An entry is a two-tuple
    `(item_type, dict)`, where `item_type` is eg. article, book, ... and
    `dict` is a dictionary with all bibtex properties for the respective
    item. In this dictionary all properties are store as plain strings,
    except for the list of authors (which is a list of strings) and the pages
    which is a two-tuple with first and last page.
    """
    def __init__(self, filename = None):

        if not filename == None:
            self.open(filename)

        # spaces to be used for indentation
        self.indent = 17

        # maximum line length
        self.line_length = 80


    def open(self, filename):
        """Read and parse bibtex file using python-bibtex."""
        # figure out what the second argument means
        file = _bibtex.open_file(filename, 1)

        while 1:
            entry = _bibtex.next(file)

            if entry == None: break

            eprops = {}

            for k,v in entry[4].iteritems():
                # figure out what the last argument really does
                # leaving in -1 seems to be save
                value = _bibtex.expand(file, v,  0)[2]
                try:
                    value = unicode(value, 'utf-8')
                except UnicodeDecodeError, e:
                    print "ERROR: Failed to decode string '%s'" % value
                    raise
                if k.lower() == 'author':
                    value = value.split(' and ')

                if k.lower() == 'pages':
                    value = tuple(value.replace('-', ' ').split())

                eprops[k] = value

            # bibtex key is dict key
            self[entry[0]] = (entry[1],eprops)


    def __str__(self):
        """Pretty print in bibtex format."""
        bibstring = ''

        for k, v in self.iteritems():
            bibstring += '@' + v[0] + ' { ' + k

            for ek, ev in v[1].iteritems():
                if ek.lower() == 'author':
                    ev = ' and '.join(ev)
                if ek.lower() == 'pages':
                    ev = '--'.join(ev)
                keyname = '  ' + ek

                bibstring += ',\n'
                bibstring += format_property( keyname.ljust(15) + '= {' + ev + '}', 
                                         self.indent,
                                         self.line_length )

            bibstring += "\n}\n\n"


        return bibstring.encode(self.enc)


def bib2rst_references(bib):
    """Compose the reference page."""
    # do it in unicode
    rst = u''
    intro = open('doc/misc/references.in').readlines()
    rst += intro[0]
    rst += "  #\n  # THIS IS A GENERATED FILE -- DO NOT EDIT!\n  #\n"
    rst += ''.join(intro[1:])
    rst += '\n\n'

    biblist = bib.items()
    biblist.sort(compare_bib_by_author)

    for id, (cat, prop) in biblist:
        # put reference target for citations
        rst += '.. _' + id + ':\n\n'

        # compose the citation as the list item label
        cit = u''
        # initial details equal for all item types
        if prop.has_key('author'):
            cit += u'**' + join_author_list(prop['author']) + u'**'
        if prop.has_key('year'):
            cit += ' (' + prop['year'] + ').'
        if prop.has_key('title'):
            cit += ' ' + smooth_rst(prop['title'])
            if not prop['title'].endswith('.'):
                cit += '.'

        # appendix for journal articles
        if cat.lower() == 'article':
            # needs to have journal, volume, pages
            cit += ' *' + prop['journal'] + '*'
            if prop.has_key('volume'):
                cit += ', *' + prop['volume'] + '*'
            if prop.has_key('pages'):
                cit += ', ' + '-'.join(prop['pages'])
        elif cat.lower() == 'book':
            # needs to have publisher, address
            cit += ' ' + prop['publisher']
            cit += ': ' + prop['address']
        elif cat.lower() == 'manual':
            cit += ' ' + prop['address']
        elif cat.lower() == 'inproceedings':
            cit += ' ' + prop['booktitle']
            if prop.has_key('pages'):
                cit += ', ' + '-'.join(prop['pages'])
        else:
            print "WARNING: Cannot handle bibtex item type:", cat

        cit += '.'

        # beautify citation with linebreaks and proper indentation
        # damn, no. list label has to be a single line... :(
        #rst += format_property(cit, 0)
        rst += cit

        # place optional paper summary
        if prop.has_key('pymvpa-summary'):
            rst += '\n  *' + format_property(prop['pymvpa-summary'], 2) + '*\n'

        # make keywords visible
        if prop.has_key('pymvpa-keywords'):
            rst += '\n  Keywords: ' \
                   + ', '.join([':keyword:`' + kw.strip() + '`' 
                                for kw in prop['pymvpa-keywords'].split(',')]) \
                   + '\n'

        # place DOI link
        if prop.has_key('doi'):
            rst += '\n  DOI: '
            if not prop['doi'].startswith('http://dx.doi.org/'):
                rst += 'http://dx.doi.org/'
            rst += prop['doi']
            rst += '\n'
        # use URL (even if DOI is available -- might lead to a copy outside of the paywall)
        if prop.has_key('url'):
            rst += '\n  URL: ' + prop['url'] + '\n'

        rst += '\n\n'

    # end list with blank line
    rst += '\n\n'

    return rst.encode('utf-8')


##REF: Name was automagically refactored
def smooth_rst(s):
    """Replace problematic stuff with less problematic stuff."""
    s = re.sub("``", '"', s)
    # assuming that empty strings to not occur in a bib file
    s = re.sub("''", '"', s)

    return s


# do it
bib = BibTeX('doc/misc/references.bib')

refpage = open('doc/source/references.rst', 'w')
refpage.write(bib2rst_references(bib))
refpage.close()

########NEW FILE########
__FILENAME__ = build_modref_templates
#!/usr/bin/env python
"""Script to auto-generate our API docs."""

import os

from apigen import ApiDocWriter

if __name__ == '__main__':
    package = 'mvpa'
    outdir = os.path.join('build', 'doc', 'modref')
    docwriter = ApiDocWriter(package, rst_extension='.rst')
    #docwriter.package_skip_patterns += ['\\.fixes$',
    #                                    '\\.externals$']
    docwriter.write_api_docs(outdir)
    #docwriter.write_index(outdir, 'gen', relative_to='api')
    print '%d files written' % len(docwriter.written_modules)

########NEW FILE########
__FILENAME__ = mpkg_wrapper
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Simple wrapper to use setuptools extension bdist_mpkg with PyMVPA
distutils setup.py.

This script is a minimal version of a wrapper script shipped with the
bdist_mpkg packge.
"""

__docformat__ = 'restructuredtext'

import sys
import setuptools
import bdist_mpkg

def main():
    del sys.argv[0]
    sys.argv.insert(1, 'bdist_mpkg')
    g = dict(globals())
    g['__file__'] = sys.argv[0]
    g['__name__'] = '__main__'
    execfile(sys.argv[0], g, g)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = refactor_parameters
#!/usr/bin/python
# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""A little helper to convert docstrings from epydoc/rest to match
numpy convention
"""

import sys, re

fname = sys.argv[1]
mappings = {':Parameters?:': 'Parameters',
            ':Examples?:': 'Examples',
            ':Raises:': 'Raises',
            '.. note::': 'Notes',
            '.. seealso::': 'See Also',
            ':Returns:': 'Returns'}

alltext = ''.join(open(fname).readlines())
counts = {}
for mn, mt in mappings.iteritems():
    reparam = re.compile('(?P<spaces>\n *)(?P<header>'
                         + mn
                         + ')(?P<body>\n.*?(?:\n\s*\n|"""))',
                         flags=re.DOTALL)
    count = 0
    #for i in [1,2]:#
    while True:
        res = reparam.search(alltext)
        if not res:
            break
        #print ">", res.group(), "<"
        resd = res.groupdict()
        s, e = res.start(), res.end()

        # Lets adjust alltext
        body = resd['body']
        for i in xrange(2):
            body = body.replace(resd['spaces'] + ' ', resd['spaces'])
        # if any 4-spaces survived
        # body = body.replace(resd['spaces'] + '    ', resd['spaces'] + '  ')
        body = body.replace('``', '`')
        body = re.sub(' *\| *', ' or ', body)
        body = body.replace('basestring', 'str')
        body = body.replace('basestr', 'str')
        adjusted = resd['spaces'] + mt + resd['spaces'] \
                   + '-'*len(mt) + body
        #remove Cheap initialization.
        #print "Converted >%s< to >%s<" % (res.group(), adjusted)
        alltext = alltext[:s] + adjusted + alltext[e:]
        count += 1
    counts[mn] = count

print "File %s: %s" % (
    fname, ', '.join(['%s: %i' % i for i in counts.items() if i[1]]))
file(fname, 'w').write(alltext)

########NEW FILE########
