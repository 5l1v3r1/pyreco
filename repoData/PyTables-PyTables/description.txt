Using the Blosc filter from HDF5
================================

In order to register Blosc into your HDF5 application, you only need
to call a function in blosc_filter.h, with the following signature:

    int register_blosc(char **version, char **date)

Calling this will register the filter with the HDF5 library and will
return info about the Blosc release in `**version` and `**date`
char pointers.

A non-negative return value indicates success.  If the registration
fails, an error is pushed onto the current error stack and a negative
value is returned.

An example C program ("example.c") is included which demonstrates the
proper use of the filter.

This filter has been tested against HDF5 versions 1.6.5 through
1.8.10.  It is released under the MIT license (see LICENSE.txt for
details).


Compiling
=========

The filter consists of a single '.c' source file and '.h' header,
along with an embedded version of the BLOSC compression library.
Also, as Blosc uses SSE2 and multithreading, you must remember to use
some special flags and libraries to make sure that these features are
used (only necessary when compiling Blosc from sources).

To compile using GCC on UNIX:

  gcc -O3 -msse2 -lhdf5 ../blosc/*.c blosc_filter.c \
        example.c -o example -lpthread

or, if you have the Blosc library already installed (recommended):

  gcc -O3 -lhdf5 -lblosc blosc_filter.c example.c -o example -lpthread

Using MINGW on Windows:

  gcc -O3 -lhdf5 -lblosc blosc_filter.c example.c -o example

Using Windows and MSVC (2008 or higher recommended):

  cl /Ox /Feexample.exe example.c ..\blosc\*.c blosc_filter.c

Intel ICC compilers should work too.

For activating the support for other compressors than the integrated
BloscLZ (like LZ4, LZ4HC, Snappy or Zlib) see the README file in the
main Blosc directory.


Acknowledgments
===============

This HDF5 filter interface and its example is based in the LZF interface
(http://h5py.alfven.org) by Andrew Collette.

===============================================================
 Blosc: A blocking, shuffling and lossless compression library
===============================================================

:Author: Francesc Alted
:Contact: faltet@gmail.com
:URL: http://www.blosc.org

What is it?
===========

Blosc [1]_ is a high performance compressor optimized for binary data.
It has been designed to transmit data to the processor cache faster
than the traditional, non-compressed, direct memory fetch approach via
a memcpy() OS call.  Blosc is the first compressor (that I'm aware of)
that is meant not only to reduce the size of large datasets on-disk or
in-memory, but also to accelerate memory-bound computations.

It uses the blocking technique (as described in [2]_) to reduce
activity on the memory bus as much as possible. In short, this
technique works by dividing datasets in blocks that are small enough
to fit in caches of modern processors and perform compression /
decompression there.  It also leverages, if available, SIMD
instructions (SSE2) and multi-threading capabilities of CPUs, in order
to accelerate the compression / decompression process to a maximum.

Blosc is actually a metacompressor, that meaning that it can use a range
of compression libraries for performing the actual
compression/decompression. Right now, it comes with integrated support
for BloscLZ (the original one), LZ4, LZ4HC, Snappy and Zlib. Blosc comes
with full sources for all compressors, so in case it does not find the
libraries installed in your system, it will compile from the included
sources and they will be integrated into the Blosc library anyway. That
means that you can trust in having all supported compressors integrated
in Blosc in all supported platforms.

You can see some benchmarks about Blosc performance in [3]_

Blosc is distributed using the MIT license, see LICENSES/BLOSC.txt for
details.

.. [1] http://www.blosc.org
.. [2] http://blosc.org/docs/StarvingCPUs-CISE-2010.pdf
.. [3] http://blosc.org/trac/wiki/SyntheticBenchmarks

Meta-compression and other advantages over existing compressors
===============================================================

Blosc is not like other compressors: it should rather be called a
meta-compressor.  This is so because it can use different compressors
and pre-conditioners (programs that generally improve compression
ratio).  At any rate, it can also be called a compressor because it
happens that it already integrates one compressor and one
pre-conditioner, so it can actually work like so.

Currently it uses BloscLZ, a compressor heavily based on FastLZ
(http://fastlz.org/), and a highly optimized (it can use SSE2
instructions, if available) Shuffle pre-conditioner. However,
different compressors or pre-conditioners may be added in the future.

Blosc is in charge of coordinating the compressor and pre-conditioners
so that they can leverage the blocking technique (described above) as
well as multi-threaded execution (if several cores are available)
automatically. That makes that every compressor and pre-conditioner
will work at very high speeds, even if it was not initially designed
for doing blocking or multi-threading.

Other advantages of Blosc are:

* Meant for binary data: can take advantage of the type size
  meta-information for improved compression ratio (using the
  integrated shuffle pre-conditioner).

* Small overhead on non-compressible data: only a maximum of 16
  additional bytes over the source buffer length are needed to
  compress *every* input.

* Maximum destination length: contrarily to many other
  compressors, both compression and decompression routines have
  support for maximum size lengths for the destination buffer.

* Replacement for memcpy(): it supports a 0 compression level that
  does not compress at all and only adds 16 bytes of overhead. In
  this mode Blosc can copy memory usually faster than a plain
  memcpy().

When taken together, all these features set Blosc apart from other
similar solutions.

Compiling your application with a minimalistic Blosc
====================================================

The minimal Blosc consists of the next files (in blosc/ directory)::

    blosc.h and blosc.c      -- the main routines
    shuffle.h and shuffle.c  -- the shuffle code
    blosclz.h and blosclz.c  -- the blosclz compressor

Just add these files to your project in order to use Blosc.  For
information on compression and decompression routines, see blosc.h.

To compile using GCC (4.4 or higher recommended) on Unix:

.. code-block:: console

   $ gcc -O3 -msse2 -o myprog myprog.c blosc/*.c -lpthread

Using Windows and MINGW:

.. code-block:: console

   $ gcc -O3 -msse2 -o myprog myprog.c blosc\*.c

Using Windows and MSVC (2010 or higher recommended):

.. code-block:: console

  $ cl /Ox /Femyprog.exe myprog.c blosc\*.c

A simple usage example is the benchmark in the bench/bench.c file.
Another example for using Blosc as a generic HDF5 filter is in the
hdf5/ directory.

I have not tried to compile this with compilers other than GCC, clang,
MINGW, Intel ICC or MSVC yet. Please report your experiences with your
own platforms.

Adding support for other compressors (LZ4, LZ4HC, Snappy, Zlib)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want to add support for the LZ4, LZ4HC, Snappy or Zlib
compressors, just add the symbols HAVE_LZ4 (will include both LZ4 and
LZ4HC), HAVE_SNAPPY and HAVE_ZLIB during compilation and add the
libraries. For example, for compiling Blosc with Zlib support do:

.. code-block:: console

   $ gcc -O3 -msse2 -o myprog myprog.c blosc/*.c -lpthread -DHAVE_ZLIB -lz

In the bench/ directory there a couple of Makefile files (one for UNIX
and the other for MinGW) with more complete building examples, like
selecting between libraries or internal sources for the compressors.

Compiling the Blosc library with CMake
======================================

Blosc can also be built, tested and installed using CMake_. Although
this procedure is a bit more invloved than the one described above, it
is the most general because it allows to integrate other compressors
than BloscLZ either from libraries or from internal sources. Hence,
serious library developers should use this way.

The following procedure describes the "out of source" build.

Create the build directory and move into it:

.. code-block:: console

  $ mkdir build
  $ cd build

Now run CMake configuration and optionally specify the installation
directory (e.g. '/usr' or '/usr/local'):

.. code-block:: console

  $ cmake -DCMAKE_INSTALL_PREFIX=your_install_prefix_directory ..

CMake allows to configure Blosc in many different ways, like prefering
internal or external sources for compressors or enabling/disabling
them.  Please note that configuration can also be performed using UI
tools provided by CMake_ (ccmake or cmake-gui):

.. code-block:: console

  $ ccmake ..      # run a curses-based interface
  $ cmake-gui ..   # run a graphical interface

Build, test and install Blosc:

.. code-block:: console

  $ make
  $ make test
  $ make install

The static and dynamic version of the Blosc library, together with
header files, will be installed into the specified
CMAKE_INSTALL_PREFIX.

.. _CMake: http://www.cmake.org

Adding support for other compressors (LZ4, LZ4HC, Snappy, Zlib) with CMake
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The CMake files in Blosc are configured to automatically detect other
compressors like LZ4, LZ4HC, Snappy or Zlib by default.  So as long as
the libraries and the header files for these libraries are accessible,
these will be used by default.

*Note on Zlib*: the library should be easily found on UNIX systems,
although on Windows, you can help CMake to find it by setting the
environment variable 'ZLIB_ROOT' to where zlib 'include' and 'lib'
directories are. Also, make sure that Zlib DDL library is in your
'\Windows' directory.

However, the full sources for LZ4, LZ4HC, Snappy and Zlib have been
included in Blosc too. So, in general, you should not worry about not
having (or CMake not finding) the libraries in your system because in
this case, their sources will be automaticall compiled for you. That
effectively means that you can be confident in having a complete
support for all the supported compression libraries in all supported
platforms.

If you want to force Blosc to use the included compression sources
instead of trying to find the libraries in the system first, you can
switch off the PREFER_EXTERNAL_COMPLIBS CMake option:

.. code-block:: console

  $ cmake -DPREFER_EXTERNAL_COMPLIBS=OFF ..

You can also disable support for some compression libraries:

.. code-block:: console

  $ cmake -DDEACTIVATE_SNAPPY=ON ..

Mac OSX troubleshooting
=======================

If you run into compilation troubles when using Mac OSX, please make
sure that you have installed the command line developer tools.  You
can always install them with:

.. code-block:: console

  $ xcode-select --install

Wrapper for Python
==================

Blosc has an official wrapper for Python.  See:

https://github.com/Blosc/python-blosc

Command line interface and serialization format for Blosc
=========================================================

Blosc can be used from command line by using Bloscpack.  See:

https://github.com/Blosc/bloscpack

Filter for HDF5
===============

For those that want to use Blosc as a filter in the HDF5 library,
there is a sample implementation in the hdf5/ directory.

Mailing list
============

There is an official mailing list for Blosc at:

blosc@googlegroups.com
http://groups.google.es/group/blosc

Acknowledgments
===============

I'd like to thank the PyTables community that have collaborated in the
exhaustive testing of Blosc.  With an aggregate amount of more than
300 TB of different datasets compressed *and* decompressed
successfully, I can say that Blosc is pretty safe now and ready for
production purposes.

Other important contributions:

* Valentin Haenel did a terrific work implementing the support for the
  Snappy compression, fixing typos and improving docs and the plotting
  script.

* Thibault North, with ideas from Oscar Villellas, contributed a way
  to call Blosc from different threads in a safe way.

* The CMake support was initially contributed by Thibault North, and
  Antonio Valentino and Mark Wiebe made great enhancements to it.


----

  **Enjoy data!**

Blosc Header Format
===================

Blosc (as of Version 1.0.0) has the following 16 byte header that stores
information about the compressed buffer::

    |-0-|-1-|-2-|-3-|-4-|-5-|-6-|-7-|-8-|-9-|-A-|-B-|-C-|-D-|-E-|-F-|
      ^   ^   ^   ^ |     nbytes    |   blocksize   |    ctbytes    |
      |   |   |   |
      |   |   |   +--typesize
      |   |   +------flags
      |   +----------versionlz
      +--------------version

Datatypes of the Header Entries
-------------------------------

All entries are little endian.

:version:
    (``uint8``) Blosc format version.
:versionlz:
    (``uint8``) Version of the internal compressor used.
:flags and compressor enumeration:
    (``bitfield``) The flags of the buffer 

    :bit 0 (``0x01``):
        Whether the shuffle filter has been applied or not.
    :bit 1 (``0x02``):
        Whether the internal buffer is a pure memcpy or not.
    :bit 2 (``0x04``):
        Reserved
    :bit 3 (``0x08``):
        Reserved
    :bit 4 (``0x16``):
        Reserved
    :bit 5 (``0x32``):
        Part of the enumeration for compressors.
    :bit 6 (``0x64``):
        Part of the enumeration for compressors.
    :bit 7 (``0x64``):
        Part of the enumeration for compressors.

    The last three bits form an enumeration that allows to use alternative
    compressors.

    :``0``:
        ``blosclz``
    :``1``:
        ``lz4`` or ``lz4hc``
    :``2``:
        ``snappy``
    :``3``:
        ``zlib``

:typesize:
    (``uint8``) Number of bytes for the atomic type.
:nbytes:
    (``uint32``) Uncompressed size of the buffer.
:blocksize:
    (``uint32``) Size of internal blocks.
:ctbytes:
    (``uint32``) Compressed size of the buffer.


Blosc supports threading
========================

Threads are the most efficient way to program parallel code for
multi-core processors, but also the more difficult to program well.
Also, they has a non-negligible start-up time that does not fit well
with a high-performance compressor as Blosc tries to be.

In order to reduce the overhead of threads as much as possible, I've
decided to implement a pool of threads (the workers) that are waiting
for the main process (the master) to send them jobs (basically,
compressing and decompressing small blocks of the initial buffer).

Despite this and many other internal optimizations in the threaded
code, it does not work faster than the serial version for buffer sizes
around 64/128 KB or less.  This is for Intel Quad Core2 (Q8400 @ 2.66
GHz) / Linux (openSUSE 11.2, 64 bit), but your mileage may vary (and
will vary!) for other processors / operating systems.

In contrast, for buffers larger than 64/128 KB, the threaded version
starts to perform significantly better, being the sweet point at 1 MB
(again, this is with my setup).  For larger buffer sizes than 1 MB,
the threaded code slows down again, but it is probably due to a cache
size issue and besides, it is still considerably faster than serial
code.

This is why Blosc falls back to use the serial version for such a
'small' buffers.  So, you don't have to worry too much about deciding
whether you should set the number of threads to 1 (serial) or more
(parallel).  Just set it to the number of cores in your processor and
your are done!

Francesc Alted

In these directories you can find some scripts contributed by PyTables
users. If you have any suggestion on them, please contact the original
authors.

nctoh5.py: Converts netcdf files to hdf5. You can find an improved nctoh5
utility included in utils/ directory
Author: Jeff Whitaker <jeffrey.s.whitaker@noaa.gov>

make_hdf.py: Converts general python data structures into hdf5
Author: John Nielsen <John.N.1@bwc.state.oh.us>

===========================================
 PyTables: hierarchical datasets in Python
===========================================

:URL: http://www.pytables.org/


PyTables is a package for managing hierarchical datasets and designed
to efficiently cope with extremely large amounts of data.

It is built on top of the HDF5 library and the NumPy package. It
features an object-oriented interface that, combined with C extensions
for the performance-critical parts of the code (generated using
Cython), makes it a fast, yet extremely easy to use tool for
interactively save and retrieve very large amounts of data. One
important feature of PyTables is that it optimizes memory and disk
resources so that they take much less space (between a factor 3 to 5,
and more if the data is compressible) than other solutions, like for
example, relational or object oriented databases.

Not a RDBMS replacement
-----------------------

PyTables is not designed to work as a relational database replacement,
but rather as a teammate. If you want to work with large datasets of
multidimensional data (for example, for multidimensional analysis), or
just provide a categorized structure for some portions of your
cluttered RDBS, then give PyTables a try. It works well for storing
data from data acquisition systems (DAS), simulation software, network
data monitoring systems (for example, traffic measurements of IP
packets on routers), or as a centralized repository for system logs,
to name only a few possible uses.

Tables
------

A table is defined as a collection of records whose values are stored
in fixed-length fields. All records have the same structure and all
values in each field have the same data type. The terms "fixed-length"
and strict "data types" seems to be quite a strange requirement for an
interpreted language like Python, but they serve a useful function if
the goal is to save very large quantities of data (such as is
generated by many scientific applications, for example) in an
efficient manner that reduces demand on CPU time and I/O.

Arrays
------

There are other useful objects like arrays, enlargeable arrays or
variable length arrays that can cope with different missions on your
project. Also, quite a bit of effort has been invested to make
browsing the hierarchical data structure a pleasant
experience. PyTables implements a few easy-to-use methods for
browsing. See the documentation (located in the ``doc/`` directory)
for more details.

Easy to use
-----------

One of the principal objectives of PyTables is to be user-friendly.
To that end, special Python features like generators, slots and
metaclasses in new-brand classes have been used. In addition,
iterators has been implemented were context was appropriate so as to
enable the interactive work to be as productive as possible. For these
reasons, you will need to use Python 2.6 or higher to take advantage of
PyTables.

Platforms
---------

We are using Linux on top of Intel32 and Intel64 boxes as the main
development platforms, but PyTables should be easy to compile/install
on other UNIX or Windows machines.  Nonetheless, caveat emptor: more
testing is needed to achieve complete portability, we'd appreciate
input on how it compiles and installs on your platform.

Compiling
---------

To compile PyTables you will need, at least, a recent version of HDF5
(C flavor) library, the Zlib compression library and the NumPy and
Numexpr packages. Besides, if you want to take advantage of the LZO
and bzip2 compression libraries support you will also need recent
versions of them. LZO and bzip2 compression libraries are, however,
optional.

We've tested this PyTables version with HDF5 1.8.11/1.8.12, NumPy 1.7.1/1.8.0
and Numexpr 2.2.2, and you *need* to use these versions, or higher, to
make use of PyTables.

Installation
------------

The Python Distutils are used to build and install PyTables, so it is
fairly simple to get things ready to go. Following are very simple
instructions on how to proceed. However, more detailed instructions,
including a section on binary installation for Windows users, is
available in Chapter 2 of the User's Manual (``doc/usersguide.pdf`` or
http://www.pytables.org/moin/HowToUse).

1. First, make sure that you have HDF5, NumPy and Numexpr installed
   (you will need at least HDF5 1.8.4, HDF5 >= 1.8.7 is strongly recommended,
   NumPy 1.4.1 and Numexpr 2.0).
   If don't, get them from http://www.hdfgroup.org/HDF5/,
   http://www.numpy.org and http://code.google.com/p/numexpr.
   Compile/install them.

   Optionally, consider to install the excellent LZO compression
   library from http://www.oberhumer.com/opensource/.  You can also
   install the high-performance bzip2 compression library, available
   at http://www.bzip.org/.

2. From the main PyTables distribution directory run this command,
   (plus any extra flags needed as discussed above)::

    $ python setup.py build_ext --inplace

3. To run the test suite, set the PYTHONPATH environment variable to
   include the ``.`` directory, enter the Python interpreter and issue
   the commands::

    >>> import tables
    >>> tables.test()

   If there is some test that does not pass, please send the
   complete output for tests back to us.

4. To install the entire PyTables Python package, run this command as
   the root user (remember to add any extra flags needed)::

    $ python setup.py install


That's it!  Good luck, and let us know of any bugs, suggestions,
gripes, kudos, etc. you may have.

----

  **Enjoy data!**

  -- The PyTables Team

.. Local Variables:
.. mode: text
.. coding: utf-8
.. fill-column: 70
.. End:

