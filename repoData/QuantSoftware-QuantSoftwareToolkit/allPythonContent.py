__FILENAME__ = converter
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''

#
# fundsToPNG.py
#
# Short script which produces a graph of funds 
# over time from a pickle file.
#
# Drew Bratcher
#

from pylab import *
from QSTK.qstkutil import DataAccess as da
from QSTK.qstkutil import tsutil as tsu
# from quicksim import quickSim
from copy import deepcopy
import math
from pandas import *
import matplotlib.pyplot as plt
import cPickle

def fundsToPNG(funds,output_file):
	plt.clf()
	if(type(funds)==type(list())):
		for i in range(0,len(funds)):
			plt.plot(funds[i].index,funds[i].values)
	else:
		plt.plot(funds.index,funds.values)
	plt.ylabel('Fund Value')
	plt.xlabel('Date')
	plt.gcf().autofmt_xdate(rotation=45)
	plt.draw()
	savefig(output_file, format='png')

def fundsAnalysisToPNG(funds,output_file):
	plt.clf()
	if(type(funds)!=type(list())):
		print 'fundsmatrix only contains one timeseries, not able to analyze.'
	#convert to daily returns
	count=list()
	dates=list()
	sum=list()
	for i in range(0,len(funds)):
		ret=tsu.daily(funds[i].values)
		for j in range(0, len(ret)):
			if (funds[i].index[j] in dates):
				sum[dates.index(funds[i].index[j])]+=ret[j]
				count[dates.index(funds[i].index[j])]+=1
			else:
				dates.append(funds[i].index[j])	
				count.append(1)
				sum.append(ret[j])
	#compute average
	tot_ret=deepcopy(sum)
	for i in range(0,len(sum)):
		tot_ret[i]=sum[i]/count[i]
	
	#compute std
	std=zeros(len(sum))
	for i in range(0,len(funds)):
		temp=tsu.daily(funds[i].values)
		for j in range(0,len(temp)):
			std[dates.index(funds[i].index[j])]=0
			std[dates.index(funds[i].index[j])]+=math.pow(temp[j]-tot_ret[dates.index(funds[i].index[j])],2)
	
	for i in range(1, len(std)):
#		std[i]=math.sqrt(std[i]/count[i])+std[i-1]
		std[i]=math.sqrt(std[i]/count[i])
	
	#compute total returns
	lower=deepcopy(tot_ret)
	upper=deepcopy(tot_ret)
	tot_ret[0]=funds[0].values[0]
	lower[0]=funds[0].values[0]
	upper[0]=lower[0]
#	for i in range(1,len(tot_ret)):
#		tot_ret[i]=tot_ret[i-1]+(tot_ret[i])*tot_ret[i-1]
#		lower[i]=tot_ret[i-1]-(std[i])*tot_ret[i-1]
#		upper[i]=tot_ret[i-1]+(std[i])*tot_ret[i-1]
	for i in range(1,len(tot_ret)):
		lower[i]=(tot_ret[i]-std[i]+1)*lower[i-1]
		upper[i]=(tot_ret[i]+std[i]+1)*upper[i-1]
		tot_ret[i]=(tot_ret[i]+1)*tot_ret[i-1]
		
	
	plt.clf()
	plt.plot(dates,tot_ret)
	plt.plot(dates,lower)
	plt.plot(dates,upper)
	plt.legend(('Tot_Ret','Lower','Upper'),loc='upper left')
	plt.ylabel('Fund Total Return')
	plt.ylim(ymin=0,ymax=2*tot_ret[0])
	plt.draw()
	savefig(output_file, format='png')

########NEW FILE########
__FILENAME__ = csvformatter
import csv
# import dateutil.parser as dp
import string


def csv_converter(inputfile, outputfile):
    actualheader = ['Symbol', 'Name', 'Type', 'Date', 'Shares', 'Price', 'Cash value', 'Commission', 'Notes']

    reader = csv.reader(open(inputfile, 'r'), delimiter=',')
    header = reader.next()
    print "Header : ", header
    input_str = []
    for row in reader:
        input_str.append(row)
    input_str = input_str[::-1]

    writer = csv.writer(open(outputfile, 'wb'), delimiter=',')
    writer.writerow(actualheader)

    for row in input_str[:]:
        # print row
        if row[0] != '--':
            date = row[0]
        else:
            date = row[1]

        Commission = '0'
        Notes = ''
        cashvalue = row[12]

        if row[9] == 'BRKB':
            row[9] = 'BRK.B'

        if row[7] == 'Close Short':
            continue

        if row[7] == 'Sale ':
            ordertype = 'Sell'
            symbol = row[9]
            name = row[8]
            shares = str(int(row[10]) * (-1))
            price = row[11]
        elif row[7] == 'Purchase ':
            ordertype = 'Buy'
            symbol = row[9]
            name = row[8]
            shares = row[10]
            price = row[11]
        elif row[7] == 'Dividend' or row[7] == 'Bank Interest' \
                 or row[7] == 'Interest Charge' or row[7] == 'Paymnt In Lieu' \
                 or row[7] == 'Annual Charge' or row[7] == 'Funds Transfer':
            ordertype = 'Deposit Cash'
            symbol = ''
            name = row[7]
            shares = ''
            price = ''
        elif row[7] == 'Stock Dividend' or row[7] == 'DUDB':
            ordertype = 'Buy'
            symbol = row[9]
            name = 'Dividend'
            shares = row[10]
            price = '0'
        elif row[7] == 'Journal Entry':
            if row[10] != '--':
                ordertype = 'Buy'
                symbol = row[9]
                name = row[7] + row[8]
                shares = row[10]
                price = '0'
            else:
                ordertype = 'Deposit Cash'
                symbol = ''
                name = 'Deposit Cash'
                shares = ''
                price = ''
        else:
            name = row[7] + row[8]
            if row[10] == '--':
                shares = ''
                ordertype = 'Deposit Cash'
                price = ''
                symbol = ''
            else:
                shares = row[10]
                ordertype = 'Buy'
                if row[11] == '--':
                    price = '0'
                else:
                    price = row[11]
                symbol = row[9]

        shares = string.replace(shares, ',', '')
        price = string.replace(price, ',', '')
        cashvalue = string.replace(cashvalue, ',', '')

        row_to_enter = [symbol, name, ordertype, date, shares, price, cashvalue, Commission, Notes]
        # print row_to_enter
        writer.writerow(row_to_enter)

if __name__ == "__main__":
    inputfile = "./Settled.csv"
    outputfile = 'trans.csv'
    csv_converter(inputfile, outputfile)
    print "Done"

########NEW FILE########
__FILENAME__ = DataGenerate_SineWave
import datetime as dt
import csv
import copy
import os
import pickle
import math

# 3rd party imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# QSTK imports
from QSTK.qstkutil import qsdateutil as du
# import qstkutil.DataEvolved as de

def write(ls_symbols, d_data, ldt_timestamps):

    ldt_timestamps.reverse()
    ls_keys = ['actual_open', 'actual_high', 'actual_low', 'actual_close', 'volume', 'close']

    length = len(ldt_timestamps)

    for symbol in ls_symbols:

        sym_file = open('./' + symbol + '.csv', 'w')
        sym_file.write("Date,Open,High,Low,Close,Volume,Adj Close \n")

        for i,date in enumerate(ldt_timestamps):
            date_to_csv = '{:%Y-%m-%d}'.format(date)
            string_to_csv = date_to_csv

            for key in ls_keys:
                string_to_csv = string_to_csv + ',' + str(d_data[symbol][length-i-1])

            string_to_csv = string_to_csv + '\n'
            sym_file.write(string_to_csv)


def main():
    print "Creating Stock data from Sine Waves"
    dt_start = dt.datetime(2000, 1, 1)
    dt_end = dt.datetime(2012, 10, 31)
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt.timedelta(hours=16))

    x = np.array(range(len(ldt_timestamps)))

    ls_symbols = ['SINE_FAST', 'SINE_SLOW', 'SINE_FAST_NOISE', 'SINE_SLOW_NOISE']
    sine_fast = 10*np.sin(x/10.) + 100
    sine_slow = 10*np.sin(x/30.) + 100

    sine_fast_noise = 10*(np.sin(x/10.) + np.random.randn(x.size)) + 100
    sine_slow_noise = 10*(np.sin(x/30.) + np.random.randn(x.size)) + 100

    d_data = dict(zip(ls_symbols, [sine_fast, sine_slow, sine_fast_noise, sine_slow_noise]))

    write(ls_symbols, d_data, ldt_timestamps)

    plt.clf()
    plt.plot(ldt_timestamps, sine_fast)
    plt.plot(ldt_timestamps, sine_slow)
    plt.plot(ldt_timestamps, sine_fast_noise)
    plt.plot(ldt_timestamps, sine_slow_noise)
    plt.ylim(50,150)
    plt.xticks(size='xx-small')
    plt.legend(ls_symbols, loc='best')
    plt.savefig('test.png',format='png')


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = Data_CSV
#File to read the data from mysql and push into CSV.

# Python imports
import datetime as dt
import csv
import copy
import os
import pickle

# 3rd party imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# QSTK imports
from QSTK.qstkutil import qsdateutil as du
import QSTK.qstkutil.DataEvolved as de

def get_data(ls_symbols, ls_keys):
    '''
    @summary: Gets a data chunk for backtesting
    @param dt_start: Start time
    @param dt_end: End time
    @param ls_symbols: symbols to use
    @note: More data will be pulled from before and after the limits to ensure
           valid data on the start/enddates which requires lookback/forward
    @return: data dictionry
    '''
    print "Getting Data from MySQL"
    # Modify dates to ensure enough data for all features
    dt_start = dt.datetime(2005,1,1)
    dt_end = dt.datetime(2012, 8, 31)
    ldt_timestamps = du.getNYSEdays( dt_start, dt_end, dt.timedelta(hours=16) )
    
    c_da = de.DataAccess('mysql')

    ldf_data = c_da.get_data(ldt_timestamps, ls_symbols, ls_keys)

    d_data = dict(zip(ls_keys, ldf_data))

    return d_data

def read_symbols(s_symbols_file):

    ls_symbols=[]
    file = open(s_symbols_file, 'r')
    for f in file.readlines():
        j = f[:-1]
        ls_symbols.append(j)
    file.close()
    
    return ls_symbols   


def csv_sym(sym, d_data, ls_keys, s_directory):

    bool_first_iter = True

    for key in ls_keys:
        if bool_first_iter == True:
            df_sym = d_data[key].reindex(columns = [sym])
            df_sym = df_sym.rename(columns = {sym : key})
            bool_first_iter = False
        else: 
            df_temp = d_data[key].reindex(columns = [sym])
            df_temp = df_temp.rename(columns = {sym : key})
            df_sym = df_sym.join(df_temp, how= 'outer')

    symfilename = sym.split('-')[0]
    sym_file = open(s_directory + symfilename + '.csv', 'w')
    sym_file.write("Date,Open,High,Low,Close,Volume,Adj Close \n")

    ldt_timestamps = list(df_sym.index)
    ldt_timestamps.reverse()
    
    for date in ldt_timestamps:
        date_to_csv = '{:%Y-%m-%d}'.format(date)
        string_to_csv = date_to_csv
        for key in ls_keys:
            string_to_csv = string_to_csv + ',' + str(df_sym[key][date])
        string_to_csv = string_to_csv + '\n'
        sym_file.write(string_to_csv)


def main(s_directory, s_symbols_file):

    #ls_symbols = read_symbols(s_symbols_file)
    ls_symbols = ['ACS-201002','BDK-201003','BJS-201004','BSC-201108','CCT-201111','EQ-200907','JAVA-201002','NCC-200901','NOVL-201104','PBG-201003','PTV-201011','ROH-200904','SGP-200911','SII-201008','WB-200901','WYE-200910','XTO-201006']
    ls_keys = ['actual_open', 'actual_high', 'actual_low', 'actual_close', 'volume', 'close']
    d_data = get_data(ls_symbols, ls_keys)
    # print d_data
    print "Creating CSV files now"

    for sym in ls_symbols:
        print sym
        csv_sym(sym,d_data, ls_keys, s_directory)

    print "Created all CSV files"


if __name__ == '__main__' :
    s_directory = 'MLTData/'
    s_directory = os.environ['QSDATA'] + '/Yahoo/' 
    s_symbols_file1 = 'MLTData/sp5002012.txt'
    s_symbols_file2 = 'MLTData/index.txt'
    s_symbols_file3 = 'MLTData/sp5002008.txt'
    main(s_directory, s_symbols_file3)
########NEW FILE########
__FILENAME__ = gen_nyse_dates
"""
Created on March 22, 2011

@author: Tucker Balch
@contact: tucker@cc.gatech.edu

This script generates a list of days on which we expect the NYSE to be
open.  Information is drawn from two websites:

http://www.timeanddate.com/holidays/
http://www.nyse.com/about/newsevents/1176373643795.html
"""

__version__ = "$Revision$"

import numpy as np
from datetime import *


rootday = datetime(2011,1,3) # first monday in 2011
endday = datetime(2031,1,1) # don't go beyond this date
weekdays = [] # all weekdays
oneday = timedelta(1) # length of one day
curr = 0
candidateday = rootday + curr*oneday

#
# create a list of all weekdays
#
while (candidateday < endday):
    # add 5 weekdays to list
    for j in range(5): # 5 days per week
        weekdays.append(rootday + curr*oneday)
        curr = curr+1
    curr = curr+2 # skip the weekend
    candidateday = rootday + curr*oneday

#
# Create a list of all NYSE observed holidays (from NYSE's website)
# The dates are confirmed through 2013, and estimated thereafter.
#
NYSEholidays = []

# New Year's Day
NYSEholidays.append(datetime(2011,1,1))
NYSEholidays.append(datetime(2012,1,2))
NYSEholidays.append(datetime(2013,1,1))
NYSEholidays.append(datetime(2014,1,1))
NYSEholidays.append(datetime(2015,1,1))
NYSEholidays.append(datetime(2016,1,1))
NYSEholidays.append(datetime(2017,1,2))
NYSEholidays.append(datetime(2018,1,1))
NYSEholidays.append(datetime(2019,1,1))
NYSEholidays.append(datetime(2020,1,1))
NYSEholidays.append(datetime(2021,1,1))
NYSEholidays.append(datetime(2022,1,1))
NYSEholidays.append(datetime(2023,1,2))
NYSEholidays.append(datetime(2024,1,1))
NYSEholidays.append(datetime(2025,1,1))
NYSEholidays.append(datetime(2026,1,1))
NYSEholidays.append(datetime(2027,1,1))
NYSEholidays.append(datetime(2028,1,1))
NYSEholidays.append(datetime(2029,1,1))
NYSEholidays.append(datetime(2030,1,1))
NYSEholidays.append(datetime(2031,1,1))



# MLK Day
NYSEholidays.append(datetime(2011,1,17))
NYSEholidays.append(datetime(2012,1,16))
NYSEholidays.append(datetime(2013,1,21))
NYSEholidays.append(datetime(2014,1,20))
NYSEholidays.append(datetime(2015,1,19))
NYSEholidays.append(datetime(2016,1,18))
NYSEholidays.append(datetime(2017,1,16))
NYSEholidays.append(datetime(2018,1,15))
NYSEholidays.append(datetime(2019,1,21))
NYSEholidays.append(datetime(2020,1,20))
NYSEholidays.append(datetime(2021,1,18))
NYSEholidays.append(datetime(2022,1,17))
NYSEholidays.append(datetime(2023,1,16))
NYSEholidays.append(datetime(2024,1,15))
NYSEholidays.append(datetime(2025,1,20))
NYSEholidays.append(datetime(2026,1,19))
NYSEholidays.append(datetime(2027,1,18))
NYSEholidays.append(datetime(2028,1,17))
NYSEholidays.append(datetime(2029,1,15))
NYSEholidays.append(datetime(2030,1,21))



# Washington's Birthday
NYSEholidays.append(datetime(2011,2,21))
NYSEholidays.append(datetime(2012,2,20))
NYSEholidays.append(datetime(2013,2,18))
NYSEholidays.append(datetime(2014,2,17))
NYSEholidays.append(datetime(2015,2,16))
NYSEholidays.append(datetime(2016,2,15))
NYSEholidays.append(datetime(2017,2,20))
NYSEholidays.append(datetime(2018,2,19))
NYSEholidays.append(datetime(2019,2,18))
NYSEholidays.append(datetime(2020,2,17))
NYSEholidays.append(datetime(2021,2,15))
NYSEholidays.append(datetime(2022,2,21))
NYSEholidays.append(datetime(2023,2,20))
NYSEholidays.append(datetime(2024,2,19))
NYSEholidays.append(datetime(2025,2,17))
NYSEholidays.append(datetime(2026,2,16))
NYSEholidays.append(datetime(2027,2,15))
NYSEholidays.append(datetime(2028,2,21))
NYSEholidays.append(datetime(2029,2,19))
NYSEholidays.append(datetime(2030,2,18))

# Good Friday
NYSEholidays.append(datetime(2011,4,22))
NYSEholidays.append(datetime(2012,4,6))
NYSEholidays.append(datetime(2013,3,29))
NYSEholidays.append(datetime(2014,4,18))
NYSEholidays.append(datetime(2015,4,3))
NYSEholidays.append(datetime(2016,3,25))
NYSEholidays.append(datetime(2017,4,14))
NYSEholidays.append(datetime(2018,3,30))
NYSEholidays.append(datetime(2019,4,19))
NYSEholidays.append(datetime(2020,4,10))
NYSEholidays.append(datetime(2021,4,2))
NYSEholidays.append(datetime(2022,4,15))
NYSEholidays.append(datetime(2023,4,7))
NYSEholidays.append(datetime(2024,3,29))
NYSEholidays.append(datetime(2025,4,18))
NYSEholidays.append(datetime(2026,4,3))
NYSEholidays.append(datetime(2027,3,26))
NYSEholidays.append(datetime(2028,4,14))
NYSEholidays.append(datetime(2029,3,30))
NYSEholidays.append(datetime(2030,4,19))

# Memorial Day
NYSEholidays.append(datetime(2011,5,30))
NYSEholidays.append(datetime(2012,5,28))
NYSEholidays.append(datetime(2013,5,27))
NYSEholidays.append(datetime(2014,5,26))
NYSEholidays.append(datetime(2015,5,25))
NYSEholidays.append(datetime(2016,5,30))
NYSEholidays.append(datetime(2017,5,29))
NYSEholidays.append(datetime(2018,5,28))
NYSEholidays.append(datetime(2019,5,27))
NYSEholidays.append(datetime(2020,5,25))
NYSEholidays.append(datetime(2021,5,31))
NYSEholidays.append(datetime(2022,5,30))
NYSEholidays.append(datetime(2023,5,29))
NYSEholidays.append(datetime(2024,5,27))
NYSEholidays.append(datetime(2025,5,26))
NYSEholidays.append(datetime(2026,5,25))
NYSEholidays.append(datetime(2027,5,31))
NYSEholidays.append(datetime(2028,5,29))
NYSEholidays.append(datetime(2029,5,28))
NYSEholidays.append(datetime(2030,5,27))


# Independance Day
NYSEholidays.append(datetime(2011,7,4))
NYSEholidays.append(datetime(2012,7,4))
NYSEholidays.append(datetime(2013,7,4))
NYSEholidays.append(datetime(2014,7,4))
NYSEholidays.append(datetime(2015,7,3))
NYSEholidays.append(datetime(2016,7,4))
NYSEholidays.append(datetime(2017,7,4))
NYSEholidays.append(datetime(2018,7,4))
NYSEholidays.append(datetime(2019,7,4))
NYSEholidays.append(datetime(2020,7,3))
NYSEholidays.append(datetime(2021,7,5))
NYSEholidays.append(datetime(2022,7,4))
NYSEholidays.append(datetime(2023,7,4))
NYSEholidays.append(datetime(2024,7,4))
NYSEholidays.append(datetime(2025,7,4))
NYSEholidays.append(datetime(2026,7,3))
NYSEholidays.append(datetime(2027,7,5))
NYSEholidays.append(datetime(2028,7,4))
NYSEholidays.append(datetime(2029,7,4))
NYSEholidays.append(datetime(2030,7,4))



# Labor Day
NYSEholidays.append(datetime(2011,9,5))
NYSEholidays.append(datetime(2012,9,3))
NYSEholidays.append(datetime(2013,9,2))
NYSEholidays.append(datetime(2014,9,1))
NYSEholidays.append(datetime(2015,9,7))
NYSEholidays.append(datetime(2016,9,5))
NYSEholidays.append(datetime(2017,9,4))
NYSEholidays.append(datetime(2018,9,3))
NYSEholidays.append(datetime(2019,9,2))
NYSEholidays.append(datetime(2020,9,7))
NYSEholidays.append(datetime(2021,9,6))
NYSEholidays.append(datetime(2022,9,5))
NYSEholidays.append(datetime(2023,9,4))
NYSEholidays.append(datetime(2024,9,2))
NYSEholidays.append(datetime(2025,9,1))
NYSEholidays.append(datetime(2026,9,7))
NYSEholidays.append(datetime(2027,9,6))
NYSEholidays.append(datetime(2028,9,4))
NYSEholidays.append(datetime(2029,9,3))
NYSEholidays.append(datetime(2030,9,2))



# Thanksgiving Day
NYSEholidays.append(datetime(2011,11,24))
NYSEholidays.append(datetime(2012,11,22))
NYSEholidays.append(datetime(2013,11,28))
NYSEholidays.append(datetime(2014,11,27))
NYSEholidays.append(datetime(2015,11,26))
NYSEholidays.append(datetime(2016,11,24))
NYSEholidays.append(datetime(2017,11,23))
NYSEholidays.append(datetime(2018,11,22))
NYSEholidays.append(datetime(2019,11,28))
NYSEholidays.append(datetime(2020,11,26))
NYSEholidays.append(datetime(2021,11,25))
NYSEholidays.append(datetime(2022,11,24))
NYSEholidays.append(datetime(2023,11,23))
NYSEholidays.append(datetime(2024,11,28))
NYSEholidays.append(datetime(2025,11,27))
NYSEholidays.append(datetime(2026,11,26))
NYSEholidays.append(datetime(2027,11,25))
NYSEholidays.append(datetime(2028,11,23))
NYSEholidays.append(datetime(2029,11,22))
NYSEholidays.append(datetime(2030,11,28))


# Christmas
NYSEholidays.append(datetime(2011,12,26))
NYSEholidays.append(datetime(2012,12,25))
NYSEholidays.append(datetime(2013,12,25))
NYSEholidays.append(datetime(2014,12,25))
NYSEholidays.append(datetime(2015,12,25))
NYSEholidays.append(datetime(2016,12,26))
NYSEholidays.append(datetime(2017,12,25))
NYSEholidays.append(datetime(2018,12,25))
NYSEholidays.append(datetime(2019,12,25))
NYSEholidays.append(datetime(2020,12,25))
NYSEholidays.append(datetime(2021,12,24))
NYSEholidays.append(datetime(2022,12,26))
NYSEholidays.append(datetime(2023,12,25))
NYSEholidays.append(datetime(2024,12,25))
NYSEholidays.append(datetime(2025,12,25))
NYSEholidays.append(datetime(2026,12,25))
NYSEholidays.append(datetime(2027,12,24))
NYSEholidays.append(datetime(2028,12,25))
NYSEholidays.append(datetime(2029,12,25))
NYSEholidays.append(datetime(2030,12,25))

# Holidays due to unexpected event
# Sandy Storm
NYSEholidays.append(datetime(2012,10,29))
NYSEholidays.append(datetime(2012,10,30))


#
# Now we use Python's set magic to remove the holidays
#
setweekdays = set(weekdays) # convert weekdays to a set
setNYSEholidays = set(NYSEholidays)
newdays = set.difference(setweekdays,NYSEholidays) # remove holidays
newdays = sorted(list(newdays)) # convert back to sorted list

log = []
# print it out
for i in newdays:
    print i.strftime("%m/%d/%Y")
    log.append(i.strftime("%m/%d/%Y"))

np.savetxt('NYSE_dates.csv', log, fmt='%s', delimiter=',') 

########NEW FILE########
__FILENAME__ = investors_report
#
# report.py
#
# Generates a html file containing a report based 
# off a timeseries of funds from a pickle file.
#
# Drew Bratcher
#

from pylab import *
import numpy
from QSTK.qstkutil import DataAccess as da
from QSTK.qstkutil import qsdateutil as du
from QSTK.qstkutil import tsutil as tsu
from QSTK.quicksim import quickSim as qs
import converter
import datetime as dt
from pandas import *
import matplotlib.pyplot as plt
import cPickle

def readableDate(date):
	return str(date.month)+"/"+str(date.day)+"/"+str(date.year)

def getYearReturn(funds, year):
	days=[]
	for date in funds.index:
		if(date.year==year):
			days.append(date)
	return funds[days[-1]]/funds[days[0]]-1

def getYearMaxDrop(funds, year):
	days=[]
	for date in funds.index:
		if(date.year==year):
			days.append(date)
	maxdrop=0
	prevday=days[0]
	for day in days[1:-1]:
		if((funds[day]/funds[prevday]-1)<maxdrop):
			maxdrop=funds[day]/funds[prevday]-1
		prevday=day
	return maxdrop

def getYearRatioUsingMonth(funds,year):
	days=[]
	for date in funds.index:
		if(date.year==year):
			days.append(date)
	funds=funds.reindex(index=days)
	m=tsu.monthly(funds)
	avg=float(sum(m))/len(m)
	std=0
	for a in m:
		std=std+float((float(a-avg))**2)
	std=sqrt(float(std)/(len(m)-1))
	return (avg/std)

def getWinningDays(funds1,funds2,year):
	days=[]
	i=0;
	win=0
	tot=0
	f1ret=tsu.daily(funds1)
	f2ret=tsu.daily(funds2)
	relf1=[]
	relf2=[]
	for date in funds1.index:
		if(date.year==year):
			for date2 in funds2.index:
				if(date==date2):
					relf1.append(f1ret[i])
					relf2.append(f2ret[i])
		i+=1
	
	for i in range(0,len(relf1)):
		if(f1ret[i]>f2ret[i]):
			win+=1
		tot+=1
	return float(win)/tot

def runOther(funds,symbols):
	tsstart =dt.datetime(funds.index[0].year,funds.index[0].month,funds.index[0].day)
	tsend =dt.datetime(funds.index[-1].year,funds.index[-1].month,funds.index[-1].day)
	timeofday=dt.timedelta(hours=16)
	timestamps=du.getNYSEdays(tsstart,tsend,timeofday)
	dataobj=da.DataAccess('Norgate')
	historic=dataobj.get_data(timestamps,symbols,"close")
	alloc_val=float(0.1/(float(len(symbols))+1))
	alloc_vals=alloc_val*ones(len(symbols))
	alloc=DataMatrix(index=[historic.index[0]],data=[alloc_vals], columns=symbols)
	alloc=alloc.append(DataMatrix(index=[historic.index[-1]], data=[alloc_vals], columns=symbols))
	alloc['_CASH']=alloc_val
	return qs.quickSim(alloc,historic,1000)

def reportFunctionality(funds, symbols,filename=sys.stdout):
	if(len(symbols)!=0):
		funds2=runOther(funds,symbols)
		arg2=1
	else:
		arg2=0

	if(filename==sys.stdout):
		html_file=sys.stdout
	else:
		html_file = open(filename,"w")
	
	#top
	html_file.write("<HTML>\n")
	html_file.write("<HEAD>\n")	
	html_file.write("<TITLE>QSTK Generated Report from "+readableDate(funds.index[0])+" to "+readableDate(funds.index[-1])+"</TITLE>\n")
	html_file.write("</HEAD>\n\n")
	html_file.write("<BODY><CENTER>\n\n")
	
	years=du.getYears(funds)

	html_file.write("<H2>Performance Summary for "+sys.argv[1]+"</H2>\n")
	html_file.write("For the dates "+readableDate(funds.index[0])+" to "+readableDate(funds.index[-1])+"\n")


	html_file.write("<H3>Yearly Performance Metrics</H3>\n")


	html_file.write("<TABLE CELLPADDING=10>\n")
	html_file.write("<TR><TH></TH>\n")
	for year in years:
		html_file.write("<TH>"+str(year)+"</TH>\n")
	html_file.write("</TR>\n")

	#yearly return
	html_file.write("<TR>\n")
	html_file.write("<TH>Annualized Return:</TH>\n")
	for year in years:
		retur=getYearReturn(funds,year)
		html_file.write("<TD>\n")
		print >>html_file, "%.2f\n" % (retur*100) 
		html_file.write("%</TD>\n")
	html_file.write("</TR>\n")

	#yearly winning days
	html_file.write("<TR>\n")
	html_file.write("<TH>Winning Days:</TH>\n")
	for year in years:
		# change to compare to inputs - ratio=tsu.getYearRatio(funds,year)
		if(arg2!=0):
			win=getWinningDays(funds,funds2,year)
			html_file.write("<TD>\n")
			print >>html_file, "%.2f\n" % (win*100)
			html_file.write("%</TD>\n")
		else:
			html_file.write("<TD>No comparison.</TD>\n")
	html_file.write("</TR>\n")

	#max draw down
	html_file.write("<TR>\n")
	html_file.write("<TH>Max Draw Down:</TH>\n")
	for year in years:
		drop=getYearMaxDrop(funds,year)
		html_file.write("<TD>\n")
		print >>html_file, "%.2f" % (drop*100)
		html_file.write("%</TD>\n")
	html_file.write("</TR>\n")

	#yearly sharpe ratio using daily rets
	html_file.write("<TR>\n")
	html_file.write("<TH>Daily Sharpe Ratio:</TH>\n")
	for year in years:
		ratio=tsu.getYearRatio(funds,year)
		html_file.write("<TD>\n")
		print >>html_file, "%.2f\n" % ratio
		html_file.write("</TD>\n")
	html_file.write("</TR>\n")


	#yearly sharpe ratio using monthly rets
	html_file.write("<TR>\n")
	html_file.write("<TH>Monthly Sharpe Ratio:</TH>\n")
	for year in years:
		ratio=getYearRatioUsingMonth(funds,year)
		html_file.write("<TD>\n")
		print >>html_file, "%.2f\n" % ratio
		html_file.write("</TD>\n")
	html_file.write("</TR>\n")
	
	html_file.write("</TABLE>\n")
	html_file.write("<BR/>\n\n")

	vals=funds.values;
	vals2=np.append(vals,funds2.values,2)


	df=DataMatrix(index=funds.index,data=funds.values, columns=['fund'])
	df2=DataMatrix(index=funds2.index,data=funds2.values,columns=['other'])
	df['other']=df2['other']

	corrcoef=numpy.corrcoef(funds.values[0:-1],funds2.values)
	html_file.write("<H3>Correlation=")
	print >>html_file, "%.2f\n" % corrcoef[0][1]
	html_file.write("<H3>\n")
	html_file.write("<BR/>\n\n")


	#montly returns
	mrets=tsu.monthly(funds)
	html_file.write("<H2>Monthly Returns</H2>\n")
	html_file.write("<TABLE CELLPADDING=10>\n")
	html_file.write("<TR>\n")
	html_file.write("<TH></TH>\n")
	month_names=du.getMonthNames()
	for name in month_names:
		html_file.write("<TH>"+str(name)+"</TH>\n")
	html_file.write("</TR>\n")

	i=0
	for year in years:
		html_file.write("<TR>\n")
		html_file.write("<TH>"+str(year)+"</TH>\n")
		months=du.getMonths(funds,year)
		for month in months:
			html_file.write("<TD>\n")
			print >>html_file, "%.2f\n" % (mrets[i]*100)
			html_file.write("%</TD>\n")
			i+=1
		html_file.write("</TR>\n")
	html_file.write("</TABLE>\n")
	html_file.write("<BR/>\n\n")

	#fund value graph
	fundlist=[];
	fundlist.append(funds)
	fundlist.append(funds2)
	converter.fundsToPNG(fundlist,'funds.png')
	html_file.write("<IMG SRC=\'./funds.png\'/>\n")
	html_file.write("<BR/>\n\n")
	
	#end
	html_file.write("</CENTER></BODY>\n\n")
	html_file.write("</HTML>")



if __name__ == '__main__':
	input=open(sys.argv[1],"r")
	funds=cPickle.load(input)

	if(len(sys.argv)>2):
		input2=sys.argv[2]
		symbols=sys.argv[2].split(',')
		reportFunctionality(funds,symbols,'investors_report.html')
	else:
		reportFunctionality(funds,0,'investors_report.html')

########NEW FILE########
__FILENAME__ = sinewave_data_generator
import datetime
import QSTK.qstkutil.qsdateutil
import StringIO
import math
import random

START = datetime.datetime(2000,2,1)
END = datetime.datetime(2012,9,13)
NUMFILES=400

def genfile(fname,dt_start,dt_end):
	datelist = QSTK.qstkutil.qsdateutil.getNYSEdays(dt_start,dt_end)
	#write_to = StringIO.StringIO()
	write_to = open(fname,"w")
	write_to.write("Date,Open,High,Low,Close,Volume,Adj Close\n")
	mean = 20.0+(random.random()*480.0)
	amp =  20.0*random.random()
	period = 10.0+(random.random()*100.0)
	sin_gen = lambda x: (mean+(amp*math.sin(((math.pi*2)/period)*x)))
	print fname,"parameters"
	print "Mean:",mean
	print "Amplitude:",amp
	print "Period:", period
	dllen = len(datelist)
	for t in xrange(dllen):
		date = datelist[(dllen-1)-t]
		val = sin_gen(t)
		line = (date.date().isoformat(),)+((val,)*5)
		write_to.write("%s,%f,%f,%f,%f,1,%f\n"%line)
	#print write_to.getvalue()
	write_to.close()

for i in xrange(NUMFILES):
	genfile("ML4T-%03d.csv"%i,START,END)
#genfile("foo.txt",datetime.datetime(2011,9,13),datetime.datetime(2012,9,13))

########NEW FILE########
__FILENAME__ = numpy-tutorial
########################################
# A brief introduction to numpy arrays #
########################################
#
# Prereqs: Basic python. "import", built-in data types (numbers, lists, 
#          strings), range
#
# This short tutorial is mostly about introducing numpy arrays, how they're
# different from basic python lists/tuples, and the various ways you can
# manipulate them.  It's intended to be both a runnable python script, and
# a step by step tutorial. 
#
# This tutorial does NOT cover
# 	1) Installing numpy/dependencies. For that see 
#
#			http://docs.scipy.org/doc/numpy/user/install.html
#
#	2) Basic python. This includes getting, installing, running the python
#		interpreter, the basic python data types (strings, numbers, sequences),
#		if statements, or for loops. If you're new to python an excellent place
#		to start is here:
#
#			http://docs.python.org/2/tutorial/
#
#	3) Any numpy libraries in depth. It may include references to utility
#		functions where necessary, but this is strictly a tutorial for 
#		beginners. More advanced documentation is available here:
#
#			(Users guide)
#			http://docs.scipy.org/doc/numpy/user/index.html
#			(Reference documentation)
#			http://docs.scipy.org/doc/numpy/reference/
#
#
#
#
## Lets get started!
print "Importing numpy"
import numpy as np

## This loads the numpy library and lets us refer to it by the shorthand "np",
## which is the convention used in the numpy documentation and in many
## online tutorials/examples 

print "Creating arrays"
## Now lets make an array to play around with. You can make numpy arrays in
## a number of ways,
## Filled with zeros:
zeroArray = np.zeros( (2,3) ) # [[ 0.  0.  0.]
print zeroArray               #  [ 0.  0.  0.]]

## Or ones:
oneArray = np.ones( (2,3) )   # [[ 1.  1.  1.]
print oneArray                #  [ 1.  1.  1.]]

## Or filled with junk:
emptyArray = np.empty( (2,3) ) 
print emptyArray

## Note, emptyArray might look random, but it's just uninitialized which means
## you shouldn't count on it having any particular data in it, even random
## data! If you do want random data you can use random():
randomArray = np.random.random( (2,3) )
print randomArray

## If you're following along and trying these commands out, you should have
## noticed that making randomArray took a lot longer than emptyArray. That's
## because np.random.random(...) is actually using a random number generator
## to fill in each of the spots in the array with a randomly sampled number
## from 0 to 1.

## You can also create an array by hand:
foo = [ [1,2,3],
        [4,5,6]]

myArray = np.array(foo) # [[1 2 3] 
print myArray           #  [4 5 6]]


print "Reshaping arrays"
## Of course, if you're typing out a range for a larger matrix, it's easier to
## use arange(...):
rangeArray = np.arange(6,12).reshape( (2,3) ) # [[ 6  7  8]
print rangeArray                              #  [ 9 10 11]]

## there's two things going on here. First, the arange(...) function returns a
## 1D array similar to what you'd get from using the built-in python function
## range(...) with the same arguments, except it returns a numpy array
## instead of a list.
print np.arange(6,12) # [ 6  7  8  9 10 11 12]

## the reshape method takes the data in an existing array, and stuffs it into
## an array with the given shape and returns it.  
print rangeArray.reshape( (3,2) ) # [[ 6  7]
                                  #  [ 8  9]
                                  #  [10 11]]

#The original array doesn't change though.
print rangeArray # [[ 6  7  8]
                 #  [ 9 10 11]

## When you use reshape(...) the total number of things in the array must stay
## the same. So reshaping an array with 2 rows and 3 columns into one with 
## 3 rows and 2 columns is fine, but 3x3 or 1x5 won't work
#print rangeArray.reshape( (3,3) ) #ERROR
squareArray = np.arange(1,10).reshape( (3,3) ) #this is fine, 9 elements


print "Accessing array elements"
## Accessing an array is also pretty straight forward. You access a specific
## spot in the table by referring to its row and column inside square braces
## after the array:
print rangeArray[0,1] #7

## Note that row and column numbers start from 0, not 1! Numpy also lets you 
## refer to ranges inside an array:
print rangeArray[0,0:2] #[6 7]
print squareArray[0:2,0:2] #[[1 2]  # the top left corner of squareArray
                           # [4 5]]

## These ranges work just like slices and python lists. n:m:t specifies a range
## that starts at n, and stops before m, in steps of size t. If any of these 
## are left off, they're assumed to be the start, the end+1, and 1 respectively
print squareArray[:,0:3:2] #[[1 3]   #skip the middle column
                           # [4 6]
                           # [7 9]]

## Also like python lists, you can assign values to specific positions, or
## ranges of values to slices
squareArray[0,:] = np.array(range(1,4)) #set the first row to 1,2,3
squareArray[1,1] = 0                    # set the middle spot to zero
squareArray[2,:] = 1                    # set the last row to ones
print squareArray                       # [[1 2 3]
                                        #  [4 0 6]
                                        #  [1 1 1]]

## Something new to numpy arrays is indexing using an array of indices:
fibIndices = np.array( [1, 1, 2, 3] )
randomRow = np.random.random( (10,1) ) # an array of 10 random numbers
print randomRow
print randomRow[fibIndices] # the first, first, second and third element of
                             # randomRow 

## You can also use an array of true/false values to index:
boolIndices = np.array( [[ True, False,  True],
                          [False,  True, False],
                          [ True, False,  True]] )
print squareArray[boolIndices] # a 1D array with the selected values
                                # [1 3 0 1 1]

## It gets a little more complicated with 2D (and higher) arrays.  You need
## two index arrays for a 2D array:
rows = np.array( [[0,0],[2,2]] ) #get the corners of our square array
cols = np.array( [[0,2],[0,2]] )
print squareArray[rows,cols]     #[[1 3]
                                 # [1 1]]
boolRows = np.array( [False, True, False] ) # just the middle row
boolCols = np.array( [True, False, True] )  # Not the middle column
print squareArray[boolRows,boolCols]        # [4 6]

print "Operations on arrays"
## One useful trick is to create a boolean matrix based on some test and use
## that as an index in order to get the elements of a matrix that pass the
## test:
sqAverage = np.average(squareArray) # average(...) returns the average of all
                                    # the elements in the given array
betterThanAverage = squareArray > sqAverage
print betterThanAverage             #[[False False  True]
                                    # [ True False  True]
                                    # [False False False]]
print squareArray[betterThanAverage] #[3 4 6]

## Indexing like this can also be used to assign values to elements of the
## array. This is particularly useful if you want to filter an array, say by 
## making sure that all of its values are above/below a certain threshold:
sqStdDev = np.std(squareArray) # std(...) returns the standard deviation of
                               # all the elements in the given array
clampedSqArray = np.array(squareArray.copy(), dtype=float) 
                                    # make a copy of squareArray that will
                                    # be "clamped". It will only contain
                                    # values within one standard deviation
                                    # of the mean. Values that are too low
                                    # or to high will be set to the min
                                    # and max respectively. We set 
                                    # dtype=float because sqAverage
                                    # and sqStdDev are floating point
                                    # numbers, and we don't want to 
                                    # truncate them down to integers.
clampedSqArray[ (squareArray-sqAverage) > sqStdDev ] = sqAverage+sqStdDev
clampedSqArray[ (squareArray-sqAverage) < -sqStdDev ] = sqAverage-sqStdDev
print clampedSqArray # [[ 1.          2.          3.        ]
                     #  [ 3.90272394  0.31949828  3.90272394]
                     #  [ 1.          1.          1.        ]]


## Multiplying and dividing arrays by numbers does what you'd expect. It
## multiples/divides element-wise
print squareArray * 2 # [[ 2  4  6]
                      #  [ 8  0 12]
                      #  [ 2  2  2]]

## Addition works similarly:
print squareArray + np.ones( (3,3) ) #[[2 3 4]
                                     # [5 1 7]
                                     # [2 2 2]]

## Multiplying two arrays together (of the same size) is also element wise
print squareArray * np.arange(1,10).reshape( (3,3) ) #[[ 1  4  9]
                                                     # [16  0 36]
                                                     # [ 7  8  9]]

## Unless you use the dot(...) function, which does matrix multiplication
## from linear algebra:
matA = np.array( [[1,2],[3,4]] )
matB = np.array( [[5,6],[7,8]] )
print np.dot(matA,matB) #[[19 22]
                        # [43 50]]

## And thats it! There's a lot more to the numpy library, and there are a few
## things I skipped over here, such as what happens when array dimensions
## don't line up when you're indexing or multiplying them together, so if 
## you're interested, I strongly suggest you head over to the scipy wiki's
## numpy tutorial for a more in depth look at using numpy arrays:
##
##			http://www.scipy.org/Tentative_NumPy_Tutorial

########NEW FILE########
__FILENAME__ = pandas-tutorial
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on October, 4, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Example tutorial code.
'''

import pandas as pd
import datetime as dt
import numpy as np

## Tutorial on using Pandas in QSTK
ldt_timestamps = []
for i in range(1, 6):
    ldt_timestamps.append(dt.datetime(2011, 1, i, 16))

print "The index we created has the following dates : "
print ldt_timestamps
print

## TimeSeries
ts_single_value = pd.TimeSeries(0.0, index=ldt_timestamps)
print "A timeseries initialized to one single value : "

na_vals = np.arange(len(ldt_timestamps))
print "Dummy initialized array : "
print na_vals
print

ts_array = pd.TimeSeries(na_vals, index=ldt_timestamps)
print "A timeseries initialized using a numpy array : "
print ts_array
print 

print "Reading the timeseries for a particular date"
print "Date :  ", ldt_timestamps[1]
print "Value : ", ts_array[ldt_timestamps[1]]
print

print "Initializing a list of symbols : "
ls_symbols = ['AAPL', 'GOOG', 'MSFT', 'IBM']
print ls_symbols
print

print "Initializing a dataframe with one value : "
df_single = pd.DataFrame(index=ldt_timestamps, columns=ls_symbols)
df_single = df_single.fillna(0.0)
print df_single
print

print "Initializing a dataframe with a numpy array : "
na_vals_2 = np.random.randn(len(ldt_timestamps), len(ls_symbols))
df_vals = pd.DataFrame(na_vals_2, index=ldt_timestamps, columns=ls_symbols)
print df_vals
print 

print "Access the timeseries of a particular symbol : "
print df_vals[ls_symbols[1]]
print

print "Access the timeseries of a particular date : "
print df_vals.ix[ldt_timestamps[1]]
print

print "Access the value for a specific symbol on a specific date: "
print df_vals[ls_symbols[1]].ix[ldt_timestamps[1]]
print

print "Reindexing the dataframe"
ldt_new_dates = [dt.datetime(2011, 1, 3, 16), 
                 dt.datetime(2011, 1, 5, 16),
                 dt.datetime(2011, 1, 7, 16)]
ls_new_symbols = ['AAPL', 'IBM', 'XOM']
df_new = df_vals.reindex(index=ldt_new_dates, columns=ls_new_symbols)
print df_new
print "Observe that reindex carried over whatever values it could find and set the rest to NAN"
print

print "For pandas rolling statistics please refer : http://pandas.pydata.org/pandas-docs/dev/computation.html#moving-rolling-statistics-moments"


########NEW FILE########
__FILENAME__ = tutorial1
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 24, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Example tutorial code.
'''

# QSTK Imports
import QSTK.qstkutil.qsdateutil as du
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.DataAccess as da

# Third Party Imports
import datetime as dt
import matplotlib.pyplot as plt
import pandas as pd

print "Pandas Version", pd.__version__


def main():
    ''' Main Function'''

    # List of symbols
    ls_symbols = ["AAPL", "GLD", "GOOG", "$SPX", "XOM"]

    # Start and End date of the charts
    dt_start = dt.datetime(2006, 1, 1)
    dt_end = dt.datetime(2010, 12, 31)

    # We need closing prices so the timestamp should be hours=16.
    dt_timeofday = dt.timedelta(hours=16)

    # Get a list of trading days between the start and the end.
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)

    # Creating an object of the dataaccess class with Yahoo as the source.
    c_dataobj = da.DataAccess('Yahoo')

    # Keys to be read from the data, it is good to read everything in one go.
    ls_keys = ['open', 'high', 'low', 'close', 'volume', 'actual_close']

    # Reading the data, now d_data is a dictionary with the keys above.
    # Timestamps and symbols are the ones that were specified before.
    ldf_data = c_dataobj.get_data(ldt_timestamps, ls_symbols, ls_keys)
    d_data = dict(zip(ls_keys, ldf_data))

    # Filling the data for NAN
    for s_key in ls_keys:
        d_data[s_key] = d_data[s_key].fillna(method='ffill')
        d_data[s_key] = d_data[s_key].fillna(method='bfill')
        d_data[s_key] = d_data[s_key].fillna(1.0)

    # Getting the numpy ndarray of close prices.
    na_price = d_data['close'].values

    # Plotting the prices with x-axis=timestamps
    plt.clf()
    plt.plot(ldt_timestamps, na_price)
    plt.legend(ls_symbols)
    plt.ylabel('Adjusted Close')
    plt.xlabel('Date')
    plt.savefig('adjustedclose.pdf', format='pdf')

    # Normalizing the prices to start at 1 and see relative returns
    na_normalized_price = na_price / na_price[0, :]

    # Plotting the prices with x-axis=timestamps
    plt.clf()
    plt.plot(ldt_timestamps, na_normalized_price)
    plt.legend(ls_symbols)
    plt.ylabel('Normalized Close')
    plt.xlabel('Date')
    plt.savefig('normalized.pdf', format='pdf')

    # Copy the normalized prices to a new ndarry to find returns.
    na_rets = na_normalized_price.copy()

    # Calculate the daily returns of the prices. (Inplace calculation)
    # returnize0 works on ndarray and not dataframes.
    tsu.returnize0(na_rets)

    # Plotting the plot of daily returns
    plt.clf()
    plt.plot(ldt_timestamps[0:50], na_rets[0:50, 3])  # $SPX 50 days
    plt.plot(ldt_timestamps[0:50], na_rets[0:50, 4])  # XOM 50 days
    plt.axhline(y=0, color='r')
    plt.legend(['$SPX', 'XOM'])
    plt.ylabel('Daily Returns')
    plt.xlabel('Date')
    plt.savefig('rets.pdf', format='pdf')

    # Plotting the scatter plot of daily returns between XOM VS $SPX
    plt.clf()
    plt.scatter(na_rets[:, 3], na_rets[:, 4], c='blue')
    plt.ylabel('XOM')
    plt.xlabel('$SPX')
    plt.savefig('scatterSPXvXOM.pdf', format='pdf')

    # Plotting the scatter plot of daily returns between $SPX VS GLD
    plt.clf()
    plt.scatter(na_rets[:, 3], na_rets[:, 1], c='blue')  # $SPX v GLD
    plt.ylabel('GLD')
    plt.xlabel('$SPX')
    plt.savefig('scatterSPXvGLD.pdf', format='pdf')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tutorial2
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 24, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Example tutorial code.
'''

# QSTK Imports
import QSTK.qstkutil.qsdateutil as du
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.DataAccess as da

# Third Party Imports
import datetime as dt
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np


def main():
    ''' Main Function'''

    # Reading the csv file.
    na_data = np.loadtxt('example-data.csv', delimiter=',', skiprows=1)
    na_price = na_data[:, 3:]  # Default np.loadtxt datatype is float.
    na_dates = np.int_(na_data[:, 0:3])  # Dates should be int
    ls_symbols = ['$SPX', 'XOM', 'GOOG', 'GLD']

    # Printing the first 5 rows
    print "First 5 rows of Price Data:"
    print na_price[:5, :]
    print
    print "First 5 rows of Dates:"
    print na_dates[:5, :]

    # Creating the timestamps from dates read
    ldt_timestamps = []
    for i in range(0, na_dates.shape[0]):
        ldt_timestamps.append(dt.date(na_dates[i, 0],
                        na_dates[i, 1], na_dates[i, 2]))

    # Plotting the prices with x-axis=timestamps
    plt.clf()
    plt.plot(ldt_timestamps, na_price)
    plt.legend(ls_symbols)
    plt.ylabel('Adjusted Close')
    plt.xlabel('Date')
    plt.savefig('adjustedclose.pdf', format='pdf')

    # Normalizing the prices to start at 1 and see relative returns
    na_normalized_price = na_price / na_price[0, :]

    # Plotting the prices with x-axis=timestamps
    plt.clf()
    plt.plot(ldt_timestamps, na_normalized_price)
    plt.legend(ls_symbols)
    plt.ylabel('Normalized Close')
    plt.xlabel('Date')
    plt.savefig('normalized.pdf', format='pdf')

    # Copy the normalized prices to a new ndarry to find returns.
    na_rets = na_normalized_price.copy()

    # Calculate the daily returns of the prices. (Inplace calculation)
    tsu.returnize0(na_rets)

    # Plotting the plot of daily returns
    plt.clf()
    plt.plot(ldt_timestamps[0:50], na_rets[0:50, 0])  # $SPX 50 days
    plt.plot(ldt_timestamps[0:50], na_rets[0:50, 1])  # XOM 50 days
    plt.axhline(y=0, color='r')
    plt.legend(['$SPX', 'XOM'])
    plt.ylabel('Daily Returns')
    plt.xlabel('Date')
    plt.savefig('rets.pdf', format='pdf')

    # Plotting the scatter plot of daily returns between XOM VS $SPX
    plt.clf()
    plt.scatter(na_rets[:, 0], na_rets[:, 1], c='blue')
    plt.ylabel('XOM')
    plt.xlabel('$SPX')
    plt.savefig('scatterSPXvXOM.pdf', format='pdf')

    # Plotting the scatter plot of daily returns between $SPX VS GLD
    plt.clf()
    plt.scatter(na_rets[:, 0], na_rets[:, 3], c='blue')  # $SPX v GLD
    plt.ylabel('GLD')
    plt.xlabel('$SPX')
    plt.savefig('scatterSPXvGLD.pdf', format='pdf')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tutorial3
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 24, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Example tutorial code.
'''

# QSTK Imports
import QSTK.qstkutil.qsdateutil as du
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.DataAccess as da

# Third Party Imports
import datetime as dt
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np


def main():
    ''' Main Function'''
    # Reading the portfolio
    na_portfolio = np.loadtxt('tutorial3portfolio.csv', dtype='S5,f4',
                        delimiter=',', comments="#", skiprows=1)
    print na_portfolio

    # Sorting the portfolio by symbol name
    na_portfolio = sorted(na_portfolio, key=lambda x: x[0])
    print na_portfolio

    # Create two list for symbol names and allocation
    ls_port_syms = []
    lf_port_alloc = []
    for port in na_portfolio:
        ls_port_syms.append(port[0])
        lf_port_alloc.append(port[1])

    # Creating an object of the dataaccess class with Yahoo as the source.
    c_dataobj = da.DataAccess('Yahoo')
    ls_all_syms = c_dataobj.get_all_symbols()
    # Bad symbols are symbols present in portfolio but not in all syms
    ls_bad_syms = list(set(ls_port_syms) - set(ls_all_syms))

    if len(ls_bad_syms) != 0:
        print "Portfolio contains bad symbols : ", ls_bad_syms

    for s_sym in ls_bad_syms:
        i_index = ls_port_syms.index(s_sym)
        ls_port_syms.pop(i_index)
        lf_port_alloc.pop(i_index)

    # Reading the historical data.
    dt_end = dt.datetime(2011, 1, 1)
    dt_start = dt_end - dt.timedelta(days=1095)  # Three years
    # We need closing prices so the timestamp should be hours=16.
    dt_timeofday = dt.timedelta(hours=16)

    # Get a list of trading days between the start and the end.
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)

    # Keys to be read from the data, it is good to read everything in one go.
    ls_keys = ['open', 'high', 'low', 'close', 'volume', 'actual_close']

    # Reading the data, now d_data is a dictionary with the keys above.
    # Timestamps and symbols are the ones that were specified before.
    ldf_data = c_dataobj.get_data(ldt_timestamps, ls_port_syms, ls_keys)
    d_data = dict(zip(ls_keys, ldf_data))

    # Copying close price into separate dataframe to find rets
    df_rets = d_data['close'].copy()
    # Filling the data.
    df_rets = df_rets.fillna(method='ffill')
    df_rets = df_rets.fillna(method='bfill')
    df_rets = df_rets.fillna(1.0)

    # Numpy matrix of filled data values
    na_rets = df_rets.values
    # returnize0 works on ndarray and not dataframes.
    tsu.returnize0(na_rets)

    # Estimate portfolio returns
    na_portrets = np.sum(na_rets * lf_port_alloc, axis=1)
    na_port_total = np.cumprod(na_portrets + 1)
    na_component_total = np.cumprod(na_rets + 1, axis=0)

    # Plotting the results
    plt.clf()
    fig = plt.figure()
    fig.add_subplot(111)
    plt.plot(ldt_timestamps, na_component_total, alpha=0.4)
    plt.plot(ldt_timestamps, na_port_total)
    ls_names = ls_port_syms
    ls_names.append('Portfolio')
    plt.legend(ls_names)
    plt.ylabel('Cumulative Returns')
    plt.xlabel('Date')
    fig.autofmt_xdate(rotation=45)
    plt.savefig('tutorial3.pdf', format='pdf')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tutorial4
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 24, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Example tutorial code.
'''

# QSTK Imports
import QSTK.qstkutil.qsdateutil as du
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.DataAccess as da

# Third Party Imports
import datetime as dt
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import cPickle


def main():
    ''' Main Function'''

    # Start and End date of the charts
    dt_start = dt.datetime(2004, 1, 1)
    dt_end = dt.datetime(2009, 12, 31)

    # We need closing prices so the timestamp should be hours=16.
    dt_timeofday = dt.timedelta(hours=16)

    # Get a list of trading days between the start and the end.
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)

    # Creating an object of the dataaccess class with Yahoo as the source.
    c_dataobj = da.DataAccess('Yahoo')

    # List of symbols - First 20
    ls_symbols = c_dataobj.get_symbols_from_list('sp5002012')
    ls_symbols = ls_symbols[:20]
    ls_symbols.append('_CASH')

    # Creating the first allocation row
    na_vals = np.random.randint(0, 1000, len(ls_symbols))
    # Normalize the row - Typecasting as everything is int.
    na_vals = na_vals / float(sum(na_vals))
    # Reshape to a 2D matrix to append into dataframe.
    na_vals = na_vals.reshape(1, -1)

    # Creating Allocation DataFrames
    df_alloc = pd.DataFrame(na_vals, index=[ldt_timestamps[0]],
                                    columns=ls_symbols)

    dt_last_date = ldt_timestamps[0]
    # Looping through all dates and creating monthly allocations
    for dt_date in ldt_timestamps[1:]:
        if dt_last_date.month != dt_date.month:
            # Create allocation
            na_vals = np.random.randint(0, 1000, len(ls_symbols))
            na_vals = na_vals / float(sum(na_vals))
            na_vals = na_vals.reshape(1, -1)
            # Append to the dataframe
            df_new_row = pd.DataFrame(na_vals, index=[dt_date],
                                        columns=ls_symbols)
            df_alloc = df_alloc.append(df_new_row)
        dt_last_date = dt_date

    # Create the outpul pickle file for the dataframe.
    output = open('allocation.pkl', 'wb')
    cPickle.dump(df_alloc, output)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tutorial5
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 24, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Contains tutorial for backtester.
'''

# QSTK Imports
import QSTK.qstkutil.qsdateutil as du
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.DataAccess as da
import QSTK.qstktools.report as report
import QSTK.qstksim as qstksim

# Third Party Imports
import datetime as dt
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np


def main():
    '''Main Function'''

    # List of symbols
    ls_symbols = ["AAPL", "GOOG"]

    # Start and End date of the charts
    dt_start = dt.datetime(2008, 1, 1)
    dt_end = dt.datetime(2010, 12, 31)

    # We need closing prices so the timestamp should be hours=16.
    dt_timeofday = dt.timedelta(hours=16)

    # Get a list of trading days between the start and the end.
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)

    # Creating an object of the dataaccess class with Yahoo as the source.
    c_dataobj = da.DataAccess('Yahoo')

    # Reading just the close prices
    df_close = c_dataobj.get_data(ldt_timestamps, ls_symbols, "close")
    df_close = df_close.fillna(method='ffill')
    df_close = df_close.fillna(method='bfill')
    df_close = df_close.fillna(1.0)

    # Creating the allocation dataframe
    # We offset the time for the simulator to have atleast one
    # datavalue before the allocation.
    df_alloc = pd.DataFrame(np.array([[0.5, 0.5]]),
                index=[ldt_timestamps[0] + dt.timedelta(hours=5)],
                columns=ls_symbols)

    dt_last_date = ldt_timestamps[0]
    # Looping through all dates and creating monthly allocations
    for dt_date in ldt_timestamps[1:]:
        if dt_last_date.month != dt_date.month:
            # Create allocation
            na_vals = np.random.randint(0, 1000, len(ls_symbols))
            na_vals = na_vals / float(sum(na_vals))
            na_vals = na_vals.reshape(1, -1)
            # Append to the dataframe
            df_new_row = pd.DataFrame(na_vals, index=[dt_date],
                                        columns=ls_symbols)
            df_alloc = df_alloc.append(df_new_row)
        dt_last_date = dt_date

    # Adding cash to the allocation matrix
    df_alloc['_CASH'] = 0.0

    # Running the simulator on the allocation frame
    (ts_funds, ts_leverage, f_commission, f_slippage, f_borrow_cost) = qstksim.tradesim(df_alloc,
                    df_close, f_start_cash=10000.0, i_leastcount=1, b_followleastcount=True,
                    f_slippage=0.0005, f_minimumcommision=5.0, f_commision_share=0.0035,
                    i_target_leverage=1, f_rate_borrow=3.5, log="transaction.csv")

    print "Simulated Fund Time Series : "
    print ts_funds
    print "Transaction Costs : "
    print "Commissions : ", f_commission
    print "Slippage : ", f_slippage
    print "Borrowing Cost : ", f_borrow_cost

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tutorial8
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 24, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Demonstrates the use of the CVXOPT portfolio optimization call.
'''

# QSTK Imports
import QSTK.qstkutil.qsdateutil as du
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.DataAccess as da

# Third Party Imports
import datetime as dt
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np


def getFrontier(na_data):
    '''Function gets a 100 sample point frontier for given returns'''

    # Special Case with fTarget=None, just returns average rets.
    (na_avgrets, na_std, b_error) = tsu.OptPort(na_data, None)

    # Declaring bounds on the optimized portfolio
    na_lower = np.zeros(na_data.shape[1])
    na_upper = np.ones(na_data.shape[1])

    # Getting the range of possible returns with these bounds
    (f_min, f_max) = tsu.getRetRange(na_data, na_lower, na_upper,
                            na_avgrets, s_type="long")

    # Getting the step size and list of returns to optimize for.
    f_step = (f_max - f_min) / 100.0
    lf_returns = [f_min + x * f_step for x in range(101)]

    # Declaring empty lists
    lf_std = []
    lna_portfolios = []

    # Calling the optimization for all returns
    for f_target in lf_returns:
        (na_weights, f_std, b_error) = tsu.OptPort(na_data, f_target,
                                na_lower, na_upper, s_type="long")
        lf_std.append(f_std)
        lna_portfolios.append(na_weights)

    return (lf_returns, lf_std, lna_portfolios, na_avgrets, na_std)


def main():
    '''Main Function'''

    # S&P 100
    ls_symbols = ['AAPL', 'ABT', 'ACN', 'AEP', 'ALL', 'AMGN', 'AMZN', 'APC', 'AXP', 'BA', 'BAC', 'BAX', 'BHI', 'BK', 'BMY', 'BRK.B', 'CAT', 'C', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CPB', 'CSCO', 'CVS', 'CVX', 'DD', 'DELL', 'DIS', 'DOW', 'DVN', 'EBAY', 'EMC', 'EXC', 'F', 'FCX', 'FDX', 'GD', 'GE', 'GILD', 'GOOG', 'GS', 'HAL', 'HD', 'HNZ', 'HON', 'HPQ', 'IBM', 'INTC', 'JNJ', 'JPM', 'KFT', 'KO', 'LLY', 'LMT', 'LOW', 'MA', 'MCD', 'MDT', 'MET', 'MMM', 'MO', 'MON', 'MRK', 'MS', 'MSFT', 'NKE', 'NOV', 'NSC', 'NWSA', 'NYX', 'ORCL', 'OXY', 'PEP', 'PFE', 'PG', 'PM', 'QCOM', 'RF', 'RTN', 'SBUX', 'SLB', 'HSH', 'SO', 'SPG', 'T', 'TGT', 'TWX', 'TXN', 'UNH', 'UPS', 'USB', 'UTX', 'VZ', 'WAG', 'WFC', 'WMB', 'WMT', 'XOM']

    # Creating an object of the dataaccess class with Yahoo as the source.
    c_dataobj = da.DataAccess('Yahoo')

    ls_all_syms = c_dataobj.get_all_symbols()
    # Bad symbols are symbols present in portfolio but not in all syms
    ls_bad_syms = list(set(ls_symbols) - set(ls_all_syms))
    for s_sym in ls_bad_syms:
        i_index = ls_symbols.index(s_sym)
        ls_symbols.pop(i_index)

    # Start and End date of the charts
    dt_end = dt.datetime(2010, 1, 1)
    dt_start = dt_end - dt.timedelta(days=365)
    dt_test = dt_end + dt.timedelta(days=365)

    # We need closing prices so the timestamp should be hours=16.
    dt_timeofday = dt.timedelta(hours=16)

    # Get a list of trading days between the start and the end.
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)
    ldt_timestamps_test = du.getNYSEdays(dt_end, dt_test, dt_timeofday)

    # Reading just the close prices
    df_close = c_dataobj.get_data(ldt_timestamps, ls_symbols, "close")
    df_close_test = c_dataobj.get_data(ldt_timestamps_test, ls_symbols, "close")

    # Filling the data for missing NAN values
    df_close = df_close.fillna(method='ffill')
    df_close = df_close.fillna(method='bfill')
    df_close_test = df_close_test.fillna(method='ffill')
    df_close_test = df_close_test.fillna(method='bfill')

    # Copying the data values to a numpy array to get returns
    na_data = df_close.values.copy()
    na_data_test = df_close_test.values.copy()

    # Getting the daily returns
    tsu.returnize0(na_data)
    tsu.returnize0(na_data_test)

    # Calculating the frontier.
    (lf_returns, lf_std, lna_portfolios, na_avgrets, na_std) = getFrontier(na_data)
    (lf_returns_test, lf_std_test, unused, unused, unused) = getFrontier(na_data_test)

    # Plotting the efficient frontier
    plt.clf()
    plt.plot(lf_std, lf_returns, 'b')
    plt.plot(lf_std_test, lf_returns_test, 'r')

    # Plot where the efficient frontier would be the following year
    lf_ret_port_test = []
    lf_std_port_test = []
    for na_portfolio in lna_portfolios:
        na_port_rets = np.dot(na_data_test, na_portfolio)
        lf_std_port_test.append(np.std(na_port_rets))
        lf_ret_port_test.append(np.average(na_port_rets))

    plt.plot(lf_std_port_test, lf_ret_port_test, 'k')

    # Plot indivisual stock risk/return as green +
    for i, f_ret in enumerate(na_avgrets):
        plt.plot(na_std[i], f_ret, 'g+')

    # # Plot some arrows showing transistion of efficient frontier
    # for i in range(0, 101, 10):
    #     plt.arrow(lf_std[i], lf_returns[i], lf_std_port_test[i] - lf_std[i],
    #                 lf_ret_port_test[i] - lf_returns[i], color='k')

    # Labels and Axis
    plt.legend(['2009 Frontier', '2010 Frontier',
        'Performance of \'09 Frontier in 2010'], loc='lower right')
    plt.title('Efficient Frontier For S&P 100 ')
    plt.ylabel('Expected Return')
    plt.xlabel('StDev')
    plt.savefig('tutorial8.pdf', format='pdf')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = setexample
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 24, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: An example to show how dataAccess works.
'''

# QSTK Imports
import QSTK.qstkutil.qsdateutil as du
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.DataAccess as da

# Third Party Imports
import datetime as dt
import matplotlib.pyplot as plt
import pandas as pd


def main():
    ''' Main Function'''
    # Creating an object of DataAccess Class
    c_dataobj = da.DataAccess('Yahoo')

    # Getting a list of symbols from Lists
    # Lists : S&P5002012, S&P5002008, Index
    ls_symbols = c_dataobj.get_symbols_from_list('sp5002012')
    print "Symbols from the list : ", ls_symbols

    # All symbols possible
    ls_all_syms = c_dataobj.get_all_symbols()
    print "All symbols : ", ls_all_syms

    ls_syms_toread = ['AAPL', 'GOOG']

    # List of TimeStamps to read
    ldt_timestamps = []
    ldt_timestamps.append(dt.datetime(2010, 10, 14, 16))
    ldt_timestamps.append(dt.datetime(2010, 10, 15, 16))
    ldt_timestamps.append(dt.datetime(2010, 11, 21, 16))
    ldt_timestamps.append(dt.datetime(2010, 11, 22, 16))
    ldt_timestamps.append(dt.datetime(2010, 11, 23, 16))
    ldt_timestamps.append(dt.datetime(2010, 11, 24, 16))
    ldt_timestamps.append(dt.datetime(2010, 11, 25, 16))
    ldt_timestamps.append(dt.datetime(2010, 11, 26, 16))
    ldt_timestamps.append(dt.datetime(2010, 11, 27, 10))
    ldt_timestamps.append(dt.datetime(2010, 11, 27, 16))
    ldt_timestamps.append(dt.datetime(2020, 11, 27, 16))
    ldt_timestamps.append(dt.datetime(2020, 11, 27, 18))

    # Reading the data
    # By default it'll read data from the default data provided,
    # But a path can be provided using either an environment variable or
    # as a prarameter.
    df_close = c_dataobj.get_data(ldt_timestamps, ls_syms_toread, "close")
    print df_close


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tutorial
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on January, 23, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Event Profiler Tutorial
'''


import pandas as pd
import numpy as np
import math
import copy
import QSTK.qstkutil.qsdateutil as du
import datetime as dt
import QSTK.qstkutil.DataAccess as da
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkstudy.EventProfiler as ep

"""
Accepts a list of symbols along with start and end date
Returns the Event Matrix which is a pandas Datamatrix
Event matrix has the following structure :
    |IBM |GOOG|XOM |MSFT| GS | JP |
(d1)|nan |nan | 1  |nan |nan | 1  |
(d2)|nan | 1  |nan |nan |nan |nan |
(d3)| 1  |nan | 1  |nan | 1  |nan |
(d4)|nan |  1 |nan | 1  |nan |nan |
...................................
...................................
Also, d1 = start date
nan = no information about any event.
1 = status bit(positively confirms the event occurence)
"""


def find_events(ls_symbols, d_data):
    ''' Finding the event dataframe '''
    df_close = d_data['close']
    ts_market = df_close['SPY']

    print "Finding Events"

    # Creating an empty dataframe
    df_events = copy.deepcopy(df_close)
    df_events = df_events * np.NAN

    # Time stamps for the event range
    ldt_timestamps = df_close.index

    for s_sym in ls_symbols:
        for i in range(1, len(ldt_timestamps)):
            # Calculating the returns for this timestamp
            f_symprice_today = df_close[s_sym].ix[ldt_timestamps[i]]
            f_symprice_yest = df_close[s_sym].ix[ldt_timestamps[i - 1]]
            f_marketprice_today = ts_market.ix[ldt_timestamps[i]]
            f_marketprice_yest = ts_market.ix[ldt_timestamps[i - 1]]
            f_symreturn_today = (f_symprice_today / f_symprice_yest) - 1
            f_marketreturn_today = (f_marketprice_today / f_marketprice_yest) - 1

            # Event is found if the symbol is down more then 3% while the
            # market is up more then 2%
            if f_symreturn_today <= -0.03 and f_marketreturn_today >= 0.02:
                df_events[s_sym].ix[ldt_timestamps[i]] = 1

    return df_events


if __name__ == '__main__':
    dt_start = dt.datetime(2008, 1, 1)
    dt_end = dt.datetime(2009, 12, 31)
    ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt.timedelta(hours=16))

    dataobj = da.DataAccess('Yahoo')
    ls_symbols = dataobj.get_symbols_from_list('sp5002012')
    ls_symbols.append('SPY')

    ls_keys = ['open', 'high', 'low', 'close', 'volume', 'actual_close']
    ldf_data = dataobj.get_data(ldt_timestamps, ls_symbols, ls_keys)
    d_data = dict(zip(ls_keys, ldf_data))

    for s_key in ls_keys:
        d_data[s_key] = d_data[s_key].fillna(method='ffill')
        d_data[s_key] = d_data[s_key].fillna(method='bfill')
        d_data[s_key] = d_data[s_key].fillna(1.0)

    df_events = find_events(ls_symbols, d_data)
    print "Creating Study"
    ep.eventprofiler(df_events, d_data, i_lookback=20, i_lookforward=20,
                s_filename='MyEventStudy.pdf', b_market_neutral=True, b_errorbars=True,
                s_market_sym='SPY')

########NEW FILE########
__FILENAME__ = featuretest
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Nov 7, 2011

@author: John Cornwell
@contact: JohnWCornwellV@gmail.com
@summary: File containing a simple test of the feature engine.
'''

''' Python imports '''
import datetime as dt

''' 3rd party imports '''
import numpy as np
import pandas as pand
import matplotlib.pyplot as plt

''' QSTK imports '''
from QSTK.qstkutil import DataAccess as da
from QSTK.qstkutil import qsdateutil as du

from QSTK.qstkfeat.features import featMA, featRSI
from QSTK.qstkfeat.classes import class_fut_ret
import QSTK.qstkfeat.featutil as ftu


def learnerTest( naTrain, naTest ):
    ''' 
    @summary: Takes testing and training data and computes average error over the test set
              This is compared to a baseline guess which is just the average of the training set
    '''
    llRange = range(5,51,5)
    
    lfRes = []
    for lK in llRange:
        cLearn = ftu.createKnnLearner( naTrain, lKnn=lK, leafsize=100 )
        fError = 0.0

        naResult = cLearn.query( naTest[:,:-1] )
        naError = abs( naResult - naTest[:,-1] )
        lfRes.append( np.average(naError) )
    
    ''' Generate error of 'dumb' case, just assume average returns every time '''
    naGuess = np.ones( naTest.shape[0] ) * np.average( naTrain[:,-1] )
    lfGuess = [ np.average( abs(naGuess - naTest[:,-1]) ) ] * len(lfRes)
    
    fAvgRets = np.average(naTest[:,-1])
    
    plt.clf()
    plt.plot( llRange, lfRes )
    plt.plot( llRange, lfGuess )
    plt.title( 'Average error on average returns of %.04lf'%fAvgRets )
    plt.legend( ('Learner Predict', 'Average Return Predict') )
    plt.xlabel('K value')
    plt.ylabel('Error')
    plt.show()
    plt.savefig( 'FeatureTest.png', format='png' )
    
    

if __name__ == '__main__':
    
    ''' Use Dow 30 '''
    lsSym = ['AA', 'AXP', 'BA', 'BAC', 'CAT', 'CSCO', 'CVX', 'DD', 'DIS', 'GE', 'HD', 'HPQ', 'IBM', 'INTC', 'JNJ', \
             'JPM', 'KFT', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'PFE', 'PG', 'T', 'TRV', 'UTX', 'WMT', 'XOM'  ]
    #lsSym = ['XOM']
    
    ''' Get data for 2009-2010 '''
    dtStart = dt.datetime(2010,8,01)
    dtEnd = dt.datetime(2010,12,31)
    
    norObj = da.DataAccess('Yahoo')      
    ldtTimestamps = du.getNYSEdays( dtStart, dtEnd, dt.timedelta(hours=16) )
    
    lsKeys = ['open', 'high', 'low', 'close', 'volume']
    ldfData = norObj.get_data( ldtTimestamps, lsSym, lsKeys )
    dData = dict(zip(lsKeys, ldfData))
    
    ''' Imported functions from qstkfeat.features, NOTE: last function is classification '''
    lfcFeatures = [ featMA, featRSI, class_fut_ret ]

    ''' Default Arguments '''
    #ldArgs = [{}] * len(lfcFeatures) 
    
    ''' Custom Arguments '''
    ldArgs = [ {'lLookback':30, 'bRel':True},\
               {},\
               {}]                    
    
    ''' Generate a list of DataFrames, one for each feature, with the same index/column structure as price data '''
    ldfFeatures = ftu.applyFeatures( dData, lfcFeatures, ldArgs )
    
    
    bPlot = False
    if bPlot:
        ''' Plot feature for XOM '''
        for i, fcFunc in enumerate(lfcFeatures[:-1]):
            plt.clf()
            plt.subplot(211)
            plt.title( fcFunc.__name__ )
            plt.plot( dfPrice.index, dfPrice['XOM'].values, 'r-' )
            plt.subplot(212)
            plt.plot( dfPrice.index, ldfFeatures[i]['XOM'].values, 'g-' )
            plt.show()
     
    ''' Pick Test and Training Points '''
    lSplit = int(len(ldtTimestamps) * 0.7)
    dtStartTrain = ldtTimestamps[0]
    dtEndTrain = ldtTimestamps[lSplit]
    dtStartTest = ldtTimestamps[lSplit+1]
    dtEndTest = ldtTimestamps[-1]
     
    ''' Stack all information into one Numpy array ''' 
    naFeatTrain = ftu.stackSyms( ldfFeatures, dtStartTrain, dtEndTrain )
    naFeatTest = ftu.stackSyms( ldfFeatures, dtStartTest, dtEndTest )
    
    ''' Normalize features, use same normalization factors for testing data as training data '''
    ltWeights = ftu.normFeatures( naFeatTrain, -1.0, 1.0, False )
    ''' Normalize query points with same weights that come from test data '''
    ftu.normQuery( naFeatTest[:,:-1], ltWeights )

    learnerTest( naFeatTrain, naFeatTest )
    
    
    

########NEW FILE########
__FILENAME__ = code
''' Python imports '''
import datetime as dt

''' 3rd party imports '''
import numpy as np
import pandas as pand
import matplotlib.pyplot as plt

''' QSTK imports '''
from QSTK.qstkutil import DataAccess as da
from QSTK.qstkutil import qsdateutil as du

from QSTK.qstkfeat.features import *
from QSTK.qstkfeat.classes import class_fut_ret
import QSTK.qstkfeat.featutil as ftu	
	
import sys
import time

from functions import *

if __name__ == '__main__':
	''' Use Dow 30 '''
	#lsSym = ['AA', 'AXP', 'BA', 'BAC', 'CAT', 'CSCO', 'CVX', 'DD', 'DIS', 'GE', 'HD', 'HPQ', 'IBM', 'INTC', 'JNJ', \
	#		 'JPM', 'KFT', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'PFE', 'PG', 'T', 'TRV', 'UTX', 'WMT', 'XOM'  ]

	#lsSymTrain = lsSym[0:4] + ['$SPX']
	#lsSymTest = lsSym[4:8] + ['$SPX']
	
	f = open('2008Dow30.txt')
	lsSymTrain = f.read().splitlines() + ['$SPX']
	f.close()
	
	f = open('2010Dow30.txt')
	lsSymTest = f.read().splitlines() + ['$SPX']
	f.close()
	
	lsSym = list(set(lsSymTrain).union(set(lsSymTest)))
	
	dtStart = dt.datetime(2008,01,01)
	dtEnd = dt.datetime(2010,12,31)
	
	norObj = da.DataAccess('Norgate')	  
	ldtTimestamps = du.getNYSEdays( dtStart, dtEnd, dt.timedelta(hours=16) )	
	
	lsKeys = ['open', 'high', 'low', 'close', 'volume']
	
	ldfData = norObj.get_data( ldtTimestamps, lsSym, lsKeys ) #this line is important even though the ret value is not used
	
	for temp in ldfData:
		temp.fillna(method="ffill").fillna(method="bfill")
	
	ldfDataTrain = norObj.get_data( ldtTimestamps, lsSymTrain, lsKeys )
	ldfDataTest = norObj.get_data( ldtTimestamps, lsSymTest, lsKeys)
	
	for temp in ldfDataTrain:
		temp.fillna(method="ffill").fillna(method="bfill")
		
	for temp in ldfDataTest:
		temp.fillna(method="ffill").fillna(method="bfill")
	
	dDataTrain = dict(zip(lsKeys, ldfDataTrain))
	dDataTest = dict(zip(lsKeys, ldfDataTest))
	
	''' Imported functions from qstkfeat.features, NOTE: last function is classification '''
	lfcFeatures = [ featMA, featMA, featMA, featMA, featMA, featMA, \
					featRSI, featRSI, featRSI, featRSI, featRSI, featRSI, \
					featDrawDown, featDrawDown, featDrawDown, featDrawDown, featDrawDown, featDrawDown, \
					featRunUp, featRunUp, featRunUp, featRunUp, featRunUp, featRunUp, \
					featVolumeDelta, featVolumeDelta, featVolumeDelta, featVolumeDelta, featVolumeDelta, featVolumeDelta, \
					featAroon, featAroon, featAroon, featAroon, featAroon, featAroon, featAroon, featAroon, featAroon, featAroon, featAroon, featAroon, \
					#featStochastic, featStochastic, featStochastic, featStochastic, featStochastic, featStochastic,featStochastic, featStochastic, featStochastic, featStochastic, featStochastic, featStochastic, \
					featBeta, featBeta, featBeta, featBeta, featBeta, featBeta,\
					featBollinger, featBollinger, featBollinger, featBollinger, featBollinger, featBollinger,\
					featCorrelation, featCorrelation, featCorrelation, featCorrelation, featCorrelation, featCorrelation,\
					featPrice, \
					featVolume, \
					class_fut_ret]

	ldArgs = [  {'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{'lLookback':5,'bDown':True},{'lLookback':10,'bDown':True},{'lLookback':20,'bDown':True},{'lLookback':5,'bDown':False},{'lLookback':10,'bDown':False},{'lLookback':20,'bDown':False},{'lLookback':5,'bDown':True,'MR':True},{'lLookback':10,'bDown':True,'MR':True},{'lLookback':20,'bDown':True,'MR':True},{'lLookback':5,'bDown':False,'MR':True},{'lLookback':10,'bDown':False,'MR':True},{'lLookback':20,'bDown':False,'MR':True},\
				#{'lLookback':5,'bFast':True},{'lLookback':10,'bFast':True},{'lLookback':20,'bFast':True},{'lLookback':5,'bFast':False},{'lLookback':10,'bFast':False},{'lLookback':20,'bFast':False},{'lLookback':5,'bFast':True,'MR':True},{'lLookback':10,'bFast':True,'MR':True},{'lLookback':20,'bFast':True,'MR':True},{'lLookback':5,'bFast':False,'MR':True},{'lLookback':10,'bFast':False,'MR':True},{'lLookback':20,'bFast':False,'MR':True},\
				{'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{'lLookback':5},{'lLookback':10},{'lLookback':20}, {'lLookback':5,'MR':True},{'lLookback':10,'MR':True},{'lLookback':20,'MR':True},\
				{},\
				{},\
				{'i_lookforward':5}
				]
	
	
	''' Generate a list of DataFrames, one for each feature, with the same index/column structure as price data '''
	ldfFeaturesTrain = ftu.applyFeatures( dDataTrain, lfcFeatures, ldArgs, '$SPX')
	ldfFeaturesTest = ftu.applyFeatures( dDataTest, lfcFeatures, ldArgs, '$SPX')

	''' Pick Test and Training Points '''		
	dtStartTrain = dt.datetime(2008,01,01)
	dtEndTrain = dt.datetime(2009,12,31)
	dtStartTest = dt.datetime(2010,01,01)
	dtEndTest = dt.datetime(2010,12,31)
	
	''' Stack all information into one Numpy array ''' 
	naFeatTrain = ftu.stackSyms( ldfFeaturesTrain, dtStartTrain, dtEndTrain )
	naFeatTest = ftu.stackSyms( ldfFeaturesTest, dtStartTest, dtEndTest )
	
	''' Normalize features, use same normalization factors for testing data as training data '''
	ltWeights = ftu.normFeatures( naFeatTrain, -1.0, 1.0, False )
	''' Normalize query points with same weights that come from test data '''
	ftu.normQuery( naFeatTest[:,:-1], ltWeights )	
	

	lFeatures = range(0,len(lfcFeatures)-1)
	classLabelIndex = len(lfcFeatures) - 1
	
	funccall = sys.argv[1] + '(naFeatTrain,naFeatTest,lFeatures,classLabelIndex)'
	
	timestart = time.time()
	clockstart = time.clock()
	eval(funccall)
	clockend = time.clock()
	timeend = time.time()
	
	sys.stdout.write('\n\nclock diff: '+str(clockend-clockstart)+'sec\n')
	sys.stdout.write('time diff: '+str(timeend-timestart)+'sec\n')
	



########NEW FILE########
__FILENAME__ = functions
''' Python imports '''
import datetime as dt

''' 3rd party imports '''
import numpy as np
import pandas as pand
import matplotlib.pyplot as plt

''' QSTK imports '''
from QSTK.qstkutil import DataAccess as da
from QSTK.qstkutil import qsdateutil as du

from QSTK.qstkfeat.features import featMA, featRSI, featDrawDown, featRunUp, featVolumeDelta, featAroon
from QSTK.qstkfeat.classes import class_fut_ret
import QSTK.qstkfeat.featutil as ftu	
	
import sys

MAX_ITERATIONS = 500
	
def learnerTest( naTrain, naTest ):
	#create the learner with the train set with K=5
	cLearn = ftu.createKnnLearner( naTrain, lKnn=5 )
	#get the Y values predicted by the learner
	Ypredicted = cLearn.query( naTest[:,:-1] )
	#get the actual Y values
	Y = naTest[:,-1]
	#calculate the correlation coefficient
	corrcoef = np.corrcoef(Y,Ypredicted)[0][1]
	#return the corrcoef
	return corrcoef	

def nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex):
	sys.stdout.write('nextBestFeature\n')
	
	bestFeature = -1
	bestFeatureCorrCoef = 0
	for x in lRemainingFeatures:
		lCurrentFeatureSet = [x]+lSelectedFeatures+[classLabelIndex]
		sys.stdout.write('testing feature set ' +  str(lCurrentFeatureSet))
		currentTrain = naFeatTrain[:,lCurrentFeatureSet]
		currentTest = naFeatTest[:,lCurrentFeatureSet]
		currentCorrCoef = learnerTest(currentTrain,currentTest)
		sys.stdout.write(' :: corr coef = ' + str(currentCorrCoef) + '\n')
		if bestFeature == -1 or bestFeatureCorrCoef < currentCorrCoef:
			bestFeature = x
			bestFeatureCorrCoef = currentCorrCoef
			
	sys.stdout.write('nextBestFeature: ' + str(bestFeature) + '\n')
	sys.stdout.write('bestFeatureCorrCoef: ' + str(bestFeatureCorrCoef) + '\n\n')
	return {'bestFeature':bestFeature,'bestFeatureCorrCoef':bestFeatureCorrCoef}
			
def nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex):
	sys.stdout.write('nextWorstFeature\n')

	if len(lSelectedFeatures) == 1:
		sys.stdout.write('nextWorstFeature: ' + str(lSelectedFeatures[0]) + '\n')
		sys.stdout.write('worstFeatureCorrCoef: ' + str(-999) + '\n\n')
		return {'worstFeature':lSelectedFeatures[0],'worstFeatureCorrCoef':-999}

	worstFeature = -1
	worstFeatureCorrCoef = 0
	for x in lSelectedFeatures:
		lSelectedFeaturesCopy = lSelectedFeatures[:]
		lSelectedFeaturesCopy.remove(x);			
		lCurrentFeatureSet = lSelectedFeaturesCopy + [classLabelIndex]
		sys.stdout.write('testing feature set ' +  str(lCurrentFeatureSet))
		currentTrain = naFeatTrain[:,lCurrentFeatureSet]
		currentTest = naFeatTest[:,lCurrentFeatureSet]
		currentCorrCoef = learnerTest(currentTrain,currentTest)
		sys.stdout.write(' :: corr coef = ' + str(currentCorrCoef) + '\n')
		if worstFeature == -1 or worstFeatureCorrCoef < currentCorrCoef:
			worstFeature = x
			worstFeatureCorrCoef = currentCorrCoef
			
	sys.stdout.write('nextWorstFeature: ' + str(worstFeature) + '\n')
	sys.stdout.write('worstFeatureCorrCoef: ' + str(worstFeatureCorrCoef) + '\n\n')
	return {'worstFeature':worstFeature,'worstFeatureCorrCoef':worstFeatureCorrCoef}

def sequentialForwardSelection(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	lSelectedFeatures = list()
	lRemainingFeatures = lFeatures[:]
	lCorrCoef = list();
	while len(lRemainingFeatures) > 0:
		sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
		sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
		sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
		retValue = nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex)
		lSelectedFeatures.append(retValue['bestFeature'])
		lRemainingFeatures.remove(retValue['bestFeature'])
		lCorrCoef.append(retValue['bestFeatureCorrCoef'])
		
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	sys.stdout.write('best feature set is ' + str(lSelectedFeatures[0:maxlCorrCoefIndex+1]+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef

def sequentialBackwardSelection(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	lSelectedFeatures = lFeatures[:]
	lCorrCoef = list()
	lRemovedFeatures = list()
	while len(lSelectedFeatures) > 0:
		sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
		sys.stdout.write('lRemovedFeatures: ' + str(lRemovedFeatures) + '\n')
		sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
		retValue = nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex)
		lSelectedFeatures.remove(retValue['worstFeature'])
		lCorrCoef.append(retValue['worstFeatureCorrCoef'])
		lRemovedFeatures.append(retValue['worstFeature'])
		
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	lBestSet = list(set(lFeatures) - set(lRemovedFeatures[0:maxlCorrCoefIndex+1]))
	sys.stdout.write('best feature set is ' + str(lBestSet+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef

def sequentialFloatingForwardSelection(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	global MAX_ITERATIONS
	lSelectedFeatures = list()
	lRemainingFeatures = lFeatures[:]
	lCorrCoef = list()
	lSeenStates = list()
	while len(lRemainingFeatures) > 0:
		sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
		sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
		sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
		retValue = nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex)
		lSelectedFeatures.append(retValue['bestFeature'])
		lSeenStates.append(set(lSelectedFeatures))
		lRemainingFeatures.remove(retValue['bestFeature'])
		lCorrCoef.append(retValue['bestFeatureCorrCoef'])
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue2 = nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex)
			if lCorrCoef[-1] < retValue2['worstFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.remove(retValue2['worstFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not removed b/c state already seen. \n\n')
					break
				lSelectedFeatures.remove(retValue2['worstFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.append(retValue2['worstFeature'])
				lCorrCoef.append(retValue2['worstFeatureCorrCoef'])
			else:
				sys.stdout.write('feature not removed b/c corr not higher. \n\n')
				break
		if len(lSeenStates) >= MAX_ITERATIONS:
			sys.stdout.write('QUITTING B/C len(lSeenStates) >= MAX_ITERATIONS: ' + str(len(lSeenStates)) + '\n\n')
			break
	
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	sys.stdout.write('best feature set is ' + str(list(lSeenStates[maxlCorrCoefIndex])+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef
	
def sequentialFloatingBackwardSelection(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	global MAX_ITERATIONS
	lSelectedFeatures = lFeatures[:]
	lRemainingFeatures = list()
	lCorrCoef = list()
	lSeenStates = list()
	while len(lSelectedFeatures) > 0:
		sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
		sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
		sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
		retValue = nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex)
		lSelectedFeatures.remove(retValue['worstFeature'])
		lSeenStates.append(set(lSelectedFeatures))
		lRemainingFeatures.append(retValue['worstFeature'])
		lCorrCoef.append(retValue['worstFeatureCorrCoef'])
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue2 = nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex)
			if lCorrCoef[-1] < retValue2['bestFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.add(retValue2['bestFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not added b/c state already seen. \n\n')
					break
				lSelectedFeatures.append(retValue2['bestFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.remove(retValue2['bestFeature'])
				lCorrCoef.append(retValue2['bestFeatureCorrCoef'])
			else:
				sys.stdout.write('feature not added b/c corr not higher. \n\n')
				break
		if len(lSeenStates) >= MAX_ITERATIONS:
			sys.stdout.write('QUITTING B/C len(lSeenStates) >= MAX_ITERATIONS: ' + str(len(lSeenStates)) + '\n\n')
			break
	
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	sys.stdout.write('best feature set is ' + str(list(lSeenStates[maxlCorrCoefIndex])+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef
	
def sequentialFloatingForwardSelectionNew(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	global MAX_ITERATIONS
	lSelectedFeatures = list()
	lRemainingFeatures = lFeatures[:]
	lCorrCoef = list()
	lSeenStates = list()
	
	retValue = 0
	while len(lRemainingFeatures) > 0:
		change = False
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue = nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex)
			if len(lCorrCoef)==0 or lCorrCoef[-1] < retValue['bestFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.add(retValue['bestFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not added b/c state already seen. \n\n')
					break
				lSelectedFeatures.append(retValue['bestFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.remove(retValue['bestFeature'])
				lCorrCoef.append(retValue['bestFeatureCorrCoef'])
			else:
				sys.stdout.write('feature not added b/c corr not higher. \n\n')
				break
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue2 = nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex)
			if lCorrCoef[-1] < retValue2['worstFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.remove(retValue2['worstFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not removed b/c state already seen. \n\n')
					break
				lSelectedFeatures.remove(retValue2['worstFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.append(retValue2['worstFeature'])
				lCorrCoef.append(retValue2['worstFeatureCorrCoef'])
				change = True
			else:
				sys.stdout.write('feature not removed b/c corr not higher. \n\n')
				break
		if not change:
			lSelectedFeatures.append(retValue['bestFeature'])
			lSeenStates.append(set(lSelectedFeatures))
			lRemainingFeatures.remove(retValue['bestFeature'])
			lCorrCoef.append(retValue['bestFeatureCorrCoef'])
			sys.stdout.write('feature added b/c no features were removed. \n\n')
			
		if len(lSeenStates) >= MAX_ITERATIONS:
			sys.stdout.write('QUITTING B/C len(lSeenStates) >= MAX_ITERATIONS: ' + str(len(lSeenStates)) + '\n\n')
			break
	
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	sys.stdout.write('best feature set is ' + str(list(lSeenStates[maxlCorrCoefIndex])+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef

def sequentialFloatingBackwardSelectionNew(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	global MAX_ITERATIONS
	lSelectedFeatures = lFeatures[:]
	lRemainingFeatures = list()
	lCorrCoef = list()
	lSeenStates = list()
	
	retValue = 0
	while len(lSelectedFeatures) > 0:
		change = False
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue = nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex)
			if len(lCorrCoef)==0 or lCorrCoef[-1] < retValue['worstFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.remove(retValue['worstFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not removed b/c state already seen. \n\n')
					break
				lSelectedFeatures.remove(retValue['worstFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.append(retValue['worstFeature'])
				lCorrCoef.append(retValue['worstFeatureCorrCoef'])
			else:
				sys.stdout.write('feature not removed b/c corr not higher. \n\n')
				break
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue2 = nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex)
			if lCorrCoef[-1] < retValue2['bestFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.add(retValue2['bestFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not added b/c state already seen. \n\n')
					break
				lSelectedFeatures.append(retValue2['bestFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.remove(retValue2['bestFeature'])
				lCorrCoef.append(retValue2['bestFeatureCorrCoef'])
				change = True
			else:
				sys.stdout.write('feature not added b/c corr not higher. \n\n')
				break
		if not change:
			lSelectedFeatures.remove(retValue['worstFeature'])
			lSeenStates.append(set(lSelectedFeatures))
			lRemainingFeatures.append(retValue['worstFeature'])
			lCorrCoef.append(retValue['worstFeatureCorrCoef'])
			sys.stdout.write('feature removed b/c no features were added. \n\n')
			
		if len(lSeenStates) >= MAX_ITERATIONS:
			sys.stdout.write('QUITTING B/C len(lSeenStates) >= MAX_ITERATIONS: ' + str(len(lSeenStates)) + '\n\n')
			break
	
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	sys.stdout.write('best feature set is ' + str(list(lSeenStates[maxlCorrCoefIndex])+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef
	
def sequentialFloatingForwardSelectionNew_Max(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	global MAX_ITERATIONS
	lSelectedFeatures = list()
	lRemainingFeatures = lFeatures[:]
	lCorrCoef = list()
	lSeenStates = list()
	maxCorrCoef = -100
	
	retValue = 0
	while len(lRemainingFeatures) > 0:
		change = False
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue = nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex)
			if len(lCorrCoef)==0 or maxCorrCoef < retValue['bestFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.add(retValue['bestFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not added b/c state already seen. \n\n')
					break
				lSelectedFeatures.append(retValue['bestFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.remove(retValue['bestFeature'])
				lCorrCoef.append(retValue['bestFeatureCorrCoef'])
				maxCorrCoef = max(maxCorrCoef,retValue['bestFeatureCorrCoef'])
			else:
				sys.stdout.write('feature not added b/c corr not higher. \n\n')
				break
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue2 = nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex)
			if maxCorrCoef < retValue2['worstFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.remove(retValue2['worstFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not removed b/c state already seen. \n\n')
					break
				lSelectedFeatures.remove(retValue2['worstFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.append(retValue2['worstFeature'])
				lCorrCoef.append(retValue2['worstFeatureCorrCoef'])
				maxCorrCoef = max(maxCorrCoef,retValue2['worstFeatureCorrCoef'])
				change = True
			else:
				sys.stdout.write('feature not removed b/c corr not higher. \n\n')
				break
		if not change:
			lSelectedFeatures.append(retValue['bestFeature'])
			lSeenStates.append(set(lSelectedFeatures))
			lRemainingFeatures.remove(retValue['bestFeature'])
			lCorrCoef.append(retValue['bestFeatureCorrCoef'])
			maxCorrCoef = max(maxCorrCoef,retValue['bestFeatureCorrCoef'])
			sys.stdout.write('feature added b/c no features were removed. \n\n')
			
		if len(lSeenStates) >= MAX_ITERATIONS:
			sys.stdout.write('QUITTING B/C len(lSeenStates) >= MAX_ITERATIONS: ' + str(len(lSeenStates)) + '\n\n')
			break
	
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	sys.stdout.write('best feature set is ' + str(list(lSeenStates[maxlCorrCoefIndex])+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef

def sequentialFloatingBackwardSelectionNew_Max(naFeatTrain,naFeatTest,lFeatures,classLabelIndex):
	global MAX_ITERATIONS
	lSelectedFeatures = lFeatures[:]
	lRemainingFeatures = list()
	lCorrCoef = list()
	lSeenStates = list()
	maxCorrCoef = -100
	
	retValue = 0
	while len(lSelectedFeatures) > 0:
		change = False
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue = nextWorstFeature(naFeatTrain,naFeatTest,lSelectedFeatures,classLabelIndex)
			if len(lCorrCoef)==0 or maxCorrCoef < retValue['worstFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.remove(retValue['worstFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not removed b/c state already seen. \n\n')
					break
				lSelectedFeatures.remove(retValue['worstFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.append(retValue['worstFeature'])
				lCorrCoef.append(retValue['worstFeatureCorrCoef'])
				maxCorrCoef = max(maxCorrCoef,retValue['worstFeatureCorrCoef'])
			else:
				sys.stdout.write('feature not removed b/c corr not higher. \n\n')
				break
		while True:
			sys.stdout.write('lSelectedFeatures: ' + str(lSelectedFeatures) + '\n')
			sys.stdout.write('lRemainingFeatures: ' + str(lRemainingFeatures) + '\n')
			sys.stdout.write('lCorrCoef: ' + str(lCorrCoef) + '\n')
			retValue2 = nextBestFeature(naFeatTrain,naFeatTest,lSelectedFeatures,lRemainingFeatures,classLabelIndex)
			if maxCorrCoef < retValue2['bestFeatureCorrCoef']:
				newState = set(lSelectedFeatures)
				newState.add(retValue2['bestFeature'])
				if newState in lSeenStates:
					sys.stdout.write('feature not added b/c state already seen. \n\n')
					break
				lSelectedFeatures.append(retValue2['bestFeature'])
				lSeenStates.append(set(lSelectedFeatures))
				lRemainingFeatures.remove(retValue2['bestFeature'])
				lCorrCoef.append(retValue2['bestFeatureCorrCoef'])
				maxCorrCoef = max(maxCorrCoef,retValue2['bestFeatureCorrCoef'])
				change = True
			else:
				sys.stdout.write('feature not added b/c corr not higher. \n\n')
				break
		if not change:
			lSelectedFeatures.remove(retValue['worstFeature'])
			lSeenStates.append(set(lSelectedFeatures))
			lRemainingFeatures.append(retValue['worstFeature'])
			lCorrCoef.append(retValue['worstFeatureCorrCoef'])
			maxCorrCoef = max(maxCorrCoef,retValue['worstFeatureCorrCoef'])
			sys.stdout.write('feature removed b/c no features were added. \n\n')
	
		if len(lSeenStates) >= MAX_ITERATIONS:
			sys.stdout.write('QUITTING B/C len(lSeenStates) >= MAX_ITERATIONS: ' + str(len(lSeenStates)) + '\n\n')
			break
	
	maxlCorrCoef = max(lCorrCoef)
	maxlCorrCoefIndex = lCorrCoef.index(maxlCorrCoef)
	sys.stdout.write('best feature set is ' + str(list(lSeenStates[maxlCorrCoefIndex])+[classLabelIndex]) + '\n')
	sys.stdout.write('corr coef = ' + str(maxlCorrCoef))
	return maxlCorrCoef

########NEW FILE########
__FILENAME__ = Validation
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on February, 9, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Python Validation Script
'''

# Printing what Python Version is installed : QSTK uses 2.7
import sys
import platform
print "Python Details : "
print sys.version
print "Your Python Version is : ", platform.python_version()
print "QSTK uses Python 2.7.X (2.7.3 recommended and supported)"
print "Please make sure you're using the correct python version."
print

# Printing the directory you are in
import os
print "Current Directory : ", os.path.abspath('.')
print

# Printing files in the current directory.
print "Files in the current directory"
ls_files = os.listdir('.')
for s_file in ls_files:
    print s_file
print

# Testing the dependencies
# Testing numpy
try:
    import numpy
    print "Numpy is installed and the version used is : ", numpy.__version__
    print "Please make sure you're using version >= 1.6.1"
except ImportError:
    sys.exit("Error : Numpy can not be imported or not installed.")
print

# Testing matplotlib
try:
    import matplotlib
    print "Matplotlib is installed and version is : ", matplotlib.__version__
    print "Please make sure you're using version >= 1.1.0"
except ImportError:
    sys.exit("Error : Matplotlib can not be imported or not installed.")
print

# Testing Pandas
try:
    import pandas
    print "Pandas is installed and the version used is : ", pandas.__version__
    print "Please make sure you're using version >= 0.7.3"
except ImportError:
    sys.exit("Error : Pandas can not be imported or not installed.")
print


# Testing Scipy
try:
    import scipy
    print "Scipy is installed and the version used is : ", scipy.__version__
    print "Please make sure you're using version >= 0.9.0"
except ImportError:
    sys.exit("Error : Scipy can not be imported or not installed.")
print

# Testing Dateutil
try:
    import dateutil
    print "Dateutil is installed and the version used is : ", dateutil.__version__
    print "Please make sure you're using version == 1.5"
except ImportError:
    sys.exit("Error : Dateutil can not be imported or not installed.")
print

# Testing Setuptools
try:
    import setuptools
    print "Setuptools is installed and the version used is : ", setuptools.__version__
    print "Please make sure you're using version >= 0.6"
except ImportError:
    sys.exit("Error : Setuptools can not be imported or not installed.")
print

# # Testing CVXOPT
# try:
#     import cvxopt
#     print "CVXOPT is installed and can be imported"
# except ImportError:
#     sys.exit("Error : CVXOPT can not be imported or not installed.")
# print

# Testing datetime
try:
    import datetime as dt
    print "datetime is installed and can be imported"
except ImportError:
    sys.exit("Error : datetime can not be imported or not installed.")
print

# All dependencies are installed and working
print "All dependencies are installed and working\n"

# Testing import of QSTK
# Testing QSTK
try:
    import QSTK
    print "QSTK is installed and can be imported"
except ImportError:
    sys.exit("Error : QSTK can not be imported or not installed.")
print

# Testing QSTK.qstkutil
try:
    import QSTK.qstkutil.tsutil as tsu
    import QSTK.qstkutil.qsdateutil as du
    import QSTK.qstkutil.DataAccess as da
    print "QSTK.qstkutil is installed and can be imported"
except ImportError:
    exit("Error : QSTK.qstkutil can not be imported.")
print

# Testing QSTK.qstkstudy
try:
    import QSTK.qstkstudy.EventProfiler
    print "QSTK.qstkstudy is installed and can be imported"
except ImportError:
    exit("Error : QSTK.qstkstudy can not be imported.")
print

# Checking that the data installed is correct.
# Start and End date of the charts
dt_start = dt.datetime(2012, 2, 10)
dt_end = dt.datetime(2012, 2, 24)
dt_timeofday = dt.timedelta(hours=16)

# Get a list of trading days between the start and the end.
ldt_timestamps = du.getNYSEdays(dt_start, dt_end, dt_timeofday)
ls_symbols = ['MSFT', 'GOOG']

# Creating an object of the dataaccess class with Yahoo as the source.
c_dataobj = da.DataAccess('Yahoo', verbose=True)
# Reading adjusted_close prices
df_close = c_dataobj.get_data(ldt_timestamps, ls_symbols, "close")
print df_close
print
print "\nCorrect Output using the Default Data should be : "
print "Assignments use this data for grading"
print "                      MSFT    GOOG"
print "2012-02-10 16:00:00  29.90  605.91"
print "2012-02-13 16:00:00  29.98  612.20"
print "2012-02-14 16:00:00  29.86  609.76"
print "2012-02-15 16:00:00  29.66  605.56"
print "2012-02-16 16:00:00  30.88  606.52"
print "2012-02-17 16:00:00  30.84  604.64"
print "2012-02-21 16:00:00  31.03  614.00"
print "2012-02-22 16:00:00  30.86  607.94"
print "2012-02-23 16:00:00  30.96  606.11"
print

dt_test = dt.datetime(2012, 2, 15, 16)
print "Close price of MSFT on 2012/2/15 is : ", df_close['MSFT'].ix[dt_test]
if df_close['MSFT'].ix[dt_test] == 29.66:
    print "Data looks correct as the close price in default data is 29.66"
else:
    print "Default data used in the assisgnments has close price as 29.66"
    sys.exit("Error : Data has changed so does not match data used in Assignments")
print

print "Everything works fine: You're all set."

########NEW FILE########
__FILENAME__ = compustat_csv_to_pkl
'''

(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on June 1, 2011

@author: John Cornwell
@contact: JohnWCornwellV@gmail.com
@summary: Used to extract Compustat data from a csv dump and convert into individual pickle files
'''

import numpy as np
import datetime as dt
import pickle as pkl
import qstkutil.utils as utils
import qstkutil.DataAccess as da
import os
import dircache
import time
import csv
import sys


def _dumpFiles( dData, lSets, lsOutPaths ):
    '''
    @summary Helper function to store files on disk: files from dictionary dData, set of paths lSets
    @param dData: Dictionary of indexes to data label strings. 
    @param lSets: List of symbol sets, each corresponds to a directory, e.g. NYSE, NASDAQ.
    @param lsOutPaths: List of path strings, same indexes as lSets.
    '''
    lKeys = dData.keys()
     
    for key in lKeys:
        for i, symSet in enumerate( lSets ):
            if key in symSet:
                sFilename = lsOutPaths[i] + key + '.pkl'
                break
        
        fOut = open( sFilename, 'wb' )
        pkl.dump( dData[key], fOut, -1)
        fOut.close()
    
    
def _analyze():
    '''
    @summary Helper function to analyzes the csv data to determine which columns are float values, and generate the set of valid labels.
    '''
    try:
        rootdir = os.environ['QSDATA']
    except KeyError:
        print "Please be sure to set the value for QSDATA in config.sh or local.sh\n"    
        
    ''' Create lists of input and output paths '''
    fFile = ( rootdir + "/Raw/Compustat/Compustat.csv")
      
    spamReader = csv.reader(open(fFile, 'rb'), delimiter=',')
    
    ''' Take first row as the labels '''
    for row in spamReader:
        lsLabels = row
        break    
    
    used = [0] * len(lsLabels)
    badSet = set()
    
    ''' find non-float valued entities '''
    for i, row in enumerate(spamReader):
        for j, elem in enumerate(row):
            if used[j] == 1:
                continue
            
            if elem == '':
                elem = 0.0
            
            try: 
                float(elem)
            except:
                badSet.add( lsLabels[j] )
                used[j] = 1
        
        if( i % 10000 == 0 ):
            print (i / 1378625.0)*100, '%'   
    
    print 'Bad (non float) labels:'
    print badSet   
    
    for i,label in enumerate(lsLabels):
        if label in badSet:
            del lsLabels[i]
    
    print '\n\nGood (float) labels:'
    print lsLabels
    
    return  
    

def convert ():
    '''
    @summary: Converts a Compustat CSV file to pickle files of numpy arrays.
    '''
    
    print "Starting..."+ str(time.strftime("%H:%M:%S"))
    
    ''' Write every so often to save memory, 20k lines is usually < .5GB '''
    lSaveMem = 20000
    
    try:
        rootdir = os.environ['QSDATA']
    except KeyError:
        print "Please be sure to set the value for QSDATA in config.sh or local.sh\n"    
    
    ''' Create lists of input and output paths '''
    fFile = ( rootdir + "/Raw/Compustat/Compustat.csv")

    listOfOutputPaths= []
    listOfOutputPaths.append(rootdir + "/Processed/Compustat/US/AMEX/")
    listOfOutputPaths.append(rootdir + "/Processed/Compustat/US/NASDAQ/")
    listOfOutputPaths.append(rootdir + "/Processed/Compustat/US/NYSE/")    
    
    #If the output paths don't exist, then create them...
    for path in listOfOutputPaths:
        if not (os.access(path, os.F_OK)):
            #Path does not exist, so create it
            os.makedirs(path) #Makes paths recursively
    #done making all output paths!
    
    #In case there are already some files there- remove them. This will remove all the pkl fils from the previous run
    utils.clean_paths (listOfOutputPaths)
      
    spamReader = csv.reader(open(fFile, 'rb'), delimiter=',')
    ''' 1378625 rows typical '''    
    
    ''' Take first row as the labels '''
    for row in spamReader:
        lsLabels = row
        break
   
    ''' Generated from _Analyze() '''
    lsBadLabels = set(['LOC', 'ADD4', 'ADD3', 'ADD2', 'ADD1', 'ACCTCHGQ', 'WEBURL', 'IDBFLAG', 'popsrc', 'DATACQTR', 'conm', 'COSTAT', 'FINALQ', 'fdateq', 'FAX', 'RP', 'PRIROW', 'dldte', 'indfmt', 'SPCSRC', 'BUSDESC', 'ipodate', 'PHONE', 'CURCDQ', 'pdateq', 'DATAFQTR', 'PRICAN', 'EIN', 'datadate', 'tic', 'ADDZIP', 'CONML', 'consol', 'datafmt', 'cusip', 'BSPRQ', 'OGMQ', 'COMPSTQ', 'COUNTY', 'STATE', 'CURNCDQ', 'CITY', 'rdq', 'apdedateq', 'STALTQ', 'INCORP'])

    ''' get list of stocks in 3 US indexes '''
    Access = da.DataAccess( 'Norgate' )
    setNyse = set( Access.get_symbols_in_sublist("/US/NYSE") )
    setNasdaq = set( Access.get_symbols_in_sublist("/US/NASDAQ") )
    setAmex = set( Access.get_symbols_in_sublist("/US/AMEX") )
    
    ''' If stock appears in more than one index, remove to avoid ambiguity '''
    print 'Ignoring duplicate stocks:',
    dup1 =  setNyse.intersection( setNasdaq.union(setAmex))
    dup2 =  setNasdaq.intersection( setAmex )
    print dup1.union(dup2)

    setNyse   = setNyse - dup1.union(dup2)
    setAmex   = setAmex - dup1.union(dup2)
    setNasdaq = setNasdaq - dup1.union(dup2)
    
    ''' Note the two lists below must be in the same order '''
    lsOutPaths = []
    lsOutPaths.append(rootdir + "/Processed/Compustat/US/AMEX/")
    lsOutPaths.append(rootdir + "/Processed/Compustat/US/NASDAQ/")
    lsOutPaths.append(rootdir + "/Processed/Compustat/US/NYSE/")  
    
    lSets = [setAmex, setNasdaq, setNyse]  
    
    #If the output paths don't exist, then create them...
    for path in lsOutPaths:
        if not (os.access(path, os.F_OK)):
            #Path does not exist, so create it
            os.makedirs(path) #Makes paths recursively
    #done making all output paths!
    
    #In case there are already some files there- remove them. This will remove all the pkl fils from the previous run
    utils.clean_paths (lsOutPaths)
        
    lDateCol = 0
    llUseCols = []

    
    ''' We have first row (the labels), loop through saving good label indicies '''
    for j, sElem in enumerate(lsLabels):
        if( sElem not in lsBadLabels ):
            llUseCols.append(j)
        
        ''' Keep track of ticker column and date, specially handled later '''
        if( sElem == 'datadate' ):
            lDateCol = j 
        if( sElem == 'tic' ):
            lTicCol = j 
        
        
    ''' Dict of ticker->numpy array mapping '''
    dData = dict()
    print ''
    
    
    ''' Main loop, iterate over the rows in the csv file '''
    for j, row in enumerate(spamReader):
        lsLabels = row
        sTic = row[lTicCol]
        
        ''' Find out what index this belongs to '''
        lIndex = -1
        for i, symSet in enumerate( lSets ):
            if sTic in symSet:
                lIndex = i
                break
        if lIndex == -1:
            continue
        
        sFilename = lsOutPaths[lIndex] + sTic + '.pkl'
        
        ''' If the file exists (temporary memory saving measure), read it in and delete file from disk '''
        if( os.path.isfile(sFilename)):
            if dData.has_key(sTic):
               print 'File should not be both on disk and in dict'
               sys.exit("FAILURE")
            
            fIn = open( sFilename, 'rb' )
            dData[sTic] = pkl.load( fIn )
            fIn.close()
            os.remove( sFilename )
            
        fDate = float( dt.datetime.strptime( row[lDateCol], "%m/%d/%Y").strftime("%Y%m%d") )
        
        ''' convert blanks to nans '''
        for i in llUseCols:
            if row[i] == '':
                row[i] = 'nan'
        
        ''' Add row if data exists, if not, create new array '''
        if dData.has_key(sTic):       
            dData[sTic] = np.vstack( (dData[sTic], np.array([fDate] + [row[i] for i in llUseCols], dtype=np.float)) )
        else:
            dData[sTic]= np.array( [fDate] + [row[i] for i in llUseCols], dtype=np.float )
        
        if( (j+1) % 1000 == 0):
            fDone = (j / 1378625.0) * 100
            print '\rApprox %.2lf%%'%((j / 1378625.0) * 100),
            
        if( (j+1) % lSaveMem == 0):
            ''' Write all the pickle files we currently have '''
            
            print '\nWriting %i lines to pickle files to save memory\n'%(lSaveMem)
            _dumpFiles( dData, lSets, lsOutPaths)
            ''' Remember to delete! '''
            del dData
            dData = dict()
            
        # Done writing files
    # Done with main loop

    print ''
    print 'Writing final pickle files\n'       
    _dumpFiles( dData, lSets, lsOutPaths)
    del dData
    
    print "Finished..."+ str(time.strftime("%H:%M:%S"))
    
    return
    
if __name__ == '__main__':

    #_analyze()
    convert()
    
    
    
    
########NEW FILE########
__FILENAME__ = csvapi
import numpy as np
import dircache
from sets import Set
import time
import tables as pt
import sys
import time
import os
from optparse import OptionParser

class TimestampsModel (pt.IsDescription):
    timestamp = pt.Time64Col()
#class TimestampsModel ends

class StrategyDataModel(pt.IsDescription):
    symbol = pt.StringCol(30)           #30 char string; Ticker
    exchange = pt.StringCol(10)         #10 char string; NYSE, NASDAQ, etc.
    adj_high = pt.Float32Col()
    adj_low = pt.Float32Col()
    adj_open = pt.Float32Col()
    adj_close = pt.Float32Col()
    close = pt.Float32Col()
    volume = pt.Float32Col()  #Changing from Int32Col()
    timestamp = pt.Time64Col()
    date = pt.Int32Col()
    interval = pt.Time64Col()
#class StrategyDataModel done
    
class StockPriceData:
    def __init__(self):
        self.filt_list=[]
        self.timestamps=[]
        
    def getSymbols(self, listOfPaths, fileExtensionToRemove):
        '''
        @bug: This might not work if the path contains a folder whose name has .csv, or is some random file that has a .csv in it..
               So, lets assume that whoever is using this is not going to "cheat" us
        '''
        listOflistOfStocks=list()
        for path in listOfPaths:
            stocksAtThisPath=list ()
            stocksAtThisPath= dircache.listdir(str(path))
            #Next, throw away everything that is not a .csv And these are our stocks!
            stocksAtThisPath = filter (lambda x:(str(x).find(str(fileExtensionToRemove)) > -1), stocksAtThisPath)
            #Now, we remove the .csv to get the name of the stock
            stocksAtThisPath = map(lambda x:(x.partition(str(fileExtensionToRemove))[0]),stocksAtThisPath)
            
            #Then add that list to listOflistOfStocks
            listOflistOfStocks.append(stocksAtThisPath)       
        return listOflistOfStocks       
        #getSymbols done
   
#build the array
    def getData(self, listOfListOfStocks, listOfInputPaths, startDate, endDate, listOfOutputPaths):
        '''
        @summary: This is where all the work happens
        @attention: Assumption here is that past data never changes
        @bug: The exchange is currently set pretty randomly
        '''

        #Finding no. of stocks
        noOfStocks=0        
        for stock_list in listOfListOfStocks:
            noOfStocks+= len (stock_list)
            #for stock in stock_list:
                #print str(stock)
        
        print "No. of stocks: " + str(noOfStocks)
        print "No. of timestamps: " +  str(len(self.timestamps))

        
        listIndex=-1
        ctr=1;
        for inputFileFolder in listOfInputPaths:
          listIndex+=1
          outputFileFolder= str(listOfOutputPaths[listIndex])
          stocks_list= listOfListOfStocks[listIndex]
          for i in range(0, len(stocks_list)): # - self.count_of_non_existent_stocks):
                print str(stocks_list[i]) +"   "+str(ctr)+" of "+ str(noOfStocks)+"  "+ str(time.strftime("%H:%M:%S"))
                ctr= ctr+1
                
                beginTS= startDate
                
                #Check if the file exists
                if (os.path.exists(str(outputFileFolder) + str(stocks_list[i]+".h5"))):
                    
                     #Checking the last timestamp in the hdf file               
                    h5f=pt.openFile(outputFileFolder + str(stocks_list[i]+".h5"), mode = "a")
                    print "Updating " +str(outputFileFolder + str(stocks_list[i]+".h5"))
                    table= h5f.getNode('/StrategyData', 'StrategyData')
                    beginTS= int(time.strftime("%Y%m%d", time.gmtime(table[table.nrows-1]['timestamp']))) #+ 1 #POSSIBLE BUG?
                    if (str(beginTS) >= self.timestamps[len(self.timestamps)-1]): #if (os.path.getmtime(str(outputFileFolder)+str(stocks_list[i])+".h5") > os.path.getmtime(str(self.dirname+ "/"+ str(stocks_list[i]+".CSV")))):
                        #The hdf5 file for this stock has been modified after the CSV file was modified. Ergo- no changes need to be made to it now..
                        print str(stocks_list[i])+".h5 already is up to date. "+ str(time.strftime("%H:%M:%S"))
                        h5f.close()
                        continue
                    else:
                        #File is present but not upto date
                         beginTS= int(time.strftime("%Y%m%d", time.gmtime(table[table.nrows-1]['timestamp']))) 
                else:
                    #The only foreseeable reason why there might be an exception here is that the hdf file does not exist. So, creating it.
                    print "Creating file: " + str(outputFileFolder) + str(stocks_list[i]+".h5")+"  "+ str(time.strftime("%H:%M:%S"))
                    
                    h5f = pt.openFile(str(outputFileFolder) + str(stocks_list[i]+".h5"), mode = "w")
                    group = h5f.createGroup("/", 'StrategyData')
                    table = h5f.createTable(group, 'StrategyData', StrategyDataModel)
                    beginTS= startDate
                    #else done
          
                f=open(str(inputFileFolder)+str(stocks_list[i]+str(".CSV")))
                jk=f.readlines()
                f.close()
                jk.pop(0)
                
                self.filt_list=list()
                filt_list_temp=filter(lambda x: (int(x.split(',')[1])> int(beginTS)) ,jk) #Because we only want timestamps strictly greater than the last timestamp currently in the file.
                filt_list_temp=filter(lambda x: (int(x.split(',')[1])<= int(endDate)) ,filt_list_temp)
                filt_list_temp=map(lambda x:(x.split(',')[0],x.split(',')[1],x.split(',')[2],x.split(',')[3],x.split(',')[4],x.split(',')[5],x.split(',')[6],(x.split(',')[7]).strip()),filt_list_temp)
                
                self.filt_list.append(filt_list_temp)

                if (table.nrows > 0):
                    #we are appending to an old file and not creating a new file..

                    tsStartIndex= np.array(self.timestamps).searchsorted(beginTS) +1
                    
                else:
                    #creating a new file...
                    tsStartIndex =0
                #if (table.nrows > 0) done
                
                k = 0
                for j in range(tsStartIndex, len(self.timestamps)):
                    if (k< len(self.filt_list[0])):
                                            
                     if((self.timestamps[j])< (self.filt_list[0][k][1])):
                        row=table.row 
                        row['exchange'] = 'NYSE'
                        row['symbol'] = self.filt_list[0][k][0]
                        row['adj_open'] = np.NaN 
                        row['adj_close'] = np.NaN
                        row['adj_high'] = np.NaN
                        row['adj_low'] = np.NaN
                        row['close'] = np.NaN
                        row['volume'] = np.NaN
                        parseddate = time.strptime(self.timestamps[j],'%Y%m%d')
#                        row['date'] = self.timestamps[j]
                        row['timestamp'] = time.mktime(parseddate)
                        row.append()
                         
                     elif(self.timestamps[j]==self.filt_list[0][k][1]):
                        row=table.row 
                        row['exchange'] = 'NASDAQ'
                        row['symbol'] = self.filt_list[0][k][0]
                        row['adj_open'] = float(self.filt_list[0][k][2]) 
                        row['adj_close'] = float(self.filt_list[0][k][5])
                        row['adj_high'] = float(self.filt_list[0][k][3])
                        row['adj_low'] = float(self.filt_list[0][k][4])
                        row['close'] = float(self.filt_list[0][k][7])
                        row['volume'] = int(self.filt_list[0][k][6])
                        parseddate = time.strptime(self.timestamps[j],'%Y%m%d')
#                        row['date'] = self.timestamps[j]
                        row['timestamp'] = time.mktime(parseddate)
                        row.append()

                        k=k+1 
                     else:
                         print"###############Something has gone wrong. A stock had a timestamp which was not in the timestamp list..."
                         print "TS: " + str(self.timestamps[j]) + ", Stock: " + str (self.filt_list[0][k][1]) 
                         k=k+1
                         #should stop executing here? Naah
#                         sys.exit()
            
                    else:
                        row=table.row 
                        row['exchange'] = 'NYSE'
                        row['symbol'] = stocks_list[i] #self.filt_list[0][len(self.filt_list[0])-1][0] ####NOTE. POSSIBLE BUG?
                        row['adj_open'] = np.NaN 
                        row['adj_close'] = np.NaN
                        row['adj_high'] = np.NaN
                        row['adj_low'] = np.NaN
                        row['close'] = np.NaN
                        row['volume'] = np.NaN
                        parseddate = time.strptime(self.timestamps[j],'%Y%m%d')
#                        row['date'] = self.timestamps[j]
                        row['timestamp'] = time.mktime(parseddate)
#                        row['interval'] = 86400
                        row.append()
            
                #for j in range(len(self.timestamps)) ends
                table.flush()
                h5f.close()   
            #for i in range(0, stocks.size) done
        
        print "Writing data done. "+ str(time.strftime("%H:%M:%S"))
        

    def makeOrUpdateTimestampsFile(self, fileName, listOflistOfStocks, listOfInputPaths, startDate, endDate):
        '''
        @bug: Formerly did not take care of DST
        @attention: fixed DST bug. No known DST problems now.
        '''
        
        DAY=86400
        
        if (os.path.exists(fileName)):
           print "Updating timestamps"
           h5f = pt.openFile(str(fileName), mode = "a")
           table=h5f.getNode('/timestamps','timestamps')
                      
           lastTSFromFile= str(time.strftime("%Y%m%d", time.gmtime(table[table.nrows-1]['timestamp'])))
           
           if (str(startDate)<= lastTSFromFile):
               startDate=str(time.strftime("%Y%m%d", time.gmtime(table[table.nrows-1]['timestamp']))) # TO FIX DST BUG
               
        else:
           print "Creating new timestamp file"
           h5f = pt.openFile(str(fileName), mode = "w")
           group = h5f.createGroup("/", 'timestamps')
           table = h5f.createTable(group, 'timestamps', TimestampsModel)
           
        print "start: " + str(startDate)+", end: "+ str(endDate)
        
        tslist=list()    
        ctr=1
        if (str(startDate) <= str(endDate)):
           listIndex=-1
           for path in listOfInputPaths:
             listIndex+=1
             for stock in listOflistOfStocks[listIndex]:
                #print str(stock)+" "+str(ctr)+"  "+str(time.strftime("%H:%M:%S")) 
                ctr+=1
                           
                f=open(str(path)+str(stock+str(".CSV")))
                j=f.readlines()
                j.pop(0) #To remove the "header" row
                f.close()
                
                filt_list_temp=filter(lambda x: (int(x.split(',')[1])> int(startDate)) ,j) # To fix DST bug
                filt_list_temp=filter(lambda x: (int(x.split(',')[1])<= int(endDate)) ,filt_list_temp)

                if not (filt_list_temp):
                    print str(stock.split('.')[0]) + " didn't exist in this period\n"
                    #ENHANCEMENT- CAN ALSO REMOVE STOCK FROM THE STOCKLIST
                    #This can not be done right now- because if a stock did not exist- but another stock did exist then NaNs have to be added to the stock that did not exist.
                else:
                     #it existed and now we need the timestamps
                     filt_list_temp=map(lambda x:(x.split(',')[1]),filt_list_temp)
                     filt_list_temp= map(lambda item:(time.mktime(time.strptime(item,'%Y%m%d'))), filt_list_temp)

                     for item in filt_list_temp:
                        try:
                          tslist.index(int(item))
                        except:
                          tslist.append(int(item))
                          
                          
                if (len(tslist)>0):
                   if (self.continueChecking(tslist, startDate, endDate)== False):
                       break   #All dates are covered..       
                
                #for stock in stocks_list done           
        
        tslist.sort() #this should all fit into memory
        
        for ts in tslist:
            row= table.row
            row['timestamp']= ts
            #print "Adding timestamp " + str (ts)
            row.append()
            #for ts in tslist ends
            
        table.flush()
        h5f.close()    
        
        #makeTimestampsFile ends

        
    def continueChecking(self, tsList, beginTS, endTS):
        '''
        @summary: This function basically checks if a day that we haven't found any trades on is a weekend. If so- we don't need to keep looking. The converter will work just fine even without this function- but it will take more time- because it will keep looking for timestamps that it is not going to find.
        @bug: There is a Daylight savings time bug here too- but it won't adversely affect anything because DST always happens over the weekends! Though if the time change happens on a weekday sometime in the distant past/future this function may break.
        '''
        
        index=1
        DAY=86400
        
        while (index < len(tsList)):
            if (int(tsList[index])- int(tsList[index -1]) > DAY):
                tempTS= tsList[index-1] + DAY
                while (tempTS< tsList[index]):
                    timeStruct= time.gmtime(tempTS)
                    if not ((timeStruct[6] == 5) or (timeStruct[6] == 6)):
                        #Keep looking
                        return True #if its not a Saturday or a Sunday then keep looking
                    tempTS+=DAY
                    #while (tempTS< tsList[index]) ends
            index+=1        
            #while ends
        #Checking from beginTS to start of list
        tempTS=time.mktime(time.strptime(str(beginTS),'%Y%m%d'))
        
        while (int(tsList[0])- int(tempTS) > DAY):
            timeStruct= time.gmtime((tempTS))
            if not ((timeStruct[6] == 5) or (timeStruct[6] == 6)):
                return True
                #if not... ends    
            tempTS+=DAY
            #while (tsList[0]- tempTS > DAY) ends
        #Checking from endTS to end of list
        tempTS=time.mktime(time.strptime(str(endTS),'%Y%m%d'))
        
        while (int(tempTS)- int(tsList[len(tsList)-1]) > DAY):
            timeStruct= time.gmtime(tempTS)
            if not ((timeStruct[6] == 5) or (timeStruct[6] == 6)):
                return True
                #if not... ends    
            tempTS+=DAY
            #while (tempTS- tsList[len(tsList)-1] > DAY) ends
        
        print "Smartly figured out that we don't need to continue"
        return False #stop looking for more timestamps because all the timestamps that can be in the list are now there..
                     #we will not get any more timestamps by looking for more..because there just aren't any left... cool huh?    
        #continueChecking ends    
        
        
    def readTimestampsFromFile(self, fileName, beginTS, endTS):
        h5f = pt.openFile(str(fileName), mode = "a")
        fileIterator= h5f.root.timestamps.timestamps
        
        tslist=[]
        for row in fileIterator.iterrows():
            temp= str(time.strftime("%Y%m%d", time.gmtime(row['timestamp'])))
            
            if (temp>= str(beginTS)) and (temp <= str(endTS)):
              tslist.append(temp)
              
            if (temp > str(endTS)):
                break
            
        h5f.close()
        
        self.timestamps=tslist
        #readTimestampsFromFile ends
     
    def keepHDFFilesInSyncWithCSV(self, listOfInputPaths, listOfOutputPaths):
        '''
        @summary: This function removes HDF files that correspond to CSV files that existed in the past- but don't exist anymore. Possibly because the stock was delisted or something like that. 
        '''
        print "Removing HDF files for which there is no corresponding CSV file"
        listOfListOfHdfFiles=self.getSymbols(listOfOutputPaths, ".h5")
        listOfListOfCsvFiles=self.getSymbols(listOfInputPaths, ".csv") #I guess this isn't really necessary, we could just reuse the stock list or something
                                                           #but let's just keep things "proper"
        ctr=-1
        for listofHDFFiles in listOfListOfHdfFiles:
              ctr+=1
              for hdfFile in listofHDFFiles:
                  try:
                    #Check if the HDF file exists...
                    listOfListOfCsvFiles[ctr].index(hdfFile)
                  except:   
                      print "Removing "+str(listOfOutputPaths[ctr]) + str(hdfFile)+".h5"
                      os.remove(str(listOfOutputPaths[ctr]) + str(hdfFile)+".h5")
                      #if ends
                  #for hdfFile in listOfListOfHdfFiles ends
              #for listofHDFFiles in listOfListOfHdfFiles ends
        
        print "Done removing HDF files (if any)"
        #keepHDFFilesInSyncWithCSV done
                

         
if __name__ == "__main__":
    '''
    @attention: The HDF file containing the timestamps should not be in any of the output paths because, if it is, then it will be deleted at the end.
    '''
    
    print "Starting..."+ str(time.strftime("%H:%M:%S"))
    
    parser = OptionParser()
    args = parser.parse_args()[1]
    endDate= args[0]
    print "End date is: " + str (endDate)
    
    
    #Date to start reading data Format: YYYYMMDD
    startDate = 19840101
    
    #Date to end reading data Format: YYYYMMDD
    #endDate = 20100831
    
    #The complete path to the file containing the list of timestamps. This should not be in the output folder because it will be removed by the keepHDFFilesInSyncWithCSV function!
    timestampsFile="C:\\generated data files\\timestamp files\\timestamps.h5"
    
    
    spd = StockPriceData()
    
    #Remember the '\\' at the end...
    listOfInputPaths= list()
    listOfInputPaths.append("C:\\Trading data text\\Stocks\\Delisted Securities\\US Recent\\")
    listOfInputPaths.append ("C:\\Trading data text\\Stocks\\US\\AMEX\\")
    listOfInputPaths.append ("C:\\Trading data text\\Stocks\\US\\Delisted Securities\\")
    listOfInputPaths.append ("C:\\Trading data text\\Stocks\\US\OTC\\")
    listOfInputPaths.append ("C:\\Trading data text\\Stocks\\US\\NASDAQ\\")
    listOfInputPaths.append ("C:\\Trading data text\\Stocks\\US\NYSE\\")
    listOfInputPaths.append ("C:\\Trading data text\\Stocks\\US\\NYSE Arca\\")
    
    listOfOutputPaths= list()
    listOfOutputPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\\Delisted_US_Recent\\")
    listOfOutputPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\\US_AMEX\\")
    listOfOutputPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\\US_Delisted\\")
    listOfOutputPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\OTC\\")
    listOfOutputPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\\US_NASDAQ\\")
    listOfOutputPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\\US_NYSE\\")
    listOfOutputPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\\US_NYSE Arca\\")
    
    #If the output paths don't exist, then create them...
    for path in listOfOutputPaths:
        if not (os.access(path, os.F_OK)):
            #Path does not exist, so create it
            os.makedirs(path)
    #done making all output paths!        
    
    if (len(listOfInputPaths)!= len(listOfOutputPaths)):
        print "No. of input paths not equal to the number of output paths.. quitting"
        sys.exit("FAILURE")
    listOfListOfStocks=spd.getSymbols(listOfInputPaths, ".csv")
    
    
    if(endDate<startDate):
        print "Error: enddate earlier than startdate"
        sys.exit(0)
        
    spd.makeOrUpdateTimestampsFile(timestampsFile, listOfListOfStocks, listOfInputPaths, startDate, endDate)
    spd.readTimestampsFromFile(timestampsFile, startDate, endDate) 
    spd.getData(listOfListOfStocks, listOfInputPaths, startDate, endDate, listOfOutputPaths)
    spd.keepHDFFilesInSyncWithCSV(listOfInputPaths, listOfOutputPaths)
    
    print "All Done. Conversion from CSV to HDF5 is complete."

########NEW FILE########
__FILENAME__ = norgate_csv_to_pkl
#!/usr/bin/env python
'''
Created on Feb 25, 2011

@note: This assumes that all the CSV files will have exactly 7 columns. It ignores the first row in the csv files because it it the heading.
        All the data is converted from csv to pkl
@author: Shreyas Joshi
@contact: shreyasj@gatech.edu
@summary: This is used to convert CSV files from norgate to pkl files- which are used when the simulator runs.
'''

import numpy as np
import pickle as pkl
import qstkutil.utils as utils
import os
import dircache
import time
import sys

def main ():
    
    print "Starting..."+ str(time.strftime("%H:%M:%S"))
    
    try:
        rootdir = os.environ['QSDATA']
    except KeyError:
        #rootdir = "/hzr71/research/QSData"
        print "Please be sure to set the value for QSDATA in config.sh or local.sh\n"    
    
    fileExtensionToRemove = ".csv"
    
    listOfInputPaths= list()

    
#For Gekko
    rootindir = rootdir + "/Raw/Norgate/Stocks/CSV/US"
    listOfInputPaths.append (rootindir + "/AMEX/")
    listOfInputPaths.append (rootindir + "/Delisted Securities/")
    listOfInputPaths.append (rootindir + "/NASDAQ/")
    listOfInputPaths.append (rootindir + "/NYSE/")
    listOfInputPaths.append (rootindir + "/NYSE Arca/")
    listOfInputPaths.append (rootindir + "/OTC/")
    listOfInputPaths.append (rootindir + "/Indices/ADRs/")
    listOfInputPaths.append (rootindir + "/Indices/AMEX/")
    listOfInputPaths.append (rootindir + "/Indices/CBOE/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones Americas/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones Averages/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones Broad Market/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones Misc/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones Style/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones US Industries/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones US Sectors/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones US Subsectors/")
    listOfInputPaths.append (rootindir + "/Indices/Dow Jones US Supersectors/")
    listOfInputPaths.append (rootindir + "/Indices/ISE/")
    listOfInputPaths.append (rootindir + "/Indices/Merrill Lynch/")
    listOfInputPaths.append (rootindir + "/Indices/Misc/")
    listOfInputPaths.append (rootindir + "/Indices/Morgan Stanley/")
    listOfInputPaths.append (rootindir + "/Indices/NASDAQ/")
    listOfInputPaths.append (rootindir + "/Indices/NYSE/")
    listOfInputPaths.append (rootindir + "/Indices/NYSE Arca/")
    listOfInputPaths.append (rootindir + "/Indices/PHLX/")
    listOfInputPaths.append (rootindir + "/Indices/Russell/")
    listOfInputPaths.append (rootindir + "/Indices/S&P/")
    listOfInputPaths.append (rootindir + "/Indices/S&P 500 Industries/")
    listOfInputPaths.append (rootindir + "/Indices/S&P 500 Industry Groups/")
    listOfInputPaths.append (rootindir + "/Indices/S&P 500 Sectors/")
    listOfInputPaths.append (rootindir + "/Indices/S&P 500 Sub-Industries/")
    listOfInputPaths.append (rootindir + "/Indices/S&P Technology/")
    listOfInputPaths.append (rootindir + "/Indices/S&P TSX/")
    listOfInputPaths.append (rootindir + "/Indices/Wilshire/")

    rootoutdir = rootdir + "/Processed/Norgate/Stocks/US"
    listOfOutputPaths= list()
    listOfOutputPaths.append(rootoutdir + "/AMEX/")
    listOfOutputPaths.append(rootoutdir + "/Delisted Securities/")
    listOfOutputPaths.append(rootoutdir + "/NASDAQ/")
    listOfOutputPaths.append(rootoutdir + "/NYSE/")
    listOfOutputPaths.append(rootoutdir + "/NYSE Arca/")
    listOfOutputPaths.append(rootoutdir + "/OTC/")    
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    listOfOutputPaths.append(rootoutdir + "/Indices/")
    
    #If the output paths don't exist, then create them...
    for path in listOfOutputPaths:
        if not (os.access(path, os.F_OK)):
            #Path does not exist, so create it
            os.makedirs(path)
    #done making all output paths!
    
    #In case there are already some files there- remove them. This will remove all the pkl fils from the previous run
    utils.clean_paths (listOfOutputPaths)
    
    if (len(listOfInputPaths)!= len(listOfOutputPaths)):
        print "No. of input paths not equal to the number of output paths.. quitting"
        sys.exit("FAILURE")
        #if ends
    
    path_ctr = -1;
    use_cols = range (1, 7 + 1) # will now use cols 1 to 7
    for path in listOfInputPaths:
        path_ctr =  path_ctr + 1;
        stocks_at_this_path = dircache.listdir(str(path))
        #Next, throw away everything that is not a .csv And these are our stocks! Example: this should throw away the '$' folder in the NYSE folder
        filtered_names= filter (lambda x:(str(x).find(str(fileExtensionToRemove)) > -1), stocks_at_this_path)
        #Now, we remove the .csv to get the name of the stock
        filtered_names = map(lambda x:(x.partition(str(fileExtensionToRemove))[0]),filtered_names)
        stock_ctr = -1
        for stock in filtered_names:
            stock_ctr = stock_ctr + 1
            print "csv_to_pkl: processing: " + str (path + stock)
            #read in the stock date from the CSV file
            stock_data= np.loadtxt (path + stock+".csv", np.float, None, ",", None, 1, use_cols)
            stock_data_shape = stock_data.shape
            #print "stock_data_shape is: " + str(stock_data_shape)
            f = open (listOfOutputPaths[path_ctr] + filtered_names[stock_ctr] + ".pkl", "wb" )
            pkl.dump (stock_data, f, -1)
            f.close()
        #for stock in stocks_at_this_path ends
    #for path in listOfInputPaths ends
    print "Finished..."+ str(time.strftime("%H:%M:%S"))
    
    #main ends
    
    
if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = yahoo_csv_to_pkl
'''
Created on Feb 25, 2011

@note: This assumes that all the CSV files will have exactly 7 columns. It ignores the first row in the csv files because it it the heading.
        All the data is converted from csv to pkl
@author: Shreyas Joshi
@contact: shreyasj@gatech.edu
@summary: This is used to convert CSV files from norgate to pkl files- which are used when the simulator runs.
'''

import numpy as np
import pickle as pkl
import qstkutil.utils as utils
import os
import dircache
import time
import sys


def main ():
    
    print "Starting..."+ str(time.strftime("%H:%M:%S"))
    
    try:
        rootdir = os.environ['QSDATA']
    except KeyError:
        #rootdir = "/hzr71/research/QSData"
        print "Please be sure to set the value for QSDATA in config.sh or local.sh\n"    
    
    fileExtensionToRemove = ".csv"
    
    listOfInputPaths= list()

    
#For Gekko
    #listOfInputPaths.append("/hzr71/research/QSData/Processed/Norgate/raw/Delisted Securities/US Recent/")
    listOfInputPaths.append (rootdir + "/Raw/Yahoo/US/AMEX/")
    listOfInputPaths.append (rootdir + "/Raw/Yahoo/US/NASDAQ/")
    listOfInputPaths.append (rootdir + "/Raw/Yahoo/US/NYSE/")
    
    
    
    listOfOutputPaths= list()
#    listOfOutputPaths.append("C:\\test\\temp\\pkl1\\")
#    listOfOutputPaths.append("C:\\test\\temp\\pkl2\\")    
    
    #listOfOutputPaths.append(rootdir + "/Norgate/Delisted Securities/US Recent/")
    listOfOutputPaths.append(rootdir + "/Processed/Yahoo/US/AMEX/")
    listOfOutputPaths.append(rootdir + "/Processed/Yahoo/US/NASDAQ/")
    listOfOutputPaths.append(rootdir + "/Processed/Yahoo/US/NYSE/")    
    
    #If the output paths don't exist, then create them...
    for path in listOfOutputPaths:
        if not (os.access(path, os.F_OK)):
            #Path does not exist, so create it
            os.makedirs(path) #Makes paths recursively
    #done making all output paths!
    
    #In case there are already some files there- remove them. This will remove all the pkl fils from the previous run
    utils.clean_paths (listOfOutputPaths)
    
    
    if (len(listOfInputPaths)!= len(listOfOutputPaths)):
        print "No. of input paths not equal to the number of output paths.. quitting"
        sys.exit("FAILURE")
        #if ends
    
    path_ctr = -1;
    use_cols = range (1, 7 + 1) # will now use cols 1 to 7
    for path in listOfInputPaths:
        path_ctr =  path_ctr + 1;
        stocks_at_this_path = dircache.listdir(str(path))
        #Next, throw away everything that is not a .csv And these are our stocks! Example: this should throw away the '$' folder in the NYSE folder
        filtered_names= filter (lambda x:(str(x).find(str(fileExtensionToRemove)) > -1), stocks_at_this_path)
        #Now, we remove the .csv to get the name of the stock
        filtered_names = map(lambda x:(x.partition(str(fileExtensionToRemove))[0]),filtered_names)
        stock_ctr = -1
        for stock in filtered_names:
            stock_ctr = stock_ctr + 1
            print "Reading file: " + str (path + stock)
            #read in the stock date from the CSV file
            stock_data= np.loadtxt (path + stock+".csv", np.float, None, ",", None, 1, use_cols)
            
            stock_data_shape = stock_data.shape
            #print "stock_data_shape is: " + str(stock_data_shape)
        
            
#            for i in range (0, stock_data_shape[0]):
#                print stock_data [i,: ]
            
#            print "Reading: " + str(stock)
            f = open (listOfOutputPaths[path_ctr] + filtered_names[stock_ctr] + ".pkl", "wb" )
            pkl.dump (stock_data, f, -1)
            f.close()
        #for stock in stocks_at_this_path ends
    #for path in listOfInputPaths ends
    print "Finished..."+ str(time.strftime("%H:%M:%S"))
    
    #main ends
    
    
if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = yahoo_data_getter
'''
Created on Apr 8, 2011

@author: sjoshi42
@summary: This module get stock data from yahoo finance
'''

import qstkutil.DataAccess as da
import qstkutil.utils as utils
import urllib2
import urllib
import datetime
import sys
import os

def adjust (_str):
    _list= list()
#    print str ("Before" + _str)
#    symbol= _str.partition (",")[0]
#    rest= _str.partition (",")[2] #Everything but symbol
    
    _date= _str.partition(",")[0]
    rest= _str.partition(",")[2] #Everything but symbol and date
#    print str ("DT: " + _date)

    _list.append(rest.partition(",")[0]) #open
    rest= rest.partition (",")[2] #Removed open
#    print str ("OP: " + _list[-1])
    
    _list.append(rest.partition(",")[0]) #high
    rest= rest.partition (",")[2] #Removed high
#    print str ("HI: " + _list[-1])
    
    _list.append (rest.partition(",")[0]) #low
    rest= rest.partition(",")[2] #Removed low
#    print str ("LO: " + _list[-1])
    
    _list.append (rest.partition(",")[0]) #close
    rest= rest.partition (",")[2] #Removed close
#    print str ("CL: " + _list[-1])
    
    
    vol= rest.partition(",")[0] #volume
#    print str ("VOL: " + vol)
    
    adj_close= float (rest.partition (",")[2]) #Removed volume, and what we have left is adjusted close..
#    print str ("ADJ_CL: " + str(adj_close))
    
    _list = map (lambda x: (float(x)), _list) #convert strings to floats
    
    try:
        ratio= adj_close/ _list[-1] #Adjusted close / close
        _list= map (lambda x: (x * ratio), _list) #Updating the values
        _list= map (lambda x: (str(x)), _list)
        _list_concat= ",".join(_list) #Concat all the elements in the list using `,' as the separator
        _str= _date + "," + _list_concat + "," + vol + "," + str (adj_close) + "\n"
    except ZeroDivisionError:
        print ("Warning: Close value is Zero.")
        
    
    return _str
    #adjust ends 

def main():
    #Getting path
    path= os.environ['QSDATA']
    
    get_data_for_exchange("NASDAQ", path)
    get_data_for_exchange("NYSE", path)
    get_data_for_exchange("AMEX", path)
    
    #main ends
def get_data_for_exchange (exchange, data_path):
    
    #data_access= da.DataAccess('norgate')
    #symbol_list= data_access.get_all_symbols()
    data_path= data_path + "/Raw/Yahoo/US/" + str (exchange) + "/"
    
    #Create path if it doesn't exist
    if not (os.access(data_path, os.F_OK)):
        os.makedirs(data_path)
        
    utils.clean_paths(data_path)    
    
    symbol_list= list()
    
    print "Getting list of stocks.."
    
    try:
        nasdaq_params= urllib.urlencode ({'exchange':str(exchange), 'render':'download'})
        nasdaq_get= urllib2.urlopen ('http://www.nasdaq.com/screening/companies-by-name.aspx?%s' % nasdaq_params)
        symbol_list.append (nasdaq_get.readline()) #Now we have all the data in a list- but we need only the symbols so we remove the rest
        while (len (symbol_list[-1]) > 0):
            symbol_list.append (nasdaq_get.readline())
            #while ends
        symbol_list.pop(0) #This is just the word "symbol" and not a symbol itself    
        symbol_list.pop(-1) #Remove the last element because its going to be blank anyway    
        #symbol_list = map(lambda x:(x.partition(str(","))[0]),symbol_list) #Get the stuff before the first comma- which is the symbol
        
        #Unfortunately this symbol is in quotes. So we have to remove them now
        symbol_list = map(lambda x:(x.partition(str("\""))[2]),symbol_list) #Keep the stuff only after the first "
        symbol_list = map(lambda x:(x.partition(str("\""))[0]),symbol_list) #Keep the stuff before the second "
        
    except urllib2.HTTPError:
        print "Unable to get list of stocks from server. Please check your internet connection and retry."
    except:
        print"Unknown error occoured when getting list of stocks from server."
    
    print "Got " + str (len(symbol_list)) + " symbols. Now getting symbol data..."
    
    _now =datetime.datetime.now();
    miss_ctr=0; #Counts how many symbols we could get
    for symbol in symbol_list:
        symbol_data=list()
        print "Getting " + str (symbol)
        
        try:
            params= urllib.urlencode ({'a':03, 'b':12, 'c':2000, 'd':_now.month, 'e':_now.day, 'f':_now.year, 's': str(symbol)})
            url_get= urllib2.urlopen("http://ichart.finance.yahoo.com/table.csv?%s" % params)
            
            header= url_get.readline()
            symbol_data.append (url_get.readline())
            while (len(symbol_data[-1]) > 0):
                symbol_data.append(url_get.readline())
#                print str(symbol_data[-1])
            
            symbol_data.pop(-1) #The last element is going to be the string of length zero. We don't want to write that to file.

            #To change adjusted close so that Yahoo data is same as Norgate data
            symbol_data= map (adjust, symbol_data)
            
            #Following changes so that the data looks like Norgate data and the change to cav_to_pkl.csv is minimized
            symbol_data = map(lambda x:(x.replace("-", "")),symbol_data) 
            symbol_data = map(lambda x:(str(symbol) + "," + str(x)) ,symbol_data) #This means that the header is wrong but since it is ignored later anyways- this will work
            
            #now writing data to file
            f= open (data_path + symbol + ".csv", 'w')
            
            #Writing the header
            f.write (header)
            
            while (len(symbol_data) > 0):
                f.write (symbol_data.pop())
             
            f.close();    
#            print url_get.readline()
#            f= open (data_path + symbol + ".csv", 'w')
#            f.write (url_get.read())
#            f.close()
                        
        except urllib2.HTTPError:
            miss_ctr= miss_ctr+1
            print "Unable to fetch data for stock: " + str (symbol)
        except urllib2.URLError:
            print "URL Error for stock: " + str (symbol)
            
#        except:
#            print "Some error occurred"
            #except ends 
            
        
        #for ends
    print "All done. Got " + str (len(symbol_list) - miss_ctr) + " stocks. Could not get " + str (miss_ctr) + " stocks."   
    #main ends


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = apidoc
# epydoc -- API Documentation Classes
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: apidoc.py 1675 2008-01-29 17:12:56Z edloper $

"""
Classes for encoding API documentation about Python programs.
These classes are used as a common representation for combining
information derived from introspection and from parsing.

The API documentation for a Python program is encoded using a graph of
L{APIDoc} objects, each of which encodes information about a single
Python variable or value.  C{APIDoc} has two direct subclasses:
L{VariableDoc}, for documenting variables; and L{ValueDoc}, for
documenting values.  The C{ValueDoc} class is subclassed further, to
define the different pieces of information that should be recorded
about each value type:

G{classtree: APIDoc}

The distinction between variables and values is intentionally made
explicit.  This allows us to distinguish information about a variable
itself (such as whether it should be considered 'public' in its
containing namespace) from information about the value it contains
(such as what type the value has).  This distinction is also important
because several variables can contain the same value: each variable
should be described by a separate C{VariableDoc}; but we only need one
C{ValueDoc}, since they share a single value.

@todo: Add a cache to canonical name lookup?
"""
__docformat__ = 'epytext en'

######################################################################
## Imports
######################################################################

import types, re, os.path, pickle
from epydoc import log
import epydoc
import __builtin__
from epydoc.compat import * # Backwards compatibility
from epydoc.util import decode_with_backslashreplace, py_src_filename
import epydoc.markup.pyval_repr

######################################################################
# Dotted Names
######################################################################

class DottedName:
    """
    A sequence of identifiers, separated by periods, used to name a
    Python variable, value, or argument.  The identifiers that make up
    a dotted name can be accessed using the indexing operator:

        >>> name = DottedName('epydoc', 'api_doc', 'DottedName')
        >>> print name
        epydoc.apidoc.DottedName
        >>> name[1]
        'api_doc'
    """
    UNREACHABLE = "??"
    _IDENTIFIER_RE = re.compile("""(?x)
        (%s |             # UNREACHABLE marker, or..
         (script-)?       #   Prefix: script (not a module)
         \w+              #   Identifier (yes, identifiers starting with a
                          #   digit are allowed. See SF bug #1649347)
         '?)              #   Suffix: submodule that is shadowed by a var
        (-\d+)?           # Suffix: unreachable vals with the same name
        $"""
        % re.escape(UNREACHABLE))

    class InvalidDottedName(ValueError):
        """
        An exception raised by the DottedName constructor when one of
        its arguments is not a valid dotted name.
        """

    _ok_identifiers = set()
    """A cache of identifier strings that have been checked against
    _IDENTIFIER_RE and found to be acceptable."""
    
    def __init__(self, *pieces, **options):
        """
        Construct a new dotted name from the given sequence of pieces,
        each of which can be either a C{string} or a C{DottedName}.
        Each piece is divided into a sequence of identifiers, and
        these sequences are combined together (in order) to form the
        identifier sequence for the new C{DottedName}.  If a piece
        contains a string, then it is divided into substrings by
        splitting on periods, and each substring is checked to see if
        it is a valid identifier.

        As an optimization, C{pieces} may also contain a single tuple
        of values.  In that case, that tuple will be used as the
        C{DottedName}'s identifiers; it will I{not} be checked to
        see if it's valid.

        @kwparam strict: if true, then raise an L{InvalidDottedName}
        if the given name is invalid.
        """
        if len(pieces) == 1 and isinstance(pieces[0], tuple):
            self._identifiers = pieces[0] # Optimization
            return
        if len(pieces) == 0:
            raise DottedName.InvalidDottedName('Empty DottedName')
        self._identifiers = []
        for piece in pieces:
            if isinstance(piece, DottedName):
                self._identifiers += piece._identifiers
            elif isinstance(piece, basestring):
                for subpiece in piece.split('.'):
                    if piece not in self._ok_identifiers:
                        if not self._IDENTIFIER_RE.match(subpiece):
                            if options.get('strict'):
                                raise DottedName.InvalidDottedName(
                                    'Bad identifier %r' % (piece,))
                            else:
                                log.warning("Identifier %r looks suspicious; "
                                            "using it anyway." % piece)
                        self._ok_identifiers.add(piece)
                    self._identifiers.append(subpiece)
            else:
                raise TypeError('Bad identifier %r: expected '
                                'DottedName or str' % (piece,))
        self._identifiers = tuple(self._identifiers)

    def __repr__(self):
        idents = [`ident` for ident in self._identifiers]
        return 'DottedName(' + ', '.join(idents) + ')'

    def __str__(self):
        """
        Return the dotted name as a string formed by joining its
        identifiers with periods:

            >>> print DottedName('epydoc', 'api_doc', DottedName')
            epydoc.apidoc.DottedName
        """
        return '.'.join(self._identifiers)

    def __add__(self, other):
        """
        Return a new C{DottedName} whose identifier sequence is formed
        by adding C{other}'s identifier sequence to C{self}'s.
        """
        if isinstance(other, (basestring, DottedName)):
            return DottedName(self, other)
        else:
            return DottedName(self, *other)

    def __radd__(self, other):
        """
        Return a new C{DottedName} whose identifier sequence is formed
        by adding C{self}'s identifier sequence to C{other}'s.
        """
        if isinstance(other, (basestring, DottedName)):
            return DottedName(other, self)
        else:
            return DottedName(*(list(other)+[self]))

    def __getitem__(self, i):
        """
        Return the C{i}th identifier in this C{DottedName}.  If C{i} is
        a non-empty slice, then return a C{DottedName} built from the
        identifiers selected by the slice.  If C{i} is an empty slice,
        return an empty list (since empty C{DottedName}s are not valid).
        """
        if isinstance(i, types.SliceType):
            pieces = self._identifiers[i.start:i.stop]
            if pieces: return DottedName(pieces)
            else: return []
        else:
            return self._identifiers[i]

    def __hash__(self):
        return hash(self._identifiers)

    def __cmp__(self, other):
        """
        Compare this dotted name to C{other}.  Two dotted names are
        considered equal if their identifier subsequences are equal.
        Ordering between dotted names is lexicographic, in order of
        identifier from left to right.
        """
        if not isinstance(other, DottedName):
            return -1
        return cmp(self._identifiers, other._identifiers)

    def __len__(self):
        """
        Return the number of identifiers in this dotted name.
        """
        return len(self._identifiers)

    def container(self):
        """
        Return the DottedName formed by removing the last identifier
        from this dotted name's identifier sequence.  If this dotted
        name only has one name in its identifier sequence, return
        C{None} instead.
        """
        if len(self._identifiers) == 1:
            return None
        else:
            return DottedName(*self._identifiers[:-1])

    def dominates(self, name, strict=False):
        """
        Return true if this dotted name is equal to a prefix of
        C{name}.  If C{strict} is true, then also require that
        C{self!=name}.

            >>> DottedName('a.b').dominates(DottedName('a.b.c.d'))
            True
        """
        len_self = len(self._identifiers)
        len_name = len(name._identifiers)

        if (len_self > len_name) or (strict and len_self == len_name):
            return False
        # The following is redundant (the first clause is implied by
        # the second), but is done as an optimization.
        return ((self._identifiers[0] == name._identifiers[0]) and
                self._identifiers == name._identifiers[:len_self])

    def contextualize(self, context):
        """
        If C{self} and C{context} share a common ancestor, then return
        a name for C{self}, relative to that ancestor.  If they do not
        share a common ancestor (or if C{context} is C{UNKNOWN}), then
        simply return C{self}.

        This is used to generate shorter versions of dotted names in
        cases where users can infer the intended target from the
        context.
        
        @type context: L{DottedName}
        @rtype: L{DottedName}
        """
        if context is UNKNOWN or not context or len(self) <= 1:
            return self
        if self[0] == context[0]:
            return self[1:].contextualize(context[1:])
        else:
            return self

        # Find the first index where self & context differ.
        for i in range(min(len(context), len(self))):
            if self._identifiers[i] != context._identifiers[i]:
                first_difference = i
                break
        else:
            first_difference = i+1
            
        # Strip off anything before that index.
        if first_difference == 0:
            return self
        elif first_difference == len(self):
            return self[-1:]
        else:
            return self[first_difference:]

######################################################################
# UNKNOWN Value
######################################################################

class _Sentinel:
    """
    A unique value that won't compare equal to any other value.  This
    class is used to create L{UNKNOWN}.
    """
    def __init__(self, name):
        self.name = name
    def __repr__(self):
        return '<%s>' % self.name
    def __nonzero__(self):
        raise ValueError('Sentinel value <%s> can not be used as a boolean' %
                         self.name)

UNKNOWN = _Sentinel('UNKNOWN')
"""A special value used to indicate that a given piece of
information about an object is unknown.  This is used as the
default value for all instance variables."""

######################################################################
# API Documentation Objects: Abstract Base Classes
######################################################################

class APIDoc(object):
    """
    API documentation information for a single element of a Python
    program.  C{APIDoc} itself is an abstract base class; subclasses
    are used to specify what information should be recorded about each
    type of program element.  In particular, C{APIDoc} has two direct
    subclasses, C{VariableDoc} for documenting variables and
    C{ValueDoc} for documenting values; and the C{ValueDoc} class is
    subclassed further for different value types.

    Each C{APIDoc} subclass specifies the set of attributes that
    should be used to record information about the corresponding
    program element type.  The default value for each attribute is
    stored in the class; these default values can then be overridden
    with instance variables.  Most attributes use the special value
    L{UNKNOWN} as their default value, to indicate that the correct
    value for that attribute has not yet been determined.  This makes
    it easier to merge two C{APIDoc} objects that are documenting the
    same element (in particular, to merge information about an element
    that was derived from parsing with information that was derived
    from introspection).

    For all attributes with boolean values, use only the constants
    C{True} and C{False} to designate true and false.  In particular,
    do I{not} use other values that evaluate as true or false, such as
    C{2} or C{()}.  This restriction makes it easier to handle
    C{UNKNOWN} values.  For example, to test if a boolean attribute is
    C{True} or C{UNKNOWN}, use 'C{attrib in (True, UNKNOWN)}' or
    'C{attrib is not False}'.

    Two C{APIDoc} objects describing the same object can be X{merged},
    using the method L{merge_and_overwrite(other)}.  After two
    C{APIDoc}s are merged, any changes to one will be reflected in the
    other.  This is accomplished by setting the two C{APIDoc} objects
    to use a shared instance dictionary.  See the documentation for
    L{merge_and_overwrite} for more information, and some important
    caveats about hashing.
    """
    #{ Docstrings
    docstring = UNKNOWN
    """@ivar: The documented item's docstring.
       @type: C{string} or C{None}"""
    
    docstring_lineno = UNKNOWN
    """@ivar: The line number on which the documented item's docstring
       begins.
       @type: C{int}"""
    #} end of "docstrings" group

    #{ Information Extracted from Docstrings
    descr = UNKNOWN
    """@ivar: A description of the documented item, extracted from its
       docstring.
       @type: L{ParsedDocstring<epydoc.markup.ParsedDocstring>}"""
    
    summary = UNKNOWN
    """@ivar: A summary description of the documented item, extracted from
       its docstring.
       @type: L{ParsedDocstring<epydoc.markup.ParsedDocstring>}"""
    
    other_docs = UNKNOWN
    """@ivar: A flag indicating if the entire L{docstring} body (except tags
       if any) is entirely included in the L{summary}.
       @type: C{bool}"""
    
    metadata = UNKNOWN
    """@ivar: Metadata about the documented item, extracted from fields in
       its docstring.  I{Currently} this is encoded as a list of tuples
       C{(field, arg, descr)}.  But that may change.
       @type: C{(str, str, L{ParsedDocstring<markup.ParsedDocstring>})}"""
    
    extra_docstring_fields = UNKNOWN
    """@ivar: A list of new docstring fields tags that are defined by the
       documented item's docstring.  These new field tags can be used by
       this item or by any item it contains.
       @type: L{DocstringField <epydoc.docstringparser.DocstringField>}"""
    #} end of "information extracted from docstrings" group

    #{ Source Information
    docs_extracted_by = UNKNOWN # 'parser' or 'introspecter' or 'both'
    """@ivar: Information about where the information contained by this
       C{APIDoc} came from.  Can be one of C{'parser'},
       C{'introspector'}, or C{'both'}.
       @type: C{str}"""
    #} end of "source information" group

    def __init__(self, **kwargs):
        """
        Construct a new C{APIDoc} object.  Keyword arguments may be
        used to initialize the new C{APIDoc}'s attributes.
        
        @raise TypeError: If a keyword argument is specified that does
            not correspond to a valid attribute for this (sub)class of
            C{APIDoc}.
        """
        if epydoc.DEBUG:
            for key in kwargs:
                if key[0] != '_' and not hasattr(self.__class__, key):
                    raise TypeError('%s got unexpected arg %r' %
                                    (self.__class__.__name__, key))
        self.__dict__.update(kwargs)

    def _debug_setattr(self, attr, val):
        """
        Modify an C{APIDoc}'s attribute.  This is used when
        L{epydoc.DEBUG} is true, to make sure we don't accidentally
        set any inappropriate attributes on C{APIDoc} objects.

        @raise AttributeError: If C{attr} is not a valid attribute for
            this (sub)class of C{APIDoc}.  (C{attr} is considered a
            valid attribute iff C{self.__class__} defines an attribute
            with that name.)
        """
        # Don't intercept special assignments like __class__, or
        # assignments to private variables.
        if attr.startswith('_'):
            return object.__setattr__(self, attr, val)
        if not hasattr(self, attr):
            raise AttributeError('%s does not define attribute %r' %
                            (self.__class__.__name__, attr))
        self.__dict__[attr] = val

    if epydoc.DEBUG:
        __setattr__ = _debug_setattr

    def __repr__(self):
       return '<%s>' % self.__class__.__name__
    
    def pp(self, doublespace=0, depth=5, exclude=(), include=()):
        """
        Return a pretty-printed string representation for the
        information contained in this C{APIDoc}.
        """
        return pp_apidoc(self, doublespace, depth, exclude, include)
    __str__ = pp

    def specialize_to(self, cls):
        """
        Change C{self}'s class to C{cls}.  C{cls} must be a subclass
        of C{self}'s current class.  For example, if a generic
        C{ValueDoc} was created for a value, and it is determined that
        the value is a routine, you can update its class with:
        
            >>> valdoc.specialize_to(RoutineDoc)
        """
        if not issubclass(cls, self.__class__):
            raise ValueError('Can not specialize to %r' % cls)
        # Update the class.
        self.__class__ = cls
        # Update the class of any other apidoc's in the mergeset.
        if self.__mergeset is not None:
            for apidoc in self.__mergeset:
                apidoc.__class__ = cls
        # Re-initialize self, in case the subclass constructor does
        # any special processing on its arguments.
        self.__init__(**self.__dict__)

    __has_been_hashed = False
    """True iff L{self.__hash__()} has ever been called."""
    
    def __hash__(self):
        self.__has_been_hashed = True
        return id(self.__dict__)

    def __cmp__(self, other):
        if not isinstance(other, APIDoc): return -1
        if self.__dict__ is other.__dict__: return 0
        name_cmp = cmp(self.canonical_name, other.canonical_name)
        if name_cmp == 0: return -1
        else: return name_cmp

    def is_detailed(self):
        """
        Does this object deserve a box with extra details?

        @return: True if the object needs extra details, else False.
        @rtype: C{bool}
        """
        if self.other_docs is True:
            return True

        if self.metadata is not UNKNOWN:
            return bool(self.metadata)

    __mergeset = None
    """The set of all C{APIDoc} objects that have been merged with
    this C{APIDoc} (using L{merge_and_overwrite()}).  Each C{APIDoc}
    in this set shares a common instance dictionary (C{__dict__})."""
    
    def merge_and_overwrite(self, other, ignore_hash_conflict=False):
        """
        Combine C{self} and C{other} into a X{merged object}, such
        that any changes made to one will affect the other.  Any
        attributes that C{other} had before merging will be discarded.
        This is accomplished by copying C{self.__dict__} over
        C{other.__dict__} and C{self.__class__} over C{other.__class__}.

        Care must be taken with this method, since it modifies the
        hash value of C{other}.  To help avoid the problems that this
        can cause, C{merge_and_overwrite} will raise an exception if
        C{other} has ever been hashed, unless C{ignore_hash_conflict}
        is True.  Note that adding C{other} to a dictionary, set, or
        similar data structure will implicitly cause it to be hashed.
        If you do set C{ignore_hash_conflict} to True, then any
        existing data structures that rely on C{other}'s hash staying
        constant may become corrupted.

        @return: C{self}
        @raise ValueError: If C{other} has ever been hashed.
        """
        # If we're already merged, then there's nothing to do.
        if (self.__dict__ is other.__dict__ and
            self.__class__ is other.__class__): return self
            
        if other.__has_been_hashed and not ignore_hash_conflict:
            raise ValueError("%r has already been hashed!  Merging it "
                             "would cause its has value to change." % other)

        # If other was itself already merged with anything,
        # then we need to merge those too.
        a,b = (self.__mergeset, other.__mergeset)
        mergeset = (self.__mergeset or [self]) + (other.__mergeset or [other])
        other.__dict__.clear()
        for apidoc in mergeset:
            #if apidoc is self: pass
            apidoc.__class__ = self.__class__
            apidoc.__dict__ = self.__dict__
        self.__mergeset = mergeset
        # Sanity chacks.
        assert self in mergeset and other in mergeset
        for apidoc in mergeset:
            assert apidoc.__dict__ is self.__dict__
        # Return self.
        return self

    def apidoc_links(self, **filters):
        """
        Return a list of all C{APIDoc}s that are directly linked from
        this C{APIDoc} (i.e., are contained or pointed to by one or
        more of this C{APIDoc}'s attributes.)

        Keyword argument C{filters} can be used to selectively exclude
        certain categories of attribute value.  For example, using
        C{includes=False} will exclude variables that were imported
        from other modules; and C{subclasses=False} will exclude
        subclasses.  The filter categories currently supported by
        epydoc are:
          - C{imports}: Imported variables.
          - C{packages}: Containing packages for modules.
          - C{submodules}: Contained submodules for packages.
          - C{bases}: Bases for classes.
          - C{subclasses}: Subclasses for classes.
          - C{variables}: All variables.
          - C{private}: Private variables.
          - C{overrides}: Points from class variables to the variables
            they override.  This filter is False by default.
        """
        return []

def reachable_valdocs(root, **filters):
    """
    Return a list of all C{ValueDoc}s that can be reached, directly or
    indirectly from the given root list of C{ValueDoc}s.

    @param filters: A set of filters that can be used to prevent
        C{reachable_valdocs} from following specific link types when
        looking for C{ValueDoc}s that can be reached from the root
        set.  See C{APIDoc.apidoc_links} for a more complete
        description.
    """
    apidoc_queue = list(root)
    val_set = set()
    var_set = set()
    while apidoc_queue:
        api_doc = apidoc_queue.pop()
        if isinstance(api_doc, ValueDoc):
            val_set.add(api_doc)
        else:
            var_set.add(api_doc)
        apidoc_queue.extend([v for v in api_doc.apidoc_links(**filters)
                             if v not in val_set and v not in var_set])
    return val_set

######################################################################
# Variable Documentation Objects
######################################################################

class VariableDoc(APIDoc):
    """
    API documentation information about a single Python variable.

    @note: The only time a C{VariableDoc} will have its own docstring
    is if that variable was created using an assignment statement, and
    that assignment statement had a docstring-comment or was followed
    by a pseudo-docstring.
    """
    #{ Basic Variable Information
    name = UNKNOWN
    """@ivar: The name of this variable in its containing namespace.
       @type: C{str}"""
    
    container = UNKNOWN
    """@ivar: API documentation for the namespace that contains this
       variable.
       @type: L{ValueDoc}"""
    
    canonical_name = UNKNOWN
    """@ivar: A dotted name that serves as a unique identifier for
       this C{VariableDoc}.  It should be formed by concatenating
       the C{VariableDoc}'s C{container} with its C{name}.
       @type: L{DottedName}"""

    value = UNKNOWN
    """@ivar: The API documentation for this variable's value.
       @type: L{ValueDoc}"""
    #}

    #{ Information Extracted from Docstrings
    type_descr = UNKNOWN 
    """@ivar: A description of the variable's expected type, extracted from
       its docstring.
       @type: L{ParsedDocstring<epydoc.markup.ParsedDocstring>}"""
    #} end of "information extracted from docstrings" group
    
    #{ Information about Imported Variables
    imported_from = UNKNOWN
    """@ivar: The fully qualified dotted name of the variable that this
       variable's value was imported from.  This attribute should only
       be defined if C{is_instvar} is true.
       @type: L{DottedName}"""

    is_imported = UNKNOWN
    """@ivar: Was this variable's value imported from another module?
       (Exception: variables that are explicitly included in __all__ have
       C{is_imported} set to C{False}, even if they are in fact
       imported.)
       @type: C{bool}"""
    #} end of "information about imported variables" group

    #{ Information about Variables in Classes
    is_instvar = UNKNOWN
    """@ivar: If true, then this variable is an instance variable; if false,
       then this variable is a class variable.  This attribute should
       only be defined if the containing namespace is a class    
       @type: C{bool}"""
    
    overrides = UNKNOWN # [XXX] rename -- don't use a verb.
    """@ivar: The API documentation for the variable that is overridden by
       this variable.  This attribute should only be defined if the
       containing namespace is a class.
       @type: L{VariableDoc}"""
    #} end of "information about variables in classes" group

    #{ Flags
    is_alias = UNKNOWN
    """@ivar: Is this variable an alias for another variable with the same
       value?  If so, then this variable will be dispreferred when
       assigning canonical names.
       @type: C{bool}"""
    
    is_public = UNKNOWN
    """@ivar: Is this variable part of its container's public API?
       @type: C{bool}"""
    #} end of "flags" group

    def __init__(self, **kwargs):
        APIDoc.__init__(self, **kwargs)
        if self.is_public is UNKNOWN and self.name is not UNKNOWN:
            self.is_public = (not self.name.startswith('_') or
                              self.name.endswith('_'))
        
    def __repr__(self):
        if self.canonical_name is not UNKNOWN:
            return '<%s %s>' % (self.__class__.__name__, self.canonical_name)
        if self.name is not UNKNOWN:
            return '<%s %s>' % (self.__class__.__name__, self.name)
        else:                     
            return '<%s>' % self.__class__.__name__

    def _get_defining_module(self):
        if self.container is UNKNOWN:
            return UNKNOWN
        return self.container.defining_module
    defining_module = property(_get_defining_module, doc="""
    A read-only property that can be used to get the variable's
    defining module.  This is defined as the defining module
    of the variable's container.""")

    def apidoc_links(self, **filters):
        # nb: overrides filter is *False* by default.
        if (filters.get('overrides', False) and
            (self.overrides not in (None, UNKNOWN))):
            overrides = [self.overrides]
        else:
            overrides = []
        if self.value in (None, UNKNOWN):
            return []+overrides
        else:
            return [self.value]+overrides

    def is_detailed(self):
        pval = super(VariableDoc, self).is_detailed()
        if pval or self.value in (None, UNKNOWN):
            return pval

        if (self.overrides not in (None, UNKNOWN) and
            isinstance(self.value, RoutineDoc)):
            return True

        if isinstance(self.value, GenericValueDoc):
            # [XX] This is a little hackish -- we assume that the
            # summary lines will have SUMMARY_REPR_LINELEN chars,
            # that len(name) of those will be taken up by the name,
            # and that 3 of those will be taken up by " = " between
            # the name & val.  Note that if any docwriter uses a
            # different formula for maxlen for this, then it will
            # not get the right value for is_detailed().
            maxlen = self.value.SUMMARY_REPR_LINELEN-3-len(self.name)
            return (not self.value.summary_pyval_repr(maxlen).is_complete)
        else:
            return self.value.is_detailed()

######################################################################
# Value Documentation Objects
######################################################################

class ValueDoc(APIDoc):
    """
    API documentation information about a single Python value.
    """
    canonical_name = UNKNOWN
    """@ivar: A dotted name that serves as a unique identifier for
       this C{ValueDoc}'s value.  If the value can be reached using a
       single sequence of identifiers (given the appropriate imports),
       then that sequence of identifiers is used as its canonical name.
       If the value can be reached by multiple sequences of identifiers
       (i.e., if it has multiple aliases), then one of those sequences of
       identifiers is used.  If the value cannot be reached by any
       sequence of identifiers (e.g., if it was used as a base class but
       then its variable was deleted), then its canonical name will start
       with C{'??'}.  If necessary, a dash followed by a number will be
       appended to the end of a non-reachable identifier to make its
       canonical name unique.

       When possible, canonical names are chosen when new C{ValueDoc}s
       are created.  However, this is sometimes not possible.  If a
       canonical name can not be chosen when the C{ValueDoc} is created,
       then one will be assigned by L{assign_canonical_names()
       <docbuilder.assign_canonical_names>}.
       
       @type: L{DottedName}"""

    #{ Value Representation
    pyval = UNKNOWN
    """@ivar: A pointer to the actual Python object described by this
       C{ValueDoc}.  This is used to display the value (e.g., when
       describing a variable.)  Use L{pyval_repr()} to generate a
       plaintext string representation of this value.
       @type: Python object"""

    parse_repr = UNKNOWN
    """@ivar: A text representation of this value, extracted from 
       parsing its source code.  This representation may not accurately
       reflect the actual value (e.g., if the value was modified after
       the initial assignment).
       @type: C{unicode}"""

    REPR_MAXLINES = 5
    """@cvar: The maximum number of lines of text that should be
    generated by L{pyval_repr()}.  If the string representation does
    not fit in this number of lines, an ellpsis marker (...) will
    be placed at the end of the formatted representation."""

    REPR_LINELEN = 75
    """@cvar: The maximum number of characters for lines of text that
    should be generated by L{pyval_repr()}.  Any lines that exceed
    this number of characters will be line-wrappped; The S{crarr}
    symbol will be used to indicate that the line was wrapped."""

    SUMMARY_REPR_LINELEN = 75
    """@cvar: The maximum number of characters for the single-line
    text representation generated by L{summary_pyval_repr()}.  If
    the value's representation does not fit in this number of
    characters, an ellipsis marker (...) will be placed at the end
    of the formatted representation."""

    REPR_MIN_SCORE = 0
    """@cvar: The minimum score that a value representation based on
    L{pyval} should have in order to be used instead of L{parse_repr}
    as the canonical representation for this C{ValueDoc}'s value.
    @see: L{epydoc.markup.pyval_repr}"""
    #} end of "value representation" group

    #{ Context
    defining_module = UNKNOWN
    """@ivar: The documentation for the module that defines this
       value.  This is used, e.g., to lookup the appropriate markup
       language for docstrings.  For a C{ModuleDoc},
       C{defining_module} should be C{self}.
       @type: L{ModuleDoc}"""
    #} end of "context group"

    #{ Information about Imported Variables
    proxy_for = None # [xx] in progress.
    """@ivar: If C{proxy_for} is not None, then this value was
       imported from another file.  C{proxy_for} is the dotted name of
       the variable that this value was imported from.  If that
       variable is documented, then its C{value} may contain more
       complete API documentation about this value.  The C{proxy_for}
       attribute is used by the source code parser to link imported
       values to their source values (in particular, for base
       classes).  When possible, these proxy C{ValueDoc}s are replaced
       by the imported value's C{ValueDoc} by
       L{link_imports()<docbuilder.link_imports>}.
       @type: L{DottedName}"""
    #} end of "information about imported variables" group

    #: @ivar:
    #: This is currently used to extract values from __all__, etc, in
    #: the docparser module; maybe I should specialize
    #: process_assignment and extract it there?  Although, for __all__,
    #: it's not clear where I'd put the value, since I just use it to
    #: set private/public/imported attribs on other vars (that might not
    #: exist yet at the time.)
    toktree = UNKNOWN

    def __repr__(self):
        if self.canonical_name is not UNKNOWN:
            return '<%s %s>' % (self.__class__.__name__, self.canonical_name)
        else:
            return '<%s %s>' % (self.__class__.__name__,
                                self.summary_pyval_repr().to_plaintext(None))

    def __setstate__(self, state):
        self.__dict__ = state

    def __getstate__(self):
        """
        State serializer for the pickle module.  This is necessary
        because sometimes the C{pyval} attribute contains an
        un-pickleable value.
        """
        # Construct our pickled dictionary.  Maintain this dictionary
        # as a private attribute, so we can reuse it later, since
        # merged objects need to share a single dictionary.
        if not hasattr(self, '_ValueDoc__pickle_state'):
            # Make sure __pyval_repr & __summary_pyval_repr are cached:
            self.pyval_repr(), self.summary_pyval_repr()
            # Construct the dictionary; leave out 'pyval'.
            self.__pickle_state = self.__dict__.copy()
            self.__pickle_state['pyval'] = UNKNOWN

        if not isinstance(self, GenericValueDoc):
            assert self.__pickle_state != {}
        # Return the pickle state.
        return self.__pickle_state

    #{ Value Representation
    def pyval_repr(self):
        """
        Return a formatted representation of the Python object
        described by this C{ValueDoc}.  This representation may
        include data from introspection or parsing, and is authorative
        as 'the best way to represent a Python value.'  Any lines that
        go beyond L{REPR_LINELEN} characters will be wrapped; and if
        the representation as a whole takes more than L{REPR_MAXLINES}
        lines, then it will be truncated (with an ellipsis marker).
        This function will never return L{UNKNOWN} or C{None}.
    
        @rtype: L{ColorizedPyvalRepr}
        """
        # Use self.__pyval_repr to cache the result.
        if not hasattr(self, '_ValueDoc__pyval_repr'):
            self.__pyval_repr = epydoc.markup.pyval_repr.colorize_pyval(
                self.pyval, self.parse_repr, self.REPR_MIN_SCORE,
                self.REPR_LINELEN, self.REPR_MAXLINES, linebreakok=True)
        return self.__pyval_repr

    def summary_pyval_repr(self, max_len=None):
        """
        Return a single-line formatted representation of the Python
        object described by this C{ValueDoc}.  This representation may
        include data from introspection or parsing, and is authorative
        as 'the best way to summarize a Python value.'  If the
        representation takes more then L{SUMMARY_REPR_LINELEN}
        characters, then it will be truncated (with an ellipsis
        marker).  This function will never return L{UNKNOWN} or
        C{None}.
    
        @rtype: L{ColorizedPyvalRepr}
        """
        # If max_len is specified, then do *not* cache the result.
        if max_len is not None:
            return epydoc.markup.pyval_repr.colorize_pyval(
                self.pyval, self.parse_repr, self.REPR_MIN_SCORE,
                max_len, maxlines=1, linebreakok=False)
            
        # Use self.__summary_pyval_repr to cache the result.
        if not hasattr(self, '_ValueDoc__summary_pyval_repr'):
            self.__summary_pyval_repr = epydoc.markup.pyval_repr.colorize_pyval(
                self.pyval, self.parse_repr, self.REPR_MIN_SCORE,
                self.SUMMARY_REPR_LINELEN, maxlines=1, linebreakok=False)
        return self.__summary_pyval_repr
    #} end of "value representation" group

    def apidoc_links(self, **filters):
        return []

class GenericValueDoc(ValueDoc):
    """
    API documentation about a 'generic' value, i.e., one that does not
    have its own docstring or any information other than its value and
    parse representation.  C{GenericValueDoc}s do not get assigned
    cannonical names.
    """
    canonical_name = None
    
    def is_detailed(self):
        return (not self.summary_pyval_repr().is_complete)

class NamespaceDoc(ValueDoc):
    """
    API documentation information about a singe Python namespace
    value.  (I.e., a module or a class).
    """
    #{ Information about Variables
    variables = UNKNOWN
    """@ivar: The contents of the namespace, encoded as a
        dictionary mapping from identifiers to C{VariableDoc}s.  This
        dictionary contains all names defined by the namespace,
        including imported variables, aliased variables, and variables
        inherited from base classes (once L{inherit_docs()
        <epydoc.docbuilder.inherit_docs>} has added them).
       @type: C{dict} from C{string} to L{VariableDoc}"""
    sorted_variables = UNKNOWN
    """@ivar: A list of all variables defined by this
       namespace, in sorted order.  The elements of this list should
       exactly match the values of L{variables}.  The sort order for
       this list is defined as follows:
          - Any variables listed in a C{@sort} docstring field are
            listed in the order given by that field.
          - These are followed by any variables that were found while
            parsing the source code, in the order in which they were
            defined in the source file.
          - Finally, any remaining variables are listed in
            alphabetical order.
       @type: C{list} of L{VariableDoc}"""
    sort_spec = UNKNOWN
    """@ivar: The order in which variables should be listed,
       encoded as a list of names.  Any variables whose names are not
       included in this list should be listed alphabetically,
       following the variables that are included.
       @type: C{list} of C{str}"""
    group_specs = UNKNOWN
    """@ivar: The groups that are defined by this namespace's
       docstrings.  C{group_specs} is encoded as an ordered list of
       tuples C{(group_name, elt_names)}, where C{group_name} is the
        
       name of a group and C{elt_names} is a list of element names in
       that group.  (An element can be a variable or a submodule.)  A
       '*' in an element name will match any string of characters.
       @type: C{list} of C{(str,list)}"""
    variable_groups = UNKNOWN
    """@ivar: A dictionary specifying what group each
       variable belongs to.  The keys of the dictionary are group
       names, and the values are lists of C{VariableDoc}s.  The order
       that groups should be listed in should be taken from
       L{group_specs}.
       @type: C{dict} from C{str} to C{list} of L{VariableDoc}"""
    #} end of group "information about variables"

    def __init__(self, **kwargs):
        kwargs.setdefault('variables', {})
        APIDoc.__init__(self, **kwargs)
        assert self.variables is not UNKNOWN

    def is_detailed(self):
        return True

    def apidoc_links(self, **filters):
        variables = filters.get('variables', True)
        imports = filters.get('imports', True)
        private = filters.get('private', True)
        if variables and imports and private:
            return self.variables.values() # list the common case first.
        elif not variables:
            return []
        elif not imports and not private:
            return [v for v in self.variables.values() if
                    v.is_imported != True and v.is_public != False]
        elif not private:
            return [v for v in self.variables.values() if
                    v.is_public != False]
        elif not imports:
            return [v for v in self.variables.values() if
                    v.is_imported != True]
        assert 0, 'this line should be unreachable'

    def init_sorted_variables(self):
        """
        Initialize the L{sorted_variables} attribute, based on the
        L{variables} and L{sort_spec} attributes.  This should usually
        be called after all variables have been added to C{variables}
        (including any inherited variables for classes).  
        """
        unsorted = self.variables.copy()
        self.sorted_variables = []
    
        # Add any variables that are listed in sort_spec
        if self.sort_spec is not UNKNOWN:
            unused_idents = set(self.sort_spec)
            for ident in self.sort_spec:
                if ident in unsorted:
                    self.sorted_variables.append(unsorted.pop(ident))
                    unused_idents.discard(ident)
                elif '*' in ident:
                    regexp = re.compile('^%s$' % ident.replace('*', '(.*)'))
                    # sort within matching group?
                    for name, var_doc in unsorted.items():
                        if regexp.match(name):
                            self.sorted_variables.append(unsorted.pop(name))
                            unused_idents.discard(ident)
            for ident in unused_idents:
                if ident not in ['__all__', '__docformat__', '__path__']:
                    log.warning("@sort: %s.%s not found" %
                                (self.canonical_name, ident))
                    
    
        # Add any remaining variables in alphabetical order.
        var_docs = unsorted.items()
        var_docs.sort()
        for name, var_doc in var_docs:
            self.sorted_variables.append(var_doc)

    def init_variable_groups(self):
        """
        Initialize the L{variable_groups} attribute, based on the
        L{sorted_variables} and L{group_specs} attributes.
        """
        if self.sorted_variables is UNKNOWN:
            self.init_sorted_variables()
        assert len(self.sorted_variables) == len(self.variables)

        elts = [(v.name, v) for v in self.sorted_variables]
        self._unused_groups = dict([(n,set(i)) for (n,i) in self.group_specs])
        self.variable_groups = self._init_grouping(elts)

    def group_names(self):
        """
        Return a list of the group names defined by this namespace, in
        the order in which they should be listed, with no duplicates.
        """
        name_list = ['']
        name_set = set()
        for name, spec in self.group_specs:
            if name not in name_set:
                name_set.add(name)
                name_list.append(name)
        return name_list

    def _init_grouping(self, elts):
        """
        Divide a given a list of APIDoc objects into groups, as
        specified by L{self.group_specs}.

        @param elts: A list of tuples C{(name, apidoc)}.
        
        @return: A list of tuples C{(groupname, elts)}, where
        C{groupname} is the name of a group and C{elts} is a list of
        C{APIDoc}s in that group.  The first tuple has name C{''}, and
        is used for ungrouped elements.  The remaining tuples are
        listed in the order that they appear in C{self.group_specs}.
        Within each tuple, the elements are listed in the order that
        they appear in C{api_docs}.
        """
        # Make the common case fast.
        if len(self.group_specs) == 0:
            return {'': [elt[1] for elt in elts]}

        ungrouped = set([elt_doc for (elt_name, elt_doc) in elts])

        ungrouped = dict(elts)
        groups = {}
        for elt_name, elt_doc in elts:
            for (group_name, idents) in self.group_specs:
                group = groups.setdefault(group_name, [])
                unused_groups = self._unused_groups[group_name]
                for ident in idents:
                    if re.match('^%s$' % ident.replace('*', '(.*)'), elt_name):
                        unused_groups.discard(ident)
                        if elt_name in ungrouped:
                            group.append(ungrouped.pop(elt_name))
                        else:
                            log.warning("%s.%s in multiple groups" %
                                        (self.canonical_name, elt_name))

        # Convert ungrouped from an unordered set to an ordered list.
        groups[''] = [elt_doc for (elt_name, elt_doc) in elts
                      if elt_name in ungrouped]
        return groups
    
    def report_unused_groups(self):
        """
        Issue a warning for any @group items that were not used by
        L{_init_grouping()}.
        """
        for (group, unused_idents) in self._unused_groups.items():
            for ident in unused_idents:
                log.warning("@group %s: %s.%s not found" %
                            (group, self.canonical_name, ident))
                        
class ModuleDoc(NamespaceDoc):
    """
    API documentation information about a single module.
    """
    #{ Information about the Module
    filename = UNKNOWN
    """@ivar: The name of the file that defines the module.
       @type: C{string}"""
    docformat = UNKNOWN
    """@ivar: The markup language used by docstrings in this module.
       @type: C{string}"""
    #{ Information about Submodules
    submodules = UNKNOWN
    """@ivar: Modules contained by this module (if this module
       is a package).  (Note: on rare occasions, a module may have a
       submodule that is shadowed by a variable with the same name.)
       @type: C{list} of L{ModuleDoc}"""
    submodule_groups = UNKNOWN
    """@ivar: A dictionary specifying what group each
       submodule belongs to.  The keys of the dictionary are group
       names, and the values are lists of C{ModuleDoc}s.  The order
       that groups should be listed in should be taken from
       L{group_specs}.
       @type: C{dict} from C{str} to C{list} of L{ModuleDoc}"""
    #{ Information about Packages
    package = UNKNOWN
    """@ivar: API documentation for the module's containing package.
       @type: L{ModuleDoc}"""
    is_package = UNKNOWN
    """@ivar: True if this C{ModuleDoc} describes a package.
       @type: C{bool}"""
    path = UNKNOWN
    """@ivar: If this C{ModuleDoc} describes a package, then C{path}
       contains a list of directories that constitute its path (i.e.,
       the value of its C{__path__} variable).
       @type: C{list} of C{str}"""
    #{ Information about Imported Variables
    imports = UNKNOWN
    """@ivar: A list of the source names of variables imported into
       this module.  This is used to construct import graphs.
       @type: C{list} of L{DottedName}"""
    #}

    def apidoc_links(self, **filters):
        val_docs = NamespaceDoc.apidoc_links(self, **filters)
        if (filters.get('packages', True) and
            self.package not in (None, UNKNOWN)):
            val_docs.append(self.package)
        if (filters.get('submodules', True) and
            self.submodules not in (None, UNKNOWN)):
            val_docs += self.submodules
        return val_docs

    def init_submodule_groups(self):
        """
        Initialize the L{submodule_groups} attribute, based on the
        L{submodules} and L{group_specs} attributes.
        """
        if self.submodules in (None, UNKNOWN):
            return
        self.submodules = sorted(self.submodules,
                                 key=lambda m:m.canonical_name)
        elts = [(m.canonical_name[-1], m) for m in self.submodules]
        self.submodule_groups = self._init_grouping(elts)

    def select_variables(self, group=None, value_type=None, public=None,
                         imported=None, detailed=None):
        """
        Return a specified subset of this module's L{sorted_variables}
        list.  If C{value_type} is given, then only return variables
        whose values have the specified type.  If C{group} is given,
        then only return variables that belong to the specified group.

        @require: The L{sorted_variables}, L{variable_groups}, and
            L{submodule_groups} attributes must be initialized before
            this method can be used.  See L{init_sorted_variables()},
            L{init_variable_groups()}, and L{init_submodule_groups()}.

        @param value_type: A string specifying the value type for
            which variables should be returned.  Valid values are:
              - 'class' - variables whose values are classes or types.
              - 'function' - variables whose values are functions.
              - 'other' - variables whose values are not classes,
                 exceptions, types, or functions.
        @type value_type: C{string}
        
        @param group: The name of the group for which variables should
            be returned.  A complete list of the groups defined by
            this C{ModuleDoc} is available in the L{group_names}
            instance variable.  The first element of this list is
            always the special group name C{''}, which is used for
            variables that do not belong to any group.
        @type group: C{string}

        @param detailed: If True (False), return only the variables
            deserving (not deserving) a detailed informative box.
            If C{None}, don't care.
        @type detailed: C{bool}
        """
        if (self.sorted_variables is UNKNOWN or 
            self.variable_groups is UNKNOWN):
            raise ValueError('sorted_variables and variable_groups '
                             'must be initialized first.')
        
        if group is None: var_list = self.sorted_variables
        else:
            var_list = self.variable_groups.get(group, self.sorted_variables)

        # Public/private filter (Count UNKNOWN as public)
        if public is True:
            var_list = [v for v in var_list if v.is_public is not False]
        elif public is False:
            var_list = [v for v in var_list if v.is_public is False]

        # Imported filter (Count UNKNOWN as non-imported)
        if imported is True:
            var_list = [v for v in var_list if v.is_imported is True]
        elif imported is False:
            var_list = [v for v in var_list if v.is_imported is not True]

        # Detailed filter
        if detailed is True:
            var_list = [v for v in var_list if v.is_detailed() is True]
        elif detailed is False:
            var_list = [v for v in var_list if v.is_detailed() is not True]

        # [xx] Modules are not currently included in any of these
        # value types.
        if value_type is None:
            return var_list
        elif value_type == 'class':
            return [var_doc for var_doc in var_list
                    if (isinstance(var_doc.value, ClassDoc))]
        elif value_type == 'function':
            return [var_doc for var_doc in var_list
                    if isinstance(var_doc.value, RoutineDoc)]
        elif value_type == 'other':
            return [var_doc for var_doc in var_list
                    if not isinstance(var_doc.value,
                                      (ClassDoc, RoutineDoc, ModuleDoc))]
        else:
            raise ValueError('Bad value type %r' % value_type)

class ClassDoc(NamespaceDoc):
    """
    API documentation information about a single class.
    """
    #{ Information about Base Classes
    bases = UNKNOWN
    """@ivar: API documentation for the class's base classes.
    @type: C{list} of L{ClassDoc}"""
    #{ Information about Subclasses
    subclasses = UNKNOWN
    """@ivar: API documentation for the class's known subclasses.
    @type: C{list} of L{ClassDoc}"""
    #}

    def apidoc_links(self, **filters):
        val_docs = NamespaceDoc.apidoc_links(self, **filters)
        if (filters.get('bases', True) and 
            self.bases not in (None, UNKNOWN)):
            val_docs += self.bases
        if (filters.get('subclasses', True) and
            self.subclasses not in (None, UNKNOWN)):
            val_docs += self.subclasses
        return val_docs
    
    def is_type(self):
        if self.canonical_name == DottedName('type'): return True
        if self.bases is UNKNOWN: return False
        for base in self.bases:
            if isinstance(base, ClassDoc) and base.is_type():
                return True
        return False
    
    def is_exception(self):
        if self.canonical_name == DottedName('Exception'): return True
        if self.bases is UNKNOWN: return False
        for base in self.bases:
            if isinstance(base, ClassDoc) and base.is_exception():
                return True
        return False
    
    def is_newstyle_class(self):
        if self.canonical_name == DottedName('object'): return True
        if self.bases is UNKNOWN: return False
        for base in self.bases:
            if isinstance(base, ClassDoc) and base.is_newstyle_class():
                return True
        return False

    def mro(self, warn_about_bad_bases=False):
        if self.is_newstyle_class():
            return self._c3_mro(warn_about_bad_bases)
        else:
            return self._dfs_bases([], set(), warn_about_bad_bases)
                
    def _dfs_bases(self, mro, seen, warn_about_bad_bases):
        if self in seen: return mro
        mro.append(self)
        seen.add(self)
        if self.bases is not UNKNOWN:
            for base in self.bases:
                if isinstance(base, ClassDoc) and base.proxy_for is None:
                    base._dfs_bases(mro, seen, warn_about_bad_bases)
                elif warn_about_bad_bases:
                    self._report_bad_base(base)
        return mro

    def _c3_mro(self, warn_about_bad_bases):
        """
        Compute the class precedence list (mro) according to C3.
        @seealso: U{http://www.python.org/2.3/mro.html}
        """
        bases = [base for base in self.bases if isinstance(base, ClassDoc)]
        if len(bases) != len(self.bases) and warn_about_bad_bases:
            for base in self.bases:
                if (not isinstance(base, ClassDoc) or
                    base.proxy_for is not None):
                    self._report_bad_base(base)
        w = [warn_about_bad_bases]*len(bases)
        return self._c3_merge([[self]] + map(ClassDoc._c3_mro, bases, w) +
                              [list(bases)])

    def _report_bad_base(self, base):
        if not isinstance(base, ClassDoc):
            if not isinstance(base, GenericValueDoc):
                base_name = base.canonical_name
            elif base.parse_repr is not UNKNOWN:
                base_name = base.parse_repr
            else:
                base_name = '%r' % base
            log.warning("%s's base %s is not a class" %
                        (self.canonical_name, base_name))
        elif base.proxy_for is not None:
            log.warning("No information available for %s's base %s" %
                        (self.canonical_name, base.proxy_for))

    def _c3_merge(self, seqs):
        """
        Helper function for L{_c3_mro}.
        """
        res = []
        while 1:
          nonemptyseqs=[seq for seq in seqs if seq]
          if not nonemptyseqs: return res
          for seq in nonemptyseqs: # find merge candidates among seq heads
              cand = seq[0]
              nothead=[s for s in nonemptyseqs if cand in s[1:]]
              if nothead: cand=None #reject candidate
              else: break
          if not cand: raise "Inconsistent hierarchy"
          res.append(cand)
          for seq in nonemptyseqs: # remove cand
              if seq[0] == cand: del seq[0]
    
    def select_variables(self, group=None, value_type=None, inherited=None,
                         public=None, imported=None, detailed=None):
        """
        Return a specified subset of this class's L{sorted_variables}
        list.  If C{value_type} is given, then only return variables
        whose values have the specified type.  If C{group} is given,
        then only return variables that belong to the specified group.
        If C{inherited} is True, then only return inherited variables;
        if C{inherited} is False, then only return local variables.

        @require: The L{sorted_variables} and L{variable_groups}
            attributes must be initialized before this method can be
            used.  See L{init_sorted_variables()} and
            L{init_variable_groups()}.

        @param value_type: A string specifying the value type for
            which variables should be returned.  Valid values are:
              - 'instancemethod' - variables whose values are
                instance methods.
              - 'classmethod' - variables whose values are class
                methods.
              - 'staticmethod' - variables whose values are static
                methods.
              - 'properties' - variables whose values are properties.
              - 'class' - variables whose values are nested classes
                (including exceptions and types).
              - 'instancevariable' - instance variables.  This includes
                any variables that are explicitly marked as instance
                variables with docstring fields; and variables with
                docstrings that are initialized in the constructor.
              - 'classvariable' - class variables.  This includes any
                variables that are not included in any of the above
                categories.
        @type value_type: C{string}
        
        @param group: The name of the group for which variables should
            be returned.  A complete list of the groups defined by
            this C{ClassDoc} is available in the L{group_names}
            instance variable.  The first element of this list is
            always the special group name C{''}, which is used for
            variables that do not belong to any group.
        @type group: C{string}

        @param inherited: If C{None}, then return both inherited and
            local variables; if C{True}, then return only inherited
            variables; if C{False}, then return only local variables.

        @param detailed: If True (False), return only the variables
            deserving (not deserving) a detailed informative box.
            If C{None}, don't care.
        @type detailed: C{bool}
        """
        if (self.sorted_variables is UNKNOWN or 
            self.variable_groups is UNKNOWN):
            raise ValueError('sorted_variables and variable_groups '
                             'must be initialized first.')
        
        if group is None: var_list = self.sorted_variables
        else: var_list = self.variable_groups[group]

        # Public/private filter (Count UNKNOWN as public)
        if public is True:
            var_list = [v for v in var_list if v.is_public is not False]
        elif public is False:
            var_list = [v for v in var_list if v.is_public is False]

        # Inherited filter (Count UNKNOWN as non-inherited)
        if inherited is None: pass
        elif inherited:
            var_list = [v for v in var_list if v.container != self]
        else:
            var_list = [v for v in var_list if v.container == self ]

        # Imported filter (Count UNKNOWN as non-imported)
        if imported is True:
            var_list = [v for v in var_list if v.is_imported is True]
        elif imported is False:
            var_list = [v for v in var_list if v.is_imported is not True]

        # Detailed filter
        if detailed is True:
            var_list = [v for v in var_list if v.is_detailed() is True]
        elif detailed is False:
            var_list = [v for v in var_list if v.is_detailed() is not True]

        if value_type is None:
            return var_list
        elif value_type == 'method':
            return [var_doc for var_doc in var_list
                    if (isinstance(var_doc.value, RoutineDoc) and
                        var_doc.is_instvar in (False, UNKNOWN))]
        elif value_type == 'instancemethod':
            return [var_doc for var_doc in var_list
                    if (isinstance(var_doc.value, RoutineDoc) and
                        not isinstance(var_doc.value, ClassMethodDoc) and
                        not isinstance(var_doc.value, StaticMethodDoc) and
                        var_doc.is_instvar in (False, UNKNOWN))]
        elif value_type == 'classmethod':
            return [var_doc for var_doc in var_list
                    if (isinstance(var_doc.value, ClassMethodDoc) and
                        var_doc.is_instvar in (False, UNKNOWN))]
        elif value_type == 'staticmethod':
            return [var_doc for var_doc in var_list
                    if (isinstance(var_doc.value, StaticMethodDoc) and
                        var_doc.is_instvar in (False, UNKNOWN))]
        elif value_type == 'property':
            return [var_doc for var_doc in var_list
                    if (isinstance(var_doc.value, PropertyDoc) and
                        var_doc.is_instvar in (False, UNKNOWN))]
        elif value_type == 'class':
            return [var_doc for var_doc in var_list
                    if (isinstance(var_doc.value, ClassDoc) and
                        var_doc.is_instvar in (False, UNKNOWN))]
        elif value_type == 'instancevariable':
            return [var_doc for var_doc in var_list
                    if var_doc.is_instvar is True]
        elif value_type == 'classvariable':
            return [var_doc for var_doc in var_list
                    if (var_doc.is_instvar in (False, UNKNOWN) and
                        not isinstance(var_doc.value,
                                       (RoutineDoc, ClassDoc, PropertyDoc)))]
        else:
            raise ValueError('Bad value type %r' % value_type)

class RoutineDoc(ValueDoc):
    """
    API documentation information about a single routine.
    """
    #{ Signature
    posargs = UNKNOWN
    """@ivar: The names of the routine's positional arguments.
       If an argument list contains \"unpacking\" arguments, then
       their names will be specified using nested lists.  E.g., if
       a function's argument list is C{((x1,y1), (x2,y2))}, then
       posargs will be C{[['x1','y1'], ['x2','y2']]}.
       @type: C{list}"""
    posarg_defaults = UNKNOWN
    """@ivar: API documentation for the positional arguments'
       default values.  This list has the same length as C{posargs}, and
       each element of C{posarg_defaults} describes the corresponding
       argument in C{posargs}.  For positional arguments with no default,
       C{posargs_defaults} will contain None.
       @type: C{list} of C{ValueDoc} or C{None}"""
    vararg = UNKNOWN
    """@ivar: The name of the routine's vararg argument, or C{None} if
       it has no vararg argument.
       @type: C{string} or C{None}"""
    kwarg = UNKNOWN
    """@ivar: The name of the routine's keyword argument, or C{None} if
       it has no keyword argument.
       @type: C{string} or C{None}"""
    lineno = UNKNOWN # used to look up profiling info from pstats.
    """@ivar: The line number of the first line of the function's
       signature.  For Python functions, this is equal to
       C{func.func_code.co_firstlineno}.  The first line of a file
       is considered line 1.
       @type: C{int}"""
    #} end of "signature" group

    #{ Decorators
    decorators = UNKNOWN
    """@ivar: A list of names of decorators that were applied to this
       routine, in the order that they are listed in the source code.
       (I.e., in the reverse of the order that they were applied in.)
       @type: C{list} of C{string}"""
    #} end of "decorators" group

    #{ Information Extracted from Docstrings
    arg_descrs = UNKNOWN
    """@ivar: A list of descriptions of the routine's
       arguments.  Each element of this list is a tuple C{(args,
       descr)}, where C{args} is a list of argument names; and
       C{descr} is a L{ParsedDocstring
       <epydoc.markup.ParsedDocstring>} describing the argument(s)
       specified by C{arg}.
       @type: C{list}"""
    arg_types = UNKNOWN
    """@ivar: Descriptions of the expected types for the
       routine's arguments, encoded as a dictionary mapping from
       argument names to type descriptions.
       @type: C{dict} from C{string} to L{ParsedDocstring
       <epydoc.markup.ParsedDocstring>}"""
    return_descr = UNKNOWN
    """@ivar: A description of the value returned by this routine.
       @type: L{ParsedDocstring<epydoc.markup.ParsedDocstring>}"""
    return_type = UNKNOWN
    """@ivar: A description of expected type for the value
       returned by this routine.
       @type: L{ParsedDocstring<epydoc.markup.ParsedDocstring>}"""
    exception_descrs = UNKNOWN
    """@ivar: A list of descriptions of exceptions
       that the routine might raise.  Each element of this list is a
       tuple C{(exc, descr)}, where C{exc} is a string contianing the
       exception name; and C{descr} is a L{ParsedDocstring
       <epydoc.markup.ParsedDocstring>} describing the circumstances
       under which the exception specified by C{exc} is raised.
       @type: C{list}"""
    #} end of "information extracted from docstrings" group
    callgraph_uid = None
    """@ivar: L{DotGraph}.uid of the call graph for the function.
       @type: C{str}"""

    def is_detailed(self):
        if super(RoutineDoc, self).is_detailed():
            return True

        if self.arg_descrs not in (None, UNKNOWN) and self.arg_descrs:
            return True

        if self.arg_types not in (None, UNKNOWN) and self.arg_types:
            return True

        if self.return_descr not in (None, UNKNOWN):
            return True

        if self.exception_descrs not in (None, UNKNOWN) and self.exception_descrs:
            return True

        if (self.decorators not in (None, UNKNOWN)
            and [ d for d in self.decorators
                 if d not in ('classmethod', 'staticmethod') ]):
            return True

        return False

    def all_args(self):
        """
        @return: A list of the names of all arguments (positional,
        vararg, and keyword), in order.  If a positional argument
        consists of a tuple of names, then that tuple will be
        flattened.
        """
        if self.posargs is UNKNOWN:
            return UNKNOWN
            
        all_args = _flatten(self.posargs)
        if self.vararg not in (None, UNKNOWN):
            all_args.append(self.vararg)
        if self.kwarg not in (None, UNKNOWN):
            all_args.append(self.kwarg)
        return all_args

def _flatten(lst, out=None):
    """
    Return a flattened version of C{lst}.
    """
    if out is None: out = []
    for elt in lst:
        if isinstance(elt, (list,tuple)):
            _flatten(elt, out)
        else:
            out.append(elt)
    return out

class ClassMethodDoc(RoutineDoc): pass
class StaticMethodDoc(RoutineDoc): pass

class PropertyDoc(ValueDoc):
    """
    API documentation information about a single property.
    """
    #{ Property Access Functions
    fget = UNKNOWN
    """@ivar: API documentation for the property's get function.
       @type: L{RoutineDoc}"""
    fset = UNKNOWN
    """@ivar: API documentation for the property's set function.
       @type: L{RoutineDoc}"""
    fdel = UNKNOWN
    """@ivar: API documentation for the property's delete function.
       @type: L{RoutineDoc}"""
    #}
    #{ Information Extracted from Docstrings
    type_descr = UNKNOWN
    """@ivar: A description of the property's expected type, extracted
       from its docstring.
       @type: L{ParsedDocstring<epydoc.markup.ParsedDocstring>}"""
    #} end of "information extracted from docstrings" group

    def apidoc_links(self, **filters):
        val_docs = []
        if self.fget not in (None, UNKNOWN): val_docs.append(self.fget)
        if self.fset not in (None, UNKNOWN): val_docs.append(self.fset)
        if self.fdel not in (None, UNKNOWN): val_docs.append(self.fdel)
        return val_docs

    def is_detailed(self):
        if super(PropertyDoc, self).is_detailed():
            return True

        if self.fget not in (None, UNKNOWN) and self.fget.pyval is not None:
             return True
        if self.fset not in (None, UNKNOWN) and self.fset.pyval is not None:
             return True
        if self.fdel not in (None, UNKNOWN) and self.fdel.pyval is not None:
             return True

        return False

######################################################################
## Index
######################################################################

class DocIndex:
    """
    [xx] out of date.
    
    An index that .. hmm...  it *can't* be used to access some things,
    cuz they're not at the root level.  Do I want to add them or what?
    And if so, then I have a sort of a new top level.  hmm..  so
    basically the question is what to do with a name that's not in the
    root var's name space.  2 types:
      - entirely outside (eg os.path)
      - inside but not known (eg a submodule that we didn't look at?)
      - container of current thing not examined?
    
    An index of all the C{APIDoc} objects that can be reached from a
    root set of C{ValueDoc}s.  
    
    The members of this index can be accessed by dotted name.  In
    particular, C{DocIndex} defines two mappings, accessed via the
    L{get_vardoc()} and L{get_valdoc()} methods, which can be used to
    access C{VariableDoc}s or C{ValueDoc}s respectively by name.  (Two
    separate mappings are necessary because a single name can be used
    to refer to both a variable and to the value contained by that
    variable.)

    Additionally, the index defines two sets of C{ValueDoc}s:
    \"reachable C{ValueDoc}s\" and \"contained C{ValueDoc}s\".  The
    X{reachable C{ValueDoc}s} are defined as the set of all
    C{ValueDoc}s that can be reached from the root set by following
    I{any} sequence of pointers to C{ValueDoc}s or C{VariableDoc}s.
    The X{contained C{ValueDoc}s} are defined as the set of all
    C{ValueDoc}s that can be reached from the root set by following
    only the C{ValueDoc} pointers defined by non-imported
    C{VariableDoc}s.  For example, if the root set contains a module
    C{m}, then the contained C{ValueDoc}s includes the C{ValueDoc}s
    for any functions, variables, or classes defined in that module,
    as well as methods and variables defined in classes defined in the
    module.  The reachable C{ValueDoc}s includes all of those
    C{ValueDoc}s, as well as C{ValueDoc}s for any values imported into
    the module, and base classes for classes defined in the module.
    """

    def __init__(self, root):
        """
        Create a new documentation index, based on the given root set
        of C{ValueDoc}s.  If any C{APIDoc}s reachable from the root
        set does not have a canonical name, then it will be assigned
        one.  etc.
        
        @param root: A list of C{ValueDoc}s.
        """
        for apidoc in root:
            if apidoc.canonical_name in (None, UNKNOWN):
                raise ValueError("All APIdocs passed to DocIndexer "
                                 "must already have canonical names.")
        
        # Initialize the root items list.  We sort them by length in
        # ascending order.  (This ensures that variables will shadow
        # submodules when appropriate.)
        # When the elements name is the same, list in alphabetical order:
        # this is needed by the check for duplicates below.
        self.root = sorted(root,
            key=lambda d: (len(d.canonical_name), d.canonical_name))
        """The list of C{ValueDoc}s to document.
            @type: C{list}"""

        # Drop duplicated modules
        # [xx] maybe what causes duplicates should be fixed instead.
        #      If fixed, adjust the sort here above: sorting by names will not
        #      be required anymore
        i = 1
        while i < len(self.root):
            if self.root[i-1] is self.root[i]:
                del self.root[i]
            else:
                i += 1

        self.mlclasses = self._get_module_classes(self.root)
        """A mapping from class names to L{ClassDoc}. Contains
           classes defined at module level for modules in L{root}
           and which can be used as fallback by L{find()} if looking
           in containing namespaces fails.
           @type: C{dict} from C{str} to L{ClassDoc} or C{list}"""

        self.callers = None
        """A dictionary mapping from C{RoutineDoc}s in this index
           to lists of C{RoutineDoc}s for the routine's callers.
           This dictionary is initialized by calling
           L{read_profiling_info()}.
           @type: C{list} of L{RoutineDoc}"""
        
        self.callees = None
        """A dictionary mapping from C{RoutineDoc}s in this index
           to lists of C{RoutineDoc}s for the routine's callees.
           This dictionary is initialized by calling
           L{read_profiling_info()}.
           @type: C{list} of L{RoutineDoc}"""

        self._funcid_to_doc = {}
        """A mapping from C{profile} function ids to corresponding
           C{APIDoc} objects.  A function id is a tuple of the form
           C{(filename, lineno, funcname)}.  This is used to update
           the L{callers} and L{callees} variables."""

        self._container_cache = {}
        """A cache for the L{container()} method, to increase speed."""

        self._get_cache = {}
        """A cache for the L{get_vardoc()} and L{get_valdoc()} methods,
        to increase speed."""

    #////////////////////////////////////////////////////////////
    # Lookup methods
    #////////////////////////////////////////////////////////////
    # [xx]
    # Currently these only work for things reachable from the
    # root... :-/  I might want to change this so that imported
    # values can be accessed even if they're not contained.  
    # Also, I might want canonical names to not start with ??
    # if the thing is a top-level imported module..?

    def get_vardoc(self, name):
        """
        Return the C{VariableDoc} with the given name, or C{None} if this
        index does not contain a C{VariableDoc} with the given name.
        """
        var, val = self._get(name)
        return var

    def get_valdoc(self, name):
        """
        Return the C{ValueDoc} with the given name, or C{None} if this
        index does not contain a C{ValueDoc} with the given name.
        """
        var, val = self._get(name)
        return val

    def _get(self, name):
        """
        A helper function that's used to implement L{get_vardoc()}
        and L{get_valdoc()}.
        """
        # Convert name to a DottedName, if necessary.
        if not isinstance(name, DottedName):
            name = DottedName(name)

        # Check if the result is cached.
        val = self._get_cache.get(name)
        if val is not None: return val

        # Look for an element in the root set whose name is a prefix
        # of `name`.  If we can't find one, then return None.
        for root_valdoc in self.root:
            if root_valdoc.canonical_name.dominates(name):
                # Starting at the root valdoc, walk down the variable/
                # submodule chain until we find the requested item.
                var_doc = None
                val_doc = root_valdoc
                for identifier in name[len(root_valdoc.canonical_name):]:
                    if val_doc is None: break
                    var_doc, val_doc = self._get_from(val_doc, identifier)
                else:
                    # If we found it, then return.
                    if var_doc is not None or val_doc is not None:
                        self._get_cache[name] = (var_doc, val_doc)
                        return var_doc, val_doc

        # We didn't find it.
        self._get_cache[name] = (None, None)
        return None, None

    def _get_from(self, val_doc, identifier):
        if isinstance(val_doc, NamespaceDoc):
            child_var = val_doc.variables.get(identifier)
            if child_var is not None:
                child_val = child_var.value
                if child_val is UNKNOWN: child_val = None
                return child_var, child_val

        # If that fails, then see if it's a submodule.
        if (isinstance(val_doc, ModuleDoc) and
            val_doc.submodules is not UNKNOWN):
            for submodule in val_doc.submodules:
                if submodule.canonical_name[-1] == identifier:
                    var_doc = None
                    val_doc = submodule
                    if val_doc is UNKNOWN: val_doc = None
                    return var_doc, val_doc

        return None, None

    def find(self, name, context):
        """
        Look for an C{APIDoc} named C{name}, relative to C{context}.
        Return the C{APIDoc} if one is found; otherwise, return
        C{None}.  C{find} looks in the following places, in order:
          - Function parameters (if one matches, return C{None})
          - All enclosing namespaces, from closest to furthest.
          - If C{name} starts with C{'self'}, then strip it off and
            look for the remaining part of the name using C{find}
          - Builtins
          - Parameter attributes
          - Classes at module level (if the name is not ambiguous)
        
        @type name: C{str} or L{DottedName}
        @type context: L{APIDoc}
        """
        if isinstance(name, basestring):
            name = re.sub(r'\(.*\)$', '', name.strip())
            if re.match('^([a-zA-Z_]\w*)(\.[a-zA-Z_]\w*)*$', name):
                name = DottedName(name)
            else:
                return None
        elif not isinstance(name, DottedName):
            raise TypeError("'name' should be a string or DottedName")
        
        if context is None or context.canonical_name is None:
            container_name = []
        else:
            container_name = context.canonical_name

        # Check for the name in all containing namespaces, starting
        # with the closest one.
        for i in range(len(container_name), -1, -1):
            relative_name = container_name[:i]+name
            # Is `name` the absolute name of a documented value?
            # (excepting GenericValueDoc values.)
            val_doc = self.get_valdoc(relative_name)
            if (val_doc is not None and
                not isinstance(val_doc, GenericValueDoc)):
                return val_doc
            # Is `name` the absolute name of a documented variable?
            var_doc = self.get_vardoc(relative_name)
            if var_doc is not None: return var_doc

        # If the name begins with 'self', then try stripping that off
        # and see if we can find the variable.
        if name[0] == 'self':
            doc = self.find('.'.join(name[1:]), context)
            if doc is not None: return doc

        # Is it the name of a builtin?
        if len(name)==1 and hasattr(__builtin__, name[0]):
            return None
        
        # Is it a parameter's name or an attribute of a parameter?
        if isinstance(context, RoutineDoc):
            all_args = context.all_args()
            if all_args is not UNKNOWN and name[0] in all_args:
                return None

        # Is this an object directly contained by any module?
        doc = self.mlclasses.get(name[-1])
        if isinstance(doc, APIDoc):
            return doc
        elif isinstance(doc, list):
            log.warning("%s is an ambiguous name: it may be %s" % (
                name[-1],
                ", ".join([ "'%s'" % d.canonical_name for d in doc ])))

            # Drop this item so that the warning is reported only once.
            # fail() will fail anyway.
            del self.mlclasses[name[-1]]

    def _get_module_classes(self, docs):
        """
        Gather all the classes defined in a list of modules.

        Very often people refers to classes only by class name,
        even if they are not imported in the namespace. Linking
        to such classes will fail if we look for them only in nested
        namespaces. Allow them to retrieve only by name.

        @param docs: containers of the objects to collect
        @type docs: C{list} of C{APIDoc}
        @return: mapping from objects name to the object(s) with that name
        @rtype: C{dict} from C{str} to L{ClassDoc} or C{list}
        """
        classes = {}
        for doc in docs:
            if not isinstance(doc, ModuleDoc):
                continue

            for var in doc.variables.values():
                if not isinstance(var.value, ClassDoc):
                    continue

                val = var.value
                if val in (None, UNKNOWN) or val.defining_module is not doc:
                    continue
                if val.canonical_name in (None, UNKNOWN):
                    continue

                name = val.canonical_name[-1]
                vals = classes.get(name)
                if vals is None:
                    classes[name] = val
                elif not isinstance(vals, list):
                    classes[name] = [ vals, val ]
                else:
                    vals.append(val)

        return classes

    #////////////////////////////////////////////////////////////
    # etc
    #////////////////////////////////////////////////////////////

    def reachable_valdocs(self, **filters):
        """
        Return a list of all C{ValueDoc}s that can be reached,
        directly or indirectly from this C{DocIndex}'s root set.
        
        @param filters: A set of filters that can be used to prevent
            C{reachable_valdocs} from following specific link types
            when looking for C{ValueDoc}s that can be reached from the
            root set.  See C{APIDoc.apidoc_links} for a more complete
            description.
        """
        return reachable_valdocs(self.root, **filters)

    def container(self, api_doc):
        """
        Return the C{ValueDoc} that contains the given C{APIDoc}, or
        C{None} if its container is not in the index.
        """
        # Check if the result is cached.
        val = self._container_cache.get(api_doc)
        if val is not None: return val
        
        if isinstance(api_doc, GenericValueDoc):
            self._container_cache[api_doc] = None
            return None # [xx] unknown.
        if isinstance(api_doc, VariableDoc):
            self._container_cache[api_doc] = api_doc.container
            return api_doc.container
        if len(api_doc.canonical_name) == 1:
            self._container_cache[api_doc] = None
            return None
        elif isinstance(api_doc, ModuleDoc) and api_doc.package is not UNKNOWN:
            self._container_cache[api_doc] = api_doc.package
            return api_doc.package
        else:
            parent = self.get_valdoc(api_doc.canonical_name.container())
            self._container_cache[api_doc] = parent
            return parent

    #////////////////////////////////////////////////////////////
    # Profiling information
    #////////////////////////////////////////////////////////////

    def read_profiling_info(self, profile_stats):
        """
        Initialize the L{callers} and L{callees} variables, given a
        C{Stat} object from the C{pstats} module.
        
        @warning: This method uses undocumented data structures inside
            of C{profile_stats}.
        """
        if self.callers is None: self.callers = {}
        if self.callees is None: self.callees = {}
        
        # The Stat object encodes functions using `funcid`s, or
        # tuples of (filename, lineno, funcname).  Create a mapping
        # from these `funcid`s to `RoutineDoc`s.
        self._update_funcid_to_doc(profile_stats)
        
        for callee, (cc, nc, tt, ct, callers) in profile_stats.stats.items():
            callee = self._funcid_to_doc.get(callee)
            if callee is None: continue
            for caller in callers:
                caller = self._funcid_to_doc.get(caller)
                if caller is None: continue
                self.callers.setdefault(callee, []).append(caller)
                self.callees.setdefault(caller, []).append(callee)

    def _update_funcid_to_doc(self, profile_stats):
        """
        Update the dictionary mapping from C{pstat.Stat} funciton ids to
        C{RoutineDoc}s.  C{pstat.Stat} function ids are tuples of
        C{(filename, lineno, funcname)}.
        """
        # Maps (filename, lineno, funcname) -> RoutineDoc
        for val_doc in self.reachable_valdocs():
            # We only care about routines.
            if not isinstance(val_doc, RoutineDoc): continue
            # Get the filename from the defining module.
            module = val_doc.defining_module
            if module is UNKNOWN or module.filename is UNKNOWN: continue
            # Normalize the filename.
            filename = os.path.abspath(module.filename)
            try: filename = py_src_filename(filename)
            except: pass
            # Look up the stat_func_id
            funcid = (filename, val_doc.lineno, val_doc.canonical_name[-1])
            if funcid in profile_stats.stats:
                self._funcid_to_doc[funcid] = val_doc

######################################################################
## Pretty Printing
######################################################################

def pp_apidoc(api_doc, doublespace=0, depth=5, exclude=(), include=(),
              backpointers=None):
    """
    @return: A multiline pretty-printed string representation for the
        given C{APIDoc}.
    @param doublespace: If true, then extra lines will be
        inserted to make the output more readable.
    @param depth: The maximum depth that pp_apidoc will descend
        into descendent VarDocs.  To put no limit on
        depth, use C{depth=-1}.
    @param exclude: A list of names of attributes whose values should
        not be shown.
    @param backpointers: For internal use.
    """
    pyid = id(api_doc.__dict__)
    if backpointers is None: backpointers = {}
    if (hasattr(api_doc, 'canonical_name') and
        api_doc.canonical_name not in (None, UNKNOWN)):
        name = '%s for %s' % (api_doc.__class__.__name__,
                              api_doc.canonical_name)
    elif getattr(api_doc, 'name', None) not in (UNKNOWN, None):
        if (getattr(api_doc, 'container', None) not in (UNKNOWN, None) and
            getattr(api_doc.container, 'canonical_name', None)
            not in (UNKNOWN, None)):
            name ='%s for %s' % (api_doc.__class__.__name__,
                                 api_doc.container.canonical_name+
                                 api_doc.name)
        else:
            name = '%s for %s' % (api_doc.__class__.__name__, api_doc.name)
    else:
        name = api_doc.__class__.__name__
        
    if pyid in backpointers:
        return '%s [%s] (defined above)' % (name, backpointers[pyid])
    
    if depth == 0:
        if hasattr(api_doc, 'name') and api_doc.name is not None:
            return '%s...' % api_doc.name
        else:
            return '...'

    backpointers[pyid] = len(backpointers)
    s = '%s [%s]' % (name, backpointers[pyid])

    # Only print non-empty fields:
    fields = [field for field in api_doc.__dict__.keys()
              if (field in include or
                  (getattr(api_doc, field) is not UNKNOWN
                   and field not in exclude))]
    if include:
        fields = [field for field in dir(api_doc)
                  if field in include]
    else:
        fields = [field for field in api_doc.__dict__.keys()
                  if (getattr(api_doc, field) is not UNKNOWN
                      and field not in exclude)]
    fields.sort()
    
    for field in fields:
        fieldval = getattr(api_doc, field)
        if doublespace: s += '\n |'
        s += '\n +- %s' % field

        if (isinstance(fieldval, types.ListType) and
            len(fieldval)>0 and
            isinstance(fieldval[0], APIDoc)):
            s += _pp_list(api_doc, fieldval, doublespace, depth,
                          exclude, include, backpointers,
                          (field is fields[-1]))
        elif (isinstance(fieldval, types.DictType) and
              len(fieldval)>0 and 
              isinstance(fieldval.values()[0], APIDoc)):
            s += _pp_dict(api_doc, fieldval, doublespace, 
                          depth, exclude, include, backpointers,
                          (field is fields[-1]))
        elif isinstance(fieldval, APIDoc):
            s += _pp_apidoc(api_doc, fieldval, doublespace, depth,
                            exclude, include, backpointers,
                            (field is fields[-1]))
        else:
            s += ' = ' + _pp_val(api_doc, fieldval, doublespace,
                                 depth, exclude, include, backpointers)
                
    return s

def _pp_list(api_doc, items, doublespace, depth, exclude, include,
              backpointers, is_last):
    line1 = (is_last and ' ') or '|'
    s = ''
    for item in items:
        line2 = ((item is items[-1]) and ' ') or '|'
        joiner = '\n %s  %s ' % (line1, line2)
        if doublespace: s += '\n %s  |' % line1
        s += '\n %s  +- ' % line1
        valstr = _pp_val(api_doc, item, doublespace, depth, exclude, include,
                         backpointers)
        s += joiner.join(valstr.split('\n'))
    return s

def _pp_dict(api_doc, dict, doublespace, depth, exclude, include,
              backpointers, is_last):
    items = dict.items()
    items.sort()
    line1 = (is_last and ' ') or '|'
    s = ''
    for item in items:
        line2 = ((item is items[-1]) and ' ') or '|'
        joiner = '\n %s  %s ' % (line1, line2)
        if doublespace: s += '\n %s  |' % line1
        s += '\n %s  +- ' % line1
        valstr = _pp_val(api_doc, item[1], doublespace, depth, exclude,
                         include, backpointers)
        s += joiner.join(('%s => %s' % (item[0], valstr)).split('\n'))
    return s

def _pp_apidoc(api_doc, val, doublespace, depth, exclude, include,
                backpointers, is_last):
    line1 = (is_last and ' ') or '|'
    s = ''
    if doublespace: s += '\n %s  |  ' % line1
    s += '\n %s  +- ' % line1
    joiner = '\n %s    ' % line1
    childstr = pp_apidoc(val, doublespace, depth-1, exclude,
                         include, backpointers)
    return s + joiner.join(childstr.split('\n'))
    
def _pp_val(api_doc, val, doublespace, depth, exclude, include, backpointers):
    from epydoc import markup
    if isinstance(val, APIDoc):
        return pp_apidoc(val, doublespace, depth-1, exclude,
                         include, backpointers)
    elif isinstance(val, markup.ParsedDocstring):
        valrepr = `val.to_plaintext(None)`
        if len(valrepr) < 40: return valrepr
        else: return valrepr[:37]+'...'
    else:
        valrepr = repr(val)
        if len(valrepr) < 40: return valrepr
        else: return valrepr[:37]+'...'


########NEW FILE########
__FILENAME__ = checker
#
# objdoc: epydoc documentation completeness checker
# Edward Loper
#
# Created [01/30/01 05:18 PM]
# $Id: checker.py 1366 2006-09-07 15:54:59Z edloper $
#

"""
Documentation completeness checker.  This module defines a single
class, C{DocChecker}, which can be used to check the that specified
classes of objects are documented.
"""
__docformat__ = 'epytext en'

##################################################
## Imports
##################################################

import re, sys, os.path, string
from xml.dom.minidom import Text as _Text
from epydoc.apidoc import *

# The following methods may be undocumented:
_NO_DOCS = ['__hash__', '__repr__', '__str__', '__cmp__']

# The following methods never need descriptions, authors, or
# versions:
_NO_BASIC = ['__hash__', '__repr__', '__str__', '__cmp__']

# The following methods never need return value descriptions.
_NO_RETURN = ['__init__', '__hash__', '__repr__', '__str__', '__cmp__']

# The following methods don't need parameters documented:
_NO_PARAM = ['__cmp__']

class DocChecker:
    """
    Documentation completeness checker.  C{DocChecker} can be used to
    check that specified classes of objects are documented.  To check
    the documentation for a group of objects, you should create a
    C{DocChecker} from a L{DocIndex<apidoc.DocIndex>} that documents
    those objects; and then use the L{check} method to run specified
    checks on the objects' documentation.

    What checks are run, and what objects they are run on, are
    specified by the constants defined by C{DocChecker}.  These
    constants are divided into three groups.  

      - Type specifiers indicate what type of objects should be
        checked: L{MODULE}; L{CLASS}; L{FUNC}; L{VAR}; L{IVAR};
        L{CVAR}; L{PARAM}; and L{RETURN}.
      - Public/private specifiers indicate whether public or private
        objects should be checked: L{PRIVATE}.
      - Check specifiers indicate what checks should be run on the
        objects: L{TYPE}; L{DESCR}; L{AUTHOR};
        and L{VERSION}.

    The L{check} method is used to perform a check on the
    documentation.  Its parameter is formed by or-ing together at
    least one value from each specifier group:

        >>> checker.check(DocChecker.MODULE | DocChecker.DESCR)
        
    To specify multiple values from a single group, simply or their
    values together:
    
        >>> checker.check(DocChecker.MODULE | DocChecker.CLASS |
        ...               DocChecker.FUNC )

    @group Types: MODULE, CLASS, FUNC, VAR, IVAR, CVAR, PARAM,
        RETURN, ALL_T
    @type MODULE: C{int}
    @cvar MODULE: Type specifier that indicates that the documentation
        of modules should be checked.
    @type CLASS: C{int}
    @cvar CLASS: Type specifier that indicates that the documentation
        of classes should be checked.
    @type FUNC: C{int}
    @cvar FUNC: Type specifier that indicates that the documentation
        of functions should be checked.
    @type VAR: C{int}
    @cvar VAR: Type specifier that indicates that the documentation
        of module variables should be checked.
    @type IVAR: C{int}
    @cvar IVAR: Type specifier that indicates that the documentation
        of instance variables should be checked.
    @type CVAR: C{int}
    @cvar CVAR: Type specifier that indicates that the documentation
        of class variables should be checked.
    @type PARAM: C{int}
    @cvar PARAM: Type specifier that indicates that the documentation
        of function and method parameters should be checked.
    @type RETURN: C{int}
    @cvar RETURN: Type specifier that indicates that the documentation
        of return values should be checked.
    @type ALL_T: C{int}
    @cvar ALL_T: Type specifier that indicates that the documentation
        of all objects should be checked.

    @group Checks: TYPE, AUTHOR, VERSION, DESCR, ALL_C
    @type TYPE: C{int}
    @cvar TYPE: Check specifier that indicates that every variable and
        parameter should have a C{@type} field.
    @type AUTHOR: C{int}
    @cvar AUTHOR: Check specifier that indicates that every object
        should have an C{author} field.
    @type VERSION: C{int}
    @cvar VERSION: Check specifier that indicates that every object
        should have a C{version} field.
    @type DESCR: C{int}
    @cvar DESCR: Check specifier that indicates that every object
        should have a description.  
    @type ALL_C: C{int}
    @cvar ALL_C: Check specifier that indicates that  all checks
        should be run.

    @group Publicity: PRIVATE
    @type PRIVATE: C{int}
    @cvar PRIVATE: Specifier that indicates that private objects should
        be checked.
    """
    # Types
    MODULE = 1
    CLASS  = 2
    FUNC   = 4
    VAR    = 8
    #IVAR   = 16
    #CVAR   = 32
    PARAM  = 64
    RETURN = 128
    PROPERTY = 256
    ALL_T  = 1+2+4+8+16+32+64+128+256

    # Checks
    TYPE = 256
    AUTHOR = 1024
    VERSION = 2048
    DESCR = 4096
    ALL_C = 256+512+1024+2048+4096

    # Private/public
    PRIVATE = 16384

    ALL = ALL_T + ALL_C + PRIVATE

    def __init__(self, docindex):
        """
        Create a new C{DocChecker} that can be used to run checks on
        the documentation of the objects documented by C{docindex}

        @param docindex: A documentation map containing the
            documentation for the objects to be checked.
        @type docindex: L{Docindex<apidoc.DocIndex>}
        """
        self._docindex = docindex

        # Initialize instance variables
        self._checks = 0
        self._last_warn = None
        self._out = sys.stdout
        self._num_warnings = 0

    def check(self, *check_sets):
        """
        Run the specified checks on the documentation of the objects
        contained by this C{DocChecker}'s C{DocIndex}.  Any errors found
        are printed to standard out.

        @param check_sets: The checks that should be run on the
            documentation.  This value is constructed by or-ing
            together the specifiers that indicate which objects should
            be checked, and which checks should be run.  See the
            L{module description<checker>} for more information.
            If no checks are specified, then a default set of checks
            will be run.
        @type check_sets: C{int}
        @return: True if no problems were found.
        @rtype: C{boolean}
        """
        if not check_sets:
            check_sets = (DocChecker.MODULE | DocChecker.CLASS |
                          DocChecker.FUNC | DocChecker.VAR | 
                          DocChecker.DESCR,)
            
        self._warnings = {}
        log.start_progress('Checking docs')
        for j, checks in enumerate(check_sets):
            self._check(checks)
        log.end_progress()

        for (warning, docs) in self._warnings.items():
            docs = sorted(docs)
            docnames = '\n'.join(['  - %s' % self._name(d) for d in docs])
            log.warning('%s:\n%s' % (warning, docnames))

    def _check(self, checks):
        self._checks = checks
        
        # Get the list of objects to check.
        valdocs = sorted(self._docindex.reachable_valdocs(
            imports=False, packages=False, bases=False, submodules=False, 
            subclasses=False, private = (checks & DocChecker.PRIVATE)))
        docs = set()
        for d in valdocs:
            if not isinstance(d, GenericValueDoc): docs.add(d)
        for doc in valdocs:
            if isinstance(doc, NamespaceDoc):
                for d in doc.variables.values():
                    if isinstance(d.value, GenericValueDoc): docs.add(d)

        for i, doc in enumerate(sorted(docs)):
            if isinstance(doc, ModuleDoc):
                self._check_module(doc)
            elif isinstance(doc, ClassDoc):
                self._check_class(doc)
            elif isinstance(doc, RoutineDoc):
                self._check_func(doc)
            elif isinstance(doc, PropertyDoc):
                self._check_property(doc)
            elif isinstance(doc, VariableDoc):
                self._check_var(doc)
            else:
                log.error("Don't know how to check %r" % doc)

    def _name(self, doc):
        name = str(doc.canonical_name)
        if isinstance(doc, RoutineDoc): name += '()'
        return name

    def _check_basic(self, doc):
        """
        Check the description, author, version, and see-also fields of
        C{doc}.  This is used as a helper function by L{_check_module},
        L{_check_class}, and L{_check_func}.

        @param doc: The documentation that should be checked.
        @type doc: L{APIDoc}
        @rtype: C{None}
        """
        if ((self._checks & DocChecker.DESCR) and
            (doc.descr in (None, UNKNOWN))):
            if doc.docstring in (None, UNKNOWN):
                self.warning('Undocumented', doc)
            else:
                self.warning('No description', doc)
        if self._checks & DocChecker.AUTHOR:
            for tag, arg, descr in doc.metadata:
                if 'author' == tag: break
            else:
                self.warning('No authors', doc)
        if self._checks & DocChecker.VERSION:
            for tag, arg, descr in doc.metadata:
                if 'version' == tag: break
            else:
                self.warning('No version', doc)
            
    def _check_module(self, doc):
        """
        Run checks on the module whose APIDoc is C{doc}.
        
        @param doc: The APIDoc of the module to check.
        @type doc: L{APIDoc}
        @rtype: C{None}
        """
        if self._checks & DocChecker.MODULE:
            self._check_basic(doc)
        
    def _check_class(self, doc):
        """
        Run checks on the class whose APIDoc is C{doc}.
        
        @param doc: The APIDoc of the class to check.
        @type doc: L{APIDoc}
        @rtype: C{None}
        """
        if self._checks & DocChecker.CLASS:
            self._check_basic(doc)

    def _check_property(self, doc):
        if self._checks & DocChecker.PROPERTY:
            self._check_basic(doc)

    def _check_var(self, doc):
        """
        Run checks on the variable whose documentation is C{var} and
        whose name is C{name}.
        
        @param doc: The documentation for the variable to check.
        @type doc: L{APIDoc}
        @rtype: C{None}
        """
        if self._checks & DocChecker.VAR:
            if (self._checks & (DocChecker.DESCR|DocChecker.TYPE) and
                doc.descr in (None, UNKNOWN) and
                doc.type_descr in (None, UNKNOWN) and
                doc.docstring in (None, UNKNOWN)):
                self.warning('Undocumented', doc)
            else:
                if (self._checks & DocChecker.DESCR and
                    doc.descr in (None, UNKNOWN)):
                    self.warning('No description', doc)
                if (self._checks & DocChecker.TYPE and
                    doc.type_descr in (None, UNKNOWN)):
                    self.warning('No type information', doc)
            
    def _check_func(self, doc):
        """
        Run checks on the function whose APIDoc is C{doc}.
        
        @param doc: The APIDoc of the function to check.
        @type doc: L{APIDoc}
        @rtype: C{None}
        """
        name = doc.canonical_name
        if (self._checks & DocChecker.FUNC and
            doc.docstring in (None, UNKNOWN) and
            doc.canonical_name[-1] not in _NO_DOCS):
            self.warning('Undocumented', doc)
            return
        if (self._checks & DocChecker.FUNC and
            doc.canonical_name[-1] not in _NO_BASIC):
                self._check_basic(doc)
        if (self._checks & DocChecker.RETURN and
            doc.canonical_name[-1] not in _NO_RETURN):
            if (doc.return_type in (None, UNKNOWN) and
                doc.return_descr in (None, UNKNOWN)):
                self.warning('No return descr', doc)
        if (self._checks & DocChecker.PARAM and
            doc.canonical_name[-1] not in _NO_PARAM):
            if doc.arg_descrs in (None, UNKNOWN):
                self.warning('No argument info', doc)
            else:
                args_with_descr = []
                for arg, descr in doc.arg_descrs:
                    if isinstance(arg, basestring):
                        args_with_descr.append(arg)
                    else:
                        args_with_descr += arg
                for posarg in doc.posargs:
                    if (self._checks & DocChecker.DESCR and
                        posarg not in args_with_descr):
                        self.warning('Argument(s) not described', doc)
                    if (self._checks & DocChecker.TYPE and
                        posarg not in doc.arg_types):
                        self.warning('Argument type(s) not described', doc)

    def warning(self, msg, doc):
        self._warnings.setdefault(msg,set()).add(doc)

########NEW FILE########
__FILENAME__ = cli
# epydoc -- Command line interface
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: cli.py 1678 2008-01-29 17:21:29Z edloper $

"""
Command-line interface for epydoc.  Abbreviated Usage::

 epydoc [options] NAMES...
 
     NAMES...                  The Python modules to document.
     --html                    Generate HTML output (default).
     --latex                   Generate LaTeX output.
     --pdf                     Generate pdf output, via LaTeX.
     -o DIR, --output DIR      The output directory.
     --inheritance STYLE       The format for showing inherited objects.
     -V, --version             Print the version of epydoc.
     -h, --help                Display a usage message.

Run \"epydoc --help\" for a complete option list.  See the epydoc(1)
man page for more information.

Config Files
============
Configuration files can be specified with the C{--config} option.
These files are read using U{ConfigParser
<http://docs.python.org/lib/module-ConfigParser.html>}.  Configuration
files may set options or add names of modules to document.  Option
names are (usually) identical to the long names of command line
options.  To specify names to document, use any of the following
option names::

  module modules value values object objects

A simple example of a config file is::

  [epydoc]
  modules: sys, os, os.path, re, %(MYSANDBOXPATH)/utilities.py
  name: Example
  graph: classtree
  introspect: no

All ConfigParser interpolations are done using local values and the
environment variables.


Verbosity Levels
================
The C{-v} and C{-q} options increase and decrease verbosity,
respectively.  The default verbosity level is zero.  The verbosity
levels are currently defined as follows::

                Progress    Markup warnings   Warnings   Errors
 -3               none            no             no        no
 -2               none            no             no        yes
 -1               none            no             yes       yes
  0 (default)     bar             no             yes       yes
  1               bar             yes            yes       yes
  2               list            yes            yes       yes
"""
__docformat__ = 'epytext en'

import sys, os, time, re, pickle, textwrap
from glob import glob
from optparse import OptionParser, OptionGroup, SUPPRESS_HELP
import optparse
import epydoc
from epydoc import log
from epydoc.util import wordwrap, run_subprocess, RunSubprocessError
from epydoc.util import plaintext_to_html
from epydoc.apidoc import UNKNOWN
from epydoc.compat import *
import ConfigParser
from epydoc.docwriter.html_css import STYLESHEETS as CSS_STYLESHEETS

# This module is only available if Docutils are in the system
try:
    from epydoc.docwriter import xlink
except:
    xlink = None

INHERITANCE_STYLES = ('grouped', 'listed', 'included')
GRAPH_TYPES = ('classtree', 'callgraph', 'umlclasstree')
ACTIONS = ('html', 'text', 'latex', 'dvi', 'ps', 'pdf', 'check')
DEFAULT_DOCFORMAT = 'epytext'
PROFILER = 'profile' #: Which profiler to use: 'hotshot' or 'profile'

######################################################################
#{ Help Topics
######################################################################

DOCFORMATS = ('epytext', 'plaintext', 'restructuredtext', 'javadoc')
HELP_TOPICS = {
    'docformat': textwrap.dedent('''\
        __docformat__ is a module variable that specifies the markup
        language for the docstrings in a module.  Its value is a 
        string, consisting the name of a markup language, optionally 
        followed by a language code (such as "en" for English).  Epydoc
        currently recognizes the following markup language names:
        ''' + ', '.join(DOCFORMATS)),
    'inheritance': textwrap.dedent('''\
        The following inheritance formats are currently supported:
            - grouped: inherited objects are gathered into groups,
              based on what class they were inherited from.
            - listed: inherited objects are listed in a short list
              at the end of their section.
            - included: inherited objects are mixed in with 
              non-inherited objects.'''),
    'css': textwrap.dedent(
        'The following built-in CSS stylesheets are available:\n' +
        '\n'.join(['  %10s: %s' % (key, descr)
                   for (key, (sheet, descr))
                   in CSS_STYLESHEETS.items()])),
    #'checks': textwrap.dedent('''\
    #
    #    '''),
    }
        

HELP_TOPICS['topics'] = wordwrap(
    'Epydoc can provide additional help for the following topics: ' +
    ', '.join(['%r' % topic for topic in HELP_TOPICS.keys()]))
    
######################################################################
#{ Argument & Config File Parsing
######################################################################

OPTION_DEFAULTS = dict(
    action="html", show_frames=True, docformat=DEFAULT_DOCFORMAT, 
    show_private=True, show_imports=False, inheritance="listed",
    verbose=0, quiet=0, load_pickle=False, parse=True, introspect=True,
    debug=epydoc.DEBUG, profile=False, graphs=[],
    list_classes_separately=False, graph_font=None, graph_font_size=None,
    include_source_code=True, pstat_files=[], simple_term=False, fail_on=None,
    exclude=[], exclude_parse=[], exclude_introspect=[],
    external_api=[], external_api_file=[], external_api_root=[],
    redundant_details=False, src_code_tab_width=8)

def parse_arguments():
    # Construct the option parser.
    usage = '%prog [ACTION] [options] NAMES...'
    version = "Epydoc, version %s" % epydoc.__version__
    optparser = OptionParser(usage=usage, add_help_option=False)

    optparser.add_option('--config',
        action='append', dest="configfiles", metavar='FILE',
        help=("A configuration file, specifying additional OPTIONS "
              "and/or NAMES.  This option may be repeated."))

    optparser.add_option("--output", "-o",
        dest="target", metavar="PATH",
        help="The output directory.  If PATH does not exist, then "
        "it will be created.")

    optparser.add_option("--quiet", "-q",
        action="count", dest="quiet",
        help="Decrease the verbosity.")

    optparser.add_option("--verbose", "-v",
        action="count", dest="verbose",
        help="Increase the verbosity.")

    optparser.add_option("--debug",
        action="store_true", dest="debug",
        help="Show full tracebacks for internal errors.")

    optparser.add_option("--simple-term",
        action="store_true", dest="simple_term",
        help="Do not try to use color or cursor control when displaying "
        "the progress bar, warnings, or errors.")


    action_group = OptionGroup(optparser, 'Actions')
    optparser.add_option_group(action_group)

    action_group.add_option("--html",
        action="store_const", dest="action", const="html",
        help="Write HTML output.")

    action_group.add_option("--text",
        action="store_const", dest="action", const="text",
        help="Write plaintext output. (not implemented yet)")

    action_group.add_option("--latex",
        action="store_const", dest="action", const="latex",
        help="Write LaTeX output.")

    action_group.add_option("--dvi",
        action="store_const", dest="action", const="dvi",
        help="Write DVI output.")

    action_group.add_option("--ps",
        action="store_const", dest="action", const="ps",
        help="Write Postscript output.")

    action_group.add_option("--pdf",
        action="store_const", dest="action", const="pdf",
        help="Write PDF output.")

    action_group.add_option("--check",
        action="store_const", dest="action", const="check",
        help="Check completeness of docs.")

    action_group.add_option("--pickle",
        action="store_const", dest="action", const="pickle",
        help="Write the documentation to a pickle file.")

    # Provide our own --help and --version options.
    action_group.add_option("--version",
        action="store_const", dest="action", const="version",
        help="Show epydoc's version number and exit.")

    action_group.add_option("-h", "--help",
        action="store_const", dest="action", const="help",
        help="Show this message and exit.  For help on specific "
        "topics, use \"--help TOPIC\".  Use \"--help topics\" for a "
        "list of available help topics")


    generation_group = OptionGroup(optparser, 'Generation Options')
    optparser.add_option_group(generation_group)

    generation_group.add_option("--docformat",
        dest="docformat", metavar="NAME",
        help="The default markup language for docstrings.  Defaults "
        "to \"%s\"." % DEFAULT_DOCFORMAT)

    generation_group.add_option("--parse-only",
        action="store_false", dest="introspect",
        help="Get all information from parsing (don't introspect)")

    generation_group.add_option("--introspect-only",
        action="store_false", dest="parse",
        help="Get all information from introspecting (don't parse)")

    generation_group.add_option("--exclude",
        dest="exclude", metavar="PATTERN", action="append",
        help="Exclude modules whose dotted name matches "
             "the regular expression PATTERN")

    generation_group.add_option("--exclude-introspect",
        dest="exclude_introspect", metavar="PATTERN", action="append",
        help="Exclude introspection of modules whose dotted name matches "
             "the regular expression PATTERN")

    generation_group.add_option("--exclude-parse",
        dest="exclude_parse", metavar="PATTERN", action="append",
        help="Exclude parsing of modules whose dotted name matches "
             "the regular expression PATTERN")

    generation_group.add_option("--inheritance",
        dest="inheritance", metavar="STYLE",
        help="The format for showing inheritance objects.  STYLE "
        "should be one of: %s." % ', '.join(INHERITANCE_STYLES))

    generation_group.add_option("--show-private",
        action="store_true", dest="show_private",
        help="Include private variables in the output. (default)")

    generation_group.add_option("--no-private",
        action="store_false", dest="show_private",
        help="Do not include private variables in the output.")

    generation_group.add_option("--show-imports",
        action="store_true", dest="show_imports",
        help="List each module's imports.")

    generation_group.add_option("--no-imports",
        action="store_false", dest="show_imports",
        help="Do not list each module's imports. (default)")

    generation_group.add_option('--show-sourcecode',
        action='store_true', dest='include_source_code',
        help=("Include source code with syntax highlighting in the "
              "HTML output. (default)"))

    generation_group.add_option('--no-sourcecode',
        action='store_false', dest='include_source_code',
        help=("Do not include source code with syntax highlighting in the "
              "HTML output."))

    generation_group.add_option('--include-log',
        action='store_true', dest='include_log',
        help=("Include a page with the process log (epydoc-log.html)"))

    generation_group.add_option(
        '--redundant-details',
        action='store_true', dest='redundant_details',
        help=("Include values in the details lists even if all info "
              "about them is already provided by the summary table."))

    output_group = OptionGroup(optparser, 'Output Options')
    optparser.add_option_group(output_group)

    output_group.add_option("--name", "-n",
        dest="prj_name", metavar="NAME",
        help="The documented project's name (for the navigation bar).")

    output_group.add_option("--css", "-c",
        dest="css", metavar="STYLESHEET",
        help="The CSS stylesheet.  STYLESHEET can be either a "
        "builtin stylesheet or the name of a CSS file.")

    output_group.add_option("--url", "-u",
        dest="prj_url", metavar="URL",
        help="The documented project's URL (for the navigation bar).")

    output_group.add_option("--navlink",
        dest="prj_link", metavar="HTML",
        help="HTML code for a navigation link to place in the "
        "navigation bar.")

    output_group.add_option("--top",
        dest="top_page", metavar="PAGE",
        help="The \"top\" page for the HTML documentation.  PAGE can "
        "be a URL, the name of a module or class, or one of the "
        "special names \"trees.html\", \"indices.html\", or \"help.html\"")

    output_group.add_option("--help-file",
        dest="help_file", metavar="FILE",
        help="An alternate help file.  FILE should contain the body "
        "of an HTML file -- navigation bars will be added to it.")

    output_group.add_option("--show-frames",
        action="store_true", dest="show_frames",
        help="Include frames in the HTML output. (default)")

    output_group.add_option("--no-frames",
        action="store_false", dest="show_frames",
        help="Do not include frames in the HTML output.")

    output_group.add_option('--separate-classes',
        action='store_true', dest='list_classes_separately',
        help=("When generating LaTeX or PDF output, list each class in "
              "its own section, instead of listing them under their "
              "containing module."))

    output_group.add_option('--src-code-tab-width',
        action='store', type='int', dest='src_code_tab_width',
        help=("When generating HTML output, sets the number of spaces "
              "each tab in source code listings is replaced with."))
    
    # The group of external API options.
    # Skip if the module couldn't be imported (usually missing docutils)
    if xlink is not None:
        link_group = OptionGroup(optparser,
                                 xlink.ApiLinkReader.settings_spec[0])
        optparser.add_option_group(link_group)

        for help, names, opts in xlink.ApiLinkReader.settings_spec[2]:
            opts = opts.copy()
            opts['help'] = help
            link_group.add_option(*names, **opts)

    graph_group = OptionGroup(optparser, 'Graph Options')
    optparser.add_option_group(graph_group)

    graph_group.add_option('--graph',
        action='append', dest='graphs', metavar='GRAPHTYPE',
        help=("Include graphs of type GRAPHTYPE in the generated output.  "
              "Graphs are generated using the Graphviz dot executable.  "
              "If this executable is not on the path, then use --dotpath "
              "to specify its location.  This option may be repeated to "
              "include multiple graph types in the output.  GRAPHTYPE "
              "should be one of: all, %s." % ', '.join(GRAPH_TYPES)))

    graph_group.add_option("--dotpath",
        dest="dotpath", metavar='PATH',
        help="The path to the Graphviz 'dot' executable.")

    graph_group.add_option('--graph-font',
        dest='graph_font', metavar='FONT',
        help=("Specify the font used to generate Graphviz graphs.  (e.g., "
              "helvetica or times)."))

    graph_group.add_option('--graph-font-size',
        dest='graph_font_size', metavar='SIZE',
        help=("Specify the font size used to generate Graphviz graphs, "
              "in points."))

    graph_group.add_option('--pstat',
        action='append', dest='pstat_files', metavar='FILE',
        help="A pstat output file, to be used in generating call graphs.")

    # this option is for developers, not users.
    graph_group.add_option("--profile-epydoc",
        action="store_true", dest="profile",
        help=SUPPRESS_HELP or
             ("Run the hotshot profiler on epydoc itself.  Output "
              "will be written to profile.out."))


    return_group = OptionGroup(optparser, 'Return Value Options')
    optparser.add_option_group(return_group)

    return_group.add_option("--fail-on-error",
        action="store_const", dest="fail_on", const=log.ERROR,
        help="Return a non-zero exit status, indicating failure, if any "
        "errors are encountered.")

    return_group.add_option("--fail-on-warning",
        action="store_const", dest="fail_on", const=log.WARNING,
        help="Return a non-zero exit status, indicating failure, if any "
        "errors or warnings are encountered (not including docstring "
        "warnings).")

    return_group.add_option("--fail-on-docstring-warning",
        action="store_const", dest="fail_on", const=log.DOCSTRING_WARNING,
        help="Return a non-zero exit status, indicating failure, if any "
        "errors or warnings are encountered (including docstring "
        "warnings).")

    # Set the option parser's defaults.
    optparser.set_defaults(**OPTION_DEFAULTS)

    # Parse the arguments.
    options, names = optparser.parse_args()

    # Print help message, if requested.  We also provide support for
    # --help [topic]
    if options.action == 'help':
        names = set([n.lower() for n in names])
        for (topic, msg) in HELP_TOPICS.items():
            if topic.lower() in names:
                print '\n' + msg.rstrip() + '\n'
                sys.exit(0)
        optparser.print_help()
        sys.exit(0)

    # Print version message, if requested.
    if options.action == 'version':
        print version
        sys.exit(0)
    
    # Process any config files.
    if options.configfiles:
        try:
            parse_configfiles(options.configfiles, options, names)
        except (KeyboardInterrupt,SystemExit): raise
        except Exception, e:
            if len(options.configfiles) == 1:
                cf_name = 'config file %s' % options.configfiles[0]
            else:
                cf_name = 'config files %s' % ', '.join(options.configfiles)
            optparser.error('Error reading %s:\n    %s' % (cf_name, e))

    # Check if the input file is a pickle file.
    for name in names:
        if name.endswith('.pickle'):
            if len(names) != 1:
                optparser.error("When a pickle file is specified, no other "
                               "input files may be specified.")
            options.load_pickle = True
    
    # Check to make sure all options are valid.
    if len(names) == 0:
        optparser.error("No names specified.")
        
    # perform shell expansion.
    for i, name in reversed(list(enumerate(names[:]))):
        if '?' in name or '*' in name:
            names[i:i+1] = glob(name)
        
    if options.inheritance not in INHERITANCE_STYLES:
        optparser.error("Bad inheritance style.  Valid options are " +
                        ",".join(INHERITANCE_STYLES))
    if not options.parse and not options.introspect:
        optparser.error("Invalid option combination: --parse-only "
                        "and --introspect-only.")
    if options.action == 'text' and len(names) > 1:
        optparser.error("--text option takes only one name.")

    # Check the list of requested graph types to make sure they're
    # acceptable.
    options.graphs = [graph_type.lower() for graph_type in options.graphs]
    for graph_type in options.graphs:
        if graph_type == 'callgraph' and not options.pstat_files:
            optparser.error('"callgraph" graph type may only be used if '
                            'one or more pstat files are specified.')
        # If it's 'all', then add everything (but don't add callgraph if
        # we don't have any profiling info to base them on).
        if graph_type == 'all':
            if options.pstat_files:
                options.graphs = GRAPH_TYPES
            else:
                options.graphs = [g for g in GRAPH_TYPES if g != 'callgraph']
            break
        elif graph_type not in GRAPH_TYPES:
            optparser.error("Invalid graph type %s." % graph_type)

    # Calculate verbosity.
    verbosity = getattr(options, 'verbosity', 0)
    options.verbosity = verbosity + options.verbose - options.quiet

    # The target default depends on the action.
    if options.target is None:
        options.target = options.action
    
    # Return parsed args.
    options.names = names
    return options, names

def parse_configfiles(configfiles, options, names):
    configparser = ConfigParser.ConfigParser()
    # ConfigParser.read() silently ignores errors, so open the files
    # manually (since we want to notify the user of any errors).
    for configfile in configfiles:
        fp = open(configfile, 'r') # may raise IOError.
        configparser.readfp(fp, configfile)
        fp.close()
    for optname in configparser.options('epydoc'):
        val = configparser.get('epydoc', optname, vars=os.environ).strip()
        optname = optname.lower().strip()

        if optname in ('modules', 'objects', 'values',
                       'module', 'object', 'value'):
            names.extend(_str_to_list(val))
        elif optname == 'target':
            options.target = val
        elif optname == 'output':
            if val.lower() not in ACTIONS:
                raise ValueError('"%s" expected one of: %s' %
                                 (optname, ', '.join(ACTIONS)))
            options.action = val.lower()
        elif optname == 'verbosity':
            options.verbosity = _str_to_int(val, optname)
        elif optname == 'debug':
            options.debug = _str_to_bool(val, optname)
        elif optname in ('simple-term', 'simple_term'):
            options.simple_term = _str_to_bool(val, optname)

        # Generation options
        elif optname == 'docformat':
            options.docformat = val
        elif optname == 'parse':
            options.parse = _str_to_bool(val, optname)
        elif optname == 'introspect':
            options.introspect = _str_to_bool(val, optname)
        elif optname == 'exclude':
            options.exclude.extend(_str_to_list(val))
        elif optname in ('exclude-parse', 'exclude_parse'):
            options.exclude_parse.extend(_str_to_list(val))
        elif optname in ('exclude-introspect', 'exclude_introspect'):
            options.exclude_introspect.extend(_str_to_list(val))
        elif optname == 'inheritance':
            if val.lower() not in INHERITANCE_STYLES:
                raise ValueError('"%s" expected one of: %s.' %
                                 (optname, ', '.join(INHERITANCE_STYLES)))
            options.inheritance = val.lower()
        elif optname =='private':
            options.show_private = _str_to_bool(val, optname)
        elif optname =='imports':
            options.show_imports = _str_to_bool(val, optname)
        elif optname == 'sourcecode':
            options.include_source_code = _str_to_bool(val, optname)
        elif optname in ('include-log', 'include_log'):
            options.include_log = _str_to_bool(val, optname)
        elif optname in ('redundant-details', 'redundant_details'):
            options.redundant_details = _str_to_bool(val, optname)

        # Output options
        elif optname == 'name':
            options.prj_name = val
        elif optname == 'css':
            options.css = val
        elif optname == 'url':
            options.prj_url = val
        elif optname == 'link':
            options.prj_link = val
        elif optname == 'top':
            options.top_page = val
        elif optname == 'help':
            options.help_file = val
        elif optname =='frames':
            options.show_frames = _str_to_bool(val, optname)
        elif optname in ('separate-classes', 'separate_classes'):
            options.list_classes_separately = _str_to_bool(val, optname)
        elif optname in ('src-code-tab-width', 'src_code_tab_width'):
            options.src_code_tab_width = _str_to_int(val, optname)

        # External API
        elif optname in ('external-api', 'external_api'):
            options.external_api.extend(_str_to_list(val))
        elif optname in ('external-api-file', 'external_api_file'):
            options.external_api_file.extend(_str_to_list(val))
        elif optname in ('external-api-root', 'external_api_root'):
            options.external_api_root.extend(_str_to_list(val))

        # Graph options
        elif optname == 'graph':
            graphtypes = _str_to_list(val)
            for graphtype in graphtypes:
                if graphtype not in GRAPH_TYPES + ('all',):
                    raise ValueError('"%s" expected one of: all, %s.' %
                                     (optname, ', '.join(GRAPH_TYPES)))
            options.graphs.extend(graphtypes)
        elif optname == 'dotpath':
            options.dotpath = val
        elif optname in ('graph-font', 'graph_font'):
            options.graph_font = val
        elif optname in ('graph-font-size', 'graph_font_size'):
            options.graph_font_size = _str_to_int(val, optname)
        elif optname == 'pstat':
            options.pstat_files.extend(_str_to_list(val))

        # Return value options
        elif optname in ('failon', 'fail-on', 'fail_on'):
            if val.lower().strip() in ('error', 'errors'):
                options.fail_on = log.ERROR
            elif val.lower().strip() in ('warning', 'warnings'):
                options.fail_on = log.WARNING
            elif val.lower().strip() in ('docstring_warning',
                                         'docstring_warnings'):
                options.fail_on = log.DOCSTRING_WARNING
            else:
                raise ValueError("%r expected one of: error, warning, "
                                 "docstring_warning" % optname)
        else:
            raise ValueError('Unknown option %s' % optname)

def _str_to_bool(val, optname):
    if val.lower() in ('0', 'no', 'false', 'n', 'f', 'hide'):
        return False
    elif val.lower() in ('1', 'yes', 'true', 'y', 't', 'show'):
        return True
    else:
        raise ValueError('"%s" option expected a boolean' % optname)
        
def _str_to_int(val, optname):
    try:
        return int(val)
    except ValueError:
        raise ValueError('"%s" option expected an int' % optname)

def _str_to_list(val):
    return val.replace(',', ' ').split()

######################################################################
#{ Interface
######################################################################

def main(options, names):
    # Set the debug flag, if '--debug' was specified.
    if options.debug:
        epydoc.DEBUG = True

    ## [XX] Did this serve a purpose?  Commenting out for now:
    #if options.action == 'text':
    #    if options.parse and options.introspect:
    #        options.parse = False

    # Set up the logger
    if options.simple_term:
        TerminalController.FORCE_SIMPLE_TERM = True
    if options.action == 'text':
        logger = None # no logger for text output.
    elif options.verbosity > 1:
        logger = ConsoleLogger(options.verbosity)
        log.register_logger(logger)
    else:
        # Each number is a rough approximation of how long we spend on
        # that task, used to divide up the unified progress bar.
        stages = [40,  # Building documentation
                  7,   # Merging parsed & introspected information
                  1,   # Linking imported variables
                  3,   # Indexing documentation
                  1,   # Checking for overridden methods
                  30,  # Parsing Docstrings
                  1,   # Inheriting documentation
                  2]   # Sorting & Grouping
        if options.load_pickle:
            stages = [30] # Loading pickled documentation
        if options.action == 'html': stages += [100]
        elif options.action == 'text': stages += [30]
        elif options.action == 'latex': stages += [60]
        elif options.action == 'dvi': stages += [60,30]
        elif options.action == 'ps': stages += [60,40]
        elif options.action == 'pdf': stages += [60,50]
        elif options.action == 'check': stages += [10]
        elif options.action == 'pickle': stages += [10]
        else: raise ValueError, '%r not supported' % options.action
        if options.parse and not options.introspect:
            del stages[1] # no merging
        if options.introspect and not options.parse:
            del stages[1:3] # no merging or linking
        logger = UnifiedProgressConsoleLogger(options.verbosity, stages)
        log.register_logger(logger)

    # check the output directory.
    if options.action not in ('text', 'check', 'pickle'):
        if os.path.exists(options.target):
            if not os.path.isdir(options.target):
                log.error("%s is not a directory" % options.target)
                sys.exit(1)

    if options.include_log:
        if options.action == 'html':
            if not os.path.exists(options.target):
                os.mkdir(options.target)
            log.register_logger(HTMLLogger(options.target, options))
        else:
            log.warning("--include-log requires --html")

    # Set the default docformat
    from epydoc import docstringparser
    docstringparser.DEFAULT_DOCFORMAT = options.docformat

    # Configure the external API linking
    if xlink is not None:
        try:
            xlink.ApiLinkReader.read_configuration(options, problematic=False)
        except Exception, exc:
            log.error("Error while configuring external API linking: %s: %s"
                % (exc.__class__.__name__, exc))

    # Set the dot path
    if options.dotpath:
        from epydoc.docwriter import dotgraph
        dotgraph.DOT_COMMAND = options.dotpath

    # Set the default graph font & size
    if options.graph_font:
        from epydoc.docwriter import dotgraph
        fontname = options.graph_font
        dotgraph.DotGraph.DEFAULT_NODE_DEFAULTS['fontname'] = fontname
        dotgraph.DotGraph.DEFAULT_EDGE_DEFAULTS['fontname'] = fontname
    if options.graph_font_size:
        from epydoc.docwriter import dotgraph
        fontsize = options.graph_font_size
        dotgraph.DotGraph.DEFAULT_NODE_DEFAULTS['fontsize'] = fontsize
        dotgraph.DotGraph.DEFAULT_EDGE_DEFAULTS['fontsize'] = fontsize

    # If the input name is a pickle file, then read the docindex that
    # it contains.  Otherwise, build the docs for the input names.
    if options.load_pickle:
        assert len(names) == 1
        log.start_progress('Deserializing')
        log.progress(0.1, 'Loading %r' % names[0])
        t0 = time.time()
        unpickler = pickle.Unpickler(open(names[0], 'rb'))
        unpickler.persistent_load = pickle_persistent_load
        docindex = unpickler.load()
        log.debug('deserialization time: %.1f sec' % (time.time()-t0))
        log.end_progress()
    else:
        # Build docs for the named values.
        from epydoc.docbuilder import build_doc_index
        exclude_parse = '|'.join(options.exclude_parse+options.exclude)
        exclude_introspect = '|'.join(options.exclude_introspect+
                                      options.exclude)
        docindex = build_doc_index(names, options.introspect, options.parse,
                                   add_submodules=(options.action!='text'),
                                   exclude_introspect=exclude_introspect,
                                   exclude_parse=exclude_parse)

    if docindex is None:
        if log.ERROR in logger.reported_message_levels:
            sys.exit(1)
        else:
            return # docbuilder already logged an error.

    # Load profile information, if it was given.
    if options.pstat_files:
        try: import pstats
        except ImportError:
            log.error("Could not import pstats -- ignoring pstat files.")
        try:
            profile_stats = pstats.Stats(options.pstat_files[0])
            for filename in options.pstat_files[1:]:
                profile_stats.add(filename)
        except KeyboardInterrupt: raise
        except Exception, e:
            log.error("Error reading pstat file: %s" % e)
            profile_stats = None
        if profile_stats is not None:
            docindex.read_profiling_info(profile_stats)

    # Perform the specified action.
    if options.action == 'html':
        write_html(docindex, options)
    elif options.action in ('latex', 'dvi', 'ps', 'pdf'):
        write_latex(docindex, options, options.action)
    elif options.action == 'text':
        write_text(docindex, options)
    elif options.action == 'check':
        check_docs(docindex, options)
    elif options.action == 'pickle':
        write_pickle(docindex, options)
    else:
        print >>sys.stderr, '\nUnsupported action %s!' % options.action

    # If we suppressed docstring warnings, then let the user know.
    if logger is not None and logger.suppressed_docstring_warning:
        if logger.suppressed_docstring_warning == 1:
            prefix = '1 markup error was found'
        else:
            prefix = ('%d markup errors were found' %
                      logger.suppressed_docstring_warning)
        log.warning("%s while processing docstrings.  Use the verbose "
                    "switch (-v) to display markup errors." % prefix)

    # Basic timing breakdown:
    if options.verbosity >= 2 and logger is not None:
        logger.print_times()

    # If we encountered any message types that we were requested to
    # fail on, then exit with status 2.
    if options.fail_on is not None:
        max_reported_message_level = max(logger.reported_message_levels)
        if max_reported_message_level >= options.fail_on:
            sys.exit(2)

def write_html(docindex, options):
    from epydoc.docwriter.html import HTMLWriter
    html_writer = HTMLWriter(docindex, **options.__dict__)
    if options.verbose > 0:
        log.start_progress('Writing HTML docs to %r' % options.target)
    else:
        log.start_progress('Writing HTML docs')
    html_writer.write(options.target)
    log.end_progress()

def write_pickle(docindex, options):
    """Helper for writing output to a pickle file, which can then be
    read in at a later time.  But loading the pickle is only marginally
    faster than building the docs from scratch, so this has pretty
    limited application."""
    if options.target == 'pickle':
        options.target = 'api.pickle'
    elif not options.target.endswith('.pickle'):
        options.target += '.pickle'

    log.start_progress('Serializing output')
    log.progress(0.2, 'Writing %r' % options.target)
    outfile = open(options.target, 'wb')
    pickler = pickle.Pickler(outfile, protocol=0)
    pickler.persistent_id = pickle_persistent_id
    pickler.dump(docindex)
    outfile.close()
    log.end_progress()

def pickle_persistent_id(obj):
    """Helper for pickling, which allows us to save and restore UNKNOWN,
    which is required to be identical to apidoc.UNKNOWN."""
    if obj is UNKNOWN: return 'UNKNOWN'
    else: return None

def pickle_persistent_load(identifier):
    """Helper for pickling, which allows us to save and restore UNKNOWN,
    which is required to be identical to apidoc.UNKNOWN."""
    if identifier == 'UNKNOWN': return UNKNOWN
    else: raise pickle.UnpicklingError, 'Invalid persistent id'

_RERUN_LATEX_RE = re.compile(r'(?im)^LaTeX\s+Warning:\s+Label\(s\)\s+may'
                             r'\s+have\s+changed.\s+Rerun')

def write_latex(docindex, options, format):
    from epydoc.docwriter.latex import LatexWriter
    latex_writer = LatexWriter(docindex, **options.__dict__)
    log.start_progress('Writing LaTeX docs')
    latex_writer.write(options.target)
    log.end_progress()
    # If we're just generating the latex, and not any output format,
    # then we're done.
    if format == 'latex': return
    
    if format == 'dvi': steps = 4
    elif format == 'ps': steps = 5
    elif format == 'pdf': steps = 6
    
    log.start_progress('Processing LaTeX docs')
    oldpath = os.path.abspath(os.curdir)
    running = None # keep track of what we're doing.
    try:
        try:
            os.chdir(options.target)

            # Clear any old files out of the way.
            for ext in 'tex aux log out idx ilg toc ind'.split():
                if os.path.exists('apidoc.%s' % ext):
                    os.remove('apidoc.%s' % ext)

            # The first pass generates index files.
            running = 'latex'
            log.progress(0./steps, 'LaTeX: First pass')
            run_subprocess('latex api.tex')

            # Build the index.
            running = 'makeindex'
            log.progress(1./steps, 'LaTeX: Build index')
            run_subprocess('makeindex api.idx')

            # The second pass generates our output.
            running = 'latex'
            log.progress(2./steps, 'LaTeX: Second pass')
            out, err = run_subprocess('latex api.tex')
            
            # The third pass is only necessary if the second pass
            # changed what page some things are on.
            running = 'latex'
            if _RERUN_LATEX_RE.match(out):
                log.progress(3./steps, 'LaTeX: Third pass')
                out, err = run_subprocess('latex api.tex')
 
            # A fourth path should (almost?) never be necessary.
            running = 'latex'
            if _RERUN_LATEX_RE.match(out):
                log.progress(3./steps, 'LaTeX: Fourth pass')
                run_subprocess('latex api.tex')

            # If requested, convert to postscript.
            if format in ('ps', 'pdf'):
                running = 'dvips'
                log.progress(4./steps, 'dvips')
                run_subprocess('dvips api.dvi -o api.ps -G0 -Ppdf')

            # If requested, convert to pdf.
            if format in ('pdf'):
                running = 'ps2pdf'
                log.progress(5./steps, 'ps2pdf')
                run_subprocess(
                    'ps2pdf -sPAPERSIZE#letter -dMaxSubsetPct#100 '
                    '-dSubsetFonts#true -dCompatibilityLevel#1.2 '
                    '-dEmbedAllFonts#true api.ps api.pdf')
        except RunSubprocessError, e:
            if running == 'latex':
                e.out = re.sub(r'(?sm)\A.*?!( LaTeX Error:)?', r'', e.out)
                e.out = re.sub(r'(?sm)\s*Type X to quit.*', '', e.out)
                e.out = re.sub(r'(?sm)^! Emergency stop.*', '', e.out)
            log.error("%s failed: %s" % (running, (e.out+e.err).lstrip()))
        except OSError, e:
            log.error("%s failed: %s" % (running, e))
    finally:
        os.chdir(oldpath)
        log.end_progress()

def write_text(docindex, options):
    log.start_progress('Writing output')
    from epydoc.docwriter.plaintext import PlaintextWriter
    plaintext_writer = PlaintextWriter()
    s = ''
    for apidoc in docindex.root:
        s += plaintext_writer.write(apidoc)
    log.end_progress()
    if isinstance(s, unicode):
        s = s.encode('ascii', 'backslashreplace')
    print s

def check_docs(docindex, options):
    from epydoc.checker import DocChecker
    DocChecker(docindex).check()
                
def cli():
    # Parse command-line arguments.
    options, names = parse_arguments()

    try:
        try:
            if options.profile:
                _profile()
            else:
                main(options, names)
        finally:
            log.close()
    except SystemExit:
        raise
    except KeyboardInterrupt:
        print '\n\n'
        print >>sys.stderr, 'Keyboard interrupt.'
    except:
        if options.debug: raise
        print '\n\n'
        exc_info = sys.exc_info()
        if isinstance(exc_info[0], basestring): e = exc_info[0]
        else: e = exc_info[1]
        print >>sys.stderr, ('\nUNEXPECTED ERROR:\n'
                             '%s\n' % (str(e) or e.__class__.__name__))
        print >>sys.stderr, 'Use --debug to see trace information.'
        sys.exit(3)
    
def _profile():
    # Hotshot profiler.
    if PROFILER == 'hotshot':
        try: import hotshot, hotshot.stats
        except ImportError:
            print >>sys.stderr, "Could not import profile module!"
            return
        try:
            prof = hotshot.Profile('hotshot.out')
            prof = prof.runctx('main(*parse_arguments())', globals(), {})
        except SystemExit:
            pass
        prof.close()
        # Convert profile.hotshot -> profile.out
        print 'Consolidating hotshot profiling info...'
        hotshot.stats.load('hotshot.out').dump_stats('profile.out')

    # Standard 'profile' profiler.
    elif PROFILER == 'profile':
        # cProfile module was added in Python 2.5 -- use it if its'
        # available, since it's faster.
        try: from cProfile import Profile
        except ImportError:
            try: from profile import Profile
            except ImportError:
                print >>sys.stderr, "Could not import profile module!"
                return

        # There was a bug in Python 2.4's profiler.  Check if it's
        # present, and if so, fix it.  (Bug was fixed in 2.4maint:
        # <http://mail.python.org/pipermail/python-checkins/
        #                         2005-September/047099.html>)
        if (hasattr(Profile, 'dispatch') and
            Profile.dispatch['c_exception'] is
            Profile.trace_dispatch_exception.im_func):
            trace_dispatch_return = Profile.trace_dispatch_return.im_func
            Profile.dispatch['c_exception'] = trace_dispatch_return
        try:
            prof = Profile()
            prof = prof.runctx('main(*parse_arguments())', globals(), {})
        except SystemExit:
            pass
        prof.dump_stats('profile.out')

    else:
        print >>sys.stderr, 'Unknown profiler %s' % PROFILER
        return
    
######################################################################
#{ Logging
######################################################################
    
class TerminalController:
    """
    A class that can be used to portably generate formatted output to
    a terminal.  See
    U{http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/475116}
    for documentation.  (This is a somewhat stripped-down version.)
    """
    BOL = ''             #: Move the cursor to the beginning of the line
    UP = ''              #: Move the cursor up one line
    DOWN = ''            #: Move the cursor down one line
    LEFT = ''            #: Move the cursor left one char
    RIGHT = ''           #: Move the cursor right one char
    CLEAR_EOL = ''       #: Clear to the end of the line.
    CLEAR_LINE = ''      #: Clear the current line; cursor to BOL.
    BOLD = ''            #: Turn on bold mode
    NORMAL = ''          #: Turn off all modes
    COLS = 75            #: Width of the terminal (default to 75)
    BLACK = BLUE = GREEN = CYAN = RED = MAGENTA = YELLOW = WHITE = ''
    
    _STRING_CAPABILITIES = """
    BOL=cr UP=cuu1 DOWN=cud1 LEFT=cub1 RIGHT=cuf1
    CLEAR_EOL=el BOLD=bold UNDERLINE=smul NORMAL=sgr0""".split()
    _COLORS = """BLACK BLUE GREEN CYAN RED MAGENTA YELLOW WHITE""".split()
    _ANSICOLORS = "BLACK RED GREEN YELLOW BLUE MAGENTA CYAN WHITE".split()

    #: If this is set to true, then new TerminalControllers will
    #: assume that the terminal is not capable of doing manipulation
    #: of any kind.
    FORCE_SIMPLE_TERM = False

    def __init__(self, term_stream=sys.stdout):
        # If the stream isn't a tty, then assume it has no capabilities.
        if not term_stream.isatty(): return
        if self.FORCE_SIMPLE_TERM: return

        # Curses isn't available on all platforms
        try: import curses
        except:
            # If it's not available, then try faking enough to get a
            # simple progress bar.
            self.BOL = '\r'
            self.CLEAR_LINE = '\r' + ' '*self.COLS + '\r'
            
        # Check the terminal type.  If we fail, then assume that the
        # terminal has no capabilities.
        try: curses.setupterm()
        except: return

        # Look up numeric capabilities.
        self.COLS = curses.tigetnum('cols')
        
        # Look up string capabilities.
        for capability in self._STRING_CAPABILITIES:
            (attrib, cap_name) = capability.split('=')
            setattr(self, attrib, self._tigetstr(cap_name) or '')
        if self.BOL and self.CLEAR_EOL:
            self.CLEAR_LINE = self.BOL+self.CLEAR_EOL

        # Colors
        set_fg = self._tigetstr('setf')
        if set_fg:
            for i,color in zip(range(len(self._COLORS)), self._COLORS):
                setattr(self, color, curses.tparm(set_fg, i) or '')
        set_fg_ansi = self._tigetstr('setaf')
        if set_fg_ansi:
            for i,color in zip(range(len(self._ANSICOLORS)), self._ANSICOLORS):
                setattr(self, color, curses.tparm(set_fg_ansi, i) or '')

    def _tigetstr(self, cap_name):
        # String capabilities can include "delays" of the form "$<2>".
        # For any modern terminal, we should be able to just ignore
        # these, so strip them out.
        import curses
        cap = curses.tigetstr(cap_name) or ''
        return re.sub(r'\$<\d+>[/*]?', '', cap)

class ConsoleLogger(log.Logger):
    def __init__(self, verbosity, progress_mode=None):
        self._verbosity = verbosity
        self._progress = None
        self._message_blocks = []
        # For ETA display:
        self._progress_start_time = None
        # For per-task times:
        self._task_times = []
        self._progress_header = None

        self.reported_message_levels = set()
        """This set contains all the message levels (WARNING, ERROR,
        etc) that have been reported.  It is used by the options
        --fail-on-warning etc to determine the return value."""
        
        self.suppressed_docstring_warning = 0
        """This variable will be incremented once every time a
        docstring warning is reported tothe logger, but the verbosity
        level is too low for it to be displayed."""

        self.term = TerminalController()

        # Set the progress bar mode.
        if verbosity >= 2: self._progress_mode = 'list'
        elif verbosity >= 0:
            if progress_mode is not None:
                self._progress_mode = progress_mode
            elif self.term.COLS < 15:
                self._progress_mode = 'simple-bar'
            elif self.term.BOL and self.term.CLEAR_EOL and self.term.UP:
                self._progress_mode = 'multiline-bar'
            elif self.term.BOL and self.term.CLEAR_LINE:
                self._progress_mode = 'bar'
            else:
                self._progress_mode = 'simple-bar'
        else: self._progress_mode = 'hide'

    def start_block(self, header):
        self._message_blocks.append( (header, []) )

    def end_block(self):
        header, messages = self._message_blocks.pop()
        if messages:
            width = self.term.COLS - 5 - 2*len(self._message_blocks)
            prefix = self.term.CYAN+self.term.BOLD+'| '+self.term.NORMAL
            divider = (self.term.CYAN+self.term.BOLD+'+'+'-'*(width-1)+
                       self.term.NORMAL)
            # Mark up the header:
            header = wordwrap(header, right=width-2, splitchars='\\/').rstrip()
            header = '\n'.join([prefix+self.term.CYAN+l+self.term.NORMAL
                                for l in header.split('\n')])
            # Construct the body:
            body = ''
            for message in messages:
                if message.endswith('\n'): body += message
                else: body += message+'\n'
            # Indent the body:
            body = '\n'.join([prefix+'  '+l for l in body.split('\n')])
            # Put it all together:
            message = divider + '\n' + header + '\n' + body + '\n'
            self._report(message)
            
    def _format(self, prefix, message, color):
        """
        Rewrap the message; but preserve newlines, and don't touch any
        lines that begin with spaces.
        """
        lines = message.split('\n')
        startindex = indent = len(prefix)
        for i in range(len(lines)):
            if lines[i].startswith(' '):
                lines[i] = ' '*(indent-startindex) + lines[i] + '\n'
            else:
                width = self.term.COLS - 5 - 4*len(self._message_blocks)
                lines[i] = wordwrap(lines[i], indent, width, startindex, '\\/')
            startindex = 0
        return color+prefix+self.term.NORMAL+''.join(lines)

    def log(self, level, message):
        self.reported_message_levels.add(level)
        if self._verbosity >= -2 and level >= log.ERROR:
            message = self._format('  Error: ', message, self.term.RED)
        elif self._verbosity >= -1 and level >= log.WARNING:
            message = self._format('Warning: ', message, self.term.YELLOW)
        elif self._verbosity >= 1 and level >= log.DOCSTRING_WARNING:
            message = self._format('Warning: ', message, self.term.YELLOW)
        elif self._verbosity >= 3 and level >= log.INFO:
            message = self._format('   Info: ', message, self.term.NORMAL)
        elif epydoc.DEBUG and level == log.DEBUG:
            message = self._format('  Debug: ', message, self.term.CYAN)
        else:
            if level >= log.DOCSTRING_WARNING:
                self.suppressed_docstring_warning += 1
            return
            
        self._report(message)

    def _report(self, message):
        if not message.endswith('\n'): message += '\n'
        
        if self._message_blocks:
            self._message_blocks[-1][-1].append(message)
        else:
            # If we're in the middle of displaying a progress bar,
            # then make room for the message.
            if self._progress_mode == 'simple-bar':
                if self._progress is not None:
                    print
                    self._progress = None
            if self._progress_mode == 'bar':
                sys.stdout.write(self.term.CLEAR_LINE)
            if self._progress_mode == 'multiline-bar':
                sys.stdout.write((self.term.CLEAR_EOL + '\n')*2 +
                                 self.term.CLEAR_EOL + self.term.UP*2)

            # Display the message message.
            sys.stdout.write(message)
            sys.stdout.flush()
                
    def progress(self, percent, message=''):
        percent = min(1.0, percent)
        message = '%s' % message
        
        if self._progress_mode == 'list':
            if message:
                print '[%3d%%] %s' % (100*percent, message)
                sys.stdout.flush()
                
        elif self._progress_mode == 'bar':
            dots = int((self.term.COLS/2-8)*percent)
            background = '-'*(self.term.COLS/2-8)
            if len(message) > self.term.COLS/2:
                message = message[:self.term.COLS/2-3]+'...'
            sys.stdout.write(self.term.CLEAR_LINE + '%3d%% '%(100*percent) +
                             self.term.GREEN + '[' + self.term.BOLD +
                             '='*dots + background[dots:] + self.term.NORMAL +
                             self.term.GREEN + '] ' + self.term.NORMAL +
                             message + self.term.BOL)
            sys.stdout.flush()
            self._progress = percent
        elif self._progress_mode == 'multiline-bar':
            dots = int((self.term.COLS-10)*percent)
            background = '-'*(self.term.COLS-10)
            
            if len(message) > self.term.COLS-10:
                message = message[:self.term.COLS-10-3]+'...'
            else:
                message = message.center(self.term.COLS-10)

            time_elapsed = time.time()-self._progress_start_time
            if percent > 0:
                time_remain = (time_elapsed / percent) * (1-percent)
            else:
                time_remain = 0

            sys.stdout.write(
                # Line 1:
                self.term.CLEAR_EOL + '      ' +
                '%-8s' % self._timestr(time_elapsed) +
                self.term.BOLD + 'Progress:'.center(self.term.COLS-26) +
                self.term.NORMAL + '%8s' % self._timestr(time_remain) + '\n' +
                # Line 2:
                self.term.CLEAR_EOL + ('%3d%% ' % (100*percent)) +
                self.term.GREEN + '[' +  self.term.BOLD + '='*dots +
                background[dots:] + self.term.NORMAL + self.term.GREEN +
                ']' + self.term.NORMAL + '\n' +
                # Line 3:
                self.term.CLEAR_EOL + '      ' + message + self.term.BOL +
                self.term.UP + self.term.UP)
            
            sys.stdout.flush()
            self._progress = percent
        elif self._progress_mode == 'simple-bar':
            if self._progress is None:
                sys.stdout.write('  [')
                self._progress = 0.0
            dots = int((self.term.COLS-2)*percent)
            progress_dots = int((self.term.COLS-2)*self._progress)
            if dots > progress_dots:
                sys.stdout.write('.'*(dots-progress_dots))
                sys.stdout.flush()
                self._progress = percent

    def _timestr(self, dt):
        dt = int(dt)
        if dt >= 3600:
            return '%d:%02d:%02d' % (dt/3600, dt%3600/60, dt%60)
        else:
            return '%02d:%02d' % (dt/60, dt%60)

    def start_progress(self, header=None):
        if self._progress is not None:
            raise ValueError
        self._progress = None
        self._progress_start_time = time.time()
        self._progress_header = header
        if self._progress_mode != 'hide' and header:
            print self.term.BOLD + header + self.term.NORMAL

    def end_progress(self):
        self.progress(1.)
        if self._progress_mode == 'bar':
            sys.stdout.write(self.term.CLEAR_LINE)
        if self._progress_mode == 'multiline-bar':
                sys.stdout.write((self.term.CLEAR_EOL + '\n')*2 +
                                 self.term.CLEAR_EOL + self.term.UP*2)
        if self._progress_mode == 'simple-bar':
            print ']'
        self._progress = None
        self._task_times.append( (time.time()-self._progress_start_time,
                                  self._progress_header) )

    def print_times(self):
        print
        print 'Timing summary:'
        total = sum([time for (time, task) in self._task_times])
        max_t = max([time for (time, task) in self._task_times])
        for (time, task) in self._task_times:
            task = task[:31]
            print '  %s%s %7.1fs' % (task, '.'*(35-len(task)), time),
            if self.term.COLS > 55:
                print '|'+'=' * int((self.term.COLS-53) * time / max_t)
            else:
                print
        print

class UnifiedProgressConsoleLogger(ConsoleLogger):
    def __init__(self, verbosity, stages, progress_mode=None):
        self.stage = 0
        self.stages = stages
        self.task = None
        ConsoleLogger.__init__(self, verbosity, progress_mode)
        
    def progress(self, percent, message=''):
        #p = float(self.stage-1+percent)/self.stages
        i = self.stage-1
        p = ((sum(self.stages[:i]) + percent*self.stages[i]) /
             float(sum(self.stages)))

        if message is UNKNOWN: message = None
        if message: message = '%s: %s' % (self.task, message)
        ConsoleLogger.progress(self, p, message)

    def start_progress(self, header=None):
        self.task = header
        if self.stage == 0:
            ConsoleLogger.start_progress(self)
        self.stage += 1

    def end_progress(self):
        if self.stage == len(self.stages):
            ConsoleLogger.end_progress(self)

    def print_times(self):
        pass

class HTMLLogger(log.Logger):
    """
    A logger used to generate a log of all warnings and messages to an
    HTML file.
    """
    
    FILENAME = "epydoc-log.html"
    HEADER = textwrap.dedent('''\
        <?xml version="1.0" encoding="ascii"?>
        <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
                  "DTD/xhtml1-transitional.dtd">
        <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
        <head>
          <title>Epydoc Log</title>
          <link rel="stylesheet" href="epydoc.css" type="text/css" />
        </head>
        
        <body bgcolor="white" text="black" link="blue" vlink="#204080"
              alink="#204080">
        <h1 class="epydoc">Epydoc Log</h1>
        <p class="log">Epydoc started at %s</p>''')
    START_BLOCK = '<div class="log-block"><h2 class="log-hdr">%s</h2>'
    MESSAGE = ('<div class="log-%s"><b>%s</b>: \n'
               '%s</div>\n')
    END_BLOCK = '</div>'
    FOOTER = "</body>\n</html>\n"
    
    def __init__(self, directory, options):
        self.start_time = time.time()
        self.out = open(os.path.join(directory, self.FILENAME), 'w')
        self.out.write(self.HEADER % time.ctime(self.start_time))
        self.is_empty = True
        self.options = options

    def write_options(self, options):
        self.out.write(self.START_BLOCK % 'Epydoc Options')
        msg = '<table border="0" cellpadding="0" cellspacing="0">\n'
        opts = [(key, getattr(options, key)) for key in dir(options)
                if key not in dir(optparse.Values)]
        opts = [(val==OPTION_DEFAULTS.get(key), key, val)
                for (key, val) in opts]
        for is_default, key, val in sorted(opts):
            css = is_default and 'opt-default' or 'opt-changed'
            msg += ('<tr valign="top" class="%s"><td valign="top">%s</td>'
                    '<td valign="top"><tt>&nbsp;=&nbsp;</tt></td>'
                    '<td valign="top"><tt>%s</tt></td></tr>' %
                    (css, key, plaintext_to_html(repr(val))))
        msg += '</table>\n'
        self.out.write('<div class="log-info">\n%s</div>\n' % msg)
        self.out.write(self.END_BLOCK)

    def start_block(self, header):
        self.out.write(self.START_BLOCK % header)

    def end_block(self):
        self.out.write(self.END_BLOCK)

    def log(self, level, message):
        if message.endswith("(-v) to display markup errors."): return
        if level >= log.ERROR:
            self.out.write(self._message('error', message))
        elif level >= log.WARNING:
            self.out.write(self._message('warning', message))
        elif level >= log.DOCSTRING_WARNING:
            self.out.write(self._message('docstring warning', message))

    def _message(self, level, message):
        self.is_empty = False
        message = plaintext_to_html(message)
        if '\n' in message:
            message = '<pre class="log">%s</pre>' % message
        hdr = ' '.join([w.capitalize() for w in level.split()])
        return self.MESSAGE % (level.split()[-1], hdr, message)

    def close(self):
        if self.is_empty:
            self.out.write('<div class="log-info">'
                           'No warnings or errors!</div>')
        self.write_options(self.options)
        self.out.write('<p class="log">Epydoc finished at %s</p>\n'
                       '<p class="log">(Elapsed time: %s)</p>' %
                       (time.ctime(), self._elapsed_time()))
        self.out.write(self.FOOTER)
        self.out.close()

    def _elapsed_time(self):
        secs = int(time.time()-self.start_time)
        if secs < 60:
            return '%d seconds' % secs
        if secs < 3600:
            return '%d minutes, %d seconds' % (secs/60, secs%60)
        else:
            return '%d hours, %d minutes' % (secs/3600, secs%3600)
            

######################################################################
## main
######################################################################

if __name__ == '__main__':
    cli()


########NEW FILE########
__FILENAME__ = compat
# epydoc -- Backwards compatibility
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: util.py 956 2006-03-10 01:30:51Z edloper $

"""
Backwards compatibility with previous versions of Python.

This module provides backwards compatibility by defining several
functions and classes that were not available in earlier versions of
Python.  Intented usage:

    >>> from epydoc.compat import *

Currently, epydoc requires Python 2.3+.
"""
__docformat__ = 'epytext'

######################################################################
#{ New in Python 2.4
######################################################################

# set
try:
    set
except NameError:
    try:
        from sets import Set as set, ImmutableSet as frozenset
    except ImportError:
        pass # use fallback, in the next section.

# sorted
try: 
    sorted
except NameError:
    def sorted(iterable, cmp=None, key=None, reverse=False):
        if key is None:
            elts = list(iterable)
        else:
            elts = [(key(v), v) for v in iterable]

        if reverse: elts.reverse() # stable sort.
        if cmp is None: elts.sort()
        else: elts.sort(cmp)
        if reverse: elts.reverse()
    
        if key is None:
            return elts
        else:
            return [v for (k,v) in elts]

# reversed
try: 
    reversed
except NameError:
    def reversed(iterable):
        elts = list(iterable)
        elts.reverse()
        return elts

######################################################################
#{ New in Python 2.3
######################################################################
# Below is my initial attempt at backporting enough code that 
# epydoc 3 would run under python 2.2.  However, I'm starting
# to think that it's not worth the trouble.  At the very least,
# epydoc's current unicode handling still doesn't work under
# 2.2 (after the backports below), since the 'xmlcharrefreplace'
# error handler was introduced in python 2.3.

# # basestring
# try:
#     basestring
# except NameError:
#     basestring = (str, unicode)

# # sum
# try:
#     sum
# except NameError:
#     def _add(a,b): return a+b
#     def sum(vals): return reduce(_add, vals, 0)

# # True & False
# try:
#     True
# except NameError:
#     True = 1
#     False = 0

# # enumerate
# try:
#     enumerate
# except NameError:
#     def enumerate(iterable):
#         lst = list(iterable)
#         return zip(range(len(lst)), lst)

# # set
# try:
#     set
# except NameError:
#     class set(dict):
#         def __init__(self, elts=()):
#             dict.__init__(self, [(e,1) for e in elts])
#         def __repr__(self):
#             return 'set(%r)' % list(self)
#         def add(self, key): self[key] = 1
#         def copy(self):
#             return set(dict.copy(self))
#         def difference(self, other):
#             return set([v for v in self if v not in other])
#         def difference_udpate(self, other):
#             newval = self.difference(other)
#             self.clear(); self.update(newval)
#         def discard(self, elt):
#             try: del self[elt]
#             except: pass
#         def intersection(self, other):
#             return self.copy().update(other)
#         def intersection_update(self, other):
#             newval = self.intersection(other)
#             self.clear(); self.update(newval)
#         def issubset(self, other):
#             for elt in self:
#                 if elt not in other: return False
#             return True
#         def issuperset(self, other):
#             for elt in other:
#                 if elt not in self: return False
#             return True
#         def pop(self): self.popitem()[0]
#         def remove(self, elt): del self[elt]
#         def symmetric_difference(self, other):
#             return set([v for v in list(self)+list(other)
#                         if (v in self)^(v in other)])
#         def symmatric_difference_update(self, other):
#             newval = self.symmetric_difference(other)
#             self.clear(); self.update(newval)
#         def union(self, other):
#             return set([v for v in list(self)+list(other)
#                         if (v in self) or (v in other)])
#         def union_update(self, other):
#             newval = self.union(other)
#             self.clear(); self.update(newval)
#         def update(self, other):
#             dict.update(self, set(other))

# # optparse module
# try:
#     import optparse
# except ImportError:
#     import new, sys, getopt
#     class _OptionVals:
#         def __init__(self, vals): self.__dict__.update(vals)
#     class OptionParser:
#         def __init__(self, usage=None, version=None):
#             self.usage = usage
#             self.version = version
#             self.shortops = ['h']
#             self.longops = []
#             self.option_specs = {}
#             self.defaults = {}
#         def fail(self, message, exitval=1):
#             print >>sys.stderr, message
#             system.exit(exitval)
#         def add_option_group(self, group): pass
#         def set_defaults(self, **defaults):
#             self.defaults = defaults.copy()
#         def parse_args(self):
#             try:
#                 (opts, names) = getopt.getopt(sys.argv[1:],
#                                               ''.join(self.shortops),
#                                               self.longops)
#             except getopt.GetoptError, e:
#                 self.fail(e)

#             options = self.defaults.copy()
#             for (opt,val) in opts:
#                 if opt == '-h':
#                     self.fail('No help available')
#                 if opt not in self.option_specs:
#                     self.fail('Unknown option %s' % opt)
#                 (action, dest, const) = self.option_specs[opt]
#                 if action == 'store':
#                     options[dest] = val
#                 elif action == 'store_const':
#                     options[dest] = const
#                 elif action == 'count':
#                     options[dest] = options.get(dest,0)+1
#                 elif action == 'append':
#                     options.setdefault(dest, []).append(val)
#                 else:
#                     self.fail('unsupported action: %s' % action)
#             for (action,dest,const) in self.option_specs.values():
#                 if dest not in options:
#                     if action == 'count': options[dest] = 0
#                     elif action == 'append': options[dest] = []
#                     else: options[dest] = None
#             for name in names:
#                 if name.startswith('-'):
#                     self.fail('names must follow options')
#             return _OptionVals(options), names
#     class OptionGroup:
#         def __init__(self, optparser, name):
#             self.optparser = optparser
#             self.name = name

#         def add_option(self, *args, **kwargs):
#             action = 'store'
#             dest = None
#             const = None
#             for (key,val) in kwargs.items():
#                 if key == 'action': action = val
#                 elif key == 'dest': dest = val
#                 elif key == 'const': const = val
#                 elif key in ('help', 'metavar'): pass
#                 else: self.fail('unsupported: %s' % key)

#             if action not in ('store_const', 'store_true', 'store_false',
#                               'store', 'count', 'append'):
#                 self.fail('unsupported action: %s' % action)

#             optparser = self.optparser
#             for arg in args:
#                 if arg.startswith('--'):
#                     optparser.longops.append(arg[2:])
#                 elif arg.startswith('-') and len(arg)==2:
#                     optparser.shortops += arg[1]
#                     if action in ('store', 'append'):
#                         optparser.shortops += ':'
#                 else:
#                     self.fail('bad option name %s' % arg)
#                 if action == 'store_true':
#                     (action, const) = ('store_const', True)
#                 if action == 'store_false':
#                     (action, const) = ('store_const', False)
#                 optparser.option_specs[arg] = (action, dest, const)

#     # Install a fake module.
#     optparse = new.module('optparse')
#     optparse.OptionParser = OptionParser
#     optparse.OptionGroup = OptionGroup
#     sys.modules['optparse'] = optparse
#     # Clean up
#     del OptionParser, OptionGroup


########NEW FILE########
__FILENAME__ = docbuilder
# epydoc -- Documentation Builder
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: docbuilder.py 1683 2008-01-29 22:17:39Z edloper $

"""
Construct data structures that encode the API documentation for Python
objects.  These data structures are created using a series of steps:

  1. B{Building docs}: Extract basic information about the objects,
     and objects that are related to them.  This can be done by
     introspecting the objects' values (with L{epydoc.docintrospecter}; or
     by parsing their source code (with L{epydoc.docparser}.

  2. B{Merging}: Combine the information obtained from introspection &
     parsing each object into a single structure.

  3. B{Linking}: Replace any 'pointers' that were created for imported
     variables by their target (if it's available).
  
  4. B{Naming}: Chose a unique 'canonical name' for each
     object.
  
  5. B{Docstring Parsing}: Parse the docstring of each object, and
     extract any pertinant information.
  
  6. B{Inheritance}: Add information about variables that classes
     inherit from their base classes.

The documentation information for each individual object is
represented using an L{APIDoc}; and the documentation for a collection
of objects is represented using a L{DocIndex}.

The main interface to C{epydoc.docbuilder} consists of two functions:

  - L{build_doc()} -- Builds documentation for a single item, and
    returns it as an L{APIDoc} object.
  - L{build_doc_index()} -- Builds documentation for a collection of
    items, and returns it as a L{DocIndex} object.

The remaining functions are used by these two main functions to
perform individual steps in the creation of the documentation.

@group Documentation Construction: build_doc, build_doc_index,
    _get_docs_from_*, _report_valdoc_progress
@group Merging: *MERGE*, *merge*
@group Linking: link_imports
@group Naming: _name_scores, _unreachable_names, assign_canonical_names,
    _var_shadows_self, _fix_self_shadowing_var, _unreachable_name_for
@group Inheritance: inherit_docs, _inherit_info
"""
__docformat__ = 'epytext en'

######################################################################
## Contents
######################################################################
## 1. build_doc() & build_doc_index() -- the main interface.
## 2. merge_docs() -- helper, used to merge parse & introspect info
## 3. link_imports() -- helper, used to connect imported vars w/ values
## 4. assign_canonical_names() -- helper, used to set canonical names
## 5. inherit_docs() -- helper, used to inherit docs from base classes

######################################################################
## Imports
######################################################################

import sys, os, os.path, __builtin__, imp, re, inspect
from epydoc.apidoc import *
from epydoc.docintrospecter import introspect_docs
from epydoc.docparser import parse_docs, ParseError
from epydoc.docstringparser import parse_docstring
from epydoc import log
from epydoc.util import *
from epydoc.compat import * # Backwards compatibility

######################################################################
## 1. build_doc()
######################################################################

class BuildOptions:
    """
    Holds the parameters for a documentation building process.
    """
    def __init__(self, introspect=True, parse=True,
                 exclude_introspect=None, exclude_parse=None,
                 add_submodules=True):
        self.introspect = introspect
        self.parse = parse
        self.exclude_introspect = exclude_introspect
        self.exclude_parse = exclude_parse
        self.add_submodules = add_submodules

        # Test for pattern syntax and compile them into pattern objects.
        try:
            self._introspect_regexp = (exclude_introspect
                and re.compile(exclude_introspect) or None)
            self._parse_regexp = (exclude_parse
                and re.compile(exclude_parse) or None)
        except Exception, exc:
            log.error('Error in regular expression pattern: %s' % exc)
            raise

    def must_introspect(self, name):
        """
        Return C{True} if a module is to be introsepcted with the current
        settings.

        @param name: The name of the module to test
        @type name: L{DottedName} or C{str}
        """
        return self.introspect \
            and not self._matches_filter(name, self._introspect_regexp)

    def must_parse(self, name):
        """
        Return C{True} if a module is to be parsed with the current settings.

        @param name: The name of the module to test
        @type name: L{DottedName} or C{str}
        """
        return self.parse \
            and not self._matches_filter(name, self._parse_regexp)

    def _matches_filter(self, name, regexp):
        """
        Test if a module name matches a pattern.

        @param name: The name of the module to test
        @type name: L{DottedName} or C{str}
        @param regexp: The pattern object to match C{name} against.
            If C{None}, return C{False}
        @type regexp: C{pattern}
        @return: C{True} if C{name} in dotted format matches C{regexp},
            else C{False}
        @rtype: C{bool}
        """
        if regexp is None: return False

        if isinstance(name, DottedName):
            name = str(name)

        return bool(regexp.search(name))


def build_doc(item, introspect=True, parse=True, add_submodules=True,
              exclude_introspect=None, exclude_parse=None):
    """
    Build API documentation for a given item, and return it as
    an L{APIDoc} object.

    @rtype: L{APIDoc}
    @param item: The item to document, specified using any of the
        following:
          - A string, naming a python package directory
            (e.g., C{'epydoc/markup'})
          - A string, naming a python file
            (e.g., C{'epydoc/docparser.py'})
          - A string, naming a python object
            (e.g., C{'epydoc.docparser.DocParser'})
          - Any (non-string) python object
            (e.g., C{list.append})
    @param introspect: If true, then use introspection to examine the
        specified items.  Otherwise, just use parsing.
    @param parse: If true, then use parsing to examine the specified
        items.  Otherwise, just use introspection.
    """
    docindex = build_doc_index([item], introspect, parse, add_submodules,
                               exclude_introspect=exclude_introspect,
                               exclude_parse=exclude_parse)
    return docindex.root[0]

def build_doc_index(items, introspect=True, parse=True, add_submodules=True,
                    exclude_introspect=None, exclude_parse=None):
    """
    Build API documentation for the given list of items, and
    return it in the form of a L{DocIndex}.

    @rtype: L{DocIndex}
    @param items: The items to document, specified using any of the
        following:
          - A string, naming a python package directory
            (e.g., C{'epydoc/markup'})
          - A string, naming a python file
            (e.g., C{'epydoc/docparser.py'})
          - A string, naming a python object
            (e.g., C{'epydoc.docparser.DocParser'})
          - Any (non-string) python object
            (e.g., C{list.append})
    @param introspect: If true, then use introspection to examine the
        specified items.  Otherwise, just use parsing.
    @param parse: If true, then use parsing to examine the specified
        items.  Otherwise, just use introspection.
    """
    try:
        options = BuildOptions(parse=parse, introspect=introspect,
            exclude_introspect=exclude_introspect, exclude_parse=exclude_parse,
            add_submodules=add_submodules)
    except Exception, e:
        # log.error already reported by constructor.
        return None

    # Get the basic docs for each item.
    doc_pairs = _get_docs_from_items(items, options)

    # Merge the introspection & parse docs.
    if options.parse and options.introspect:
        log.start_progress('Merging parsed & introspected information')
        docs = []
        for i, (introspect_doc, parse_doc) in enumerate(doc_pairs):
            if introspect_doc is not None and parse_doc is not None:
                if introspect_doc.canonical_name not in (None, UNKNOWN):
                    name = introspect_doc.canonical_name
                else:
                    name = parse_doc.canonical_name
                log.progress(float(i)/len(doc_pairs), name)
                docs.append(merge_docs(introspect_doc, parse_doc))
            elif introspect_doc is not None:
                docs.append(introspect_doc)
            elif parse_doc is not None:
                docs.append(parse_doc)
        log.end_progress()
    elif options.introspect:
        docs = [doc_pair[0] for doc_pair in doc_pairs if doc_pair[0]]
    else:
        docs = [doc_pair[1] for doc_pair in doc_pairs if doc_pair[1]]

    if len(docs) == 0:
        log.error('Nothing left to document!')
        return None

    # Collect the docs into a single index.
    docindex = DocIndex(docs)

    # Replace any proxy valuedocs that we got from importing with
    # their targets.
    if options.parse:
        log.start_progress('Linking imported variables')
        valdocs = sorted(docindex.reachable_valdocs(
            imports=False, submodules=False, packages=False, subclasses=False))
        for i, val_doc in enumerate(valdocs):
            _report_valdoc_progress(i, val_doc, valdocs)
            link_imports(val_doc, docindex)
        log.end_progress()

    # Assign canonical names.
    log.start_progress('Indexing documentation')
    for i, val_doc in enumerate(docindex.root):
        log.progress(float(i)/len(docindex.root), val_doc.canonical_name)
        assign_canonical_names(val_doc, val_doc.canonical_name, docindex)
    log.end_progress()

    # Set overrides pointers
    log.start_progress('Checking for overridden methods')
    valdocs = sorted(docindex.reachable_valdocs(
        imports=False, submodules=False, packages=False, subclasses=False))
    for i, val_doc in enumerate(valdocs):
        if isinstance(val_doc, ClassDoc):
            percent = float(i)/len(valdocs)
            log.progress(percent, val_doc.canonical_name)
            find_overrides(val_doc)
    log.end_progress()
    
    # Parse the docstrings for each object.
    log.start_progress('Parsing docstrings')
    suppress_warnings = set(valdocs).difference(
        docindex.reachable_valdocs(
            imports=False, submodules=False, packages=False, subclasses=False,
            bases=False, overrides=True))
    for i, val_doc in enumerate(valdocs):
        _report_valdoc_progress(i, val_doc, valdocs)
        # the value's docstring
        parse_docstring(val_doc, docindex, suppress_warnings)
        # the value's variables' docstrings
        if (isinstance(val_doc, NamespaceDoc) and
            val_doc.variables not in (None, UNKNOWN)):
            for var_doc in val_doc.variables.values():
                # Now we have a chance to propagate the defining module
                # to objects for which introspection is not possible,
                # such as properties.
                if (isinstance(var_doc.value, ValueDoc)
                    and var_doc.value.defining_module is UNKNOWN):
                    var_doc.value.defining_module = val_doc.defining_module
                parse_docstring(var_doc, docindex, suppress_warnings)
    log.end_progress()

    # Take care of inheritance.
    log.start_progress('Inheriting documentation')
    for i, val_doc in enumerate(valdocs):
        if isinstance(val_doc, ClassDoc):
            percent = float(i)/len(valdocs)
            log.progress(percent, val_doc.canonical_name)
            inherit_docs(val_doc)
    log.end_progress()

    # Initialize the groups & sortedvars attributes.
    log.start_progress('Sorting & Grouping')
    for i, val_doc in enumerate(valdocs):
        if isinstance(val_doc, NamespaceDoc):
            percent = float(i)/len(valdocs)
            log.progress(percent, val_doc.canonical_name)
            val_doc.init_sorted_variables()
            val_doc.init_variable_groups()
            if isinstance(val_doc, ModuleDoc):
                val_doc.init_submodule_groups()
            val_doc.report_unused_groups()
    log.end_progress()

    return docindex

def _report_valdoc_progress(i, val_doc, val_docs):
    if (isinstance(val_doc, (ModuleDoc, ClassDoc)) and
        val_doc.canonical_name is not UNKNOWN and
        not val_doc.canonical_name[0].startswith('??')):
        log.progress(float(i)/len(val_docs), val_doc.canonical_name)

#/////////////////////////////////////////////////////////////////
# Documentation Generation
#/////////////////////////////////////////////////////////////////

def _get_docs_from_items(items, options):

    # Start the progress bar.
    log.start_progress('Building documentation')
    progress_estimator = _ProgressEstimator(items)

    # Check for duplicate item names.
    item_set = set()
    for item in items[:]:
        if item in item_set:
            log.warning("Name %r given multiple times" % item)
            items.remove(item)
        item_set.add(item)

    # Keep track of what top-level canonical names we've assigned, to
    # make sure there are no naming conflicts.  This dict maps
    # canonical names to the item names they came from (so we can print
    # useful error messages).
    canonical_names = {}

    # Collect (introspectdoc, parsedoc) pairs for each item.
    doc_pairs = []
    for item in items:
        if isinstance(item, basestring):
            if is_module_file(item):
                doc_pairs.append(_get_docs_from_module_file(
                    item, options, progress_estimator))
            elif is_package_dir(item):
                pkgfile = os.path.abspath(os.path.join(item, '__init__'))
                doc_pairs.append(_get_docs_from_module_file(
                    pkgfile, options, progress_estimator))
            elif os.path.isfile(item):
                doc_pairs.append(_get_docs_from_pyscript(
                    item, options, progress_estimator))
            elif hasattr(__builtin__, item):
                val = getattr(__builtin__, item)
                doc_pairs.append(_get_docs_from_pyobject(
                    val, options, progress_estimator))
            elif is_pyname(item):
                doc_pairs.append(_get_docs_from_pyname(
                    item, options, progress_estimator))
            elif os.path.isdir(item):
                log.error("Directory %r is not a package" % item)
                continue
            elif os.path.isfile(item):
                log.error("File %s is not a Python module" % item)
                continue
            else:
                log.error("Could not find a file or object named %s" %
                          item)
                continue
        else:
            doc_pairs.append(_get_docs_from_pyobject(
                item, options, progress_estimator))

        # Make sure there are no naming conflicts.
        name = (getattr(doc_pairs[-1][0], 'canonical_name', None) or
                getattr(doc_pairs[-1][1], 'canonical_name', None))
        if name in canonical_names:
            log.error(
                'Two of the specified items, %r and %r, have the same '
                'canonical name ("%s").  This may mean that you specified '
                'two different files that both use the same module name.  '
                'Ignoring the second item (%r)' %
                (canonical_names[name], item, name, canonical_names[name]))
            doc_pairs.pop()
        else:
            canonical_names[name] = item                

        # This will only have an effect if doc_pairs[-1] contains a
        # package's docs.  The 'not is_module_file(item)' prevents
        # us from adding subdirectories if they explicitly specify
        # a package's __init__.py file.
        if options.add_submodules and not is_module_file(item):
            doc_pairs += _get_docs_from_submodules(
                item, doc_pairs[-1], options, progress_estimator)

    log.end_progress()
    return doc_pairs

def _get_docs_from_pyobject(obj, options, progress_estimator):
    progress_estimator.complete += 1
    log.progress(progress_estimator.progress(), repr(obj))
    
    if not options.introspect:
        log.error("Cannot get docs for Python objects without "
                  "introspecting them.")
            
    introspect_doc = parse_doc = None
    introspect_error = parse_error = None
    try:
        introspect_doc = introspect_docs(value=obj)
    except ImportError, e:
        log.error(e)
        return (None, None)
    if options.parse:
        if introspect_doc.canonical_name is not None:
            prev_introspect = options.introspect
            options.introspect = False
            try:
                _, parse_docs = _get_docs_from_pyname(
                    str(introspect_doc.canonical_name), options,
                    progress_estimator, suppress_warnings=True)
            finally:
                options.introspect = prev_introspect

    # We need a name:
    if introspect_doc.canonical_name in (None, UNKNOWN):
        if hasattr(obj, '__name__'):
            introspect_doc.canonical_name = DottedName(
                DottedName.UNREACHABLE, obj.__name__)
        else:
            introspect_doc.canonical_name = DottedName(
                DottedName.UNREACHABLE)
    return (introspect_doc, parse_doc)

def _get_docs_from_pyname(name, options, progress_estimator,
                          suppress_warnings=False):
    progress_estimator.complete += 1
    if options.must_introspect(name) or options.must_parse(name):
        log.progress(progress_estimator.progress(), name)
    
    introspect_doc = parse_doc = None
    introspect_error = parse_error = None
    if options.must_introspect(name):
        try:
            introspect_doc = introspect_docs(name=name)
        except ImportError, e:
            introspect_error = str(e)
    if options.must_parse(name):
        try:
            parse_doc = parse_docs(name=name)
        except ParseError, e:
            parse_error = str(e)
        except ImportError, e:
            # If we get here, then there' probably no python source
            # available; don't bother to generate a warnining.
            pass
        
    # Report any errors we encountered.
    if not suppress_warnings:
        _report_errors(name, introspect_doc, parse_doc,
                       introspect_error, parse_error)

    # Return the docs we found.
    return (introspect_doc, parse_doc)

def _get_docs_from_pyscript(filename, options, progress_estimator):
    # [xx] I should be careful about what names I allow as filenames,
    # and maybe do some munging to prevent problems.

    introspect_doc = parse_doc = None
    introspect_error = parse_error = None
    if options.introspect:
        try:
            introspect_doc = introspect_docs(filename=filename, is_script=True)
            if introspect_doc.canonical_name is UNKNOWN:
                introspect_doc.canonical_name = munge_script_name(filename)
        except ImportError, e:
            introspect_error = str(e)
    if options.parse:
        try:
            parse_doc = parse_docs(filename=filename, is_script=True)
        except ParseError, e:
            parse_error = str(e)
        except ImportError, e:
            parse_error = str(e)
                
    # Report any errors we encountered.
    _report_errors(filename, introspect_doc, parse_doc,
                   introspect_error, parse_error)

    # Return the docs we found.
    return (introspect_doc, parse_doc)
    
def _get_docs_from_module_file(filename, options, progress_estimator,
                               parent_docs=(None,None)):
    """
    Construct and return the API documentation for the python
    module with the given filename.

    @param parent_docs: The C{ModuleDoc} of the containing package.
        If C{parent_docs} is not provided, then this method will
        check if the given filename is contained in a package; and
        if so, it will construct a stub C{ModuleDoc} for the
        containing package(s).  C{parent_docs} is a tuple, where
        the first element is the parent from introspection, and
        the second element is the parent from parsing.
    """
    # Record our progress.
    modulename = os.path.splitext(os.path.split(filename)[1])[0]
    if modulename == '__init__':
        modulename = os.path.split(os.path.split(filename)[0])[1]
    if parent_docs[0]:
        modulename = DottedName(parent_docs[0].canonical_name, modulename)
    elif parent_docs[1]:
        modulename = DottedName(parent_docs[1].canonical_name, modulename)
    if options.must_introspect(modulename) or options.must_parse(modulename):
        log.progress(progress_estimator.progress(),
                     '%s (%s)' % (modulename, filename))
    progress_estimator.complete += 1
    
    # Normalize the filename.
    filename = os.path.normpath(os.path.abspath(filename))

    # When possible, use the source version of the file.
    try:
        filename = py_src_filename(filename)
        src_file_available = True
    except ValueError:
        src_file_available = False

    # Get the introspected & parsed docs (as appropriate)
    introspect_doc = parse_doc = None
    introspect_error = parse_error = None
    if options.must_introspect(modulename):
        try:
            introspect_doc = introspect_docs(
                filename=filename, context=parent_docs[0])
            if introspect_doc.canonical_name is UNKNOWN:
                introspect_doc.canonical_name = modulename
        except ImportError, e:
            introspect_error = str(e)
    if src_file_available and options.must_parse(modulename):
        try:
            parse_doc = parse_docs(
                filename=filename, context=parent_docs[1])
        except ParseError, e:
            parse_error = str(e)
        except ImportError, e:
            parse_error = str(e)

    # Report any errors we encountered.
    _report_errors(filename, introspect_doc, parse_doc,
                   introspect_error, parse_error)

    # Return the docs we found.
    return (introspect_doc, parse_doc)

def _get_docs_from_submodules(item, pkg_docs, options, progress_estimator):
    # Extract the package's __path__.
    if isinstance(pkg_docs[0], ModuleDoc) and pkg_docs[0].is_package:
        pkg_path = pkg_docs[0].path
        package_dir = os.path.split(pkg_docs[0].filename)[0]
    elif isinstance(pkg_docs[1], ModuleDoc) and pkg_docs[1].is_package:
        pkg_path = pkg_docs[1].path
        package_dir = os.path.split(pkg_docs[1].filename)[0]
    else:
        return []

    module_filenames = {}
    subpackage_dirs = set()
    for subdir in pkg_path:
        if os.path.isdir(subdir):
            for name in os.listdir(subdir):
                filename = os.path.join(subdir, name)
                # Is it a valid module filename?
                if is_module_file(filename):
                    basename = os.path.splitext(filename)[0]
                    if os.path.split(basename)[1] != '__init__':
                        module_filenames[basename] = filename
                # Is it a valid package filename?
                if is_package_dir(filename):
                    subpackage_dirs.add(filename)

    # Update our estimate of the number of modules in this package.
    progress_estimator.revise_estimate(item, module_filenames.items(),
                                       subpackage_dirs)

    docs = [pkg_docs]
    for module_filename in module_filenames.values():
        d = _get_docs_from_module_file(
            module_filename, options, progress_estimator, pkg_docs)
        docs.append(d)
    for subpackage_dir in subpackage_dirs:
        subpackage_file = os.path.join(subpackage_dir, '__init__')
        docs.append(_get_docs_from_module_file(
            subpackage_file, options, progress_estimator, pkg_docs))
        docs += _get_docs_from_submodules(
            subpackage_dir, docs[-1], options, progress_estimator)
    return docs

def _report_errors(name, introspect_doc, parse_doc,
                   introspect_error, parse_error):
    hdr = 'In %s:\n' % name
    if introspect_doc == parse_doc == None:
        log.start_block('%sNo documentation available!' % hdr)
        if introspect_error:
            log.error('Import failed:\n%s' % introspect_error)
        if parse_error:
            log.error('Source code parsing failed:\n%s' % parse_error)
        log.end_block()
    elif introspect_error:
        log.start_block('%sImport failed (but source code parsing '
                        'was successful).' % hdr)
        log.error(introspect_error)
        log.end_block()
    elif parse_error:
        log.start_block('%sSource code parsing failed (but '
                        'introspection was successful).' % hdr)
        log.error(parse_error)
        log.end_block()


#/////////////////////////////////////////////////////////////////
# Progress Estimation (for Documentation Generation)
#/////////////////////////////////////////////////////////////////

class _ProgressEstimator:
    """
    Used to keep track of progress when generating the initial docs
    for the given items.  (It is not known in advance how many items a
    package directory will contain, since it might depend on those
    packages' __path__ values.)
    """
    def __init__(self, items):
        self.est_totals = {}
        self.complete = 0
        
        for item in items:
            if is_package_dir(item):
                self.est_totals[item] = self._est_pkg_modules(item)
            else:
                self.est_totals[item] = 1

    def progress(self):
        total = sum(self.est_totals.values())
        return float(self.complete) / total

    def revise_estimate(self, pkg_item, modules, subpackages):
        del self.est_totals[pkg_item]
        for item in modules:
            self.est_totals[item] = 1
        for item in subpackages:
            self.est_totals[item] = self._est_pkg_modules(item)

    def _est_pkg_modules(self, package_dir):
        num_items = 0
        
        if is_package_dir(package_dir):
            for name in os.listdir(package_dir):
                filename = os.path.join(package_dir, name)
                if is_module_file(filename):
                    num_items += 1
                elif is_package_dir(filename):
                    num_items += self._est_pkg_modules(filename)
                    
        return num_items
        
######################################################################
## Doc Merger
######################################################################

MERGE_PRECEDENCE = {
    'repr': 'parse',

    # The names we get from introspection match the names that users
    # can actually use -- i.e., they take magic into account.
    'canonical_name': 'introspect',

    # Only fall-back on the parser for is_imported if the introspecter
    # isn't sure.  Otherwise, we can end up thinking that vars
    # containing modules are not imported, which can cause external
    # modules to show up in the docs (sf bug #1653486)
    'is_imported': 'introspect',

    # The parser can tell if an assignment creates an alias or not.
    'is_alias': 'parse',

    # The parser is better able to determine what text file something
    # came from; e.g., it can't be fooled by 'covert' imports.
    'docformat': 'parse',

    # The parse should be able to tell definitively whether a module
    # is a package or not.
    'is_package': 'parse',

    # Extract the sort spec from the order in which values are defined
    # in the source file.
    'sort_spec': 'parse',
    
    'submodules': 'introspect',

    # The filename used by 'parse' is the source file.
    'filename': 'parse',

    # 'parse' is more likely to get the encoding right, but
    # 'introspect' will handle programatically generated docstrings.
    # Which is better?
    'docstring': 'introspect',
    }
"""Indicates whether information from introspection or parsing should be
given precedence, for specific attributes.  This dictionary maps from
attribute names to either C{'introspect'} or C{'parse'}."""

DEFAULT_MERGE_PRECEDENCE = 'introspect'
"""Indicates whether information from introspection or parsing should be
given precedence.  Should be either C{'introspect'} or C{'parse'}"""

_attribute_mergefunc_registry = {}
def register_attribute_mergefunc(attrib, mergefunc):
    """
    Register an attribute merge function.  This function will be
    called by L{merge_docs()} when it needs to merge the attribute
    values of two C{APIDoc}s.

    @param attrib: The name of the attribute whose values are merged
    by C{mergefunc}.

    @param mergefunc: The merge function, whose sinature is:

    >>> def mergefunc(introspect_val, parse_val, precedence, cyclecheck, path):
    ...     return calculate_merged_value(introspect_val, parse_val)

    Where C{introspect_val} and C{parse_val} are the two values to
    combine; C{precedence} is a string indicating which value takes
    precedence for this attribute (C{'introspect'} or C{'parse'});
    C{cyclecheck} is a value used by C{merge_docs()} to make sure that
    it only visits each pair of docs once; and C{path} is a string
    describing the path that was taken from the root to this
    attribute (used to generate log messages).

    If the merge function needs to call C{merge_docs}, then it should
    pass C{cyclecheck} and C{path} back in.  (When appropriate, a
    suffix should be added to C{path} to describe the path taken to
    the merged values.)
    """
    _attribute_mergefunc_registry[attrib] = mergefunc

def merge_docs(introspect_doc, parse_doc, cyclecheck=None, path=None):
    """
    Merge the API documentation information that was obtained from
    introspection with information that was obtained from parsing.
    C{introspect_doc} and C{parse_doc} should be two C{APIDoc} instances
    that describe the same object.  C{merge_docs} combines the
    information from these two instances, and returns the merged
    C{APIDoc}.

    If C{introspect_doc} and C{parse_doc} are compatible, then they will
    be I{merged} -- i.e., they will be coerced to a common class, and
    their state will be stored in a shared dictionary.  Once they have
    been merged, any change made to the attributes of one will affect
    the other.  The value for the each of the merged C{APIDoc}'s
    attributes is formed by combining the values of the source
    C{APIDoc}s' attributes, as follows:

      - If either of the source attributes' value is C{UNKNOWN}, then
        use the other source attribute's value.
      - Otherwise, if an attribute merge function has been registered
        for the attribute, then use that function to calculate the
        merged value from the two source attribute values.
      - Otherwise, if L{MERGE_PRECEDENCE} is defined for the
        attribute, then use the attribute value from the source that
        it indicates.
      - Otherwise, use the attribute value from the source indicated
        by L{DEFAULT_MERGE_PRECEDENCE}.

    If C{introspect_doc} and C{parse_doc} are I{not} compatible (e.g., if
    their values have incompatible types), then C{merge_docs()} will
    simply return either C{introspect_doc} or C{parse_doc}, depending on
    the value of L{DEFAULT_MERGE_PRECEDENCE}.  The two input
    C{APIDoc}s will not be merged or modified in any way.

    @param cyclecheck, path: These arguments should only be provided
        when C{merge_docs()} is called by an attribute merge
        function.  See L{register_attribute_mergefunc()} for more
        details.
    """
    assert isinstance(introspect_doc, APIDoc)
    assert isinstance(parse_doc, APIDoc)

    if cyclecheck is None:
        cyclecheck = set()
        if introspect_doc.canonical_name not in (None, UNKNOWN):
            path = '%s' % introspect_doc.canonical_name
        elif parse_doc.canonical_name not in (None, UNKNOWN):
            path = '%s' % parse_doc.canonical_name
        else:
            path = '??'

    # If we've already examined this pair, then there's nothing
    # more to do.  The reason that we check id's here is that we
    # want to avoid hashing the APIDoc objects for now, so we can
    # use APIDoc.merge_and_overwrite() later.
    if (id(introspect_doc), id(parse_doc)) in cyclecheck:
        return introspect_doc
    cyclecheck.add( (id(introspect_doc), id(parse_doc)) )

    # If these two are already merged, then we're done.  (Two
    # APIDoc's compare equal iff they are identical or have been
    # merged.)
    if introspect_doc == parse_doc:
        return introspect_doc

    # If both values are GenericValueDoc, then we don't want to merge
    # them.  E.g., we don't want to merge 2+2 with 4.  So just copy
    # the parse_doc's parse_repr to introspect_doc, & return it.
    # (In particular, do *not* call merge_and_overwrite.)
    if type(introspect_doc) == type(parse_doc) == GenericValueDoc:
        if parse_doc.parse_repr is not UNKNOWN:
            introspect_doc.parse_repr = parse_doc.parse_repr
        introspect_doc.docs_extracted_by = 'both'
        return introspect_doc

    # Perform several sanity checks here -- if we accidentally
    # merge values that shouldn't get merged, then bad things can
    # happen.
    mismatch = None
    if (introspect_doc.__class__ != parse_doc.__class__ and
        not (issubclass(introspect_doc.__class__, parse_doc.__class__) or
             issubclass(parse_doc.__class__, introspect_doc.__class__))):
        mismatch = ("value types don't match -- i=%r, p=%r." %
                    (introspect_doc.__class__, parse_doc.__class__))
    if (isinstance(introspect_doc, ValueDoc) and
        isinstance(parse_doc, ValueDoc)):
        if (introspect_doc.pyval is not UNKNOWN and
            parse_doc.pyval is not UNKNOWN and
            introspect_doc.pyval is not parse_doc.pyval):
            mismatch = "values don't match."
        elif (introspect_doc.canonical_name not in (None, UNKNOWN) and
            parse_doc.canonical_name not in (None, UNKNOWN) and
            introspect_doc.canonical_name != parse_doc.canonical_name):
            mismatch = "canonical names don't match."
    if mismatch is not None:
        log.info("Not merging the parsed & introspected values of %s, "
                 "since their %s" % (path, mismatch))
        if DEFAULT_MERGE_PRECEDENCE == 'introspect':
            return introspect_doc
        else:
            return parse_doc

    # If one apidoc's class is a superclass of the other's, then
    # specialize it to the more specific class.
    if introspect_doc.__class__ is not parse_doc.__class__:
        if issubclass(introspect_doc.__class__, parse_doc.__class__):
            parse_doc.specialize_to(introspect_doc.__class__)
        if issubclass(parse_doc.__class__, introspect_doc.__class__):
            introspect_doc.specialize_to(parse_doc.__class__)
    assert introspect_doc.__class__ is parse_doc.__class__

    # The posargs and defaults are tied together -- if we merge
    # the posargs one way, then we need to merge the defaults the
    # same way.  So check them first.  (This is a minor hack)
    if (isinstance(introspect_doc, RoutineDoc) and
        isinstance(parse_doc, RoutineDoc)):
        _merge_posargs_and_defaults(introspect_doc, parse_doc, path)
    
    # Merge the two api_doc's attributes.
    for attrib in set(introspect_doc.__dict__.keys() +
                      parse_doc.__dict__.keys()):
        # Be sure not to merge any private attributes (especially
        # __mergeset or __has_been_hashed!)
        if attrib.startswith('_'): continue
        merge_attribute(attrib, introspect_doc, parse_doc,
                             cyclecheck, path)

    # Set the dictionaries to be shared.
    return introspect_doc.merge_and_overwrite(parse_doc)

def _merge_posargs_and_defaults(introspect_doc, parse_doc, path):
    # If either is unknown, then let merge_attrib handle it.
    if introspect_doc.posargs is UNKNOWN or parse_doc.posargs is UNKNOWN:
        return 
        
    # If the introspected doc just has '...', then trust the parsed doc.
    if introspect_doc.posargs == ['...'] and parse_doc.posargs != ['...']:
        introspect_doc.posargs = parse_doc.posargs
        introspect_doc.posarg_defaults = parse_doc.posarg_defaults

    # If they are incompatible, then check the precedence.
    elif introspect_doc.posargs != parse_doc.posargs:
        log.info("Not merging the parsed & introspected arg "
                 "lists for %s, since they don't match (%s vs %s)"
                  % (path, introspect_doc.posargs, parse_doc.posargs))
        if (MERGE_PRECEDENCE.get('posargs', DEFAULT_MERGE_PRECEDENCE) ==
            'introspect'):
            parse_doc.posargs = introspect_doc.posargs
            parse_doc.posarg_defaults = introspect_doc.posarg_defaults
        else:
            introspect_doc.posargs = parse_doc.posargs
            introspect_doc.posarg_defaults = parse_doc.posarg_defaults

def merge_attribute(attrib, introspect_doc, parse_doc, cyclecheck, path):
    precedence = MERGE_PRECEDENCE.get(attrib, DEFAULT_MERGE_PRECEDENCE)
    if precedence not in ('parse', 'introspect'):
        raise ValueError('Bad precedence value %r' % precedence)
    
    if (getattr(introspect_doc, attrib) is UNKNOWN and
        getattr(parse_doc, attrib) is not UNKNOWN):
        setattr(introspect_doc, attrib, getattr(parse_doc, attrib))
    elif (getattr(introspect_doc, attrib) is not UNKNOWN and
          getattr(parse_doc, attrib) is UNKNOWN):
        setattr(parse_doc, attrib, getattr(introspect_doc, attrib))
    elif (getattr(introspect_doc, attrib) is UNKNOWN and
          getattr(parse_doc, attrib) is UNKNOWN):
        pass
    else:
        # Both APIDoc objects have values; we need to merge them.
        introspect_val = getattr(introspect_doc, attrib)
        parse_val = getattr(parse_doc, attrib)
        if attrib in _attribute_mergefunc_registry:
            handler = _attribute_mergefunc_registry[attrib]
            merged_val = handler(introspect_val, parse_val, precedence,
                                 cyclecheck, path)
        elif precedence == 'introspect':
            merged_val = introspect_val
        elif precedence == 'parse':
            merged_val = parse_val

        setattr(introspect_doc, attrib, merged_val)
        setattr(parse_doc, attrib, merged_val)

def merge_variables(varlist1, varlist2, precedence, cyclecheck, path):
    # Merge all variables that are in both sets.
    for varname, var1 in varlist1.items():
        var2 = varlist2.get(varname)
        if var2 is not None:
            var = merge_docs(var1, var2, cyclecheck, path+'.'+varname)
            varlist1[varname] = var
            varlist2[varname] = var

    # Copy any variables that are not in varlist1 over.
    for varname, var in varlist2.items():
        varlist1.setdefault(varname, var)

    return varlist1

def merge_value(value1, value2, precedence, cyclecheck, path):
    assert value1 is not None and value2 is not None
    return merge_docs(value1, value2, cyclecheck, path)

def merge_overrides(v1, v2, precedence, cyclecheck, path):
    return merge_value(v1, v2, precedence, cyclecheck, path+'.<overrides>')
def merge_fget(v1, v2, precedence, cyclecheck, path):
    return merge_value(v1, v2, precedence, cyclecheck, path+'.fget')
def merge_fset(v1, v2, precedence, cyclecheck, path):
    return merge_value(v1, v2, precedence, cyclecheck, path+'.fset')
def merge_fdel(v1, v2, precedence, cyclecheck, path):
    return merge_value(v1, v2, precedence, cyclecheck, path+'.fdel')

def merge_proxy_for(v1, v2, precedence, cyclecheck, path):
    # Anything we got from introspection shouldn't have a proxy_for
    # attribute -- it should be the actual object's documentation.
    return v1

def merge_bases(baselist1, baselist2, precedence, cyclecheck, path):
    # Be careful here -- if we get it wrong, then we could end up
    # merging two unrelated classes, which could lead to bad
    # things (e.g., a class that's its own subclass).  So only
    # merge two bases if we're quite sure they're the same class.
    # (In particular, if they have the same canonical name.)

    # If the lengths don't match up, then give up.  This is most
    # often caused by __metaclass__.
    if len(baselist1) != len(baselist2):
        log.info("Not merging the introspected & parsed base lists "
                 "for %s, since their lengths don't match (%s vs %s)" %
                 (path, len(baselist1), len(baselist2)))
        if precedence == 'introspect': return baselist1
        else: return baselist2

    # If any names disagree, then give up.
    for base1, base2 in zip(baselist1, baselist2):
        if ((base1.canonical_name not in (None, UNKNOWN) and
             base2.canonical_name not in (None, UNKNOWN)) and
            base1.canonical_name != base2.canonical_name):
            log.info("Not merging the parsed & introspected base "
                     "lists for %s, since the bases' names don't match "
                     "(%s vs %s)" % (path, base1.canonical_name,
                                     base2.canonical_name))
            if precedence == 'introspect': return baselist1
            else: return baselist2

    for i, (base1, base2) in enumerate(zip(baselist1, baselist2)):
        base = merge_docs(base1, base2, cyclecheck,
                           '%s.__bases__[%d]' % (path, i))
        baselist1[i] = baselist2[i] = base

    return baselist1

def merge_posarg_defaults(defaults1, defaults2, precedence, cyclecheck, path):
    if len(defaults1) != len(defaults2):
        if precedence == 'introspect': return defaults1
        else: return defaults2
    defaults = []
    for i, (d1, d2) in enumerate(zip(defaults1, defaults2)):
        if d1 is not None and d2 is not None:
            d_path = '%s.<default-arg-val>[%d]' % (path, i)
            defaults.append(merge_docs(d1, d2, cyclecheck, d_path))
        elif precedence == 'introspect':
            defaults.append(d1)
        else:
            defaults.append(d2)
    return defaults

def merge_docstring(docstring1, docstring2, precedence, cyclecheck, path):
    if docstring1 is None or docstring1 is UNKNOWN or precedence=='parse':
        return docstring2
    else:
        return docstring1

def merge_docs_extracted_by(v1, v2, precedence, cyclecheck, path):
    return 'both'

def merge_submodules(v1, v2, precedence, cyclecheck, path):
    n1 = sorted([m.canonical_name for m in v1])
    n2 = sorted([m.canonical_name for m in v2])
    if (n1 != n2) and (n2 != []):
        log.info('Introspector & parser disagree about submodules '
                 'for %s: (%s) vs (%s)' % (path,
                                           ', '.join([str(n) for n in n1]),
                                           ', '.join([str(n) for n in n2])))
        return v1 + [m for m in v2 if m.canonical_name not in n1]
                
    return v1

register_attribute_mergefunc('variables', merge_variables)
register_attribute_mergefunc('value', merge_value)
register_attribute_mergefunc('overrides', merge_overrides)
register_attribute_mergefunc('fget', merge_fget)
register_attribute_mergefunc('fset', merge_fset)
register_attribute_mergefunc('fdel', merge_fdel)
register_attribute_mergefunc('proxy_for', merge_proxy_for)
register_attribute_mergefunc('bases', merge_bases)
register_attribute_mergefunc('posarg_defaults', merge_posarg_defaults)
register_attribute_mergefunc('docstring', merge_docstring)
register_attribute_mergefunc('docs_extracted_by', merge_docs_extracted_by)
register_attribute_mergefunc('submodules', merge_submodules)

######################################################################
## Import Linking
######################################################################

def link_imports(val_doc, docindex):
    # Check if the ValueDoc has an unresolved proxy_for link.
    # If so, then resolve it.
    while val_doc.proxy_for not in (UNKNOWN, None):
        # Find the valuedoc that the proxy_for name points to.
        src_doc = docindex.get_valdoc(val_doc.proxy_for)

        # If we don't have any valuedoc at that address, then
        # set that address as its canonical name.
        # [XXX] Do I really want to do this?
        if src_doc is None:
            val_doc.canonical_name = val_doc.proxy_for
            return

        # If we *do* have something at that address, then
        # merge the proxy `val_doc` with it.
        elif src_doc != val_doc:
            # Copy any subclass information from val_doc->src_doc.
            if (isinstance(val_doc, ClassDoc) and
                isinstance(src_doc, ClassDoc)):
                for subclass in val_doc.subclasses:
                    if subclass not in src_doc.subclasses:
                        src_doc.subclasses.append(subclass)
            # Then overwrite val_doc with the contents of src_doc.
            src_doc.merge_and_overwrite(val_doc, ignore_hash_conflict=True)

        # If the proxy_for link points back at src_doc
        # itself, then we most likely have a variable that's
        # shadowing a submodule that it should be equal to.
        # So just get rid of the variable.
        elif src_doc == val_doc:
            parent_name = val_doc.proxy_for[:-1]
            var_name = val_doc.proxy_for[-1]
            parent = docindex.get_valdoc(parent_name)
            if parent is not None and var_name in parent.variables:
                del parent.variables[var_name]
            src_doc.proxy_for = None

######################################################################
## Canonical Name Assignment
######################################################################

_name_scores = {}
"""A dictionary mapping from each C{ValueDoc} to the score that has
been assigned to its current cannonical name.  If
L{assign_canonical_names()} finds a canonical name with a better
score, then it will replace the old name."""

_unreachable_names = {DottedName(DottedName.UNREACHABLE):1}
"""The set of names that have been used for unreachable objects.  This
is used to ensure there are no duplicate cannonical names assigned.
C{_unreachable_names} is a dictionary mapping from dotted names to
integer ids, where the next unused unreachable name derived from
dotted name C{n} is
C{DottedName('%s-%s' % (n, str(_unreachable_names[n]+1))}"""

def assign_canonical_names(val_doc, name, docindex, score=0):
    """
    Assign a canonical name to C{val_doc} (if it doesn't have one
    already), and (recursively) to each variable in C{val_doc}.
    In particular, C{val_doc} will be assigned the canonical name
    C{name} iff either:
      - C{val_doc}'s canonical name is C{UNKNOWN}; or
      - C{val_doc}'s current canonical name was assigned by this
        method; but the score of the new name (C{score}) is higher
        than the score of the current name (C{score_dict[val_doc]}).
        
    Note that canonical names will even be assigned to values
    like integers and C{None}; but these should be harmless.
    """
    # If we've already visited this node, and our new score
    # doesn't beat our old score, then there's nothing more to do.
    # Note that since score increases strictly monotonically, this
    # also prevents us from going in cycles.
    if val_doc in _name_scores and score <= _name_scores[val_doc]:
        return

    # Update val_doc's canonical name, if appropriate.
    if (val_doc not in _name_scores and
        val_doc.canonical_name is not UNKNOWN):
        # If this is the first time we've seen val_doc, and it
        # already has a name, then don't change that name.
        _name_scores[val_doc] = sys.maxint
        name = val_doc.canonical_name
        score = 0
    else:
        # Otherwise, update the name iff the new score is better
        # than the old one.
        if (val_doc not in _name_scores or
            score > _name_scores[val_doc]):
            val_doc.canonical_name = name
            _name_scores[val_doc] = score

    # Recurse to any contained values.
    if isinstance(val_doc, NamespaceDoc):
        for var_doc in val_doc.variables.values():
            # Set the variable's canonical name.
            varname = DottedName(name, var_doc.name)
            var_doc.canonical_name = varname

            # If the value is unknown, or is a generic value doc, then
            # the valuedoc doesn't get assigned a name; move on.
            if (var_doc.value is UNKNOWN
                or isinstance(var_doc.value, GenericValueDoc)):
                continue
            
            # [XX] After svn commit 1644-1647, I'm not sure if this
            # ever gets used:  This check is for cases like
            # curses.wrapper, where an imported variable shadows its
            # value's "real" location.
            if _var_shadows_self(var_doc, varname):
                _fix_self_shadowing_var(var_doc, varname, docindex)
    
            # Find the score for this new name.            
            vardoc_score = score-1
            if var_doc.is_imported is UNKNOWN: vardoc_score -= 10
            elif var_doc.is_imported: vardoc_score -= 100
            if var_doc.is_alias is UNKNOWN: vardoc_score -= 10
            elif var_doc.is_alias: vardoc_score -= 1000
            
            assign_canonical_names(var_doc.value, varname,
                                   docindex, vardoc_score)

    # Recurse to any directly reachable values.
    for val_doc_2 in val_doc.apidoc_links(variables=False):
        val_name, val_score = _unreachable_name_for(val_doc_2, docindex)
        assign_canonical_names(val_doc_2, val_name, docindex, val_score)

def _var_shadows_self(var_doc, varname):
    return (var_doc.value not in (None, UNKNOWN) and
            var_doc.value.canonical_name not in (None, UNKNOWN) and
            var_doc.value.canonical_name != varname and
            varname.dominates(var_doc.value.canonical_name))

def _fix_self_shadowing_var(var_doc, varname, docindex):
    # If possible, find another name for the shadowed value.
    cname = var_doc.value.canonical_name
    for i in range(1, len(cname)-1):
        new_name = cname[:i] + (cname[i]+"'") + cname[i+1:]
        val_doc = docindex.get_valdoc(new_name)
        if val_doc is not None:
            log.warning("%s shadows its own value -- using %s instead" %
                     (varname, new_name))
            var_doc.value = val_doc
            return

    # If we couldn't find the actual value, use an unreachable name.
    name, score = _unreachable_name_for(var_doc.value, docindex)
    log.warning('%s shadows itself -- using %s instead' % (varname, name))
    var_doc.value.canonical_name = name

def _unreachable_name_for(val_doc, docindex):
    assert isinstance(val_doc, ValueDoc)
    
    # [xx] (when) does this help?
    if (isinstance(val_doc, ModuleDoc) and
        len(val_doc.canonical_name)==1 and val_doc.package is None):
        for root_val in docindex.root:
            if root_val.canonical_name == val_doc.canonical_name:
                if root_val != val_doc: 
                    log.error("Name conflict: %r vs %r" %
                              (val_doc, root_val))
                break
        else:
            return val_doc.canonical_name, -1000

    # Assign it an 'unreachable' name:
    if (val_doc.pyval is not UNKNOWN and
          hasattr(val_doc.pyval, '__name__')):
        try:
            name = DottedName(DottedName.UNREACHABLE,
                              val_doc.pyval.__name__, strict=True)
        except DottedName.InvalidDottedName:
            name = DottedName(DottedName.UNREACHABLE)
    else:
        name = DottedName(DottedName.UNREACHABLE)

    # Uniquify the name.
    if name in _unreachable_names:
        _unreachable_names[name] += 1
        name = DottedName('%s-%s' % (name, _unreachable_names[name]-1))
    else:
        _unreachable_names[name] = 1
    
    return name, -10000

######################################################################
## Documentation Inheritance
######################################################################

def find_overrides(class_doc):
    """
    Set the C{overrides} attribute for all variables in C{class_doc}.
    This needs to be done early (before docstring parsing), so we can
    know which docstrings to suppress warnings for.
    """
    for base_class in list(class_doc.mro(warn_about_bad_bases=True)):
        if base_class == class_doc: continue
        if base_class.variables is UNKNOWN: continue
        for name, var_doc in base_class.variables.items():
            if ( not (name.startswith('__') and not name.endswith('__')) and
                 base_class == var_doc.container and
                 name in class_doc.variables and 
                 class_doc.variables[name].container==class_doc and
                 class_doc.variables[name].overrides is UNKNOWN ):
                class_doc.variables[name].overrides = var_doc
    
    
def inherit_docs(class_doc):
    for base_class in list(class_doc.mro(warn_about_bad_bases=True)):
        if base_class == class_doc: continue

        # Inherit any groups.  Place them *after* this class's groups,
        # so that any groups that are important to this class come
        # first.
        if base_class.group_specs not in (None, UNKNOWN):
            class_doc.group_specs += [gs for gs in base_class.group_specs
                                      if gs not in class_doc.group_specs]

        # Inherit any variables.
        if base_class.variables is UNKNOWN: continue
        for name, var_doc in base_class.variables.items():
            # If it's a __private variable, then don't inherit it.
            if name.startswith('__') and not name.endswith('__'):
                continue
            
            # Inhetit only from the defining class. Or else, in case of
            # multiple inheritance, we may import from a grand-ancestor
            # variables overridden by a class that follows in mro.
            if base_class != var_doc.container:
                continue
            
            # If class_doc doesn't have a variable with this name,
            # then inherit it.
            if name not in class_doc.variables:
                class_doc.variables[name] = var_doc

            # Otherwise, class_doc already contains a variable
            # that shadows var_doc.  But if class_doc's var is
            # local, then record the fact that it overrides
            # var_doc.
            elif class_doc.variables[name].container==class_doc:
                class_doc.variables[name].overrides = var_doc
                _inherit_info(class_doc.variables[name])

_INHERITED_ATTRIBS = [
    'descr', 'summary', 'metadata', 'extra_docstring_fields',
    'type_descr', 'arg_descrs', 'arg_types', 'return_descr',
    'return_type', 'exception_descrs']

_method_descriptor = type(list.append)

def _inherit_info(var_doc):
    """
    Copy any relevant documentation information from the variable that
    C{var_doc} overrides into C{var_doc} itself.
    """
    src_var = var_doc.overrides
    src_val = var_doc.overrides.value
    val_doc = var_doc.value

    # Special case: if the source value and target values are both c
    # extension methods, and the target value's signature is not
    # specified, then inherit the source value's signature.
    if (isinstance(val_doc, RoutineDoc) and
        isinstance(src_val, RoutineDoc) and
        (inspect.isbuiltin(val_doc.pyval) or
         isinstance(val_doc.pyval, _method_descriptor)) and
        (inspect.isbuiltin(src_val.pyval) or
         isinstance(src_val.pyval, _method_descriptor)) and
        val_doc.all_args() in (['...'], UNKNOWN) and
        src_val.all_args() not in (['...'], UNKNOWN)):
        for attrib in ['posargs', 'posarg_defaults', 'vararg',
                       'kwarg', 'return_type']:
            setattr(val_doc, attrib, getattr(src_val, attrib))
    
    # If the new variable has a docstring, then don't inherit
    # anything, even if the docstring is blank.
    if var_doc.docstring not in (None, UNKNOWN):
        return
    # [xx] Do I want a check like this:?
#     # If it's a method and the signature doesn't match well enough,
#     # then give up.
#     if (isinstance(src_val, RoutineDoc) and
#         isinstance(val_doc, RoutineDoc)):
#         if (src_val.posargs != val_doc.posargs[:len(src_val.posargs)] or
#             src_val.vararg != None and src_val.vararg != val_doc.vararg):
#             log.docstring_warning(
#                 "The signature of %s does not match the signature of the "
#                 "method it overrides (%s); not inheriting documentation." %
#                 (var_doc.canonical_name, src_var.canonical_name))
#             return

    # Inherit attributes!
    for attrib in _INHERITED_ATTRIBS:
        if (hasattr(var_doc, attrib) and hasattr(src_var, attrib) and
            getattr(src_var, attrib) not in (None, UNKNOWN)):
            setattr(var_doc, attrib, getattr(src_var, attrib))
        elif (src_val is not None and
              hasattr(val_doc, attrib) and hasattr(src_val, attrib) and
              getattr(src_val, attrib) not in (None, UNKNOWN) and
              getattr(val_doc, attrib) in (None, UNKNOWN, [])):
            setattr(val_doc, attrib, getattr(src_val, attrib))

########NEW FILE########
__FILENAME__ = docintrospecter
# epydoc -- Introspection
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: docintrospecter.py 1678 2008-01-29 17:21:29Z edloper $

"""
Extract API documentation about python objects by directly introspecting
their values.

The function L{introspect_docs()}, which provides the main interface
of this module, examines a Python objects via introspection, and uses
the information it finds to create an L{APIDoc} objects containing the
API documentation for that objects.

The L{register_introspecter()} method can be used to extend the
functionality of C{docintrospector}, by providing methods that handle
special value types.
"""
__docformat__ = 'epytext en'

######################################################################
## Imports
######################################################################

import inspect, re, sys, os.path, imp
# API documentation encoding:
from epydoc.apidoc import *
# Type comparisons:
from types import *
# Error reporting:
from epydoc import log
# Helper functions:
from epydoc.util import *
# For extracting encoding for docstrings:
import epydoc.docparser
# Builtin values
import __builtin__
# Backwards compatibility
from epydoc.compat import * 

######################################################################
## Caches
######################################################################

_valuedoc_cache = {}
"""A cache containing the API documentation for values that we've
already seen.  This cache is implemented as a dictionary that maps a
value's pyid to its L{ValueDoc}.

Note that if we encounter a value but decide not to introspect it
(because it's imported from another module), then C{_valuedoc_cache}
will contain an entry for the value, but the value will not be listed
in L{_introspected_values}."""

_introspected_values = {}
"""A record which values we've introspected, encoded as a dictionary from
pyid to C{bool}."""

def clear_cache():
    """
    Discard any cached C{APIDoc} values that have been computed for
    introspected values.
    """
    _valuedoc_cache.clear()
    _introspected_values.clear()

######################################################################
## Introspection
######################################################################

def introspect_docs(value=None, name=None, filename=None, context=None,
                    is_script=False, module_name=None):
    """
    Generate the API documentation for a specified object by
    introspecting Python values, and return it as a L{ValueDoc}.  The
    object to generate documentation for may be specified using
    the C{value} parameter, the C{filename} parameter, I{or} the
    C{name} parameter.  (It is an error to specify more than one
    of these three parameters, or to not specify any of them.)

    @param value: The python object that should be documented.
    @param filename: The name of the file that contains the python
        source code for a package, module, or script.  If
        C{filename} is specified, then C{introspect} will return a
        C{ModuleDoc} describing its contents.
    @param name: The fully-qualified python dotted name of any
        value (including packages, modules, classes, and
        functions).  C{DocParser} will automatically figure out
        which module(s) it needs to import in order to find the
        documentation for the specified object.
    @param context: The API documentation for the class of module
        that contains C{value} (if available).
    @param module_name: The name of the module where the value is defined.
        Useful to retrieve the docstring encoding if there is no way to
        detect the module by introspection (such as in properties)
    """
    if value is None and name is not None and filename is None:
        value = get_value_from_name(DottedName(name))
    elif value is None and name is None and filename is not None:
        if is_script:
            value = get_value_from_scriptname(filename)
        else:
            value = get_value_from_filename(filename, context)
    elif name is None and filename is None:
        # it's ok if value is None -- that's a value, after all.
        pass 
    else:
        raise ValueError("Expected exactly one of the following "
                         "arguments: value, name, filename")
    
    pyid = id(value)

    # If we've already introspected this value, then simply return
    # its ValueDoc from our cache.
    if pyid in _introspected_values:
        # If the file is a script, then adjust its name.
        if is_script and filename is not None:
            _valuedoc_cache[pyid].canonical_name = DottedName(
                munge_script_name(str(filename)))
        return _valuedoc_cache[pyid]

    # Create an initial value doc for this value & add it to the cache.
    val_doc = _get_valuedoc(value)

    # Introspect the value.
    _introspected_values[pyid] = True
    introspect_func = _get_introspecter(value)
    introspect_func(value, val_doc, module_name=module_name)

    # Set canonical name, if it was given
    if val_doc.canonical_name is UNKNOWN and name is not None:
        val_doc.canonical_name = DottedName(name)

    # If the file is a script, then adjust its name.
    if is_script and filename is not None:
        val_doc.canonical_name = DottedName(munge_script_name(str(filename)))
        
    if val_doc.canonical_name is UNKNOWN and filename is not None:
        shadowed_name = DottedName(value.__name__)
        log.warning("Module %s is shadowed by a variable with "
                    "the same name." % shadowed_name)
        val_doc.canonical_name = DottedName(str(shadowed_name)+"'")

    return val_doc

def _get_valuedoc(value):
    """
    If a C{ValueDoc} for the given value exists in the valuedoc
    cache, then return it; otherwise, create a new C{ValueDoc},
    add it to the cache, and return it.  When possible, the new
    C{ValueDoc}'s C{pyval}, C{repr}, and C{canonical_name}
    attributes will be set appropriately.
    """
    pyid = id(value)
    val_doc = _valuedoc_cache.get(pyid)
    if val_doc is None:
        try: canonical_name = get_canonical_name(value, strict=True)
        except DottedName.InvalidDottedName: canonical_name = UNKNOWN
        val_doc = ValueDoc(pyval=value, canonical_name = canonical_name,
                           docs_extracted_by='introspecter')
        _valuedoc_cache[pyid] = val_doc
        
        # If it's a module, then do some preliminary introspection.
        # Otherwise, check what the containing module is (used e.g.
        # to decide what markup language should be used for docstrings)
        if inspect.ismodule(value):
            introspect_module(value, val_doc, preliminary=True)
            val_doc.defining_module = val_doc
        else:
            module_name = str(get_containing_module(value))
            module = sys.modules.get(module_name)
            if module is not None and inspect.ismodule(module):
                val_doc.defining_module = _get_valuedoc(module)
            
    return val_doc

#////////////////////////////////////////////////////////////
# Module Introspection
#////////////////////////////////////////////////////////////

#: A list of module variables that should not be included in a
#: module's API documentation.
UNDOCUMENTED_MODULE_VARS = (
    '__builtins__', '__doc__', '__all__', '__file__', '__path__',
    '__name__', '__extra_epydoc_fields__', '__docformat__')

def introspect_module(module, module_doc, module_name=None, preliminary=False):
    """
    Add API documentation information about the module C{module}
    to C{module_doc}.
    """
    module_doc.specialize_to(ModuleDoc)

    # Record the module's docformat
    if hasattr(module, '__docformat__'):
        module_doc.docformat = unicode(module.__docformat__)
                                  
    # Record the module's filename
    if hasattr(module, '__file__'):
        try: module_doc.filename = unicode(module.__file__)
        except KeyboardInterrupt: raise
        except: pass
        if module_doc.filename is not UNKNOWN:
            try: module_doc.filename = py_src_filename(module_doc.filename)
            except ValueError: pass

    # If this is just a preliminary introspection, then don't do
    # anything else.  (Typically this is true if this module was
    # imported, but is not included in the set of modules we're
    # documenting.)
    module_doc.variables = {}
    if preliminary: return

    # Record the module's docstring
    if hasattr(module, '__doc__'):
        module_doc.docstring = get_docstring(module)

    # If the module has a __path__, then it's (probably) a
    # package; so set is_package=True and record its __path__.
    if hasattr(module, '__path__'):
        module_doc.is_package = True
        try: module_doc.path = [unicode(p) for p in module.__path__]
        except KeyboardInterrupt: raise
        except: pass
    else:
        module_doc.is_package = False

    # Make sure we have a name for the package.
    dotted_name = module_doc.canonical_name
    if dotted_name is UNKNOWN:
        dotted_name = DottedName(module.__name__)
    name_without_primes = DottedName(str(dotted_name).replace("'", ""))
        
    # Record the module's parent package, if it has one.
    if len(dotted_name) > 1:
        package_name = str(dotted_name.container())
        package = sys.modules.get(package_name)
        if package is not None:
            module_doc.package = introspect_docs(package)
    else:
        module_doc.package = None

    # Initialize the submodules property
    module_doc.submodules = []

    # Add the module to its parent package's submodules list.
    if module_doc.package not in (None, UNKNOWN):
        module_doc.package.submodules.append(module_doc)

    # Look up the module's __all__ attribute (public names).
    public_names = None
    if hasattr(module, '__all__'):
        try:
            public_names = set([str(name) for name in module.__all__])
        except KeyboardInterrupt: raise
        except: pass

    # Record the module's variables.
    module_doc.variables = {}
    for child_name in dir(module):
        if child_name in UNDOCUMENTED_MODULE_VARS: continue
        child = getattr(module, child_name)

        # Create a VariableDoc for the child, and introspect its
        # value if it's defined in this module.
        container = get_containing_module(child)
        if ((container is not None and
             container == name_without_primes) or
            (public_names is not None and
             child_name in public_names)):
            # Local variable.
            child_val_doc = introspect_docs(child, context=module_doc,
                                            module_name=dotted_name)
            child_var_doc = VariableDoc(name=child_name,
                                        value=child_val_doc,
                                        is_imported=False,
                                        container=module_doc,
                                        docs_extracted_by='introspecter')
        elif container is None or module_doc.canonical_name is UNKNOWN:

            # Don't introspect stuff "from __future__"
            if is_future_feature(child): continue

            # Possibly imported variable.
            child_val_doc = introspect_docs(child, context=module_doc)
            child_var_doc = VariableDoc(name=child_name,
                                        value=child_val_doc,
                                        container=module_doc,
                                        docs_extracted_by='introspecter')
        else:
            # Imported variable.
            child_val_doc = _get_valuedoc(child)
            child_var_doc = VariableDoc(name=child_name,
                                        value=child_val_doc,
                                        is_imported=True,
                                        container=module_doc,
                                        docs_extracted_by='introspecter')

        # If the module's __all__ attribute is set, use it to set the
        # variables public/private status and imported status.
        if public_names is not None:
            if child_name in public_names:
                child_var_doc.is_public = True
                if not isinstance(child_var_doc, ModuleDoc):
                    child_var_doc.is_imported = False
            else:
                child_var_doc.is_public = False

        module_doc.variables[child_name] = child_var_doc

    return module_doc

#////////////////////////////////////////////////////////////
# Class Introspection
#////////////////////////////////////////////////////////////

#: A list of class variables that should not be included in a
#: class's API documentation.
UNDOCUMENTED_CLASS_VARS = (
    '__doc__', '__module__', '__dict__', '__weakref__', '__slots__',
    '__pyx_vtable__')

def introspect_class(cls, class_doc, module_name=None):
    """
    Add API documentation information about the class C{cls}
    to C{class_doc}.
    """
    class_doc.specialize_to(ClassDoc)

    # Record the class's docstring.
    class_doc.docstring = get_docstring(cls)

    # Record the class's __all__ attribute (public names).
    public_names = None
    if hasattr(cls, '__all__'):
        try:
            public_names = set([str(name) for name in cls.__all__])
        except KeyboardInterrupt: raise
        except: pass

    # Start a list of subclasses.
    class_doc.subclasses = []

    # Sometimes users will define a __metaclass__ that copies all
    # class attributes from bases directly into the derived class's
    # __dict__ when the class is created.  (This saves the lookup time
    # needed to search the base tree for an attribute.)  But for the
    # docs, we only want to list these copied attributes in the
    # parent.  So only add an attribute if it is not identical to an
    # attribute of a base class.  (Unfortunately, this can sometimes
    # cause an attribute to look like it was inherited, even though it
    # wasn't, if it happens to have the exact same value as the
    # corresponding base's attribute.)  An example of a case where
    # this helps is PyQt -- subclasses of QWidget get about 300
    # methods injected into them.
    base_children = {}
    
    # Record the class's base classes; and add the class to its
    # base class's subclass lists.
    if hasattr(cls, '__bases__'):
        try: bases = list(cls.__bases__)
        except:
            bases = None
            log.warning("Class '%s' defines __bases__, but it does not "
                        "contain an iterable; ignoring base list."
                        % getattr(cls, '__name__', '??'))
        if bases is not None:
            class_doc.bases = []
            for base in bases:
                basedoc = introspect_docs(base)
                class_doc.bases.append(basedoc)
                basedoc.subclasses.append(class_doc)
            
            bases.reverse()
            for base in bases:
                if hasattr(base, '__dict__'):
                    base_children.update(base.__dict__)

    # The module name is not defined if the class is being introspected
    # as another class base.
    if module_name is None and class_doc.defining_module not in (None, UNKNOWN):
        module_name = class_doc.defining_module.canonical_name
        
    # Record the class's local variables.
    class_doc.variables = {}
    if hasattr(cls, '__dict__'):
        private_prefix = '_%s__' % getattr(cls, '__name__', '<none>')
        for child_name, child in cls.__dict__.items():
            if (child_name in base_children
                and base_children[child_name] == child):
                continue

            if child_name.startswith(private_prefix):
                child_name = child_name[len(private_prefix)-2:]
            if child_name in UNDOCUMENTED_CLASS_VARS: continue
            val_doc = introspect_docs(child, context=class_doc,
                                      module_name=module_name)
            var_doc = VariableDoc(name=child_name, value=val_doc,
                                  container=class_doc,
                                  docs_extracted_by='introspecter')
            if public_names is not None:
                var_doc.is_public = (child_name in public_names)
            class_doc.variables[child_name] = var_doc

    return class_doc

#////////////////////////////////////////////////////////////
# Routine Introspection
#////////////////////////////////////////////////////////////

def introspect_routine(routine, routine_doc, module_name=None):
    """Add API documentation information about the function
    C{routine} to C{routine_doc} (specializing it to C{Routine_doc})."""
    routine_doc.specialize_to(RoutineDoc)
    
    # Extract the underying function
    if isinstance(routine, MethodType):
        func = routine.im_func
    elif isinstance(routine, staticmethod):
        func = routine.__get__(0)
    elif isinstance(routine, classmethod):
        func = routine.__get__(0).im_func
    else:
        func = routine

    # Record the function's docstring.
    routine_doc.docstring = get_docstring(func)

    # Record the function's signature.
    if isinstance(func, FunctionType):
        (args, vararg, kwarg, defaults) = inspect.getargspec(func)

        # Add the arguments.
        routine_doc.posargs = args
        routine_doc.vararg = vararg
        routine_doc.kwarg = kwarg

        # Set default values for positional arguments.
        routine_doc.posarg_defaults = [None]*len(args)
        if defaults is not None:
            offset = len(args)-len(defaults)
            for i in range(len(defaults)):
                default_val = introspect_docs(defaults[i])
                routine_doc.posarg_defaults[i+offset] = default_val

        # If it's a bound method, then strip off the first argument.
        if isinstance(routine, MethodType) and routine.im_self is not None:
            routine_doc.posargs = routine_doc.posargs[1:]
            routine_doc.posarg_defaults = routine_doc.posarg_defaults[1:]

        # Set the routine's line number.
        if hasattr(func, 'func_code'):
            routine_doc.lineno = func.func_code.co_firstlineno

    else:
        # [XX] I should probably use UNKNOWN here??
        # dvarrazzo: if '...' is to be changed, also check that
        # `docstringparser.process_arg_field()` works correctly.
        # See SF bug #1556024.
        routine_doc.posargs = ['...']
        routine_doc.posarg_defaults = [None]
        routine_doc.kwarg = None
        routine_doc.vararg = None

    # Change type, if appropriate.
    if isinstance(routine, staticmethod):
        routine_doc.specialize_to(StaticMethodDoc)
    if isinstance(routine, classmethod):
        routine_doc.specialize_to(ClassMethodDoc)
        
    return routine_doc

#////////////////////////////////////////////////////////////
# Property Introspection
#////////////////////////////////////////////////////////////

def introspect_property(prop, prop_doc, module_name=None):
    """Add API documentation information about the property
    C{prop} to C{prop_doc} (specializing it to C{PropertyDoc})."""
    prop_doc.specialize_to(PropertyDoc)

    # Record the property's docstring.
    prop_doc.docstring = get_docstring(prop, module_name=module_name)

    # Record the property's access functions.
    if hasattr(prop, 'fget'):
        prop_doc.fget = introspect_docs(prop.fget)
        prop_doc.fset = introspect_docs(prop.fset)
        prop_doc.fdel = introspect_docs(prop.fdel)
    
    return prop_doc

#////////////////////////////////////////////////////////////
# Generic Value Introspection
#////////////////////////////////////////////////////////////

def introspect_other(val, val_doc, module_name=None):
    """Specialize val_doc to a C{GenericValueDoc} and return it."""
    val_doc.specialize_to(GenericValueDoc)
    return val_doc

#////////////////////////////////////////////////////////////
# Helper functions
#////////////////////////////////////////////////////////////

def isclass(object):
    """
    Return true if the given object is a class.  In particular, return
    true if object is an instance of C{types.TypeType} or of
    C{types.ClassType}.  This is used instead of C{inspect.isclass()},
    because the latter returns true for objects that are not classes
    (in particular, it returns true for any object that has a
    C{__bases__} attribute, including objects that define
    C{__getattr__} to always return a value).
    """
    return isinstance(object, tuple(_CLASS_TYPES))

_CLASS_TYPES = set([TypeType, ClassType])
"""A list of types that should be treated as classes."""

def register_class_type(typ):
    """Add a type to the lists of types that should be treated as
    classes.  By default, this list contains C{TypeType} and
    C{ClassType}."""
    _CLASS_TYPES.add(typ)

__future_check_works = None

def is_future_feature(object):
    """
    Return True if C{object} results from a C{from __future__ import feature}
    statement.
    """
    # Guard from unexpected implementation changes of the __future__ module.
    global __future_check_works
    if __future_check_works is not None:
        if __future_check_works:
            import __future__
            return isinstance(object, __future__._Feature)
        else:
            return False
    else:
        __future_check_works = True
        try:
            return is_future_feature(object)
        except:
            __future_check_works = False
            log.warning("Troubles inspecting __future__. Python implementation"
                        " may have been changed.")
            return False

def get_docstring(value, module_name=None):
    """
    Return the docstring for the given value; or C{None} if it
    does not have a docstring.
    @rtype: C{unicode}
    """
    docstring = getattr(value, '__doc__', None)
    if docstring is None:
        return None
    elif isinstance(docstring, unicode):
        return docstring
    elif isinstance(docstring, str):
        try: return unicode(docstring, 'ascii')
        except UnicodeDecodeError:
            if module_name is None:
                module_name = get_containing_module(value)
            if module_name is not None:
                try:
                    module = get_value_from_name(module_name)
                    filename = py_src_filename(module.__file__)
                    encoding = epydoc.docparser.get_module_encoding(filename)
                    return unicode(docstring, encoding)
                except KeyboardInterrupt: raise
                except Exception: pass
            if hasattr(value, '__name__'): name = value.__name__
            else: name = repr(value)
            log.warning("%s's docstring is not a unicode string, but it "
                        "contains non-ascii data -- treating it as "
                        "latin-1." % name)
            return unicode(docstring, 'latin-1')
        return None
    elif value is BuiltinMethodType:
        # Don't issue a warning for this special case.
        return None
    else:
        if hasattr(value, '__name__'): name = value.__name__
        else: name = repr(value)
        log.warning("%s's docstring is not a string -- ignoring it." %
                    name)
        return None

def get_canonical_name(value, strict=False):
    """
    @return: the canonical name for C{value}, or C{UNKNOWN} if no
    canonical name can be found.  Currently, C{get_canonical_name}
    can find canonical names for: modules; functions; non-nested
    classes; methods of non-nested classes; and some class methods
    of non-nested classes.
    
    @rtype: L{DottedName} or C{UNKNOWN}
    """
    if not hasattr(value, '__name__'): return UNKNOWN

    # Get the name via introspection.
    if isinstance(value, ModuleType):
        try:
            dotted_name = DottedName(value.__name__, strict=strict)
            # If the module is shadowed by a variable in its parent
            # package(s), then add a prime mark to the end, to
            # differentiate it from the variable that shadows it.
            if verify_name(value, dotted_name) is UNKNOWN:
                log.warning("Module %s is shadowed by a variable with "
                            "the same name." % dotted_name)
                # Note -- this return bypasses verify_name check:
                return DottedName(value.__name__+"'")
        except DottedName.InvalidDottedName:
            # Name is not a valid Python identifier -- treat as script.
            if hasattr(value, '__file__'):
                filename = '%s' % value.__str__
                dotted_name = DottedName(munge_script_name(filename))
        
    elif isclass(value):
        if value.__module__ == '__builtin__':
            dotted_name = DottedName(value.__name__, strict=strict)
        else:
            dotted_name = DottedName(value.__module__, value.__name__,
                                     strict=strict)
            
    elif (inspect.ismethod(value) and value.im_self is not None and
          value.im_class is ClassType and
          not value.__name__.startswith('<')): # class method.
        class_name = get_canonical_name(value.im_self)
        if class_name is UNKNOWN: return UNKNOWN
        dotted_name = DottedName(class_name, value.__name__, strict=strict)
    elif (inspect.ismethod(value) and
          not value.__name__.startswith('<')):
        class_name = get_canonical_name(value.im_class)
        if class_name is UNKNOWN: return UNKNOWN
        dotted_name = DottedName(class_name, value.__name__, strict=strict)
    elif (isinstance(value, FunctionType) and
          not value.__name__.startswith('<')):
        module_name = _find_function_module(value)
        if module_name is None: return UNKNOWN
        dotted_name = DottedName(module_name, value.__name__, strict=strict)
    else:
        return UNKNOWN

    return verify_name(value, dotted_name)

def verify_name(value, dotted_name):
    """
    Verify the name.  E.g., if it's a nested class, then we won't be
    able to find it with the name we constructed.
    """
    if dotted_name is UNKNOWN: return UNKNOWN
    if len(dotted_name) == 1 and hasattr(__builtin__, dotted_name[0]):
        return dotted_name
    named_value = sys.modules.get(dotted_name[0])
    if named_value is None: return UNKNOWN
    for identifier in dotted_name[1:]:
        try: named_value = getattr(named_value, identifier)
        except: return UNKNOWN
    if value is named_value:
        return dotted_name
    else:
        return UNKNOWN

# [xx] not used:
def value_repr(value):
    try:
        s = '%r' % value
        if isinstance(s, str):
            s = decode_with_backslashreplace(s)
        return s
    except:
        return UNKNOWN

def get_containing_module(value):
    """
    Return the name of the module containing the given value, or
    C{None} if the module name can't be determined.
    @rtype: L{DottedName}
    """
    if inspect.ismodule(value):
        return DottedName(value.__name__)
    elif isclass(value):
        return DottedName(value.__module__)
    elif (inspect.ismethod(value) and value.im_self is not None and
          value.im_class is ClassType): # class method.
        return DottedName(value.im_self.__module__)
    elif inspect.ismethod(value):
        return DottedName(value.im_class.__module__)
    elif inspect.isroutine(value):
        module = _find_function_module(value)
        if module is None: return None
        return DottedName(module)
    else:
        return None

def _find_function_module(func):
    """
    @return: The module that defines the given function.
    @rtype: C{module}
    @param func: The function whose module should be found.
    @type func: C{function}
    """
    if hasattr(func, '__module__'):
        return func.__module__
    try:
        module = inspect.getmodule(func)
        if module: return module.__name__
    except KeyboardInterrupt: raise
    except: pass

    # This fallback shouldn't usually be needed.  But it is needed in
    # a couple special cases (including using epydoc to document
    # itself).  In particular, if a module gets loaded twice, using
    # two different names for the same file, then this helps.
    for module in sys.modules.values():
        if (hasattr(module, '__dict__') and
            hasattr(func, 'func_globals') and
            func.func_globals is module.__dict__):
            return module.__name__
    return None

#////////////////////////////////////////////////////////////
# Introspection Dispatch Table
#////////////////////////////////////////////////////////////

_introspecter_registry = []
def register_introspecter(applicability_test, introspecter, priority=10):
    """
    Register an introspecter function.  Introspecter functions take
    two arguments, a python value and a C{ValueDoc} object, and should
    add information about the given value to the the C{ValueDoc}.
    Usually, the first line of an inspecter function will specialize
    it to a sublass of C{ValueDoc}, using L{ValueDoc.specialize_to()}:

        >>> def typical_introspecter(value, value_doc):
        ...     value_doc.specialize_to(SomeSubclassOfValueDoc)
        ...     <add info to value_doc>

    @param priority: The priority of this introspecter, which determines
    the order in which introspecters are tried -- introspecters with lower
    numbers are tried first.  The standard introspecters have priorities
    ranging from 20 to 30.  The default priority (10) will place new
    introspecters before standard introspecters.
    """
    _introspecter_registry.append( (priority, applicability_test,
                                    introspecter) )
    _introspecter_registry.sort()
    
def _get_introspecter(value):
    for (priority, applicability_test, introspecter) in _introspecter_registry:
        if applicability_test(value):
            return introspecter
    else:
        return introspect_other

# Register the standard introspecter functions.
def is_classmethod(v): return isinstance(v, classmethod)
def is_staticmethod(v): return isinstance(v, staticmethod)
def is_property(v): return isinstance(v, property)
register_introspecter(inspect.ismodule, introspect_module, priority=20)
register_introspecter(isclass, introspect_class, priority=24)
register_introspecter(inspect.isroutine, introspect_routine, priority=28)
register_introspecter(is_property, introspect_property, priority=30)

# Register getset_descriptor as a property
try:
    import array
    getset_type = type(array.array.typecode)
    del array
    def is_getset(v): return isinstance(v, getset_type)
    register_introspecter(is_getset, introspect_property, priority=32)
except:
    pass

# Register member_descriptor as a property
try:
    import datetime
    member_type = type(datetime.timedelta.days)
    del datetime
    def is_member(v): return isinstance(v, member_type)
    register_introspecter(is_member, introspect_property, priority=34)
except:
    pass

#////////////////////////////////////////////////////////////
# Import support
#////////////////////////////////////////////////////////////

def get_value_from_filename(filename, context=None):
    # Normalize the filename.
    filename = os.path.normpath(os.path.abspath(filename))

    # Divide the filename into a base directory and a name.  (For
    # packages, use the package's parent directory as the base, and
    # the directory name as its name).
    basedir = os.path.split(filename)[0]
    name = os.path.splitext(os.path.split(filename)[1])[0]
    if name == '__init__':
        basedir, name = os.path.split(basedir)
    name = DottedName(name)

    # If the context wasn't provided, then check if the file is in a
    # package directory.  If so, then update basedir & name to contain
    # the topmost package's directory and the fully qualified name for
    # this file.  (This update assume the default value of __path__
    # for the parent packages; if the parent packages override their
    # __path__s, then this can cause us not to find the value.)
    if context is None:
        while is_package_dir(basedir):
            basedir, pkg_name = os.path.split(basedir)
            name = DottedName(pkg_name, name)
            
    # If a parent package was specified, then find the directory of
    # the topmost package, and the fully qualified name for this file.
    if context is not None:
        # Combine the name.
        name = DottedName(context.canonical_name, name)
        # Find the directory of the base package.
        while context not in (None, UNKNOWN):
            pkg_dir = os.path.split(context.filename)[0]
            basedir = os.path.split(pkg_dir)[0]
            context = context.package

    # Import the module.  (basedir is the directory of the module's
    # topmost package, or its own directory if it's not in a package;
    # and name is the fully qualified dotted name for the module.)
    old_sys_path = sys.path[:]
    try:
        sys.path.insert(0, basedir)
        # This will make sure that we get the module itself, even
        # if it is shadowed by a variable.  (E.g., curses.wrapper):
        _import(str(name))
        if str(name) in sys.modules:
            return sys.modules[str(name)]
        else:
            # Use this as a fallback -- it *shouldn't* ever be needed.
            return get_value_from_name(name)
    finally:
        sys.path = old_sys_path

def get_value_from_scriptname(filename):
    name = munge_script_name(filename)
    return _import(name, filename)

def get_value_from_name(name, globs=None):
    """
    Given a name, return the corresponding value.
    
    @param globs: A namespace to check for the value, if there is no
        module containing the named value.  Defaults to __builtin__.
    """
    name = DottedName(name)

    # Import the topmost module/package.  If we fail, then check if
    # the requested name refers to a builtin.
    try:
        module = _import(name[0])
    except ImportError, e:
        if globs is None: globs = __builtin__.__dict__
        if name[0] in globs:
            try: return _lookup(globs[name[0]], name[1:])
            except: raise e
        else:
            raise

    # Find the requested value in the module/package or its submodules.
    for i in range(1, len(name)):
        try: return _lookup(module, name[i:])
        except ImportError: pass
        module = _import('.'.join(name[:i+1]))
        module = _lookup(module, name[1:i+1])
    return module

def _lookup(module, name):
    val = module
    for i, identifier in enumerate(name):
        try: val = getattr(val, identifier)
        except AttributeError:
            exc_msg = ('no variable named %s in %s' %
                       (identifier, '.'.join(name[:1+i])))
            raise ImportError(exc_msg)
    return val
            
def _import(name, filename=None):
    """
    Run the given callable in a 'sandboxed' environment.
    Currently, this includes saving and restoring the contents of
    sys and __builtins__; and suppressing stdin, stdout, and stderr.
    """
    # Note that we just do a shallow copy of sys.  In particular,
    # any changes made to sys.modules will be kept.  But we do
    # explicitly store sys.path.
    old_sys = sys.__dict__.copy()
    old_sys_path = sys.path[:]
    old_builtins = __builtin__.__dict__.copy()

    # Add the current directory to sys.path, in case they're trying to
    # import a module by name that resides in the current directory.
    # But add it to the end -- otherwise, the explicit directory added
    # in get_value_from_filename might get overwritten
    sys.path.append('')

    # Suppress input and output.  (These get restored when we restore
    # sys to old_sys).  
    sys.stdin = sys.stdout = sys.stderr = _dev_null
    sys.__stdin__ = sys.__stdout__ = sys.__stderr__ = _dev_null

    # Remove any command-line arguments
    sys.argv = ['(imported)']

    try:
        try:
            if filename is None:
                return __import__(name)
            else:
                # For importing scripts:
                return imp.load_source(name, filename)
        except KeyboardInterrupt: raise
        except:
            exc_typ, exc_val, exc_tb = sys.exc_info()
            if exc_val is None:
                estr = '%s' % (exc_typ,)
            else:
                estr = '%s: %s' % (exc_typ.__name__, exc_val)
            if exc_tb.tb_next is not None:
                estr += ' (line %d)' % (exc_tb.tb_next.tb_lineno,)
            raise ImportError(estr)
    finally:
        # Restore the important values that we saved.
        __builtin__.__dict__.clear()
        __builtin__.__dict__.update(old_builtins)
        sys.__dict__.clear()
        sys.__dict__.update(old_sys)
        sys.path = old_sys_path
        
def introspect_docstring_lineno(api_doc):
    """
    Try to determine the line number on which the given item's
    docstring begins.  Return the line number, or C{None} if the line
    number can't be determined.  The line number of the first line in
    the file is 1.
    """
    if api_doc.docstring_lineno is not UNKNOWN:
        return api_doc.docstring_lineno
    if isinstance(api_doc, ValueDoc) and api_doc.pyval is not UNKNOWN:
        try:
            lines, lineno = inspect.findsource(api_doc.pyval)
            if not isinstance(api_doc, ModuleDoc): lineno += 1
            for lineno in range(lineno, len(lines)):
                if lines[lineno].split('#', 1)[0].strip():
                    api_doc.docstring_lineno = lineno + 1
                    return lineno + 1
        except IOError: pass
        except TypeError: pass
        except IndexError:
            log.warning('inspect.findsource(%s) raised IndexError'
                        % api_doc.canonical_name)
    return None

class _DevNull:
    """
    A "file-like" object that discards anything that is written and
    always reports end-of-file when read.  C{_DevNull} is used by
    L{_import()} to discard output when importing modules; and to
    ensure that stdin appears closed.
    """
    def __init__(self):
        self.closed = 1
        self.mode = 'r+'
        self.softspace = 0
        self.name='</dev/null>'
    def close(self): pass
    def flush(self): pass
    def read(self, size=0): return ''
    def readline(self, size=0): return ''
    def readlines(self, sizehint=0): return []
    def seek(self, offset, whence=0): pass
    def tell(self): return 0L
    def truncate(self, size=0): pass
    def write(self, str): pass
    def writelines(self, sequence): pass
    xreadlines = readlines
_dev_null = _DevNull()
    
######################################################################
## Zope InterfaceClass
######################################################################

try:
    from zope.interface.interface import InterfaceClass as _ZopeInterfaceClass
    register_class_type(_ZopeInterfaceClass)
except:
    pass

######################################################################
## Zope Extension classes
######################################################################

try:
    # Register type(ExtensionClass.ExtensionClass)
    from ExtensionClass import ExtensionClass as _ExtensionClass
    _ZopeType = type(_ExtensionClass)
    def _is_zope_type(val):
        return isinstance(val, _ZopeType)
    register_introspecter(_is_zope_type, introspect_class)

    # Register ExtensionClass.*MethodType
    from ExtensionClass import PythonMethodType as _ZopeMethodType
    from ExtensionClass import ExtensionMethodType as _ZopeCMethodType
    def _is_zope_method(val):
        return isinstance(val, (_ZopeMethodType, _ZopeCMethodType))
    register_introspecter(_is_zope_method, introspect_routine)
except:
    pass
                         


    
# [xx]
0 # hm..  otherwise the following gets treated as a docstring!  ouch!
"""
######################################################################
## Zope Extension...
######################################################################
class ZopeIntrospecter(Introspecter):
    VALUEDOC_CLASSES = Introspecter.VALUEDOC_CLASSES.copy()
    VALUEDOC_CLASSES.update({
        'module': ZopeModuleDoc,
        'class': ZopeClassDoc,
        'interface': ZopeInterfaceDoc,
        'attribute': ZopeAttributeDoc,
        })
    
    def add_module_child(self, child, child_name, module_doc):
        if isinstance(child, zope.interfaces.Interface):
            module_doc.add_zope_interface(child_name)
        else:
            Introspecter.add_module_child(self, child, child_name, module_doc)

    def add_class_child(self, child, child_name, class_doc):
        if isinstance(child, zope.interfaces.Interface):
            class_doc.add_zope_interface(child_name)
        else:
            Introspecter.add_class_child(self, child, child_name, class_doc)

    def introspect_zope_interface(self, interface, interfacename):
        pass # etc...
"""        

########NEW FILE########
__FILENAME__ = docparser
# epydoc -- Source code parsing
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: docparser.py 1673 2008-01-29 05:42:58Z edloper $

"""
Extract API documentation about python objects by parsing their source
code.

The function L{parse_docs()}, which provides the main interface
of this module, reads and parses the Python source code for a
module, and uses it to create an L{APIDoc} object containing
the API documentation for the variables and values defined in
that modules.

Currently, C{parse_docs()} extracts documentation from the following
source code constructions:

  - module docstring
  - import statements
  - class definition blocks
  - function definition blocks
  - assignment statements
    - simple assignment statements
    - assignment statements with multiple C{'='}s
    - assignment statements with unpacked left-hand sides
    - assignment statements that wrap a function in classmethod
      or staticmethod.
    - assignment to special variables __path__, __all__, and
      __docformat__.
  - delete statements

C{parse_docs()} does not yet support the following source code
constructions:

  - assignment statements that create properties

By default, C{parse_docs()} will expore the contents of top-level
C{try} and C{if} blocks.  If desired, C{parse_docs()} can also
be configured to explore the contents of C{while} and C{for} blocks.
(See the configuration constants, below.)

@todo: Make it possible to extend the functionality of C{parse_docs()},
       by replacing process_line with a dispatch table that can be
       customized (similarly to C{docintrospector.register_introspector()}).
"""
__docformat__ = 'epytext en'

######################################################################
## Imports
######################################################################

# Python source code parsing:
import token, tokenize
# Finding modules:
import imp
# File services:
import os, os.path, sys
# Unicode:
import codecs
# API documentation encoding:
from epydoc.apidoc import *
# For looking up the docs of builtins:
import __builtin__, exceptions
import epydoc.docintrospecter 
# Misc utility functions:
from epydoc.util import *
# Backwards compatibility
from epydoc.compat import *

######################################################################
## Doc Parser
######################################################################

class ParseError(Exception):
    """
    An exception that is used to signify that C{docparser} encountered
    syntactically invalid Python code while processing a Python source
    file.
    """

_moduledoc_cache = {}
"""A cache of C{ModuleDoc}s that we've already created.
C{_moduledoc_cache} is a dictionary mapping from filenames to
C{ValueDoc} objects.
@type: C{dict}"""

#////////////////////////////////////////////////////////////
# Configuration Constants
#////////////////////////////////////////////////////////////

#{ Configuration Constants: Control Flow 
PARSE_TRY_BLOCKS = True
"""Should the contents of C{try} blocks be examined?"""
PARSE_EXCEPT_BLOCKS = True
"""Should the contents of C{except} blocks be examined?"""
PARSE_FINALLY_BLOCKS = True
"""Should the contents of C{finally} blocks be examined?"""
PARSE_IF_BLOCKS = True
"""Should the contents of C{if} blocks be examined?"""
PARSE_ELSE_BLOCKS = True
"""Should the contents of C{else} and C{elif} blocks be examined?"""
PARSE_WHILE_BLOCKS = False
"""Should the contents of C{while} blocks be examined?"""
PARSE_FOR_BLOCKS = False
"""Should the contents of C{for} blocks be examined?"""

#{ Configuration Constants: Imports
IMPORT_HANDLING = 'link'
"""What should C{docparser} do when it encounters an import
statement?
  - C{'link'}: Create variabledoc objects with imported_from pointers
    to the source object.
  - C{'parse'}: Parse the imported file, to find the actual
    documentation for the imported object.  (This will fall back
    to the 'link' behavior if the imported file can't be parsed,
    e.g., if it's a builtin.)
"""

IMPORT_STAR_HANDLING = 'parse'
"""When C{docparser} encounters a C{'from M{m} import *'}
statement, and is unable to parse C{M{m}} (either because
L{IMPORT_HANDLING}=C{'link'}, or because parsing failed), how
should it determine the list of identifiers expored by C{M{m}}?
  - C{'ignore'}: ignore the import statement, and don't create
    any new variables.
  - C{'parse'}: parse it to find a list of the identifiers that it
    exports.  (This will fall back to the 'ignore' behavior if the
    imported file can't be parsed, e.g., if it's a builtin.)
  - C{'introspect'}: import the module and introspect it (using C{dir})
    to find a list of the identifiers that it exports.  (This will
    fall back to the 'ignore' behavior if the imported file can't
    be parsed, e.g., if it's a builtin.)
"""

DEFAULT_DECORATOR_BEHAVIOR = 'transparent'
"""When C{DocParse} encounters an unknown decorator, what should
it do to the documentation of the decorated function?
  - C{'transparent'}: leave the function's documentation as-is.
  - C{'opaque'}: replace the function's documentation with an
    empty C{ValueDoc} object, reflecting the fact that we have no
    knowledge about what value the decorator returns.
"""

BASE_HANDLING = 'parse'#'link'
"""What should C{docparser} do when it encounters a base class that
was imported from another module?
  - C{'link'}: Create a valuedoc with a C{proxy_for} pointer to the
    base class.
  - C{'parse'}: Parse the file containing the base class, to find
    the actual documentation for it.  (This will fall back to the
    'link' behavior if the imported file can't be parsed, e.g., if
    it's a builtin.)
"""

#{ Configuration Constants: Comment docstrings
COMMENT_DOCSTRING_MARKER = '#:'
"""The prefix used to mark comments that contain attribute
docstrings for variables."""

#{ Configuration Constants: Grouping
START_GROUP_MARKER = '#{'
"""The prefix used to mark a comment that starts a group.  This marker
should be followed (on the same line) by the name of the group.
Following a start-group comment, all variables defined at the same
indentation level will be assigned to this group name, until the
parser reaches the end of the file, a matching end-group comment, or
another start-group comment at the same indentation level.
"""

END_GROUP_MARKER = '#}'
"""The prefix used to mark a comment that ends a group.  See
L{START_GROUP_MARKER}."""

#/////////////////////////////////////////////////////////////////
#{ Module parser
#/////////////////////////////////////////////////////////////////

def parse_docs(filename=None, name=None, context=None, is_script=False):
    """
    Generate the API documentation for a specified object by
    parsing Python source files, and return it as a L{ValueDoc}.
    The object to generate documentation for may be specified
    using the C{filename} parameter I{or} the C{name} parameter.
    (It is an error to specify both a filename and a name; or to
    specify neither a filename nor a name).

    @param filename: The name of the file that contains the python
        source code for a package, module, or script.  If
        C{filename} is specified, then C{parse} will return a
        C{ModuleDoc} describing its contents.
    @param name: The fully-qualified python dotted name of any
        value (including packages, modules, classes, and
        functions).  C{parse_docs()} will automatically figure out
        which module(s) it needs to parse in order to find the
        documentation for the specified object.
    @param context: The API documentation for the package that
        contains C{filename}.  If no context is given, then
        C{filename} is assumed to contain a top-level module or
        package.  It is an error to specify a C{context} if the
        C{name} argument is used.
    @rtype: L{ValueDoc}
    """
    # Always introspect __builtins__ & exceptions (e.g., in case
    # they're used as base classes.)
    epydoc.docintrospecter.introspect_docs(__builtin__)
    epydoc.docintrospecter.introspect_docs(exceptions)
    
    # If our input is a python object name, then delegate to
    # _find().
    if filename is None and name is not None:
        if context:
            raise ValueError("context should only be specified together "
                             "with filename, not with name.")
        name = DottedName(name)
        val_doc = _find(name)
        if val_doc.canonical_name is UNKNOWN:
            val_doc.canonical_name = name
        return val_doc

    # If our input is a filename, then create a ModuleDoc for it,
    # and use process_file() to populate its attributes.
    elif filename is not None and name is None:
        # Use a python source version, if possible.
        if not is_script:
            try: filename = py_src_filename(filename)
            except ValueError, e: raise ImportError('%s' % e)

        # Check the cache, first.
        if filename in _moduledoc_cache:
            return _moduledoc_cache[filename]
        
        log.info("Parsing %s" % filename)

        # If the context wasn't provided, then check if the file is in
        # a package directory.  If so, then update basedir & name to
        # contain the topmost package's directory and the fully
        # qualified name for this file.  (This update assume the
        # default value of __path__ for the parent packages; if the
        # parent packages override their __path__s, then this can
        # cause us not to find the value.)
        if context is None and not is_script:
            basedir = os.path.split(filename)[0]
            name = os.path.splitext(os.path.split(filename)[1])[0]
            if name == '__init__':
                basedir, name = os.path.split(basedir)
            context = _parse_package(basedir)

        # Figure out the canonical name of the module we're parsing.
        if not is_script:
            module_name, is_pkg = _get_module_name(filename, context)
        else:
            module_name = DottedName(munge_script_name(filename))
            is_pkg = False

        # Create a new ModuleDoc for the module, & add it to the cache.
        module_doc = ModuleDoc(canonical_name=module_name, variables={},
                               sort_spec=[], imports=[],
                               filename=filename, package=context,
                               is_package=is_pkg, submodules=[],
                               docs_extracted_by='parser')
        module_doc.defining_module = module_doc
        _moduledoc_cache[filename] = module_doc

        # Set the module's __path__ to its default value.
        if is_pkg:
            module_doc.path = [os.path.split(module_doc.filename)[0]]
        
        # Add this module to the parent package's list of submodules.
        if context is not None:
            context.submodules.append(module_doc)

        # Tokenize & process the contents of the module's source file.
        try:
            process_file(module_doc)
        except tokenize.TokenError, e:
            msg, (srow, scol) = e.args
            raise ParseError('Error during parsing: %s '
                             '(%s, line %d, char %d)' %
                             (msg, module_doc.filename, srow, scol))
        except IndentationError, e:
            raise ParseError('Error during parsing: %s (%s)' %
                             (e, module_doc.filename))

        # Handle any special variables (__path__, __docformat__, etc.)
        handle_special_module_vars(module_doc)

        # Return the completed ModuleDoc
        return module_doc
    else:
        raise ValueError("Expected exactly one of the following "
                         "arguments: name, filename")

def _parse_package(package_dir):
    """
    If the given directory is a package directory, then parse its
    __init__.py file (and the __init__.py files of all ancestor
    packages); and return its C{ModuleDoc}.
    """
    if not is_package_dir(package_dir):
        return None
    parent_dir = os.path.split(package_dir)[0]
    parent_doc = _parse_package(parent_dir)
    package_file = os.path.join(package_dir, '__init__')
    return parse_docs(filename=package_file, context=parent_doc)
        
# Special vars:
# C{__docformat__}, C{__all__}, and C{__path__}.
def handle_special_module_vars(module_doc):
    # If __docformat__ is defined, parse its value.
    toktree = _module_var_toktree(module_doc, '__docformat__')
    if toktree is not None:
        try: module_doc.docformat = parse_string(toktree)
        except: pass
        del module_doc.variables['__docformat__']
            
    # If __all__ is defined, parse its value.
    toktree = _module_var_toktree(module_doc, '__all__')
    if toktree is not None:
        try:
            public_names = set(parse_string_list(toktree))
            for name, var_doc in module_doc.variables.items():
                if name in public_names:
                    var_doc.is_public = True
                    if not isinstance(var_doc, ModuleDoc):
                        var_doc.is_imported = False
                else:
                    var_doc.is_public = False
        except ParseError:
            # If we couldn't parse the list, give precedence to introspection.
            for name, var_doc in module_doc.variables.items():
                if not isinstance(var_doc, ModuleDoc):
                    var_doc.is_imported = UNKNOWN
        del module_doc.variables['__all__']

    # If __path__ is defined, then extract its value (pkgs only)
    if module_doc.is_package:
        toktree = _module_var_toktree(module_doc, '__path__')
        if toktree is not None:
            try:
                module_doc.path = parse_string_list(toktree)
            except ParseError:
                pass # [xx]
            del module_doc.variables['__path__']

def _module_var_toktree(module_doc, name):
    var_doc = module_doc.variables.get(name)
    if (var_doc is None or var_doc.value in (None, UNKNOWN) or
        var_doc.value.toktree is UNKNOWN):
        return None
    else:
        return var_doc.value.toktree

#////////////////////////////////////////////////////////////
#{ Module Lookup
#////////////////////////////////////////////////////////////

def _find(name, package_doc=None):
    """
    Return the API documentaiton for the object whose name is
    C{name}.  C{package_doc}, if specified, is the API
    documentation for the package containing the named object.
    """
    # If we're inside a package, then find the package's path.
    if package_doc is None:
        path = None
    elif package_doc.path is not UNKNOWN:
        path = package_doc.path
    else:
        path = [os.path.split(package_doc.filename)[0]]

    # The leftmost identifier in `name` should be a module or
    # package on the given path; find it and parse it.
    filename = _get_filename(name[0], path)
    module_doc = parse_docs(filename, context=package_doc)

    # If the name just has one identifier, then the module we just
    # parsed is the object we're looking for; return it.
    if len(name) == 1: return module_doc

    # Otherwise, we're looking for something inside the module.
    # First, check to see if it's in a variable (but ignore
    # variables that just contain imported submodules).
    if not _is_submodule_import_var(module_doc, name[1]):
        try: return _find_in_namespace(name[1:], module_doc)
        except ImportError: pass

    # If not, then check to see if it's in a subpackage.
    if module_doc.is_package:
        return _find(name[1:], module_doc)

    # If it's not in a variable or a subpackage, then we can't
    # find it.
    raise ImportError('Could not find value')

def _is_submodule_import_var(module_doc, var_name):
    """
    Return true if C{var_name} is the name of a variable in
    C{module_doc} that just contains an C{imported_from} link to a
    submodule of the same name.  (I.e., is a variable created when
    a package imports one of its own submodules.)
    """
    var_doc = module_doc.variables.get(var_name)
    full_var_name = DottedName(module_doc.canonical_name, var_name)
    return (var_doc is not None and
            var_doc.imported_from == full_var_name)
    
def _find_in_namespace(name, namespace_doc):
    if name[0] not in namespace_doc.variables:
        raise ImportError('Could not find value')
    
    # Look up the variable in the namespace.
    var_doc = namespace_doc.variables[name[0]]
    if var_doc.value is UNKNOWN:
        raise ImportError('Could not find value')
    val_doc = var_doc.value

    # If the variable's value was imported, then follow its
    # alias link.
    if var_doc.imported_from not in (None, UNKNOWN):
        return _find(var_doc.imported_from+name[1:])

    # Otherwise, if the name has one identifier, then this is the
    # value we're looking for; return it.
    elif len(name) == 1:
        return val_doc

    # Otherwise, if this value is a namespace, look inside it.
    elif isinstance(val_doc, NamespaceDoc):
        return _find_in_namespace(name[1:], val_doc)

    # Otherwise, we ran into a dead end.
    else:
        raise ImportError('Could not find value')
    
def _get_filename(identifier, path=None):
    if path is UNKNOWN: path = None
    try:
        fp, filename, (s,m,typ) = imp.find_module(identifier, path)
        if fp is not None: fp.close()
    except ImportError:
        raise ImportError, 'No Python source file found.'

    if typ == imp.PY_SOURCE:
        return filename
    elif typ == imp.PY_COMPILED:
        # See if we can find a corresponding non-compiled version.
        filename = re.sub('.py\w$', '.py', filename)
        if not os.path.exists(filename):
            raise ImportError, 'No Python source file found.'
        return filename
    elif typ == imp.PKG_DIRECTORY:
        filename = os.path.join(filename, '__init__.py')
        if not os.path.exists(filename):
            filename = os.path.join(filename, '__init__.pyw')
            if not os.path.exists(filename):
                raise ImportError, 'No package file found.'
        return filename
    elif typ == imp.C_BUILTIN:
        raise ImportError, 'No Python source file for builtin modules.'
    elif typ == imp.C_EXTENSION:
        raise ImportError, 'No Python source file for c extensions.'
    else:
        raise ImportError, 'No Python source file found.'

#/////////////////////////////////////////////////////////////////
#{ File tokenization loop
#/////////////////////////////////////////////////////////////////

def process_file(module_doc):
    """
    Read the given C{ModuleDoc}'s file, and add variables
    corresponding to any objects defined in that file.  In
    particular, read and tokenize C{module_doc.filename}, and
    process each logical line using L{process_line()}.
    """
    # Keep track of the current line number:
    lineno = None
    
    # Use this list to collect the tokens on a single logical line:
    line_toks = []
    
    # This list contains one APIDoc for each indentation level.
    # The first element is the APIDoc for the module, and each
    # subsequent element is the APIDoc for the object at that
    # indentation level.  The final element of the list is the
    # C{APIDoc} for the entity that we're currently processing.
    parent_docs = [module_doc]

    # The APIDoc for the object that was defined by the previous
    # line, if any; or None otherwise.  This is used to update
    # parent_docs when we encounter an indent; and to decide what
    # object (if any) is described by a docstring.
    prev_line_doc = module_doc

    # A list of comments that occur before or on the current
    # logical line, used to build the comment docstring.  Each
    # element is a tuple (comment_text, comment_lineno).
    comments = []

    # A list of decorator lines that occur before the current
    # logical line.  This is used so we can process a function
    # declaration line and its decorators all at once.
    decorators = []

    # A list of group names, one for each indentation level.  This is
    # used to keep track groups that are defined by comment markers
    # START_GROUP_MARKER and END_GROUP_MARKER.
    groups = [None]

    # When we encounter a comment start group marker, set this to the
    # name of the group; but wait until we're ready to process the
    # next line before we actually set groups[-1] to this value.  This
    # is necessary because at the top of a block, the tokenizer gives
    # us comments before the INDENT token; but if we encounter a group
    # start marker at the top of a block, then we want it to apply
    # inside that block, not outside it.
    start_group = None

    # Check if the source file declares an encoding.
    encoding = get_module_encoding(module_doc.filename)

    # The token-eating loop:
    try:
        module_file = codecs.open(module_doc.filename, 'rU', encoding)
    except LookupError:
        log.warning("Unknown encoding %r for %s; using the default"
                    "encoding instead (iso-8859-1)" %
                    (encoding, module_doc.filename))
        encoding = 'iso-8859-1'
        module_file = codecs.open(module_doc.filename, 'rU', encoding)
    tok_iter = tokenize.generate_tokens(module_file.readline)
    for toktype, toktext, (srow,scol), (erow,ecol), line_str in tok_iter:
        # BOM encoding marker: ignore.
        if (toktype == token.ERRORTOKEN and
            (toktext == u'\ufeff' or
             toktext.encode(encoding) == '\xef\xbb\xbf')):
            pass
            
        # Error token: abort
        elif toktype == token.ERRORTOKEN:
            raise ParseError('Error during parsing: invalid syntax '
                             '(%s, line %d, char %d: %r)' %
                             (module_doc.filename, srow, scol, toktext))
        
        # Indent token: update the parent_doc stack.
        elif toktype == token.INDENT:
            if prev_line_doc is None:
                parent_docs.append(parent_docs[-1])
            else:
                parent_docs.append(prev_line_doc)
            groups.append(None)
                
        # Dedent token: update the parent_doc stack.
        elif toktype == token.DEDENT:
            if line_toks == []:
                parent_docs.pop()
                groups.pop()
            else:
                # This *should* only happen if the file ends on an
                # indented line, with no final newline.
                # (otherwise, this is the wrong thing to do.)
                pass
            
        # Line-internal newline token: if we're still at the start of
        # the logical line, and we've seen one or more comment lines,
        # then discard them: blank lines are not allowed between a
        # comment block and the thing it describes.
        elif toktype == tokenize.NL:
            if comments and not line_toks:
                log.warning('Ignoring docstring comment block followed by '
                            'a blank line in %r on line %r' %
                            (module_doc.filename, srow-1))
                comments = []
                
        # Comment token: add to comments if appropriate.
        elif toktype == tokenize.COMMENT:
            if toktext.startswith(COMMENT_DOCSTRING_MARKER):
                comment_line = toktext[len(COMMENT_DOCSTRING_MARKER):].rstrip()
                if comment_line.startswith(" "):
                    comment_line = comment_line[1:]
                comments.append( [comment_line, srow])
            elif toktext.startswith(START_GROUP_MARKER):
                start_group = toktext[len(START_GROUP_MARKER):].strip()
            elif toktext.startswith(END_GROUP_MARKER):
                for i in range(len(groups)-1, -1, -1):
                    if groups[i]:
                        groups[i] = None
                        break
                else:
                    log.warning("Got group end marker without a corresponding "
                                "start marker in %r on line %r" % 
                                (module_doc.filename, srow))
            
        # Normal token: Add it to line_toks.  (If it's a non-unicode
        # string literal, then we need to re-encode using the file's
        # encoding, to get back to the original 8-bit data; and then
        # convert that string with 8-bit data to a 7-bit ascii
        # representation.)
        elif toktype != token.NEWLINE and toktype != token.ENDMARKER:
            if lineno is None: lineno = srow
            if toktype == token.STRING:
                str_prefixes = re.match('[^\'"]*', toktext).group()
                if 'u' not in str_prefixes:
                    s = toktext.encode(encoding)
                    toktext = decode_with_backslashreplace(s)
            line_toks.append( (toktype, toktext) )
            
        # Decorator line: add it to the decorators list.
        elif line_toks and line_toks[0] == (token.OP, '@'):
            decorators.append(shallow_parse(line_toks))
            line_toks = []

        # End of line token, but nothing to do.
        elif line_toks == []:
            pass
            
        # End of line token: parse the logical line & process it.
        else:
            if start_group:
                groups[-1] = start_group
                start_group = None

            if parent_docs[-1] != 'skip_block':
                try:
                    prev_line_doc = process_line(
                        shallow_parse(line_toks), parent_docs, prev_line_doc, 
                        lineno, comments, decorators, encoding)
                except ParseError, e:
                    raise ParseError('Error during parsing: invalid '
                                     'syntax (%s, line %d) -- %s' %
                                     (module_doc.filename, lineno, e))
                except KeyboardInterrupt, e: raise
                except Exception, e:
                    log.error('Internal error during parsing (%s, line '
                              '%s):\n%s' % (module_doc.filename, lineno, e))
                    raise

                # grouping...
                if groups[-1] and prev_line_doc not in (None, 'skip_block'):
                    if isinstance(prev_line_doc, VariableDoc):
                        # prev_line_doc's container will only be
                        # UNKNOWN if it's an instance variable that
                        # didn't have a doc-comment, but might still
                        # be followed by a docstring.  Since we
                        # tokenize in order, we can't do lookahead to
                        # see if the variable will have a comment; but
                        # it should only be added to the container if
                        # it does.  So we defer the grouping of that
                        # to be handled by process_docstring instead.
                        if prev_line_doc.container is not UNKNOWN:
                            add_to_group(prev_line_doc.container,
                                         prev_line_doc, groups[-1])
                    elif isinstance(parent_docs[-1], NamespaceDoc):
                        add_to_group(parent_docs[-1], prev_line_doc,
                                     groups[-1])
            else:
                prev_line_doc = None

            # Reset line contents.
            line_toks = []
            lineno = None
            comments = []
            decorators = []
            
def add_to_group(container, api_doc, group_name):
    if container.group_specs is UNKNOWN:
        container.group_specs = []

    if isinstance(api_doc, VariableDoc):
        var_name = api_doc.name
    else:
        if api_doc.canonical_name is UNKNOWN: log.debug('ouch', `api_doc`)
        var_name = api_doc.canonical_name[-1]

    for (name, group_vars) in container.group_specs:
        if name == group_name:
            group_vars.append(var_name)
            return
    else:
        container.group_specs.append( (group_name, [var_name]) )

def script_guard(line):
    """Detect the idiomatic trick C{if __name__ == "__main__":}"""
    return (len(line) == 5
        and line[1][1] == '__name__' # this is the most selective
        and line[0][1] == 'if'
        and line[2][1] == '=='
        and line[4][1] == ':'
        and line[3][1][1:-1] == '__main__')

#/////////////////////////////////////////////////////////////////
#{ Shallow parser
#/////////////////////////////////////////////////////////////////

def shallow_parse(line_toks):
    """
    Given a flat list of tokens, return a nested tree structure
    (called a X{token tree}), whose leaves are identical to the
    original list, but whose structure reflects the structure
    implied by the grouping tokens (i.e., parenthases, braces, and
    brackets).  If the parenthases, braces, and brackets do not
    match, or are not balanced, then raise a ParseError.
    
    Assign some structure to a sequence of structure (group parens).
    """
    stack = [[]]
    parens = []
    for tok in line_toks:
        toktype, toktext = tok
        if toktext in ('(','[','{'):
            parens.append(tok)
            stack.append([tok])
        elif toktext in ('}',']',')'):
            if not parens:
                raise ParseError('Unbalanced parens')
            left_paren = parens.pop()[1]
            if left_paren+toktext not in ('()', '[]', '{}'):
                raise ParseError('Mismatched parens')
            lst = stack.pop()
            lst.append(tok)
            stack[-1].append(lst)
        else:
            stack[-1].append(tok)
    if len(stack) != 1 or len(parens) != 0:
        raise ParseError('Unbalanced parens')
    return stack[0]

#/////////////////////////////////////////////////////////////////
#{ Line processing
#/////////////////////////////////////////////////////////////////
# The methods process_*() are used to handle lines.

def process_line(line, parent_docs, prev_line_doc, lineno,
                 comments, decorators, encoding):
    """
    @return: C{new-doc}, C{decorator}..?
    """
    args = (line, parent_docs, prev_line_doc, lineno,
            comments, decorators, encoding)

    if not line: # blank line.
        return None
    elif (token.OP, ':') in line[:-1]:
        return process_one_line_block(*args)
    elif (token.OP, ';') in line:
        return process_multi_stmt(*args)
    elif line[0] == (token.NAME, 'def'):
        return process_funcdef(*args)
    elif line[0] == (token.OP, '@'):
        return process_funcdef(*args)
    elif line[0] == (token.NAME, 'class'):
        return process_classdef(*args)
    elif line[0] == (token.NAME, 'import'):
        return process_import(*args)
    elif line[0] == (token.NAME, 'from'):
        return process_from_import(*args)
    elif line[0] == (token.NAME, 'del'):
        return process_del(*args)
    elif len(line)==1 and line[0][0] == token.STRING:
        return process_docstring(*args)
    elif (token.OP, '=') in line:
        return process_assignment(*args)
    elif (line[0][0] == token.NAME and
          line[0][1] in CONTROL_FLOW_KEYWORDS):
        return process_control_flow_line(*args)
    else:
        return None
        # [xx] do something with control structures like for/if?

#/////////////////////////////////////////////////////////////////
# Line handler: control flow
#/////////////////////////////////////////////////////////////////

CONTROL_FLOW_KEYWORDS = [
    #: A list of the control flow keywords.  If a line begins with
    #: one of these keywords, then it should be handled by
    #: C{process_control_flow_line}.
    'if', 'elif', 'else', 'while', 'for', 'try', 'except', 'finally']

def process_control_flow_line(line, parent_docs, prev_line_doc,
                              lineno, comments, decorators, encoding):
    keyword = line[0][1]

    # If it's a 'for' block: create the loop variable.
    if keyword == 'for' and PARSE_FOR_BLOCKS:
        loopvar_name = parse_dotted_name(
            split_on(line[1:], (token.NAME, 'in'))[0])
        parent = get_lhs_parent(loopvar_name, parent_docs)
        if parent is not None:
            var_doc = VariableDoc(name=loopvar_name[-1], is_alias=False, 
                                  is_imported=False, is_instvar=False,
                                  docs_extracted_by='parser')
            set_variable(parent, var_doc)
    
    if ((keyword == 'if' and PARSE_IF_BLOCKS and not script_guard(line)) or
        (keyword == 'elif' and PARSE_ELSE_BLOCKS) or
        (keyword == 'else' and PARSE_ELSE_BLOCKS) or
        (keyword == 'while' and PARSE_WHILE_BLOCKS) or
        (keyword == 'for' and PARSE_FOR_BLOCKS) or
        (keyword == 'try' and PARSE_TRY_BLOCKS) or
        (keyword == 'except' and PARSE_EXCEPT_BLOCKS) or
        (keyword == 'finally' and PARSE_FINALLY_BLOCKS)):
        # Return "None" to indicate that we should process the
        # block using the same context that we were already in.
        return None
    else:
        # Return 'skip_block' to indicate that we should ignore
        # the contents of this block.
        return 'skip_block'

#/////////////////////////////////////////////////////////////////
# Line handler: imports
#/////////////////////////////////////////////////////////////////
# [xx] I could optionally add ValueDoc's for the imported
# variables with proxy_for set to the imported source; but
# I don't think I gain much of anything by doing so.

def process_import(line, parent_docs, prev_line_doc, lineno,
                   comments, decorators, encoding):
    if not isinstance(parent_docs[-1], NamespaceDoc): return
    
    names = split_on(line[1:], (token.OP, ','))
    
    for name in names:
        name_pieces = split_on(name, (token.NAME, 'as'))
        if len(name_pieces) == 1:
            src_name = parse_dotted_name(name_pieces[0])
            _import_var(src_name, parent_docs)
        elif len(name_pieces) == 2:
            if len(name_pieces[1]) != 1:
                raise ParseError('Expected identifier after "as"')
            src_name = parse_dotted_name(name_pieces[0])
            var_name = parse_name(name_pieces[1][0])
            _import_var_as(src_name, var_name, parent_docs)
        else:
            raise ParseError('Multiple "as" tokens in import')

def process_from_import(line, parent_docs, prev_line_doc, lineno,
                        comments, decorators, encoding):
    if not isinstance(parent_docs[-1], NamespaceDoc): return
    
    pieces = split_on(line[1:], (token.NAME, 'import'))
    if len(pieces) != 2 or not pieces[0] or not pieces[1]:
        raise ParseError("Bad from-import")
    lhs, rhs = pieces

    # The RHS might be parenthasized, as specified by PEP 328:
    # http://www.python.org/peps/pep-0328.html
    if (len(rhs) == 1 and isinstance(rhs[0], list) and
        rhs[0][0] == (token.OP, '(') and rhs[0][-1] == (token.OP, ')')):
        rhs = rhs[0][1:-1]

    # >>> from __future__ import nested_scopes
    if lhs == [(token.NAME, '__future__')]:
        return

    # >>> from sys import *
    elif rhs == [(token.OP, '*')]:
        src_name = parse_dotted_name(lhs)
        _process_fromstar_import(src_name, parent_docs)

    # >>> from os.path import join, split
    else:
        # Allow relative imports in this case, as per PEP 328
        src_name = parse_dotted_name(lhs, 
            parent_name=parent_docs[-1].canonical_name)
        parts = split_on(rhs, (token.OP, ','))
        for part in parts:
            # from m import x
            if len(part) == 1:
                var_name = parse_name(part[0])
                _import_var_as(DottedName(src_name, var_name),
                                    var_name, parent_docs)

            # from m import x as y
            elif len(part) == 3 and part[1] == (token.NAME, 'as'):
                orig_name = parse_name(part[0])
                var_name = parse_name(part[2])
                _import_var_as(DottedName(src_name, orig_name),
                                    var_name, parent_docs)

            else:
                ParseError("Bad from-import")

def _process_fromstar_import(src, parent_docs):
    """
    Handle a statement of the form:
        >>> from <src> import *

    If L{IMPORT_HANDLING} is C{'parse'}, then first try to parse
    the module C{M{<src>}}, and copy all of its exported variables
    to C{parent_docs[-1]}.

    Otherwise, try to determine the names of the variables exported by
    C{M{<src>}}, and create a new variable for each export.  If
    L{IMPORT_STAR_HANDLING} is C{'parse'}, then the list of exports if
    found by parsing C{M{<src>}}; if it is C{'introspect'}, then the
    list of exports is found by importing and introspecting
    C{M{<src>}}.
    """
    # This is redundant: already checked by caller.
    if not isinstance(parent_docs[-1], NamespaceDoc): return
    
    # If src is package-local, then convert it to a global name.
    src = _global_name(src, parent_docs)

    # Record the import
    parent_docs[0].imports.append(src) # mark that it's .*??
    
    # [xx] add check for if we already have the source docs in our
    # cache??

    if (IMPORT_HANDLING == 'parse' or
        IMPORT_STAR_HANDLING == 'parse'): # [xx] is this ok?
        try: module_doc = _find(src)
        except ImportError: module_doc = None
        if isinstance(module_doc, ModuleDoc):
            for name, imp_var in module_doc.variables.items():
                # [xx] this is not exactly correct, but close.  It
                # does the wrong thing if a __var__ is explicitly
                # listed in __all__.
                if (imp_var.is_public and
                    not (name.startswith('__') and name.endswith('__'))):
                    var_doc = _add_import_var(DottedName(src, name), name,
                                              parent_docs[-1])
                    if IMPORT_HANDLING == 'parse':
                        var_doc.value = imp_var.value

    # If we got here, then either IMPORT_HANDLING='link' or we
    # failed to parse the `src` module.
    if IMPORT_STAR_HANDLING == 'introspect':
        try: module = __import__(str(src), {}, {}, [0])
        except: return # We couldn't import it.
        if module is None: return # We couldn't import it.
        if hasattr(module, '__all__'):
            names = list(module.__all__)
        else:
            names = [n for n in dir(module) if not n.startswith('_')]
        for name in names:
            _add_import_var(DottedName(src, name), name, parent_docs[-1])

def _import_var(name, parent_docs):
    """
    Handle a statement of the form:
        >>> import <name>

    If L{IMPORT_HANDLING} is C{'parse'}, then first try to find
    the value by parsing; and create an appropriate variable in
    parentdoc.

    Otherwise, add a variable for the imported variable.  (More than
    one variable may be created for cases like C{'import a.b'}, where
    we need to create a variable C{'a'} in parentdoc containing a
    proxy module; and a variable C{'b'} in the proxy module.
    """
    # This is redundant: already checked by caller.
    if not isinstance(parent_docs[-1], NamespaceDoc): return
    
    # If name is package-local, then convert it to a global name.
    src = _global_name(name, parent_docs)
    src_prefix = src[:len(src)-len(name)]

    # Record the import
    parent_docs[0].imports.append(name)
    
    # [xx] add check for if we already have the source docs in our
    # cache??

    if IMPORT_HANDLING == 'parse':
        # Check to make sure that we can actually find the value.
        try: val_doc = _find(src)
        except ImportError: val_doc = None
        if val_doc is not None:
            # We found it; but it's not the value itself we want to
            # import, but the module containing it; so import that
            # module (=top_mod) and create a variable for it.
            top_mod = src_prefix+name[0]
            var_doc = _add_import_var(top_mod, name[0], parent_docs[-1])
            var_doc.value = _find(DottedName(name[0]))
            return

    # If we got here, then either IMPORT_HANDLING='link', or we
    # did not successfully find the value's docs by parsing; use
    # a variable with an UNKNOWN value.
    
    # Create any necessary intermediate proxy module values.
    container = parent_docs[-1]
    for i, identifier in enumerate(name[:-1]):
        if (identifier not in container.variables or
            not isinstance(container.variables[identifier], ModuleDoc)):
            var_doc = _add_import_var(name[:i+1], identifier, container)
            var_doc.value = ModuleDoc(variables={}, sort_spec=[],
                                      proxy_for=src_prefix+name[:i+1],
                                      submodules={}, 
                                      docs_extracted_by='parser')
        container = container.variables[identifier].value

    # Add the variable to the container.
    _add_import_var(src, name[-1], container)

def _import_var_as(src, name, parent_docs):
    """
    Handle a statement of the form:
        >>> import src as name
        
    If L{IMPORT_HANDLING} is C{'parse'}, then first try to find
    the value by parsing; and create an appropriate variable in
    parentdoc.

    Otherwise, create a variables with its C{imported_from} attribute
    pointing to the imported object.
    """
    # This is redundant: already checked by caller.
    if not isinstance(parent_docs[-1], NamespaceDoc): return
    
    # If src is package-local, then convert it to a global name.
    src = _global_name(src, parent_docs)
    
    # Record the import
    parent_docs[0].imports.append(src)
    
    if IMPORT_HANDLING == 'parse':
        # Parse the value and create a variable for it.
        try: val_doc = _find(src)
        except ImportError: val_doc = None
        if val_doc is not None:
            var_doc = VariableDoc(name=name, value=val_doc,
                                  is_imported=True, is_alias=False,
                                  imported_from=src,
                                  docs_extracted_by='parser')
            set_variable(parent_docs[-1], var_doc)
            return

    # If we got here, then either IMPORT_HANDLING='link', or we
    # did not successfully find the value's docs by parsing; use a
    # variable with a proxy value.
    _add_import_var(src, name, parent_docs[-1])

def _add_import_var(src, name, container):
    """
    Add a new imported variable named C{name} to C{container}, with
    C{imported_from=src}.
    """
    var_doc = VariableDoc(name=name, is_imported=True, is_alias=False,
                          imported_from=src, docs_extracted_by='parser')
    set_variable(container, var_doc)
    return var_doc

def _global_name(name, parent_docs):
    """
    If the given name is package-local (relative to the current
    context, as determined by C{parent_docs}), then convert it
    to a global name.
    """
    # Get the containing package from parent_docs.
    if parent_docs[0].is_package:
        package = parent_docs[0]
    else:
        package = parent_docs[0].package

    # Check each package (from closest to furthest) to see if it
    # contains a module named name[0]; if so, then treat `name` as
    # relative to that package.
    while package not in (None, UNKNOWN):
        try:
            fp = imp.find_module(name[0], package.path)[0]
            if fp is not None: fp.close()
        except ImportError:
            # No submodule found here; try the next package up.
            package = package.package
            continue
        # A submodule was found; return its name.
        return package.canonical_name + name

    # We didn't find any package containing `name`; so just return
    # `name` as-is.
    return name

#/////////////////////////////////////////////////////////////////
# Line handler: assignment
#/////////////////////////////////////////////////////////////////

def process_assignment(line, parent_docs, prev_line_doc, lineno,
                       comments, decorators, encoding):
    # Divide the assignment statement into its pieces.
    pieces = split_on(line, (token.OP, '='))

    lhs_pieces = pieces[:-1]
    rhs = pieces[-1]

    # Decide whether the variable is an instance variable or not.
    # If it's an instance var, then discard the value.
    is_instvar = lhs_is_instvar(lhs_pieces, parent_docs)
    
    # if it's not an instance var, and we're not in a namespace,
    # then it's just a local var -- so ignore it.
    if not (is_instvar or isinstance(parent_docs[-1], NamespaceDoc)):
        return None
    
    # Evaluate the right hand side.
    if not is_instvar:
        rhs_val, is_alias = rhs_to_valuedoc(rhs, parent_docs)
    else:
        rhs_val, is_alias = UNKNOWN, False

    # Assign the right hand side value to each left hand side.
    # (Do the rightmost assignment first)
    lhs_pieces.reverse()
    for lhs in lhs_pieces:
        # Try treating the LHS as a simple dotted name.
        try: lhs_name = parse_dotted_name(lhs)
        except: lhs_name = None
        if lhs_name is not None:
            lhs_parent = get_lhs_parent(lhs_name, parent_docs)
            if lhs_parent is None: continue

            # Skip a special class variable.
            if lhs_name[-1] == '__slots__':
                continue

            # Create the VariableDoc.
            var_doc = VariableDoc(name=lhs_name[-1], value=rhs_val,
                                  is_imported=False, is_alias=is_alias,
                                  is_instvar=is_instvar,
                                  docs_extracted_by='parser')
            # Extract a docstring from the comments, when present,
            # but only if there's a single LHS.
            if len(lhs_pieces) == 1:
                add_docstring_from_comments(var_doc, comments)

            # Assign the variable to the containing namespace,
            # *unless* the variable is an instance variable
            # without a comment docstring.  In that case, we'll
            # only want to add it if we later discover that it's
            # followed by a variable docstring.  If it is, then
            # process_docstring will take care of adding it to the
            # containing clas.  (This is a little hackish, but
            # unfortunately is necessary because we won't know if
            # this assignment line is followed by a docstring
            # until later.)
            if (not is_instvar) or comments:
                set_variable(lhs_parent, var_doc, True)

            # If it's the only var, then return the VarDoc for use
            # as the new `prev_line_doc`.
            if (len(lhs_pieces) == 1 and
                (len(lhs_name) == 1 or is_instvar)):
                return var_doc

        # Otherwise, the LHS must be a complex expression; use
        # dotted_names_in() to decide what variables it contains,
        # and create VariableDoc's for all of them (with UNKNOWN
        # value).
        else:
            for lhs_name in dotted_names_in(lhs_pieces):
                lhs_parent = get_lhs_parent(lhs_name, parent_docs)
                if lhs_parent is None: continue
                var_doc = VariableDoc(name=lhs_name[-1],
                                      is_imported=False,
                                      is_alias=is_alias,
                                      is_instvar=is_instvar,
                                      docs_extracted_by='parser')
                set_variable(lhs_parent, var_doc, True)

        # If we have multiple left-hand-sides, then all but the
        # rightmost one are considered aliases.
        is_alias = True
        

def lhs_is_instvar(lhs_pieces, parent_docs):
    if not isinstance(parent_docs[-1], RoutineDoc):
        return False
    # make sure that lhs_pieces is <self>.<name>, where <self> is
    # the name of the first arg to the containing routinedoc, and
    # <name> is a simple name.
    posargs = parent_docs[-1].posargs
    if posargs is UNKNOWN: return False
    if not (len(lhs_pieces)==1 and len(posargs) > 0 and 
            len(lhs_pieces[0]) == 3 and
            lhs_pieces[0][0] == (token.NAME, posargs[0]) and
            lhs_pieces[0][1] == (token.OP, '.') and
            lhs_pieces[0][2][0] == token.NAME):
        return False
    # Make sure we're in an instance method, and not a
    # module-level function.
    for i in range(len(parent_docs)-1, -1, -1):
        if isinstance(parent_docs[i], ClassDoc):
            return True
        elif parent_docs[i] != parent_docs[-1]:
            return False
    return False
        
def rhs_to_valuedoc(rhs, parent_docs):
    # Dotted variable:
    try:
        rhs_name = parse_dotted_name(rhs)
        rhs_val = lookup_value(rhs_name, parent_docs)
        if rhs_val is not None and rhs_val is not UNKNOWN:
            return rhs_val, True
    except ParseError:
        pass

    # Decorators:
    if (len(rhs)==2 and rhs[0][0] == token.NAME and
        isinstance(rhs[1], list)):
        arg_val, _ = rhs_to_valuedoc(rhs[1][1:-1], parent_docs)
        if isinstance(arg_val, RoutineDoc):
            doc = apply_decorator(DottedName(rhs[0][1]), arg_val)
            doc.canonical_name = UNKNOWN
            doc.parse_repr = pp_toktree(rhs)
            return doc, False

    # Nothing else to do: make a val with the source as its repr.
    return GenericValueDoc(parse_repr=pp_toktree(rhs), toktree=rhs,
                           defining_module=parent_docs[0],
                           docs_extracted_by='parser'), False

def get_lhs_parent(lhs_name, parent_docs):
    assert isinstance(lhs_name, DottedName)

    # For instance vars inside an __init__ method:
    if isinstance(parent_docs[-1], RoutineDoc):
        for i in range(len(parent_docs)-1, -1, -1):
            if isinstance(parent_docs[i], ClassDoc):
                return parent_docs[i]
        else:
            raise ValueError("%r is not a namespace or method" %
                             parent_docs[-1])

    # For local variables:
    if len(lhs_name) == 1:
        return parent_docs[-1]

    # For non-local variables:
    return lookup_value(lhs_name.container(), parent_docs)

#/////////////////////////////////////////////////////////////////
# Line handler: single-line blocks
#/////////////////////////////////////////////////////////////////

def process_one_line_block(line, parent_docs, prev_line_doc, lineno,
                           comments, decorators, encoding):
    """
    The line handler for single-line blocks, such as:

        >>> def f(x): return x*2

    This handler calls L{process_line} twice: once for the tokens
    up to and including the colon, and once for the remaining
    tokens.  The comment docstring is applied to the first line
    only.
    @return: C{None}
    """
    i = line.index((token.OP, ':'))
    doc1 = process_line(line[:i+1], parent_docs, prev_line_doc,
                             lineno, comments, decorators, encoding)
    doc2 = process_line(line[i+1:], parent_docs+[doc1],
                             doc1, lineno, None, [], encoding)
    return doc1

#/////////////////////////////////////////////////////////////////
# Line handler: semicolon-separated statements
#/////////////////////////////////////////////////////////////////

def process_multi_stmt(line, parent_docs, prev_line_doc, lineno,
                       comments, decorators, encoding):
    """
    The line handler for semicolon-separated statements, such as:

        >>> x=1; y=2; z=3

    This handler calls L{process_line} once for each statement.
    The comment docstring is not passed on to any of the
    sub-statements.
    @return: C{None}
    """
    for statement in split_on(line, (token.OP, ';')):
        if not statement: continue
        doc = process_line(statement, parent_docs, prev_line_doc, 
                           lineno, None, decorators, encoding)
        prev_line_doc = doc
        decorators = []
    return None

#/////////////////////////////////////////////////////////////////
# Line handler: delete statements
#/////////////////////////////////////////////////////////////////

def process_del(line, parent_docs, prev_line_doc, lineno,
                comments, decorators, encoding):
    """
    The line handler for delete statements, such as:

        >>> del x, y.z

    This handler calls L{del_variable} for each dotted variable in
    the variable list.  The variable list may be nested.  Complex
    expressions in the variable list (such as C{x[3]}) are ignored.
    @return: C{None}
    """
    # If we're not in a namespace, then ignore it.
    parent_doc = parent_docs[-1]
    if not isinstance(parent_doc, NamespaceDoc): return

    var_list = split_on(line[1:], (token.OP, ','))
    for var_name in dotted_names_in(var_list):
        del_variable(parent_docs[-1], var_name)

    return None

#/////////////////////////////////////////////////////////////////
# Line handler: docstrings
#/////////////////////////////////////////////////////////////////

def process_docstring(line, parent_docs, prev_line_doc, lineno,
                      comments, decorators, encoding):
    """
    The line handler for bare string literals.  If
    C{prev_line_doc} is not C{None}, then the string literal is
    added to that C{APIDoc} as a docstring.  If it already has a
    docstring (from comment docstrings), then the new docstring
    will be appended to the old one.
    """
    if prev_line_doc is None: return
    docstring = parse_string(line)

    # If the docstring is a str, then convert it to unicode.
    # According to a strict reading of PEP 263, this might not be the
    # right thing to do; but it will almost always be what the
    # module's author intended.
    if isinstance(docstring, str):
        try:
            docstring = docstring.decode(encoding)
        except UnicodeDecodeError:
            # If decoding failed, then fall back on using
            # decode_with_backslashreplace, which will map e.g.
            # "\xe9" -> u"\\xe9".
            docstring = decode_with_backslashreplace(docstring)
            log.warning("While parsing %s: docstring is not a unicode "
                        "string, but it contains non-ascii data." %
                        prev_line_doc.canonical_name)

    # If the modified APIDoc is an instance variable, and it has
    # not yet been added to its class's C{variables} list,
    # then add it now.  This is done here, rather than in the
    # process_assignment() call that created the variable, because
    # we only want to add instance variables if they have an
    # associated docstring.  (For more info, see the comment above
    # the set_variable() call in process_assignment().)
    added_instvar = False
    if (isinstance(prev_line_doc, VariableDoc) and
         prev_line_doc.is_instvar and
         prev_line_doc.docstring in (None, UNKNOWN)):
        for i in range(len(parent_docs)-1, -1, -1):
            if isinstance(parent_docs[i], ClassDoc):
                set_variable(parent_docs[i], prev_line_doc, True)
                added_instvar = True
                break

    if prev_line_doc.docstring not in (None, UNKNOWN):
        log.warning("%s has both a comment-docstring and a normal "
                    "(string) docstring; ignoring the comment-"
                    "docstring." % prev_line_doc.canonical_name)
        
    prev_line_doc.docstring = docstring
    prev_line_doc.docstring_lineno = lineno

    # If the modified APIDoc is an instance variable, and we added it
    # to the class's variables list here, then it still needs to be
    # grouped too; so return it for use as the new "prev_line_doc."
    if added_instvar:
        return prev_line_doc

    
#/////////////////////////////////////////////////////////////////
# Line handler: function declarations
#/////////////////////////////////////////////////////////////////

def process_funcdef(line, parent_docs, prev_line_doc, lineno,
                    comments, decorators, encoding):
    """
    The line handler for function declaration lines, such as:

        >>> def f(a, b=22, (c,d)):

    This handler creates and initializes a new C{VariableDoc}
    containing a C{RoutineDoc}, adds the C{VariableDoc} to the
    containing namespace, and returns the C{RoutineDoc}.
    """
    # Check syntax.
    if len(line) != 4 or line[3] != (token.OP, ':'):
        raise ParseError("Bad function definition line")
    
    # If we're not in a namespace, then ignore it.
    parent_doc = parent_docs[-1]
    if not isinstance(parent_doc, NamespaceDoc): return

    # Get the function's name
    func_name = parse_name(line[1])
    canonical_name = DottedName(parent_doc.canonical_name, func_name)

    # Create the function's RoutineDoc.
    func_doc = RoutineDoc(canonical_name=canonical_name,
                          defining_module=parent_docs[0],
                          lineno=lineno, docs_extracted_by='parser')

    # Process the signature.
    init_arglist(func_doc, line[2])

    # If the preceeding comment includes a docstring, then add it.
    add_docstring_from_comments(func_doc, comments)
    
    # Apply any decorators.
    func_doc.decorators = [pp_toktree(deco[1:]) for deco in decorators]
    decorators.reverse()
    for decorator in decorators:
        try:
            deco_name = parse_dotted_name(decorator[1:])
        except ParseError:
            deco_name = None
        if func_doc.canonical_name is not UNKNOWN:
            deco_repr = '%s(%s)' % (pp_toktree(decorator[1:]),
                                    func_doc.canonical_name)
        elif func_doc.parse_repr not in (None, UNKNOWN):
            # [xx] this case should be improved.. when will func_doc
            # have a known parse_repr??
            deco_repr = '%s(%s)' % (pp_toktree(decorator[1:]),
                                    func_doc.parse_repr)
        else:
            deco_repr = UNKNOWN
        func_doc = apply_decorator(deco_name, func_doc)
        func_doc.parse_repr = deco_repr
        # [XX] Is there a reson the following should be done?  It
        # causes the grouping code to break.  Presumably the canonical
        # name should remain valid if we're just applying a standard
        # decorator.
        #func_doc.canonical_name = UNKNOWN

    # Add a variable to the containing namespace.
    var_doc = VariableDoc(name=func_name, value=func_doc,
                          is_imported=False, is_alias=False,
                          docs_extracted_by='parser')
    set_variable(parent_doc, var_doc)
    
    # Return the new ValueDoc.
    return func_doc

def apply_decorator(decorator_name, func_doc):
    # [xx] what if func_doc is not a RoutineDoc?
    if decorator_name == DottedName('staticmethod'):
        return StaticMethodDoc(**func_doc.__dict__)
    elif decorator_name == DottedName('classmethod'):
        return ClassMethodDoc(**func_doc.__dict__)
    elif DEFAULT_DECORATOR_BEHAVIOR == 'transparent':
        return func_doc.__class__(**func_doc.__dict__) # make a copy.
    elif DEFAULT_DECORATOR_BEHAVIOR == 'opaque':
        return GenericValueDoc(docs_extracted_by='parser')
    else:
        raise ValueError, 'Bad value for DEFAULT_DECORATOR_BEHAVIOR'

def init_arglist(func_doc, arglist):
    if not isinstance(arglist, list) or arglist[0] != (token.OP, '('):
        raise ParseError("Bad argument list")

    # Initialize to defaults.
    func_doc.posargs = []
    func_doc.posarg_defaults = []
    func_doc.vararg = None
    func_doc.kwarg = None

    # Divide the arglist into individual args.
    args = split_on(arglist[1:-1], (token.OP, ','))

    # Keyword argument.
    if args and args[-1][0] == (token.OP, '**'):
        if len(args[-1]) != 2 or args[-1][1][0] != token.NAME:
            raise ParseError("Expected name after ** in argument list")
        func_doc.kwarg = args[-1][1][1]
        args.pop()

    # Vararg argument.
    if args and args[-1][0] == (token.OP, '*'):
        if len(args[-1]) != 2 or args[-1][1][0] != token.NAME:
            raise ParseError("Expected name after * in argument list")
        func_doc.vararg = args[-1][1][1]
        args.pop()

    # Positional arguments.
    for arg in args:
        func_doc.posargs.append(parse_funcdef_arg(arg[0]))
        if len(arg) == 1:
            func_doc.posarg_defaults.append(None)
        elif arg[1] != (token.OP, '=') or len(arg) == 2:
            raise ParseError("Bad argument list")
        else:
            default_repr = pp_toktree(arg[2:], 'tight')
            default_val = GenericValueDoc(parse_repr=default_repr,
                                          docs_extracted_by='parser')
            func_doc.posarg_defaults.append(default_val)

#/////////////////////////////////////////////////////////////////
# Line handler: class declarations
#/////////////////////////////////////////////////////////////////

def process_classdef(line, parent_docs, prev_line_doc, lineno,
                     comments, decorators, encoding):
    """
    The line handler for class declaration lines, such as:
    
        >>> class Foo(Bar, Baz):

    This handler creates and initializes a new C{VariableDoc}
    containing a C{ClassDoc}, adds the C{VariableDoc} to the
    containing namespace, and returns the C{ClassDoc}.
    """
    # Check syntax
    if len(line)<3 or len(line)>4 or line[-1] != (token.OP, ':'):
        raise ParseError("Bad class definition line")

    # If we're not in a namespace, then ignore it.
    parent_doc = parent_docs[-1]
    if not isinstance(parent_doc, NamespaceDoc): return

    # Get the class's name
    class_name = parse_name(line[1])
    canonical_name = DottedName(parent_doc.canonical_name, class_name)

    # Create the class's ClassDoc & VariableDoc.
    class_doc = ClassDoc(variables={}, sort_spec=[],
                         bases=[], subclasses=[],
                         canonical_name=canonical_name,
                         defining_module=parent_docs[0],
                         docs_extracted_by='parser')
    var_doc = VariableDoc(name=class_name, value=class_doc,
                          is_imported=False, is_alias=False,
                          docs_extracted_by='parser')

    # Add the bases.
    if len(line) == 4:
        if (not isinstance(line[2], list) or
            line[2][0] != (token.OP, '(')):
            raise ParseError("Expected base list")
        try:
            for base_name in parse_classdef_bases(line[2]):
                class_doc.bases.append(find_base(base_name, parent_docs))
        except ParseError, e:
            log.warning("Unable to extract the base list for %s: %s" %
                        (canonical_name, e))
            class_doc.bases = UNKNOWN
    else:
        class_doc.bases = []

    # Register ourselves as a subclass to our bases.
    if class_doc.bases is not UNKNOWN:
        for basedoc in class_doc.bases:
            if isinstance(basedoc, ClassDoc):
                # This test avoids that a subclass gets listed twice when
                # both introspection and parsing.
                # [XXX] This check only works because currently parsing is
                # always performed just after introspection of the same
                # class. A more complete fix shuld be independent from
                # calling order; probably the subclasses list should be
                # replaced by a ClassDoc set or a {name: ClassDoc} mapping.
                if (basedoc.subclasses
                    and basedoc.subclasses[-1].canonical_name
                        != class_doc.canonical_name):
                    basedoc.subclasses.append(class_doc)
    
    # If the preceeding comment includes a docstring, then add it.
    add_docstring_from_comments(class_doc, comments)
    
    # Add the VariableDoc to our container.
    set_variable(parent_doc, var_doc)

    return class_doc

def _proxy_base(**attribs):
    return ClassDoc(variables={}, sort_spec=[], bases=[], subclasses=[], 
                    docs_extracted_by='parser', **attribs)

def find_base(name, parent_docs):
    assert isinstance(name, DottedName)

    # Find the variable containing the base.
    base_var = lookup_variable(name, parent_docs)
    if base_var is None:
        # If we didn't find it, then it must have been imported.
        # First, check if it looks like it's contained in any
        # known imported variable:
        if len(name) > 1:
            src = lookup_name(name[0], parent_docs)
            if (src is not None and
                src.imported_from not in (None, UNKNOWN)):
                base_src = DottedName(src.imported_from, name[1:])
                base_var = VariableDoc(name=name[-1], is_imported=True,
                                       is_alias=False, imported_from=base_src,
                                       docs_extracted_by='parser')
        # Otherwise, it must have come from an "import *" statement
        # (or from magic, such as direct manipulation of the module's
        # dictionary), so we don't know where it came from.  So
        # there's nothing left but to use an empty proxy.
        if base_var is None:
            return _proxy_base(parse_repr=str(name))
            #raise ParseError("Could not find %s" % name)

    # If the variable has a value, return that value.
    if base_var.value is not UNKNOWN:
        return base_var.value

    # Otherwise, if BASE_HANDLING is 'parse', try parsing the docs for
    # the base class; if that fails, or if BASE_HANDLING is 'link',
    # just make a proxy object.
    if base_var.imported_from not in (None, UNKNOWN):
        if BASE_HANDLING == 'parse':
            old_sys_path = sys.path
            try:
                dirname = os.path.split(parent_docs[0].filename)[0]
                sys.path = [dirname] + sys.path
                try:
                    return parse_docs(name=str(base_var.imported_from))
                except ParseError:
                    log.info('Unable to parse base', base_var.imported_from)
                except ImportError:
                    log.info('Unable to find base', base_var.imported_from)
            finally:
                sys.path = old_sys_path
                
        # Either BASE_HANDLING='link' or parsing the base class failed;
        # return a proxy value for the base class.
        return _proxy_base(proxy_for=base_var.imported_from)
    else:
        return _proxy_base(parse_repr=str(name))

#/////////////////////////////////////////////////////////////////
#{ Parsing
#/////////////////////////////////////////////////////////////////

def dotted_names_in(elt_list):
    """
    Return a list of all simple dotted names in the given
    expression.
    """
    names = []
    while elt_list:
        elt = elt_list.pop()
        if len(elt) == 1 and isinstance(elt[0], list):
            # Nested list: process the contents
            elt_list.extend(split_on(elt[0][1:-1], (token.OP, ',')))
        else:
            try:
                names.append(parse_dotted_name(elt))
            except ParseError:
                pass # complex expression -- ignore
    return names

def parse_name(elt, strip_parens=False):
    """
    If the given token tree element is a name token, then return
    that name as a string.  Otherwise, raise ParseError.
    @param strip_parens: If true, then if elt is a single name
        enclosed in parenthases, then return that name.
    """
    if strip_parens and isinstance(elt, list):
        while (isinstance(elt, list) and len(elt) == 3 and
               elt[0] == (token.OP, '(') and
               elt[-1] == (token.OP, ')')):
            elt = elt[1]
    if isinstance(elt, list) or elt[0] != token.NAME:
        raise ParseError("Bad name")
    return elt[1]

def parse_dotted_name(elt_list, strip_parens=True, parent_name=None):
    """
    @param parent_name: canonical name of referring module, to resolve
        relative imports.
    @type parent_name: L{DottedName}
    @bug: does not handle 'x.(y).z'
    """
    if len(elt_list) == 0: raise ParseError("Bad dotted name")
    
    # Handle ((x.y).z).  (If the contents of the parens include
    # anything other than dotted names, such as (x,y), then we'll
    # catch it below and raise a ParseError.
    while (isinstance(elt_list[0], list) and
           len(elt_list[0]) >= 3 and
           elt_list[0][0] == (token.OP, '(') and
           elt_list[0][-1] == (token.OP, ')')):
        elt_list[:1] = elt_list[0][1:-1]

    # Convert a relative import into an absolute name.
    prefix_name = None
    if parent_name is not None and elt_list[0][-1] == '.':
        items = 1
        while len(elt_list) > items and elt_list[items][-1] == '.':
            items += 1
            
        elt_list = elt_list[items:]
        prefix_name = parent_name[:-items]
            
        # >>> from . import foo
        if not elt_list:
            if prefix_name == []:
                raise ParseError("Attempted relative import in non-package, "
                                 "or beyond toplevel package")
            return prefix_name

    if len(elt_list) % 2 != 1: raise ParseError("Bad dotted name")
    name = DottedName(parse_name(elt_list[0], True))
    if prefix_name is not None:
        name = prefix_name + name
        
    for i in range(2, len(elt_list), 2):
        dot, identifier = elt_list[i-1], elt_list[i]
        if  dot != (token.OP, '.'):
            raise ParseError("Bad dotted name")
        name = DottedName(name, parse_name(identifier, True))
    return name
        
def split_on(elt_list, split_tok):
    # [xx] add code to guarantee each elt is non-empty.
    result = [[]]
    for elt in elt_list:
        if elt == split_tok:
            if result[-1] == []: raise ParseError("Empty element from split")
            result.append([])
        else:
            result[-1].append(elt)
    if result[-1] == []: result.pop()
    return result

def parse_funcdef_arg(elt):
    """
    If the given tree token element contains a valid function
    definition argument (i.e., an identifier token or nested list
    of identifiers), then return a corresponding string identifier
    or nested list of string identifiers.  Otherwise, raise a
    ParseError.
    """
    if isinstance(elt, list):
        if elt[0] == (token.OP, '('):
            if len(elt) == 3:
                return parse_funcdef_arg(elt[1])
            else:
                return [parse_funcdef_arg(e)
                        for e in elt[1:-1]
                        if e != (token.OP, ',')]
        else:
            raise ParseError("Bad argument -- expected name or tuple")
    elif elt[0] == token.NAME:
        return elt[1]
    else:
        raise ParseError("Bad argument -- expected name or tuple")
    
def parse_classdef_bases(elt):
    """
    If the given tree token element contains a valid base list
    (that contains only dotted names), then return a corresponding
    list of L{DottedName}s.  Otherwise, raise a ParseError.
    
    @bug: Does not handle either of::
        - class A( (base.in.parens) ): pass
        - class B( (lambda:calculated.base)() ): pass
    """
    if (not isinstance(elt, list) or
        elt[0] != (token.OP, '(')):
        raise ParseError("Bad base list")

    return [parse_dotted_name(n)
            for n in split_on(elt[1:-1], (token.OP, ','))]

# Used by: base list; 'del'; ...
def parse_dotted_name_list(elt_list):
    """
    If the given list of tree token elements contains a
    comma-separated list of dotted names, then return a
    corresponding list of L{DottedName} objects.  Otherwise, raise
    ParseError.
    """
    names = []
    
    state = 0
    for elt in elt_list:
        # State 0 -- Expecting a name, or end of arglist
        if state == 0:
            # Make sure it's a name
            if isinstance(elt, tuple) and elt[0] == token.NAME:
                names.append(DottedName(elt[1]))
                state = 1
            else:
                raise ParseError("Expected a name")
        # State 1 -- Expecting comma, period, or end of arglist
        elif state == 1:
            if elt == (token.OP, '.'):
                state = 2
            elif elt == (token.OP, ','):
                state = 0
            else:
                raise ParseError("Expected '.' or ',' or end of list")
        # State 2 -- Continuation of dotted name.
        elif state == 2:
            if isinstance(elt, tuple) and elt[0] == token.NAME:
                names[-1] = DottedName(names[-1], elt[1])
                state = 1
            else:
                raise ParseError("Expected a name")
    if state == 2:
        raise ParseError("Expected a name")
    return names

def parse_string(elt_list):
    if len(elt_list) == 1 and elt_list[0][0] == token.STRING:
        # [xx] use something safer here?  But it needs to deal with
        # any string type (eg r"foo\bar" etc).
        return eval(elt_list[0][1])
    else:
        raise ParseError("Expected a string")

# ['1', 'b', 'c']
def parse_string_list(elt_list):
    if (len(elt_list) == 1 and isinstance(elt_list, list) and
        elt_list[0][0][1] in ('(', '[')):
        elt_list = elt_list[0][1:-1]

    string_list = []
    for string_elt in split_on(elt_list, (token.OP, ',')):
        string_list.append(parse_string(string_elt))

    return string_list

#/////////////////////////////////////////////////////////////////
#{ Variable Manipulation
#/////////////////////////////////////////////////////////////////

def set_variable(namespace, var_doc, preserve_docstring=False):
    """
    Add var_doc to namespace.  If namespace already contains a
    variable with the same name, then discard the old variable.  If
    C{preserve_docstring} is true, then keep the old variable's
    docstring when overwriting a variable.
    """
    # Choose which dictionary we'll be storing the variable in.
    if not isinstance(namespace, NamespaceDoc):
        return

    # This happens when the class definition has not been parsed, e.g. in
    # sf bug #1693253 on ``Exception.x = y``
    if namespace.sort_spec is UNKNOWN:
        namespace.sort_spec = namespace.variables.keys()

    # If we already have a variable with this name, then remove the
    # old VariableDoc from the sort_spec list; and if we gave its
    # value a canonical name, then delete it.
    if var_doc.name in namespace.variables:
        namespace.sort_spec.remove(var_doc.name)
        old_var_doc = namespace.variables[var_doc.name]
        if (old_var_doc.is_alias == False and
            old_var_doc.value is not UNKNOWN):
            old_var_doc.value.canonical_name = UNKNOWN
        if (preserve_docstring and var_doc.docstring in (None, UNKNOWN) and
            old_var_doc.docstring not in (None, UNKNOWN)):
            var_doc.docstring = old_var_doc.docstring
            var_doc.docstring_lineno = old_var_doc.docstring_lineno
    # Add the variable to the namespace.
    namespace.variables[var_doc.name] = var_doc
    namespace.sort_spec.append(var_doc.name)
    assert var_doc.container is UNKNOWN
    var_doc.container = namespace

def del_variable(namespace, name):
    if not isinstance(namespace, NamespaceDoc):
        return

    if name[0] in namespace.variables:
        if len(name) == 1:
            var_doc = namespace.variables[name[0]]
            namespace.sort_spec.remove(name[0])
            del namespace.variables[name[0]]
            if not var_doc.is_alias and var_doc.value is not UNKNOWN:
                var_doc.value.canonical_name = UNKNOWN
        else:
            del_variable(namespace.variables[name[0]].value, name[1:])
            
#/////////////////////////////////////////////////////////////////
#{ Name Lookup
#/////////////////////////////////////////////////////////////////

def lookup_name(identifier, parent_docs):
    """
    Find and return the documentation for the variable named by
    the given identifier.
    
    @rtype: L{VariableDoc} or C{None}
    """
    # We need to check 3 namespaces: locals, globals, and builtins.
    # Note that this is true even if we're in a version of python with
    # nested scopes, because nested scope lookup does not apply to
    # nested class definitions, and we're not worried about variables
    # in nested functions.
    if not isinstance(identifier, basestring):
        raise TypeError('identifier must be a string')

    # Locals
    if isinstance(parent_docs[-1], NamespaceDoc):
        if identifier in parent_docs[-1].variables:
            return parent_docs[-1].variables[identifier]

    # Globals (aka the containing module)
    if isinstance(parent_docs[0], NamespaceDoc):
        if identifier in parent_docs[0].variables:
            return parent_docs[0].variables[identifier]

    # Builtins
    builtins = epydoc.docintrospecter.introspect_docs(__builtin__)
    if isinstance(builtins, NamespaceDoc):
        if identifier in builtins.variables:
            return builtins.variables[identifier]

    # We didn't find it; return None.
    return None

def lookup_variable(dotted_name, parent_docs):
    assert isinstance(dotted_name, DottedName)
    # If it's a simple identifier, use lookup_name.
    if len(dotted_name) == 1:
        return lookup_name(dotted_name[0], parent_docs)

    # If it's a dotted name with multiple pieces, look up the
    # namespace containing the var (=parent) first; and then
    # look for the var in that namespace.
    else:
        parent = lookup_value(dotted_name[:-1], parent_docs)
        if (isinstance(parent, NamespaceDoc) and
            dotted_name[-1] in parent.variables):
            return parent.variables[dotted_name[-1]]
        else:
            return None # var not found.

def lookup_value(dotted_name, parent_docs):
    """
    Find and return the documentation for the value contained in
    the variable with the given name in the current namespace.
    """
    assert isinstance(dotted_name, DottedName)
    var_doc = lookup_name(dotted_name[0], parent_docs)

    for i in range(1, len(dotted_name)):
        if var_doc is None: return None

        if isinstance(var_doc.value, NamespaceDoc):
            var_dict = var_doc.value.variables
        elif (var_doc.value is UNKNOWN and
            var_doc.imported_from not in (None, UNKNOWN)):
            src_name = var_doc.imported_from + dotted_name[i:]
            # [xx] do I want to create a proxy here??
            return GenericValueDoc(proxy_for=src_name,
                                   parse_repr=str(dotted_name),
                                   docs_extracted_by='parser')
        else:
            return None

        var_doc = var_dict.get(dotted_name[i])

    if var_doc is None: return None
    return var_doc.value

#/////////////////////////////////////////////////////////////////
#{ Docstring Comments
#/////////////////////////////////////////////////////////////////

def add_docstring_from_comments(api_doc, comments):
    if api_doc is None or not comments: return
    api_doc.docstring = '\n'.join([line for (line, lineno) in comments])
    api_doc.docstring_lineno = comments[0][1]

#/////////////////////////////////////////////////////////////////
#{ Tree tokens
#/////////////////////////////////////////////////////////////////

def _join_toktree(s1, s2):
    # Join them.  s1 = left side; s2 = right side.
    if (s2=='' or s1=='' or
        s1 in ('-','`') or s2 in ('}',']',')','`',':') or
        s2[0] in ('.',',') or s1[-1] in ('(','[','{','.','\n',' ') or
        (s2[0] == '(' and s1[-1] not in (',','='))):
        return '%s%s' % (s1,s2)
    elif (spacing=='tight' and
          s1[-1] in '+-*/=,' or s2[0] in '+-*/=,'):
        return '%s%s' % (s1, s2)
    else:
        return '%s %s' % (s1, s2)

def _pp_toktree_add_piece(spacing, pieces, piece):
    s1 = pieces[-1]
    s2 = piece
    
    if (s2=='' or s1=='' or
        s1 in ('-','`') or s2 in ('}',']',')','`',':') or
        s2[0] in ('.',',') or s1[-1] in ('(','[','{','.','\n',' ') or
        (s2[0] == '(' and s1[-1] not in (',','='))):
        pass
    elif (spacing=='tight' and
          s1[-1] in '+-*/=,' or s2[0] in '+-*/=,'):
        pass
    else:
        pieces.append(' ')
        
    pieces.append(piece)

def pp_toktree(elts, spacing='normal', indent=0):
    pieces = ['']
    _pp_toktree(elts, spacing, indent, pieces)
    return ''.join(pieces)
    
def _pp_toktree(elts, spacing, indent, pieces):
    add_piece = _pp_toktree_add_piece
    
    for elt in elts:
        # Put a blank line before class & def statements.
        if elt == (token.NAME, 'class') or elt == (token.NAME, 'def'):
            add_piece(spacing, pieces, '\n%s' % ('    '*indent))

        if isinstance(elt, tuple):
            if elt[0] == token.NEWLINE:
                add_piece(spacing, pieces, '    '+elt[1])
                add_piece(spacing, pieces, '\n%s' % ('    '*indent))
            elif elt[0] == token.INDENT:
                add_piece(spacing, pieces, '    ')
                indent += 1
            elif elt[0] == token.DEDENT:
                assert pieces[-1] == '    '
                pieces.pop()
                indent -= 1
            elif elt[0] == tokenize.COMMENT:
                add_piece(spacing, pieces, elt[1].rstrip() + '\n')
                add_piece('    '*indent)
            else:
                add_piece(spacing, pieces, elt[1])
        else:
            _pp_toktree(elt, spacing, indent, pieces)
        
#/////////////////////////////////////////////////////////////////
#{ Helper Functions
#/////////////////////////////////////////////////////////////////

def get_module_encoding(filename):
    """
    @see: U{PEP 263<http://www.python.org/peps/pep-0263.html>}
    """
    module_file = open(filename, 'rU')
    try:
        lines = [module_file.readline() for i in range(2)]
        if lines[0].startswith('\xef\xbb\xbf'):
            return 'utf-8'
        else:
            for line in lines:
                m = re.search("coding[:=]\s*([-\w.]+)", line)
                if m: return m.group(1)
                
        # Fall back on Python's default encoding.
        return 'iso-8859-1' # aka 'latin-1'
    finally:
        module_file.close()
        
def _get_module_name(filename, package_doc):
    """
    Return (dotted_name, is_package)
    """
    name = re.sub(r'.py\w?$', '', os.path.split(filename)[1])
    if name == '__init__':
        is_package = True
        name = os.path.split(os.path.split(filename)[0])[1]
    else:
        is_package = False

    # [XX] if the module contains a script, then `name` may not
    # necessarily be a valid identifier -- which will cause
    # DottedName to raise an exception.  Is that what I want?
    if package_doc is None:
        dotted_name = DottedName(name)
    else:
        dotted_name = DottedName(package_doc.canonical_name, name)

    # Check if the module looks like it's shadowed by a variable.
    # If so, then add a "'" to the end of its canonical name, to
    # distinguish it from the variable.
    if package_doc is not None and name in package_doc.variables:
        vardoc = package_doc.variables[name]
        if (vardoc.value not in (None, UNKNOWN) and
            vardoc.imported_from != dotted_name):
            log.warning("Module %s might be shadowed by a variable with "
                        "the same name." % dotted_name)
            dotted_name = DottedName(str(dotted_name)+"'")

    return dotted_name, is_package

def flatten(lst, out=None):
    """
    @return: a flat list containing the leaves of the given nested
        list.
    @param lst: The nested list that should be flattened.
    """
    if out is None: out = []
    for elt in lst:
        if isinstance(elt, (list, tuple)):
            flatten(elt, out)
        else:
            out.append(elt)
    return out


########NEW FILE########
__FILENAME__ = docstringparser
# epydoc -- Docstring processing
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: docstringparser.py 1689 2008-01-30 17:01:02Z edloper $

"""
Parse docstrings and handle any fields it defines, such as C{@type}
and C{@author}.  Fields are used to describe specific information
about an object.  There are two classes of fields: X{simple fields}
and X{special fields}.

Simple fields are fields that get stored directly in an C{APIDoc}'s
metadata dictionary, without any special processing.  The set of
simple fields is defined by the list L{STANDARD_FIELDS}, whose
elements are L{DocstringField}s.

Special fields are fields that perform some sort of processing on the
C{APIDoc}, or add information to attributes other than the metadata
dictionary.  Special fields are are handled by field handler
functions, which are registered using L{register_field_handler}.
"""
__docformat__ = 'epytext en'


######################################################################
## Imports
######################################################################

import re, sys
from epydoc import markup
from epydoc.markup import epytext
from epydoc.apidoc import *
from epydoc.docintrospecter import introspect_docstring_lineno
from epydoc.util import py_src_filename
from epydoc import log
import epydoc.docparser
import __builtin__, exceptions

######################################################################
# Docstring Fields
######################################################################

class DocstringField:
    """
    A simple docstring field, which can be used to describe specific
    information about an object, such as its author or its version.
    Simple docstring fields are fields that take no arguments, and
    are displayed as simple sections.

    @ivar tags: The set of tags that can be used to identify this
        field.
    @ivar singular: The label that should be used to identify this
        field in the output, if the field contains one value.
    @ivar plural: The label that should be used to identify this
        field in the output, if the field contains multiple values.
    @ivar short: If true, then multiple values should be combined
        into a single comma-delimited list.  If false, then
        multiple values should be listed separately in a bulleted
        list.
    @ivar multivalue: If true, then multiple values may be given
        for this field; if false, then this field can only take a
        single value, and a warning should be issued if it is
        redefined.
    @ivar takes_arg: If true, then this field expects an argument;
        and a separate field section will be constructed for each
        argument value.  The label (and plural label) should include
        a '%s' to mark where the argument's string rep should be
        added.
    """
    def __init__(self, tags, label, plural=None,
                 short=0, multivalue=1, takes_arg=0,
                 varnames=None):
        if type(tags) in (list, tuple):
            self.tags = tuple(tags)
        elif type(tags) is str:
            self.tags = (tags,)
        else: raise TypeError('Bad tags: %s' % tags)
        self.singular = label
        if plural is None: self.plural = label
        else: self.plural = plural
        self.multivalue = multivalue
        self.short = short
        self.takes_arg = takes_arg
        self.varnames = varnames or []

    def __cmp__(self, other):
        if not isinstance(other, DocstringField): return -1
        return cmp(self.tags, other.tags)
    
    def __hash__(self):
        return hash(self.tags)

    def __repr__(self):
        return '<Field: %s>' % self.tags[0]

STANDARD_FIELDS = [
    #: A list of the standard simple fields accepted by epydoc.  This
    #: list can be augmented at run-time by a docstring with the special
    #: C{@deffield} field.  The order in which fields are listed here
    #: determines the order in which they will be displayed in the
    #: output.
    
    # If it's deprecated, put that first.
    DocstringField(['deprecated', 'depreciated'],
             'Deprecated', multivalue=0, varnames=['__deprecated__']),

    # Status info
    DocstringField(['version'], 'Version', multivalue=0,
                   varnames=['__version__']),
    DocstringField(['date'], 'Date', multivalue=0,
                   varnames=['__date__']),
    DocstringField(['status'], 'Status', multivalue=0),
    
    # Bibliographic Info
    DocstringField(['author', 'authors'], 'Author', 'Authors', short=1,
                   varnames=['__author__', '__authors__']),
    DocstringField(['contact'], 'Contact', 'Contacts', short=1,
                   varnames=['__contact__']),
    DocstringField(['organization', 'org'],
                   'Organization', 'Organizations'),
    DocstringField(['copyright', '(c)'], 'Copyright', multivalue=0,
                   varnames=['__copyright__']),
    DocstringField(['license'], 'License', multivalue=0,
                   varnames=['__license__']),

    # Various warnings etc.
    DocstringField(['bug'], 'Bug', 'Bugs'),
    DocstringField(['warning', 'warn'], 'Warning', 'Warnings'),
    DocstringField(['attention'], 'Attention'),
    DocstringField(['note'], 'Note', 'Notes'),

    # Formal conditions
    DocstringField(['requires', 'require', 'requirement'], 'Requires'),
    DocstringField(['precondition', 'precond'],
             'Precondition', 'Preconditions'),
    DocstringField(['postcondition', 'postcond'],
             'Postcondition', 'Postconditions'),
    DocstringField(['invariant'], 'Invariant'),

    # When was it introduced (version # or date)
    DocstringField(['since'], 'Since', multivalue=0),

    # Changes made
    DocstringField(['change', 'changed'], 'Change Log'),
                   
    # Crossreferences
    DocstringField(['see', 'seealso'], 'See Also', short=1),

    # Future Work
    DocstringField(['todo'], 'To Do', takes_arg=True),

    # Permissions (used by zope-based projects)
    DocstringField(['permission', 'permissions'], 'Permission', 'Permissions')
    ]

######################################################################
#{ Docstring Parsing
######################################################################

DEFAULT_DOCFORMAT = 'epytext'
"""The name of the default markup languge used to process docstrings."""

# [xx] keep track of which ones we've already done, in case we're
# asked to process one twice?  e.g., for @include we might have to
# parse the included docstring earlier than we might otherwise..??

def parse_docstring(api_doc, docindex, suppress_warnings=[]):
    """
    Process the given C{APIDoc}'s docstring.  In particular, populate
    the C{APIDoc}'s C{descr} and C{summary} attributes, and add any
    information provided by fields in the docstring.
    
    @param docindex: A DocIndex, used to find the containing
        module (to look up the docformat); and to find any
        user docfields defined by containing objects.
    @param suppress_warnings: A set of objects for which docstring
        warnings should be suppressed.
    """
    if api_doc.metadata is not UNKNOWN:
        if not (isinstance(api_doc, RoutineDoc)
                and api_doc.canonical_name[-1] == '__init__'):
            log.debug("%s's docstring processed twice" %
                      api_doc.canonical_name)
        return
        
    initialize_api_doc(api_doc)

    # If there's no docstring, then check for special variables (e.g.,
    # __version__), and then return -- there's nothing else to do.
    if (api_doc.docstring in (None, UNKNOWN)):
        if isinstance(api_doc, NamespaceDoc):
            for field in STANDARD_FIELDS + user_docfields(api_doc, docindex):
                add_metadata_from_var(api_doc, field)
        return

    # Remove leading indentation from the docstring.
    api_doc.docstring = unindent_docstring(api_doc.docstring)

    # Decide which docformat is used by this module.
    docformat = get_docformat(api_doc, docindex)

    # A list of markup errors from parsing.
    parse_errors = []
    
    # Extract a signature from the docstring, if it has one.  This
    # overrides any signature we got via introspection/parsing.
    if isinstance(api_doc, RoutineDoc):
        parse_function_signature(api_doc, None, docformat, parse_errors)

    # Parse the docstring.  Any errors encountered are stored as
    # `ParseError` objects in the errors list.
    parsed_docstring = markup.parse(api_doc.docstring, docformat,
                                    parse_errors)
        
    # Divide the docstring into a description and a list of
    # fields.
    descr, fields = parsed_docstring.split_fields(parse_errors)
    api_doc.descr = descr

    field_warnings = []

    # Handle the constructor fields that have been defined in the class
    # docstring. This code assumes that a class docstring is parsed before
    # the same class __init__ docstring.
    if isinstance(api_doc, ClassDoc):

        # Parse ahead the __init__ docstring for this class
        initvar = api_doc.variables.get('__init__')
        if initvar and isinstance(initvar.value, RoutineDoc):
            init_api_doc = initvar.value
            parse_docstring(init_api_doc, docindex, suppress_warnings)

            parse_function_signature(init_api_doc, api_doc,
                                     docformat, parse_errors)
            init_fields = split_init_fields(fields, field_warnings)

            # Process fields
            for field in init_fields:
                try:
                    process_field(init_api_doc, docindex, field.tag(),
                                    field.arg(), field.body())
                except ValueError, e: field_warnings.append(str(e))

    # Process fields
    for field in fields:
        try:
            process_field(api_doc, docindex, field.tag(),
                               field.arg(), field.body())
        except ValueError, e: field_warnings.append(str(e))

    # Check to make sure that all type parameters correspond to
    # some documented parameter.
    check_type_fields(api_doc, field_warnings)

    # Check for special variables (e.g., __version__)
    if isinstance(api_doc, NamespaceDoc):
        for field in STANDARD_FIELDS + user_docfields(api_doc, docindex):
            add_metadata_from_var(api_doc, field)

    # Extract a summary
    if api_doc.summary is None and api_doc.descr is not None:
        api_doc.summary, api_doc.other_docs = api_doc.descr.summary()

    # If the summary is empty, but the return field is not, then use
    # the return field to generate a summary description.
    if (isinstance(api_doc, RoutineDoc) and api_doc.summary is None and
        api_doc.return_descr is not None):
        s, o = api_doc.return_descr.summary()
        api_doc.summary = RETURN_PDS + s
        api_doc.other_docs = o

    # [XX] Make sure we don't have types/param descrs for unknown
    # vars/params?

    # Report any errors that occured
    if api_doc in suppress_warnings:
        if parse_errors or field_warnings:
            log.info("Suppressing docstring warnings for %s, since it "
                     "is not included in the documented set." %
                     api_doc.canonical_name)
    else:
        report_errors(api_doc, docindex, parse_errors, field_warnings)

def add_metadata_from_var(api_doc, field):
    for varname in field.varnames:
        # Check if api_doc has a variable w/ the given name.
        if varname not in api_doc.variables: continue

        # Check moved here from before the for loop because we expect to
        # reach rarely this point. The loop below is to be performed more than
        # once only for fields with more than one varname, which currently is
        # only 'author'.
        for md in api_doc.metadata:
            if field == md[0]:
                return # We already have a value for this metadata.

        var_doc = api_doc.variables[varname]
        if var_doc.value is UNKNOWN: continue
        val_doc = var_doc.value
        value = []

        # Try extracting the value from the pyval.
        ok_types = (basestring, int, float, bool, type(None))
        if val_doc.pyval is not UNKNOWN:
            if isinstance(val_doc.pyval, ok_types):
                value = [val_doc.pyval]
            elif field.multivalue:
                if isinstance(val_doc.pyval, (tuple, list)):
                    for elt in val_doc.pyval:
                        if not isinstance(elt, ok_types): break
                    else:
                        value = list(val_doc.pyval)

        # Try extracting the value from the parse tree.
        elif val_doc.toktree is not UNKNOWN:
            try: value = [epydoc.docparser.parse_string(val_doc.toktree)]
            except KeyboardInterrupt: raise
            except: pass
            if field.multivalue and not value:
                try: value = epydoc.docparser.parse_string_list(val_doc.toktree)
                except KeyboardInterrupt: raise
                except: raise
                
        # Add any values that we found.
        for elt in value:
            if isinstance(elt, str):
                elt = decode_with_backslashreplace(elt)
            else:
                elt = unicode(elt)
            elt = epytext.ParsedEpytextDocstring(
                epytext.parse_as_para(elt), inline=True)

            # Add in the metadata and remove from the variables
            api_doc.metadata.append( (field, varname, elt) )

        # Remove the variable itself (unless it's documented)
        if var_doc.docstring in (None, UNKNOWN):
            del api_doc.variables[varname]
            if api_doc.sort_spec is not UNKNOWN:
                try: api_doc.sort_spec.remove(varname)
                except ValueError: pass

def initialize_api_doc(api_doc):
    """A helper function for L{parse_docstring()} that initializes
    the attributes that C{parse_docstring()} will write to."""
    if api_doc.descr is UNKNOWN:
        api_doc.descr = None
    if api_doc.summary is UNKNOWN:
        api_doc.summary = None
    if api_doc.metadata is UNKNOWN:
        api_doc.metadata = []
    if isinstance(api_doc, RoutineDoc):
        if api_doc.arg_descrs is UNKNOWN:
            api_doc.arg_descrs = []
        if api_doc.arg_types is UNKNOWN:
            api_doc.arg_types = {}
        if api_doc.return_descr is UNKNOWN:
            api_doc.return_descr = None
        if api_doc.return_type is UNKNOWN:
            api_doc.return_type = None
        if api_doc.exception_descrs is UNKNOWN:
            api_doc.exception_descrs = []
    if isinstance(api_doc, (VariableDoc, PropertyDoc)):
        if api_doc.type_descr is UNKNOWN:
            api_doc.type_descr = None
    if isinstance(api_doc, NamespaceDoc):
        if api_doc.group_specs is UNKNOWN:
            api_doc.group_specs = []
        if api_doc.sort_spec is UNKNOWN:
            api_doc.sort_spec = []

def split_init_fields(fields, warnings):
    """
    Remove the fields related to the constructor from a class docstring
    fields list.

    @param fields: The fields to process. The list will be modified in place
    @type fields: C{list} of L{markup.Field}
    @param warnings: A list to emit processing warnings
    @type warnings: C{list}
    @return: The C{fields} items to be applied to the C{__init__} method
    @rtype: C{list} of L{markup.Field}
    """
    init_fields = []

    # Split fields in lists according to their argument, keeping order.
    arg_fields = {}
    args_order = []
    i = 0
    while i < len(fields):
        field = fields[i]

        # gather together all the fields with the same arg
        if field.arg() is not None:
            arg_fields.setdefault(field.arg(), []).append(fields.pop(i))
            args_order.append(field.arg())
        else:
            i += 1

    # Now check that for each argument there is at most a single variable
    # and a single parameter, and at most a single type for each of them.
    for arg in args_order:
        ff = arg_fields.pop(arg, None)
        if ff is None:
            continue

        var = tvar = par = tpar = None
        for field in ff:
            if field.tag() in VARIABLE_TAGS:
                if var is None:
                    var = field
                    fields.append(field)
                else:
                    warnings.append(
                        "There is more than one variable named '%s'"
                        % arg)
            elif field.tag() in PARAMETER_TAGS:
                if par is None:
                    par = field
                    init_fields.append(field)
                else:
                    warnings.append(
                        "There is more than one parameter named '%s'"
                        % arg)

            elif field.tag() == 'type':
                if var is None and par is None:
                    # type before obj
                    tvar = tpar = field
                else:
                    if var is not None and tvar is None:
                        tvar = field
                    if par is not None and tpar is None:
                        tpar = field

            elif field.tag() in EXCEPTION_TAGS:
                init_fields.append(field)

            else: # Unespected field
                fields.append(field)

        # Put selected types into the proper output lists
        if tvar is not None:
            if var is not None:
                fields.append(tvar)
            else:
                pass # [xx] warn about type w/o object?

        if tpar is not None:
            if par is not None:
                init_fields.append(tpar)
            else:
                pass # [xx] warn about type w/o object?

    return init_fields

def report_errors(api_doc, docindex, parse_errors, field_warnings):
    """A helper function for L{parse_docstring()} that reports any
    markup warnings and field warnings that we encountered while
    processing C{api_doc}'s docstring."""
    if not parse_errors and not field_warnings: return

    # Get the name of the item containing the error, and the
    # filename of its containing module.
    name = api_doc.canonical_name
    module = api_doc.defining_module
    if module is not UNKNOWN and module.filename not in (None, UNKNOWN):
        try: filename = py_src_filename(module.filename)
        except: filename = module.filename
    else:
        filename = '??'

    # [xx] Don't report markup errors for standard builtins.
    # n.b. that we must use 'is' to compare pyvals here -- if we use
    # 'in' or '==', then a user __cmp__ method might raise an
    # exception, or lie.
    if isinstance(api_doc, ValueDoc) and api_doc != module:
        if module not in (None, UNKNOWN) and module.pyval is exceptions:
            return
        for builtin_val in __builtin__.__dict__.values():
            if builtin_val is api_doc.pyval:
                return
        
    # Get the start line of the docstring containing the error.
    startline = api_doc.docstring_lineno
    if startline in (None, UNKNOWN):
        startline = introspect_docstring_lineno(api_doc)
        if startline in (None, UNKNOWN):
            startline = None

    # Display a block header.
    header = 'File %s, ' % filename
    if startline is not None:
        header += 'line %d, ' % startline
    header += 'in %s' % name
    log.start_block(header)
    

    # Display all parse errors.  But first, combine any errors
    # with duplicate description messages.
    if startline is None:
        # remove dups, but keep original order:
        dups = {}
        for error in parse_errors:
            message = error.descr()
            if message not in dups:
                log.docstring_warning(message)
                dups[message] = 1
    else:
        # Combine line number fields for dup messages:
        messages = {} # maps message -> list of linenum
        for error in parse_errors:
            error.set_linenum_offset(startline)
            message = error.descr()
            messages.setdefault(message, []).append(error.linenum())
        message_items = messages.items()
        message_items.sort(lambda a,b:cmp(min(a[1]), min(b[1])))
        for message, linenums in message_items:
            linenums = [n for n in linenums if n is not None]
            if len(linenums) == 0:
                log.docstring_warning(message)
            elif len(linenums) == 1:
                log.docstring_warning("Line %s: %s" % (linenums[0], message))
            else:
                linenums = ', '.join(['%s' % l for l in linenums])
                log.docstring_warning("Lines %s: %s" % (linenums, message))

    # Display all field warnings.
    for warning in field_warnings:
        log.docstring_warning(warning)

    # End the message block.
    log.end_block()

RETURN_PDS = markup.parse('Returns:', markup='epytext')
"""A ParsedDocstring containing the text 'Returns'.  This is used to
construct summary descriptions for routines that have empty C{descr},
but non-empty C{return_descr}."""
RETURN_PDS._tree.children[0].attribs['inline'] = True

######################################################################
#{ Field Processing Error Messages
######################################################################

UNEXPECTED_ARG = '%r did not expect an argument'
EXPECTED_ARG = '%r expected an argument'
EXPECTED_SINGLE_ARG = '%r expected a single argument'
BAD_CONTEXT = 'Invalid context for %r'
REDEFINED = 'Redefinition of %s'
UNKNOWN_TAG = 'Unknown field tag %r'
BAD_PARAM = '@%s for unknown parameter %s'

######################################################################
#{ Field Processing
######################################################################

def process_field(api_doc, docindex, tag, arg, descr):
    """
    Process a single field, and use it to update C{api_doc}.  If
    C{tag} is the name of a special field, then call its handler
    function.  If C{tag} is the name of a simple field, then use
    C{process_simple_field} to process it.  Otherwise, check if it's a
    user-defined field, defined in this docstring or the docstring of
    a containing object; and if so, process it with
    C{process_simple_field}.

    @param tag: The field's tag, such as C{'author'}
    @param arg: The field's optional argument
    @param descr: The description following the field tag and
        argument.
    @raise ValueError: If a problem was encountered while processing
        the field.  The C{ValueError}'s string argument is an
        explanation of the problem, which should be displayed as a
        warning message.
    """
    # standard special fields
    if tag in _field_dispatch_table:
        handler = _field_dispatch_table[tag]
        handler(api_doc, docindex, tag, arg, descr)
        return

    # standard simple fields & user-defined fields
    for field in STANDARD_FIELDS + user_docfields(api_doc, docindex):
        if tag in field.tags:
            # [xx] check if it's redefined if it's not multivalue??
            if not field.takes_arg:
                _check(api_doc, tag, arg, expect_arg=False)
            api_doc.metadata.append((field, arg, descr))
            return

    # If we didn't handle the field, then report a warning.
    raise ValueError(UNKNOWN_TAG % tag)

def user_docfields(api_doc, docindex):
    """
    Return a list of user defined fields that can be used for the
    given object.  This list is taken from the given C{api_doc}, and
    any of its containing C{NamepaceDoc}s.

    @note: We assume here that a parent's docstring will always be
        parsed before its childrens'.  This is indeed the case when we
        are called via L{docbuilder.build_doc_index()}.  If a child's
        docstring is parsed before its parents, then its parent won't
        yet have had its C{extra_docstring_fields} attribute
        initialized.
    """
    docfields = []
    # Get any docfields from `api_doc` itself
    if api_doc.extra_docstring_fields not in (None, UNKNOWN):
        docfields += api_doc.extra_docstring_fields
    # Get any docfields from `api_doc`'s ancestors
    for i in range(len(api_doc.canonical_name)-1, 0, -1):
        ancestor = docindex.get_valdoc(api_doc.canonical_name[:i])
        if ancestor is not None \
        and ancestor.extra_docstring_fields not in (None, UNKNOWN):
            docfields += ancestor.extra_docstring_fields
    return docfields

_field_dispatch_table = {}
def register_field_handler(handler, *field_tags):
    """
    Register the given field handler function for processing any
    of the given field tags.  Field handler functions should
    have the following signature:

        >>> def field_handler(api_doc, docindex, tag, arg, descr):
        ...     '''update api_doc in response to the field.'''

    Where C{api_doc} is the documentation object to update;
    C{docindex} is a L{DocIndex} that can be used to look up the
    documentation for related objects; C{tag} is the field tag that
    was used; C{arg} is the optional argument; and C{descr} is the
    description following the field tag and argument.
    """
    for field_tag in field_tags:
        _field_dispatch_table[field_tag] = handler

######################################################################
#{ Field Handler Functions
######################################################################

def process_summary_field(api_doc, docindex, tag, arg, descr):
    """Store C{descr} in C{api_doc.summary}"""
    _check(api_doc, tag, arg, expect_arg=False)
    if api_doc.summary is not None:
        raise ValueError(REDEFINED % tag)
    api_doc.summary = descr

def process_include_field(api_doc, docindex, tag, arg, descr):
    """Copy the docstring contents from the object named in C{descr}"""
    _check(api_doc, tag, arg, expect_arg=False)
    # options:
    #   a. just append the descr to our own
    #   b. append descr and update metadata
    #   c. append descr and process all fields.
    # in any case, mark any errors we may find as coming from an
    # imported docstring.
    
    # how does this interact with documentation inheritance??
    raise ValueError('%s not implemented yet' % tag)

def process_undocumented_field(api_doc, docindex, tag, arg, descr):
    """Remove any documentation for the variables named in C{descr}"""
    _check(api_doc, tag, arg, context=NamespaceDoc, expect_arg=False)
    for ident in _descr_to_identifiers(descr):
        var_name_re = re.compile('^%s$' % ident.replace('*', '(.*)'))
        for var_name, var_doc in api_doc.variables.items():
            if var_name_re.match(var_name):
                # Remove the variable from `variables`.
                api_doc.variables.pop(var_name, None)
                if api_doc.sort_spec is not UNKNOWN:
                    try: api_doc.sort_spec.remove(var_name)
                    except ValueError: pass
        # For modules, remove any submodules that match var_name_re.
        if isinstance(api_doc, ModuleDoc):
            removed = set([m for m in api_doc.submodules
                           if var_name_re.match(m.canonical_name[-1])])
            if removed:
                # Remove the indicated submodules from this module.
                api_doc.submodules = [m for m in api_doc.submodules
                                      if m not in removed]
                # Remove all ancestors of the indicated submodules
                # from the docindex root.  E.g., if module x
                # declares y to be undocumented, then x.y.z should
                # also be undocumented.
                for elt in docindex.root[:]:
                    for m in removed:
                        if m.canonical_name.dominates(elt.canonical_name):
                            docindex.root.remove(elt)

def process_group_field(api_doc, docindex, tag, arg, descr):
    """Define a group named C{arg} containing the variables whose
    names are listed in C{descr}."""
    _check(api_doc, tag, arg, context=NamespaceDoc, expect_arg=True)
    api_doc.group_specs.append( (arg, _descr_to_identifiers(descr)) )
    # [xx] should this also set sort order?

def process_deffield_field(api_doc, docindex, tag, arg, descr):
    """Define a new custom field."""
    _check(api_doc, tag, arg, expect_arg=True)
    if api_doc.extra_docstring_fields is UNKNOWN:
        api_doc.extra_docstring_fields = []
    try:
        docstring_field = _descr_to_docstring_field(arg, descr)
        docstring_field.varnames.append("__%s__" % arg)
        api_doc.extra_docstring_fields.append(docstring_field)
    except ValueError, e:
        raise ValueError('Bad %s: %s' % (tag, e))

def process_raise_field(api_doc, docindex, tag, arg, descr):
    """Record the fact that C{api_doc} can raise the exception named
    C{tag} in C{api_doc.exception_descrs}."""
    _check(api_doc, tag, arg, context=RoutineDoc, expect_arg='single')
    try: name = DottedName(arg, strict=True)
    except DottedName.InvalidDottedName: name = arg
    api_doc.exception_descrs.append( (name, descr) )

def process_sort_field(api_doc, docindex, tag, arg, descr):
    _check(api_doc, tag, arg, context=NamespaceDoc, expect_arg=False)
    api_doc.sort_spec = _descr_to_identifiers(descr) + api_doc.sort_spec

# [xx] should I notice when they give a type for an unknown var?
def process_type_field(api_doc, docindex, tag, arg, descr):
    # In namespace, "@type var: ..." describes the type of a var.
    if isinstance(api_doc, NamespaceDoc):
        _check(api_doc, tag, arg, expect_arg='single')
        set_var_type(api_doc, arg, descr)

    # For variables & properties, "@type: ..." describes the variable.
    elif isinstance(api_doc, (VariableDoc, PropertyDoc)):
        _check(api_doc, tag, arg, expect_arg=False)
        if api_doc.type_descr is not None:
            raise ValueError(REDEFINED % tag)
        api_doc.type_descr = descr

    # For routines, "@type param: ..." describes a parameter.
    elif isinstance(api_doc, RoutineDoc):
        _check(api_doc, tag, arg, expect_arg='single')
        if arg in api_doc.arg_types:
            raise ValueError(REDEFINED % ('type for '+arg))
        api_doc.arg_types[arg] = descr

    else:
        raise ValueError(BAD_CONTEXT % tag)
        
def process_var_field(api_doc, docindex, tag, arg, descr):
    _check(api_doc, tag, arg, context=ModuleDoc, expect_arg=True)
    for ident in re.split('[:;, ] *', arg):
        set_var_descr(api_doc, ident, descr)
        
def process_cvar_field(api_doc, docindex, tag, arg, descr):
    # If @cvar is used *within* a variable, then use it as the
    # variable's description, and treat the variable as a class var.
    if (isinstance(api_doc, VariableDoc) and
        isinstance(api_doc.container, ClassDoc)):
        _check(api_doc, tag, arg, expect_arg=False)
        api_doc.is_instvar = False
        api_doc.descr = markup.ConcatenatedDocstring(api_doc.descr, descr)
        api_doc.summary, api_doc.other_docs = descr.summary()

    # Otherwise, @cvar should be used in a class.
    else:
        _check(api_doc, tag, arg, context=ClassDoc, expect_arg=True)
        for ident in re.split('[:;, ] *', arg):
            set_var_descr(api_doc, ident, descr)
            api_doc.variables[ident].is_instvar = False
        
def process_ivar_field(api_doc, docindex, tag, arg, descr):
    # If @ivar is used *within* a variable, then use it as the
    # variable's description, and treat the variable as an instvar.
    if (isinstance(api_doc, VariableDoc) and
        isinstance(api_doc.container, ClassDoc)):
        _check(api_doc, tag, arg, expect_arg=False)
        # require that there be no other descr?
        api_doc.is_instvar = True
        api_doc.descr = markup.ConcatenatedDocstring(api_doc.descr, descr)
        api_doc.summary, api_doc.other_docs = descr.summary()

    # Otherwise, @ivar should be used in a class.
    else:
        _check(api_doc, tag, arg, context=ClassDoc, expect_arg=True)
        for ident in re.split('[:;, ] *', arg):
            set_var_descr(api_doc, ident, descr)
            api_doc.variables[ident].is_instvar = True

# [xx] '@return: foo' used to get used as a descr if no other
# descr was present.  is that still true?
def process_return_field(api_doc, docindex, tag, arg, descr):
    _check(api_doc, tag, arg, context=RoutineDoc, expect_arg=False)
    if api_doc.return_descr is not None:
        raise ValueError(REDEFINED % 'return value description')
    api_doc.return_descr = descr

def process_rtype_field(api_doc, docindex, tag, arg, descr):
    _check(api_doc, tag, arg,
           context=(RoutineDoc, PropertyDoc), expect_arg=False)
    if isinstance(api_doc, RoutineDoc):
        if api_doc.return_type is not None:
            raise ValueError(REDEFINED % 'return value type')
        api_doc.return_type = descr

    elif isinstance(api_doc, PropertyDoc):
        _check(api_doc, tag, arg, expect_arg=False)
        if api_doc.type_descr is not None:
            raise ValueError(REDEFINED % tag)
        api_doc.type_descr = descr

def process_arg_field(api_doc, docindex, tag, arg, descr):
    _check(api_doc, tag, arg, context=RoutineDoc, expect_arg=True)
    idents = re.split('[:;, ] *', arg)
    api_doc.arg_descrs.append( (idents, descr) )
    # Check to make sure that the documented parameter(s) are
    # actually part of the function signature.
    all_args = api_doc.all_args()
    if all_args not in (['...'], UNKNOWN):
        bad_params = ['"%s"' % i for i in idents if i not in all_args]
        if bad_params:
            raise ValueError(BAD_PARAM % (tag, ', '.join(bad_params)))

def process_kwarg_field(api_doc, docindex, tag, arg, descr):
    # [xx] these should -not- be checked if they exist..
    # and listed separately or not??
    _check(api_doc, tag, arg, context=RoutineDoc, expect_arg=True)
    idents = re.split('[:;, ] *', arg)
    api_doc.arg_descrs.append( (idents, descr) )

register_field_handler(process_group_field, 'group')
register_field_handler(process_deffield_field, 'deffield', 'newfield')
register_field_handler(process_sort_field, 'sort')
register_field_handler(process_summary_field, 'summary')
register_field_handler(process_undocumented_field, 'undocumented')
register_field_handler(process_include_field, 'include')
register_field_handler(process_var_field, 'var', 'variable')
register_field_handler(process_type_field, 'type')
register_field_handler(process_cvar_field, 'cvar', 'cvariable')
register_field_handler(process_ivar_field, 'ivar', 'ivariable')
register_field_handler(process_return_field, 'return', 'returns')
register_field_handler(process_rtype_field, 'rtype', 'returntype')
register_field_handler(process_arg_field, 'arg', 'argument',
                                          'parameter', 'param')
register_field_handler(process_kwarg_field, 'kwarg', 'keyword', 'kwparam')
register_field_handler(process_raise_field, 'raise', 'raises',
                                            'except', 'exception')

# Tags related to function parameters
PARAMETER_TAGS = ('arg', 'argument', 'parameter', 'param',
                  'kwarg', 'keyword', 'kwparam')

# Tags related to variables in a class
VARIABLE_TAGS = ('cvar', 'cvariable', 'ivar', 'ivariable')

# Tags related to exceptions
EXCEPTION_TAGS = ('raise', 'raises', 'except', 'exception')

######################################################################
#{ Helper Functions
######################################################################

def check_type_fields(api_doc, field_warnings):
    """Check to make sure that all type fields correspond to some
    documented parameter; if not, append a warning to field_warnings."""
    if isinstance(api_doc, RoutineDoc):
        for arg in api_doc.arg_types:
            if arg not in api_doc.all_args():
                for args, descr in api_doc.arg_descrs:
                    if arg in args:
                        break
                else:
                    field_warnings.append(BAD_PARAM % ('type', '"%s"' % arg))

def set_var_descr(api_doc, ident, descr):
    if ident not in api_doc.variables:
        api_doc.variables[ident] = VariableDoc(
            container=api_doc, name=ident,
            canonical_name=api_doc.canonical_name+ident)
                                      
    var_doc = api_doc.variables[ident]
    if var_doc.descr not in (None, UNKNOWN):
        raise ValueError(REDEFINED % ('description for '+ident))
    var_doc.descr = descr
    if var_doc.summary in (None, UNKNOWN):
        var_doc.summary, var_doc.other_docs = var_doc.descr.summary()

def set_var_type(api_doc, ident, descr):
    if ident not in api_doc.variables:
        api_doc.variables[ident] = VariableDoc(
            container=api_doc, name=ident,
            canonical_name=api_doc.canonical_name+ident)
        
    var_doc = api_doc.variables[ident]
    if var_doc.type_descr not in (None, UNKNOWN):
        raise ValueError(REDEFINED % ('type for '+ident))
    var_doc.type_descr = descr
        
def _check(api_doc, tag, arg, context=None, expect_arg=None):
    if context is not None:
        if not isinstance(api_doc, context):
            raise ValueError(BAD_CONTEXT % tag)
    if expect_arg is not None:
        if expect_arg == True:
            if arg is None:
                raise ValueError(EXPECTED_ARG % tag)
        elif expect_arg == False:
            if arg is not None:
                raise ValueError(UNEXPECTED_ARG % tag)
        elif expect_arg == 'single':
            if (arg is None or ' ' in arg):
                raise ValueError(EXPECTED_SINGLE_ARG % tag)
        else:
            assert 0, 'bad value for expect_arg'

def get_docformat(api_doc, docindex):
    """
    Return the name of the markup language that should be used to
    parse the API documentation for the given object.
    """
    # Find the module that defines api_doc.
    module = api_doc.defining_module
    # Look up its docformat.
    if module is not UNKNOWN and module.docformat not in (None, UNKNOWN):
        docformat = module.docformat
    else:
        docformat = DEFAULT_DOCFORMAT
    # Convert to lower case & strip region codes.
    try: return docformat.lower().split()[0]
    except: return DEFAULT_DOCFORMAT

def unindent_docstring(docstring):
    # [xx] copied from inspect.getdoc(); we can't use inspect.getdoc()
    # itself, since it expects an object, not a string.
    
    if not docstring: return ''
    lines = docstring.expandtabs().split('\n')

    # Find minimum indentation of any non-blank lines after first line.
    margin = sys.maxint
    for line in lines[1:]:
        content = len(line.lstrip())
        if content:
            indent = len(line) - content
            margin = min(margin, indent)
    # Remove indentation.
    if lines:
        lines[0] = lines[0].lstrip()
    if margin < sys.maxint:
        for i in range(1, len(lines)): lines[i] = lines[i][margin:]
    # Remove any trailing (but not leading!) blank lines.
    while lines and not lines[-1]:
        lines.pop()
    #while lines and not lines[0]:
    #    lines.pop(0)
    return '\n'.join(lines)
                           
_IDENTIFIER_LIST_REGEXP = re.compile(r'^[\w.\*]+([\s,:;]\s*[\w.\*]+)*$')
def _descr_to_identifiers(descr):
    """
    Given a C{ParsedDocstring} that contains a list of identifiers,
    return a list of those identifiers.  This is used by fields such
    as C{@group} and C{@sort}, which expect lists of identifiers as
    their values.  To extract the identifiers, the docstring is first
    converted to plaintext, and then split.  The plaintext content of
    the docstring must be a a list of identifiers, separated by
    spaces, commas, colons, or semicolons.
    
    @rtype: C{list} of C{string}
    @return: A list of the identifier names contained in C{descr}.
    @type descr: L{markup.ParsedDocstring}
    @param descr: A C{ParsedDocstring} containing a list of
        identifiers.
    @raise ValueError: If C{descr} does not contain a valid list of
        identifiers.
    """
    idents = descr.to_plaintext(None).strip()
    idents = re.sub(r'\s+', ' ', idents)
    if not _IDENTIFIER_LIST_REGEXP.match(idents):
        raise ValueError, 'Bad Identifier list: %r' % idents
    rval = re.split('[:;, ] *', idents)
    return rval
    
def _descr_to_docstring_field(arg, descr):
    tags = [s.lower() for s in re.split('[:;, ] *', arg)]
    descr = descr.to_plaintext(None).strip()
    args = re.split('[:;,] *', descr)
    if len(args) == 0 or len(args) > 3:
        raise ValueError, 'Wrong number of arguments'
    singular = args[0]
    if len(args) >= 2: plural = args[1]
    else: plural = None
    short = 0
    if len(args) >= 3:
        if args[2] == 'short': short = 1
        else: raise ValueError('Bad arg 2 (expected "short")')
    return DocstringField(tags, singular, plural, short)

######################################################################
#{ Function Signature Extraction
######################################################################

# [XX] todo: add optional type modifiers?
_SIGNATURE_RE = re.compile(
    # Class name (for builtin methods)
    r'^\s*((?P<self>\w+)\.)?' +
    # The function name (must match exactly) [XX] not anymore!
    r'(?P<func>\w+)' +
    # The parameters
    r'\((?P<params>(\s*\[?\s*\*{0,2}[\w\-\.]+(\s*=.+?)?'+
    r'(\s*\[?\s*,\s*\]?\s*\*{0,2}[\w\-\.]+(\s*=.+?)?)*\]*)?)\s*\)' +
    # The return value (optional)
    r'(\s*(->)\s*(?P<return>\S.*?))?'+
    # The end marker
    r'\s*(\n|\s+(--|<=+>)\s+|$|\.\s+|\.\n)')
"""A regular expression that is used to extract signatures from
docstrings."""
    
def parse_function_signature(func_doc, doc_source, docformat, parse_errors):
    """
    Construct the signature for a builtin function or method from
    its docstring.  If the docstring uses the standard convention
    of including a signature in the first line of the docstring
    (and formats that signature according to standard
    conventions), then it will be used to extract a signature.
    Otherwise, the signature will be set to a single varargs
    variable named C{"..."}.

    @param func_doc: The target object where to store parsed signature. Also
        container of the docstring to parse if doc_source is C{None}
    @type func_doc: L{RoutineDoc}
    @param doc_source: Contains the docstring to parse. If C{None}, parse
        L{func_doc} docstring instead
    @type doc_source: L{APIDoc}
    @rtype: C{None}
    """
    if doc_source is None:
        doc_source = func_doc

    # If there's no docstring, then don't do anything.
    if not doc_source.docstring: return False

    m = _SIGNATURE_RE.match(doc_source.docstring)
    if m is None: return False

    # Do I want to be this strict?
    # Notice that __init__ must match the class name instead, if the signature
    # comes from the class docstring
#     if not (m.group('func') == func_doc.canonical_name[-1] or
#             '_'+m.group('func') == func_doc.canonical_name[-1]):
#         log.warning("Not extracting function signature from %s's "
#                     "docstring, since the name doesn't match." %
#                     func_doc.canonical_name)
#         return False
    
    params = m.group('params')
    rtype = m.group('return')
    selfparam = m.group('self')
    
    # Extract the parameters from the signature.
    func_doc.posargs = []
    func_doc.vararg = None
    func_doc.kwarg = None
    if func_doc.posarg_defaults is UNKNOWN:
        func_doc.posarg_defaults = []
    if params:
        # Figure out which parameters are optional.
        while '[' in params or ']' in params:
            m2 = re.match(r'(.*)\[([^\[\]]+)\](.*)', params)
            if not m2: return False
            (start, mid, end) = m2.groups()
            mid = re.sub(r'((,|^)\s*[\w\-\.]+)', r'\1=...', mid)
            params = start+mid+end

        params = re.sub(r'=...=' , r'=', params)
        for name in params.split(','):
            if '=' in name:
                (name, default_repr) = name.split('=',1)
                default = GenericValueDoc(parse_repr=default_repr)
            else:
                default = None
            name = name.strip()
            if name == '...':
                func_doc.vararg = '...'
            elif name.startswith('**'):
                func_doc.kwarg = name[2:]
            elif name.startswith('*'):
                func_doc.vararg = name[1:]
            else:
                func_doc.posargs.append(name)
                if len(func_doc.posarg_defaults) < len(func_doc.posargs):
                    func_doc.posarg_defaults.append(default)
                elif default is not None:
                    argnum = len(func_doc.posargs)-1
                    func_doc.posarg_defaults[argnum] = default

    # Extract the return type/value from the signature
    if rtype:
        func_doc.return_type = markup.parse(rtype, docformat, parse_errors,
                                            inline=True)

    # Add the self parameter, if it was specified.
    if selfparam:
        func_doc.posargs.insert(0, selfparam)
        func_doc.posarg_defaults.insert(0, None)

    # Remove the signature from the docstring.
    doc_source.docstring = doc_source.docstring[m.end():]
        
    # We found a signature.
    return True


########NEW FILE########
__FILENAME__ = dotgraph
# epydoc -- Graph generation
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: dotgraph.py 1663 2007-11-07 15:29:47Z dvarrazzo $

"""
Render Graphviz directed graphs as images.  Below are some examples.

.. importgraph::

.. classtree:: epydoc.apidoc.APIDoc

.. packagetree:: epydoc

:see: `The Graphviz Homepage
       <http://www.research.att.com/sw/tools/graphviz/>`__
"""
__docformat__ = 'restructuredtext'

import re
import sys
from epydoc import log
from epydoc.apidoc import *
from epydoc.util import *
from epydoc.compat import * # Backwards compatibility

# colors for graphs of APIDocs
MODULE_BG = '#d8e8ff'
CLASS_BG = '#d8ffe8'
SELECTED_BG = '#ffd0d0'
BASECLASS_BG = '#e0b0a0'
SUBCLASS_BG = '#e0b0a0'
ROUTINE_BG = '#e8d0b0' # maybe?
INH_LINK_COLOR = '#800000'

######################################################################
#{ Dot Graphs
######################################################################

DOT_COMMAND = 'dot'
"""The command that should be used to spawn dot"""

class DotGraph:
    """
    A ``dot`` directed graph.  The contents of the graph are
    constructed from the following instance variables:

      - `nodes`: A list of `DotGraphNode`\\s, encoding the nodes
        that are present in the graph.  Each node is characterized
        a set of attributes, including an optional label.
      - `edges`: A list of `DotGraphEdge`\\s, encoding the edges
        that are present in the graph.  Each edge is characterized
        by a set of attributes, including an optional label.
      - `node_defaults`: Default attributes for nodes.
      - `edge_defaults`: Default attributes for edges.
      - `body`: A string that is appended as-is in the body of
        the graph.  This can be used to build more complex dot
        graphs.

    The `link()` method can be used to resolve crossreference links
    within the graph.  In particular, if the 'href' attribute of any
    node or edge is assigned a value of the form ``<name>``, then it
    will be replaced by the URL of the object with that name.  This
    applies to the `body` as well as the `nodes` and `edges`.

    To render the graph, use the methods `write()` and `render()`.
    Usually, you should call `link()` before you render the graph.
    """
    _uids = set()
    """A set of all uids that that have been generated, used to ensure
    that each new graph has a unique uid."""

    DEFAULT_NODE_DEFAULTS={'fontsize':10, 'fontname': 'Helvetica'}
    DEFAULT_EDGE_DEFAULTS={'fontsize':10, 'fontname': 'Helvetica'}
    
    def __init__(self, title, body='', node_defaults=None,
                 edge_defaults=None, caption=None):
        """
        Create a new `DotGraph`.
        """
        self.title = title
        """The title of the graph."""

        self.caption = caption
        """A caption for the graph."""
        
        self.nodes = []
        """A list of the nodes that are present in the graph.
        
        :type: ``list`` of `DotGraphNode`"""
        
        self.edges = []
        """A list of the edges that are present in the graph.
        
        :type: ``list`` of `DotGraphEdge`"""

        self.body = body
        """A string that should be included as-is in the body of the
        graph.
        
        :type: ``str``"""
        
        self.node_defaults = node_defaults or self.DEFAULT_NODE_DEFAULTS
        """Default attribute values for nodes."""
        
        self.edge_defaults = edge_defaults or self.DEFAULT_EDGE_DEFAULTS
        """Default attribute values for edges."""

        self.uid = re.sub(r'\W', '_', title).lower()
        """A unique identifier for this graph.  This can be used as a
        filename when rendering the graph.  No two `DotGraph`\s will
        have the same uid."""

        # Encode the title, if necessary.
        if isinstance(self.title, unicode):
            self.title = self.title.encode('ascii', 'xmlcharrefreplace')

        # Make sure the UID isn't too long.
        self.uid = self.uid[:30]
        
        # Make sure the UID is unique
        if self.uid in self._uids:
            n = 2
            while ('%s_%s' % (self.uid, n)) in self._uids: n += 1
            self.uid = '%s_%s' % (self.uid, n)
        self._uids.add(self.uid)

    def to_html(self, image_file, image_url, center=True):
        """
        Return the HTML code that should be uesd to display this graph
        (including a client-side image map).
        
        :param image_url: The URL of the image file for this graph;
            this should be generated separately with the `write()` method.
        """
        # If dotversion >1.8.10, then we can generate the image and
        # the cmapx with a single call to dot.  Otherwise, we need to
        # run dot twice.
        if get_dot_version() > [1,8,10]:
            cmapx = self._run_dot('-Tgif', '-o%s' % image_file, '-Tcmapx')
            if cmapx is None: return '' # failed to render
        else:
            if not self.write(image_file):
                return '' # failed to render
            cmapx = self.render('cmapx') or ''

        # Decode the cmapx (dot uses utf-8)
        try:
            cmapx = cmapx.decode('utf-8')
        except UnicodeDecodeError:
            log.debug('%s: unable to decode cmapx from dot; graph will '
                      'not have clickable regions' % image_file)
            cmapx = ''

        title = plaintext_to_html(self.title or '')
        caption = plaintext_to_html(self.caption or '')
        if title or caption:
            css_class = 'graph-with-title'
        else:
            css_class = 'graph-without-title'
        if len(title)+len(caption) > 80:
            title_align = 'left'
            table_width = ' width="600"'
        else:
            title_align = 'center'
            table_width = ''
            
        if center: s = '<center>'
        if title or caption:
            s += ('<table border="0" cellpadding="0" cellspacing="0" '
                  'class="graph"%s>\n  <tr><td align="center">\n' %
                  table_width)
        s += ('  %s\n  <img src="%s" alt=%r usemap="#%s" '
              'ismap="ismap" class="%s" />\n' %
              (cmapx.strip(), image_url, title, self.uid, css_class))
        if title or caption:
            s += '  </td></tr>\n  <tr><td align=%r>\n' % title_align
            if title:
                s += '<span class="graph-title">%s</span>' % title
            if title and caption:
                s += ' -- '
            if caption:
                s += '<span class="graph-caption">%s</span>' % caption
            s += '\n  </td></tr>\n</table><br />'
        if center: s += '</center>'
        return s

    def link(self, docstring_linker):
        """
        Replace any href attributes whose value is ``<name>`` with 
        the url of the object whose name is ``<name>``.
        """
        # Link xrefs in nodes
        self._link_href(self.node_defaults, docstring_linker)
        for node in self.nodes:
            self._link_href(node.attribs, docstring_linker)

        # Link xrefs in edges
        self._link_href(self.edge_defaults, docstring_linker)
        for edge in self.nodes:
            self._link_href(edge.attribs, docstring_linker)

        # Link xrefs in body
        def subfunc(m):
            url = docstring_linker.url_for(m.group(1))
            if url: return 'href="%s"%s' % (url, m.group(2))
            else: return ''
        self.body = re.sub("href\s*=\s*['\"]?<([\w\.]+)>['\"]?\s*(,?)",
                           subfunc, self.body)

    def _link_href(self, attribs, docstring_linker):
        """Helper for `link()`"""
        if 'href' in attribs:
            m = re.match(r'^<([\w\.]+)>$', attribs['href'])
            if m:
                url = docstring_linker.url_for(m.group(1))
                if url: attribs['href'] = url
                else: del attribs['href']
                
    def write(self, filename, language='gif'):
        """
        Render the graph using the output format `language`, and write
        the result to `filename`.
        
        :return: True if rendering was successful.
        """
        result = self._run_dot('-T%s' % language,
                               '-o%s' % filename)
        # Decode into unicode, if necessary.
        if language == 'cmapx' and result is not None:
            result = result.decode('utf-8')
        return (result is not None)

    def render(self, language='gif'):
        """
        Use the ``dot`` command to render this graph, using the output
        format `language`.  Return the result as a string, or ``None``
        if the rendering failed.
        """
        return self._run_dot('-T%s' % language)

    def _run_dot(self, *options):
        try:
            result, err = run_subprocess((DOT_COMMAND,)+options,
                                         self.to_dotfile())
            if err: log.warning("Graphviz dot warning(s):\n%s" % err)
        except OSError, e:
            log.warning("Unable to render Graphviz dot graph:\n%s" % e)
            #log.debug(self.to_dotfile())
            return None

        return result

    def to_dotfile(self):
        """
        Return the string contents of the dot file that should be used
        to render this graph.
        """
        lines = ['digraph %s {' % self.uid,
                 'node [%s]' % ','.join(['%s="%s"' % (k,v) for (k,v)
                                         in self.node_defaults.items()]),
                 'edge [%s]' % ','.join(['%s="%s"' % (k,v) for (k,v)
                                         in self.edge_defaults.items()])]
        if self.body:
            lines.append(self.body)
        lines.append('/* Nodes */')
        for node in self.nodes:
            lines.append(node.to_dotfile())
        lines.append('/* Edges */')
        for edge in self.edges:
            lines.append(edge.to_dotfile())
        lines.append('}')

        # Default dot input encoding is UTF-8
        return u'\n'.join(lines).encode('utf-8')

class DotGraphNode:
    _next_id = 0
    def __init__(self, label=None, html_label=None, **attribs):
        if label is not None and html_label is not None:
            raise ValueError('Use label or html_label, not both.')
        if label is not None: attribs['label'] = label
        self._html_label = html_label
        self._attribs = attribs
        self.id = self.__class__._next_id
        self.__class__._next_id += 1
        self.port = None

    def __getitem__(self, attr):
        return self._attribs[attr]

    def __setitem__(self, attr, val):
        if attr == 'html_label':
            self._attribs.pop('label')
            self._html_label = val
        else:
            if attr == 'label': self._html_label = None
            self._attribs[attr] = val

    def to_dotfile(self):
        """
        Return the dot commands that should be used to render this node.
        """
        attribs = ['%s="%s"' % (k,v) for (k,v) in self._attribs.items()
                   if v is not None]
        if self._html_label:
            attribs.insert(0, 'label=<%s>' % (self._html_label,))
        if attribs: attribs = ' [%s]' % (','.join(attribs))
        return 'node%d%s' % (self.id, attribs)

class DotGraphEdge:
    def __init__(self, start, end, label=None, **attribs):
        """
        :type start: `DotGraphNode`
        :type end: `DotGraphNode`
        """
        assert isinstance(start, DotGraphNode)
        assert isinstance(end, DotGraphNode)
        if label is not None: attribs['label'] = label
        self.start = start       #: :type: `DotGraphNode`
        self.end = end           #: :type: `DotGraphNode`
        self._attribs = attribs

    def __getitem__(self, attr):
        return self._attribs[attr]

    def __setitem__(self, attr, val):
        self._attribs[attr] = val

    def to_dotfile(self):
        """
        Return the dot commands that should be used to render this edge.
        """
        # Set head & tail ports, if the nodes have preferred ports.
        attribs = self._attribs.copy()
        if (self.start.port is not None and 'headport' not in attribs):
            attribs['headport'] = self.start.port
        if (self.end.port is not None and 'tailport' not in attribs):
            attribs['tailport'] = self.end.port
        # Convert attribs to a string
        attribs = ','.join(['%s="%s"' % (k,v) for (k,v) in attribs.items()
                            if v is not None])
        if attribs: attribs = ' [%s]' % attribs
        # Return the dotfile edge.
        return 'node%d -> node%d%s' % (self.start.id, self.end.id, attribs)

######################################################################
#{ Specialized Nodes for UML Graphs
######################################################################

class DotGraphUmlClassNode(DotGraphNode):
    """
    A specialized dot graph node used to display `ClassDoc`\s using
    UML notation.  The node is rendered as a table with three cells:
    the top cell contains the class name; the middle cell contains a
    list of attributes; and the bottom cell contains a list of
    operations::

         +-------------+
         |  ClassName  |
         +-------------+
         | x: int      |
         |     ...     |
         +-------------+
         | f(self, x)  |
         |     ...     |
         +-------------+

    `DotGraphUmlClassNode`\s may be *collapsed*, in which case they are
    drawn as a simple box containing the class name::
    
         +-------------+
         |  ClassName  |
         +-------------+
         
    Attributes with types corresponding to documented classes can
    optionally be converted into edges, using `link_attributes()`.

    :todo: Add more options?
      - show/hide operation signature
      - show/hide operation signature types
      - show/hide operation signature return type
      - show/hide attribute types
      - use qualifiers
    """

    def __init__(self, class_doc, linker, context, collapsed=False,
                 bgcolor=CLASS_BG, **options):
        """
        Create a new `DotGraphUmlClassNode` based on the class
        `class_doc`.

        :Parameters:
            `linker` : `markup.DocstringLinker`
                Used to look up URLs for classes.
            `context` : `APIDoc`
                The context in which this node will be drawn; dotted
                names will be contextualized to this context.
            `collapsed` : ``bool``
                If true, then display this node as a simple box.
            `bgcolor` : ```str```
                The background color for this node.
            `options` : ``dict``
                A set of options used to control how the node should
                be displayed.

        :Keywords:
          - `show_private_vars`: If false, then private variables
            are filtered out of the attributes & operations lists.
            (Default: *False*)
          - `show_magic_vars`: If false, then magic variables
            (such as ``__init__`` and ``__add__``) are filtered out of
            the attributes & operations lists. (Default: *True*)
          - `show_inherited_vars`: If false, then inherited variables
            are filtered out of the attributes & operations lists.
            (Default: *False*)
          - `max_attributes`: The maximum number of attributes that
            should be listed in the attribute box.  If the class has
            more than this number of attributes, some will be
            ellided.  Ellipsis is marked with ``'...'``.
          - `max_operations`: The maximum number of operations that
            should be listed in the operation box.
          - `add_nodes_for_linked_attributes`: If true, then
            `link_attributes()` will create new a collapsed node for
            the types of a linked attributes if no node yet exists for
            that type.
        """
        if not isinstance(class_doc, ClassDoc):
            raise TypeError('Expected a ClassDoc as 1st argument')
        
        self.class_doc = class_doc
        """The class represented by this node."""
        
        self.linker = linker
        """Used to look up URLs for classes."""
        
        self.context = context
        """The context in which the node will be drawn."""
        
        self.bgcolor = bgcolor
        """The background color of the node."""
        
        self.options = options
        """Options used to control how the node is displayed."""

        self.collapsed = collapsed
        """If true, then draw this node as a simple box."""
        
        self.attributes = []
        """The list of VariableDocs for attributes"""
        
        self.operations = []
        """The list of VariableDocs for operations"""
        
        self.qualifiers = []
        """List of (key_label, port) tuples."""

        self.edges = []
        """List of edges used to represent this node's attributes.
        These should not be added to the `DotGraph`; this node will
        generate their dotfile code directly."""

        # Initialize operations & attributes lists.
        show_private = options.get('show_private_vars', False)
        show_magic = options.get('show_magic_vars', True)
        show_inherited = options.get('show_inherited_vars', False)
        for var in class_doc.sorted_variables:
            name = var.canonical_name[-1]
            if ((not show_private and var.is_public == False) or
                (not show_magic and re.match('__\w+__$', name)) or
                (not show_inherited and var.container != class_doc)):
                pass
            elif isinstance(var.value, RoutineDoc):
                self.operations.append(var)
            else:
                self.attributes.append(var)

        # Initialize our dot node settings.
        tooltip = self._summary(class_doc)
        if tooltip:
            # dot chokes on a \n in the attribute...
            tooltip = " ".join(tooltip.split())
        else:
            tooltip = class_doc.canonical_name
        DotGraphNode.__init__(self, tooltip=tooltip,
                              width=0, height=0, shape='plaintext',
                              href=linker.url_for(class_doc) or NOOP_URL)

    #/////////////////////////////////////////////////////////////////
    #{ Attribute Linking
    #/////////////////////////////////////////////////////////////////
    
    SIMPLE_TYPE_RE = re.compile(
        r'^([\w\.]+)$')
    """A regular expression that matches descriptions of simple types."""
    
    COLLECTION_TYPE_RE = re.compile(
        r'^(list|set|sequence|tuple|collection) of ([\w\.]+)$')
    """A regular expression that matches descriptions of collection types."""

    MAPPING_TYPE_RE = re.compile(
        r'^(dict|dictionary|map|mapping) from ([\w\.]+) to ([\w\.]+)$')
    """A regular expression that matches descriptions of mapping types."""

    MAPPING_TO_COLLECTION_TYPE_RE = re.compile(
        r'^(dict|dictionary|map|mapping) from ([\w\.]+) to '
        r'(list|set|sequence|tuple|collection) of ([\w\.]+)$')
    """A regular expression that matches descriptions of mapping types
    whose value type is a collection."""

    OPTIONAL_TYPE_RE = re.compile(
        r'^(None or|optional) ([\w\.]+)$|^([\w\.]+) or None$')
    """A regular expression that matches descriptions of optional types."""
    
    def link_attributes(self, nodes):
        """
        Convert any attributes with type descriptions corresponding to
        documented classes to edges.  The following type descriptions
        are currently handled:

          - Dotted names: Create an attribute edge to the named type,
            labelled with the variable name.
          - Collections: Create an attribute edge to the named type,
            labelled with the variable name, and marked with '*' at the
            type end of the edge.
          - Mappings: Create an attribute edge to the named type,
            labelled with the variable name, connected to the class by
            a qualifier box that contains the key type description.
          - Optional: Create an attribute edge to the named type,
            labelled with the variable name, and marked with '0..1' at
            the type end of the edge.

        The edges created by `link_attributes()` are handled internally
        by `DotGraphUmlClassNode`; they should *not* be added directly
        to the `DotGraph`.

        :param nodes: A dictionary mapping from `ClassDoc`\s to
            `DotGraphUmlClassNode`\s, used to look up the nodes for
            attribute types.  If the ``add_nodes_for_linked_attributes``
            option is used, then new nodes will be added to this
            dictionary for any types that are not already listed.
            These added nodes must be added to the `DotGraph`.
        """
        # Try to convert each attribute var into a graph edge.  If
        # _link_attribute returns true, then it succeeded, so remove
        # that var from our attribute list; otherwise, leave that var
        # in our attribute list.
        self.attributes = [var for var in self.attributes
                           if not self._link_attribute(var, nodes)]

    def _link_attribute(self, var, nodes):
        """
        Helper for `link_attributes()`: try to convert the attribute
        variable `var` into an edge, and add that edge to
        `self.edges`.  Return ``True`` iff the variable was
        successfully converted to an edge (in which case, it should be
        removed from the attributes list).
        """
        type_descr = self._type_descr(var) or self._type_descr(var.value)
        
        # Simple type.
        m = self.SIMPLE_TYPE_RE.match(type_descr)
        if m and self._add_attribute_edge(var, nodes, m.group(1)):
            return True

        # Collection type.
        m = self.COLLECTION_TYPE_RE.match(type_descr)
        if m and self._add_attribute_edge(var, nodes, m.group(2),
                                          headlabel='*'):
            return True

        # Optional type.
        m = self.OPTIONAL_TYPE_RE.match(type_descr)
        if m and self._add_attribute_edge(var, nodes, m.group(2) or m.group(3),
                                          headlabel='0..1'):
            return True
                
        # Mapping type.
        m = self.MAPPING_TYPE_RE.match(type_descr)
        if m:
            port = 'qualifier_%s' % var.name
            if self._add_attribute_edge(var, nodes, m.group(3),
                                        tailport='%s:e' % port):
                self.qualifiers.append( (m.group(2), port) )
                return True

        # Mapping to collection type.
        m = self.MAPPING_TO_COLLECTION_TYPE_RE.match(type_descr)
        if m:
            port = 'qualifier_%s' % var.name
            if self._add_attribute_edge(var, nodes, m.group(4), headlabel='*', 
                                        tailport='%s:e' % port):
                self.qualifiers.append( (m.group(2), port) )
                return True

        # We were unable to link this attribute.
        return False

    def _add_attribute_edge(self, var, nodes, type_str, **attribs):
        """
        Helper for `link_attributes()`: try to add an edge for the
        given attribute variable `var`.  Return ``True`` if
        successful.
        """
        # Use the type string to look up a corresponding ValueDoc.
        type_doc = self.linker.docindex.find(type_str, var)
        if not type_doc: return False

        # Make sure the type is a class.
        if not isinstance(type_doc, ClassDoc): return False

        # Get the type ValueDoc's node.  If it doesn't have one (and
        # add_nodes_for_linked_attributes=True), then create it.
        type_node = nodes.get(type_doc)
        if not type_node:
            if self.options.get('add_nodes_for_linked_attributes', True):
                type_node = DotGraphUmlClassNode(type_doc, self.linker,
                                                 self.context, collapsed=True)
                nodes[type_doc] = type_node
            else:
                return False

        # Add an edge from self to the target type node.
        # [xx] should I set constraint=false here?
        attribs.setdefault('headport', 'body')
        attribs.setdefault('tailport', 'body')
        url = self.linker.url_for(var) or NOOP_URL
        self.edges.append(DotGraphEdge(self, type_node, label=var.name,
                        arrowhead='open', href=url,
                        tooltip=var.canonical_name, labeldistance=1.5,
                        **attribs))
        return True
                           
    #/////////////////////////////////////////////////////////////////
    #{ Helper Methods
    #/////////////////////////////////////////////////////////////////
    def _summary(self, api_doc):
        """Return a plaintext summary for `api_doc`"""
        if not isinstance(api_doc, APIDoc): return ''
        if api_doc.summary in (None, UNKNOWN): return ''
        summary = api_doc.summary.to_plaintext(None).strip()
        return plaintext_to_html(summary)

    _summary = classmethod(_summary)

    def _type_descr(self, api_doc):
        """Return a plaintext type description for `api_doc`"""
        if not hasattr(api_doc, 'type_descr'): return ''
        if api_doc.type_descr in (None, UNKNOWN): return ''
        type_descr = api_doc.type_descr.to_plaintext(self.linker).strip()
        return plaintext_to_html(type_descr)

    def _tooltip(self, var_doc):
        """Return a tooltip for `var_doc`."""
        return (self._summary(var_doc) or
                self._summary(var_doc.value) or
                var_doc.canonical_name)
    
    #/////////////////////////////////////////////////////////////////
    #{ Rendering
    #/////////////////////////////////////////////////////////////////
    
    def _attribute_cell(self, var_doc):
        # Construct the label
        label = var_doc.name
        type_descr = (self._type_descr(var_doc) or
                      self._type_descr(var_doc.value))
        if type_descr: label += ': %s' % type_descr
        # Get the URL
        url = self.linker.url_for(var_doc) or NOOP_URL
        # Construct & return the pseudo-html code
        return self._ATTRIBUTE_CELL % (url, self._tooltip(var_doc), label)

    def _operation_cell(self, var_doc):
        """
        :todo: do 'word wrapping' on the signature, by starting a new
               row in the table, if necessary.  How to indent the new
               line?  Maybe use align=right?  I don't think dot has a
               &nbsp;.
        :todo: Optionally add return type info?
        """
        # Construct the label (aka function signature)
        func_doc = var_doc.value
        args = [self._operation_arg(n, d, func_doc) for (n, d)
                in zip(func_doc.posargs, func_doc.posarg_defaults)]
        args = [plaintext_to_html(arg) for arg in args]
        if func_doc.vararg: args.append('*'+func_doc.vararg)
        if func_doc.kwarg: args.append('**'+func_doc.kwarg)
        label = '%s(%s)' % (var_doc.name, ', '.join(args))
        # Get the URL
        url = self.linker.url_for(var_doc) or NOOP_URL
        # Construct & return the pseudo-html code
        return self._OPERATION_CELL % (url, self._tooltip(var_doc), label)

    def _operation_arg(self, name, default, func_doc):
        """
        :todo: Handle tuple args better
        :todo: Optionally add type info?
        """
        if default is None:
            return '%s' % name
        else:
            pyval_repr = default.summary_pyval_repr().to_plaintext(None)
            return '%s=%s' % (name, pyval_repr)

    def _qualifier_cell(self, key_label, port):
        return self._QUALIFIER_CELL  % (port, self.bgcolor, key_label)

    #: args: (url, tooltip, label)
    _ATTRIBUTE_CELL = '''
    <TR><TD ALIGN="LEFT" HREF="%s" TOOLTIP="%s">%s</TD></TR>
    '''

    #: args: (url, tooltip, label)
    _OPERATION_CELL = '''
    <TR><TD ALIGN="LEFT" HREF="%s" TOOLTIP="%s">%s</TD></TR>
    '''

    #: args: (port, bgcolor, label)
    _QUALIFIER_CELL = '''
    <TR><TD VALIGN="BOTTOM" PORT="%s" BGCOLOR="%s" BORDER="1">%s</TD></TR>
    '''

    _QUALIFIER_DIV = '''
    <TR><TD VALIGN="BOTTOM" HEIGHT="10" WIDTH="10" FIXEDSIZE="TRUE"></TD></TR>
    '''
    
    #: Args: (rowspan, bgcolor, classname, attributes, operations, qualifiers)
    _LABEL = '''
    <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0">
      <TR><TD ROWSPAN="%s">
        <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0"
               CELLPADDING="0" PORT="body" BGCOLOR="%s">
          <TR><TD>%s</TD></TR>
          <TR><TD><TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0">
            %s</TABLE></TD></TR>
          <TR><TD><TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0">
            %s</TABLE></TD></TR>
        </TABLE>
      </TD></TR>
      %s
    </TABLE>'''

    _COLLAPSED_LABEL = '''
    <TABLE CELLBORDER="0" BGCOLOR="%s" PORT="body">
      <TR><TD>%s</TD></TR>
    </TABLE>'''

    def _get_html_label(self):
        # Get the class name & contextualize it.
        classname = self.class_doc.canonical_name
        classname = classname.contextualize(self.context.canonical_name)
        
        # If we're collapsed, display the node as a single box.
        if self.collapsed:
            return self._COLLAPSED_LABEL % (self.bgcolor, classname)
        
        # Construct the attribute list.  (If it's too long, truncate)
        attrib_cells = [self._attribute_cell(a) for a in self.attributes]
        max_attributes = self.options.get('max_attributes', 15)
        if len(attrib_cells) == 0:
            attrib_cells = ['<TR><TD></TD></TR>']
        elif len(attrib_cells) > max_attributes:
            attrib_cells[max_attributes-2:-1] = ['<TR><TD>...</TD></TR>']
        attributes = ''.join(attrib_cells)
                      
        # Construct the operation list.  (If it's too long, truncate)
        oper_cells = [self._operation_cell(a) for a in self.operations]
        max_operations = self.options.get('max_operations', 15)
        if len(oper_cells) == 0:
            oper_cells = ['<TR><TD></TD></TR>']
        elif len(oper_cells) > max_operations:
            oper_cells[max_operations-2:-1] = ['<TR><TD>...</TD></TR>']
        operations = ''.join(oper_cells)

        # Construct the qualifier list & determine the rowspan.
        if self.qualifiers:
            rowspan = len(self.qualifiers)*2+2
            div = self._QUALIFIER_DIV
            qualifiers = div+div.join([self._qualifier_cell(l,p) for
                                     (l,p) in self.qualifiers])+div
        else:
            rowspan = 1
            qualifiers = ''

        # Put it all together.
        return self._LABEL % (rowspan, self.bgcolor, classname,
                              attributes, operations, qualifiers)

    def to_dotfile(self):
        attribs = ['%s="%s"' % (k,v) for (k,v) in self._attribs.items()]
        attribs.append('label=<%s>' % self._get_html_label())
        s = 'node%d%s' % (self.id, ' [%s]' % (','.join(attribs)))
        if not self.collapsed:
            for edge in self.edges:
                s += '\n' + edge.to_dotfile()
        return s

class DotGraphUmlModuleNode(DotGraphNode):
    """
    A specialized dot grah node used to display `ModuleDoc`\s using
    UML notation.  Simple module nodes look like::

        .----.
        +------------+
        | modulename |
        +------------+

    Packages nodes are drawn with their modules & subpackages nested
    inside::
        
        .----.
        +----------------------------------------+
        | packagename                            |
        |                                        |
        |  .----.       .----.       .----.      |
        |  +---------+  +---------+  +---------+ |
        |  | module1 |  | module2 |  | module3 | |
        |  +---------+  +---------+  +---------+ |
        |                                        |
        +----------------------------------------+

    """
    def __init__(self, module_doc, linker, context, collapsed=False,
                 excluded_submodules=(), **options):
        self.module_doc = module_doc
        self.linker = linker
        self.context = context
        self.collapsed = collapsed
        self.options = options
        self.excluded_submodules = excluded_submodules
        DotGraphNode.__init__(self, shape='plaintext',
                              href=linker.url_for(module_doc) or NOOP_URL,
                              tooltip=module_doc.canonical_name)

    #: Expects: (color, color, url, tooltip, body)
    _MODULE_LABEL = ''' 
    <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" ALIGN="LEFT">
    <TR><TD ALIGN="LEFT" VALIGN="BOTTOM" HEIGHT="8" WIDTH="16"
            FIXEDSIZE="true" BGCOLOR="%s" BORDER="1" PORT="tab"></TD></TR>
    <TR><TD ALIGN="LEFT" VALIGN="TOP" BGCOLOR="%s" BORDER="1" WIDTH="20"
            PORT="body" HREF="%s" TOOLTIP="%s">%s</TD></TR>
    </TABLE>'''

    #: Expects: (name, body_rows)
    _NESTED_BODY = '''
    <TABLE BORDER="0" CELLBORDER="0" CELLPADDING="0" CELLSPACING="0">
    <TR><TD ALIGN="LEFT">%s</TD></TR>
    %s
    </TABLE>'''

    #: Expects: (cells,)
    _NESTED_BODY_ROW = '''
    <TR><TD>
      <TABLE BORDER="0" CELLBORDER="0"><TR>%s</TR></TABLE>
    </TD></TR>'''
    
    def _get_html_label(self, package):
        """
        :Return: (label, depth, width) where:
        
          - ``label`` is the HTML label
          - ``depth`` is the depth of the package tree (for coloring)
          - ``width`` is the max width of the HTML label, roughly in
            units of characters.
        """
        MAX_ROW_WIDTH = 80 # unit is roughly characters.
        pkg_name = package.canonical_name
        pkg_url = self.linker.url_for(package) or NOOP_URL
        
        if (not package.is_package or len(package.submodules) == 0 or
            self.collapsed):
            pkg_color = self._color(package, 1)
            label = self._MODULE_LABEL % (pkg_color, pkg_color,
                                          pkg_url, pkg_name, pkg_name[-1])
            return (label, 1, len(pkg_name[-1])+3)
                
        # Get the label for each submodule, and divide them into rows.
        row_list = ['']
        row_width = 0
        max_depth = 0
        max_row_width = len(pkg_name[-1])+3
        for submodule in package.submodules:
            if submodule in self.excluded_submodules: continue
            # Get the submodule's label.
            label, depth, width = self._get_html_label(submodule)
            # Check if we should start a new row.
            if row_width > 0 and width+row_width > MAX_ROW_WIDTH:
                row_list.append('')
                row_width = 0
            # Add the submodule's label to the row.
            row_width += width
            row_list[-1] += '<TD ALIGN="LEFT">%s</TD>' % label
            # Update our max's.
            max_depth = max(depth, max_depth)
            max_row_width = max(row_width, max_row_width)

        # Figure out which color to use.
        pkg_color = self._color(package, depth+1)
        
        # Assemble & return the label.
        rows = ''.join([self._NESTED_BODY_ROW % r for r in row_list])
        body = self._NESTED_BODY % (pkg_name, rows)
        label = self._MODULE_LABEL % (pkg_color, pkg_color,
                                      pkg_url, pkg_name, body)
        return label, max_depth+1, max_row_width

    _COLOR_DIFF = 24
    def _color(self, package, depth):
        if package == self.context: return SELECTED_BG
        else: 
            # Parse the base color.
            if re.match(MODULE_BG, 'r#[0-9a-fA-F]{6}$'):
                base = int(MODULE_BG[1:], 16)
            else:
                base = int('d8e8ff', 16)
            red = (base & 0xff0000) >> 16
            green = (base & 0x00ff00) >> 8
            blue = (base & 0x0000ff)
            # Make it darker with each level of depth. (but not *too*
            # dark -- package name needs to be readable)
            red = max(64, red-(depth-1)*self._COLOR_DIFF)
            green = max(64, green-(depth-1)*self._COLOR_DIFF)
            blue = max(64, blue-(depth-1)*self._COLOR_DIFF)
            # Convert it back to a color string
            return '#%06x' % ((red<<16)+(green<<8)+blue)
        
    def to_dotfile(self):
        attribs = ['%s="%s"' % (k,v) for (k,v) in self._attribs.items()]
        label, depth, width = self._get_html_label(self.module_doc)
        attribs.append('label=<%s>' % label)
        return 'node%d%s' % (self.id, ' [%s]' % (','.join(attribs)))


    
######################################################################
#{ Graph Generation Functions
######################################################################

def package_tree_graph(packages, linker, context=None, **options):
    """
    Return a `DotGraph` that graphically displays the package
    hierarchies for the given packages.
    """
    if options.get('style', 'uml') == 'uml': # default to uml style?
        if get_dot_version() >= [2]:
            return uml_package_tree_graph(packages, linker, context,
                                             **options)
        elif 'style' in options:
            log.warning('UML style package trees require dot version 2.0+')

    graph = DotGraph('Package Tree for %s' % name_list(packages, context),
                     body='ranksep=.3\n;nodesep=.1\n',
                     edge_defaults={'dir':'none'})
    
    # Options
    if options.get('dir', 'TB') != 'TB': # default: top-to-bottom
        graph.body += 'rankdir=%s\n' % options.get('dir', 'TB')

    # Get a list of all modules in the package.
    queue = list(packages)
    modules = set(packages)
    for module in queue:
        queue.extend(module.submodules)
        modules.update(module.submodules)

    # Add a node for each module.
    nodes = add_valdoc_nodes(graph, modules, linker, context)

    # Add an edge for each package/submodule relationship.
    for module in modules:
        for submodule in module.submodules:
            graph.edges.append(DotGraphEdge(nodes[module], nodes[submodule],
                                            headport='tab'))

    return graph

def uml_package_tree_graph(packages, linker, context=None, **options):
    """
    Return a `DotGraph` that graphically displays the package
    hierarchies for the given packages as a nested set of UML
    symbols.
    """
    graph = DotGraph('Package Tree for %s' % name_list(packages, context))
    # Remove any packages whose containers are also in the list.
    root_packages = []
    for package1 in packages:
        for package2 in packages:
            if (package1 is not package2 and
                package2.canonical_name.dominates(package1.canonical_name)):
                break
        else:
            root_packages.append(package1)
    # If the context is a variable, then get its value.
    if isinstance(context, VariableDoc) and context.value is not UNKNOWN:
        context = context.value
    # Return a graph with one node for each root package.
    for package in root_packages:
        graph.nodes.append(DotGraphUmlModuleNode(package, linker, context))
    return graph

######################################################################
def class_tree_graph(bases, linker, context=None, **options):
    """
    Return a `DotGraph` that graphically displays the class
    hierarchy for the given classes.  Options:

      - exclude
      - dir: LR|RL|BT requests a left-to-right, right-to-left, or
        bottom-to- top, drawing.  (corresponds to the dot option
        'rankdir'
    """
    if isinstance(bases, ClassDoc): bases = [bases]
    graph = DotGraph('Class Hierarchy for %s' % name_list(bases, context),
                     body='ranksep=0.3\n',
                     edge_defaults={'sametail':True, 'dir':'none'})

    # Options
    if options.get('dir', 'TB') != 'TB': # default: top-down
        graph.body += 'rankdir=%s\n' % options.get('dir', 'TB')
    exclude = options.get('exclude', ())

    # Find all superclasses & subclasses of the given classes.
    classes = set(bases)
    queue = list(bases)
    for cls in queue:
        if isinstance(cls, ClassDoc):
            if cls.subclasses not in (None, UNKNOWN):
                subclasses = cls.subclasses
                if exclude:
                    subclasses = [d for d in subclasses if d not in exclude]
                queue.extend(subclasses)
                classes.update(subclasses)
    queue = list(bases)
    for cls in queue:
        if isinstance(cls, ClassDoc):
            if cls.bases not in (None, UNKNOWN):
                bases = cls.bases
                if exclude:
                    bases = [d for d in bases if d not in exclude]
                queue.extend(bases)
                classes.update(bases)

    # Add a node for each cls.
    classes = [d for d in classes if isinstance(d, ClassDoc)
               if d.pyval is not object]
    nodes = add_valdoc_nodes(graph, classes, linker, context)

    # Add an edge for each package/subclass relationship.
    edges = set()
    for cls in classes:
        for subcls in cls.subclasses:
            if cls in nodes and subcls in nodes:
                edges.add((nodes[cls], nodes[subcls]))
    graph.edges = [DotGraphEdge(src,dst) for (src,dst) in edges]

    return graph

######################################################################
def uml_class_tree_graph(class_doc, linker, context=None, **options):
    """
    Return a `DotGraph` that graphically displays the class hierarchy
    for the given class, using UML notation.  Options:

      - max_attributes
      - max_operations
      - show_private_vars
      - show_magic_vars
      - link_attributes
    """
    nodes = {} # ClassDoc -> DotGraphUmlClassNode
    exclude = options.get('exclude', ())
        
    # Create nodes for class_doc and all its bases.
    for cls in class_doc.mro():
        if cls.pyval is object: continue # don't include `object`.
        if cls in exclude: break # stop if we get to an excluded class.
        if cls == class_doc: color = SELECTED_BG
        else: color = BASECLASS_BG
        nodes[cls] = DotGraphUmlClassNode(cls, linker, context,
                                          show_inherited_vars=False,
                                          collapsed=False, bgcolor=color)

    # Create nodes for all class_doc's subclasses.
    queue = [class_doc]
    for cls in queue:
        if (isinstance(cls, ClassDoc) and
            cls.subclasses not in (None, UNKNOWN)):
            for subcls in cls.subclasses:
                subcls_name = subcls.canonical_name[-1]
                if subcls not in nodes and subcls not in exclude:
                    queue.append(subcls)
                    nodes[subcls] = DotGraphUmlClassNode(
                        subcls, linker, context, collapsed=True,
                        bgcolor=SUBCLASS_BG)
                    
    # Only show variables in the class where they're defined for
    # *class_doc*.
    mro = class_doc.mro()
    for name, var in class_doc.variables.items():
        i = mro.index(var.container)
        for base in mro[i+1:]:
            if base.pyval is object: continue # don't include `object`.
            overridden_var = base.variables.get(name)
            if overridden_var and overridden_var.container == base:
                try:
                    if isinstance(overridden_var.value, RoutineDoc):
                        nodes[base].operations.remove(overridden_var)
                    else:
                        nodes[base].attributes.remove(overridden_var)
                except ValueError:
                    pass # var is filtered (eg private or magic)

    # Keep track of which nodes are part of the inheritance graph
    # (since link_attributes might add new nodes)
    inheritance_nodes = set(nodes.values())
        
    # Turn attributes into links.
    if options.get('link_attributes', True):
        for node in nodes.values():
            node.link_attributes(nodes)
            # Make sure that none of the new attribute edges break the
            # rank ordering assigned by inheritance.
            for edge in node.edges:
                if edge.end in inheritance_nodes:
                    edge['constraint'] = 'False'
                
    # Construct the graph.
    graph = DotGraph('UML class diagram for %s' % class_doc.canonical_name,
                     body='ranksep=.2\n;nodesep=.3\n')
    graph.nodes = nodes.values()
    
    # Add inheritance edges.
    for node in inheritance_nodes:
        for base in node.class_doc.bases:
            if base in nodes:
                graph.edges.append(DotGraphEdge(nodes[base], node,
                              dir='back', arrowtail='empty',
                              headport='body', tailport='body',
                              color=INH_LINK_COLOR, weight=100,
                              style='bold'))

    # And we're done!
    return graph

######################################################################
def import_graph(modules, docindex, linker, context=None, **options):
    graph = DotGraph('Import Graph', body='ranksep=.3\n;nodesep=.3\n')

    # Options
    if options.get('dir', 'RL') != 'TB': # default: right-to-left.
        graph.body += 'rankdir=%s\n' % options.get('dir', 'RL')

    # Add a node for each module.
    nodes = add_valdoc_nodes(graph, modules, linker, context)

    # Edges.
    edges = set()
    for dst in modules:
        if dst.imports in (None, UNKNOWN): continue
        for var_name in dst.imports:
            for i in range(len(var_name), 0, -1):
                val_doc = docindex.find(var_name[:i], context)
                if isinstance(val_doc, ModuleDoc):
                    if val_doc in nodes and dst in nodes:
                        edges.add((nodes[val_doc], nodes[dst]))
                    break
    graph.edges = [DotGraphEdge(src,dst) for (src,dst) in edges]

    return graph

######################################################################
def call_graph(api_docs, docindex, linker, context=None, **options):
    """
    :param options:
        - ``dir``: rankdir for the graph.  (default=LR)
        - ``add_callers``: also include callers for any of the
          routines in ``api_docs``.  (default=False)
        - ``add_callees``: also include callees for any of the
          routines in ``api_docs``.  (default=False)
    :todo: Add an ``exclude`` option?
    """
    if docindex.callers is None:
        log.warning("No profiling information for call graph!")
        return DotGraph('Call Graph') # return None instead?

    if isinstance(context, VariableDoc):
        context = context.value

    # Get the set of requested functions.
    functions = []
    for api_doc in api_docs:
        # If it's a variable, get its value.
        if isinstance(api_doc, VariableDoc):
            api_doc = api_doc.value
        # Add the value to the functions list.
        if isinstance(api_doc, RoutineDoc):
            functions.append(api_doc)
        elif isinstance(api_doc, NamespaceDoc):
            for vardoc in api_doc.variables.values():
                if isinstance(vardoc.value, RoutineDoc):
                    functions.append(vardoc.value)

    # Filter out functions with no callers/callees?
    # [xx] this isnt' quite right, esp if add_callers or add_callees
    # options are fales.
    functions = [f for f in functions if
                 (f in docindex.callers) or (f in docindex.callees)]
        
    # Add any callers/callees of the selected functions
    func_set = set(functions)
    if options.get('add_callers', False) or options.get('add_callees', False):
        for func_doc in functions:
            if options.get('add_callers', False):
                func_set.update(docindex.callers.get(func_doc, ()))
            if options.get('add_callees', False):
                func_set.update(docindex.callees.get(func_doc, ()))

    graph = DotGraph('Call Graph for %s' % name_list(api_docs, context),
                     node_defaults={'shape':'box', 'width': 0, 'height': 0})
    
    # Options
    if options.get('dir', 'LR') != 'TB': # default: left-to-right
        graph.body += 'rankdir=%s\n' % options.get('dir', 'LR')

    nodes = add_valdoc_nodes(graph, func_set, linker, context)
    
    # Find the edges.
    edges = set()
    for func_doc in functions:
        for caller in docindex.callers.get(func_doc, ()):
            if caller in nodes:
                edges.add( (nodes[caller], nodes[func_doc]) )
        for callee in docindex.callees.get(func_doc, ()):
            if callee in nodes:
                edges.add( (nodes[func_doc], nodes[callee]) )
    graph.edges = [DotGraphEdge(src,dst) for (src,dst) in edges]
    
    return graph

######################################################################
#{ Dot Version
######################################################################

_dot_version = None
_DOT_VERSION_RE = re.compile(r'dot version ([\d\.]+)')
def get_dot_version():
    global _dot_version
    if _dot_version is None:
        try:
            out, err = run_subprocess([DOT_COMMAND, '-V'])
            version_info = err or out
            m = _DOT_VERSION_RE.match(version_info)
            if m:
                _dot_version = [int(x) for x in m.group(1).split('.')]
            else:
                _dot_version = (0,)
        except OSError, e:
            _dot_version = (0,)
        log.info('Detected dot version %s' % _dot_version)
    return _dot_version

######################################################################
#{ Helper Functions
######################################################################

def add_valdoc_nodes(graph, val_docs, linker, context):
    """
    :todo: Use different node styles for different subclasses of APIDoc
    """
    nodes = {}
    val_docs = sorted(val_docs, key=lambda d:d.canonical_name)
    for i, val_doc in enumerate(val_docs):
        label = val_doc.canonical_name.contextualize(context.canonical_name)
        node = nodes[val_doc] = DotGraphNode(label)
        graph.nodes.append(node)
        specialize_valdoc_node(node, val_doc, context, linker.url_for(val_doc))
    return nodes

NOOP_URL = 'javascript:void(0);'
MODULE_NODE_HTML = '''
  <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0"
         CELLPADDING="0" PORT="table" ALIGN="LEFT">
  <TR><TD ALIGN="LEFT" VALIGN="BOTTOM" HEIGHT="8" WIDTH="16" FIXEDSIZE="true"
          BGCOLOR="%s" BORDER="1" PORT="tab"></TD></TR>
  <TR><TD ALIGN="LEFT" VALIGN="TOP" BGCOLOR="%s" BORDER="1"
          PORT="body" HREF="%s" TOOLTIP="%s">%s</TD></TR>
  </TABLE>'''.strip()

def specialize_valdoc_node(node, val_doc, context, url):
    """
    Update the style attributes of `node` to reflext its type
    and context.
    """
    # We can only use html-style nodes if dot_version>2.
    dot_version = get_dot_version()
    
    # If val_doc or context is a variable, get its value.
    if isinstance(val_doc, VariableDoc) and val_doc.value is not UNKNOWN:
        val_doc = val_doc.value
    if isinstance(context, VariableDoc) and context.value is not UNKNOWN:
        context = context.value

    # Set the URL.  (Do this even if it points to the page we're
    # currently on; otherwise, the tooltip is ignored.)
    node['href'] = url or NOOP_URL

    if isinstance(val_doc, ModuleDoc) and dot_version >= [2]:
        node['shape'] = 'plaintext'
        if val_doc == context: color = SELECTED_BG
        else: color = MODULE_BG
        node['tooltip'] = node['label']
        node['html_label'] = MODULE_NODE_HTML % (color, color, url,
                                                 val_doc.canonical_name,
                                                 node['label'])
        node['width'] = node['height'] = 0
        node.port = 'body'

    elif isinstance(val_doc, RoutineDoc):
        node['shape'] = 'box'
        node['style'] = 'rounded'
        node['width'] = 0
        node['height'] = 0
        node['label'] = '%s()' % node['label']
        node['tooltip'] = node['label']
        if val_doc == context:
            node['fillcolor'] = SELECTED_BG
            node['style'] = 'filled,rounded,bold'
            
    else:
        node['shape'] = 'box' 
        node['width'] = 0
        node['height'] = 0
        node['tooltip'] = node['label']
        if val_doc == context:
            node['fillcolor'] = SELECTED_BG
            node['style'] = 'filled,bold'

def name_list(api_docs, context=None):
    if context is not None:
        context = context.canonical_name
    names = [str(d.canonical_name.contextualize(context)) for d in api_docs]
    if len(names) == 0: return ''
    if len(names) == 1: return '%s' % names[0]
    elif len(names) == 2: return '%s and %s' % (names[0], names[1])
    else:
        return '%s, and %s' % (', '.join(names[:-1]), names[-1])


########NEW FILE########
__FILENAME__ = html
#
# epydoc -- HTML output generator
# Edward Loper
#
# Created [01/30/01 05:18 PM]
# $Id: html.py 1674 2008-01-29 06:03:36Z edloper $
#

"""
The HTML output generator for epydoc.  The main interface provided by
this module is the L{HTMLWriter} class.

@todo: Add a cache to L{HTMLWriter.url()}?
"""
__docformat__ = 'epytext en'

import re, os, sys, codecs, sre_constants, pprint, base64
import urllib
import __builtin__
from epydoc.apidoc import *
import epydoc.docstringparser
import time, epydoc, epydoc.markup, epydoc.markup.epytext
from epydoc.docwriter.html_colorize import PythonSourceColorizer
from epydoc.docwriter import html_colorize
from epydoc.docwriter.html_css import STYLESHEETS
from epydoc.docwriter.html_help import HTML_HELP
from epydoc.docwriter.dotgraph import *
from epydoc import log
from epydoc.util import plaintext_to_html, is_src_filename
from epydoc.compat import * # Backwards compatibility

######################################################################
## Template Compiler
######################################################################
# The compile_template() method defined in this section is used to
# define several of HTMLWriter's methods.

def compile_template(docstring, template_string,
                     output_function='out', debug=epydoc.DEBUG):
    """
    Given a template string containing inline python source code,
    return a python function that will fill in the template, and
    output the result.  The signature for this function is taken from
    the first line of C{docstring}.  Output is generated by making
    repeated calls to the output function with the given name (which
    is typically one of the function's parameters).

    The templating language used by this function passes through all
    text as-is, with three exceptions:

      - If every line in the template string is indented by at least
        M{x} spaces, then the first M{x} spaces are stripped from each
        line.

      - Any line that begins with '>>>' (with no indentation)
        should contain python code, and will be inserted as-is into
        the template-filling function.  If the line begins a control
        block (such as 'if' or 'for'), then the control block will
        be closed by the first '>>>'-marked line whose indentation is
        less than or equal to the line's own indentation (including
        lines that only contain comments.)

      - In any other line, any expression between two '$' signs will
        be evaluated and inserted into the line (using C{str()} to
        convert the result to a string).

    Here is a simple example:

        >>> TEMPLATE = '''
        ... <book>
        ...   <title>$book.title$</title>
        ...   <pages>$book.count_pages()$</pages>
        ... >>> for chapter in book.chapters:
        ...     <chaptername>$chapter.name$</chaptername>
        ... >>> #endfor
        ... </book>
        >>> write_book = compile_template('write_book(out, book)', TEMPLATE)

    @newfield acknowledgements: Acknowledgements
    @acknowledgements: The syntax used by C{compile_template} is
    loosely based on Cheetah.
    """
    # Extract signature from the docstring:
    signature = docstring.lstrip().split('\n',1)[0].strip()
    func_name = signature.split('(',1)[0].strip()

    # Regexp to search for inline substitutions:
    INLINE = re.compile(r'\$([^\$]+)\$')
    # Regexp to search for python statements in the template:
    COMMAND = re.compile(r'(^>>>.*)\n?', re.MULTILINE)

    # Strip indentation from the template.
    template_string = strip_indent(template_string)

    # If we're debugging, then we'll store the generated function,
    # so we can print it along with any tracebacks that depend on it.
    if debug:
        signature = re.sub(r'\)\s*$', ', __debug=__debug)', signature)

    # Funciton declaration line
    pysrc_lines = ['def %s:' % signature]
    indents = [-1]

    if debug:
        pysrc_lines.append('    try:')
        indents.append(-1)

    commands = COMMAND.split(template_string.strip()+'\n')
    for i, command in enumerate(commands):
        if command == '': continue

        # String literal segment:
        if i%2 == 0:
            pieces = INLINE.split(command)
            for j, piece in enumerate(pieces):
                if j%2 == 0:
                    # String piece
                    pysrc_lines.append('    '*len(indents)+
                                 '%s(%r)' % (output_function, piece))
                else:
                    # Variable piece
                    pysrc_lines.append('    '*len(indents)+
                                 '%s(unicode(%s))' % (output_function, piece))

        # Python command:
        else:
            srcline = command[3:].lstrip()
            # Update indentation
            indent = len(command)-len(srcline)
            while indent <= indents[-1]: indents.pop()
            # Add on the line.
            srcline = srcline.rstrip()
            pysrc_lines.append('    '*len(indents)+srcline)
            if srcline.endswith(':'):
                indents.append(indent)
        
    if debug:
        pysrc_lines.append('    except Exception,e:')
        pysrc_lines.append('        pysrc, func_name = __debug ')
        pysrc_lines.append('        lineno = sys.exc_info()[2].tb_lineno')
        pysrc_lines.append('        print ("Exception in template %s() on "')
        pysrc_lines.append('               "line %d:" % (func_name, lineno))')
        pysrc_lines.append('        print pysrc[lineno-1]')
        pysrc_lines.append('        raise')
        
    pysrc = '\n'.join(pysrc_lines)+'\n'
    #log.debug(pysrc)
    if debug: localdict = {'__debug': (pysrc_lines, func_name)}
    else: localdict = {}
    try: exec pysrc in globals(), localdict
    except SyntaxError:
        log.error('Error in script:\n' + pysrc + '\n')
        raise
    template_func = localdict[func_name]
    template_func.__doc__ = docstring
    return template_func
    
def strip_indent(s):
    """
    Given a multiline string C{s}, find the minimum indentation for
    all non-blank lines, and return a new string formed by stripping
    that amount of indentation from all lines in C{s}.
    """
    # Strip indentation from the template.
    minindent = sys.maxint
    lines = s.split('\n')
    for line in lines:
        stripline = line.lstrip()
        if stripline:
            minindent = min(minindent, len(line)-len(stripline))
    return '\n'.join([l[minindent:] for l in lines])

######################################################################
## HTML Writer
######################################################################

class HTMLWriter:
    #////////////////////////////////////////////////////////////
    # Table of Contents
    #////////////////////////////////////////////////////////////
    #
    # 1. Interface Methods
    #
    # 2. Page Generation -- write complete web page files
    #   2.1. Module Pages
    #   2.2. Class Pages
    #   2.3. Trees Page
    #   2.4. Indices Page
    #   2.5. Help Page
    #   2.6. Frames-based table of contents pages
    #   2.7. Homepage (index.html)
    #   2.8. CSS Stylesheet
    #   2.9. Javascript file
    #   2.10. Graphs
    #   2.11. Images
    #
    # 3. Page Element Generation -- write pieces of a web page file
    #   3.1. Page Header
    #   3.2. Page Footer
    #   3.3. Navigation Bar
    #   3.4. Breadcrumbs
    #   3.5. Summary Tables
    #
    # 4. Helper functions

    def __init__(self, docindex, **kwargs):
        """
        Construct a new HTML writer, using the given documentation
        index.
        
        @param docindex: The documentation index.
        
        @type prj_name: C{string}
        @keyword prj_name: The name of the project.  Defaults to
              none.
        @type prj_url: C{string}
        @keyword prj_url: The target for the project hopeage link on
              the navigation bar.  If C{prj_url} is not specified,
              then no hyperlink is created.
        @type prj_link: C{string}
        @keyword prj_link: The label for the project link on the
              navigation bar.  This link can contain arbitrary HTML
              code (e.g. images).  By default, a label is constructed
              from C{prj_name}.
        @type top_page: C{string}
        @keyword top_page: The top page for the documentation.  This
              is the default page shown main frame, when frames are
              enabled.  C{top} can be a URL, the name of a
              module, the name of a class, or one of the special
              strings C{"trees.html"}, C{"indices.html"}, or
              C{"help.html"}.  By default, the top-level package or
              module is used, if there is one; otherwise, C{"trees"}
              is used.
        @type css: C{string}
        @keyword css: The CSS stylesheet file.  If C{css} is a file
              name, then the specified file's conents will be used.
              Otherwise, if C{css} is the name of a CSS stylesheet in
              L{epydoc.docwriter.html_css}, then that stylesheet will
              be used.  Otherwise, an error is reported.  If no stylesheet 
              is specified, then the default stylesheet is used.
        @type help_file: C{string}
        @keyword help_file: The name of the help file.  If no help file is
              specified, then the default help file will be used.
        @type show_private: C{boolean}
        @keyword show_private: Whether to create documentation for
            private objects.  By default, private objects are documented.
        @type show_frames: C{boolean})
        @keyword show_frames: Whether to create a frames-based table of
              contents.  By default, it is produced.
        @type show_imports: C{boolean}
        @keyword show_imports: Whether or not to display lists of
              imported functions and classes.  By default, they are
              not shown.
        @type variable_maxlines: C{int}
        @keyword variable_maxlines: The maximum number of lines that
              should be displayed for the value of a variable in the
              variable details section.  By default, 8 lines are
              displayed.
        @type variable_linelength: C{int}
        @keyword variable_linelength: The maximum line length used for
              displaying the values of variables in the variable
              details sections.  If a line is longer than this length,
              then it will be wrapped to the next line.  The default
              line length is 70 characters.
        @type variable_summary_linelength: C{int}
        @keyword variable_summary_linelength: The maximum line length
              used for displaying the values of variables in the summary
              section.  If a line is longer than this length, then it
              will be truncated.  The default is 40 characters.
        @type variable_tooltip_linelength: C{int}
        @keyword variable_tooltip_linelength: The maximum line length
              used for tooltips for the values of variables.  If a
              line is longer than this length, then it will be
              truncated.  The default is 600 characters.
        @type property_function_linelength: C{int}
        @keyword property_function_linelength: The maximum line length
              used to dispaly property functions (C{fget}, C{fset}, and
              C{fdel}) that contain something other than a function
              object.  The default length is 40 characters.
        @type inheritance: C{string}
        @keyword inheritance: How inherited objects should be displayed.
              If C{inheritance='grouped'}, then inherited objects are
              gathered into groups; if C{inheritance='listed'}, then
              inherited objects are listed in a short list at the
              end of their group; if C{inheritance='included'}, then
              inherited objects are mixed in with non-inherited
              objects.  The default is 'grouped'.
        @type include_source_code: C{boolean}
        @keyword include_source_code: If true, then generate colorized
              source code files for each python module.
        @type include_log: C{boolean}
        @keyword include_log: If true, the the footer will include an
              href to the page 'epydoc-log.html'.
        @type src_code_tab_width: C{int}
        @keyword src_code_tab_width: Number of spaces to replace each tab
            with in source code listings.
        """
        self.docindex = docindex

        # Process keyword arguments.
        self._show_private = kwargs.get('show_private', 1)
        """Should private docs be included?"""
        
        self._prj_name = kwargs.get('prj_name', None)
        """The project's name (for the project link in the navbar)"""
        
        self._prj_url = kwargs.get('prj_url', None)
        """URL for the project link in the navbar"""
        
        self._prj_link = kwargs.get('prj_link', None)
        """HTML code for the project link in the navbar"""
        
        self._top_page = kwargs.get('top_page', None)
        """The 'main' page"""

        self._css = kwargs.get('css')
        """CSS stylesheet to use"""
        
        self._helpfile = kwargs.get('help_file', None)
        """Filename of file to extract help contents from"""
        
        self._frames_index = kwargs.get('show_frames', 1)
        """Should a frames index be created?"""
        
        self._show_imports = kwargs.get('show_imports', False)
        """Should imports be listed?"""
        
        self._propfunc_linelen = kwargs.get('property_function_linelength', 40)
        """[XXX] Not used!"""
        
        self._variable_maxlines = kwargs.get('variable_maxlines', 8)
        """Max lines for variable values"""
        
        self._variable_linelen = kwargs.get('variable_linelength', 70)
        """Max line length for variable values"""
        
        self._variable_summary_linelen = \
                         kwargs.get('variable_summary_linelength', 65)
        """Max length for variable value summaries"""
        
        self._variable_tooltip_linelen = \
                         kwargs.get('variable_tooltip_linelength', 600)
        """Max length for variable tooltips"""
        
        self._inheritance = kwargs.get('inheritance', 'listed')
        """How should inheritance be displayed?  'listed', 'included',
        or 'grouped'"""

        self._incl_sourcecode = kwargs.get('include_source_code', True)
        """Should pages be generated for source code of modules?"""

        self._mark_docstrings = kwargs.get('mark_docstrings', False)
        """Wrap <span class='docstring'>...</span> around docstrings?"""

        self._graph_types = kwargs.get('graphs', ()) or ()
        """Graphs that we should include in our output."""

        self._include_log = kwargs.get('include_log', False)
        """Are we generating an HTML log page?"""

        self._src_code_tab_width = kwargs.get('src_code_tab_width', 8)
        """Number of spaces to replace each tab with in source code
        listings."""
        
        self._callgraph_cache = {}
        """Map the callgraph L{uid<DotGraph.uid>} to their HTML
        representation."""

        self._redundant_details = kwargs.get('redundant_details', False)
        """If true, then include objects in the details list even if all
        info about them is already provided by the summary table."""

        # For use with select_variables():
        if self._show_private:
            self._public_filter = None
        else:
            self._public_filter = True
        
        # Make sure inheritance has a sane value.
        if self._inheritance not in ('listed', 'included', 'grouped'):
            raise ValueError, 'Bad value for inheritance'

        # Create the project homepage link, if it was not specified.
        if (self._prj_name or self._prj_url) and not self._prj_link:
            self._prj_link = plaintext_to_html(self._prj_name or
                                               'Project Homepage')

        # Add a hyperlink to _prj_url, if _prj_link doesn't already
        # contain any hyperlinks.
        if (self._prj_link and self._prj_url and
            not re.search(r'<a[^>]*\shref', self._prj_link)):
            self._prj_link = ('<a class="navbar" target="_top" href="'+
                              self._prj_url+'">'+self._prj_link+'</a>')

        # Precompute lists & sets of APIDoc objects that we're
        # interested in.
        self.valdocs = valdocs = sorted(docindex.reachable_valdocs(
            imports=False, packages=False, bases=False, submodules=False, 
            subclasses=False, private=self._show_private))
        self.module_list = [d for d in valdocs if isinstance(d, ModuleDoc)]
        """The list of L{ModuleDoc}s for the documented modules."""
        self.module_set = set(self.module_list)
        """The set of L{ModuleDoc}s for the documented modules."""
        self.class_list = [d for d in valdocs if isinstance(d, ClassDoc)]
        """The list of L{ClassDoc}s for the documented classes."""
        self.class_set = set(self.class_list)
        """The set of L{ClassDoc}s for the documented classes."""
        self.routine_list = [d for d in valdocs if isinstance(d, RoutineDoc)]
        """The list of L{RoutineDoc}s for the documented routines."""
        self.indexed_docs = []
        """The list of L{APIDoc}s for variables and values that should
        be included in the index."""

        # URL for 'trees' page
        if self.module_list: self._trees_url = 'module-tree.html'
        else: self._trees_url = 'class-tree.html'

        # Construct the value for self.indexed_docs.
        self.indexed_docs += [d for d in valdocs
                              if not isinstance(d, GenericValueDoc)]
        for doc in valdocs:
            if isinstance(doc, NamespaceDoc):
                # add any vars with generic values; but don't include
                # inherited vars.
                self.indexed_docs += [d for d in doc.variables.values() if
                                      isinstance(d.value, GenericValueDoc)
                                      and d.container == doc]
        self.indexed_docs.sort()

        # Figure out the url for the top page.
        self._top_page_url = self._find_top_page(self._top_page)

        # Decide whether or not to split the identifier index.
        self._split_ident_index = (len(self.indexed_docs) >=
                                   self.SPLIT_IDENT_INDEX_SIZE)
        
        # Figure out how many output files there will be (for progress
        # reporting).
        self.modules_with_sourcecode = set()
        for doc in self.module_list:
            if isinstance(doc, ModuleDoc) and is_src_filename(doc.filename):
                self.modules_with_sourcecode.add(doc)
        self._num_files = (len(self.class_list) + len(self.module_list) +
                           10 + len(self.METADATA_INDICES))
        if self._frames_index:
            self._num_files += len(self.module_list) + 3

        if self._incl_sourcecode:
            self._num_files += len(self.modules_with_sourcecode)
        if self._split_ident_index:
            self._num_files += len(self.LETTERS)
            
    def _find_top_page(self, pagename):
        """
        Find the top page for the API documentation.  This page is
        used as the default page shown in the main frame, when frames
        are used.  When frames are not used, this page is copied to 
        C{index.html}.

        @param pagename: The name of the page, as specified by the
            keyword argument C{top} to the constructor.
        @type pagename: C{string}
        @return: The URL of the top page.
        @rtype: C{string}
        """
        # If a page name was specified, then we need to figure out
        # what it points to.
        if pagename:
            # If it's a URL, then use it directly.
            if pagename.lower().startswith('http:'):
                return pagename

            # If it's an object, then use that object's page.
            try:
                doc = self.docindex.get_valdoc(pagename)
                return self.url(doc)
            except:
                pass

            # Otherwise, give up.
            log.warning('Could not find top page %r; using %s '
                        'instead' % (pagename, self._trees_url))
            return self._trees_url

        # If no page name was specified, then try to choose one
        # automatically.
        else:
            root = [val_doc for val_doc in self.docindex.root
                    if isinstance(val_doc, (ClassDoc, ModuleDoc))]
            if len(root) == 0:
                # No docs??  Try the trees page.
                return self._trees_url
            elif len(root) == 1:
                # One item in the root; use that.
                return self.url(root[0]) 
            else:
                # Multiple root items; if they're all in one package,
                # then use that.  Otherwise, use self._trees_url
                root = sorted(root, key=lambda v:len(v.canonical_name))
                top = root[0]
                for doc in root[1:]:
                    if not top.canonical_name.dominates(doc.canonical_name):
                        return self._trees_url
                else:
                    return self.url(top)
    
    #////////////////////////////////////////////////////////////
    #{ 1. Interface Methods
    #////////////////////////////////////////////////////////////

    def write(self, directory=None):
        """
        Write the documentation to the given directory.

        @type directory: C{string}
        @param directory: The directory to which output should be
            written.  If no directory is specified, output will be
            written to the current directory.  If the directory does
            not exist, it will be created.
        @rtype: C{None}
        @raise OSError: If C{directory} cannot be created.
        @raise OSError: If any file cannot be created or written to.
        """
        # For progress reporting:
        self._files_written = 0.
        
        # Set the default values for ValueDoc formatted representations.
        orig_valdoc_defaults = (ValueDoc.SUMMARY_REPR_LINELEN,
                                ValueDoc.REPR_LINELEN,
                                ValueDoc.REPR_MAXLINES)
        ValueDoc.SUMMARY_REPR_LINELEN = self._variable_summary_linelen
        ValueDoc.REPR_LINELEN = self._variable_linelen
        ValueDoc.REPR_MAXLINES = self._variable_maxlines

        # Use an image for the crarr symbol.
        from epydoc.markup.epytext import ParsedEpytextDocstring
        orig_crarr_html = ParsedEpytextDocstring.SYMBOL_TO_HTML['crarr']
        ParsedEpytextDocstring.SYMBOL_TO_HTML['crarr'] = (
            r'<span class="variable-linewrap">'
            r'<img src="crarr.png" alt="\" /></span>')

        # Keep track of failed xrefs, and report them at the end.
        self._failed_xrefs = {}

        # Create destination directories, if necessary
        if not directory: directory = os.curdir
        self._mkdir(directory)
        self._directory = directory

        # Write the CSS file.
        self._files_written += 1
        log.progress(self._files_written/self._num_files, 'epydoc.css')
        self.write_css(directory, self._css)

        # Write the Javascript file.
        self._files_written += 1
        log.progress(self._files_written/self._num_files, 'epydoc.js')
        self.write_javascript(directory)

        # Write images
        self.write_images(directory)

        # Build the indices.
        indices = {'ident': self.build_identifier_index(),
                   'term': self.build_term_index()}
        for (name, label, label2) in self.METADATA_INDICES:
            indices[name] = self.build_metadata_index(name)

        # Write the identifier index.  If requested, split it into
        # separate pages for each letter.
        ident_by_letter = self._group_by_letter(indices['ident'])
        if not self._split_ident_index:
            self._write(self.write_link_index, directory,
                        'identifier-index.html', indices,
                        'Identifier Index', 'identifier-index.html',
                        ident_by_letter)
        else:
            # Write a page for each section.
            for letter in self.LETTERS:
                filename = 'identifier-index-%s.html' % letter
                self._write(self.write_link_index, directory, filename,
                            indices, 'Identifier Index', filename,
                            ident_by_letter, [letter],
                            'identifier-index-%s.html')
            # Use the first non-empty section as the main index page.
            for letter in self.LETTERS:
                if letter in ident_by_letter:
                    filename = 'identifier-index.html'
                    self._write(self.write_link_index, directory, filename,
                                indices, 'Identifier Index', filename,
                                ident_by_letter, [letter], 
                                'identifier-index-%s.html')
                    break

        # Write the term index.
        if indices['term']:
            term_by_letter = self._group_by_letter(indices['term'])
            self._write(self.write_link_index, directory, 'term-index.html',
                        indices, 'Term Definition Index',
                        'term-index.html', term_by_letter)
        else:
            self._files_written += 1 # (skipped)

        # Write the metadata indices.
        for (name, label, label2) in self.METADATA_INDICES:
            if indices[name]:
                self._write(self.write_metadata_index, directory,
                            '%s-index.html' % name, indices, name,
                            label, label2)
            else:
                self._files_written += 1 # (skipped)

        # Write the trees file (package & class hierarchies)
        if self.module_list:
            self._write(self.write_module_tree, directory, 'module-tree.html')
        else:
            self._files_written += 1 # (skipped)
        if self.class_list:
            self._write(self.write_class_tree, directory, 'class-tree.html')
        else:
            self._files_written += 1 # (skipped)
        
        # Write the help file.
        self._write(self.write_help, directory,'help.html')
        
        # Write the frames-based table of contents.
        if self._frames_index:
            self._write(self.write_frames_index, directory, 'frames.html')
            self._write(self.write_toc, directory, 'toc.html')
            self._write(self.write_project_toc, directory, 'toc-everything.html')
            for doc in self.module_list:
                filename = 'toc-%s' % urllib.unquote(self.url(doc))
                self._write(self.write_module_toc, directory, filename, doc)

        # Write the object documentation.
        for doc in self.module_list:
            filename = urllib.unquote(self.url(doc))
            self._write(self.write_module, directory, filename, doc)
        for doc in self.class_list:
            filename = urllib.unquote(self.url(doc))
            self._write(self.write_class, directory, filename, doc)

        # Write source code files.
        if self._incl_sourcecode:
            # Build a map from short names to APIDocs, used when
            # linking names in the source code.
            name_to_docs = {}
            for api_doc in self.indexed_docs:
                if (api_doc.canonical_name is not None and
                    self.url(api_doc) is not None):
                    name = api_doc.canonical_name[-1]
                    name_to_docs.setdefault(name, []).append(api_doc)
            # Sort each entry of the name_to_docs list.
            for doc_list in name_to_docs.values():
                doc_list.sort()
            # Write the source code for each module.
            for doc in self.modules_with_sourcecode:
                filename = urllib.unquote(self.pysrc_url(doc))
                self._write(self.write_sourcecode, directory, filename, doc,
                            name_to_docs)

        # Write the auto-redirect page.
        self._write(self.write_redirect_page, directory, 'redirect.html')

        # Write the mapping object name -> URL
        self._write(self.write_api_list, directory, 'api-objects.txt')
        
        # Write the index.html files.
        # (this must be done last, since it might copy another file)
        self._files_written += 1
        log.progress(self._files_written/self._num_files, 'index.html')
        self.write_homepage(directory)

        # Don't report references to builtins as missing
        for k in self._failed_xrefs.keys(): # have a copy of keys
            if hasattr(__builtin__, k):
                del self._failed_xrefs[k]

        # Report any failed crossreferences
        if self._failed_xrefs:
            estr = 'Failed identifier crossreference targets:\n'
            failed_identifiers = self._failed_xrefs.keys()
            failed_identifiers.sort()
            for identifier in failed_identifiers:
                names = self._failed_xrefs[identifier].keys()
                names.sort()
                estr += '- %s' % identifier
                estr += '\n'
                for name in names:
                    estr += '      (from %s)\n' % name
            log.docstring_warning(estr)

        # [xx] testing:
        if self._num_files != int(self._files_written):
            log.debug("Expected to write %d files, but actually "
                      "wrote %d files" %
                      (self._num_files, int(self._files_written)))

        # Restore defaults that we changed.
        (ValueDoc.SUMMARY_REPR_LINELEN, ValueDoc.REPR_LINELEN,
         ValueDoc.REPR_MAXLINES) = orig_valdoc_defaults
        ParsedEpytextDocstring.SYMBOL_TO_HTML['crarr'] = orig_crarr_html

    def _write(self, write_func, directory, filename, *args):
        # Display our progress.
        self._files_written += 1
        log.progress(self._files_written/self._num_files, filename)
        
        path = os.path.join(directory, filename)
        f = codecs.open(path, 'w', 'ascii', errors='xmlcharrefreplace')
        write_func(f.write, *args)
        f.close()

    def _mkdir(self, directory):
        """
        If the given directory does not exist, then attempt to create it.
        @rtype: C{None}
        """
        if not os.path.isdir(directory):
            if os.path.exists(directory):
                raise OSError('%r is not a directory' % directory)
            os.mkdir(directory)
        
    #////////////////////////////////////////////////////////////
    #{ 2.1. Module Pages
    #////////////////////////////////////////////////////////////

    def write_module(self, out, doc):
        """
        Write an HTML page containing the API documentation for the
        given module to C{out}.
        
        @param doc: A L{ModuleDoc} containing the API documentation
        for the module that should be described.
        """
        longname = doc.canonical_name
        shortname = doc.canonical_name[-1]

        # Write the page header (incl. navigation bar & breadcrumbs)
        self.write_header(out, str(longname))
        self.write_navbar(out, doc)
        self.write_breadcrumbs(out, doc, self.url(doc))

        # Write the name of the module we're describing.
        if doc.is_package is True: typ = 'Package'
        else: typ = 'Module'
        if longname[0].startswith('script-'):
            shortname = str(longname)[7:]
            typ = 'Script'
        out('<!-- ==================== %s ' % typ.upper() +
            'DESCRIPTION ==================== -->\n')
        out('<h1 class="epydoc">%s %s</h1>' % (typ, shortname))
        out('<p class="nomargin-top">%s</p>\n' % self.pysrc_link(doc))
        
        # If the module has a description, then list it.
        if doc.descr not in (None, UNKNOWN):
            out(self.descr(doc, 2)+'\n\n')

        # Write any standarad metadata (todo, author, etc.)
        if doc.metadata is not UNKNOWN and doc.metadata:
            out('<hr />\n')
        self.write_standard_fields(out, doc)

        # If it's a package, then list the modules it contains.
        if doc.is_package is True:
            self.write_module_list(out, doc)

        # Write summary tables describing the variables that the
        # module defines.
        self.write_summary_table(out, "Classes", doc, "class")
        self.write_summary_table(out, "Functions", doc, "function")
        self.write_summary_table(out, "Variables", doc, "other")

        # Write a list of all imported objects.
        if self._show_imports:
            self.write_imports(out, doc)

        # Write detailed descriptions of functions & variables defined
        # in this module.
        self.write_details_list(out, "Function Details", doc, "function")
        self.write_details_list(out, "Variables Details", doc, "other")

        # Write the page footer (including navigation bar)
        self.write_navbar(out, doc)
        self.write_footer(out)

    #////////////////////////////////////////////////////////////
    #{ 2.??. Source Code Pages
    #////////////////////////////////////////////////////////////

    def write_sourcecode(self, out, doc, name_to_docs):
        #t0 = time.time()
        
        filename = doc.filename
        name = str(doc.canonical_name)
        
        # Header
        self.write_header(out, name)
        self.write_navbar(out, doc)
        self.write_breadcrumbs(out, doc, self.pysrc_url(doc))

        # Source code listing
        out('<h1 class="epydoc">Source Code for %s</h1>\n' %
            self.href(doc, label='%s %s' % (self.doc_kind(doc), name)))
        out('<pre class="py-src">\n')
        out(PythonSourceColorizer(filename, name, self.docindex,
                                  self.url, name_to_docs,
                                  self._src_code_tab_width).colorize())
        out('</pre>\n<br />\n')

        # Footer
        self.write_navbar(out, doc)
        self.write_footer(out)
        
        #log.debug('[%6.2f sec] Wrote pysrc for %s' %
        #          (time.time()-t0, name))

    #////////////////////////////////////////////////////////////
    #{ 2.2. Class Pages
    #////////////////////////////////////////////////////////////

    def write_class(self, out, doc):
        """
        Write an HTML page containing the API documentation for the
        given class to C{out}.
        
        @param doc: A L{ClassDoc} containing the API documentation
        for the class that should be described.
        """
        longname = doc.canonical_name
        shortname = doc.canonical_name[-1]

        # Write the page header (incl. navigation bar & breadcrumbs)
        self.write_header(out, str(longname))
        self.write_navbar(out, doc)
        self.write_breadcrumbs(out, doc, self.url(doc))

        # Write the name of the class we're describing.
        if doc.is_type(): typ = 'Type'
        elif doc.is_exception(): typ = 'Exception'
        else: typ = 'Class'
        out('<!-- ==================== %s ' % typ.upper() +
            'DESCRIPTION ==================== -->\n')
        out('<h1 class="epydoc">%s %s</h1>' % (typ, shortname))
        out('<p class="nomargin-top">%s</p>\n' % self.pysrc_link(doc))

        if ((doc.bases not in (UNKNOWN, None) and len(doc.bases) > 0) or
            (doc.subclasses not in (UNKNOWN,None) and len(doc.subclasses)>0)):
            # Display bases graphically, if requested.
            if 'umlclasstree' in self._graph_types:
                self.write_class_tree_graph(out, doc, uml_class_tree_graph)
            elif 'classtree' in self._graph_types:
                self.write_class_tree_graph(out, doc, class_tree_graph)
                
            # Otherwise, use ascii-art.
            else:
                # Write the base class tree.
                if doc.bases not in (UNKNOWN, None) and len(doc.bases) > 0:
                    out('<pre class="base-tree">\n%s</pre>\n\n' %
                        self.base_tree(doc))

                # Write the known subclasses
                if (doc.subclasses not in (UNKNOWN, None) and
                    len(doc.subclasses) > 0):
                    out('<dl><dt>Known Subclasses:</dt>\n<dd>\n    ')
                    out('  <ul class="subclass-list">\n')
                    for i, subclass in enumerate(doc.subclasses):
                        href = self.href(subclass, context=doc)
                        if self._val_is_public(subclass): css = ''
                        else: css = ' class="private"'
                        if i > 0: href = ', '+href
                        out('<li%s>%s</li>' % (css, href))
                    out('  </ul>\n')
                    out('</dd></dl>\n\n')

            out('<hr />\n')
        
        # If the class has a description, then list it.
        if doc.descr not in (None, UNKNOWN):
            out(self.descr(doc, 2)+'\n\n')

        # Write any standarad metadata (todo, author, etc.)
        if doc.metadata is not UNKNOWN and doc.metadata:
            out('<hr />\n')
        self.write_standard_fields(out, doc)

        # Write summary tables describing the variables that the
        # class defines.
        self.write_summary_table(out, "Nested Classes", doc, "class")
        self.write_summary_table(out, "Instance Methods", doc,
                                 "instancemethod")
        self.write_summary_table(out, "Class Methods", doc, "classmethod")
        self.write_summary_table(out, "Static Methods", doc, "staticmethod")
        self.write_summary_table(out, "Class Variables", doc,
                                 "classvariable")
        self.write_summary_table(out, "Instance Variables", doc,
                                 "instancevariable")
        self.write_summary_table(out, "Properties", doc, "property")

        # Write a list of all imported objects.
        if self._show_imports:
            self.write_imports(out, doc)

        # Write detailed descriptions of functions & variables defined
        # in this class.
        # [xx] why group methods into one section but split vars into two?
        # seems like we should either group in both cases or split in both
        # cases.
        self.write_details_list(out, "Method Details", doc, "method")
        self.write_details_list(out, "Class Variable Details", doc,
                                "classvariable")
        self.write_details_list(out, "Instance Variable Details", doc,
                                "instancevariable")
        self.write_details_list(out, "Property Details", doc, "property")

        # Write the page footer (including navigation bar)
        self.write_navbar(out, doc)
        self.write_footer(out)

    def write_class_tree_graph(self, out, doc, graphmaker):
        """
        Write HTML code for a class tree graph of C{doc} (a classdoc),
        using C{graphmaker} to draw the actual graph.  C{graphmaker}
        should be L{class_tree_graph()}, or L{uml_class_tree_graph()},
        or any other function with a compatible signature.

        If the given class has any private sublcasses (including
        recursive subclasses), then two graph images will be generated
        -- one to display when private values are shown, and the other
        to display when private values are hidden.
        """
        linker = _HTMLDocstringLinker(self, doc)
        private_subcls = self._private_subclasses(doc)
        if private_subcls:
            out('<center>\n'
                '  <div class="private">%s</div>\n'
                '  <div class="public" style="display:none">%s</div>\n'
                '</center>\n' %
                (self.render_graph(graphmaker(doc, linker, doc)),
                 self.render_graph(graphmaker(doc, linker, doc,
                                              exclude=private_subcls))))
        else:
            out('<center>\n%s\n</center>\n' %
                self.render_graph(graphmaker(doc, linker, doc)))

    #////////////////////////////////////////////////////////////
    #{ 2.3. Trees pages
    #////////////////////////////////////////////////////////////

    def write_module_tree(self, out):
        # Header material
        self.write_treepage_header(out, 'Module Hierarchy', 'module-tree.html')
        out('<h1 class="epydoc">Module Hierarchy</h1>\n')

        # Write entries for all top-level modules/packages.
        out('<ul class="nomargin-top">\n')
        for doc in self.module_list:
            if (doc.package in (None, UNKNOWN) or
                doc.package not in self.module_set):
                self.write_module_tree_item(out, doc)
        out('</ul>\n')

        # Footer material
        self.write_navbar(out, 'trees')
        self.write_footer(out)

    def write_class_tree(self, out):
        """
        Write HTML code for a nested list showing the base/subclass
        relationships between all documented classes.  Each element of
        the top-level list is a class with no (documented) bases; and
        under each class is listed all of its subclasses.  Note that
        in the case of multiple inheritance, a class may appear
        multiple times.  
        
        @todo: For multiple inheritance, don't repeat subclasses the
            second time a class is mentioned; instead, link to the
            first mention.
        """
        # [XX] backref for multiple inheritance?
        # Header material
        self.write_treepage_header(out, 'Class Hierarchy', 'class-tree.html')
        out('<h1 class="epydoc">Class Hierarchy</h1>\n')

        # Build a set containing all classes that we should list.
        # This includes everything in class_list, plus any of those
        # class' bases, but not undocumented subclasses.
        class_set = self.class_set.copy()
        for doc in self.class_list:
            if doc.bases != UNKNOWN:
                for base in doc.bases:
                    if base not in class_set:
                        if isinstance(base, ClassDoc):
                            class_set.update(base.mro())
                        else:
                            # [XX] need to deal with this -- how?
                            pass
                            #class_set.add(base)
 
        out('<ul class="nomargin-top">\n')
        for doc in sorted(class_set, key=lambda c:c.canonical_name[-1]):
            if doc.bases != UNKNOWN and len(doc.bases)==0:
                self.write_class_tree_item(out, doc, class_set)
        out('</ul>\n')
        
        # Footer material
        self.write_navbar(out, 'trees')
        self.write_footer(out)

    def write_treepage_header(self, out, title, url):
        # Header material.
        self.write_header(out, title)
        self.write_navbar(out, 'trees')
        self.write_breadcrumbs(out, 'trees', url)
        if self.class_list and self.module_list:
            out('<center><b>\n')
            out(' [ <a href="module-tree.html">Module Hierarchy</a>\n')
            out(' | <a href="class-tree.html">Class Hierarchy</a> ]\n')
            out('</b></center><br />\n')


    #////////////////////////////////////////////////////////////
    #{ 2.4. Index pages
    #////////////////////////////////////////////////////////////

    SPLIT_IDENT_INDEX_SIZE = 3000
    """If the identifier index has more than this number of entries,
    then it will be split into separate pages, one for each
    alphabetical section."""

    LETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ_'
    """The alphabetical sections that are used for link index pages."""
    
    def write_link_index(self, out, indices, title, url, index_by_section,
                         sections=LETTERS, section_url='#%s'):
        
        # Header
        self.write_indexpage_header(out, indices, title, url)

        # Index title & links to alphabetical sections.
        out('<table border="0" width="100%">\n'
            '<tr valign="bottom"><td>\n')
        out('<h1 class="epydoc">%s</h1>\n</td><td>\n[\n' % title)
        for sec in self.LETTERS:
            if sec in index_by_section:
                out(' <a href="%s">%s</a>\n' % (section_url % sec, sec))
            else:
                out('  %s\n' % sec)
        out(']\n')
        out('</td></table>\n')

        # Alphabetical sections.
        sections = [s for s in sections if s in index_by_section]
        if sections:
            out('<table border="0" width="100%">\n')
            for section in sorted(sections):
                out('<tr valign="top"><td valign="top" width="1%">')
                out('<h2 class="epydoc"><a name="%s">%s</a></h2></td>\n' %
                    (section, section))
                out('<td valign="top">\n')
                self.write_index_section(out, index_by_section[section], True)
                out('</td></tr>\n')
            out('</table>\n<br />')

        # Footer material.
        out('<br />')
        self.write_navbar(out, 'indices')
        self.write_footer(out)
                
        
    def write_metadata_index(self, out, indices, field, title, typ):
        """
        Write an HTML page containing a metadata index.
        """
        index = indices[field]
        
        # Header material.
        self.write_indexpage_header(out, indices, title,
                                    '%s-index.html' % field)

        # Page title.
        out('<h1 class="epydoc"><a name="%s">%s</a></h1>\n<br />\n' %
            (field, title))

        # Index (one section per arg)
        for arg in sorted(index):
            # Write a section title.
            if arg is not None:
                if len([1 for (doc, descrs) in index[arg] if
                        not self._doc_or_ancestor_is_private(doc)]) == 0:
                    out('<div class="private">')
                else:
                    out('<div>')
                self.write_table_header(out, 'metadata-index', arg)
                out('</table>')
            # List every descr for this arg.
            for (doc, descrs) in index[arg]:
                if self._doc_or_ancestor_is_private(doc):
                    out('<div class="private">\n')
                else:
                    out('<div>\n')
                out('<table width="100%" class="metadata-index" '
                    'bgcolor="#e0e0e0"><tr><td class="metadata-index">')
                out('<b>%s in %s</b>' %
                    (typ, self.href(doc, label=doc.canonical_name)))
                out('    <ul class="nomargin">\n')
                for descr in descrs:
                    out('      <li>%s</li>\n' %
                        self.docstring_to_html(descr,doc,4))
                out('    </ul>\n')
                out('</table></div>\n')

        # Footer material.
        out('<br />')
        self.write_navbar(out, 'indices')
        self.write_footer(out)

    def write_indexpage_header(self, out, indices, title, url):
        """
        A helper for the index page generation functions, which
        generates a header that can be used to navigate between the
        different indices.
        """
        self.write_header(out, title)
        self.write_navbar(out, 'indices')
        self.write_breadcrumbs(out, 'indices', url)

        if (indices['term'] or
            [1 for (name,l,l2) in self.METADATA_INDICES if indices[name]]):
            out('<center><b>[\n')
            out(' <a href="identifier-index.html">Identifiers</a>\n')
            if indices['term']:
                out('| <a href="term-index.html">Term Definitions</a>\n')
            for (name, label, label2) in self.METADATA_INDICES:
                if indices[name]:
                    out('| <a href="%s-index.html">%s</a>\n' %
                        (name, label2))
            out(']</b></center><br />\n')

    def write_index_section(self, out, items, add_blankline=False):
        out('<table class="link-index" width="100%" border="1">\n')
        num_rows = (len(items)+2)/3
        for row in range(num_rows):
            out('<tr>\n')
            for col in range(3):
                out('<td width="33%" class="link-index">')
                i = col*num_rows+row
                if i < len(items):
                    name, url, container = items[col*num_rows+row]
                    out('<a href="%s">%s</a>' % (url, name))
                    if container is not None:
                        out('<br />\n')
                        if isinstance(container, ModuleDoc):
                            label = container.canonical_name
                        else:
                            label = container.canonical_name[-1]
                        out('<span class="index-where">(in&nbsp;%s)'
                            '</span>' % self.href(container, label))
                else:
                    out('&nbsp;')
                out('</td>\n')
            out('</tr>\n')
            if add_blankline and num_rows == 1:
                blank_cell = '<td class="link-index">&nbsp;</td>'
                out('<tr>'+3*blank_cell+'</tr>\n')
        out('</table>\n')

    #////////////////////////////////////////////////////////////
    #{ 2.5. Help Page
    #////////////////////////////////////////////////////////////

    def write_help(self, out):
        """
        Write an HTML help file to the given stream.  If
        C{self._helpfile} contains a help file, then use it;
        otherwise, use the default helpfile from
        L{epydoc.docwriter.html_help}.
        """
        # todo: optionally parse .rst etc help files?
        
        # Get the contents of the help file.
        if self._helpfile:
            if os.path.exists(self._helpfile):
                try: help = open(self._helpfile).read()
                except: raise IOError("Can't open help file: %r" %
                                      self._helpfile)
            else:
                raise IOError("Can't find help file: %r" % self._helpfile)
        else:
            if self._prj_name: thisprj = self._prj_name
            else: thisprj = 'this project'
            help = HTML_HELP % {'this_project':thisprj}

        # Insert the help contents into a webpage.
        self.write_header(out, 'Help')
        self.write_navbar(out, 'help')
        self.write_breadcrumbs(out, 'help', 'help.html')
        out(help)
        self.write_navbar(out, 'help')
        self.write_footer(out)

    #////////////////////////////////////////////////////////////
    #{ 2.6. Frames-based Table of Contents
    #////////////////////////////////////////////////////////////
    
    write_frames_index = compile_template(
        """
        write_frames_index(self, out)

        Write the frames index file for the frames-based table of
        contents to the given streams.
        """,
        # /------------------------- Template -------------------------\
        '''
        <?xml version="1.0" encoding="iso-8859-1"?>
        <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN"
                  "DTD/xhtml1-frameset.dtd">
        <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
        <head>
          <title> $self._prj_name or "API Documentation"$ </title>
        </head>
        <frameset cols="20%,80%">
          <frameset rows="30%,70%">
            <frame src="toc.html" name="moduleListFrame"
                   id="moduleListFrame" />
            <frame src="toc-everything.html" name="moduleFrame"
                   id="moduleFrame" />
          </frameset>
          <frame src="$self._top_page_url$" name="mainFrame" id="mainFrame" />
        </frameset>
        </html>
        ''')
        # \------------------------------------------------------------/
    
    write_toc = compile_template(
        """
        write_toc(self, out)
        """,
        # /------------------------- Template -------------------------\
        '''
        >>> self.write_header(out, "Table of Contents")
        <h1 class="toc">Table&nbsp;of&nbsp;Contents</h1>
        <hr />
          <a target="moduleFrame" href="toc-everything.html">Everything</a>
          <br />
        >>> self.write_toc_section(out, "Modules", self.module_list)
        <hr />
        >>> if self._show_private:
          $self.PRIVATE_LINK$
        >>> #endif
        >>> self.write_footer(out, short=True)
        ''')
        # \------------------------------------------------------------/

    def write_toc_section(self, out, name, docs, fullname=True):
        if not docs: return

        # Assign names to each item, and sort by name.
        if fullname:
            docs = [(str(d.canonical_name), d) for d in docs]
        else:
            docs = [(str(d.canonical_name[-1]), d) for d in docs]
        docs.sort()

        out('  <h2 class="toc">%s</h2>\n' % name)
        for label, doc in docs:
            doc_url = self.url(doc)
            toc_url = 'toc-%s' % doc_url
            is_private = self._doc_or_ancestor_is_private(doc)
            if is_private:
                if not self._show_private: continue
                out('  <div class="private">\n')
                
            if isinstance(doc, ModuleDoc):
                out('    <a target="moduleFrame" href="%s"\n'
                    '     onclick="setFrame(\'%s\',\'%s\');"'
                    '     >%s</a><br />' % (toc_url, toc_url, doc_url, label))
            else:
                out('    <a target="mainFrame" href="%s"\n'
                    '     >%s</a><br />' % (doc_url, label))
            if is_private:
                out('  </div>\n')

    def write_project_toc(self, out):
        self.write_header(out, "Everything")
        out('<h1 class="toc">Everything</h1>\n')
        out('<hr />\n')

        # List the classes.
        self.write_toc_section(out, "All Classes", self.class_list)

        # List the functions.
        funcs = [d for d in self.routine_list 
                 if not isinstance(self.docindex.container(d), 
                                   (ClassDoc, types.NoneType))]
        self.write_toc_section(out, "All Functions", funcs)

        # List the variables.
        vars = []
        for doc in self.module_list:
            vars += doc.select_variables(value_type='other',
                                         imported=False,
                                         public=self._public_filter)
        self.write_toc_section(out, "All Variables", vars)

        # Footer material.
        out('<hr />\n')
        if self._show_private:
            out(self.PRIVATE_LINK+'\n')
        self.write_footer(out, short=True)

    def write_module_toc(self, out, doc):
        """
        Write an HTML page containing the table of contents page for
        the given module to the given streams.  This page lists the
        modules, classes, exceptions, functions, and variables defined
        by the module.
        """
        name = doc.canonical_name[-1]
        self.write_header(out, name)
        out('<h1 class="toc">Module %s</h1>\n' % name)
        out('<hr />\n')


        # List the classes.
        classes = doc.select_variables(value_type='class', imported=False,
                                       public=self._public_filter)
        self.write_toc_section(out, "Classes", classes, fullname=False)

        # List the functions.
        funcs = doc.select_variables(value_type='function', imported=False,
                                     public=self._public_filter)
        self.write_toc_section(out, "Functions", funcs, fullname=False)

        # List the variables.
        variables = doc.select_variables(value_type='other', imported=False,
                                         public=self._public_filter)
        self.write_toc_section(out, "Variables", variables, fullname=False)
        
        # Footer material.
        out('<hr />\n')
        if self._show_private:
            out(self.PRIVATE_LINK+'\n')
        self.write_footer(out, short=True)

    #////////////////////////////////////////////////////////////
    #{ 2.7. Project homepage (index.html)
    #////////////////////////////////////////////////////////////

    def write_homepage(self, directory):
        """
        Write an C{index.html} file in the given directory.  The
        contents of this file are copied or linked from an existing
        page, so this method must be called after all pages have been
        written.  The page used is determined by L{_frames_index} and
        L{_top_page}:
            - If L{_frames_index} is true, then C{frames.html} is
              copied.
            - Otherwise, the page specified by L{_top_page} is
              copied.
        """
        filename = os.path.join(directory, 'index.html')
        if self._frames_index: top = 'frames.html'
        else: top = self._top_page_url

        # Copy the non-frames index file from top, if it's internal.
        if top[:5] != 'http:' and '/' not in top:
            try:
                # Read top into `s`.
                topfile = os.path.join(directory, top)
                s = open(topfile, 'r').read()

                # Write the output file.
                open(filename, 'w').write(s)
                return
            except:
                log.error('Warning: error copying index; '
                          'using a redirect page')

        # Use a redirect if top is external, or if we faild to copy.
        name = self._prj_name or 'this project'
        f = open(filename, 'w')
        self.write_redirect_index(f.write, top, name)
        f.close()

    write_redirect_index = compile_template(
        """
        write_redirect_index(self, out, top, name)
        """,
        # /------------------------- Template -------------------------\
        '''
        <?xml version="1.0" encoding="iso-8859-1"?>
        <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
                  "DTD/xhtml1-strict.dtd">
        <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
        <head>
          <title> Redirect </title>
          <meta http-equiv="refresh" content="1;url=$top$" />
          <link rel="stylesheet" href="epydoc.css" type="text/css"></link>
        </head>
        <body>
          <p>Redirecting to the API documentation for
            <a href="$top$">$self._prj_name or "this project"$</a>...</p>
        </body>
        </html>
        ''')
        # \------------------------------------------------------------/

    #////////////////////////////////////////////////////////////
    #{ 2.8. Stylesheet (epydoc.css)
    #////////////////////////////////////////////////////////////

    def write_css(self, directory, cssname):
        """
        Write the CSS stylesheet in the given directory.  If
        C{cssname} contains a stylesheet file or name (from
        L{epydoc.docwriter.html_css}), then use that stylesheet;
        otherwise, use the default stylesheet.

        @rtype: C{None}
        """
        filename = os.path.join(directory, 'epydoc.css')
        
        # Get the contents for the stylesheet file.
        if cssname is None:
            css = STYLESHEETS['default'][0]
        else:
            if os.path.exists(cssname):
                try: css = open(cssname).read()
                except: raise IOError("Can't open CSS file: %r" % cssname)
            elif cssname in STYLESHEETS:
                css = STYLESHEETS[cssname][0]
            else:
                raise IOError("Can't find CSS file: %r" % cssname)

        # Write the stylesheet.
        cssfile = open(filename, 'w')
        cssfile.write(css)
        cssfile.close()

    #////////////////////////////////////////////////////////////
    #{ 2.9. Javascript (epydoc.js)
    #////////////////////////////////////////////////////////////

    def write_javascript(self, directory):
        jsfile = open(os.path.join(directory, 'epydoc.js'), 'w')
        print >> jsfile, self.TOGGLE_PRIVATE_JS
        print >> jsfile, self.SHOW_PRIVATE_JS
        print >> jsfile, self.GET_COOKIE_JS
        print >> jsfile, self.SET_FRAME_JS
        print >> jsfile, self.HIDE_PRIVATE_JS
        print >> jsfile, self.TOGGLE_CALLGRAPH_JS
        print >> jsfile, html_colorize.PYSRC_JAVASCRIPTS
        print >> jsfile, self.GET_ANCHOR_JS
        print >> jsfile, self.REDIRECT_URL_JS
        jsfile.close()

    #: A javascript that is used to show or hide the API documentation
    #: for private objects.  In order for this to work correctly, all
    #: documentation for private objects should be enclosed in 
    #: C{<div class="private">...</div>} elements.
    TOGGLE_PRIVATE_JS = '''
      function toggle_private() {
        // Search for any private/public links on this page.  Store
        // their old text in "cmd," so we will know what action to
        // take; and change their text to the opposite action.
        var cmd = "?";
        var elts = document.getElementsByTagName("a");
        for(var i=0; i<elts.length; i++) {
          if (elts[i].className == "privatelink") {
            cmd = elts[i].innerHTML;
            elts[i].innerHTML = ((cmd && cmd.substr(0,4)=="show")?
                                    "hide&nbsp;private":"show&nbsp;private");
          }
        }
        // Update all DIVs containing private objects.
        var elts = document.getElementsByTagName("div");
        for(var i=0; i<elts.length; i++) {
          if (elts[i].className == "private") {
            elts[i].style.display = ((cmd && cmd.substr(0,4)=="hide")?"none":"block");
          }
          else if (elts[i].className == "public") {
            elts[i].style.display = ((cmd && cmd.substr(0,4)=="hide")?"block":"none");
          }
        }
        // Update all table rows containing private objects.  Note, we
        // use "" instead of "block" becaue IE & firefox disagree on what
        // this should be (block vs table-row), and "" just gives the
        // default for both browsers.
        var elts = document.getElementsByTagName("tr");
        for(var i=0; i<elts.length; i++) {
          if (elts[i].className == "private") {
            elts[i].style.display = ((cmd && cmd.substr(0,4)=="hide")?"none":"");
          }
        }
        // Update all list items containing private objects.
        var elts = document.getElementsByTagName("li");
        for(var i=0; i<elts.length; i++) {
          if (elts[i].className == "private") {
            elts[i].style.display = ((cmd && cmd.substr(0,4)=="hide")?
                                        "none":"");
          }
        }
        // Update all list items containing private objects.
        var elts = document.getElementsByTagName("ul");
        for(var i=0; i<elts.length; i++) {
          if (elts[i].className == "private") {
            elts[i].style.display = ((cmd && cmd.substr(0,4)=="hide")?"none":"block");
          }
        }
        // Set a cookie to remember the current option.
        document.cookie = "EpydocPrivate="+cmd;
      }
      '''.strip()

    #: A javascript that is used to read the value of a cookie.  This
    #: is used to remember whether private variables should be shown or
    #: hidden.
    GET_COOKIE_JS = '''
      function getCookie(name) {
        var dc = document.cookie;
        var prefix = name + "=";
        var begin = dc.indexOf("; " + prefix);
        if (begin == -1) {
          begin = dc.indexOf(prefix);
          if (begin != 0) return null;
        } else
        { begin += 2; }
        var end = document.cookie.indexOf(";", begin);
        if (end == -1)
        { end = dc.length; }
        return unescape(dc.substring(begin + prefix.length, end));
      }
      '''.strip()

    #: A javascript that is used to set the contents of two frames at
    #: once.  This is used by the project table-of-contents frame to
    #: set both the module table-of-contents frame and the main frame
    #: when the user clicks on a module.
    SET_FRAME_JS = '''
      function setFrame(url1, url2) {
          parent.frames[1].location.href = url1;
          parent.frames[2].location.href = url2;
      }
    '''.strip()

    #: A javascript that is used to hide private variables, unless
    #: either: (a) the cookie says not to; or (b) we appear to be
    #: linking to a private variable.
    HIDE_PRIVATE_JS = '''
      function checkCookie() {
        var cmd=getCookie("EpydocPrivate");
        if (cmd && cmd.substr(0,4)!="show" && location.href.indexOf("#_") < 0)
            toggle_private();
      }
    '''.strip()

    TOGGLE_CALLGRAPH_JS = '''
      function toggleCallGraph(id) {
        var elt = document.getElementById(id);
        if (elt.style.display == "none")
            elt.style.display = "block";
        else
            elt.style.display = "none";
      }
    '''.strip()

    SHOW_PRIVATE_JS = '''
      function show_private() {
        var elts = document.getElementsByTagName("a");
        for(var i=0; i<elts.length; i++) {
          if (elts[i].className == "privatelink") {
            cmd = elts[i].innerHTML;
            if (cmd && cmd.substr(0,4)=="show")
                toggle_private();
          }
        }
      }
    '''.strip()

    GET_ANCHOR_JS = '''
      function get_anchor() {
          var href = location.href;
          var start = href.indexOf("#")+1;
          if ((start != 0) && (start != href.length))
              return href.substring(start, href.length);
      }
    '''.strip()

    #: A javascript that is used to implement the auto-redirect page.
    #: When the user visits <redirect.html#dotted.name>, they will
    #: automatically get redirected to the page for the object with
    #: the given fully-qualified dotted name.  E.g., for epydoc,
    #: <redirect.html#epydoc.apidoc.UNKNOWN> redirects the user to
    #: <epydoc.apidoc-module.html#UNKNOWN>.
    REDIRECT_URL_JS = '''
      function redirect_url(dottedName) {
          // Scan through each element of the "pages" list, and check
          // if "name" matches with any of them.
          for (var i=0; i<pages.length; i++) {

              // Each page has the form "<pagename>-m" or "<pagename>-c";
              // extract the <pagename> portion & compare it to dottedName.
              var pagename = pages[i].substring(0, pages[i].length-2);
              if (pagename == dottedName.substring(0,pagename.length)) {

                  // We\'ve found a page that matches `dottedName`;
                  // construct its URL, using leftover `dottedName`
                  // content to form an anchor.
                  var pagetype = pages[i].charAt(pages[i].length-1);
                  var url = pagename + ((pagetype=="m")?"-module.html":
                                                        "-class.html");
                  if (dottedName.length > pagename.length)
                      url += "#" + dottedName.substring(pagename.length+1,
                                                        dottedName.length);
                  return url;
              }
          }
      }
    '''.strip()
          

    #////////////////////////////////////////////////////////////
    #{ 2.10. Graphs
    #////////////////////////////////////////////////////////////

    def render_graph(self, graph):
        if graph is None: return ''
        graph.caption = graph.title = None
        image_url = '%s.gif' % graph.uid
        image_file = os.path.join(self._directory, image_url)
        return graph.to_html(image_file, image_url)
    
    RE_CALLGRAPH_ID = re.compile(r"""["'](.+-div)['"]""")
    
    def render_callgraph(self, callgraph, token=""):
        """Render the HTML chunk of a callgraph.

        If C{callgraph} is a string, use the L{_callgraph_cache} to return
        a pre-rendered HTML chunk. This mostly avoids to run C{dot} twice for
        the same callgraph. Else, run the graph and store its HTML output in
        the cache.

        @param callgraph: The graph to render or its L{uid<DotGraph.uid>}.
        @type callgraph: L{DotGraph} or C{str}
        @param token: A string that can be used to make the C{<div>} id
            unambiguous, if the callgraph is used more than once in a page.
        @type token: C{str}
        @return: The HTML representation of the graph.
        @rtype: C{str}
        """
        if callgraph is None: return ""
        
        if isinstance(callgraph, basestring):
            uid = callgraph
            rv = self._callgraph_cache.get(callgraph, "")

        else:
            uid = callgraph.uid
            graph_html = self.render_graph(callgraph)
            if graph_html == '':
                rv = ""
            else:
                rv = ('<div style="display:none" id="%%s-div"><center>\n'
                      '<table border="0" cellpadding="0" cellspacing="0">\n'
                      '  <tr><td>%s</td></tr>\n'
                      '  <tr><th>Call Graph</th></tr>\n'
                      '</table><br />\n</center></div>\n' % graph_html)

            # Store in the cache the complete HTML chunk without the
            # div id, which may be made unambiguous by the token
            self._callgraph_cache[uid] = rv

        # Mangle with the graph
        if rv: rv = rv % (uid + token)
        return rv

    def callgraph_link(self, callgraph, token=""):
        """Render the HTML chunk of a callgraph link.

        The link can toggles the visibility of the callgraph rendered using
        L{render_callgraph} with matching parameters.

        @param callgraph: The graph to render or its L{uid<DotGraph.uid>}.
        @type callgraph: L{DotGraph} or C{str}
        @param token: A string that can be used to make the C{<div>} id
            unambiguous, if the callgraph is used more than once in a page.
        @type token: C{str}
        @return: The HTML representation of the graph link.
        @rtype: C{str}
        """
        # Use class=codelink, to match style w/ the source code link.
        if callgraph is None: return ''

        if isinstance(callgraph, basestring):
            uid = callgraph
        else:
            uid = callgraph.uid

        return ('<br /><span class="codelink"><a href="javascript:void(0);" '
                'onclick="toggleCallGraph(\'%s-div\');return false;">'
                'call&nbsp;graph</a></span>&nbsp;' % (uid + token))

    #////////////////////////////////////////////////////////////
    #{ 2.11. Images
    #////////////////////////////////////////////////////////////

    IMAGES = {'crarr.png': # Carriage-return arrow, used for LINEWRAP.
              'iVBORw0KGgoAAAANSUhEUgAAABEAAAAKCAMAAABlokWQAAAALHRFWHRD'
              'cmVhdGlvbiBUaW1lAFR1\nZSAyMiBBdWcgMjAwNiAwMDo0MzoxMCAtMD'
              'UwMGAMEFgAAAAHdElNRQfWCBYFASkQ033WAAAACXBI\nWXMAAB7CAAAe'
              'wgFu0HU+AAAABGdBTUEAALGPC/xhBQAAAEVQTFRF////zcOw18/AgGY0'
              'c1cg4dvQ\ninJEYEAAYkME3NXI6eTcloFYe2Asr5+AbE4Uh29A9fPwqp'
              'l4ZEUI8O3onopk0Ma0lH5U1nfFdgAA\nAAF0Uk5TAEDm2GYAAABNSURB'
              'VHjaY2BAAbzsvDAmK5oIlxgfioiwCAe7KJKIgKAQOzsLLwTwA0VY\n+d'
              'iRAT8T0AxuIIMHqoaXCWIPGzsHJ6orGJiYWRjQASOcBQAocgMSPKMTIg'
              'AAAABJRU5ErkJggg==\n',
              }

    def write_images(self, directory):
        for (name, data) in self.IMAGES.items():
            f = open(os.path.join(directory, name), 'wb')
            f.write(base64.decodestring(data))
            f.close()

    #////////////////////////////////////////////////////////////
    #{ 3.1. Page Header
    #////////////////////////////////////////////////////////////

    write_header = compile_template(
        """
        write_header(self, out, title)

        Generate HTML code for the standard page header, and write it
        to C{out}.  C{title} is a string containing the page title.
        It should be appropriately escaped/encoded.
        """,
        # /------------------------- Template -------------------------\
        '''
        <?xml version="1.0" encoding="ascii"?>
        <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
                  "DTD/xhtml1-transitional.dtd">
        <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
        <head>
          <title>$title$</title>
          <link rel="stylesheet" href="epydoc.css" type="text/css" />
          <script type="text/javascript" src="epydoc.js"></script>
        </head>
        
        <body bgcolor="white" text="black" link="blue" vlink="#204080"
              alink="#204080">
        ''')
        # \------------------------------------------------------------/

    #////////////////////////////////////////////////////////////
    #{ 3.2. Page Footer
    #////////////////////////////////////////////////////////////

    write_footer = compile_template(
        """
        write_footer(self, out, short=False)

        Generate HTML code for the standard page footer, and write it
        to C{out}.
        """,
        # /------------------------- Template -------------------------\
        '''
        >>> if not short:
        <table border="0" cellpadding="0" cellspacing="0" width="100%%">
          <tr>
            <td align="left" class="footer">
        >>>   if self._include_log:
            <a href="epydoc-log.html">Generated by Epydoc
            $epydoc.__version__$ on $time.asctime()$</a>
        >>>   else:
            Generated by Epydoc $epydoc.__version__$ on $time.asctime()$
        >>>   #endif
            </td>
            <td align="right" class="footer">
              <a target="mainFrame" href="http://epydoc.sourceforge.net"
                >http://epydoc.sourceforge.net</a>
            </td>
          </tr>
        </table>
        >>> #endif

        <script type="text/javascript">
          <!--
          // Private objects are initially displayed (because if
          // javascript is turned off then we want them to be
          // visible); but by default, we want to hide them.  So hide
          // them unless we have a cookie that says to show them.
          checkCookie();
          // -->
        </script>
        </body>
        </html>
        ''')
        # \------------------------------------------------------------/

    #////////////////////////////////////////////////////////////
    #{ 3.3. Navigation Bar
    #////////////////////////////////////////////////////////////

    write_navbar = compile_template(
        """
        write_navbar(self, out, context)

        Generate HTML code for the navigation bar, and write it to
        C{out}.  The navigation bar typically looks like::

             [ Home Trees Index Help             Project ]

        @param context: A value indicating what page we're generating
        a navigation bar for.  If we're generating an API
        documentation page for an object, then C{context} is a
        L{ValueDoc} containing the documentation for that object;
        otherwise, C{context} is a string name for the page.  The
        following string names are recognized: C{'tree'}, C{'index'},
        and C{'help'}.
        """,
        # /------------------------- Template -------------------------\
        '''
        <!-- ==================== NAVIGATION BAR ==================== -->
        <table class="navbar" border="0" width="100%" cellpadding="0"
               bgcolor="#a0c0ff" cellspacing="0">
          <tr valign="middle">
        >>> if self._top_page_url not in (self._trees_url, "identifier-index.html", "help.html"):
          <!-- Home link -->
        >>>   if (isinstance(context, ValueDoc) and
        >>>       self._top_page_url == self.url(context.canonical_name)):
              <th bgcolor="#70b0f0" class="navbar-select"
                  >&nbsp;&nbsp;&nbsp;Home&nbsp;&nbsp;&nbsp;</th>
        >>>   else:
              <th>&nbsp;&nbsp;&nbsp;<a
                href="$self._top_page_url$">Home</a>&nbsp;&nbsp;&nbsp;</th>
        >>> #endif
        
          <!-- Tree link -->
        >>> if context == "trees":
              <th bgcolor="#70b0f0" class="navbar-select"
                  >&nbsp;&nbsp;&nbsp;Trees&nbsp;&nbsp;&nbsp;</th>
        >>> else:
              <th>&nbsp;&nbsp;&nbsp;<a
                href="$self._trees_url$">Trees</a>&nbsp;&nbsp;&nbsp;</th>
        >>> #endif
        
          <!-- Index link -->
        >>> if context == "indices":
              <th bgcolor="#70b0f0" class="navbar-select"
                  >&nbsp;&nbsp;&nbsp;Indices&nbsp;&nbsp;&nbsp;</th>
        >>> else:
              <th>&nbsp;&nbsp;&nbsp;<a
                href="identifier-index.html">Indices</a>&nbsp;&nbsp;&nbsp;</th>
        >>> #endif
        
          <!-- Help link -->
        >>> if context == "help":
              <th bgcolor="#70b0f0" class="navbar-select"
                  >&nbsp;&nbsp;&nbsp;Help&nbsp;&nbsp;&nbsp;</th>
        >>> else:
              <th>&nbsp;&nbsp;&nbsp;<a
                href="help.html">Help</a>&nbsp;&nbsp;&nbsp;</th>
        >>> #endif
        
        >>> if self._prj_link:
          <!-- Project homepage -->
              <th class="navbar" align="right" width="100%">
                <table border="0" cellpadding="0" cellspacing="0">
                  <tr><th class="navbar" align="center"
                    >$self._prj_link.strip()$</th>
                  </tr></table></th>
        >>> else:
              <th class="navbar" width="100%"></th>
        >>> #endif
          </tr>
        </table>
        ''')
        # \------------------------------------------------------------/

    #////////////////////////////////////////////////////////////
    #{ 3.4. Breadcrumbs
    #////////////////////////////////////////////////////////////

    write_breadcrumbs = compile_template(
        """
        write_breadcrumbs(self, out, context, context_url)

        Generate HTML for the breadcrumbs line, and write it to
        C{out}.  The breadcrumbs line is an invisible table with a
        list of pointers to the current object's ancestors on the
        left; and the show/hide private selector and the
        frames/noframes selector on the right.

        @param context: The API documentation for the object whose
        breadcrumbs we should generate.
        @type context: L{ValueDoc}
        """,
        # /------------------------- Template -------------------------\
        '''
        <table width="100%" cellpadding="0" cellspacing="0">
          <tr valign="top">
        >>> if isinstance(context, APIDoc):
            <td width="100%">
              <span class="breadcrumbs">
        >>>   crumbs = self.breadcrumbs(context)
        >>>   for crumb in crumbs[:-1]:
                $crumb$ ::
        >>>   #endfor
                $crumbs[-1]$
              </span>
            </td>
        >>> else:
            <td width="100%">&nbsp;</td>
        >>> #endif
            <td>
              <table cellpadding="0" cellspacing="0">
                <!-- hide/show private -->
        >>> if self._show_private:
                <tr><td align="right">$self.PRIVATE_LINK$</td></tr>
        >>> #endif
        >>> if self._frames_index:
                <tr><td align="right"><span class="options"
                    >[<a href="frames.html" target="_top">frames</a
                    >]&nbsp;|&nbsp;<a href="$context_url$"
                    target="_top">no&nbsp;frames</a>]</span></td></tr>
        >>> #endif
              </table>
            </td>
          </tr>
        </table>
        ''')
        # \------------------------------------------------------------/

    def breadcrumbs(self, doc):
        crumbs = [self._crumb(doc)]

        # Generate the crumbs for uid's ancestors.
        while True:
            container = self.docindex.container(doc)
            assert doc != container, 'object is its own container?'
            if container is None:
                if doc.canonical_name is UNKNOWN:
                    return ['??']+crumbs
                elif isinstance(doc, ModuleDoc):
                    return ['Package&nbsp;%s' % ident
                            for ident in doc.canonical_name[:-1]]+crumbs
                else:
                    return list(doc.canonical_name)+crumbs
            else:
                label = self._crumb(container)
                name = container.canonical_name
                crumbs.insert(0, self.href(container, label)) # [xx] code=0??
                doc = container
        
    def _crumb(self, doc):
        if (len(doc.canonical_name)==1 and
            doc.canonical_name[0].startswith('script-')):
            return 'Script&nbsp;%s' % doc.canonical_name[0][7:]
        return '%s&nbsp;%s' % (self.doc_kind(doc), doc.canonical_name[-1])

    #////////////////////////////////////////////////////////////
    #{ 3.5. Summary Tables
    #////////////////////////////////////////////////////////////

    def write_summary_table(self, out, heading, doc, value_type):
        """
        Generate HTML code for a summary table, and write it to
        C{out}.  A summary table is a table that includes a one-row
        description for each variable (of a given type) in a module
        or class.

        @param heading: The heading for the summary table; typically,
            this indicates what kind of value the table describes
            (e.g., functions or classes).
        @param doc: A L{ValueDoc} object containing the API
            documentation for the module or class whose variables
            we should summarize.
        @param value_type: A string indicating what type of value
            should be listed in this summary table.  This value
            is passed on to C{doc}'s C{select_variables()} method.
        """
        # inh_var_groups is a dictionary used to hold "inheritance
        # pseudo-groups", which are created when inheritance is
        # 'grouped'.  It maps each base to a list of vars inherited
        # from that base.
        grouped_inh_vars = {}

        # Divide all public variables of the given type into groups.
        groups = [(plaintext_to_html(group_name),
                   doc.select_variables(group=group_name, imported=False,
                                        value_type=value_type,
                                        public=self._public_filter))
                  for group_name in doc.group_names()]
                
        # Discard any empty groups; and return if they're all empty.
        groups = [(g,vars) for (g,vars) in groups if vars]
        if not groups: return

        # Write a header
        self.write_table_header(out, "summary", heading)

        # Write a section for each group.
        for name, var_docs in groups:
            self.write_summary_group(out, doc, name,
                                     var_docs, grouped_inh_vars)

        # Write a section for each inheritance pseudo-group (used if
        # inheritance=='grouped')
        if grouped_inh_vars:
            for base in doc.mro():
                if base in grouped_inh_vars:
                    hdr = 'Inherited from %s' % self.href(base, context=doc)
                    tr_class = ''
                    if len([v for v in grouped_inh_vars[base]
                            if v.is_public]) == 0:
                        tr_class = ' class="private"'
                    self.write_group_header(out, hdr, tr_class)
                    for var_doc in grouped_inh_vars[base]:
                        self.write_summary_line(out, var_doc, doc)

        # Write a footer for the table.
        out(self.TABLE_FOOTER)

    def write_summary_group(self, out, doc, name, var_docs, grouped_inh_vars):
        # Split up the var_docs list, according to the way each var
        # should be displayed:
        #   - listed_inh_vars -- for listed inherited variables.
        #   - grouped_inh_vars -- for grouped inherited variables.
        #   - normal_vars -- for all other variables.
        listed_inh_vars = {}
        normal_vars = []
        for var_doc in var_docs:
            if var_doc.container != doc:
                base = var_doc.container
                if not isinstance(base, ClassDoc):
                    # This *should* never happen:
                    log.warning("%s's container is not a class!" % var_doc)
                    normal_vars.append(var_doc)
                elif (base not in self.class_set or
                    self._inheritance == 'listed'):
                    listed_inh_vars.setdefault(base,[]).append(var_doc)
                elif self._inheritance == 'grouped':
                    grouped_inh_vars.setdefault(base,[]).append(var_doc)
                else:
                    normal_vars.append(var_doc)
            else:
                normal_vars.append(var_doc)
            
        # Write a header for the group.
        if name != '':
            tr_class = ''
            if len([v for v in var_docs if v.is_public]) == 0:
                tr_class = ' class="private"'
            self.write_group_header(out, name, tr_class)

        # Write a line for each normal var:
        for var_doc in normal_vars:
            self.write_summary_line(out, var_doc, doc)
        # Write a subsection for inherited vars:
        if listed_inh_vars:
            self.write_inheritance_list(out, doc, listed_inh_vars)

    def write_inheritance_list(self, out, doc, listed_inh_vars):
        out('  <tr>\n    <td colspan="2" class="summary">\n')
        for base in doc.mro():
            if base not in listed_inh_vars: continue
            public_vars = [v for v in listed_inh_vars[base]
                           if v.is_public]
            private_vars = [v for v in listed_inh_vars[base]
                            if not v.is_public]
            if public_vars:
                out('    <p class="indent-wrapped-lines">'
                    '<b>Inherited from <code>%s</code></b>:\n' %
                    self.href(base, context=doc))
                self.write_var_list(out, public_vars)
                out('      </p>\n')
            if private_vars and self._show_private:
                out('    <div class="private">')
                out('    <p class="indent-wrapped-lines">'
                    '<b>Inherited from <code>%s</code></b> (private):\n' %
                    self.href(base, context=doc))
                self.write_var_list(out, private_vars)
                out('      </p></div>\n')
        out('    </td>\n  </tr>\n')
    
    def write_var_list(self, out, vardocs):
        out('      ')
        out(',\n      '.join(['<code>%s</code>' % self.href(v,v.name)
                              for v in vardocs])+'\n')

    def write_summary_line(self, out, var_doc, container):
        """
        Generate HTML code for a single line of a summary table, and
        write it to C{out}.  See L{write_summary_table} for more
        information.
        
        @param var_doc: The API documentation for the variable that
            should be described by this line of the summary table.
        @param container: The API documentation for the class or
            module whose summary table we're writing.
        """
        pysrc_link = None
        callgraph = None

        # If it's a private variable, then mark its <tr>.
        if var_doc.is_public: tr_class = ''
        else: tr_class = ' class="private"'

        # Decide an anchor or a link is to be generated.
        link_name = self._redundant_details or var_doc.is_detailed()
        anchor = not link_name

        # Construct the HTML code for the type (cell 1) & description
        # (cell 2).
        if isinstance(var_doc.value, RoutineDoc):
            typ = self.return_type(var_doc, indent=6)
            description = self.function_signature(var_doc, is_summary=True,
                link_name=link_name, anchor=anchor)
            pysrc_link = self.pysrc_link(var_doc.value)

            # Perpare the call-graph, if requested
            if 'callgraph' in self._graph_types:
                linker = _HTMLDocstringLinker(self, var_doc.value)
                callgraph = call_graph([var_doc.value], self.docindex,
                                       linker, var_doc, add_callers=True, 
                                       add_callees=True)
                if callgraph and callgraph.nodes:
                    var_doc.value.callgraph_uid = callgraph.uid
                else:
                    callgraph = None
        else:
            typ = self.type_descr(var_doc, indent=6)
            description = self.summary_name(var_doc,
                link_name=link_name, anchor=anchor)
            if isinstance(var_doc.value, GenericValueDoc):
                # The summary max length has been chosen setting
                # L{ValueDoc.SUMMARY_REPR_LINELEN} in the constructor
                max_len=self._variable_summary_linelen-3-len(var_doc.name)
                val_repr = var_doc.value.summary_pyval_repr(max_len)
                tooltip = self.variable_tooltip(var_doc)
                description += (' = <code%s>%s</code>' %
                                (tooltip, val_repr.to_html(None)))

        # Add the summary to the description (if there is one).
        summary = self.summary(var_doc, indent=6)
        if summary: description += '<br />\n      %s' % summary
        
        # If it's inherited, then add a note to the description.
        if var_doc.container != container and self._inheritance=="included":
            description += ("\n      <em>(Inherited from " +
                        self.href(var_doc.container) + ")</em>")

        # Write the summary line.
        self._write_summary_line(out, typ, description, tr_class, pysrc_link,
                                 callgraph)

    _write_summary_line = compile_template(
        "_write_summary_line(self, out, typ, description, tr_class, "
                            "pysrc_link, callgraph)",
        # /------------------------- Template -------------------------\
        '''
          <tr$tr_class$>
            <td width="15%" align="right" valign="top" class="summary">
              <span class="summary-type">$typ or "&nbsp;"$</span>
            </td><td class="summary">
        >>> if pysrc_link is not None or callgraph is not None:
              <table width="100%" cellpadding="0" cellspacing="0" border="0">
                <tr>
                  <td>$description$</td>
                  <td align="right" valign="top">
                    $pysrc_link$
                    $self.callgraph_link(callgraph, token='-summary')$
                  </td>
                </tr>
              </table>
              $self.render_callgraph(callgraph, token='-summary')$
        >>> #endif
        >>> if pysrc_link is None and callgraph is None:
                $description$
        >>> #endif
            </td>
          </tr>
        ''')
        # \------------------------------------------------------------/

    #////////////////////////////////////////////////////////////
    #{ 3.6. Details Lists
    #////////////////////////////////////////////////////////////

    def write_details_list(self, out, heading, doc, value_type):
        # Get a list of the VarDocs we should describe.
        if self._redundant_details:
            detailed = None
        else:
            detailed = True
        if isinstance(doc, ClassDoc):
            var_docs = doc.select_variables(value_type=value_type,
                                            imported=False, inherited=False,
                                            public=self._public_filter,
                                            detailed=detailed)
        else:
            var_docs = doc.select_variables(value_type=value_type,
                                            imported=False,
                                            public=self._public_filter,
                                            detailed=detailed)
        if not var_docs: return

        # Write a header
        self.write_table_header(out, "details", heading)
        out(self.TABLE_FOOTER)

        for var_doc in var_docs:
            self.write_details_entry(out, var_doc)

        out('<br />\n')

    def write_details_entry(self, out, var_doc):
        descr = self.descr(var_doc, indent=2) or ''
        if var_doc.is_public: div_class = ''
        else: div_class = ' class="private"'

        # Functions
        if isinstance(var_doc.value, RoutineDoc):
            rtype = self.return_type(var_doc, indent=10)
            rdescr = self.return_descr(var_doc, indent=10)
            arg_descrs = []
            args = set()
            # Find the description for each arg.  (Leave them in the
            # same order that they're listed in the docstring.)
            for (arg_names, arg_descr) in var_doc.value.arg_descrs:
                args.update(arg_names)
                lhs = ', '.join([self.arg_name_to_html(var_doc.value, n)
                                 for n in arg_names])
                rhs = self.docstring_to_html(arg_descr, var_doc.value, 10)
                arg_descrs.append( (lhs, rhs) )
            # Check for arguments for which we have @type but not @param;
            # and add them to the arg_descrs list.
            for arg in var_doc.value.arg_types:
                if arg not in args:
                    argname = self.arg_name_to_html(var_doc.value, arg)
                    arg_descrs.append( (argname,'') )

            self.write_function_details_entry(out, var_doc, descr,
                                              var_doc.value.callgraph_uid,
                                              rtype, rdescr, arg_descrs,
                                              div_class)

        # Properties
        elif isinstance(var_doc.value, PropertyDoc):
            prop_doc = var_doc.value
            accessors = [ (name,
                           self.property_accessor_to_html(val_doc, prop_doc),
                           self.summary(val_doc))
                         for (name, val_doc) in
                            [('Get', prop_doc.fget), ('Set', prop_doc.fset),
                             ('Delete', prop_doc.fdel)]
                            if val_doc not in (None, UNKNOWN)
                            and val_doc.pyval is not None ]

            self.write_property_details_entry(out, var_doc, descr,
                                              accessors, div_class)
        
        # Variables
        else:
            self.write_variable_details_entry(out, var_doc, descr, div_class)

    def labelled_list_item(self, lhs, rhs):
        # If the RHS starts with a paragraph, then move the
        # paragraph-start tag to the beginning of the lhs instead (so
        # there won't be a line break after the '-').
        m = re.match(r'^<p( [^>]+)?>', rhs)
        if m:
            lhs = m.group() + lhs
            rhs = rhs[m.end():]

        if rhs:
            return '<li>%s - %s</li>' % (lhs, rhs)
        else:
            return '<li>%s</li>' % (lhs,)

    def property_accessor_to_html(self, val_doc, context=None):
        if val_doc not in (None, UNKNOWN):
            if isinstance(val_doc, RoutineDoc):
                return self.function_signature(val_doc, is_summary=True,
                                               link_name=True, context=context)
            elif isinstance(val_doc, GenericValueDoc):
                return self.pprint_value(val_doc)
            else:
                return self.href(val_doc, context=context)
        else:
            return '??'
        
    def arg_name_to_html(self, func_doc, arg_name):
        """
        A helper function used to format an argument name, for use in
        the argument description list under a routine's details entry.
        This just wraps strong & code tags around the arg name; and if
        the arg name is associated with a type, then adds it
        parenthetically after the name.
        """
        s = '<strong class="pname"><code>%s</code></strong>' % arg_name
        if arg_name in func_doc.arg_types:
            typ = func_doc.arg_types[arg_name]
            typ_html = self.docstring_to_html(typ, func_doc, 10)
            s += " (%s)" % typ_html
        return s

    write_function_details_entry = compile_template(
        '''
        write_function_details_entry(self, out, var_doc, descr, callgraph, \
                                     rtype, rdescr, arg_descrs, div_class)
        ''',
        # /------------------------- Template -------------------------\
        '''
        >>> func_doc = var_doc.value
        <a name="$var_doc.name$"></a>
        <div$div_class$>
        >>> self.write_table_header(out, "details")
        <tr><td>
          <table width="100%" cellpadding="0" cellspacing="0" border="0">
          <tr valign="top"><td>
          <h3 class="epydoc">$self.function_signature(var_doc)$
        >>> if var_doc.name in self.SPECIAL_METHODS:
            <br /><em class="fname">($self.SPECIAL_METHODS[var_doc.name]$)</em>
        >>> #endif
        >>> if isinstance(func_doc, ClassMethodDoc):
            <br /><em class="fname">Class Method</em>
        >>> #endif
        >>> if isinstance(func_doc, StaticMethodDoc):
            <br /><em class="fname">Static Method</em>
        >>> #endif
          </h3>
          </td><td align="right" valign="top"
            >$self.pysrc_link(func_doc)$&nbsp;
            $self.callgraph_link(callgraph)$</td>
          </tr></table>
          $self.render_callgraph(callgraph)$
          $descr$
          <dl class="fields">
        >>> # === parameters ===
        >>> if arg_descrs:
            <dt>Parameters:</dt>
            <dd><ul class="nomargin-top">
        >>>   for lhs, rhs in arg_descrs:
                $self.labelled_list_item(lhs, rhs)$
        >>>   #endfor
            </ul></dd>
        >>> #endif
        >>> # === return type ===
        >>> if rdescr and rtype:
            <dt>Returns: $rtype$</dt>
                <dd>$rdescr$</dd>
        >>> elif rdescr:
            <dt>Returns:</dt>
                <dd>$rdescr$</dd>
        >>> elif rtype:
            <dt>Returns: $rtype$</dt>
        >>> #endif
        >>> # === decorators ===
        >>> if func_doc.decorators not in (None, UNKNOWN):
        >>>   # (staticmethod & classmethod are already shown, above)
        >>>   decos = filter(lambda deco:
        >>>     not ((deco=="staticmethod" and
        >>>            isinstance(func_doc, StaticMethodDoc)) or
        >>>          (deco=="classmethod" and
        >>>           isinstance(func_doc, ClassMethodDoc))),
        >>>     func_doc.decorators)
        >>> else:
        >>>   decos = None
        >>> #endif
        >>> if decos:
            <dt>Decorators:</dt>
            <dd><ul class="nomargin-top">
        >>>   for deco in decos:
                <li><code>@$deco$</code></li>
        >>>   #endfor
            </ul></dd>
        >>> #endif
        >>> # === exceptions ===
        >>> if func_doc.exception_descrs not in (None, UNKNOWN, (), []):
            <dt>Raises:</dt>
            <dd><ul class="nomargin-top">
        >>>   for name, descr in func_doc.exception_descrs:
        >>>     exc_name = self.docindex.find(name, func_doc)
        >>>     if exc_name is not None:
        >>>       name = self.href(exc_name, label=str(name))
        >>>     #endif
                $self.labelled_list_item(
                    "<code><strong class=\'fraise\'>" +
                    str(name) + "</strong></code>",
                    self.docstring_to_html(descr, func_doc, 8))$
        >>>   #endfor
            </ul></dd>
        >>> #endif
        >>> # === overrides ===
        >>> if var_doc.overrides not in (None, UNKNOWN):
            <dt>Overrides:
        >>>   # Avoid passing GenericValueDoc to href()
        >>>   if isinstance(var_doc.overrides.value, RoutineDoc):
                $self.href(var_doc.overrides.value, context=var_doc)$
        >>>   else:
        >>>     # In this case, a less interesting label is generated.
                $self.href(var_doc.overrides, context=var_doc)$
        >>>   #endif
        >>>   if (func_doc.docstring in (None, UNKNOWN) and
        >>>       var_doc.overrides.value.docstring not in (None, UNKNOWN)):
                <dd><em class="note">(inherited documentation)</em></dd>
        >>>   #endif
            </dt>
        >>> #endif
          </dl>
        >>> # === metadata ===
        >>> self.write_standard_fields(out, func_doc)
        </td></tr></table>
        </div>
        ''')
        # \------------------------------------------------------------/

    # Names for the __special__ methods.
    SPECIAL_METHODS ={
    '__init__': 'Constructor',
    '__del__': 'Destructor',
    '__add__': 'Addition operator',
    '__sub__': 'Subtraction operator',
    '__and__': 'And operator',
    '__or__': 'Or operator',
    '__xor__': 'Exclusive-Or operator',
    '__repr__': 'Representation operator',
    '__call__': 'Call operator',
    '__getattr__': 'Qualification operator',
    '__getitem__': 'Indexing operator',
    '__setitem__': 'Index assignment operator',
    '__delitem__': 'Index deletion operator',
    '__delslice__': 'Slice deletion operator',
    '__setslice__': 'Slice assignment operator',
    '__getslice__': 'Slicling operator',
    '__len__': 'Length operator',
    '__cmp__': 'Comparison operator',
    '__eq__': 'Equality operator',
    '__in__': 'Containership operator',
    '__gt__': 'Greater-than operator',
    '__lt__': 'Less-than operator',
    '__ge__': 'Greater-than-or-equals operator',
    '__le__': 'Less-than-or-equals operator',
    '__radd__': 'Right-side addition operator',
    '__hash__': 'Hashing function',
    '__contains__': 'In operator',
    '__nonzero__': 'Boolean test operator',
    '__str__': 'Informal representation operator',
    }

    write_property_details_entry = compile_template(
        '''
        write_property_details_entry(self, out, var_doc, descr, \
                                     accessors, div_class)
        ''',
        # /------------------------- Template -------------------------\
        '''
        >>> prop_doc = var_doc.value
        <a name="$var_doc.name$"></a>
        <div$div_class$>
        >>> self.write_table_header(out, "details")
        <tr><td>
          <h3 class="epydoc">$var_doc.name$</h3>
          $descr$
          <dl class="fields">
        >>> for (name, val, summary) in accessors:
            <dt>$name$ Method:</dt>
            <dd class="value">$val$
        >>>     if summary:
                - $summary$
        >>>     #endif
            </dd>
        >>> #endfor
        >>> if prop_doc.type_descr not in (None, UNKNOWN):
            <dt>Type:</dt>
              <dd>$self.type_descr(var_doc, indent=6)$</dd>
        >>> #endif
          </dl>
        >>> self.write_standard_fields(out, prop_doc)
        </td></tr></table>
        </div>
        ''')
        # \------------------------------------------------------------/
        
    write_variable_details_entry = compile_template(
        '''
        write_variable_details_entry(self, out, var_doc, descr, div_class)
        ''',
        # /------------------------- Template -------------------------\
        '''
        <a name="$var_doc.name$"></a>
        <div$div_class$>
        >>> self.write_table_header(out, "details")
        <tr><td>
          <h3 class="epydoc">$var_doc.name$</h3>
          $descr$
          <dl class="fields">
        >>> if var_doc.type_descr not in (None, UNKNOWN):
            <dt>Type:</dt>
              <dd>$self.type_descr(var_doc, indent=6)$</dd>
        >>> #endif
          </dl>
        >>> self.write_standard_fields(out, var_doc)
        >>> if var_doc.value is not UNKNOWN:
          <dl class="fields">
            <dt>Value:</dt>
              <dd>$self.pprint_value(var_doc.value)$</dd>
          </dl>
        >>> #endif
        </td></tr></table>
        </div>
        ''')
        # \------------------------------------------------------------/

    def variable_tooltip(self, var_doc):
        if var_doc.value in (None, UNKNOWN):
            return ''
        s = var_doc.value.pyval_repr().to_plaintext(None)
        if len(s) > self._variable_tooltip_linelen:
            s = s[:self._variable_tooltip_linelen-3]+'...'
        return ' title="%s"' % plaintext_to_html(s)

    def pprint_value(self, val_doc):
        if val_doc is UNKNOWN:
            return '??'
        elif isinstance(val_doc, GenericValueDoc):
            return ('<table><tr><td><pre class="variable">\n' +
                    val_doc.pyval_repr().to_html(None) +
                    '\n</pre></td></tr></table>\n')
        else:
            return self.href(val_doc)

    #////////////////////////////////////////////////////////////
    #{ Base Tree
    #////////////////////////////////////////////////////////////

    def base_tree(self, doc, width=None, postfix='', context=None):
        """
        @return: The HTML code for a class's base tree.  The tree is
            drawn 'upside-down' and right justified, to allow for
            multiple inheritance.
        @rtype: C{string}
        """
        if context is None:
            context = doc.defining_module
        if width == None: width = self.find_tree_width(doc, context)
        if isinstance(doc, ClassDoc) and doc.bases != UNKNOWN:
            bases = doc.bases
        else:
            bases = []
        
        if postfix == '':
            # [XX] use var name instead of canonical name?
            s = (' '*(width-2) + '<strong class="uidshort">'+
                   self.contextual_label(doc, context)+'</strong>\n')
        else: s = ''
        for i in range(len(bases)-1, -1, -1):
            base = bases[i]
            label = self.contextual_label(base, context)
            s = (' '*(width-4-len(label)) + self.href(base, label)
                   +' --+'+postfix+'\n' + 
                   ' '*(width-4) +
                   '   |'+postfix+'\n' +
                   s)
            if i != 0:
                s = (self.base_tree(base, width-4, '   |'+postfix, context)+s)
            else:
                s = (self.base_tree(base, width-4, '    '+postfix, context)+s)
        return s

    def find_tree_width(self, doc, context):
        """
        Helper function for L{base_tree}.
        @return: The width of a base tree, when drawn
            right-justified.  This is used by L{base_tree} to
            determine how far to indent lines of the base tree.
        @rtype: C{int}
        """
        if not isinstance(doc, ClassDoc): return 2
        if doc.bases == UNKNOWN: return 2
        width = 2
        for base in doc.bases:
            width = max(width, len(self.contextual_label(base, context))+4,
                        self.find_tree_width(base, context)+4)
        return width

    def contextual_label(self, doc, context):
        """
        Return the label for C{doc} to be shown in C{context}.
        """
        if doc.canonical_name is None:
            if doc.parse_repr is not None:
                return doc.parse_repr
            else:
                return '??'
        else:
            if context is UNKNOWN:
                return str(doc.canonical_name)
            else:
                context_name = context.canonical_name
                return str(doc.canonical_name.contextualize(context_name))
        
    #////////////////////////////////////////////////////////////
    #{ Function Signatures
    #////////////////////////////////////////////////////////////

    def function_signature(self, api_doc, is_summary=False, 
                           link_name=False, anchor=False, context=None):
        """Render a function signature in HTML.

        @param api_doc: The object whose name is to be rendered. If a
            C{VariableDoc}, its C{value} should be a C{RoutineDoc}
        @type api_doc: L{VariableDoc} or L{RoutineDoc}
        @param is_summary: True if the fuction is to be rendered in the summary.
        type css_class: C{bool}
        @param link_name: If True, the name is a link to the object anchor.
        @type link_name: C{bool}
        @param anchor: If True, the name is the object anchor.
        @type anchor: C{bool}
        @param context: If set, represent the function name from this context.
            Only useful when C{api_doc} is a L{RoutineDoc}.
        @type context: L{DottedName}

        @return: The HTML code for the object.
        @rtype: C{str}
        """
        if is_summary: css_class = 'summary-sig'
        else: css_class = 'sig'
        
        # [XX] clean this up!
        if isinstance(api_doc, VariableDoc):
            func_doc = api_doc.value
            # This should never happen, but just in case:
            if api_doc.value in (None, UNKNOWN):
                return (('<span class="%s"><span class="%s-name">%s'+
                         '</span>(...)</span>') %
                        (css_class, css_class, api_doc.name))
            # Get the function's name.
            name = self.summary_name(api_doc, css_class=css_class+'-name',
                                     link_name=link_name, anchor=anchor)
        else:
            func_doc = api_doc
            name = self.href(api_doc, css_class=css_class+'-name',
                             context=context)

        if func_doc.posargs == UNKNOWN:
            args = ['...']
        else:
            args = [self.func_arg(n, d, css_class) for (n, d)
                    in zip(func_doc.posargs, func_doc.posarg_defaults)]
        if func_doc.vararg not in (None, UNKNOWN):
            if func_doc.vararg == '...':
                args.append('<span class="%s-arg">...</span>' % css_class)
            else:
                args.append('<span class="%s-arg">*%s</span>' %
                            (css_class, func_doc.vararg))
        if func_doc.kwarg not in (None, UNKNOWN):
            args.append('<span class="%s-arg">**%s</span>' %
                        (css_class, func_doc.kwarg))

        return ('<span class="%s">%s(%s)</span>' %
                (css_class, name, ',\n        '.join(args)))

    def summary_name(self, api_doc, css_class='summary-name',
                     link_name=False, anchor=False):
        """Render an object name in HTML.

        @param api_doc: The object whose name is to be rendered
        @type api_doc: L{APIDoc}
        @param css_class: The CSS class to assign to the rendered name
        type css_class: C{str}
        @param link_name: If True, the name is a link to the object anchor.
        @type link_name: C{bool}
        @param anchor: If True, the name is the object anchor.
        @type anchor: C{bool}

        @return: The HTML code for the object.
        @rtype: C{str}
        """
        if anchor:
            rv = '<a name="%s"></a>' % api_doc.name
        else:
            rv = ''

        if link_name:
            rv += self.href(api_doc, css_class=css_class)
        else:
            rv += '<span class="%s">%s</span>' % (css_class, api_doc.name)

        return rv

    # [xx] tuple args???
    def func_arg(self, name, default, css_class):
        name = self._arg_name(name)
        s = '<span class="%s-arg">%s</span>' % (css_class, name)
        if default is not None:
            s += ('=<span class="%s-default">%s</span>' %
                    (css_class, default.summary_pyval_repr().to_html(None)))
        return s

    def _arg_name(self, arg):
        if isinstance(arg, basestring):
            return arg
        elif len(arg) == 1:
            return '(%s,)' % self._arg_name(arg[0])
        else:
            return '(%s)' % (', '.join([self._arg_name(a) for a in arg]))


    

    #////////////////////////////////////////////////////////////
    #{ Import Lists
    #////////////////////////////////////////////////////////////
        
    def write_imports(self, out, doc):
        assert isinstance(doc, NamespaceDoc)
        imports = doc.select_variables(imported=True,
                                       public=self._public_filter)
        if not imports: return

        out('<p class="indent-wrapped-lines">')
        out('<b>Imports:</b>\n  ')
        out(',\n  '.join([self._import(v, doc) for v in imports]))
        out('\n</p><br />\n')

    def _import(self, var_doc, context):
        if var_doc.imported_from not in (None, UNKNOWN):
            return self.href(var_doc.imported_from,
                             var_doc.name, context=context,
                             tooltip='%s' % var_doc.imported_from)
        elif (var_doc.value not in (None, UNKNOWN) and not
              isinstance(var_doc.value, GenericValueDoc)):
            return self.href(var_doc.value,
                             var_doc.name, context=context,
                             tooltip='%s' % var_doc.value.canonical_name)
        else:
            return plaintext_to_html(var_doc.name)
            
    #////////////////////////////////////////////////////////////
    #{ Function Attributes
    #////////////////////////////////////////////////////////////
        
    #////////////////////////////////////////////////////////////
    #{ Module Trees
    #////////////////////////////////////////////////////////////

    def write_module_list(self, out, doc):
        if len(doc.submodules) == 0: return
        self.write_table_header(out, "summary", "Submodules")

        for group_name in doc.group_names():
            if not doc.submodule_groups[group_name]: continue
            if group_name:
                self.write_group_header(out, group_name)
            out('  <tr><td class="summary">\n'
                '  <ul class="nomargin">\n')
            for submodule in doc.submodule_groups[group_name]:
                self.write_module_tree_item(out, submodule, package=doc)
            out('  </ul></td></tr>\n')
                
        out(self.TABLE_FOOTER+'\n<br />\n')

    def write_module_tree_item(self, out, doc, package=None):
        # If it's a private variable, then mark its <li>.
        var = package and package.variables.get(doc.canonical_name[-1])
        priv = ((var is not None and var.is_public is False) or
                (var is None and doc.canonical_name[-1].startswith('_')))
        out('    <li%s> <strong class="uidlink">%s</strong>'
            % (priv and ' class="private"' or '', self.href(doc)))
        if doc.summary not in (None, UNKNOWN):
            out(': <em class="summary">'+
                self.description(doc.summary, doc, 8)+'</em>')
        if doc.submodules != UNKNOWN and doc.submodules:
            if priv: out('\n    <ul class="private">\n')
            else: out('\n    <ul>\n')
            for submodule in doc.submodules:
                self.write_module_tree_item(out, submodule, package=doc)
            out('    </ul>\n')
        out('    </li>\n')

    #////////////////////////////////////////////////////////////
    #{ Class trees
    #////////////////////////////////////////////////////////////

    write_class_tree_item = compile_template(
        '''
        write_class_tree_item(self, out, doc, class_set)
        ''',
        # /------------------------- Template -------------------------\
        '''
        >>> if doc.summary in (None, UNKNOWN):
            <li> <strong class="uidlink">$self.href(doc)$</strong>
        >>> else:
            <li> <strong class="uidlink">$self.href(doc)$</strong>:
              <em class="summary">$self.description(doc.summary, doc, 8)$</em>
        >>> # endif
        >>> if doc.subclasses:
            <ul>
        >>>   for subclass in sorted(set(doc.subclasses), key=lambda c:c.canonical_name[-1]):
        >>>     if subclass in class_set:
        >>>       self.write_class_tree_item(out, subclass, class_set)
        >>>     #endif
        >>>   #endfor
            </ul>
        >>> #endif
            </li>
        ''')
        # \------------------------------------------------------------/
    
    #////////////////////////////////////////////////////////////
    #{ Standard Fields
    #////////////////////////////////////////////////////////////

    def write_standard_fields(self, out, doc):
        """
        Write HTML code containing descriptions of any standard markup
        fields that are defined by the given L{APIDoc} object (such as
        C{@author} and C{@todo} fields).

        @param doc: The L{APIDoc} object containing the API documentation
            for the object whose standard markup fields should be
            described.
        """
        fields = []
        field_values = {}
        
        for (field, arg, descr) in doc.metadata:
            if field not in field_values:
                fields.append(field)
            if field.takes_arg:
                subfields = field_values.setdefault(field,{})
                subfields.setdefault(arg,[]).append(descr)
            else:
                field_values.setdefault(field,[]).append(descr)

        if not fields: return

        out('<div class="fields">')
        for field in fields:
            if field.takes_arg:
                for arg, descrs in field_values[field].items():
                    self.write_standard_field(out, doc, field, descrs, arg)
                                              
            else:
                self.write_standard_field(out, doc, field, field_values[field])

        out('</div>')

    write_standard_field = compile_template(
        """
        write_standard_field(self, out, doc, field, descrs, arg='')
        
        """,
        # /------------------------- Template -------------------------\
        '''
        >>> if arg: arglabel = " (%s)" % arg
        >>> else: arglabel = ""
        >>>   if len(descrs) == 1:
              <p><strong>$field.singular+arglabel$:</strong>
                $self.description(descrs[0], doc, 8)$
              </p>
        >>>   elif field.short:
              <dl><dt>$field.plural+arglabel$:</dt>
                <dd>
        >>>     for descr in descrs[:-1]:
                  $self.description(descr, doc, 10)$,
        >>>     # end for
                  $self.description(descrs[-1], doc, 10)$
                </dd>
              </dl>
        >>>   else:
              <strong>$field.plural+arglabel$:</strong>
              <ul class="nomargin-top">
        >>>     for descr in descrs:
                <li>
                $self.description(descr, doc, 8)$
                </li>
        >>>     # end for
              </ul>
        >>>   # end else
        >>> # end for
        ''')
        # \------------------------------------------------------------/

    #////////////////////////////////////////////////////////////
    #{ Index generation
    #////////////////////////////////////////////////////////////

    #: A list of metadata indices that should be generated.  Each
    #: entry in this list is a tuple C{(tag, label, short_label)},
    #: where C{tag} is the cannonical tag of a metadata field;
    #: C{label} is a label for the index page; and C{short_label}
    #: is a shorter label, used in the index selector.
    METADATA_INDICES = [('bug', 'Bug List', 'Bugs'),
                        ('todo', 'To Do List', 'To Do'),
                        ('change', 'Change Log', 'Changes'),
                        ('deprecated', 'Deprecation List', 'Deprecations'),
                        ('since', 'Introductions List', 'Introductions'),
                        ]
    
    def build_identifier_index(self):
        items = []
        for doc in self.indexed_docs:
            name = plaintext_to_html(doc.canonical_name[-1])
            if isinstance(doc, RoutineDoc): name += '()'
            url = self.url(doc)
            if not url: continue
            container = self.docindex.container(doc)
            items.append( (name, url, container) )
        return sorted(items, key=lambda v:v[0].lower())

    def _group_by_letter(self, items):
        """Preserves sort order of the input."""
        index = {}
        for item in items:
            first_letter = item[0][0].upper()
            if not ("A" <= first_letter <= "Z"):
                first_letter = '_'
            index.setdefault(first_letter, []).append(item)
        return index
    
    def build_term_index(self):
        items = []
        for doc in self.indexed_docs:
            url = self.url(doc)
            items += self._terms_from_docstring(url, doc, doc.descr)
            for (field, arg, descr) in doc.metadata:
                items += self._terms_from_docstring(url, doc, descr)
                if hasattr(doc, 'type_descr'):
                    items += self._terms_from_docstring(url, doc,
                                                        doc.type_descr)
                if hasattr(doc, 'return_descr'):
                    items += self._terms_from_docstring(url, doc,
                                                        doc.return_descr)
                if hasattr(doc, 'return_type'):
                    items += self._terms_from_docstring(url, doc,
                                                        doc.return_type)
        return sorted(items, key=lambda v:v[0].lower())

    def _terms_from_docstring(self, base_url, container, parsed_docstring):
        if parsed_docstring in (None, UNKNOWN): return []
        terms = []
        # Strip any existing anchor off:
        base_url = re.sub('#.*', '', '%s' % (base_url,))
        for term in parsed_docstring.index_terms():
            anchor = self._term_index_to_anchor(term)
            url = '%s#%s' % (base_url, anchor)
            terms.append( (term.to_plaintext(None), url, container) )
        return terms

    def build_metadata_index(self, field_name):
        # Build the index.
        index = {}
        for doc in self.indexed_docs:
            if (not self._show_private and
                self._doc_or_ancestor_is_private(doc)):
                continue
            descrs = {}
            if doc.metadata is not UNKNOWN:
                for (field, arg, descr) in doc.metadata:
                    if field.tags[0] == field_name:
                        descrs.setdefault(arg, []).append(descr)
            for (arg, descr_list) in descrs.iteritems():
                index.setdefault(arg, []).append( (doc, descr_list) )
        return index

    def _term_index_to_anchor(self, term):
        """
        Given the name of an inline index item, construct a URI anchor.
        These anchors are used to create links from the index page to each
        index item.
        """
        # Include "-" so we don't accidentally collide with the name
        # of a python identifier.
        s = re.sub(r'\s\s+', '-', term.to_plaintext(None))
        return "index-"+re.sub("[^a-zA-Z0-9]", "_", s)

    #////////////////////////////////////////////////////////////
    #{ Redirect page
    #////////////////////////////////////////////////////////////

    def write_redirect_page(self, out):
        """
        Build the auto-redirect page, which translates dotted names to
        URLs using javascript.  When the user visits
        <redirect.html#dotted.name>, they will automatically get
        redirected to the page for the object with the given
        fully-qualified dotted name.  E.g., for epydoc,
        <redirect.html#epydoc.apidoc.UNKNOWN> redirects the user to
        <epydoc.apidoc-module.html#UNKNOWN>.
        """
        # Construct a list of all the module & class pages that we're
        # documenting.  The redirect_url javascript will scan through
        # this list, looking for a page name that matches the
        # requested dotted name.
        pages = (['%s-m' % val_doc.canonical_name
                  for val_doc in self.module_list] +
                 ['%s-c' % val_doc.canonical_name
                  for val_doc in self.class_list])
        # Sort the pages from longest to shortest.  This ensures that
        # we find e.g. "x.y.z" in the list before "x.y".
        pages = sorted(pages, key=lambda p:-len(p))

        # Write the redirect page.
        self._write_redirect_page(out, pages)

    _write_redirect_page = compile_template(
        '''
        _write_redirect_page(self, out, pages)
        ''',
        # /------------------------- Template -------------------------\
        '''
        <html><head><title>Epydoc Redirect Page</title>
        <meta http-equiv="cache-control" content="no-cache" />
        <meta http-equiv="expires" content="0" />
        <meta http-equiv="pragma" content="no-cache" />
          <script type="text/javascript" src="epydoc.js"></script>
        </head>
        <body>
        <script type="text/javascript">
        <!--
        var pages = $"[%s]" % ", ".join(['"%s"' % v for v in pages])$;
        var dottedName = get_anchor();
        if (dottedName) {
            var target = redirect_url(dottedName);
            if (target) window.location.replace(target);
        }
        // -->
        </script>

        <h3>Epydoc Auto-redirect page</h3>
        
        <p>When javascript is enabled, this page will redirect URLs of
        the form <tt>redirect.html#<i>dotted.name</i></tt> to the
        documentation for the object with the given fully-qualified
        dotted name.</p>
        <p><a id="message"> &nbsp; </a></p>
        
        <script type="text/javascript">
        <!--
        if (dottedName) {
            var msg = document.getElementById("message");
            msg.innerHTML = "No documentation found for <tt>"+
                            dottedName+"</tt>";
        }
        // -->
        </script>

        </body>
        </html>
        ''')
        # \------------------------------------------------------------/

    #////////////////////////////////////////////////////////////
    #{ URLs list
    #////////////////////////////////////////////////////////////

    def write_api_list(self, out):
        """
        Write a list of mapping name->url for all the documented objects.
        """
        # Construct a list of all the module & class pages that we're
        # documenting.  The redirect_url javascript will scan through
        # this list, looking for a page name that matches the
        # requested dotted name.
        skip = (ModuleDoc, ClassDoc, type(UNKNOWN))
        for val_doc in self.module_list:
            self.write_url_record(out, val_doc)
            for var in val_doc.variables.itervalues():
                if not isinstance(var.value, skip):
                    self.write_url_record(out, var)

        for val_doc in self.class_list:
            self.write_url_record(out, val_doc)
            for var in val_doc.variables.itervalues():
                self.write_url_record(out, var)

    def write_url_record(self, out, obj):
        url = self.url(obj)
        if url is not None:
            out("%s\t%s\n" % (obj.canonical_name, url))

    #////////////////////////////////////////////////////////////
    #{ Helper functions
    #////////////////////////////////////////////////////////////

    def _val_is_public(self, valdoc):
        """Make a best-guess as to whether the given class is public."""
        container = self.docindex.container(valdoc)
        if isinstance(container, NamespaceDoc):
            for vardoc in container.variables.values():
                if vardoc in (UNKNOWN, None): continue
                if vardoc.value is valdoc:
                    return vardoc.is_public
        return True

    # [XX] Is it worth-while to pull the anchor tricks that I do here?
    # Or should I just live with the fact that show/hide private moves
    # stuff around?
    write_table_header = compile_template(
        '''
        write_table_header(self, out, css_class, heading=None, \
                           private_link=True, colspan=2)
        ''',
        # /------------------------- Template -------------------------\
        '''
        >>> if heading is not None:
        >>>     anchor = "section-%s" % re.sub("\W", "", heading)
        <!-- ==================== $heading.upper()$ ==================== -->
        <a name="$anchor$"></a>
        >>> #endif
        <table class="$css_class$" border="1" cellpadding="3"
               cellspacing="0" width="100%" bgcolor="white">
        >>> if heading is not None:
        <tr bgcolor="#70b0f0" class="table-header">
        >>>     if private_link and self._show_private:
          <td colspan="$colspan$" class="table-header">
            <table border="0" cellpadding="0" cellspacing="0" width="100%">
              <tr valign="top">
                <td align="left"><span class="table-header">$heading$</span></td>
                <td align="right" valign="top"
                 ><span class="options">[<a href="#$anchor$"
                 class="privatelink" onclick="toggle_private();"
                 >hide private</a>]</span></td>
              </tr>
            </table>
          </td>
        >>>     else:
          <td align="left" colspan="2" class="table-header">
            <span class="table-header">$heading$</span></td>
        >>>     #endif
        </tr>
        >>> #endif
        ''')
        # \------------------------------------------------------------/

    TABLE_FOOTER = '</table>\n'

    PRIVATE_LINK = '''
    <span class="options">[<a href="javascript:void(0);" class="privatelink"
    onclick="toggle_private();">hide&nbsp;private</a>]</span>
    '''.strip()

    write_group_header = compile_template(
        '''
        write_group_header(self, out, group, tr_class='')
        ''',
        # /------------------------- Template -------------------------\
        '''
        <tr bgcolor="#e8f0f8" $tr_class$>
          <th colspan="2" class="group-header"
            >&nbsp;&nbsp;&nbsp;&nbsp;$group$</th></tr>
        ''')
        # \------------------------------------------------------------/

    _url_cache = {}
    def url(self, obj):
        """
        Return the URL for the given object, which can be a
        C{VariableDoc}, a C{ValueDoc}, or a C{DottedName}.
        """
        cached_url = self._url_cache.get(id(obj))
        if cached_url is not None:
            return cached_url
        else:
            url = self._url_cache[id(obj)] = self._url(obj)
            return url

    def _url(self, obj):
        """
        Internal helper for L{url}.
        """
        # Module: <canonical_name>-module.html
        if isinstance(obj, ModuleDoc):
            if obj not in self.module_set: return None
            return urllib.quote('%s'%obj.canonical_name) + '-module.html'
        # Class: <canonical_name>-class.html
        elif isinstance(obj, ClassDoc):
            if obj not in self.class_set: return None
            return urllib.quote('%s'%obj.canonical_name) + '-class.html'
        # Variable
        elif isinstance(obj, VariableDoc):
            val_doc = obj.value
            if isinstance(val_doc, (ModuleDoc, ClassDoc)):
                return self.url(val_doc)
            elif obj.container in (None, UNKNOWN):
                if val_doc in (None, UNKNOWN): return None
                return self.url(val_doc)
            elif obj.is_imported == True:
                if obj.imported_from is not UNKNOWN:
                    return self.url(obj.imported_from)
                else:
                    return None
            else:
                container_url = self.url(obj.container)
                if container_url is None: return None
                return '%s#%s' % (container_url, urllib.quote('%s'%obj.name))
        # Value (other than module or class)
        elif isinstance(obj, ValueDoc):
            container = self.docindex.container(obj)
            if container is None:
                return None # We couldn't find it!
            else:
                container_url = self.url(container)
                if container_url is None: return None
                anchor = urllib.quote('%s'%obj.canonical_name[-1])
                return '%s#%s' % (container_url, anchor)
        # Dotted name: look up the corresponding APIDoc
        elif isinstance(obj, DottedName):
            val_doc = self.docindex.get_valdoc(obj)
            if val_doc is None: return None
            return self.url(val_doc)
        # Special pages:
        elif obj == 'indices':
            return 'identifier-index.html'
        elif obj == 'help':
            return 'help.html'
        elif obj == 'trees':
            return self._trees_url
        else:
            raise ValueError, "Don't know what to do with %r" % obj

    def pysrc_link(self, api_doc):
        if not self._incl_sourcecode:
            return ''
        url = self.pysrc_url(api_doc)
        if url is not None: 
            return ('<span class="codelink"><a href="%s">source&nbsp;'
                    'code</a></span>' % url)
        else:
            return ''
    
    def pysrc_url(self, api_doc):
        if isinstance(api_doc, VariableDoc):
            if api_doc.value not in (None, UNKNOWN):
                return pysrc_url(api_doc.value)
            else:
                return None
        elif isinstance(api_doc, ModuleDoc):
            if api_doc in self.modules_with_sourcecode:
                return ('%s-pysrc.html' %
                       urllib.quote('%s' % api_doc.canonical_name))
            else:
                return None
        else:
            module = api_doc.defining_module
            if module == UNKNOWN: return None
            module_pysrc_url = self.pysrc_url(module)
            if module_pysrc_url is None: return None
            module_name = module.canonical_name
            if not module_name.dominates(api_doc.canonical_name, True):
                log.debug('%r is in %r but name does not dominate' %
                          (api_doc, module))
                return module_pysrc_url
            mname_len = len(module.canonical_name)
            anchor = '%s' % api_doc.canonical_name[mname_len:]
            return '%s#%s' % (module_pysrc_url, urllib.quote(anchor))
        
        # We didn't find it:
        return None

    # [xx] add code to automatically do <code> wrapping or the like?
    def href(self, target, label=None, css_class=None, context=None,
             tooltip=None):
        """
        Return the HTML code for an HREF link to the given target
        (which can be a C{VariableDoc}, a C{ValueDoc}, or a
        C{DottedName}.
        If a C{NamespaceDoc} C{context} is specified, the target label is
        contextualized to it.
        """
        assert isinstance(target, (APIDoc, DottedName))

        # Pick a label, if none was given.
        if label is None:
            if isinstance(target, VariableDoc):
                label = target.name
            elif (isinstance(target, ValueDoc) and
                  target.canonical_name is not UNKNOWN):
                label = target.canonical_name
            elif isinstance(target, DottedName):
                label = target
            elif isinstance(target, GenericValueDoc):
                raise ValueError("href() should not be called with "
                                 "GenericValueDoc objects (perhaps you "
                                 "meant to use the containing variable?)")
            else:
                raise ValueError("Unable to find a label for %r" % target)
                
            if context is not None and isinstance(label, DottedName):
                label = label.contextualize(context.canonical_name.container())
                
            label = plaintext_to_html(str(label))
            
            # Munge names for scripts & unreachable values
            if label.startswith('script-'):
                label = label[7:] + ' (script)'
            if label.startswith('??'):
                label = '<i>unreachable</i>' + label[2:]
                label = re.sub(r'-\d+$', '', label)

        # Get the url for the target.
        url = self.url(target)
        if url is None:
            if tooltip: return '<span title="%s">%s</span>' % (tooltip, label)
            else: return label

        # Construct a string for the class attribute.
        if css_class is None:
            css = ''
        else:
            css = ' class="%s"' % css_class

        onclick = ''
        if ((isinstance(target, VariableDoc) and not target.is_public) or
            (isinstance(target, ValueDoc) and
             not isinstance(target, GenericValueDoc) and
             not self._val_is_public(target))):
            onclick = ' onclick="show_private();"'

        if tooltip:
            tooltip = ' title="%s"' % tooltip
        else:
            tooltip = ''

        return '<a href="%s"%s%s%s>%s</a>' % (url, css, onclick, tooltip, label)

    def _attr_to_html(self, attr, api_doc, indent):
        if api_doc in (None, UNKNOWN):
            return ''
        pds = getattr(api_doc, attr, None) # pds = ParsedDocstring.
        if pds not in (None, UNKNOWN):
            return self.docstring_to_html(pds, api_doc, indent)
        elif isinstance(api_doc, VariableDoc):
            return self._attr_to_html(attr, api_doc.value, indent)
        
    def summary(self, api_doc, indent=0):
        return self._attr_to_html('summary', api_doc, indent)
        
    def descr(self, api_doc, indent=0):
        return self._attr_to_html('descr', api_doc, indent)

    def type_descr(self, api_doc, indent=0):
        return self._attr_to_html('type_descr', api_doc, indent)

    def return_type(self, api_doc, indent=0):
        return self._attr_to_html('return_type', api_doc, indent)

    def return_descr(self, api_doc, indent=0):
        return self._attr_to_html('return_descr', api_doc, indent)

    def docstring_to_html(self, parsed_docstring, where=None, indent=0):
        if parsed_docstring in (None, UNKNOWN): return ''
        linker = _HTMLDocstringLinker(self, where)
        s = parsed_docstring.to_html(linker, indent=indent,
                                     directory=self._directory,
                                     docindex=self.docindex,
                                     context=where).strip()
        if self._mark_docstrings:
            s = '<span class="docstring">%s</span><!--end docstring-->' % s
        return s

    def description(self, parsed_docstring, where=None, indent=0):
        assert isinstance(where, (APIDoc, type(None)))
        if parsed_docstring in (None, UNKNOWN): return ''
        linker = _HTMLDocstringLinker(self, where)
        descr = parsed_docstring.to_html(linker, indent=indent,
                                         directory=self._directory,
                                         docindex=self.docindex,
                                         context=where).strip()
        if descr == '': return '&nbsp;'
        return descr

    # [xx] Should this be defined by the APIDoc classes themselves??
    def doc_kind(self, doc):
        if isinstance(doc, ModuleDoc) and doc.is_package == True:
            return 'Package'
        elif (isinstance(doc, ModuleDoc) and
              doc.canonical_name[0].startswith('script')):
            return 'Script'
        elif isinstance(doc, ModuleDoc):
            return 'Module'
        elif isinstance(doc, ClassDoc):
            return 'Class'
        elif isinstance(doc, ClassMethodDoc):
            return 'Class Method'
        elif isinstance(doc, StaticMethodDoc):
            return 'Static Method'
        elif isinstance(doc, RoutineDoc):
            if isinstance(self.docindex.container(doc), ClassDoc):
                return 'Method'
            else:
                return 'Function'
        else:
            return 'Variable'
        
    def _doc_or_ancestor_is_private(self, api_doc):
        name = api_doc.canonical_name
        for i in range(len(name), 0, -1):
            # Is it (or an ancestor) a private var?
            var_doc = self.docindex.get_vardoc(name[:i])
            if var_doc is not None and var_doc.is_public == False:
                return True
            # Is it (or an ancestor) a private module?
            val_doc = self.docindex.get_valdoc(name[:i])
            if (val_doc is not None and isinstance(val_doc, ModuleDoc) and
                val_doc.canonical_name[-1].startswith('_')):
                return True
        return False

    def _private_subclasses(self, class_doc):
        """Return a list of all subclasses of the given class that are
        private, as determined by L{_val_is_private}.  Recursive
        subclasses are included in this list."""
        queue = [class_doc]
        private = set()
        for cls in queue:
            if (isinstance(cls, ClassDoc) and
                cls.subclasses not in (None, UNKNOWN)):
                queue.extend(cls.subclasses)
                private.update([c for c in cls.subclasses if
                                not self._val_is_public(c)])
        return private
                
class _HTMLDocstringLinker(epydoc.markup.DocstringLinker):
    def __init__(self, htmlwriter, container):
        self.htmlwriter = htmlwriter
        self.docindex = htmlwriter.docindex
        self.container = container
        
    def translate_indexterm(self, indexterm):
        key = self.htmlwriter._term_index_to_anchor(indexterm)
        return ('<a name="%s"></a><i class="indexterm">%s</i>' %
                (key, indexterm.to_html(self)))
    
    def translate_identifier_xref(self, identifier, label=None):
        # Pick a label for this xref.
        if label is None: label = plaintext_to_html(identifier)

        # Find the APIDoc for it (if it's available).
        doc = self.docindex.find(identifier, self.container)

        # If we didn't find a target, then try checking in the contexts
        # of the ancestor classes. 
        if doc is None and isinstance(self.container, RoutineDoc):
            container = self.docindex.get_vardoc(
                self.container.canonical_name)
            while (doc is None and container not in (None, UNKNOWN)
                   and container.overrides not in (None, UNKNOWN)):
                container = container.overrides
                doc = self.docindex.find(identifier, container)
                
        # Translate it into HTML.
        if doc is None:
            self._failed_xref(identifier)
            return '<code class="link">%s</code>' % label
        else:
            return self.htmlwriter.href(doc, label, 'link')

    # [xx] Should this be added to the DocstringLinker interface???
    # Currently, this is *only* used by dotgraph.
    def url_for(self, identifier):
        if isinstance(identifier, (basestring, DottedName)):
            doc = self.docindex.find(identifier, self.container)
            if doc:
                return self.htmlwriter.url(doc)
            else:
                return None
            
        elif isinstance(identifier, APIDoc):
            return self.htmlwriter.url(identifier)
            doc = identifier
            
        else:
            raise TypeError('Expected string or APIDoc')

    def _failed_xref(self, identifier):
        """Add an identifier to the htmlwriter's failed crossreference
        list."""
        # Don't count it as a failed xref if it's a parameter of the
        # current function.
        if (isinstance(self.container, RoutineDoc) and
            identifier in self.container.all_args()):
            return
        
        failed_xrefs = self.htmlwriter._failed_xrefs
        context = self.container.canonical_name
        failed_xrefs.setdefault(identifier,{})[context] = 1

########NEW FILE########
__FILENAME__ = html_colorize
#
# epydoc.html: HTML colorizers
# Edward Loper
#
# Created [10/16/02 09:49 PM]
# $Id: html_colorize.py 1674 2008-01-29 06:03:36Z edloper $
#

"""
Functions to produce colorized HTML code for various objects.
Currently, C{html_colorize} defines functions to colorize
Python source code.
"""
__docformat__ = 'epytext en'

import re, codecs
from epydoc import log
from epydoc.util import py_src_filename
from epydoc.apidoc import *
import tokenize, token, cgi, keyword
try: from cStringIO import StringIO
except: from StringIO import StringIO

######################################################################
## Python source colorizer
######################################################################
"""
Goals:
  - colorize tokens appropriately (using css)
  - optionally add line numbers
  - 
"""

#: Javascript code for the PythonSourceColorizer
PYSRC_JAVASCRIPTS = '''\
function expand(id) {
  var elt = document.getElementById(id+"-expanded");
  if (elt) elt.style.display = "block";
  var elt = document.getElementById(id+"-expanded-linenums");
  if (elt) elt.style.display = "block";
  var elt = document.getElementById(id+"-collapsed");
  if (elt) { elt.innerHTML = ""; elt.style.display = "none"; }
  var elt = document.getElementById(id+"-collapsed-linenums");
  if (elt) { elt.innerHTML = ""; elt.style.display = "none"; }
  var elt = document.getElementById(id+"-toggle");
  if (elt) { elt.innerHTML = "-"; }
}

function collapse(id) {
  var elt = document.getElementById(id+"-expanded");
  if (elt) elt.style.display = "none";
  var elt = document.getElementById(id+"-expanded-linenums");
  if (elt) elt.style.display = "none";
  var elt = document.getElementById(id+"-collapsed-linenums");
  if (elt) { elt.innerHTML = "<br />"; elt.style.display="block"; }
  var elt = document.getElementById(id+"-toggle");
  if (elt) { elt.innerHTML = "+"; }
  var elt = document.getElementById(id+"-collapsed");
  if (elt) {
    elt.style.display = "block";
    
    var indent = elt.getAttribute("indent");
    var pad = elt.getAttribute("pad");
    var s = "<tt class=\'py-lineno\'>";
    for (var i=0; i<pad.length; i++) { s += "&nbsp;" }
    s += "</tt>";
    s += "&nbsp;&nbsp;<tt class=\'py-line\'>";
    for (var i=0; i<indent.length; i++) { s += "&nbsp;" }
    s += "<a href=\'#\' onclick=\'expand(\\"" + id;
    s += "\\");return false\'>...</a></tt><br />";
    elt.innerHTML = s;
  }
}

function toggle(id) {
  elt = document.getElementById(id+"-toggle");
  if (elt.innerHTML == "-")
      collapse(id); 
  else
      expand(id);
  return false;
}

function highlight(id) {
  var elt = document.getElementById(id+"-def");
  if (elt) elt.className = "py-highlight-hdr";
  var elt = document.getElementById(id+"-expanded");
  if (elt) elt.className = "py-highlight";
  var elt = document.getElementById(id+"-collapsed");
  if (elt) elt.className = "py-highlight";
}

function num_lines(s) {
  var n = 1;
  var pos = s.indexOf("\\n");
  while ( pos > 0) {
    n += 1;
    pos = s.indexOf("\\n", pos+1);
  }
  return n;
}

// Collapse all blocks that mave more than `min_lines` lines.
function collapse_all(min_lines) {
  var elts = document.getElementsByTagName("div");
  for (var i=0; i<elts.length; i++) {
    var elt = elts[i];
    var split = elt.id.indexOf("-");
    if (split > 0)
      if (elt.id.substring(split, elt.id.length) == "-expanded")
        if (num_lines(elt.innerHTML) > min_lines)
          collapse(elt.id.substring(0, split));
  }
}

function expandto(href) {
  var start = href.indexOf("#")+1;
  if (start != 0 && start != href.length) {
    if (href.substring(start, href.length) != "-") {
      collapse_all(4);
      pos = href.indexOf(".", start);
      while (pos != -1) {
        var id = href.substring(start, pos);
        expand(id);
        pos = href.indexOf(".", pos+1);
      }
      var id = href.substring(start, href.length);
      expand(id);
      highlight(id);
    }
  }
}

function kill_doclink(id) {
  var parent = document.getElementById(id);
  parent.removeChild(parent.childNodes.item(0));
}
function auto_kill_doclink(ev) {
  if (!ev) var ev = window.event;
  if (!this.contains(ev.toElement)) {
    var parent = document.getElementById(this.parentID);
    parent.removeChild(parent.childNodes.item(0));
  }
}

function doclink(id, name, targets_id) {
  var elt = document.getElementById(id);

  // If we already opened the box, then destroy it.
  // (This case should never occur, but leave it in just in case.)
  if (elt.childNodes.length > 1) {
    elt.removeChild(elt.childNodes.item(0));
  }
  else {
    // The outer box: relative + inline positioning.
    var box1 = document.createElement("div");
    box1.style.position = "relative";
    box1.style.display = "inline";
    box1.style.top = 0;
    box1.style.left = 0;
  
    // A shadow for fun
    var shadow = document.createElement("div");
    shadow.style.position = "absolute";
    shadow.style.left = "-1.3em";
    shadow.style.top = "-1.3em";
    shadow.style.background = "#404040";
    
    // The inner box: absolute positioning.
    var box2 = document.createElement("div");
    box2.style.position = "relative";
    box2.style.border = "1px solid #a0a0a0";
    box2.style.left = "-.2em";
    box2.style.top = "-.2em";
    box2.style.background = "white";
    box2.style.padding = ".3em .4em .3em .4em";
    box2.style.fontStyle = "normal";
    box2.onmouseout=auto_kill_doclink;
    box2.parentID = id;

    // Get the targets
    var targets_elt = document.getElementById(targets_id);
    var targets = targets_elt.getAttribute("targets");
    var links = "";
    target_list = targets.split(",");
    for (var i=0; i<target_list.length; i++) {
        var target = target_list[i].split("=");
        links += "<li><a href=\'" + target[1] + 
               "\' style=\'text-decoration:none\'>" +
               target[0] + "</a></li>";
    }
  
    // Put it all together.
    elt.insertBefore(box1, elt.childNodes.item(0));
    //box1.appendChild(box2);
    box1.appendChild(shadow);
    shadow.appendChild(box2);
    box2.innerHTML =
        "Which <b>"+name+"</b> do you want to see documentation for?" +
        "<ul style=\'margin-bottom: 0;\'>" +
        links + 
        "<li><a href=\'#\' style=\'text-decoration:none\' " +
        "onclick=\'kill_doclink(\\""+id+"\\");return false;\'>"+
        "<i>None of the above</i></a></li></ul>";
  }
  return false;
}
'''

PYSRC_EXPANDTO_JAVASCRIPT = '''\
<script type="text/javascript">
<!--
expandto(location.href);
// -->
</script>
'''

class PythonSourceColorizer:
    """
    A class that renders a python module's source code into HTML
    pages.  These HTML pages are intended to be provided along with
    the API documentation for a module, in case a user wants to learn
    more about a particular object by examining its source code.
    Links are therefore generated from the API documentation to the
    source code pages, and from the source code pages back into the
    API documentation.

    The HTML generated by C{PythonSourceColorizer} has several notable
    features:

      - CSS styles are used to color tokens according to their type.
        (See L{CSS_CLASSES} for a list of the different token types
        that are identified).
        
      - Line numbers are included to the left of each line.

      - The first line of each class and function definition includes
        a link to the API source documentation for that object.

      - The first line of each class and function definition includes
        an anchor that can be used to link directly to that class or
        function.

      - If javascript is enabled, and the page is loaded using the
        anchor for a class or function (i.e., if the url ends in
        C{'#I{<name>}'}), then that class or function will automatically
        be highlighted; and all other classes and function definition
        blocks will be 'collapsed'.  These collapsed blocks can be
        expanded by clicking on them.

      - Unicode input is supported (including automatic detection
        of C{'coding:'} declarations).

    """
    #: A look-up table that is used to determine which CSS class
    #: should be used to colorize a given token.  The following keys
    #: may be used:
    #:   - Any token name (e.g., C{'STRING'})
    #:   - Any operator token (e.g., C{'='} or C{'@'}).
    #:   - C{'KEYWORD'} -- Python keywords such as C{'for'} and C{'if'}
    #:   - C{'DEFNAME'} -- the name of a class or function at the top
    #:     of its definition statement.
    #:   - C{'BASECLASS'} -- names of base classes at the top of a class
    #:     definition statement.
    #:   - C{'PARAM'} -- function parameters
    #:   - C{'DOCSTRING'} -- docstrings
    #:   - C{'DECORATOR'} -- decorator names
    #: If no CSS class can be found for a given token, then it won't
    #: be marked with any CSS class.
    CSS_CLASSES = {
        'NUMBER':       'py-number',
        'STRING':       'py-string',
        'COMMENT':      'py-comment',
        'NAME':         'py-name',
        'KEYWORD':      'py-keyword',
        'DEFNAME':      'py-def-name',
        'BASECLASS':    'py-base-class',
        'PARAM':        'py-param',
        'DOCSTRING':    'py-docstring',
        'DECORATOR':    'py-decorator',
        'OP':           'py-op',
        '@':            'py-decorator',
        }

    #: HTML code for the beginning of a collapsable function or class
    #: definition block.  The block contains two <div>...</div>
    #: elements -- a collapsed version and an expanded version -- and
    #: only one of these elements is visible at any given time.  By
    #: default, all definition blocks are expanded.
    #:
    #: This string should be interpolated with the following values::
    #:   (name, indentation, name)
    #: Where C{name} is the anchor name for the function or class; and
    #: indentation is a string of whitespace used to indent the
    #: ellipsis marker in the collapsed version.
    START_DEF_BLOCK = (
        '<div id="%s-collapsed" style="display:none;" '
        'pad="%s" indent="%s"></div>'
        '<div id="%s-expanded">')

    #: HTML code for the end of a collapsable function or class
    #: definition block.
    END_DEF_BLOCK = '</div>'

    #: A regular expression used to pick out the unicode encoding for
    #: the source file.
    UNICODE_CODING_RE = re.compile(r'.*?\n?.*?coding[:=]\s*([-\w.]+)')

    #: A configuration constant, used to determine whether or not to add
    #: collapsable <div> elements for definition blocks.
    ADD_DEF_BLOCKS = True

    #: A configuration constant, used to determine whether or not to
    #: add line numbers.
    ADD_LINE_NUMBERS = True

    #: A configuration constant, used to determine whether or not to
    #: add tooltips for linked names.
    ADD_TOOLTIPS = True

    #: If true, then try to guess which target is appropriate for
    #: linked names; if false, then always open a div asking the
    #: user which one they want.
    GUESS_LINK_TARGETS = False

    def __init__(self, module_filename, module_name,
                 docindex=None, url_func=None, name_to_docs=None,
                 tab_width=8):
        """
        Create a new HTML colorizer for the specified module.

        @param module_filename: The name of the file containing the
            module; its text will be loaded from this file.
        @param module_name: The dotted name of the module; this will
            be used to create links back into the API source
            documentation.
        """
        # Get the source version, if possible.
        try: module_filename = py_src_filename(module_filename)
        except: pass
        
        #: The filename of the module we're colorizing.
        self.module_filename = module_filename
        
        #: The dotted name of the module we're colorizing.
        self.module_name = module_name

        #: A docindex, used to create href links from identifiers to
        #: the API documentation for their values.
        self.docindex = docindex

        #: A mapping from short names to lists of ValueDoc, used to
        #: decide which values an identifier might map to when creating
        #: href links from identifiers to the API docs for their values.
        self.name_to_docs = name_to_docs
            
        #: A function that maps APIDoc -> URL, used to create href
        #: links from identifiers to the API documentation for their
        #: values.
        self.url_func = url_func

        #: The index in C{text} of the last character of the last
        #: token we've processed.
        self.pos = 0

        #: A list that maps line numbers to character offsets in
        #: C{text}.  In particular, line C{M{i}} begins at character
        #: C{line_offset[i]} in C{text}.  Since line numbers begin at
        #: 1, the first element of C{line_offsets} is C{None}.
        self.line_offsets = []

        #: A list of C{(toktype, toktext)} for all tokens on the
        #: logical line that we are currently processing.  Once a
        #: complete line of tokens has been collected in C{cur_line},
        #: it is sent to L{handle_line} for processing.
        self.cur_line = []

        #: A list of the names of the class or functions that include
        #: the current block.  C{context} has one element for each
        #: level of indentation; C{context[i]} is the name of the class
        #: or function defined by the C{i}th level of indentation, or
        #: C{None} if that level of indentation doesn't correspond to a
        #: class or function definition.
        self.context = []

        #: A list, corresponding one-to-one with L{self.context},
        #: indicating the type of each entry.  Each element of
        #: C{context_types} is one of: C{'func'}, C{'class'}, C{None}.
        self.context_types = []

        #: A list of indentation strings for each of the current
        #: block's indents.  I.e., the current total indentation can
        #: be found by taking C{''.join(self.indents)}.
        self.indents = []

        #: The line number of the line we're currently processing.
        self.lineno = 0

        #: The name of the class or function whose definition started
        #: on the previous logical line, or C{None} if the previous
        #: logical line was not a class or function definition.
        self.def_name = None

        #: The type of the class or function whose definition started
        #: on the previous logical line, or C{None} if the previous
        #: logical line was not a class or function definition.
        #: Can be C{'func'}, C{'class'}, C{None}.
        self.def_type = None

        #: The number of spaces to replace each tab in source code with
        self.tab_width = tab_width

        
    def find_line_offsets(self):
        """
        Construct the L{line_offsets} table from C{self.text}.
        """
        # line 0 doesn't exist; line 1 starts at char offset 0.
        self.line_offsets = [None, 0]
        # Find all newlines in `text`, and add an entry to
        # line_offsets for each one.
        pos = self.text.find('\n')
        while pos != -1:
            self.line_offsets.append(pos+1)
            pos = self.text.find('\n', pos+1)
        # Add a final entry, marking the end of the string.
        self.line_offsets.append(len(self.text))

    def lineno_to_html(self):
        template = '%%%ds' % self.linenum_size
        n = template % self.lineno
        return '<a name="L%s"></a><tt class="py-lineno">%s</tt>' \
            % (self.lineno, n)

    def colorize(self):
        """
        Return an HTML string that renders the source code for the
        module that was specified in the constructor.
        """
        # Initialize all our state variables
        self.pos = 0
        self.cur_line = []
        self.context = []
        self.context_types = []
        self.indents = []
        self.lineno = 1
        self.def_name = None
        self.def_type = None
        self.has_decorators = False

        # Cache, used so we only need to list the target elements once
        # for each variable.
        self.doclink_targets_cache = {}

        # Load the module's text.
        self.text = open(self.module_filename).read()
        self.text = self.text.expandtabs(self.tab_width).rstrip()+'\n'

        # Construct the line_offsets table.
        self.find_line_offsets()

        num_lines = self.text.count('\n')+1
        self.linenum_size = len(`num_lines+1`)
        
        # Call the tokenizer, and send tokens to our `tokeneater()`
        # method.  If anything goes wrong, then fall-back to using
        # the input text as-is (with no colorization).
        try:
            output = StringIO()
            self.out = output.write
            tokenize.tokenize(StringIO(self.text).readline, self.tokeneater)
            html = output.getvalue()
            if self.has_decorators:
                html = self._FIX_DECORATOR_RE.sub(r'\2\1', html)
        except tokenize.TokenError, ex:
            html = self.text

        # Check for a unicode encoding declaration.
        m = self.UNICODE_CODING_RE.match(self.text)
        if m: coding = m.group(1)
        else: coding = 'iso-8859-1'

        # Decode the html string into unicode, and then encode it back
        # into ascii, replacing any non-ascii characters with xml
        # character references.
        try:
            html = html.decode(coding).encode('ascii', 'xmlcharrefreplace')
        except LookupError:
            coding = 'iso-8859-1'
            html = html.decode(coding).encode('ascii', 'xmlcharrefreplace')

        # Call expandto.
        html += PYSRC_EXPANDTO_JAVASCRIPT

        return html

    def tokeneater(self, toktype, toktext, (srow,scol), (erow,ecol), line):
        """
        A callback function used by C{tokenize.tokenize} to handle
        each token in the module.  C{tokeneater} collects tokens into
        the C{self.cur_line} list until a complete logical line has
        been formed; and then calls L{handle_line} to process that line.
        """
        # If we encounter any errors, then just give up.
        if toktype == token.ERRORTOKEN:
            raise tokenize.TokenError, toktype

        # Did we skip anything whitespace?  If so, add a pseudotoken
        # for it, with toktype=None.  (Note -- this skipped string
        # might also contain continuation slashes; but I won't bother
        # to colorize them.)
        startpos = self.line_offsets[srow] + scol
        if startpos > self.pos:
            skipped = self.text[self.pos:startpos]
            self.cur_line.append( (None, skipped) )

        # Update our position.
        self.pos = startpos + len(toktext)

        # Update our current line.
        self.cur_line.append( (toktype, toktext) )

        # When we reach the end of a line, process it.
        if toktype == token.NEWLINE or toktype == token.ENDMARKER:
            self.handle_line(self.cur_line)
            self.cur_line = []

    _next_uid = 0

    # [xx] note -- this works with byte strings, not unicode strings!
    # I may change it to use unicode eventually, but when I do it
    # needs to be changed all at once.
    def handle_line(self, line):
        """
        Render a single logical line from the module, and write the
        generated HTML to C{self.out}.

        @param line: A single logical line, encoded as a list of
            C{(toktype,tokttext)} pairs corresponding to the tokens in
            the line.
        """
        # def_name is the name of the function or class defined by
        # this line; or None if no funciton or class is defined.
        def_name = None

        # def_type is the type of the function or class defined by
        # this line; or None if no funciton or class is defined.
        def_type = None

        # does this line start a class/func def?
        starting_def_block = False 

        in_base_list = False
        in_param_list = False
        in_param_default = 0
        at_module_top = (self.lineno == 1)

        ended_def_blocks = 0

        # The html output.
        if self.ADD_LINE_NUMBERS:
            s = self.lineno_to_html()
            self.lineno += 1
        else:
            s = ''
        s += '  <tt class="py-line">'

        # Loop through each token, and colorize it appropriately.
        for i, (toktype, toktext) in enumerate(line):
            if type(s) is not str:
                if type(s) is unicode:
                    log.error('While colorizing %s -- got unexpected '
                              'unicode string' % self.module_name)
                    s = s.encode('ascii', 'xmlcharrefreplace')
                else:
                    raise ValueError('Unexpected value for s -- %s' % 
                                     type(s).__name__)

            # For each token, determine its css class and whether it
            # should link to a url.
            css_class = None
            url = None
            tooltip = None
            onclick = uid = targets = None # these 3 are used together.

            # Is this token the class name in a class definition?  If
            # so, then make it a link back into the API docs.
            if i>=2 and line[i-2][1] == 'class':
                in_base_list = True
                css_class = self.CSS_CLASSES['DEFNAME']
                def_name = toktext
                def_type = 'class'
                if 'func' not in self.context_types:
                    cls_name = self.context_name(def_name)
                    url = self.name2url(cls_name)
                    s = self.mark_def(s, cls_name)
                    starting_def_block = True

            # Is this token the function name in a function def?  If
            # so, then make it a link back into the API docs.
            elif i>=2 and line[i-2][1] == 'def':
                in_param_list = True
                css_class = self.CSS_CLASSES['DEFNAME']
                def_name = toktext
                def_type = 'func'
                if 'func' not in self.context_types:
                    cls_name = self.context_name()
                    func_name = self.context_name(def_name)
                    url = self.name2url(cls_name, def_name)
                    s = self.mark_def(s, func_name)
                    starting_def_block = True

            # For each indent, update the indents list (which we use
            # to keep track of indentation strings) and the context
            # list.  If this indent is the start of a class or
            # function def block, then self.def_name will be its name;
            # otherwise, it will be None.
            elif toktype == token.INDENT:
                self.indents.append(toktext)
                self.context.append(self.def_name)
                self.context_types.append(self.def_type)

            # When we dedent, pop the last elements off the indents
            # list and the context list.  If the last context element
            # is a name, then we're ending a class or function def
            # block; so write an end-div tag.
            elif toktype == token.DEDENT:
                self.indents.pop()
                self.context_types.pop()
                if self.context.pop():
                    ended_def_blocks += 1

            # If this token contains whitespace, then don't bother to
            # give it a css tag.
            elif toktype in (None, tokenize.NL, token.NEWLINE,
                             token.ENDMARKER):
                css_class = None

            # Check if the token is a keyword.
            elif toktype == token.NAME and keyword.iskeyword(toktext):
                css_class = self.CSS_CLASSES['KEYWORD']

            elif in_base_list and toktype == token.NAME:
                css_class = self.CSS_CLASSES['BASECLASS']

            elif (in_param_list and toktype == token.NAME and
                  not in_param_default):
                css_class = self.CSS_CLASSES['PARAM']

            # Class/function docstring.
            elif (self.def_name and line[i-1][0] == token.INDENT and
                  self.is_docstring(line, i)):
                css_class = self.CSS_CLASSES['DOCSTRING']

            # Module docstring.
            elif at_module_top and self.is_docstring(line, i):
                css_class = self.CSS_CLASSES['DOCSTRING']

            # check for decorators??
            elif (toktype == token.NAME and
                  ((i>0 and line[i-1][1]=='@') or
                   (i>1 and line[i-1][0]==None and line[i-2][1] == '@'))):
                css_class = self.CSS_CLASSES['DECORATOR']
                self.has_decorators = True

            # If it's a name, try to link it.
            elif toktype == token.NAME:
                css_class = self.CSS_CLASSES['NAME']
                # If we have a variable named `toktext` in the current
                # context, then link to that.  Note that if we're inside
                # a function, then that function is our context, not
                # the namespace that contains it. [xx] this isn't always
                # the right thing to do.
                if (self.GUESS_LINK_TARGETS and self.docindex is not None
                    and self.url_func is not None):
                    context = [n for n in self.context if n is not None]
                    container = self.docindex.get_vardoc(
                        DottedName(self.module_name, *context))
                    if isinstance(container, NamespaceDoc):
                        doc = container.variables.get(toktext)
                        if doc is not None:
                            url = self.url_func(doc)
                            tooltip = str(doc.canonical_name)
                # Otherwise, check the name_to_docs index to see what
                # else this name might refer to.
                if (url is None and self.name_to_docs is not None
                    and self.url_func is not None):
                    docs = self.name_to_docs.get(toktext)
                    if docs:
                        tooltip='\n'.join([str(d.canonical_name)
                                           for d in docs])
                        if len(docs) == 1 and self.GUESS_LINK_TARGETS:
                            url = self.url_func(docs[0])
                        else:
                            uid, onclick, targets = self.doclink(toktext, docs)

            # For all other tokens, look up the CSS class to use
            # based on the token's type.
            else:
                if toktype == token.OP and toktext in self.CSS_CLASSES:
                    css_class = self.CSS_CLASSES[toktext]
                elif token.tok_name[toktype] in self.CSS_CLASSES:
                    css_class = self.CSS_CLASSES[token.tok_name[toktype]]
                else:
                    css_class = None

            # update our status..
            if toktext == ':':
                in_base_list = False
                in_param_list = False
            if toktext == '=' and in_param_list:
                in_param_default = True
            if in_param_default:
                if toktext in ('(','[','{'): in_param_default += 1
                if toktext in (')',']','}'): in_param_default -= 1
                if toktext == ',' and in_param_default == 1:
                    in_param_default = 0
                
            # Write this token, with appropriate colorization.
            if tooltip and self.ADD_TOOLTIPS:
                tooltip_html = ' title="%s"' % tooltip
            else: tooltip_html = ''
            if css_class: css_class_html = ' class="%s"' % css_class
            else: css_class_html = ''
            if onclick:
                if targets: targets_html = ' targets="%s"' % targets
                else: targets_html = ''
                s += ('<tt id="%s"%s%s><a%s%s href="#" onclick="%s">' %
                      (uid, css_class_html, targets_html, tooltip_html,
                       css_class_html, onclick))
            elif url:
                if isinstance(url, unicode):
                    url = url.encode('ascii', 'xmlcharrefreplace')
                s += ('<a%s%s href="%s">' %
                      (tooltip_html, css_class_html, url))
            elif css_class_html or tooltip_html:
                s += '<tt%s%s>' % (tooltip_html, css_class_html)
            if i == len(line)-1:
                s += ' </tt>' # Closes <tt class="py-line">
                s += cgi.escape(toktext)
            else:
                try:
                    s += self.add_line_numbers(cgi.escape(toktext), css_class)
                except Exception, e:
                    print (toktext, css_class, toktext.encode('ascii'))
                    raise

            if onclick: s += "</a></tt>"
            elif url: s += '</a>'
            elif css_class_html or tooltip_html: s += '</tt>'

        if self.ADD_DEF_BLOCKS:
            for i in range(ended_def_blocks):
                self.out(self.END_DEF_BLOCK)

        # Strip any empty <tt>s.
        s = re.sub(r'<tt class="[\w+]"></tt>', '', s)

        # Write the line.
        self.out(s)

        if def_name and starting_def_block:
            self.out('</div>')

        # Add div's if we're starting a def block.
        if (self.ADD_DEF_BLOCKS and def_name and starting_def_block and
            (line[-2][1] == ':')):
            indentation = (''.join(self.indents)+'    ').replace(' ', '+')
            linenum_padding = '+'*self.linenum_size
            name=self.context_name(def_name)
            self.out(self.START_DEF_BLOCK % (name, linenum_padding,
                                             indentation, name))
            
        self.def_name = def_name
        self.def_type = def_type

    def context_name(self, extra=None):
        pieces = [n for n in self.context if n is not None]
        if extra is not None: pieces.append(extra)
        return '.'.join(pieces)

    def doclink(self, name, docs):
        uid = 'link-%s' % self._next_uid
        self._next_uid += 1
        context = [n for n in self.context if n is not None]
        container = DottedName(self.module_name, *context)
        #else:
        #    container = None
        targets = ','.join(['%s=%s' % (str(self.doc_descr(d,container)),
                              str(self.url_func(d)))
                            for d in docs])

        if targets in self.doclink_targets_cache:
            onclick = ("return doclink('%s', '%s', '%s');" %
                       (uid, name, self.doclink_targets_cache[targets]))
            return uid, onclick, None
        else:
            self.doclink_targets_cache[targets] = uid
            onclick = ("return doclink('%s', '%s', '%s');" %
                       (uid, name, uid))
            return uid, onclick, targets
            
    def doc_descr(self, doc, context):
        name = str(doc.canonical_name)
        descr = '%s %s' % (self.doc_kind(doc), name)
        if isinstance(doc, RoutineDoc):
            descr += '()'
        return descr

    # [XX] copied streight from html.py; this should be consolidated,
    # probably into apidoc.
    def doc_kind(self, doc):
        if isinstance(doc, ModuleDoc) and doc.is_package == True:
            return 'Package'
        elif (isinstance(doc, ModuleDoc) and
              doc.canonical_name[0].startswith('script')):
            return 'Script'
        elif isinstance(doc, ModuleDoc):
            return 'Module'
        elif isinstance(doc, ClassDoc):
            return 'Class'
        elif isinstance(doc, ClassMethodDoc):
            return 'Class Method'
        elif isinstance(doc, StaticMethodDoc):
            return 'Static Method'
        elif isinstance(doc, RoutineDoc):
            if (self.docindex is not None and
                isinstance(self.docindex.container(doc), ClassDoc)):
                return 'Method'
            else:
                return 'Function'
        else:
            return 'Variable'

    def mark_def(self, s, name):
        replacement = ('<a name="%s"></a><div id="%s-def">\\1'
                       '<a class="py-toggle" href="#" id="%s-toggle" '
                       'onclick="return toggle(\'%s\');">-</a>\\2' %
                       (name, name, name, name))
        return re.sub('(.*) (<tt class="py-line">.*)\Z', replacement, s)
                    
    def is_docstring(self, line, i):
        if line[i][0] != token.STRING: return False
        for toktype, toktext in line[i:]:
            if toktype not in (token.NEWLINE, tokenize.COMMENT,
                               tokenize.NL, token.STRING, None):
                return False
        return True
                               
    def add_line_numbers(self, s, css_class):
        result = ''
        start = 0
        end = s.find('\n')+1
        while end:
            result += s[start:end-1]
            if css_class: result += '</tt>'
            result += ' </tt>' # py-line
            result += '\n'
            if self.ADD_LINE_NUMBERS:
                result += self.lineno_to_html()
            result += '  <tt class="py-line">'
            if css_class: result += '<tt class="%s">' % css_class
            start = end
            end = s.find('\n', end)+1
            self.lineno += 1
        result += s[start:]
        return result

    def name2url(self, class_name, func_name=None):
        if class_name:
            class_name = '%s.%s' % (self.module_name, class_name)
            if func_name:
                return '%s-class.html#%s' % (class_name, func_name)
            else:
                return '%s-class.html' % class_name
        else:
            return '%s-module.html#%s' % (self.module_name, func_name)

    #: A regexp used to move the <div> that marks the beginning of a
    #: function or method to just before the decorators.
    _FIX_DECORATOR_RE = re.compile(
        r'((?:^<a name="L\d+"></a><tt class="py-lineno">\s*\d+</tt>'
        r'\s*<tt class="py-line">(?:<tt class="py-decorator">.*|\s*</tt>|'
        r'\s*<tt class="py-comment">.*)\n)+)'
        r'(<a name="\w+"></a><div id="\w+-def">)', re.MULTILINE)
    
_HDR = '''\
<?xml version="1.0" encoding="ascii"?>
        <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
                  "DTD/xhtml1-transitional.dtd">
        <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
        <head>
          <title>$title$</title>
          <link rel="stylesheet" href="epydoc.css" type="text/css" />
          <script type="text/javascript" src="epydoc.js"></script>
        </head>
        
        <body bgcolor="white" text="black" link="blue" vlink="#204080"
              alink="#204080">
'''
_FOOT = '</body></html>'
if __name__=='__main__':
    #s = PythonSourceColorizer('../apidoc.py', 'epydoc.apidoc').colorize()
    s = PythonSourceColorizer('/tmp/fo.py', 'epydoc.apidoc').colorize()
    #print s
    import codecs
    f = codecs.open('/home/edloper/public_html/color3.html', 'w', 'ascii', 'xmlcharrefreplace')
    f.write(_HDR+'<pre id="py-src-top" class="py-src">'+s+'</pre>'+_FOOT)
    f.close()

########NEW FILE########
__FILENAME__ = html_css
#
# epydoc.css: default epydoc CSS stylesheets
# Edward Loper
#
# Created [01/30/01 05:18 PM]
# $Id: html_css.py 1634 2007-09-24 15:58:38Z dvarrazzo $
#

"""
Predefined CSS stylesheets for the HTML outputter (L{epydoc.docwriter.html}).

@type STYLESHEETS: C{dictionary} from C{string} to C{(string, string)}
@var STYLESHEETS: A dictionary mapping from stylesheet names to CSS
    stylesheets and descriptions.  A single stylesheet may have
    multiple names.  Currently, the following stylesheets are defined:
      - C{default}: The default stylesheet (synonym for C{white}).
      - C{white}: Black on white, with blue highlights (similar to
        javadoc).
      - C{blue}: Black on steel blue.
      - C{green}: Black on green.
      - C{black}: White on black, with blue highlights
      - C{grayscale}: Grayscale black on white.
      - C{none}: An empty stylesheet.
"""
__docformat__ = 'epytext en'

import re

############################################################
## Basic stylesheets
############################################################

# [xx] Should I do something like:
#
#    @import url(html4css1.css);
#
# But then where do I get that css file from?  Hm.
# Also, in principle I'm mangling classes, but it looks like I'm
# failing.
#

# Black on white, with blue highlights.  This is similar to how
# javadoc looks.
TEMPLATE = """

/* Epydoc CSS Stylesheet
 *
 * This stylesheet can be used to customize the appearance of epydoc's
 * HTML output.
 *
 */

/* Default Colors & Styles
 *   - Set the default foreground & background color with 'body'; and 
 *     link colors with 'a:link' and 'a:visited'.
 *   - Use bold for decision list terms.
 *   - The heading styles defined here are used for headings *within*
 *     docstring descriptions.  All headings used by epydoc itself use
 *     either class='epydoc' or class='toc' (CSS styles for both
 *     defined below).
 */
body                        { background: $body_bg; color: $body_fg; }
p                           { margin-top: 0.5em; margin-bottom: 0.5em; }
a:link                      { color: $body_link; }
a:visited                   { color: $body_visited_link; }
dt                          { font-weight: bold; }
h1                          { font-size: +140%; font-style: italic;
                              font-weight: bold; }
h2                          { font-size: +125%; font-style: italic;
                              font-weight: bold; }
h3                          { font-size: +110%; font-style: italic;
                              font-weight: normal; }
code                        { font-size: 100%; }
/* N.B.: class, not pseudoclass */
a.link                      { font-family: monospace; }
 
/* Page Header & Footer
 *   - The standard page header consists of a navigation bar (with
 *     pointers to standard pages such as 'home' and 'trees'); a
 *     breadcrumbs list, which can be used to navigate to containing
 *     classes or modules; options links, to show/hide private
 *     variables and to show/hide frames; and a page title (using
 *     <h1>).  The page title may be followed by a link to the
 *     corresponding source code (using 'span.codelink').
 *   - The footer consists of a navigation bar, a timestamp, and a
 *     pointer to epydoc's homepage.
 */ 
h1.epydoc                   { margin: 0; font-size: +140%; font-weight: bold; }
h2.epydoc                   { font-size: +130%; font-weight: bold; }
h3.epydoc                   { font-size: +115%; font-weight: bold;
                              margin-top: 0.2em; }
td h3.epydoc                { font-size: +115%; font-weight: bold;
                              margin-bottom: 0; }
table.navbar                { background: $navbar_bg; color: $navbar_fg;
                              border: $navbar_border; }
table.navbar table          { color: $navbar_fg; }
th.navbar-select            { background: $navbar_select_bg;
                              color: $navbar_select_fg; } 
table.navbar a              { text-decoration: none; }  
table.navbar a:link         { color: $navbar_link; }
table.navbar a:visited      { color: $navbar_visited_link; }
span.breadcrumbs            { font-size: 85%; font-weight: bold; }
span.options                { font-size: 70%; }
span.codelink               { font-size: 85%; }
td.footer                   { font-size: 85%; }

/* Table Headers
 *   - Each summary table and details section begins with a 'header'
 *     row.  This row contains a section title (marked by
 *     'span.table-header') as well as a show/hide private link
 *     (marked by 'span.options', defined above).
 *   - Summary tables that contain user-defined groups mark those
 *     groups using 'group header' rows.
 */
td.table-header             { background: $table_hdr_bg; color: $table_hdr_fg;
                              border: $table_border; }
td.table-header table       { color: $table_hdr_fg; }
td.table-header table a:link      { color: $table_hdr_link; }
td.table-header table a:visited   { color: $table_hdr_visited_link; }
span.table-header           { font-size: 120%; font-weight: bold; }
th.group-header             { background: $group_hdr_bg; color: $group_hdr_fg;
                              text-align: left; font-style: italic; 
                              font-size: 115%; 
                              border: $table_border; }

/* Summary Tables (functions, variables, etc)
 *   - Each object is described by a single row of the table with
 *     two cells.  The left cell gives the object's type, and is
 *     marked with 'code.summary-type'.  The right cell gives the
 *     object's name and a summary description.
 *   - CSS styles for the table's header and group headers are
 *     defined above, under 'Table Headers'
 */
table.summary               { border-collapse: collapse;
                              background: $table_bg; color: $table_fg;
                              border: $table_border;
                              margin-bottom: 0.5em; }
td.summary                  { border: $table_border; }
code.summary-type           { font-size: 85%; }
table.summary a:link        { color: $table_link; }
table.summary a:visited     { color: $table_visited_link; }


/* Details Tables (functions, variables, etc)
 *   - Each object is described in its own div.
 *   - A single-row summary table w/ table-header is used as
 *     a header for each details section (CSS style for table-header
 *     is defined above, under 'Table Headers').
 */
table.details               { border-collapse: collapse;
                              background: $table_bg; color: $table_fg;
                              border: $table_border;
                              margin: .2em 0 0 0; }
table.details table         { color: $table_fg; }
table.details a:link        { color: $table_link; }
table.details a:visited     { color: $table_visited_link; }

/* Fields */
dl.fields                   { margin-left: 2em; margin-top: 1em;
                              margin-bottom: 1em; }
dl.fields dd ul             { margin-left: 0em; padding-left: 0em; }
dl.fields dd ul li ul       { margin-left: 2em; padding-left: 0em; }
div.fields                  { margin-left: 2em; }
div.fields p                { margin-bottom: 0.5em; }

/* Index tables (identifier index, term index, etc)
 *   - link-index is used for indices containing lists of links
 *     (namely, the identifier index & term index).
 *   - index-where is used in link indices for the text indicating
 *     the container/source for each link.
 *   - metadata-index is used for indices containing metadata
 *     extracted from fields (namely, the bug index & todo index).
 */
table.link-index            { border-collapse: collapse;
                              background: $table_bg; color: $table_fg;
                              border: $table_border; }
td.link-index               { border-width: 0px; }
table.link-index a:link     { color: $table_link; }
table.link-index a:visited  { color: $table_visited_link; }
span.index-where            { font-size: 70%; }
table.metadata-index        { border-collapse: collapse;
                              background: $table_bg; color: $table_fg;
                              border: $table_border; 
                              margin: .2em 0 0 0; }
td.metadata-index           { border-width: 1px; border-style: solid; }
table.metadata-index a:link { color: $table_link; }
table.metadata-index a:visited  { color: $table_visited_link; }

/* Function signatures
 *   - sig* is used for the signature in the details section.
 *   - .summary-sig* is used for the signature in the summary 
 *     table, and when listing property accessor functions.
 * */
.sig-name                   { color: $sig_name; }
.sig-arg                    { color: $sig_arg; }
.sig-default                { color: $sig_default; }
.summary-sig                { font-family: monospace; }
.summary-sig-name           { color: $summary_sig_name; font-weight: bold; }
table.summary a.summary-sig-name:link
                            { color: $summary_sig_name; font-weight: bold; }
table.summary a.summary-sig-name:visited
                            { color: $summary_sig_name; font-weight: bold; }
.summary-sig-arg            { color: $summary_sig_arg; }
.summary-sig-default        { color: $summary_sig_default; }

/* Subclass list
 */
ul.subclass-list { display: inline; }
ul.subclass-list li { display: inline; }

/* To render variables, classes etc. like functions */
table.summary .summary-name { color: $summary_sig_name; font-weight: bold;
                              font-family: monospace; }
table.summary
     a.summary-name:link    { color: $summary_sig_name; font-weight: bold;
                              font-family: monospace; }
table.summary
    a.summary-name:visited  { color: $summary_sig_name; font-weight: bold;
                              font-family: monospace; }

/* Variable values
 *   - In the 'variable details' sections, each varaible's value is
 *     listed in a 'pre.variable' box.  The width of this box is
 *     restricted to 80 chars; if the value's repr is longer than
 *     this it will be wrapped, using a backslash marked with
 *     class 'variable-linewrap'.  If the value's repr is longer
 *     than 3 lines, the rest will be ellided; and an ellipsis
 *     marker ('...' marked with 'variable-ellipsis') will be used.
 *   - If the value is a string, its quote marks will be marked
 *     with 'variable-quote'.
 *   - If the variable is a regexp, it is syntax-highlighted using
 *     the re* CSS classes.
 */
pre.variable                { padding: .5em; margin: 0;
                              background: $variable_bg; color: $variable_fg;
                              border: $variable_border; }
.variable-linewrap          { color: $variable_linewrap; font-weight: bold; }
.variable-ellipsis          { color: $variable_ellipsis; font-weight: bold; }
.variable-quote             { color: $variable_quote; font-weight: bold; }
.variable-group             { color: $variable_group; font-weight: bold; }
.variable-op                { color: $variable_op; font-weight: bold; }
.variable-string            { color: $variable_string; }
.variable-unknown           { color: $variable_unknown; font-weight: bold; }
.re                         { color: $re; }
.re-char                    { color: $re_char; }
.re-op                      { color: $re_op; }
.re-group                   { color: $re_group; }
.re-ref                     { color: $re_ref; }

/* Base tree
 *   - Used by class pages to display the base class hierarchy.
 */
pre.base-tree               { font-size: 80%; margin: 0; }

/* Frames-based table of contents headers
 *   - Consists of two frames: one for selecting modules; and
 *     the other listing the contents of the selected module.
 *   - h1.toc is used for each frame's heading
 *   - h2.toc is used for subheadings within each frame.
 */
h1.toc                      { text-align: center; font-size: 105%;
                              margin: 0; font-weight: bold;
                              padding: 0; }
h2.toc                      { font-size: 100%; font-weight: bold; 
                              margin: 0.5em 0 0 -0.3em; }

/* Syntax Highlighting for Source Code
 *   - doctest examples are displayed in a 'pre.py-doctest' block.
 *     If the example is in a details table entry, then it will use
 *     the colors specified by the 'table pre.py-doctest' line.
 *   - Source code listings are displayed in a 'pre.py-src' block.
 *     Each line is marked with 'span.py-line' (used to draw a line
 *     down the left margin, separating the code from the line
 *     numbers).  Line numbers are displayed with 'span.py-lineno'.
 *     The expand/collapse block toggle button is displayed with
 *     'a.py-toggle' (Note: the CSS style for 'a.py-toggle' should not
 *     modify the font size of the text.)
 *   - If a source code page is opened with an anchor, then the
 *     corresponding code block will be highlighted.  The code
 *     block's header is highlighted with 'py-highlight-hdr'; and
 *     the code block's body is highlighted with 'py-highlight'.
 *   - The remaining py-* classes are used to perform syntax
 *     highlighting (py-string for string literals, py-name for names,
 *     etc.)
 */
pre.py-doctest              { padding: .5em; margin: 1em;
                              background: $doctest_bg; color: $doctest_fg;
                              border: $doctest_border; }
table pre.py-doctest        { background: $doctest_in_table_bg;
                              color: $doctest_in_table_fg; }
pre.py-src                  { border: $pysrc_border; 
                              background: $pysrc_bg; color: $pysrc_fg; }
.py-line                    { border-left: $pysrc_sep_border; 
                              margin-left: .2em; padding-left: .4em; }
.py-lineno                  { font-style: italic; font-size: 90%;
                              padding-left: .5em; }
a.py-toggle                 { text-decoration: none; }
div.py-highlight-hdr        { border-top: $pysrc_border;
                              border-bottom: $pysrc_border;
                              background: $pysrc_highlight_hdr_bg; }
div.py-highlight            { border-bottom: $pysrc_border;
                              background: $pysrc_highlight_bg; }
.py-prompt                  { color: $py_prompt; font-weight: bold;}
.py-more                    { color: $py_more; font-weight: bold;}
.py-string                  { color: $py_string; }
.py-comment                 { color: $py_comment; }
.py-keyword                 { color: $py_keyword; }
.py-output                  { color: $py_output; }
.py-name                    { color: $py_name; }
.py-name:link               { color: $py_name !important; }
.py-name:visited            { color: $py_name !important; }
.py-number                  { color: $py_number; }
.py-defname                 { color: $py_def_name; font-weight: bold; }
.py-def-name                { color: $py_def_name; font-weight: bold; }
.py-base-class              { color: $py_base_class; }
.py-param                   { color: $py_param; }
.py-docstring               { color: $py_docstring; }
.py-decorator               { color: $py_decorator; }
/* Use this if you don't want links to names underlined: */
/*a.py-name                   { text-decoration: none; }*/

/* Graphs & Diagrams
 *   - These CSS styles are used for graphs & diagrams generated using
 *     Graphviz dot.  'img.graph-without-title' is used for bare
 *     diagrams (to remove the border created by making the image
 *     clickable).
 */
img.graph-without-title     { border: none; }
img.graph-with-title        { border: $graph_border; }
span.graph-title            { font-weight: bold; }
span.graph-caption          { }

/* General-purpose classes
 *   - 'p.indent-wrapped-lines' defines a paragraph whose first line
 *     is not indented, but whose subsequent lines are.
 *   - The 'nomargin-top' class is used to remove the top margin (e.g.
 *     from lists).  The 'nomargin' class is used to remove both the
 *     top and bottom margin (but not the left or right margin --
 *     for lists, that would cause the bullets to disappear.)
 */
p.indent-wrapped-lines      { padding: 0 0 0 7em; text-indent: -7em; 
                              margin: 0; }
.nomargin-top               { margin-top: 0; }
.nomargin                   { margin-top: 0; margin-bottom: 0; }

/* HTML Log */
div.log-block               { padding: 0; margin: .5em 0 .5em 0;
                              background: $log_bg; color: $log_fg;
                              border: $log_border; }
div.log-error               { padding: .1em .3em .1em .3em; margin: 4px;
                              background: $log_error_bg; color: $log_error_fg;
                              border: $log_error_border; }
div.log-warning             { padding: .1em .3em .1em .3em; margin: 4px;
                              background: $log_warn_bg; color: $log_warn_fg;
                              border: $log_warn_border; }
div.log-info               { padding: .1em .3em .1em .3em; margin: 4px;
                              background: $log_info_bg; color: $log_info_fg;
                              border: $log_info_border; }
h2.log-hdr                  { background: $log_hdr_bg; color: $log_hdr_fg;
                              margin: 0; padding: 0em 0.5em 0em 0.5em;
                              border-bottom: $log_border; font-size: 110%; }
p.log                       { font-weight: bold; margin: .5em 0 .5em 0; }
tr.opt-changed              { color: $opt_changed_fg; font-weight: bold; }
tr.opt-default              { color: $opt_default_fg; }
pre.log                     { margin: 0; padding: 0; padding-left: 1em; }
""" 

############################################################
## Derived stylesheets
############################################################
# Use some simple manipulations to produce a wide variety of color
# schemes.  In particular, use th _COLOR_RE regular expression to
# search for colors, and to transform them in various ways.

_COLOR_RE = re.compile(r'#(..)(..)(..)')

def _set_colors(template, *dicts):
    colors = dicts[0].copy()
    for d in dicts[1:]: colors.update(d)
    return re.sub(r'\$(\w+)', lambda m:colors[m.group(1)], template)
    
def _rv(match):
    """
    Given a regexp match for a color, return the reverse-video version
    of that color.

    @param match: A regular expression match.
    @type match: C{Match}
    @return: The reverse-video color.
    @rtype: C{string}
    """
    rgb = [int(grp, 16) for grp in match.groups()]
    return '#' + ''.join(['%02x' % (255-c) for c in rgb])

def _darken_darks(match):
    rgb = [int(grp, 16) for grp in match.groups()]
    return '#' + ''.join(['%02x' % (((c/255.)**2) * 255) for c in rgb])

_WHITE_COLORS = dict(
    # Defaults:
    body_bg                 =  '#ffffff',
    body_fg                 =  '#000000',
    body_link               =  '#0000ff',
    body_visited_link       =  '#204080',
    # Navigation bar:
    navbar_bg               =  '#a0c0ff',
    navbar_fg               =  '#000000',
    navbar_border           =  '2px groove #c0d0d0',
    navbar_select_bg        =  '#70b0ff',
    navbar_select_fg        =  '#000000',
    navbar_link             =  '#0000ff',
    navbar_visited_link     =  '#204080',
    # Tables (summary tables, details tables, indices):
    table_bg                =  '#e8f0f8',
    table_fg                =  '#000000',
    table_link              =  '#0000ff',
    table_visited_link      =  '#204080',
    table_border            =  '1px solid #608090',
    table_hdr_bg            =  '#70b0ff',
    table_hdr_fg            =  '#000000', 
    table_hdr_link          =  '#0000ff',
    table_hdr_visited_link  =  '#204080',
    group_hdr_bg            =  '#c0e0f8',
    group_hdr_fg            =  '#000000',
    # Function signatures:
    sig_name                =  '#006080',
    sig_arg                 =  '#008060',
    sig_default             =  '#602000',
    summary_sig_name        =  '#006080',
    summary_sig_arg         =  '#006040',
    summary_sig_default     =  '#501800',
    # Variable values:
    variable_bg             =  '#dce4ec',
    variable_fg             =  '#000000',
    variable_border         =  '1px solid #708890',
    variable_linewrap       =  '#604000',
    variable_ellipsis       =  '#604000',
    variable_quote          =  '#604000',
    variable_group          =  '#008000',
    variable_string         =  '#006030',
    variable_op             =  '#604000',
    variable_unknown        =  '#a00000',
    re                      =  '#000000',
    re_char                 =  '#006030',
    re_op                   =  '#600000',
    re_group                =  '#003060',
    re_ref                  =  '#404040',
    # Python source code:
    doctest_bg              =  '#e8f0f8',
    doctest_fg              =  '#000000',
    doctest_border          =  '1px solid #708890',
    doctest_in_table_bg     =  '#dce4ec',
    doctest_in_table_fg     =  '#000000',
    pysrc_border            =  '2px solid #000000',
    pysrc_sep_border        =  '2px solid #000000',
    pysrc_bg                =  '#f0f0f0',
    pysrc_fg                =  '#000000',
    pysrc_highlight_hdr_bg  =  '#d8e8e8',
    pysrc_highlight_bg      =  '#d0e0e0',
    py_prompt               =  '#005050',
    py_more                 =  '#005050',
    py_string               =  '#006030',
    py_comment              =  '#003060',
    py_keyword              =  '#600000',
    py_output               =  '#404040',
    py_name                 =  '#000050',
    py_number               =  '#005000',
    py_def_name             =  '#000060',
    py_base_class           =  '#000060',
    py_param                =  '#000060',
    py_docstring            =  '#006030',
    py_decorator            =  '#804020',
    # Graphs
    graph_border            =  '1px solid #000000',
    # Log block
    log_bg                  =  '#e8f0f8',
    log_fg                  =  '#000000',
    log_border              =  '1px solid #000000',
    log_hdr_bg              =  '#70b0ff',
    log_hdr_fg              =  '#000000',
    log_error_bg            =  '#ffb0b0',
    log_error_fg            =  '#000000',
    log_error_border        =  '1px solid #000000',
    log_warn_bg             =  '#ffffb0',
    log_warn_fg             =  '#000000',
    log_warn_border         =  '1px solid #000000',
    log_info_bg             =  '#b0ffb0',
    log_info_fg             =  '#000000',
    log_info_border         =  '1px solid #000000',
    opt_changed_fg          =  '#000000',
    opt_default_fg          =  '#606060',
    )

_BLUE_COLORS = _WHITE_COLORS.copy()
_BLUE_COLORS.update(dict(
    # Body: white text on a dark blue background
    body_bg                 =  '#000070',
    body_fg                 =  '#ffffff',
    body_link               =  '#ffffff',
    body_visited_link       =  '#d0d0ff',
    # Tables: cyan headers, black on white bodies
    table_bg                =   '#ffffff',
    table_fg                =  '#000000',
    table_hdr_bg            =  '#70b0ff',
    table_hdr_fg            =  '#000000',
    table_hdr_link          =  '#000000',
    table_hdr_visited_link  =  '#000000',
    table_border            =  '1px solid #000000',
    # Navigation bar: blue w/ cyan selection
    navbar_bg               =  '#0000ff',
    navbar_fg               =  '#ffffff',
    navbar_link             =  '#ffffff',
    navbar_visited_link     =  '#ffffff',
    navbar_select_bg        =   '#70b0ff',
    navbar_select_fg        =   '#000000',
    navbar_border           =  '1px solid #70b0ff',
    # Variable values & doctest blocks: cyan
    variable_bg             =   '#c0e0f8',
    variable_fg             =  '#000000',
    doctest_bg              =   '#c0e0f8',
    doctest_fg              =  '#000000',
    doctest_in_table_bg     =   '#c0e0f8',
    doctest_in_table_fg     =  '#000000',
    ))

_WHITE = _set_colors(TEMPLATE, _WHITE_COLORS)
_BLUE = _set_colors(TEMPLATE, _BLUE_COLORS)

    # Black-on-green
_GREEN = _COLOR_RE.sub(_darken_darks, _COLOR_RE.sub(r'#\1\3\2', _BLUE))

# White-on-black, with blue highlights.
_BLACK = _COLOR_RE.sub(r'#\3\2\1', _COLOR_RE.sub(_rv, _WHITE))

# Grayscale
_GRAYSCALE = _COLOR_RE.sub(r'#\2\2\2', _WHITE)

############################################################
## Stylesheet table
############################################################

STYLESHEETS = {
    'white': (_WHITE, "Black on white, with blue highlights"),
    'blue': (_BLUE, "Black on steel blue"),
    'green': (_GREEN, "Black on green"),
    'black': (_BLACK, "White on black, with blue highlights"),
    'grayscale': (_GRAYSCALE, "Grayscale black on white"),
    'default': (_WHITE, "Default stylesheet (=white)"),
#    'none': (_LAYOUT, "A base stylesheet (no color modifications)"),
    }

########NEW FILE########
__FILENAME__ = html_help
#
# epydoc.css: default help page
# Edward Loper
#
# Created [01/30/01 05:18 PM]
# $Id: html_help.py 1239 2006-07-05 11:29:50Z edloper $
#

"""
Default help file for the HTML outputter (L{epydoc.docwriter.html}).

@type HTML_HELP: C{string}
@var HTML_HELP: The contents of the HTML body for the default
help page.
"""
__docformat__ = 'epytext en'

# Expects: {'this_project': name}
HTML_HELP = '''
<h1 class="epydoc"> API Documentation </h1>

<p> This document contains the API (Application Programming Interface)
documentation for %(this_project)s.  Documentation for the Python
objects defined by the project is divided into separate pages for each
package, module, and class.  The API documentation also includes two
pages containing information about the project as a whole: a trees
page, and an index page.  </p>

<h2> Object Documentation </h2>

  <p>Each <strong>Package Documentation</strong> page contains: </p>
  <ul>
    <li> A description of the package. </li>
    <li> A list of the modules and sub-packages contained by the
    package.  </li>
    <li> A summary of the classes defined by the package. </li>
    <li> A summary of the functions defined by the package. </li>
    <li> A summary of the variables defined by the package. </li>
    <li> A detailed description of each function defined by the
    package. </li>
    <li> A detailed description of each variable defined by the
    package. </li>
  </ul>
  
  <p>Each <strong>Module Documentation</strong> page contains:</p>
  <ul>
    <li> A description of the module. </li>
    <li> A summary of the classes defined by the module. </li>
    <li> A summary of the functions defined by the module. </li>
    <li> A summary of the variables defined by the module. </li>
    <li> A detailed description of each function defined by the
    module. </li>
    <li> A detailed description of each variable defined by the
    module. </li>
  </ul>
  
  <p>Each <strong>Class Documentation</strong> page contains: </p>
  <ul>
    <li> A class inheritance diagram. </li>
    <li> A list of known subclasses. </li>
    <li> A description of the class. </li>
    <li> A summary of the methods defined by the class. </li>
    <li> A summary of the instance variables defined by the class. </li>
    <li> A summary of the class (static) variables defined by the
    class. </li> 
    <li> A detailed description of each method defined by the
    class. </li>
    <li> A detailed description of each instance variable defined by the
    class. </li> 
    <li> A detailed description of each class (static) variable defined
    by the class. </li> 
  </ul>

<h2> Project Documentation </h2>

  <p> The <strong>Trees</strong> page contains the module and class hierarchies: </p>
  <ul>
    <li> The <em>module hierarchy</em> lists every package and module, with
    modules grouped into packages.  At the top level, and within each
    package, modules and sub-packages are listed alphabetically. </li>
    <li> The <em>class hierarchy</em> lists every class, grouped by base
    class.  If a class has more than one base class, then it will be
    listed under each base class.  At the top level, and under each base
    class, classes are listed alphabetically. </li>
  </ul>
  
  <p> The <strong>Index</strong> page contains indices of terms and
  identifiers: </p>
  <ul>
    <li> The <em>term index</em> lists every term indexed by any object\'s
    documentation.  For each term, the index provides links to each
    place where the term is indexed. </li>
    <li> The <em>identifier index</em> lists the (short) name of every package,
    module, class, method, function, variable, and parameter.  For each
    identifier, the index provides a short description, and a link to
    its documentation. </li>
  </ul>

<h2> The Table of Contents </h2>

<p> The table of contents occupies the two frames on the left side of
the window.  The upper-left frame displays the <em>project
contents</em>, and the lower-left frame displays the <em>module
contents</em>: </p>

<table class="help summary" border="1" cellspacing="0" cellpadding="3">
  <tr style="height: 30%%">
    <td align="center" style="font-size: small">
       Project<br />Contents<hr />...</td>
    <td align="center" style="font-size: small" rowspan="2" width="70%%">
      API<br />Documentation<br />Frame<br /><br /><br />
    </td>
  </tr>
  <tr>
    <td align="center" style="font-size: small">
      Module<br />Contents<hr />&nbsp;<br />...<br />&nbsp;
    </td>
  </tr>
</table><br />

<p> The <strong>project contents frame</strong> contains a list of all packages
and modules that are defined by the project.  Clicking on an entry
will display its contents in the module contents frame.  Clicking on a
special entry, labeled "Everything," will display the contents of
the entire project. </p>

<p> The <strong>module contents frame</strong> contains a list of every
submodule, class, type, exception, function, and variable defined by a
module or package.  Clicking on an entry will display its
documentation in the API documentation frame.  Clicking on the name of
the module, at the top of the frame, will display the documentation
for the module itself. </p>

<p> The "<strong>frames</strong>" and "<strong>no frames</strong>" buttons below the top
navigation bar can be used to control whether the table of contents is
displayed or not. </p>

<h2> The Navigation Bar </h2>

<p> A navigation bar is located at the top and bottom of every page.
It indicates what type of page you are currently viewing, and allows
you to go to related pages.  The following table describes the labels
on the navigation bar.  Note that not some labels (such as
[Parent]) are not displayed on all pages. </p>

<table class="summary" border="1" cellspacing="0" cellpadding="3" width="100%%">
<tr class="summary">
  <th>Label</th>
  <th>Highlighted when...</th>
  <th>Links to...</th>
</tr>
  <tr><td valign="top"><strong>[Parent]</strong></td>
      <td valign="top"><em>(never highlighted)</em></td>
      <td valign="top"> the parent of the current package </td></tr>
  <tr><td valign="top"><strong>[Package]</strong></td>
      <td valign="top">viewing a package</td>
      <td valign="top">the package containing the current object
      </td></tr>
  <tr><td valign="top"><strong>[Module]</strong></td>
      <td valign="top">viewing a module</td>
      <td valign="top">the module containing the current object
      </td></tr> 
  <tr><td valign="top"><strong>[Class]</strong></td>
      <td valign="top">viewing a class </td>
      <td valign="top">the class containing the current object</td></tr>
  <tr><td valign="top"><strong>[Trees]</strong></td>
      <td valign="top">viewing the trees page</td>
      <td valign="top"> the trees page </td></tr>
  <tr><td valign="top"><strong>[Index]</strong></td>
      <td valign="top">viewing the index page</td>
      <td valign="top"> the index page </td></tr>
  <tr><td valign="top"><strong>[Help]</strong></td>
      <td valign="top">viewing the help page</td>
      <td valign="top"> the help page </td></tr>
</table>

<p> The "<strong>show private</strong>" and "<strong>hide private</strong>" buttons below
the top navigation bar can be used to control whether documentation
for private objects is displayed.  Private objects are usually defined
as objects whose (short) names begin with a single underscore, but do
not end with an underscore.  For example, "<code>_x</code>",
"<code>__pprint</code>", and "<code>epydoc.epytext._tokenize</code>"
are private objects; but "<code>re.sub</code>",
"<code>__init__</code>", and "<code>type_</code>" are not.  However,
if a module defines the "<code>__all__</code>" variable, then its
contents are used to decide which objects are private. </p>

<p> A timestamp below the bottom navigation bar indicates when each
page was last updated. </p>
'''

########NEW FILE########
__FILENAME__ = latex
#
# epydoc.py: epydoc LaTeX output generator
# Edward Loper
#
# Created [01/30/01 05:18 PM]
# $Id: latex.py 1621 2007-09-23 18:54:23Z edloper $
#

"""
The LaTeX output generator for epydoc.  The main interface provided by
this module is the L{LatexWriter} class.

@todo: Inheritance=listed
"""
__docformat__ = 'epytext en'

import os.path, sys, time, re, textwrap, codecs

from epydoc.apidoc import *
from epydoc.compat import *
import epydoc
from epydoc import log
from epydoc import markup
from epydoc.util import plaintext_to_latex
import epydoc.markup

class LatexWriter:
    PREAMBLE = [
        "\\documentclass{article}",
        "\\usepackage{alltt, parskip, fancyhdr, boxedminipage}",
        "\\usepackage{makeidx, multirow, longtable, tocbibind, amssymb}",
        "\\usepackage{fullpage}",
        "\\usepackage[usenames]{color}",
        # Fix the heading position -- without this, the headings generated
        # by the fancyheadings package sometimes overlap the text.
        "\\setlength{\\headheight}{16pt}",
        "\\setlength{\\headsep}{24pt}",
        "\\setlength{\\topmargin}{-\\headsep}",
        # By default, do not indent paragraphs.
        "\\setlength{\\parindent}{0ex}",
        "\\setlength{\\parskip}{2ex}",
        # Double the standard size boxedminipage outlines.
        "\\setlength{\\fboxrule}{2\\fboxrule}",
        # Create a 'base class' length named BCL for use in base trees.
        "\\newlength{\\BCL} % base class length, for base trees.",
        # Display the section & subsection names in a header.
        "\\pagestyle{fancy}",
        "\\renewcommand{\\sectionmark}[1]{\\markboth{#1}{}}",
        "\\renewcommand{\\subsectionmark}[1]{\\markright{#1}}",
        # Colorization for python source code
        "\\definecolor{py@keywordcolour}{rgb}{1,0.45882,0}",
        "\\definecolor{py@stringcolour}{rgb}{0,0.666666,0}",
        "\\definecolor{py@commentcolour}{rgb}{1,0,0}",
        "\\definecolor{py@ps1colour}{rgb}{0.60784,0,0}",
        "\\definecolor{py@ps2colour}{rgb}{0.60784,0,1}",
        "\\definecolor{py@inputcolour}{rgb}{0,0,0}",
        "\\definecolor{py@outputcolour}{rgb}{0,0,1}",
        "\\definecolor{py@exceptcolour}{rgb}{1,0,0}",
        "\\definecolor{py@defnamecolour}{rgb}{1,0.5,0.5}",
        "\\definecolor{py@builtincolour}{rgb}{0.58039,0,0.58039}",
        "\\definecolor{py@identifiercolour}{rgb}{0,0,0}",
        "\\definecolor{py@linenumcolour}{rgb}{0.4,0.4,0.4}",
        "\\definecolor{py@inputcolour}{rgb}{0,0,0}",
        "% Prompt",
        "\\newcommand{\\pysrcprompt}[1]{\\textcolor{py@ps1colour}"
            "{\\small\\textbf{#1}}}",
        "\\newcommand{\\pysrcmore}[1]{\\textcolor{py@ps2colour}"
            "{\\small\\textbf{#1}}}",
        "% Source code",
        "\\newcommand{\\pysrckeyword}[1]{\\textcolor{py@keywordcolour}"
            "{\\small\\textbf{#1}}}",
        "\\newcommand{\\pysrcbuiltin}[1]{\\textcolor{py@builtincolour}"
            "{\\small\\textbf{#1}}}",
        "\\newcommand{\\pysrcstring}[1]{\\textcolor{py@stringcolour}"
            "{\\small\\textbf{#1}}}",
        "\\newcommand{\\pysrcdefname}[1]{\\textcolor{py@defnamecolour}"
            "{\\small\\textbf{#1}}}",
        "\\newcommand{\\pysrcother}[1]{\\small\\textbf{#1}}",
        "% Comments",
        "\\newcommand{\\pysrccomment}[1]{\\textcolor{py@commentcolour}"
            "{\\small\\textbf{#1}}}",
        "% Output",
        "\\newcommand{\\pysrcoutput}[1]{\\textcolor{py@outputcolour}"
            "{\\small\\textbf{#1}}}",
        "% Exceptions",
        "\\newcommand{\\pysrcexcept}[1]{\\textcolor{py@exceptcolour}"
            "{\\small\\textbf{#1}}}",
        # Size of the function description boxes.
        "\\newlength{\\funcindent}",
        "\\newlength{\\funcwidth}",
        "\\setlength{\\funcindent}{1cm}",
        "\\setlength{\\funcwidth}{\\textwidth}",
        "\\addtolength{\\funcwidth}{-2\\funcindent}",
        # Size of the var description tables.
        "\\newlength{\\varindent}",
        "\\newlength{\\varnamewidth}",
        "\\newlength{\\vardescrwidth}",
        "\\newlength{\\varwidth}",
        "\\setlength{\\varindent}{1cm}",
        "\\setlength{\\varnamewidth}{.3\\textwidth}",
        "\\setlength{\\varwidth}{\\textwidth}",
        "\\addtolength{\\varwidth}{-4\\tabcolsep}",
        "\\addtolength{\\varwidth}{-3\\arrayrulewidth}",
        "\\addtolength{\\varwidth}{-2\\varindent}",
        "\\setlength{\\vardescrwidth}{\\varwidth}",
        "\\addtolength{\\vardescrwidth}{-\\varnamewidth}",
        # Define new environment for displaying parameter lists.
        textwrap.dedent("""\
        \\newenvironment{Ventry}[1]%
         {\\begin{list}{}{%
           \\renewcommand{\\makelabel}[1]{\\texttt{##1:}\\hfil}%
           \\settowidth{\\labelwidth}{\\texttt{#1:}}%
           \\setlength{\\leftmargin}{\\labelsep}%
           \\addtolength{\\leftmargin}{\\labelwidth}}}%
         {\\end{list}}"""),
        ]

    HRULE = '\\rule{\\textwidth}{0.5\\fboxrule}\n\n'

    SECTIONS = ['\\part{%s}', '\\chapter{%s}', '\\section{%s}',
                '\\subsection{%s}', '\\subsubsection{%s}',
                '\\textbf{%s}']

    STAR_SECTIONS = ['\\part*{%s}', '\\chapter*{%s}', '\\section*{%s}',
                     '\\subsection*{%s}', '\\subsubsection*{%s}',
                     '\\textbf{%s}']

    def __init__(self, docindex, **kwargs):
        self.docindex = docindex
        # Process keyword arguments
        self._show_private = kwargs.get('private', 0)
        self._prj_name = kwargs.get('prj_name', None) or 'API Documentation'
        self._crossref = kwargs.get('crossref', 1)
        self._index = kwargs.get('index', 1)
        self._list_classes_separately=kwargs.get('list_classes_separately',0)
        self._inheritance = kwargs.get('inheritance', 'listed')
        self._exclude = kwargs.get('exclude', 1)
        self._top_section = 2
        self._index_functions = 1
        self._hyperref = 1

        #: The Python representation of the encoding.
        #: Update L{latex_encodings} in case of mismatch between it and
        #: the C{inputenc} LaTeX package.
        self._encoding = kwargs.get('encoding', 'utf-8')

        self.valdocs = valdocs = sorted(docindex.reachable_valdocs(
            imports=False, packages=False, bases=False, submodules=False, 
            subclasses=False, private=self._show_private))
        self._num_files = self.num_files()
        # For use with select_variables():
        if self._show_private: self._public_filter = None
        else: self._public_filter = True

        self.class_list = [d for d in valdocs if isinstance(d, ClassDoc)]
        """The list of L{ClassDoc}s for the documented classes."""
        self.class_set = set(self.class_list)
        """The set of L{ClassDoc}s for the documented classes."""
        
    def write(self, directory=None):
        """
        Write the API documentation for the entire project to the
        given directory.

        @type directory: C{string}
        @param directory: The directory to which output should be
            written.  If no directory is specified, output will be
            written to the current directory.  If the directory does
            not exist, it will be created.
        @rtype: C{None}
        @raise OSError: If C{directory} cannot be created,
        @raise OSError: If any file cannot be created or written to.
        """
        # For progress reporting:
        self._files_written = 0.
        
        # Set the default values for ValueDoc formatted representations.
        orig_valdoc_defaults = (ValueDoc.SUMMARY_REPR_LINELEN,
                                ValueDoc.REPR_LINELEN,
                                ValueDoc.REPR_MAXLINES)
        ValueDoc.SUMMARY_REPR_LINELEN = 60
        ValueDoc.REPR_LINELEN = 52
        ValueDoc.REPR_MAXLINES = 5

        # Create destination directories, if necessary
        if not directory: directory = os.curdir
        self._mkdir(directory)
        self._directory = directory
        
        # Write the top-level file.
        self._write(self.write_topfile, directory, 'api.tex')

        # Write the module & class files.
        for val_doc in self.valdocs:
            if isinstance(val_doc, ModuleDoc):
                filename = '%s-module.tex' % val_doc.canonical_name
                self._write(self.write_module, directory, filename, val_doc)
            elif (isinstance(val_doc, ClassDoc) and 
                  self._list_classes_separately):
                filename = '%s-class.tex' % val_doc.canonical_name
                self._write(self.write_class, directory, filename, val_doc)

        # Restore defaults that we changed.
        (ValueDoc.SUMMARY_REPR_LINELEN, ValueDoc.REPR_LINELEN,
         ValueDoc.REPR_MAXLINES) = orig_valdoc_defaults
        
    def _write(self, write_func, directory, filename, *args):
        # Display our progress.
        self._files_written += 1
        log.progress(self._files_written/self._num_files, filename)
        
        path = os.path.join(directory, filename)
        if self._encoding == 'utf-8':
            f = codecs.open(path, 'w', 'utf-8')
            write_func(f.write, *args)
            f.close()
        else:
            result = []
            write_func(result.append, *args)
            s = u''.join(result)
            try:
                s = s.encode(self._encoding)
            except UnicodeError:
                log.error("Output could not be represented with the "
                          "given encoding (%r).  Unencodable characters "
                          "will be displayed as '?'.  It is recommended "
                          "that you use a different output encoding (utf-8, "
                          "if it's supported by latex on your system)."
                          % self._encoding)
                s = s.encode(self._encoding, 'replace')
            f = open(path, 'w')
            f.write(s)
            f.close()

    def num_files(self):
        """
        @return: The number of files that this C{LatexFormatter} will
            generate.
        @rtype: C{int}
        """
        n = 1
        for doc in self.valdocs:
            if isinstance(doc, ModuleDoc): n += 1
            if isinstance(doc, ClassDoc) and self._list_classes_separately:
                n += 1
        return n
        
    def _mkdir(self, directory):
        """
        If the given directory does not exist, then attempt to create it.
        @rtype: C{None}
        """
        if not os.path.isdir(directory):
            if os.path.exists(directory):
                raise OSError('%r is not a directory' % directory)
            os.mkdir(directory)
            
    #////////////////////////////////////////////////////////////
    #{ Main Doc File
    #////////////////////////////////////////////////////////////

    def write_topfile(self, out):
        self.write_header(out, 'Include File')
        self.write_preamble(out)
        out('\n\\begin{document}\n\n')
        self.write_start_of(out, 'Header')

        # Write the title.
        self.write_start_of(out, 'Title')
        out('\\title{%s}\n' % plaintext_to_latex(self._prj_name, 1))
        out('\\author{API Documentation}\n')
        out('\\maketitle\n')

        # Add a table of contents.
        self.write_start_of(out, 'Table of Contents')
        out('\\addtolength{\\parskip}{-2ex}\n')
        out('\\tableofcontents\n')
        out('\\addtolength{\\parskip}{2ex}\n')

        # Include documentation files.
        self.write_start_of(out, 'Includes')
        for val_doc in self.valdocs:
            if isinstance(val_doc, ModuleDoc):
                out('\\include{%s-module}\n' % val_doc.canonical_name)

        # If we're listing classes separately, put them after all the
        # modules.
        if self._list_classes_separately:
            for val_doc in self.valdocs:
                if isinstance(val_doc, ClassDoc):
                    out('\\include{%s-class}\n' % val_doc.canonical_name)

        # Add the index, if requested.
        if self._index:
            self.write_start_of(out, 'Index')
            out('\\printindex\n\n')

        # Add the footer.
        self.write_start_of(out, 'Footer')
        out('\\end{document}\n\n')

    def write_preamble(self, out):
        out('\n'.join(self.PREAMBLE))
        out('\n')
        
        # Set the encoding.
        out('\\usepackage[%s]{inputenc}\n' % self.get_latex_encoding())

        # If we're generating hyperrefs, add the appropriate packages.
        if self._hyperref:
            out('\\definecolor{UrlColor}{rgb}{0,0.08,0.45}\n')
            out('\\usepackage[dvips, pagebackref, pdftitle={%s}, '
                'pdfcreator={epydoc %s}, bookmarks=true, '
                'bookmarksopen=false, pdfpagemode=UseOutlines, '
                'colorlinks=true, linkcolor=black, anchorcolor=black, '
                'citecolor=black, filecolor=black, menucolor=black, '
                'pagecolor=black, urlcolor=UrlColor]{hyperref}\n' %
                (self._prj_name or '', epydoc.__version__))
            
        # If we're generating an index, add it to the preamble.
        if self._index:
            out("\\makeindex\n")

        # If restructuredtext was used, then we need to extend
        # the prefix to include LatexTranslator.head_prefix.
        if 'restructuredtext' in epydoc.markup.MARKUP_LANGUAGES_USED:
            from epydoc.markup import restructuredtext
            rst_head = restructuredtext.latex_head_prefix()
            rst_head = ''.join(rst_head).split('\n')
            for line in rst_head[1:]:
                m = re.match(r'\\usepackage(\[.*?\])?{(.*?)}', line)
                if m and m.group(2) in (
                    'babel', 'hyperref', 'color', 'alltt', 'parskip',
                    'fancyhdr', 'boxedminipage', 'makeidx',
                    'multirow', 'longtable', 'tocbind', 'assymb',
                    'fullpage', 'inputenc'):
                    pass
                else:
                    out(line+'\n')

        
    #////////////////////////////////////////////////////////////
    #{ Chapters
    #////////////////////////////////////////////////////////////

    def write_module(self, out, doc):
        self.write_header(out, doc)
        self.write_start_of(out, 'Module Description')

        # Add this module to the index.
        out('    ' + self.indexterm(doc, 'start'))

        # Add a section marker.
        out(self.section('%s %s' % (self.doc_kind(doc),
                                    doc.canonical_name)))

        # Label our current location.
        out('    \\label{%s}\n' % self.label(doc))

        # Add the module's description.
        if doc.descr not in (None, UNKNOWN):
            out(self.docstring_to_latex(doc.descr))

        # Add version, author, warnings, requirements, notes, etc.
        self.write_standard_fields(out, doc)

        # If it's a package, list the sub-modules.
        if doc.submodules != UNKNOWN and doc.submodules:
            self.write_module_list(out, doc)

        # Contents.
        if self._list_classes_separately:
            self.write_class_list(out, doc)
        self.write_func_list(out, 'Functions', doc, 'function')
        self.write_var_list(out, 'Variables', doc, 'other')

        # Class list.
        if not self._list_classes_separately:
            classes = doc.select_variables(imported=False, value_type='class',
                                           public=self._public_filter)
            for var_doc in classes:
                self.write_class(out, var_doc.value)

        # Mark the end of the module (for the index)
        out('    ' + self.indexterm(doc, 'end'))

    def write_class(self, out, doc):
        if self._list_classes_separately:
            self.write_header(out, doc)
        self.write_start_of(out, 'Class Description')

        # Add this class to the index.
        out('    ' + self.indexterm(doc, 'start'))

        # Add a section marker.
        if self._list_classes_separately:
            seclevel = 0
            out(self.section('%s %s' % (self.doc_kind(doc),
                                        doc.canonical_name), seclevel))
        else:
            seclevel = 1
            out(self.section('%s %s' % (self.doc_kind(doc),
                                        doc.canonical_name[-1]), seclevel))

        # Label our current location.
        out('    \\label{%s}\n' % self.label(doc))

        # Add our base list.
        if doc.bases not in (UNKNOWN, None) and len(doc.bases) > 0:
            out(self.base_tree(doc))

        # The class's known subclasses
        if doc.subclasses not in (UNKNOWN, None) and len(doc.subclasses) > 0:
            sc_items = [plaintext_to_latex('%s' % sc.canonical_name)
                        for sc in doc.subclasses]
            out(self._descrlist(sc_items, 'Known Subclasses', short=1))

        # The class's description.
        if doc.descr not in (None, UNKNOWN):
            out(self.docstring_to_latex(doc.descr))

        # Version, author, warnings, requirements, notes, etc.
        self.write_standard_fields(out, doc)

        # Contents.
        self.write_func_list(out, 'Methods', doc, 'method',
                             seclevel+1)
        self.write_var_list(out, 'Properties', doc,
                            'property', seclevel+1)
        self.write_var_list(out, 'Class Variables', doc, 
                            'classvariable', seclevel+1)
        self.write_var_list(out, 'Instance Variables', doc, 
                            'instancevariable', seclevel+1)

        # Mark the end of the class (for the index)
        out('    ' + self.indexterm(doc, 'end'))

    #////////////////////////////////////////////////////////////
    #{ Module hierarchy trees
    #////////////////////////////////////////////////////////////
    
    def write_module_tree(self, out):
        modules = [doc for doc in self.valdocs
                   if isinstance(doc, ModuleDoc)]
        if not modules: return
        
        # Write entries for all top-level modules/packages.
        out('\\begin{itemize}\n')
        out('\\setlength{\\parskip}{0ex}\n')
        for doc in modules:
            if (doc.package in (None, UNKNOWN) or
                doc.package not in self.valdocs):
                self.write_module_tree_item(out, doc)
        return s +'\\end{itemize}\n'

    def write_module_list(self, out, doc):
        if len(doc.submodules) == 0: return
        self.write_start_of(out, 'Modules')
        
        out(self.section('Modules', 1))
        out('\\begin{itemize}\n')
        out('\\setlength{\\parskip}{0ex}\n')

        for group_name in doc.group_names():
            if not doc.submodule_groups[group_name]: continue
            if group_name:
                out('  \\item \\textbf{%s}\n' % group_name)
                out('  \\begin{itemize}\n')
            for submodule in doc.submodule_groups[group_name]:
                self.write_module_tree_item(out, submodule)
            if group_name:
                out('  \end{itemize}\n')

        out('\\end{itemize}\n\n')

    def write_module_tree_item(self, out, doc, depth=0):
        """
        Helper function for L{write_module_tree} and L{write_module_list}.
        
        @rtype: C{string}
        """
        out(' '*depth + '\\item \\textbf{')
        out(plaintext_to_latex(doc.canonical_name[-1]) +'}')
        if doc.summary not in (None, UNKNOWN):
            out(': %s\n' % self.docstring_to_latex(doc.summary))
        if self._crossref:
            out('\n  \\textit{(Section \\ref{%s}' % self.label(doc))
            out(', p.~\\pageref{%s})}\n\n' % self.label(doc))
        if doc.submodules != UNKNOWN and doc.submodules:
            out(' '*depth + '  \\begin{itemize}\n')
            out(' '*depth + '\\setlength{\\parskip}{0ex}\n')
            for submodule in doc.submodules:
                self.write_module_tree_item(out, submodule, depth+4)
            out(' '*depth + '  \\end{itemize}\n')

    #////////////////////////////////////////////////////////////
    #{ Base class trees
    #////////////////////////////////////////////////////////////

    def base_tree(self, doc, width=None, linespec=None):
        if width is None:
            width = self._find_tree_width(doc)+2
            linespec = []
            s = ('&'*(width-4)+'\\multicolumn{2}{l}{\\textbf{%s}}\n' %
                   plaintext_to_latex('%s'%self._base_name(doc)))
            s += '\\end{tabular}\n\n'
            top = 1
        else:
            s = self._base_tree_line(doc, width, linespec)
            top = 0
        
        if isinstance(doc, ClassDoc):
            for i in range(len(doc.bases)-1, -1, -1):
                base = doc.bases[i]
                spec = (i > 0)
                s = self.base_tree(base, width, [spec]+linespec) + s

        if top:
            s = '\\begin{tabular}{%s}\n' % (width*'c') + s

        return s

    def _base_name(self, doc):
        if doc.canonical_name is None:
            if doc.parse_repr is not None:
                return doc.parse_repr
            else:
                return '??'
        else:
            return '%s' % doc.canonical_name

    def _find_tree_width(self, doc):
        if not isinstance(doc, ClassDoc): return 2
        width = 2
        for base in doc.bases:
            width = max(width, self._find_tree_width(base)+2)
        return width

    def _base_tree_line(self, doc, width, linespec):
        base_name = plaintext_to_latex(self._base_name(doc))
        
        # linespec is a list of booleans.
        s = '%% Line for %s, linespec=%s\n' % (base_name, linespec)

        labelwidth = width-2*len(linespec)-2

        # The base class name.
        s += ('\\multicolumn{%s}{r}{' % labelwidth)
        s += '\\settowidth{\\BCL}{%s}' % base_name
        s += '\\multirow{2}{\\BCL}{%s}}\n' % base_name

        # The vertical bars for other base classes (top half)
        for vbar in linespec:
            if vbar: s += '&&\\multicolumn{1}{|c}{}\n'
            else: s += '&&\n'

        # The horizontal line.
        s += '  \\\\\\cline{%s-%s}\n' % (labelwidth+1, labelwidth+1)

        # The vertical bar for this base class.
        s += '  ' + '&'*labelwidth
        s += '\\multicolumn{1}{c|}{}\n'

        # The vertical bars for other base classes (bottom half)
        for vbar in linespec:
            if vbar: s += '&\\multicolumn{1}{|c}{}&\n'
            else: s += '&&\n'
        s += '  \\\\\n'

        return s
        
    #////////////////////////////////////////////////////////////
    #{ Class List
    #////////////////////////////////////////////////////////////
    
    def write_class_list(self, out, doc):
        groups = [(plaintext_to_latex(group_name),
                   doc.select_variables(group=group_name, imported=False,
                                        value_type='class',
                                        public=self._public_filter))
                  for group_name in doc.group_names()]

        # Discard any empty groups; and return if they're all empty.
        groups = [(g,vars) for (g,vars) in groups if vars]
        if not groups: return

        # Write a header.
        self.write_start_of(out, 'Classes')
        out(self.section('Classes', 1))
        out('\\begin{itemize}')
        out('  \\setlength{\\parskip}{0ex}\n')

        for name, var_docs in groups:
            if name:
                out('  \\item \\textbf{%s}\n' % name)
                out('  \\begin{itemize}\n')
            # Add the lines for each class
            for var_doc in var_docs:
                self.write_class_list_line(out, var_doc)
            if name:
                out('  \\end{itemize}\n')

        out('\\end{itemize}\n')

    def write_class_list_line(self, out, var_doc):
        if var_doc.value in (None, UNKNOWN): return # shouldn't happen
        doc = var_doc.value
        out('  ' + '\\item \\textbf{')
        out(plaintext_to_latex(var_doc.name) + '}')
        if doc.summary not in (None, UNKNOWN):
            out(': %s\n' % self.docstring_to_latex(doc.summary))
        if self._crossref:
            out(('\n  \\textit{(Section \\ref{%s}' % self.label(doc)))
            out((', p.~\\pageref{%s})}\n\n' % self.label(doc)))
        
    #////////////////////////////////////////////////////////////
    #{ Function List
    #////////////////////////////////////////////////////////////
    _FUNC_GROUP_HEADER = '\n\\large{\\textbf{\\textit{%s}}}\n\n'
    
    def write_func_list(self, out, heading, doc, value_type, seclevel=1):
        # Divide all public variables of the given type into groups.
        groups = [(plaintext_to_latex(group_name),
                   doc.select_variables(group=group_name, imported=False,
                                        value_type=value_type,
                                        public=self._public_filter))
                  for group_name in doc.group_names()]

        # Discard any empty groups; and return if they're all empty.
        groups = [(g,vars) for (g,vars) in groups if vars]
        if not groups: return

        # Write a header.
        self.write_start_of(out, heading)
        out('  '+self.section(heading, seclevel))

        # Write a section for each group.
        grouped_inh_vars = {}
        for name, var_docs in groups:
            self.write_func_group(out, doc, name, var_docs, grouped_inh_vars)

        # Write a section for each inheritance pseudo-group (used if
        # inheritance=='grouped')
        if grouped_inh_vars:
            for base in doc.mro():
                if base in grouped_inh_vars:
                    hdr = ('Inherited from %s' %
                           plaintext_to_latex('%s' % base.canonical_name))
                    if self._crossref and base in self.class_set:
                        hdr += ('\\textit{(Section \\ref{%s})}' %
                                self.label(base))
                    out(self._FUNC_GROUP_HEADER % (hdr))
                    for var_doc in grouped_inh_vars[base]:
                        self.write_func_list_box(out, var_doc)

    def write_func_group(self, out, doc, name, var_docs, grouped_inh_vars):
        # Split up the var_docs list, according to the way each var
        # should be displayed:
        #   - listed_inh_vars -- for listed inherited variables.
        #   - grouped_inh_vars -- for grouped inherited variables.
        #   - normal_vars -- for all other variables.
        listed_inh_vars = {}
        normal_vars = []
        for var_doc in var_docs:
            if var_doc.container != doc:
                base = var_doc.container
                if (base not in self.class_set or
                    self._inheritance == 'listed'):
                    listed_inh_vars.setdefault(base,[]).append(var_doc)
                elif self._inheritance == 'grouped':
                    grouped_inh_vars.setdefault(base,[]).append(var_doc)
                else:
                    normal_vars.append(var_doc)
            else:
                normal_vars.append(var_doc)
            
        # Write a header for the group.
        if name:
            out(self._FUNC_GROUP_HEADER % name)
        # Write an entry for each normal var:
        for var_doc in normal_vars:
            self.write_func_list_box(out, var_doc)
        # Write a subsection for inherited vars:
        if listed_inh_vars:
            self.write_func_inheritance_list(out, doc, listed_inh_vars)

    def write_func_inheritance_list(self, out, doc, listed_inh_vars):
        for base in doc.mro():
            if base not in listed_inh_vars: continue
            #if str(base.canonical_name) == 'object': continue
            var_docs = listed_inh_vars[base]
            if self._public_filter:
                var_docs = [v for v in var_docs if v.is_public]
            if var_docs:
                hdr = ('Inherited from %s' %
                       plaintext_to_latex('%s' % base.canonical_name))
                if self._crossref and base in self.class_set:
                    hdr += ('\\textit{(Section \\ref{%s})}' %
                            self.label(base))
                out(self._FUNC_GROUP_HEADER % hdr)
                out('\\begin{quote}\n')
                out('%s\n' % ', '.join(
                    ['%s()' % plaintext_to_latex(var_doc.name)
                     for var_doc in var_docs]))
                out('\\end{quote}\n')
            
    def write_func_list_box(self, out, var_doc):
        func_doc = var_doc.value
        is_inherited = (var_doc.overrides not in (None, UNKNOWN))

        # nb: this gives the containing section, not a reference
        # directly to the function.
        if not is_inherited:
            out('    \\label{%s}\n' % self.label(func_doc))
            out('    %s\n' % self.indexterm(func_doc))

        # Start box for this function.
        out('    \\vspace{0.5ex}\n\n')
        out('\\hspace{.8\\funcindent}')
        out('\\begin{boxedminipage}{\\funcwidth}\n\n')

        # Function signature.
        out('    %s\n\n' % self.function_signature(var_doc))

        if (func_doc.docstring not in (None, UNKNOWN) and
            func_doc.docstring.strip() != ''):
            out('    \\vspace{-1.5ex}\n\n')
            out('    \\rule{\\textwidth}{0.5\\fboxrule}\n')
        
        # Description
        out("\\setlength{\\parskip}{2ex}\n")
        if func_doc.descr not in (None, UNKNOWN):
            out(self.docstring_to_latex(func_doc.descr, 4))

        # Parameters
        out("\\setlength{\\parskip}{1ex}\n")
        if func_doc.arg_descrs or func_doc.arg_types:
            # Find the longest name.
            longest = max([0]+[len(n) for n in func_doc.arg_types])
            for names, descrs in func_doc.arg_descrs:
                longest = max([longest]+[len(n) for n in names])
            # Table header.
            out(' '*6+'\\textbf{Parameters}\n')
            out('      \\vspace{-1ex}\n\n')
            out(' '*6+'\\begin{quote}\n')
            out('        \\begin{Ventry}{%s}\n\n' % (longest*'x'))
            # Add params that have @type but not @param info:
            arg_descrs = list(func_doc.arg_descrs)
            args = set()
            for arg_names, arg_descr in arg_descrs:
                args.update(arg_names)
            for arg in var_doc.value.arg_types:
                if arg not in args:
                    arg_descrs.append( ([arg],None) )
            # Display params
            for (arg_names, arg_descr) in arg_descrs:
                arg_name = plaintext_to_latex(', '.join(arg_names))
                out('%s\\item[%s]\n\n' % (' '*10, arg_name))
                if arg_descr:
                    out(self.docstring_to_latex(arg_descr, 10))
                for arg_name in arg_names:
                    arg_typ = func_doc.arg_types.get(arg_name)
                    if arg_typ is not None:
                        if len(arg_names) == 1:
                            lhs = 'type'
                        else:
                            lhs = 'type of %s' % arg_name
                        rhs = self.docstring_to_latex(arg_typ).strip()
                        out('%s{\\it (%s=%s)}\n\n' % (' '*12, lhs, rhs))
            out('        \\end{Ventry}\n\n')
            out(' '*6+'\\end{quote}\n\n')
                
        # Returns
        rdescr = func_doc.return_descr
        rtype = func_doc.return_type
        if rdescr not in (None, UNKNOWN) or rtype not in (None, UNKNOWN):
            out(' '*6+'\\textbf{Return Value}\n')
            out('    \\vspace{-1ex}\n\n')
            out(' '*6+'\\begin{quote}\n')
            if rdescr not in (None, UNKNOWN):
                out(self.docstring_to_latex(rdescr, 6))
                if rtype not in (None, UNKNOWN):
                    out(' '*6+'{\\it (type=%s)}\n\n' %
                        self.docstring_to_latex(rtype, 6).strip())
            elif rtype not in (None, UNKNOWN):
                out(self.docstring_to_latex(rtype, 6))
            out(' '*6+'\\end{quote}\n\n')

        # Raises
        if func_doc.exception_descrs not in (None, UNKNOWN, [], ()):
            out(' '*6+'\\textbf{Raises}\n')
            out('    \\vspace{-1ex}\n\n')
            out(' '*6+'\\begin{quote}\n')
            out('        \\begin{description}\n\n')
            for name, descr in func_doc.exception_descrs:
                out(' '*10+'\\item[\\texttt{%s}]\n\n' %
                    plaintext_to_latex('%s' % name))
                out(self.docstring_to_latex(descr, 10))
            out('        \\end{description}\n\n')
            out(' '*6+'\\end{quote}\n\n')

        ## Overrides
        if var_doc.overrides not in (None, UNKNOWN):
            out('      Overrides: ' +
                plaintext_to_latex('%s'%var_doc.overrides.canonical_name))
            if (func_doc.docstring in (None, UNKNOWN) and
                var_doc.overrides.value.docstring not in (None, UNKNOWN)):
                out(' \textit{(inherited documentation)}')
            out('\n\n')

        # Add version, author, warnings, requirements, notes, etc.
        self.write_standard_fields(out, func_doc)

        out('    \\end{boxedminipage}\n\n')

    def function_signature(self, var_doc):
        func_doc = var_doc.value
        func_name = var_doc.name
        
        # This should never happen, but just in case:
        if func_doc in (None, UNKNOWN):
            return ('\\raggedright \\textbf{%s}(...)' %
                    plaintext_to_latex(func_name))
            
        if func_doc.posargs == UNKNOWN:
            args = ['...']
        else:
            args = [self.func_arg(name, default) for (name, default)
                    in zip(func_doc.posargs, func_doc.posarg_defaults)]
        if func_doc.vararg:
            if func_doc.vararg == '...':
                args.append('\\textit{...}')
            else:
                args.append('*\\textit{%s}' %
                            plaintext_to_latex(func_doc.vararg))
        if func_doc.kwarg:
            args.append('**\\textit{%s}' %
                        plaintext_to_latex(func_doc.kwarg))
        return ('\\raggedright \\textbf{%s}(%s)' %
                (plaintext_to_latex(func_name), ', '.join(args)))

    def func_arg(self, name, default):
        s = '\\textit{%s}' % plaintext_to_latex(self._arg_name(name))
        if default is not None:
            s += '={\\tt %s}' % default.summary_pyval_repr().to_latex(None)
        return s
    
    def _arg_name(self, arg):
        if isinstance(arg, basestring):
            return arg
        elif len(arg) == 1:
            return '(%s,)' % self._arg_name(arg[0])
        else:
            return '(%s)' % (', '.join([self._arg_name(a) for a in arg]))

    #////////////////////////////////////////////////////////////
    #{ Variable List
    #////////////////////////////////////////////////////////////
    _VAR_GROUP_HEADER = '\\multicolumn{2}{|l|}{\\textit{%s}}\\\\\n'

    # Also used for the property list.
    def write_var_list(self, out, heading, doc, value_type, seclevel=1):
        groups = [(plaintext_to_latex(group_name),
                   doc.select_variables(group=group_name, imported=False,
                                        value_type=value_type,
                                        public=self._public_filter))
                  for group_name in doc.group_names()]

        # Discard any empty groups; and return if they're all empty.
        groups = [(g,vars) for (g,vars) in groups if vars]
        if not groups: return

        # Write a header.
        self.write_start_of(out, heading)
        out('  '+self.section(heading, seclevel))

        # [xx] without this, there's a huge gap before the table -- why??
        out('    \\vspace{-1cm}\n')
        
        out('\\hspace{\\varindent}')
        out('\\begin{longtable}')
        out('{|p{\\varnamewidth}|')
        out('p{\\vardescrwidth}|l}\n')
        out('\\cline{1-2}\n')

        # Set up the headers & footer (this makes the table span
        # multiple pages in a happy way).
        out('\\cline{1-2} ')
        out('\\centering \\textbf{Name} & ')
        out('\\centering \\textbf{Description}& \\\\\n')
        out('\\cline{1-2}\n')
        out('\\endhead')
        out('\\cline{1-2}')
        out('\\multicolumn{3}{r}{\\small\\textit{')
        out('continued on next page}}\\\\')
        out('\\endfoot')
        out('\\cline{1-2}\n')
        out('\\endlastfoot')

        # Write a section for each group.
        grouped_inh_vars = {}
        for name, var_docs in groups:
            self.write_var_group(out, doc, name, var_docs, grouped_inh_vars)

        # Write a section for each inheritance pseudo-group (used if
        # inheritance=='grouped')
        if grouped_inh_vars:
            for base in doc.mro():
                if base in grouped_inh_vars:
                    hdr = ('Inherited from %s' %
                           plaintext_to_latex('%s' % base.canonical_name))
                    if self._crossref and base in self.class_set:
                        hdr += (' \\textit{(Section \\ref{%s})}' %
                                self.label(base))
                    out(self._VAR_GROUP_HEADER % (hdr))
                    out('\\cline{1-2}\n')
                    for var_doc in grouped_inh_vars[base]:
                        if isinstance(var_doc.value3, PropertyDoc):
                            self.write_property_list_line(out, var_doc)
                        else:
                            self.write_var_list_line(out, var_doc)
    
        out('\\end{longtable}\n\n')
        
    def write_var_group(self, out, doc, name, var_docs, grouped_inh_vars):
        # Split up the var_docs list, according to the way each var
        # should be displayed:
        #   - listed_inh_vars -- for listed inherited variables.
        #   - grouped_inh_vars -- for grouped inherited variables.
        #   - normal_vars -- for all other variables.
        listed_inh_vars = {}
        normal_vars = []
        for var_doc in var_docs:
            if var_doc.container != doc:
                base = var_doc.container
                if (base not in self.class_set or
                    self._inheritance == 'listed'):
                    listed_inh_vars.setdefault(base,[]).append(var_doc)
                elif self._inheritance == 'grouped':
                    grouped_inh_vars.setdefault(base,[]).append(var_doc)
                else:
                    normal_vars.append(var_doc)
            else:
                normal_vars.append(var_doc)
            
        # Write a header for the group.
        if name:
            out(self._VAR_GROUP_HEADER % name)
            out('\\cline{1-2}\n')
        # Write an entry for each normal var:
        for var_doc in normal_vars:
            if isinstance(var_doc.value, PropertyDoc):
                self.write_property_list_line(out, var_doc)
            else:
                self.write_var_list_line(out, var_doc)
        # Write a subsection for inherited vars:
        if listed_inh_vars:
            self.write_var_inheritance_list(out, doc, listed_inh_vars)

    def write_var_inheritance_list(self, out, doc, listed_inh_vars):
        for base in doc.mro():
            if base not in listed_inh_vars: continue
            #if str(base.canonical_name) == 'object': continue
            var_docs = listed_inh_vars[base]
            if self._public_filter:
                var_docs = [v for v in var_docs if v.is_public]
            if var_docs:
                hdr = ('Inherited from %s' %
                       plaintext_to_latex('%s' % base.canonical_name))
                if self._crossref and base in self.class_set:
                    hdr += (' \\textit{(Section \\ref{%s})}' %
                            self.label(base))
                out(self._VAR_GROUP_HEADER % hdr)
                out('\\multicolumn{2}{|p{\\varwidth}|}{'
                    '\\raggedright %s}\\\\\n' %
                    ', '.join(['%s' % plaintext_to_latex(var_doc.name)
                               for var_doc in var_docs]))
                out('\\cline{1-2}\n')

        
    def write_var_list_line(self, out, var_doc):
        out('\\raggedright ')
        out(plaintext_to_latex(var_doc.name, nbsp=True, breakany=True))
        out(' & ')
        has_descr = var_doc.descr not in (None, UNKNOWN)
        has_type = var_doc.type_descr not in (None, UNKNOWN)
        has_value = var_doc.value is not UNKNOWN
        if has_type or has_value:
            out('\\raggedright ')
        if has_descr:
            out(self.docstring_to_latex(var_doc.descr, 10).strip())
            if has_type or has_value: out('\n\n')
        if has_value:
            out('\\textbf{Value:} \n{\\tt %s}' %
                var_doc.value.summary_pyval_repr().to_latex(None))
        if has_type:
            ptype = self.docstring_to_latex(var_doc.type_descr, 12).strip()
            out('%s{\\it (type=%s)}' % (' '*12, ptype))
        out('&\\\\\n')
        out('\\cline{1-2}\n')

    def write_property_list_line(self, out, var_doc):
        prop_doc = var_doc.value
        out('\\raggedright ')
        out(plaintext_to_latex(var_doc.name, nbsp=True, breakany=True))
        out(' & ')
        has_descr = prop_doc.descr not in (None, UNKNOWN)
        has_type = prop_doc.type_descr not in (None, UNKNOWN)
        if has_descr or has_type:
            out('\\raggedright ')
        if has_descr:
            out(self.docstring_to_latex(prop_doc.descr, 10).strip())
            if has_type: out('\n\n')
        if has_type:
            ptype = self.docstring_to_latex(prop_doc.type_descr, 12).strip()
            out('%s{\\it (type=%s)}' % (' '*12, ptype))
        # [xx] List the fget/fset/fdel functions?
        out('&\\\\\n')
        out('\\cline{1-2}\n')

    #////////////////////////////////////////////////////////////
    #{ Standard Fields
    #////////////////////////////////////////////////////////////

    # Copied from HTMLWriter:
    def write_standard_fields(self, out, doc):
        fields = []
        field_values = {}
        
        #if _sort_fields: fields = STANDARD_FIELD_NAMES [XX]
        
        for (field, arg, descr) in doc.metadata:
            if field not in field_values:
                fields.append(field)
            if field.takes_arg:
                subfields = field_values.setdefault(field,{})
                subfields.setdefault(arg,[]).append(descr)
            else:
                field_values.setdefault(field,[]).append(descr)

        for field in fields:
            if field.takes_arg:
                for arg, descrs in field_values[field].items():
                    self.write_standard_field(out, doc, field, descrs, arg)
                                              
            else:
                self.write_standard_field(out, doc, field, field_values[field])

    def write_standard_field(self, out, doc, field, descrs, arg=''):
        singular = field.singular
        plural = field.plural
        if arg:
            singular += ' (%s)' % arg
            plural += ' (%s)' % arg
        out(self._descrlist([self.docstring_to_latex(d) for d in descrs],
                            field.singular, field.plural, field.short))
            
    def _descrlist(self, items, singular, plural=None, short=0):
        if plural is None: plural = singular
        if len(items) == 0: return ''
        if len(items) == 1 and singular is not None:
            return '\\textbf{%s:} %s\n\n' % (singular, items[0])
        if short:
            s = '\\textbf{%s:}\n' % plural
            items = [item.strip() for item in items]
            return s + ',\n    '.join(items) + '\n\n'
        else:
            s = '\\textbf{%s:}\n' % plural
            s += '\\begin{quote}\n'
            s += '  \\begin{itemize}\n\n  \item\n'
            s += '    \\setlength{\\parskip}{0.6ex}\n'
            s += '\n\n  \item '.join(items)
            return s + '\n\n\\end{itemize}\n\n\\end{quote}\n\n'


    #////////////////////////////////////////////////////////////
    #{ Docstring -> LaTeX Conversion
    #////////////////////////////////////////////////////////////

    # We only need one linker, since we don't use context:
    class _LatexDocstringLinker(markup.DocstringLinker):
        def translate_indexterm(self, indexterm):
            indexstr = re.sub(r'["!|@]', r'"\1', indexterm.to_latex(self))
            return ('\\index{%s}\\textit{%s}' % (indexstr, indexstr))
        def translate_identifier_xref(self, identifier, label=None):
            if label is None: label = markup.plaintext_to_latex(identifier)
            return '\\texttt{%s}' % label
    _docstring_linker = _LatexDocstringLinker()
    
    def docstring_to_latex(self, docstring, indent=0, breakany=0):
        if docstring is None: return ''
        return docstring.to_latex(self._docstring_linker, indent=indent,
                                  hyperref=self._hyperref)
    
    #////////////////////////////////////////////////////////////
    #{ Helpers
    #////////////////////////////////////////////////////////////

    def write_header(self, out, where):
        out('%\n% API Documentation')
        if self._prj_name: out(' for %s' % self._prj_name)
        if isinstance(where, APIDoc):
            out('\n%% %s %s' % (self.doc_kind(where), where.canonical_name))
        else:
            out('\n%% %s' % where)
        out('\n%%\n%% Generated by epydoc %s\n' % epydoc.__version__)
        out('%% [%s]\n%%\n' % time.asctime(time.localtime(time.time())))

    def write_start_of(self, out, section_name):
        out('\n' + 75*'%' + '\n')
        out('%%' + ((71-len(section_name))/2)*' ')
        out(section_name)
        out(((72-len(section_name))/2)*' ' + '%%\n')
        out(75*'%' + '\n\n')

    def section(self, title, depth=0):
        sec = self.SECTIONS[depth+self._top_section]
        return (('%s\n\n' % sec) % plaintext_to_latex(title))                
    
    def sectionstar(self, title, depth):
        sec = self.STARSECTIONS[depth+self._top_section]
        return (('%s\n\n' % sec) % plaintext_to_latex(title))

    def doc_kind(self, doc):
        if isinstance(doc, ModuleDoc) and doc.is_package == True:
            return 'Package'
        elif (isinstance(doc, ModuleDoc) and
              doc.canonical_name[0].startswith('script')):
            return 'Script'
        elif isinstance(doc, ModuleDoc):
            return 'Module'
        elif isinstance(doc, ClassDoc):
            return 'Class'
        elif isinstance(doc, ClassMethodDoc):
            return 'Class Method'
        elif isinstance(doc, StaticMethodDoc):
            return 'Static Method'
        elif isinstance(doc, RoutineDoc):
            if isinstance(self.docindex.container(doc), ClassDoc):
                return 'Method'
            else:
                return 'Function'
        else:
            return 'Variable'

    def indexterm(self, doc, pos='only'):
        """Mark a term or section for inclusion in the index."""
        if not self._index: return ''
        if isinstance(doc, RoutineDoc) and not self._index_functions:
            return ''

        pieces = []
        while doc is not None:
            if doc.canonical_name == UNKNOWN:
                return '' # Give up.
            pieces.append('%s \\textit{(%s)}' %
                          (plaintext_to_latex('%s'%doc.canonical_name),
                           self.doc_kind(doc).lower()))
            doc = self.docindex.container(doc)
            if doc == UNKNOWN:
                return '' # Give up.

        pieces.reverse()
        if pos == 'only':
            return '\\index{%s}\n' % '!'.join(pieces)
        elif pos == 'start':
            return '\\index{%s|(}\n' % '!'.join(pieces)
        elif pos == 'end':
            return '\\index{%s|)}\n' % '!'.join(pieces)
        else:
            raise AssertionError('Bad index position %s' % pos)
        
    def label(self, doc):
        return ':'.join(doc.canonical_name)

    #: Map the Python encoding representation into mismatching LaTeX ones.
    latex_encodings = {
        'utf-8': 'utf8',
    }

    def get_latex_encoding(self):
        """
        @return: The LaTeX representation of the selected encoding.
        @rtype: C{str}
        """
        enc = self._encoding.lower()
        return self.latex_encodings.get(enc, enc)

########NEW FILE########
__FILENAME__ = plaintext
# epydoc -- Plaintext output generation
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: plaintext.py 1473 2007-02-13 19:46:05Z edloper $

"""
Plaintext output generation.
"""
__docformat__ = 'epytext en'

from epydoc.apidoc import *
import re

class PlaintextWriter:
    def write(self, api_doc, **options):
        result = []
        out = result.append

        self._cols = options.get('cols', 75)

        try:
            if isinstance(api_doc, ModuleDoc):
                self.write_module(out, api_doc)
            elif isinstance(api_doc, ClassDoc):
                self.write_class(out, api_doc)
            elif isinstance(api_doc, RoutineDoc):
                self.write_function(out, api_doc)
            else:
                assert 0, ('%s not handled yet' % api_doc.__class__)
        except Exception, e:
            print '\n\n'
            print ''.join(result)
            raise

        return ''.join(result)

    def write_module(self, out, mod_doc):
        #for n,v in mod_doc.variables.items():
        #    print n, `v.value`, `v.value.value`
        
        # The cannonical name of the module.
        out(self.section('Module Name'))
        out('    %s\n\n' % mod_doc.canonical_name)

        # The module's description.
        if mod_doc.descr not in (None, '', UNKNOWN):
            out(self.section('Description'))
            out(mod_doc.descr.to_plaintext(None, indent=4))

        #out('metadata: %s\n\n' % mod_doc.metadata) # [xx] testing

        self.write_list(out, 'Classes', mod_doc, value_type='class')
        self.write_list(out, 'Functions', mod_doc, value_type='function')
        self.write_list(out, 'Variables', mod_doc, value_type='other')
        # hmm.. do this as just a flat list??
        #self.write_list(out, 'Imports', mod_doc, imported=True, verbose=False)

    def baselist(self, class_doc):
        if class_doc.bases is UNKNOWN:
            return '(unknown bases)'
        if len(class_doc.bases) == 0: return ''
        s = '('
        class_parent = class_doc.canonical_name.container()
        for i, base in enumerate(class_doc.bases):
            if base.canonical_name is None:
                if base.parse_repr is not UNKNOWN:
                    s += base.parse_repr
                else:
                    s += '??'
            elif base.canonical_name.container() == class_parent:
                s += str(base.canonical_name[-1])
            else:
                s += str(base.canonical_name)
            if i < len(class_doc.bases)-1: out(', ')
        return s+')'

    def write_class(self, out, class_doc, name=None, prefix='', verbose=True):
        baselist = self.baselist(class_doc)
        
        # If we're at the top level, then list the cannonical name of
        # the class; otherwise, our parent will have already printed
        # the name of the variable containing the class.
        if prefix == '':
            out(self.section('Class Name'))
            out('    %s%s\n\n' % (class_doc.canonical_name, baselist))
        else:
            out(prefix + 'class %s' % self.bold(str(name)) + baselist+'\n')

        if not verbose: return

        # Indent the body
        if prefix != '':
            prefix += ' |  '

        # The class's description.
        if class_doc.descr not in (None, '', UNKNOWN):
            if prefix == '':
                out(self.section('Description', prefix))
                out(self._descr(class_doc.descr, '    '))
            else:
                out(self._descr(class_doc.descr, prefix))

        # List of nested classes in this class.
        self.write_list(out, 'Methods', class_doc,
                        value_type='instancemethod', prefix=prefix,
                        noindent=len(prefix)>4)
        self.write_list(out, 'Class Methods', class_doc,
                        value_type='classmethod', prefix=prefix)
        self.write_list(out, 'Static Methods', class_doc,
                        value_type='staticmethod', prefix=prefix)
        self.write_list(out, 'Nested Classes', class_doc,
                        value_type='class', prefix=prefix)
        self.write_list(out, 'Instance Variables', class_doc,
                        value_type='instancevariable', prefix=prefix)
        self.write_list(out, 'Class Variables', class_doc,
                        value_type='classvariable', prefix=prefix)
        
        self.write_list(out, 'Inherited Methods', class_doc,
                        value_type='method', prefix=prefix,
                        inherited=True, verbose=False)
        self.write_list(out, 'Inherited Instance Variables', class_doc,
                        value_type='instancevariable', prefix=prefix,
                        inherited=True, verbose=False)
        self.write_list(out, 'Inherited Class Variables', class_doc,
                        value_type='classvariable', prefix=prefix,
                        inherited=True, verbose=False)
        self.write_list(out, 'Inherited Nested Classes', class_doc,
                        value_type='class', prefix=prefix,
                        inherited=True, verbose=False)

    def write_variable(self, out, var_doc, name=None, prefix='', verbose=True):
        if name is None: name = var_doc.name
        out(prefix+self.bold(str(name)))
        if (var_doc.value not in (UNKNOWN, None) and
            var_doc.is_alias is True and
            var_doc.value.canonical_name not in (None, UNKNOWN)):
            out(' = %s' % var_doc.value.canonical_name)
        elif var_doc.value not in (UNKNOWN, None):
            val_repr = var_doc.value.summary_pyval_repr(
                max_len=self._cols-len(name)-len(prefix)-3)
            out(' = %s' % val_repr.to_plaintext(None))
        out('\n')
        if not verbose: return
        prefix += '    ' # indent the body.
        if var_doc.descr not in (None, '', UNKNOWN):
            out(self._descr(var_doc.descr, prefix))

    def write_property(self, out, prop_doc, name=None, prefix='',
                       verbose=True):
        if name is None: name = prop_doc.canonical_name
        out(prefix+self.bold(str(name)))
        if not verbose: return
        prefix += '    ' # indent the body.
            
        if prop_doc.descr not in (None, '', UNKNOWN):
            out(self._descr(prop_doc.descr, prefix))


    def write_function(self, out, func_doc, name=None, prefix='',
                       verbose=True):
        if name is None: name = func_doc.canonical_name
        self.write_signature(out, func_doc, name, prefix)
        if not verbose: return
        
        prefix += '    ' # indent the body.
            
        if func_doc.descr not in (None, '', UNKNOWN):
            out(self._descr(func_doc.descr, prefix))

        if func_doc.return_descr not in (None, '', UNKNOWN):
            out(self.section('Returns:', prefix))
            out(self._descr(func_doc.return_descr, prefix+'    '))

        if func_doc.return_type not in (None, '', UNKNOWN):
            out(self.section('Return Type:', prefix))
            out(self._descr(func_doc.return_type, prefix+'    '))

    def write_signature(self, out, func_doc, name, prefix):
        args = [self.fmt_arg(argname, default) for (argname, default) 
                in zip(func_doc.posargs, func_doc.posarg_defaults)]
        if func_doc.vararg: args.append('*'+func_doc.vararg)
        if func_doc.kwarg: args.append('**'+func_doc.kwarg)

        out(prefix+self.bold(str(name))+'(')
        x = left = len(prefix) + len(name) + 1
        for i, arg in enumerate(args):
            if x > left and x+len(arg) > 75:
                out('\n'+prefix + ' '*len(name) + ' ')
                x = left
            out(arg)
            x += len(arg)
            if i < len(args)-1:
                out(', ')
                x += 2
        out(')\n')

    # [xx] tuple args!
    def fmt_arg(self, name, default):
        if default is None:
            return '%s' % name
        else:
            default_repr = default.summary_pyval_repr()
            return '%s=%s' % (name, default_repr.to_plaintext(None))

    def write_list(self, out, heading, doc, value_type=None, imported=False,
                   inherited=False, prefix='', noindent=False,
                   verbose=True):
        # Get a list of the VarDocs we should describe.
        if isinstance(doc, ClassDoc):
            var_docs = doc.select_variables(value_type=value_type,
                                            imported=imported,
                                            inherited=inherited)
        else:
            var_docs = doc.select_variables(value_type=value_type,
                                            imported=imported)
        if not var_docs: return

        out(prefix+'\n')
        if not noindent:
            out(self.section(heading, prefix))
            prefix += '    '

        for i, var_doc in enumerate(var_docs):
            val_doc, name = var_doc.value, var_doc.name

            if verbose:
                out(prefix+'\n')

            # hmm:
            if not verbose:
                if isinstance(doc, ClassDoc):
                    name = var_doc.canonical_name
                elif val_doc not in (None, UNKNOWN):
                    name = val_doc.canonical_name
                    
            if isinstance(val_doc, RoutineDoc):
                self.write_function(out, val_doc, name, prefix, verbose)
            elif isinstance(val_doc, PropertyDoc):
                self.write_property(out, val_doc, name, prefix, verbose)
            elif isinstance(val_doc, ClassDoc):
                self.write_class(out, val_doc, name, prefix, verbose)
            else:
                self.write_variable(out, var_doc, name, prefix, verbose)

    def _descr(self, descr, prefix):
        s = descr.to_plaintext(None, indent=len(prefix)).rstrip()
        s = '\n'.join([(prefix+l[len(prefix):]) for l in s.split('\n')])
        return s+'\n'#+prefix+'\n'
                               

#    def drawline(self, s, x):
#        s = re.sub(r'(?m)^(.{%s}) ' % x, r'\1|', s)
#        return re.sub(r'(?m)^( {,%s})$(?=\n)' % x, x*' '+'|', s)

        
    #////////////////////////////////////////////////////////////
    # Helpers
    #////////////////////////////////////////////////////////////
    
    def bold(self, text):
        """Write a string in bold by overstriking."""
        return ''.join([ch+'\b'+ch for ch in text])

    def title(self, text, indent):
        return ' '*indent + self.bold(text.capitalize()) + '\n\n'

    def section(self, text, indent=''):
        if indent == '':
            return indent + self.bold(text.upper()) + '\n'
        else:
            return indent + self.bold(text.capitalize()) + '\n'



########NEW FILE########
__FILENAME__ = xlink
"""
A Docutils_ interpreted text role for cross-API reference support.

This module allows a Docutils_ document to refer to elements defined in
external API documentation. It is possible to refer to many external API
from the same document.

Each API documentation is assigned a new interpreted text role: using such
interpreted text, an user can specify an object name inside an API
documentation. The system will convert such text into an url and generate a
reference to it. For example, if the API ``db`` is defined, being a database
package, then a certain method may be referred as::

    :db:`Connection.cursor()`

To define a new API, an *index file* must be provided. This file contains
a mapping from the object name to the URL part required to resolve such object.

Index file
----------

Each line in the the index file describes an object.

Each line contains the fully qualified name of the object and the URL at which
the documentation is located. The fields are separated by a ``<tab>``
character.

The URL's in the file are relative from the documentation root: the system can
be configured to add a prefix in front of each returned URL.

Allowed names
-------------

When a name is used in an API text role, it is split over any *separator*.
The separators defined are '``.``', '``::``', '``->``'. All the text from the
first noise char (neither a separator nor alphanumeric or '``_``') is
discarded. The same algorithm is applied when the index file is read.

First the sequence of name parts is looked for in the provided index file.
If no matching name is found, a partial match against the trailing part of the
names in the index is performed. If no object is found, or if the trailing part
of the name may refer to many objects, a warning is issued and no reference
is created.

Configuration
-------------

This module provides the class `ApiLinkReader` a replacement for the Docutils
standalone reader. Such reader specifies the settings required for the
API canonical roles configuration. The same command line options are exposed by
Epydoc.

The script ``apirst2html.py`` is a frontend for the `ApiLinkReader` reader.

API Linking Options::

    --external-api=NAME
                        Define a new API document.  A new interpreted text
                        role NAME will be added.
    --external-api-file=NAME:FILENAME
                        Use records in FILENAME to resolve objects in the API
                        named NAME.
    --external-api-root=NAME:STRING
                        Use STRING as prefix for the URL generated from the
                        API NAME.

.. _Docutils: http://docutils.sourceforge.net/
"""

# $Id: xlink.py 1586 2007-03-14 01:53:42Z dvarrazzo $
__version__ = "$Revision: 1586 $"[11:-2]
__author__ = "Daniele Varrazzo"
__copyright__ = "Copyright (C) 2007 by Daniele Varrazzo"
__docformat__ = 'reStructuredText en'

import re
import sys
from optparse import OptionValueError

from epydoc import log

class UrlGenerator:
    """
    Generate URL from an object name.
    """
    class IndexAmbiguous(IndexError):
        """
        The name looked for is ambiguous
        """

    def get_url(self, name):
        """Look for a name and return the matching URL documentation.

        First look for a fully qualified name. If not found, try with partial
        name.

        If no url exists for the given object, return `None`.

        :Parameters:
          `name` : `str`
            the name to look for

        :return: the URL that can be used to reach the `name` documentation.
            `None` if no such URL exists.
        :rtype: `str`

        :Exceptions:
          - `IndexError`: no object found with `name`
          - `DocUrlGenerator.IndexAmbiguous` : more than one object found with
            a non-fully qualified name; notice that this is an ``IndexError``
            subclass
        """
        raise NotImplementedError

    def get_canonical_name(self, name):
        """
        Convert an object name into a canonical name.

        the canonical name of an object is a tuple of strings containing its
        name fragments, splitted on any allowed separator ('``.``', '``::``',
        '``->``').

        Noise such parenthesis to indicate a function is discarded.

        :Parameters:
          `name` : `str`
            an object name, such as ``os.path.prefix()`` or ``lib::foo::bar``

        :return: the fully qualified name such ``('os', 'path', 'prefix')`` and
            ``('lib', 'foo', 'bar')``
        :rtype: `tuple` of `str`
        """
        rv = []
        for m in self._SEP_RE.finditer(name):
            groups = m.groups()
            if groups[0] is not None:
                rv.append(groups[0])
            elif groups[2] is not None:
                break

        return tuple(rv)

    _SEP_RE = re.compile(r"""(?x)
        # Tokenize the input into keyword, separator, noise
        ([a-zA-Z0-9_]+)         |   # A keyword is a alphanum word
        ( \. | \:\: | \-\> )    |   # These are the allowed separators
        (.)                         # If it doesn't fit, it's noise.
            # Matching a single noise char is enough, because it
            # is used to break the tokenization as soon as some noise
            # is found.
        """)


class VoidUrlGenerator(UrlGenerator):
    """
    Don't actually know any url, but don't report any error.

    Useful if an index file is not available, but a document linking to it
    is to be generated, and warnings are to be avoided.

    Don't report any object as missing, Don't return any url anyway.
    """
    def get_url(self, name):
        return None


class DocUrlGenerator(UrlGenerator):
    """
    Read a *documentation index* and generate URL's for it.
    """
    def __init__(self):
        self._exact_matches = {}
        """
        A map from an object fully qualified name to its URL.

        Values are both the name as tuple of fragments and as read from the
        records (see `load_records()`), mostly to help `_partial_names` to
        perform lookup for unambiguous names.
        """

        self._partial_names= {}
        """
        A map from partial names to the fully qualified names they may refer.

        The keys are the possible left sub-tuples of fully qualified names,
        the values are list of strings as provided by the index.

        If the list for a given tuple contains a single item, the partial
        match is not ambuguous. In this case the string can be looked up in
        `_exact_matches`.

        If the name fragment is ambiguous, a warning may be issued to the user.
        The items can be used to provide an informative message to the user,
        to help him qualifying the name in a unambiguous manner.
        """

        self.prefix = ''
        """
        Prefix portion for the URL's returned by `get_url()`.
        """

        self._filename = None
        """
        Not very important: only for logging.
        """

    def get_url(self, name):
        cname = self.get_canonical_name(name)
        url = self._exact_matches.get(cname, None)
        if url is None:

            # go for a partial match
            vals = self._partial_names.get(cname)
            if vals is None:
                raise IndexError(
                    "no object named '%s' found" % (name))

            elif len(vals) == 1:
                url = self._exact_matches[vals[0]]

            else:
                raise self.IndexAmbiguous(
                    "found %d objects that '%s' may refer to: %s"
                    % (len(vals), name, ", ".join(["'%s'" % n for n in vals])))

        return self.prefix + url

    #{ Content loading
    #  ---------------

    def clear(self):
        """
        Clear the current class content.
        """
        self._exact_matches.clear()
        self._partial_names.clear()

    def load_index(self, f):
        """
        Read the content of an index file.

        Populate the internal maps with the file content using `load_records()`.

        :Parameters:
          f : `str` or file
            a file name or file-like object fron which read the index.
        """
        self._filename = str(f)

        if isinstance(f, basestring):
            f = open(f)

        self.load_records(self._iter_tuples(f))

    def _iter_tuples(self, f):
        """Iterate on a file returning 2-tuples."""
        for nrow, row in enumerate(f):
            # skip blank lines
            row = row.rstrip()
            if not row: continue

            rec = row.split('\t', 2)
            if len(rec) == 2:
                yield rec
            else:
                log.warning("invalid row in '%s' row %d: '%s'"
                            % (self._filename, nrow+1, row))

    def load_records(self, records):
        """
        Read a sequence of pairs name -> url and populate the internal maps.

        :Parameters:
          records : iterable
            the sequence of pairs (*name*, *url*) to add to the maps.
        """
        for name, url in records:
            cname = self.get_canonical_name(name)
            if not cname:
                log.warning("invalid object name in '%s': '%s'"
                    % (self._filename, name))
                continue

            # discard duplicates
            if name in self._exact_matches:
                continue

            self._exact_matches[name] = url
            self._exact_matches[cname] = url

            # Link the different ambiguous fragments to the url
            for i in range(1, len(cname)):
                self._partial_names.setdefault(cname[i:], []).append(name)

#{ API register
#  ------------

api_register = {}
"""
Mapping from the API name to the `UrlGenerator` to be used.

Use `register_api()` to add new generators to the register.
"""

def register_api(name, generator=None):
    """Register the API `name` into the `api_register`.

    A registered API will be available to the markup as the interpreted text
    role ``name``.

    If a `generator` is not provided, register a `VoidUrlGenerator` instance:
    in this case no warning will be issued for missing names, but no URL will
    be generated and all the dotted names will simply be rendered as literals.

    :Parameters:
      `name` : `str`
        the name of the generator to be registered
      `generator` : `UrlGenerator`
        the object to register to translate names into URLs.
    """
    if generator is None:
        generator = VoidUrlGenerator()

    api_register[name] = generator

def set_api_file(name, file):
    """Set an URL generator populated with data from `file`.

    Use `file` to populate a new `DocUrlGenerator` instance and register it
    as `name`.

    :Parameters:
      `name` : `str`
        the name of the generator to be registered
      `file` : `str` or file
        the file to parse populate the URL generator
    """
    generator = DocUrlGenerator()
    generator.load_index(file)
    register_api(name, generator)

def set_api_root(name, prefix):
    """Set the root for the URLs returned by a registered URL generator.

    :Parameters:
      `name` : `str`
        the name of the generator to be updated
      `prefix` : `str`
        the prefix for the generated URL's

    :Exceptions:
      - `IndexError`: `name` is not a registered generator
    """
    api_register[name].prefix = prefix

######################################################################
# Below this point requires docutils.
try:
    import docutils
    from docutils.parsers.rst import roles
    from docutils import nodes, utils
    from docutils.readers.standalone import Reader
except ImportError:
    docutils = roles = nodes = utils = None
    class Reader: settings_spec = ()

def create_api_role(name, problematic):
    """
    Create and register a new role to create links for an API documentation.

    Create a role called `name`, which will use the URL resolver registered as
    ``name`` in `api_register` to create a link for an object.

    :Parameters:
      `name` : `str`
        name of the role to create.
      `problematic` : `bool`
        if True, the registered role will create problematic nodes in
        case of failed references. If False, a warning will be raised
        anyway, but the output will appear as an ordinary literal.
    """
    def resolve_api_name(n, rawtext, text, lineno, inliner,
                options={}, content=[]):
        if docutils is None:
            raise AssertionError('requires docutils')

        # node in monotype font
        text = utils.unescape(text)
        node = nodes.literal(rawtext, text, **options)

        # Get the resolver from the register and create an url from it.
        try:
            url = api_register[name].get_url(text)
        except IndexError, exc:
            msg = inliner.reporter.warning(str(exc), line=lineno)
            if problematic:
                prb = inliner.problematic(rawtext, text, msg)
                return [prb], [msg]
            else:
                return [node], []

        if url is not None:
            node = nodes.reference(rawtext, '', node, refuri=url, **options)
        return [node], []

    roles.register_local_role(name, resolve_api_name)


#{ Command line parsing
#  --------------------


def split_name(value):
    """
    Split an option in form ``NAME:VALUE`` and check if ``NAME`` exists.
    """
    parts = value.split(':', 1)
    if len(parts) != 2:
        raise OptionValueError(
            "option value must be specified as NAME:VALUE; got '%s' instead"
            % value)

    name, val = parts

    if name not in api_register:
        raise OptionValueError(
            "the name '%s' has not been registered; use --external-api"
            % name)

    return (name, val)


class ApiLinkReader(Reader):
    """
    A Docutils standalone reader allowing external documentation links.

    The reader configure the url resolvers at the time `read()` is invoked the
    first time.
    """
    #: The option parser configuration.
    settings_spec = (
    'API Linking Options',
    None,
    ((
        'Define a new API document.  A new interpreted text role NAME will be '
        'added.',
        ['--external-api'],
        {'metavar': 'NAME', 'action': 'append'}
    ), (
        'Use records in FILENAME to resolve objects in the API named NAME.',
        ['--external-api-file'],
        {'metavar': 'NAME:FILENAME', 'action': 'append'}
    ), (
        'Use STRING as prefix for the URL generated from the API NAME.',
        ['--external-api-root'],
        {'metavar': 'NAME:STRING', 'action': 'append'}
    ),)) + Reader.settings_spec

    def __init__(self, *args, **kwargs):
        if docutils is None:
            raise AssertionError('requires docutils')
        Reader.__init__(self, *args, **kwargs)

    def read(self, source, parser, settings):
        self.read_configuration(settings, problematic=True)
        return Reader.read(self, source, parser, settings)

    def read_configuration(self, settings, problematic=True):
        """
        Read the configuration for the configured URL resolver.

        Register a new role for each configured API.

        :Parameters:
          `settings`
            the settings structure containing the options to read.
          `problematic` : `bool`
            if True, the registered role will create problematic nodes in
            case of failed references. If False, a warning will be raised
            anyway, but the output will appear as an ordinary literal.
        """
        # Read config only once
        if hasattr(self, '_conf'):
            return
        ApiLinkReader._conf = True

        try:
            if settings.external_api is not None:
                for name in settings.external_api:
                    register_api(name)
                    create_api_role(name, problematic=problematic)

            if settings.external_api_file is not None:
                for name, file in map(split_name, settings.external_api_file):
                    set_api_file(name, file)

            if settings.external_api_root is not None:
                for name, root in map(split_name, settings.external_api_root):
                    set_api_root(name, root)

        except OptionValueError, exc:
            print >>sys.stderr, "%s: %s" % (exc.__class__.__name__, exc)
            sys.exit(2)

    read_configuration = classmethod(read_configuration)

########NEW FILE########
__FILENAME__ = gui
#!/usr/bin/env python
#
# objdoc: epydoc command-line interface
# Edward Loper
#
# Created [03/15/02 10:31 PM]
# $Id: gui.py 646 2004-03-19 19:01:37Z edloper $
#

"""
Graphical interface to epydoc.  This interface might be useful for
systems where it's inconvenient to use the command-line interface
(such as Windows).  It supports many (but not all) of the features
that are supported by the command-line interface.  It also supports
loading and saving of X{project files}, which store a set of related
modules, and the options that should be used to generate the
documentation for those modules.

Usage::
    epydocgui [OPTIONS] [FILE.prj | MODULES...]

    FILE.prj                  An epydoc GUI project file.
    MODULES...                A list of Python modules to document.
    -V, --version             Print the version of epydoc.
    -h, -?, --help, --usage   Display this usage message
    --debug                   Do not suppress error messages

@todo: Use ini-style project files, rather than pickles (using the
same format as the CLI).
"""
__docformat__ = 'epytext en'

import sys, os.path, re, glob
from Tkinter import *
from tkFileDialog import askopenfilename, asksaveasfilename
from thread import start_new_thread, exit_thread
from pickle import dump, load

# askdirectory is only defined in python 2.2+; fall back on
# asksaveasfilename if it's not available.
try: from tkFileDialog import askdirectory
except: askdirectory = None

# Include support for Zope, if it's available.
try: import ZODB
except: pass

##/////////////////////////////////////////////////////////////////////////
## CONSTANTS
##/////////////////////////////////////////////////////////////////////////

DEBUG = 0

# Colors for tkinter display
BG_COLOR='#e0e0e0'
ACTIVEBG_COLOR='#e0e0e0'
TEXT_COLOR='black'
ENTRYSELECT_COLOR = ACTIVEBG_COLOR
SELECT_COLOR = '#208070'
MESSAGE_COLOR = '#000060'
ERROR_COLOR = '#600000'
GUIERROR_COLOR = '#600000'
WARNING_COLOR = '#604000'
HEADER_COLOR = '#000000'

# Convenience dictionaries for specifying widget colors
COLOR_CONFIG = {'background':BG_COLOR, 'highlightcolor': BG_COLOR,
                'foreground':TEXT_COLOR, 'highlightbackground': BG_COLOR}
ENTRY_CONFIG = {'background':BG_COLOR, 'highlightcolor': BG_COLOR,
                'foreground':TEXT_COLOR, 'highlightbackground': BG_COLOR,
                'selectbackground': ENTRYSELECT_COLOR,
                'selectforeground': TEXT_COLOR}
SB_CONFIG = {'troughcolor':BG_COLOR, 'activebackground':BG_COLOR,
             'background':BG_COLOR, 'highlightbackground':BG_COLOR}
LISTBOX_CONFIG = {'highlightcolor': BG_COLOR, 'highlightbackground': BG_COLOR,
                  'foreground':TEXT_COLOR, 'selectforeground': TEXT_COLOR,
                  'selectbackground': ACTIVEBG_COLOR, 'background':BG_COLOR}
BUTTON_CONFIG = {'background':BG_COLOR, 'highlightthickness':0, 'padx':4, 
                 'highlightbackground': BG_COLOR, 'foreground':TEXT_COLOR,
                 'highlightcolor': BG_COLOR, 'activeforeground': TEXT_COLOR,
                 'activebackground': ACTIVEBG_COLOR, 'pady':0}
CBUTTON_CONFIG = {'background':BG_COLOR, 'highlightthickness':0, 'padx':4, 
                  'highlightbackground': BG_COLOR, 'foreground':TEXT_COLOR,
                  'highlightcolor': BG_COLOR, 'activeforeground': TEXT_COLOR,
                  'activebackground': ACTIVEBG_COLOR, 'pady':0,
                  'selectcolor': SELECT_COLOR}
SHOWMSG_CONFIG = CBUTTON_CONFIG.copy()
SHOWMSG_CONFIG['foreground'] = MESSAGE_COLOR
SHOWWRN_CONFIG = CBUTTON_CONFIG.copy()
SHOWWRN_CONFIG['foreground'] = WARNING_COLOR
SHOWERR_CONFIG = CBUTTON_CONFIG.copy()
SHOWERR_CONFIG['foreground'] = ERROR_COLOR

# Colors for the progress bar
PROGRESS_HEIGHT = 16
PROGRESS_WIDTH = 200
PROGRESS_BG='#305060'
PROGRESS_COLOR1 = '#30c070'
PROGRESS_COLOR2 = '#60ffa0'
PROGRESS_COLOR3 = '#106030'

# On tkinter canvases, where's the zero coordinate?
if sys.platform.lower().startswith('win'):
    DX = 3; DY = 3
    DH = 0; DW = 7
else:
    DX = 1; DY = 1
    DH = 1; DW = 3

# How much of the progress is in each subtask?
IMPORT_PROGRESS = 0.1
BUILD_PROGRESS  = 0.2
WRITE_PROGRESS  = 1.0 - BUILD_PROGRESS - IMPORT_PROGRESS

##/////////////////////////////////////////////////////////////////////////
## IMAGE CONSTANTS
##/////////////////////////////////////////////////////////////////////////

UP_GIF = '''\
R0lGODlhCwAMALMAANnZ2QDMmQCZZgBmZgAAAAAzM////////wAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAACH5BAEAAAAALAAAAAALAAwAAAQjEMhJKxCW4gzCIJxXZIEwFGDlDadqsii1sq1U0nA64+ON
5xEAOw==
'''
DOWN_GIF = '''\
R0lGODlhCwAMALMAANnZ2QDMmQCZZgBmZgAAAAAzM////////wAAAAAAAAAAAAAAAAAAAAAAAAAA
AAAAACH5BAEAAAAALAAAAAALAAwAAAQmEIQxgLVUCsppsVPngVtXEFfIfWk5nBe4xuSL0tKLy/cu
7JffJQIAOw==
'''
LEFT_GIF='''\
R0lGODlhDAALAKIAANnZ2QDMmQCZZgBmZgAAAAAzM////////yH5BAEAAAAALAAAAAAMAAsAAAM4
CLocgaCrESiDoBshOAoAgBEyMzgAEIGCowsiOLoLgEBVOLoIqlSFo4OgC1RYM4Ogq1RYg6DLVJgA
Ow==
'''
RIGHT_GIF='''\
R0lGODlhDAALAKIAANnZ2QDMmQBmZgCZZgAzMwAAAP///////yH5BAEAAAAALAAAAAAMAAsAAAM5
GIGgyzIYgaCrIigTgaALIigyEQiqKLoTgaAoujuDgKJLVAgqIoJEBQAIIkKEhaArRFgIukqFoMsJ
ADs=
'''

##/////////////////////////////////////////////////////////////////////////
## MessageIO
##/////////////////////////////////////////////////////////////////////////

from epydoc import log
from epydoc.util import wordwrap
class GUILogger(log.Logger):
    _STAGES = [40, 7, 1, 3, 1, 30, 1, 2, 100]
    
    def __init__(self, progress, cancel):
        self._progress = progress
        self._cancel = cancel
        self.clear()

    def clear(self):
        self._messages = []
        self._n = 0
        self._stage = 0
        self._message_blocks = []
        
    def log(self, level, message):
        message = wordwrap(str(message)).rstrip() + '\n'
        if self._message_blocks:
            self._message_blocks[-1][-1].append( (level, message) )
        else:
            self._messages.append( (level, message) )

    def start_block(self, header):
        self._message_blocks.append( (header, []) )

    def end_block(self):
        header, messages = self._message_blocks.pop()
        if messages:
            self._messages.append( ('uline', ' '*75+'\n') )
            self.log('header', header)
            self._messages += messages
            self._messages.append( ('uline', ' '*75+'\n') )
        
    def start_progress(self, header=None):
        self.log(log.INFO, header)
        self._stage += 1
        
    def end_progress(self):
        pass
    
    def progress(self, percent, message=''):
        if self._cancel[0]: exit_thread()
        i = self._stage - 1
        p = ((sum(self._STAGES[:i]) + percent*self._STAGES[i]) /
             float(sum(self._STAGES)))
        self._progress[0] = p
        
    def read(self):
        if self._n >= len(self._messages):
            return None, None
        else:
            self._n += 1
            return self._messages[self._n-1]
        
##/////////////////////////////////////////////////////////////////////////
## THREADED DOCUMENTER
##/////////////////////////////////////////////////////////////////////////

def document(options, cancel, done):
    """
    Create the documentation for C{modules}, using the options
    specified by C{options}.  C{document} is designed to be started in
    its own thread by L{EpydocGUI._go}.

    @param options: The options to use for generating documentation.
        This includes keyword options that can be given to
        L{docwriter.html.HTMLWriter}, as well as the option C{target}, which
        controls where the output is written to.
    @type options: C{dictionary}
    """
    from epydoc.docwriter.html import HTMLWriter
    from epydoc.docbuilder import build_doc_index
    import epydoc.docstringparser

    # Set the default docformat.
    docformat = options.get('docformat', 'epytext')
    epydoc.docstringparser.DEFAULT_DOCFORMAT = docformat

    try:
        parse = options['introspect_or_parse'] in ('parse', 'both')
        introspect = options['introspect_or_parse'] in ('introspect', 'both')
        docindex = build_doc_index(options['modules'], parse, introspect)
        html_writer = HTMLWriter(docindex, **options)
        log.start_progress('Writing HTML docs to %r' % options['target'])
        html_writer.write(options['target'])
        log.end_progress()
    
        # We're done.
        log.warning('Finished!')
        done[0] = 'done'

    except SystemExit:
        # Cancel.
        log.error('Cancelled!')
        done[0] ='cancel'
        raise
    except Exception, e:
        # We failed.
        log.error('Internal error: %s' % e)
        done[0] ='cancel'
        raise
    except:
        # We failed.
        log.error('Internal error!')
        done[0] ='cancel'
        raise
    
##/////////////////////////////////////////////////////////////////////////
## GUI
##/////////////////////////////////////////////////////////////////////////

class EpydocGUI:
    """
    A graphical user interace to epydoc.
    """
    def __init__(self):
        self._afterid = 0
        self._progress = [None]
        self._cancel = [0]
        self._filename = None
        self._init_dir = None

        # Store a copy of sys.modules, so that we can restore it
        # later.  This is useful for making sure that we reload
        # everything when we re-build its documentation.  This will
        # *not* reload the modules that are present when the EpydocGUI
        # is created, but that should only contain some builtins, some
        # epydoc modules, Tkinter, pickle, and thread..
        self._old_modules = sys.modules.keys()

        # Create the main window.
        self._root = Tk()
        self._root['background']=BG_COLOR
        self._root.bind('<Control-q>', self.destroy)
        self._root.bind('<Alt-q>', self.destroy)
        self._root.bind('<Alt-x>', self.destroy)
        self._root.bind('<Control-x>', self.destroy)
        #self._root.bind('<Control-d>', self.destroy)
        self._root.title('Epydoc')
        self._rootframe = Frame(self._root, background=BG_COLOR,
                               border=2, relief='raised')
        self._rootframe.pack(expand=1, fill='both', padx=2, pady=2)

        # Set up the basic frames.  Do not pack the options frame or
        # the messages frame; the GUI has buttons to expand them.
        leftframe = Frame(self._rootframe, background=BG_COLOR)
        leftframe.pack(expand=1, fill='both', side='left')
        optsframe = Frame(self._rootframe, background=BG_COLOR)
        mainframe = Frame(leftframe, background=BG_COLOR)
        mainframe.pack(expand=1, fill='both', side='top')
        ctrlframe = Frame(mainframe, background=BG_COLOR)
        ctrlframe.pack(side="bottom", fill='x', expand=0)
        msgsframe = Frame(leftframe, background=BG_COLOR)

        self._optsframe = optsframe
        self._msgsframe = msgsframe

        # Initialize all the frames, etc.
        self._init_menubar()
        self._init_progress_bar(mainframe)
        self._init_module_list(mainframe)
        self._init_options(optsframe, ctrlframe)
        self._init_messages(msgsframe, ctrlframe)
        self._init_bindings()

        # Set up logging
        self._logger = GUILogger(self._progress, self._cancel)
        log.register_logger(self._logger)

        # Open the messages pane by default.
        self._messages_toggle()

        ## For testing options:
        #self._options_toggle()
        
    def _init_menubar(self):
        menubar = Menu(self._root, borderwidth=2,
                       background=BG_COLOR,
                       activebackground=BG_COLOR)
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='New Project', underline=0,
                             command=self._new,
                             accelerator='Ctrl-n')
        filemenu.add_command(label='Open Project', underline=0,
                             command=self._open,
                             accelerator='Ctrl-o')
        filemenu.add_command(label='Save Project', underline=0,
                             command=self._save,
                             accelerator='Ctrl-s')
        filemenu.add_command(label='Save As..', underline=5,
                             command=self._saveas,
                             accelerator='Ctrl-a')
        filemenu.add_separator()
        filemenu.add_command(label='Exit', underline=1,
                             command=self.destroy,
                             accelerator='Ctrl-x')
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
        gomenu = Menu(menubar, tearoff=0)
        gomenu.add_command(label='Run Epydoc',  command=self._open,
                           underline=0, accelerator='Alt-g')
        menubar.add_cascade(label='Run', menu=gomenu, underline=0)
        self._root.config(menu=menubar)
        
    def _init_module_list(self, mainframe):
        mframe1 = Frame(mainframe, relief='groove', border=2,
                        background=BG_COLOR)
        mframe1.pack(side="top", fill='both', expand=1, padx=4, pady=3)
        l = Label(mframe1, text="Modules to document:",
                  justify='left', **COLOR_CONFIG) 
        l.pack(side='top', fill='none', anchor='nw', expand=0)
        mframe2 = Frame(mframe1, background=BG_COLOR)
        mframe2.pack(side="top", fill='both', expand=1)
        mframe3 = Frame(mframe1, background=BG_COLOR)
        mframe3.pack(side="bottom", fill='x', expand=0)
        self._module_list = Listbox(mframe2, width=80, height=10,
                                    selectmode='multiple',
                                    **LISTBOX_CONFIG)
        self._module_list.pack(side="left", fill='both', expand=1)
        sb = Scrollbar(mframe2, orient='vertical',**SB_CONFIG)
        sb['command']=self._module_list.yview
        sb.pack(side='right', fill='y')
        self._module_list.config(yscrollcommand=sb.set)
        Label(mframe3, text="Add:", **COLOR_CONFIG).pack(side='left')
        self._module_entry = Entry(mframe3, **ENTRY_CONFIG)
        self._module_entry.pack(side='left', fill='x', expand=1)
        self._module_entry.bind('<Return>', self._entry_module)
        self._module_delete = Button(mframe3, text="Remove",
                                     command=self._delete_module,
                                     **BUTTON_CONFIG) 
        self._module_delete.pack(side='right', expand=0, padx=2)
        self._module_browse = Button(mframe3, text="Browse",
                                     command=self._browse_module,
                                     **BUTTON_CONFIG) 
        self._module_browse.pack(side='right', expand=0, padx=2)
        
    def _init_progress_bar(self, mainframe):
        pframe1 = Frame(mainframe, background=BG_COLOR)
        pframe1.pack(side="bottom", fill='x', expand=0)
        self._go_button = Button(pframe1, width=4, text='Start',
                                 underline=0, command=self._go,
                                 **BUTTON_CONFIG)
        self._go_button.pack(side='left', padx=4)
        pframe2 = Frame(pframe1, relief='groove', border=2,
                        background=BG_COLOR) 
        pframe2.pack(side="top", fill='x', expand=1, padx=4, pady=3)
        Label(pframe2, text='Progress:', **COLOR_CONFIG).pack(side='left')
        H = self._H = PROGRESS_HEIGHT
        W = self._W = PROGRESS_WIDTH
        c = self._canvas = Canvas(pframe2, height=H+DH, width=W+DW, 
                                  background=PROGRESS_BG, border=0,
                                  selectborderwidth=0, relief='sunken',
                                  insertwidth=0, insertborderwidth=0,
                                  highlightbackground=BG_COLOR)
        self._canvas.pack(side='left', fill='x', expand=1, padx=4)
        self._r2 = c.create_rectangle(0,0,0,0, outline=PROGRESS_COLOR2)
        self._r3 = c.create_rectangle(0,0,0,0, outline=PROGRESS_COLOR3)
        self._r1 = c.create_rectangle(0,0,0,0, fill=PROGRESS_COLOR1,
                                      outline='')
        self._canvas.bind('<Configure>', self._configure)

    def _init_messages(self, msgsframe, ctrlframe):
        self._downImage = PhotoImage(master=self._root, data=DOWN_GIF)
        self._upImage = PhotoImage(master=self._root, data=UP_GIF)

        # Set up the messages control frame
        b1 = Button(ctrlframe, text="Messages", justify='center',
                    command=self._messages_toggle, underline=0, 
                    highlightthickness=0, activebackground=BG_COLOR, 
                    border=0, relief='flat', padx=2, pady=0, **COLOR_CONFIG) 
        b2 = Button(ctrlframe, image=self._downImage, relief='flat', 
                    border=0, command=self._messages_toggle,
                    activebackground=BG_COLOR, **COLOR_CONFIG) 
        self._message_button = b2
        self._messages_visible = 0
        b2.pack(side="left")
        b1.pack(side="left")

        f = Frame(msgsframe, background=BG_COLOR)
        f.pack(side='top', expand=1, fill='both')
        messages = Text(f, width=80, height=10, **ENTRY_CONFIG)
        messages['state'] = 'disabled'
        messages.pack(fill='both', expand=1, side='left')
        self._messages = messages

        # Add a scrollbar
        sb = Scrollbar(f, orient='vertical', **SB_CONFIG)
        sb.pack(fill='y', side='right')
        sb['command'] = messages.yview
        messages['yscrollcommand'] = sb.set

        # Set up some colorization tags
        messages.tag_config('error', foreground=ERROR_COLOR)
        messages.tag_config('warning', foreground=WARNING_COLOR)
        messages.tag_config('guierror', foreground=GUIERROR_COLOR)
        messages.tag_config('message', foreground=MESSAGE_COLOR)
        messages.tag_config('header', foreground=HEADER_COLOR)
        messages.tag_config('uline', underline=1)

        # Keep track of tag state..
        self._in_header = 0
        self._last_tag = 'error'

        # Add some buttons
        buttons = Frame(msgsframe, background=BG_COLOR)
        buttons.pack(side='bottom', fill='x')
        self._show_errors = IntVar(self._root)
        self._show_errors.set(1)
        self._show_warnings = IntVar(self._root)
        self._show_warnings.set(1)
        self._show_messages = IntVar(self._root)
        self._show_messages.set(0)
        Checkbutton(buttons, text='Show Messages', var=self._show_messages,
                    command=self._update_msg_tags, 
                    **SHOWMSG_CONFIG).pack(side='left')
        Checkbutton(buttons, text='Show Warnings', var=self._show_warnings,
                    command=self._update_msg_tags, 
                    **SHOWWRN_CONFIG).pack(side='left')
        Checkbutton(buttons, text='Show Errors', var=self._show_errors,
                    command=self._update_msg_tags, 
                    **SHOWERR_CONFIG).pack(side='left')
        self._update_msg_tags()

    def _update_msg_tags(self, *e):
        elide_errors = not self._show_errors.get()
        elide_warnings = not self._show_warnings.get()
        elide_messages = not self._show_messages.get()
        elide_headers = elide_errors and elide_warnings
        self._messages.tag_config('error', elide=elide_errors)
        self._messages.tag_config('guierror', elide=elide_errors)
        self._messages.tag_config('warning', elide=elide_warnings)
        self._messages.tag_config('message', elide=elide_messages)
        self._messages.tag_config('header', elide=elide_headers)

    def _init_options(self, optsframe, ctrlframe):
        self._leftImage=PhotoImage(master=self._root, data=LEFT_GIF)
        self._rightImage=PhotoImage(master=self._root, data=RIGHT_GIF)

        # Set up the options control frame
        b1 = Button(ctrlframe, text="Options", justify='center',
                    border=0, relief='flat',
                    command=self._options_toggle, padx=2,
                    underline=0, pady=0, highlightthickness=0,
                    activebackground=BG_COLOR, **COLOR_CONFIG) 
        b2 = Button(ctrlframe, image=self._rightImage, relief='flat', 
                    border=0, command=self._options_toggle,
                    activebackground=BG_COLOR, **COLOR_CONFIG) 
        self._option_button = b2
        self._options_visible = 0
        b2.pack(side="right")
        b1.pack(side="right")

        oframe2 = Frame(optsframe, relief='groove', border=2,
                        background=BG_COLOR)
        oframe2.pack(side="right", fill='both',
                     expand=0, padx=4, pady=3, ipadx=4)
        
        Label(oframe2, text="Project Options", font='helvetica -16',
              **COLOR_CONFIG).pack(anchor='w')
        oframe3 = Frame(oframe2, background=BG_COLOR)
        oframe3.pack(fill='x')
        oframe4 = Frame(oframe2, background=BG_COLOR)
        oframe4.pack(fill='x')
        oframe7 = Frame(oframe2, background=BG_COLOR)
        oframe7.pack(fill='x')
        div = Frame(oframe2, background=BG_COLOR, border=1, relief='sunk')
        div.pack(ipady=1, fill='x', padx=4, pady=2)

        Label(oframe2, text="Help File", font='helvetica -16',
              **COLOR_CONFIG).pack(anchor='w')
        oframe5 = Frame(oframe2, background=BG_COLOR)
        oframe5.pack(fill='x')
        div = Frame(oframe2, background=BG_COLOR, border=1, relief='sunk')
        div.pack(ipady=1, fill='x', padx=4, pady=2)

        Label(oframe2, text="CSS Stylesheet", font='helvetica -16',
              **COLOR_CONFIG).pack(anchor='w')
        oframe6 = Frame(oframe2, background=BG_COLOR)
        oframe6.pack(fill='x')

        #==================== oframe3 ====================
        # -n NAME, --name NAME
        row = 0
        l = Label(oframe3, text="Project Name:", **COLOR_CONFIG)
        l.grid(row=row, column=0, sticky='e')
        self._name_entry = Entry(oframe3, **ENTRY_CONFIG)
        self._name_entry.grid(row=row, column=1, sticky='ew', columnspan=3)

        # -u URL, --url URL
        row += 1
        l = Label(oframe3, text="Project URL:", **COLOR_CONFIG)
        l.grid(row=row, column=0, sticky='e')
        self._url_entry = Entry(oframe3, **ENTRY_CONFIG)
        self._url_entry.grid(row=row, column=1, sticky='ew', columnspan=3)

        # -o DIR, --output DIR
        row += 1
        l = Label(oframe3, text="Output Directory:", **COLOR_CONFIG)
        l.grid(row=row, column=0, sticky='e')
        self._out_entry = Entry(oframe3, **ENTRY_CONFIG)
        self._out_entry.grid(row=row, column=1, sticky='ew', columnspan=2)
        self._out_browse = Button(oframe3, text="Browse",
                                  command=self._browse_out,
                                  **BUTTON_CONFIG) 
        self._out_browse.grid(row=row, column=3, sticky='ew', padx=2)

        #==================== oframe4 ====================
        # --no-frames
        row = 0
        self._frames_var = IntVar(self._root)
        self._frames_var.set(1)
        l = Label(oframe4, text="Generate a frame-based table of contents",
                  **COLOR_CONFIG)
        l.grid(row=row, column=1, sticky='w')
        cb = Checkbutton(oframe4, var=self._frames_var, **CBUTTON_CONFIG)
        cb.grid(row=row, column=0, sticky='e')

        # --no-private
        row += 1
        self._private_var = IntVar(self._root)
        self._private_var.set(1)
        l = Label(oframe4, text="Generate documentation for private objects",
                  **COLOR_CONFIG)
        l.grid(row=row, column=1, sticky='w')
        cb = Checkbutton(oframe4, var=self._private_var, **CBUTTON_CONFIG)
        cb.grid(row=row, column=0, sticky='e')

        # --show-imports
        row += 1
        self._imports_var = IntVar(self._root)
        self._imports_var.set(0)
        l = Label(oframe4, text="List imported classes and functions",
                  **COLOR_CONFIG)
        l.grid(row=row, column=1, sticky='w')
        cb = Checkbutton(oframe4, var=self._imports_var, **CBUTTON_CONFIG)
        cb.grid(row=row, column=0, sticky='e')

        #==================== oframe7 ====================
        # --docformat
        row += 1
        l = Label(oframe7, text="Default Docformat:", **COLOR_CONFIG)
        l.grid(row=row, column=0, sticky='e')
        df_var = self._docformat_var = StringVar(self._root)
        self._docformat_var.set('epytext')
        b = Radiobutton(oframe7, var=df_var, text='Epytext',
                        value='epytext', **CBUTTON_CONFIG)
        b.grid(row=row, column=1, sticky='w')
        b = Radiobutton(oframe7, var=df_var, text='ReStructuredText',
                        value='restructuredtext', **CBUTTON_CONFIG)
        b.grid(row=row, column=2, columnspan=2, sticky='w')
        row += 1
        b = Radiobutton(oframe7, var=df_var, text='Plaintext',
                        value='plaintext', **CBUTTON_CONFIG)
        b.grid(row=row, column=1, sticky='w')
        b = Radiobutton(oframe7, var=df_var, text='Javadoc',
                        value='javadoc', **CBUTTON_CONFIG)
        b.grid(row=row, column=2, columnspan=2, sticky='w')
        row += 1

        # Separater
        Frame(oframe7, background=BG_COLOR).grid(row=row, column=1, pady=3)
        row += 1

        # --inheritance
        l = Label(oframe7, text="Inheritance Style:", **COLOR_CONFIG)
        l.grid(row=row, column=0, sticky='e')
        inh_var = self._inheritance_var = StringVar(self._root)
        self._inheritance_var.set('grouped')
        b = Radiobutton(oframe7, var=inh_var, text='Grouped',
                        value='grouped', **CBUTTON_CONFIG)
        b.grid(row=row, column=1, sticky='w')
        b = Radiobutton(oframe7, var=inh_var, text='Listed',
                        value='listed', **CBUTTON_CONFIG)
        b.grid(row=row, column=2, sticky='w')
        b = Radiobutton(oframe7, var=inh_var, text='Included',
                        value='included', **CBUTTON_CONFIG)
        b.grid(row=row, column=3, sticky='w')
        row += 1

        # Separater
        Frame(oframe7, background=BG_COLOR).grid(row=row, column=1, pady=3)
        row += 1

        # --parse-only, --introspect-only
        l = Label(oframe7, text="Get docs from:", **COLOR_CONFIG)
        l.grid(row=row, column=0, sticky='e')
        iop_var = self._introspect_or_parse_var = StringVar(self._root)
        self._introspect_or_parse_var.set('both')
        b = Radiobutton(oframe7, var=iop_var, text='Parsing',
                        value='parse', **CBUTTON_CONFIG)
        b.grid(row=row, column=1, sticky='w')
        b = Radiobutton(oframe7, var=iop_var, text='Introspecting',
                        value='introspect', **CBUTTON_CONFIG)
        b.grid(row=row, column=2, sticky='w')
        b = Radiobutton(oframe7, var=iop_var, text='Both',
                        value='both', **CBUTTON_CONFIG)
        b.grid(row=row, column=3, sticky='w')
        row += 1

        #==================== oframe5 ====================
        # --help-file FILE
        row = 0
        self._help_var = StringVar(self._root)
        self._help_var.set('default')
        b = Radiobutton(oframe5, var=self._help_var,
                        text='Default',
                        value='default', **CBUTTON_CONFIG)
        b.grid(row=row, column=1, sticky='w')
        row += 1
        b = Radiobutton(oframe5, var=self._help_var,
                        text='Select File',
                        value='-other-', **CBUTTON_CONFIG)
        b.grid(row=row, column=1, sticky='w')
        self._help_entry = Entry(oframe5, **ENTRY_CONFIG)
        self._help_entry.grid(row=row, column=2, sticky='ew')
        self._help_browse = Button(oframe5, text='Browse',
                                   command=self._browse_help,
                                   **BUTTON_CONFIG)
        self._help_browse.grid(row=row, column=3, sticky='ew', padx=2)
        
        from epydoc.docwriter.html_css import STYLESHEETS
        items = STYLESHEETS.items()
        def _css_sort(css1, css2):
            if css1[0] == 'default': return -1
            elif css2[0] == 'default': return 1
            else: return cmp(css1[0], css2[0])
        items.sort(_css_sort)

        #==================== oframe6 ====================
        # -c CSS, --css CSS
        # --private-css CSS
        row = 0
        #l = Label(oframe6, text="Public", **COLOR_CONFIG)
        #l.grid(row=row, column=0, sticky='e')
        #l = Label(oframe6, text="Private", **COLOR_CONFIG)
        #l.grid(row=row, column=1, sticky='w')
        row += 1
        css_var = self._css_var = StringVar(self._root)
        css_var.set('default')
        #private_css_var = self._private_css_var = StringVar(self._root)
        #private_css_var.set('default')
        for (name, (sheet, descr)) in items:
            b = Radiobutton(oframe6, var=css_var, value=name, **CBUTTON_CONFIG)
            b.grid(row=row, column=0, sticky='e')
            #b = Radiobutton(oframe6, var=private_css_var, value=name, 
            #                text=name, **CBUTTON_CONFIG)
            #b.grid(row=row, column=1, sticky='w')
            l = Label(oframe6, text=descr, **COLOR_CONFIG)
            l.grid(row=row, column=1, sticky='w')
            row += 1
        b = Radiobutton(oframe6, var=css_var, value='-other-',
                        **CBUTTON_CONFIG)
        b.grid(row=row, column=0, sticky='e')
        #b = Radiobutton(oframe6, text='Select File', var=private_css_var, 
        #                value='-other-', **CBUTTON_CONFIG)
        #b.grid(row=row, column=1, sticky='w')
        #l = Label(oframe6, text='Select File', **COLOR_CONFIG)
        #l.grid(row=row, column=1, sticky='w')
        self._css_entry = Entry(oframe6, **ENTRY_CONFIG)
        self._css_entry.grid(row=row, column=1, sticky='ew')
        self._css_browse = Button(oframe6, text="Browse",
                                  command=self._browse_css,
                                  **BUTTON_CONFIG) 
        self._css_browse.grid(row=row, column=2, sticky='ew', padx=2)

    def _init_bindings(self):
        self._root.bind('<Delete>', self._delete_module)
        self._root.bind('<Alt-o>', self._options_toggle)
        self._root.bind('<Alt-m>', self._messages_toggle)
        self._root.bind('<F5>', self._go)
        self._root.bind('<Alt-s>', self._go)
        
        self._root.bind('<Control-n>', self._new)
        self._root.bind('<Control-o>', self._open)
        self._root.bind('<Control-s>', self._save)
        self._root.bind('<Control-a>', self._saveas)

    def _options_toggle(self, *e):
        if self._options_visible:
            self._optsframe.forget()
            self._option_button['image'] = self._rightImage
            self._options_visible = 0
        else:
            self._optsframe.pack(fill='both', side='right')
            self._option_button['image'] = self._leftImage
            self._options_visible = 1

    def _messages_toggle(self, *e):
        if self._messages_visible:
            self._msgsframe.forget()
            self._message_button['image'] = self._rightImage
            self._messages_visible = 0
        else:
            self._msgsframe.pack(fill='both', side='bottom', expand=1)
            self._message_button['image'] = self._leftImage
            self._messages_visible = 1

    def _configure(self, event):
        self._W = event.width-DW

    def _delete_module(self, *e):
        selection = self._module_list.curselection()
        if len(selection) != 1: return
        self._module_list.delete(selection[0])

    def _entry_module(self, *e):
        modules = [self._module_entry.get()]
        if glob.has_magic(modules[0]):
            modules = glob.glob(modules[0])
        for name in modules:
            self.add_module(name, check=1)
        self._module_entry.delete(0, 'end')

    def _browse_module(self, *e):
        title = 'Select a module for documentation'
        ftypes = [('Python module', '.py'),
                  ('Python extension', '.so'),
                  ('All files', '*')]
        filename = askopenfilename(filetypes=ftypes, title=title,
                                   defaultextension='.py',
                                   initialdir=self._init_dir)
        if not filename: return
        self._init_dir = os.path.dirname(filename)
        self.add_module(filename, check=1)
        
    def _browse_css(self, *e):
        title = 'Select a CSS stylesheet'
        ftypes = [('CSS Stylesheet', '.css'), ('All files', '*')]
        filename = askopenfilename(filetypes=ftypes, title=title,
                                   defaultextension='.css')
        if not filename: return
        self._css_entry.delete(0, 'end')
        self._css_entry.insert(0, filename)

    def _browse_help(self, *e):
        title = 'Select a help file'
        self._help_var.set('-other-')
        ftypes = [('HTML file', '.html'), ('All files', '*')]
        filename = askopenfilename(filetypes=ftypes, title=title,
                                   defaultextension='.html')
        if not filename: return
        self._help_entry.delete(0, 'end')
        self._help_entry.insert(0, filename)

    def _browse_out(self, *e):
        ftypes = [('All files', '*')]
        title = 'Choose the output directory'
        if askdirectory is not None:
            filename = askdirectory(mustexist=0, title=title)
            if not filename: return
        else:
            # Hack for Python 2.1 or earlier:
            filename = asksaveasfilename(filetypes=ftypes, title=title,
                                         initialfile='--this directory--')
            if not filename: return
            (f1, f2) = os.path.split(filename)
            if f2 == '--this directory--': filename = f1
        self._out_entry.delete(0, 'end')
        self._out_entry.insert(0, filename)

    def destroy(self, *e):
        if self._root is None: return

        # Unload any modules that we've imported
        for m in sys.modules.keys():
            if m not in self._old_modules: del sys.modules[m]
        self._root.destroy()
        self._root = None

    def add_module(self, name, check=0):
        from epydoc.util import is_package_dir, is_pyname, is_module_file
        from epydoc.docintrospecter import get_value_from_name
        from epydoc.docintrospecter import get_value_from_filename

        if (os.path.isfile(name) or is_package_dir(name) or is_pyname(name)):
            # Check that it's a good module, if requested.
            if check:
                try:
                    if is_module_file(name) or is_package_dir(name):
                        get_value_from_filename(name)
                    elif os.path.isfile(name):
                        get_value_from_scriptname(name)
                    else:
                        get_value_from_name(name)
                except ImportError, e:
                    log.error(e)
                    self._update_messages()
                    self._root.bell()
                    return
            
            # Add the module to the list of modules.
            self._module_list.insert('end', name)
            self._module_list.yview('end')
        else:
            log.error("Couldn't find %r" % name)
            self._update_messages()
            self._root.bell()
        
    def mainloop(self, *args, **kwargs):
        self._root.mainloop(*args, **kwargs)

    def _getopts(self):
        options = {}
        options['modules'] = self._module_list.get(0, 'end')
        options['prj_name'] = self._name_entry.get() or ''
        options['prj_url'] = self._url_entry.get() or None
        options['docformat'] = self._docformat_var.get()
        options['inheritance'] = self._inheritance_var.get()
        options['introspect_or_parse'] = self._introspect_or_parse_var.get()
        options['target'] = self._out_entry.get() or 'html'
        options['frames'] = self._frames_var.get()
        options['private'] = self._private_var.get()
        options['show_imports'] = self._imports_var.get()
        if self._help_var.get() == '-other-':
            options['help'] = self._help_entry.get() or None
        else:
            options['help'] = None
        if self._css_var.get() == '-other-':
            options['css'] = self._css_entry.get() or 'default'
        else:
            options['css'] = self._css_var.get() or 'default'
        #if self._private_css_var.get() == '-other-':
        #    options['private_css'] = self._css_entry.get() or 'default'
        #else:
        #    options['private_css'] = self._private_css_var.get() or 'default'
        return options
    
    def _go(self, *e):
        if len(self._module_list.get(0,'end')) == 0:
            self._root.bell()
            return

        if self._progress[0] != None:
            self._cancel[0] = 1
            return

        # Construct the argument list for document().
        opts = self._getopts()
        self._progress[0] = 0.0
        self._cancel[0] = 0
        args = (opts, self._cancel, self._progress)

        # Clear the messages window.
        self._messages['state'] = 'normal'
        self._messages.delete('0.0', 'end')
        self._messages['state'] = 'disabled'
        self._logger.clear()

        # Restore the module list.  This will force re-loading of
        # anything that we're documenting.
        for m in sys.modules.keys():
            if m not in self._old_modules:
                del sys.modules[m]

        # [xx] Reset caches??
    
        # Start documenting
        start_new_thread(document, args)

        # Start the progress bar.
        self._go_button['text'] = 'Stop'
        self._afterid += 1
        dt = 300 # How often to update, in milliseconds
        self._update(dt, self._afterid)

    def _update_messages(self):
        while 1:
            level, data = self._logger.read()
            if data is None: break
            self._messages['state'] = 'normal'
            if level == 'header':
                self._messages.insert('end', data, 'header')
            elif level == 'uline':
                self._messages.insert('end', data, 'uline header')
            elif level >= log.ERROR:
                data= data.rstrip()+'\n\n'
                self._messages.insert('end', data, 'guierror')
            elif level >= log.DOCSTRING_WARNING:
                data= data.rstrip()+'\n\n'
                self._messages.insert('end', data, 'warning')
            elif log >= log.INFO:
                data= data.rstrip()+'\n\n'
                self._messages.insert('end', data, 'message')
#                 if data == '\n':
#                     if self._last_tag != 'header2':
#                         self._messages.insert('end', '\n', self._last_tag)
#                 elif data == '='*75:
#                     if self._messages.get('end-3c', 'end') == '\n\n\n':
#                         self._messages.delete('end-1c')
#                     self._in_header = 1
#                     self._messages.insert('end', ' '*75, 'uline header')
#                     self._last_tag = 'header'
#                 elif data == '-'*75:
#                     self._in_header = 0
#                     self._last_tag = 'header2'
#                 elif self._in_header:
#                     self._messages.insert('end', data, 'header')
#                     self._last_tag = 'header'
#                 elif re.match(r'\s*(L\d+:|-)?\s*Warning: ', data):
#                     self._messages.insert('end', data, 'warning')
#                     self._last_tag = 'warning'
#                 else:
#                     self._messages.insert('end', data, 'error')
#                     self._last_tag = 'error'

            self._messages['state'] = 'disabled'
            self._messages.yview('end')

    def _update(self, dt, id):
        if self._root is None: return
        if self._progress[0] is None: return
        if id != self._afterid: return

        # Update the messages box
        self._update_messages()

        # Update the progress bar.
        if self._progress[0] == 'done': p = self._W + DX
        elif self._progress[0] == 'cancel': p = -5
        else: p = DX + self._W * self._progress[0]
        self._canvas.coords(self._r1, DX+1, DY+1, p, self._H+1)
        self._canvas.coords(self._r2, DX, DY, p-1, self._H)
        self._canvas.coords(self._r3, DX+1, DY+1, p, self._H+1)

        # Are we done?
        if self._progress[0] in ('done', 'cancel'):
            if self._progress[0] == 'cancel': self._root.bell()
            self._go_button['text'] = 'Start'
            self._progress[0] = None
            return

        self._root.after(dt, self._update, dt, id)

    def _new(self, *e):
        self._module_list.delete(0, 'end')
        self._name_entry.delete(0, 'end')
        self._url_entry.delete(0, 'end')
        self._docformat_var.set('epytext')
        self._inheritance_var.set('grouped')
        self._introspect_or_parse_var.set('both')
        self._out_entry.delete(0, 'end')
        self._module_entry.delete(0, 'end')
        self._css_entry.delete(0, 'end')
        self._help_entry.delete(0, 'end')
        self._frames_var.set(1)
        self._private_var.set(1)
        self._imports_var.set(0)
        self._css_var.set('default')
        #self._private_css_var.set('default')
        self._help_var.set('default')
        self._filename = None
        self._init_dir = None

    def _open(self, *e):
        title = 'Open project'
        ftypes = [('Project file', '.prj'),
                  ('All files', '*')]
        filename = askopenfilename(filetypes=ftypes, title=title,
                                   defaultextension='.css')
        if not filename: return
        self.open(filename)

    def open(self, prjfile):
        from epydoc.docwriter.html_css import STYLESHEETS
        self._filename = prjfile
        try:
            opts = load(open(prjfile, 'r'))
            
            modnames = list(opts.get('modules', []))
            modnames.sort()
            self._module_list.delete(0, 'end')
            for name in modnames:
                self.add_module(name)
            self._module_entry.delete(0, 'end')
                
            self._name_entry.delete(0, 'end')
            if opts.get('prj_name'):
                self._name_entry.insert(0, opts['prj_name'])
                
            self._url_entry.delete(0, 'end')
            if opts.get('prj_url'):
                self._url_entry.insert(0, opts['prj_url'])

            self._docformat_var.set(opts.get('docformat', 'epytext'))
            self._inheritance_var.set(opts.get('inheritance', 'grouped'))
            self._introspect_or_parse_var.set(
                opts.get('introspect_or_parse', 'both'))

            self._help_entry.delete(0, 'end')
            if opts.get('help') is None:
                self._help_var.set('default')
            else:
                self._help_var.set('-other-')
                self._help_entry.insert(0, opts.get('help'))
                
            self._out_entry.delete(0, 'end')
            self._out_entry.insert(0, opts.get('target', 'html'))

            self._frames_var.set(opts.get('frames', 1))
            self._private_var.set(opts.get('private', 1))
            self._imports_var.set(opts.get('show_imports', 0))
            
            self._css_entry.delete(0, 'end')
            if opts.get('css', 'default') in STYLESHEETS.keys():
                self._css_var.set(opts.get('css', 'default'))
            else:
                self._css_var.set('-other-')
                self._css_entry.insert(0, opts.get('css', 'default'))

            #if opts.get('private_css', 'default') in STYLESHEETS.keys():
            #    self._private_css_var.set(opts.get('private_css', 'default'))
            #else:
            #    self._private_css_var.set('-other-')
            #    self._css_entry.insert(0, opts.get('private_css', 'default'))
                                                   
        except Exception, e:
            log.error('Error opening %s: %s' % (prjfile, e))
            self._root.bell()
        
    def _save(self, *e):
        if self._filename is None: return self._saveas()
        try:
            opts = self._getopts()
            dump(opts, open(self._filename, 'w'))
        except Exception, e:
            if self._filename is None:
                log.error('Error saving: %s' %  e)
            else:
                log.error('Error saving %s: %s' % (self._filename, e))
            self._root.bell()
             
    def _saveas(self, *e):
        title = 'Save project as'
        ftypes = [('Project file', '.prj'), ('All files', '*')]
        filename = asksaveasfilename(filetypes=ftypes, title=title,
                                     defaultextension='.prj')
        if not filename: return
        self._filename = filename
        self._save()

def _version():
    """
    Display the version information, and exit.
    @rtype: C{None}
    """
    import epydoc
    print "Epydoc version %s" % epydoc.__version__
    sys.exit(0)

# At some point I could add:
#   --show-messages, --hide-messages
#   --show-options, --hide-options
def _usage():
    print
    print 'Usage: epydocgui [OPTIONS] [FILE.prj | MODULES...]'
    print
    print '    FILE.prj                  An epydoc GUI project file.'
    print '    MODULES...                A list of Python modules to document.'
    print '    -V, --version             Print the version of epydoc.'
    print '    -h, -?, --help, --usage   Display this usage message'
    print '    --debug                   Do not suppress error messages'
    print
    sys.exit(0)

def _error(s):
    s = '%s; run "%s -h" for usage' % (s, os.path.basename(sys.argv[0]))
    if len(s) > 80:
        i = s.rfind(' ', 0, 80)
        if i>0: s = s[:i]+'\n'+s[i+1:]
    print >>sys.stderr, s
    sys.exit(1)
    
def gui():
    global DEBUG
    sys.stderr = sys.__stderr__
    projects = []
    modules = []
    for arg in sys.argv[1:]:
        if arg[0] == '-':
            if arg != '-V': arg = arg.lower()
            if arg in ('-h', '--help', '-?', '--usage'): _usage()
            elif arg in ('-V', '--version'): _version()
            elif arg in ('--debug',): DEBUG = 1
            else:
                _error('Unknown parameter %r' % arg)
        elif arg[-4:] == '.prj': projects.append(arg)
        else: modules.append(arg)

    if len(projects) > 1:
        _error('Too many projects')
    if len(projects) == 1:
        if len(modules) > 0:
            _error('You must specify either a project or a list of modules')
        if not os.path.exists(projects[0]):
            _error('Cannot open project file %s' % projects[0])
        gui = EpydocGUI()
        gui.open(projects[0])
        gui.mainloop()
    else:
        gui = EpydocGUI()
        for module in modules: gui.add_module(module, check=1)
        gui.mainloop()

if __name__ == '__main__': gui()


########NEW FILE########
__FILENAME__ = log
# epydoc -- Logging
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: log.py 1488 2007-02-14 00:34:27Z edloper $

"""
Functions used to report messages and progress updates to the user.
These functions are delegated to zero or more registered L{Logger}
objects, which are responsible for actually presenting the information
to the user.  Different interfaces are free to create and register
their own C{Logger}s, allowing them to present this information in the
manner that is best suited to each interface.

@note: I considered using the standard C{logging} package to provide
this functionality.  However, I found that it would be too difficult
to get that package to provide the behavior I want (esp. with respect
to progress displays; but also with respect to message blocks).

@group Message Severity Levels: DEBUG, INFO, WARNING, ERROR, FATAL
"""
__docformat__ = 'epytext en'

import sys, os

DEBUG = 10
INFO = 20
DOCSTRING_WARNING = 25
WARNING = 30
ERROR = 40
FATAL = 40

######################################################################
# Logger Base Class
######################################################################
class Logger:
    """
    An abstract base class that defines the interface for X{loggers},
    which are used by epydoc to report information back to the user.
    Loggers are responsible for tracking two types of information:
    
        - Messages, such as warnings and errors.
        - Progress on the current task.

    This abstract class allows the command-line interface and the
    graphical interface to each present this information to the user
    in the way that's most natural for each interface.  To set up a
    logger, create a subclass of C{Logger} that overrides all methods,
    and register it using L{register_logger}.
    """
    #////////////////////////////////////////////////////////////
    # Messages
    #////////////////////////////////////////////////////////////

    def log(self, level, message):
        """
        Display a message.

        @param message: The message string to display.  C{message} may
        contain newlines, but does not need to end in a newline.
        @param level: An integer value indicating the severity of the
        message.
        """

    def close(self):
        """
        Perform any tasks needed to close this logger.
        """

    #////////////////////////////////////////////////////////////
    # Message blocks
    #////////////////////////////////////////////////////////////
    
    def start_block(self, header):
        """
        Start a new message block.  Any calls to L{info()},
        L{warning()}, or L{error()} that occur between a call to
        C{start_block} and a corresponding call to C{end_block} will
        be grouped together, and displayed with a common header.
        C{start_block} can be called multiple times (to form nested
        blocks), but every call to C{start_block} I{must} be balanced
        by a call to C{end_block}.
        """
        
    def end_block(self):
        """
        End a warning block.  See L{start_block} for details.
        """

    #////////////////////////////////////////////////////////////
    # Progress bar
    #////////////////////////////////////////////////////////////
    
    def start_progress(self, header=None):
        """
        Begin displaying progress for a new task.  C{header} is a
        description of the task for which progress is being reported.
        Each call to C{start_progress} must be followed by a call to
        C{end_progress} (with no intervening calls to
        C{start_progress}).
        """

    def end_progress(self):
        """
        Finish off the display of progress for the current task.  See
        L{start_progress} for more information.
        """

    def progress(self, percent, message=''):
        """
        Update the progress display.
        
        @param percent: A float from 0.0 to 1.0, indicating how much
            progress has been made.
        @param message: A message indicating the most recent action
            that contributed towards that progress.
        """

class SimpleLogger(Logger):
    def __init__(self, threshold=WARNING):
        self.threshold = threshold
    def log(self, level, message):
        if level >= self.threshold: print message
        
######################################################################
# Logger Registry
######################################################################

_loggers = []
"""
The list of registered logging functions.
"""

def register_logger(logger):
    """
    Register a logger.  Each call to one of the logging functions
    defined by this module will be delegated to each registered
    logger.
    """
    _loggers.append(logger)

def remove_logger(logger):
    _loggers.remove(logger)

######################################################################
# Logging Functions
######################################################################
# The following methods all just delegate to the corresponding 
# methods in the Logger class (above) for each registered logger.

def fatal(*messages):
    """Display the given fatal message."""
    message = ' '.join(['%s' % (m,) for m in messages])
    for logger in _loggers: logger.log(FATAL, message)
    
def error(*messages):
    """Display the given error message."""
    message = ' '.join(['%s' % (m,) for m in messages])
    for logger in _loggers: logger.log(ERROR, message)
    
def warning(*messages):
    """Display the given warning message."""
    message = ' '.join(['%s' % (m,) for m in messages])
    for logger in _loggers: logger.log(WARNING, message)
    
def docstring_warning(*messages):
    """Display the given docstring warning message."""
    message = ' '.join(['%s' % (m,) for m in messages])
    for logger in _loggers: logger.log(DOCSTRING_WARNING, message)
    
def info(*messages):
    """Display the given informational message."""
    message = ' '.join(['%s' % (m,) for m in messages])
    for logger in _loggers: logger.log(INFO, message)
    
def debug(*messages):
    """Display the given debugging message."""
    message = ' '.join(['%s' % (m,) for m in messages])
    for logger in _loggers: logger.log(DEBUG, message)
    
def start_block(header):
    for logger in _loggers: logger.start_block(header)
start_block.__doc__ = Logger.start_block.__doc__
    
def end_block():
    for logger in _loggers: logger.end_block()
end_block.__doc__ = Logger.end_block.__doc__
    
def start_progress(header=None):
    for logger in _loggers: logger.start_progress(header)
start_progress.__doc__ = Logger.start_progress.__doc__
    
def end_progress():
    for logger in _loggers: logger.end_progress()
end_progress.__doc__ = Logger.end_progress.__doc__
    
def progress(percent, message=''):
    for logger in _loggers: logger.progress(percent, '%s' % message)
progress.__doc__ = Logger.progress.__doc__

def close():
    for logger in _loggers: logger.close()

########NEW FILE########
__FILENAME__ = doctest
#
# doctest.py: Syntax Highlighting for doctest blocks
# Edward Loper
#
# Created [06/28/03 02:52 AM]
# $Id: restructuredtext.py 1210 2006-04-10 13:25:50Z edloper $
#

"""
Syntax highlighting for doctest blocks.  This module defines two
functions, L{doctest_to_html()} and L{doctest_to_latex()}, which can
be used to perform syntax highlighting on doctest blocks.  It also
defines the more general C{colorize_doctest()}, which could be used to
do syntac highlighting on doctest blocks with other output formats.
(Both C{doctest_to_html()} and C{doctest_to_latex()} are defined using
C{colorize_doctest()}.)
"""
__docformat__ = 'epytext en'

import re
from epydoc.util import plaintext_to_html, plaintext_to_latex

__all__ = ['doctest_to_html', 'doctest_to_latex',
           'DoctestColorizer', 'XMLDoctestColorizer', 
           'HTMLDoctestColorizer', 'LaTeXDoctestColorizer']

def doctest_to_html(s):
    """
    Perform syntax highlighting on the given doctest string, and
    return the resulting HTML code.  This code consists of a C{<pre>}
    block with class=py-doctest.  Syntax highlighting is performed
    using the following css classes:
    
      - C{py-prompt} -- the Python PS1 prompt (>>>)
      - C{py-more} -- the Python PS2 prompt (...)
      - C{py-keyword} -- a Python keyword (for, if, etc.)
      - C{py-builtin} -- a Python builtin name (abs, dir, etc.)
      - C{py-string} -- a string literal
      - C{py-comment} -- a comment
      - C{py-except} -- an exception traceback (up to the next >>>)
      - C{py-output} -- the output from a doctest block.
      - C{py-defname} -- the name of a function or class defined by
        a C{def} or C{class} statement.
    """
    return HTMLDoctestColorizer().colorize_doctest(s)

def doctest_to_latex(s):
    """
    Perform syntax highlighting on the given doctest string, and
    return the resulting LaTeX code.  This code consists of an
    C{alltt} environment.  Syntax highlighting is performed using 
    the following new latex commands, which must be defined externally:
      - C{\pysrcprompt} -- the Python PS1 prompt (>>>)
      - C{\pysrcmore} -- the Python PS2 prompt (...)
      - C{\pysrckeyword} -- a Python keyword (for, if, etc.)
      - C{\pysrcbuiltin} -- a Python builtin name (abs, dir, etc.)
      - C{\pysrcstring} -- a string literal
      - C{\pysrccomment} -- a comment
      - C{\pysrcexcept} -- an exception traceback (up to the next >>>)
      - C{\pysrcoutput} -- the output from a doctest block.
      - C{\pysrcdefname} -- the name of a function or class defined by
        a C{def} or C{class} statement.
    """
    return LaTeXDoctestColorizer().colorize_doctest(s)

class DoctestColorizer:
    """
    An abstract base class for performing syntax highlighting on
    doctest blocks and other bits of Python code.  Subclasses should
    provide definitions for:

      - The L{markup()} method, which takes a substring and a tag, and
        returns a colorized version of the substring.
      - The L{PREFIX} and L{SUFFIX} variables, which will be added
        to the beginning and end of the strings returned by
        L{colorize_codeblock} and L{colorize_doctest}.  
    """

    #: A string that is added to the beginning of the strings
    #: returned by L{colorize_codeblock} and L{colorize_doctest}.
    #: Typically, this string begins a preformatted area.
    PREFIX = None

    #: A string that is added to the end of the strings
    #: returned by L{colorize_codeblock} and L{colorize_doctest}.
    #: Typically, this string ends a preformatted area.
    SUFFIX = None

    #: A list of the names of all Python keywords.  ('as' is included
    #: even though it is technically not a keyword.)
    _KEYWORDS = ("and       del       for       is        raise"
                 "assert    elif      from      lambda    return"
                 "break     else      global    not       try"
                 "class     except    if        or        while"
                 "continue  exec      import    pass      yield"
                 "def       finally   in        print     as").split()

    #: A list of all Python builtins.
    _BUILTINS = [_BI for _BI in dir(__builtins__)
                 if not _BI.startswith('__')]

    #: A regexp group that matches keywords.
    _KEYWORD_GRP = '|'.join([r'\b%s\b' % _KW for _KW in _KEYWORDS])

    #: A regexp group that matches Python builtins.
    _BUILTIN_GRP = (r'(?<!\.)(?:%s)' % '|'.join([r'\b%s\b' % _BI
                                                 for _BI in _BUILTINS]))

    #: A regexp group that matches Python strings.
    _STRING_GRP = '|'.join(
        [r'("""("""|.*?((?!").)"""))', r'("("|.*?((?!").)"))',
         r"('''('''|.*?[^\\']'''))", r"('('|.*?[^\\']'))"])

    #: A regexp group that matches Python comments.
    _COMMENT_GRP = '(#.*?$)'

    #: A regexp group that matches Python ">>>" prompts.
    _PROMPT1_GRP = r'^[ \t]*>>>(?:[ \t]|$)'
    
    #: A regexp group that matches Python "..." prompts.
    _PROMPT2_GRP = r'^[ \t]*\.\.\.(?:[ \t]|$)'

    #: A regexp group that matches function and class definitions.
    _DEFINE_GRP = r'\b(?:def|class)[ \t]+\w+'

    #: A regexp that matches Python prompts
    PROMPT_RE = re.compile('(%s|%s)' % (_PROMPT1_GRP, _PROMPT2_GRP),
                           re.MULTILINE | re.DOTALL)

    #: A regexp that matches Python "..." prompts.
    PROMPT2_RE = re.compile('(%s)' % _PROMPT2_GRP,
                            re.MULTILINE | re.DOTALL)

    #: A regexp that matches doctest exception blocks.
    EXCEPT_RE = re.compile(r'^[ \t]*Traceback \(most recent call last\):.*',
                           re.DOTALL | re.MULTILINE)

    #: A regexp that matches doctest directives.
    DOCTEST_DIRECTIVE_RE = re.compile(r'#[ \t]*doctest:.*')

    #: A regexp that matches all of the regions of a doctest block
    #: that should be colored.
    DOCTEST_RE = re.compile(
        r'(.*?)((?P<STRING>%s)|(?P<COMMENT>%s)|(?P<DEFINE>%s)|'
              r'(?P<KEYWORD>%s)|(?P<BUILTIN>%s)|'
              r'(?P<PROMPT1>%s)|(?P<PROMPT2>%s)|(?P<EOS>\Z))' % (
        _STRING_GRP, _COMMENT_GRP, _DEFINE_GRP, _KEYWORD_GRP, _BUILTIN_GRP,
        _PROMPT1_GRP, _PROMPT2_GRP), re.MULTILINE | re.DOTALL)

    #: This regular expression is used to find doctest examples in a
    #: string.  This is copied from the standard Python doctest.py
    #: module (after the refactoring in Python 2.4+).
    DOCTEST_EXAMPLE_RE = re.compile(r'''
        # Source consists of a PS1 line followed by zero or more PS2 lines.
        (?P<source>
            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line
            (?:\n           [ ]*  \.\.\. .*)*   # PS2 lines
          \n?)
        # Want consists of any non-blank lines that do not start with PS1.
        (?P<want> (?:(?![ ]*$)    # Not a blank line
                     (?![ ]*>>>)  # Not a line starting with PS1
                     .*$\n?       # But any other line
                  )*)
        ''', re.MULTILINE | re.VERBOSE)

    def colorize_inline(self, s):
        """
        Colorize a string containing Python code.  Do not add the
        L{PREFIX} and L{SUFFIX} strings to the returned value.  This
        method is intended for generating syntax-highlighted strings
        that are appropriate for inclusion as inline expressions.
        """
        return self.DOCTEST_RE.sub(self.subfunc, s)

    def colorize_codeblock(self, s):
        """
        Colorize a string containing only Python code.  This method
        differs from L{colorize_doctest} in that it will not search
        for doctest prompts when deciding how to colorize the string.
        """
        body = self.DOCTEST_RE.sub(self.subfunc, s)
        return self.PREFIX + body + self.SUFFIX

    def colorize_doctest(self, s, strip_directives=False):
        """
        Colorize a string containing one or more doctest examples.
        """
        output = []
        charno = 0
        for m in self.DOCTEST_EXAMPLE_RE.finditer(s):
            # Parse the doctest example:
            pysrc, want = m.group('source', 'want')
            # Pre-example text:
            output.append(s[charno:m.start()])
            # Example source code:
            output.append(self.DOCTEST_RE.sub(self.subfunc, pysrc))
            # Example output:
            if want:
                if self.EXCEPT_RE.match(want):
                    output += '\n'.join([self.markup(line, 'except')
                                         for line in want.split('\n')])
                else:
                    output += '\n'.join([self.markup(line, 'output')
                                         for line in want.split('\n')])
            # Update charno
            charno = m.end()
        # Add any remaining post-example text.
        output.append(s[charno:])
        
        return self.PREFIX + ''.join(output) + self.SUFFIX
    
    def subfunc(self, match):
        other, text = match.group(1, 2)
        #print 'M %20r %20r' % (other, text) # <- for debugging
        if other:
            other = '\n'.join([self.markup(line, 'other')
                               for line in other.split('\n')])
            
        if match.group('PROMPT1'):
            return other + self.markup(text, 'prompt')
        elif match.group('PROMPT2'):
            return other + self.markup(text, 'more')
        elif match.group('KEYWORD'):
            return other + self.markup(text, 'keyword')
        elif match.group('BUILTIN'):
            return other + self.markup(text, 'builtin')
        elif match.group('COMMENT'):
            return other + self.markup(text, 'comment')
        elif match.group('STRING') and '\n' not in text:
            return other + self.markup(text, 'string')
        elif match.group('STRING'):
            # It's a multiline string; colorize the string & prompt
            # portion of each line.
            pieces = []
            for line in text.split('\n'):
                if self.PROMPT2_RE.match(line):
                    if len(line) > 4:
                        pieces.append(self.markup(line[:4], 'more') +
                                      self.markup(line[4:], 'string'))
                    else:
                        pieces.append(self.markup(line[:4], 'more'))
                elif line:
                    pieces.append(self.markup(line, 'string'))
                else:
                    pieces.append('')
            return other + '\n'.join(pieces)
        elif match.group('DEFINE'):
            m = re.match('(?P<def>\w+)(?P<space>\s+)(?P<name>\w+)', text)
            return other + (self.markup(m.group('def'), 'keyword') +
                        self.markup(m.group('space'), 'other') +
                        self.markup(m.group('name'), 'defname'))
        elif match.group('EOS') is not None:
            return other
        else:
            assert 0, 'Unexpected match!'

    def markup(self, s, tag):
        """
        Apply syntax highlighting to a single substring from a doctest
        block.  C{s} is the substring, and C{tag} is the tag that
        should be applied to the substring.  C{tag} will be one of the
        following strings:
        
          - C{prompt} -- the Python PS1 prompt (>>>)
          - C{more} -- the Python PS2 prompt (...)
          - C{keyword} -- a Python keyword (for, if, etc.)
          - C{builtin} -- a Python builtin name (abs, dir, etc.)
          - C{string} -- a string literal
          - C{comment} -- a comment
          - C{except} -- an exception traceback (up to the next >>>)
          - C{output} -- the output from a doctest block.
          - C{defname} -- the name of a function or class defined by
            a C{def} or C{class} statement.
          - C{other} -- anything else (does *not* include output.)
        """
        raise AssertionError("Abstract method")

class XMLDoctestColorizer(DoctestColorizer):
    """
    A subclass of DoctestColorizer that generates XML-like output.
    This class is mainly intended to be used for testing purposes.
    """
    PREFIX = '<colorized>\n'
    SUFFIX = '</colorized>\n'
    def markup(self, s, tag):
        s = s.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        if tag == 'other': return s
        else: return '<%s>%s</%s>' % (tag, s, tag)

class HTMLDoctestColorizer(DoctestColorizer):
    """A subclass of DoctestColorizer that generates HTML output."""
    PREFIX = '<pre class="py-doctest">\n'
    SUFFIX = '</pre>\n'
    def markup(self, s, tag):
        if tag == 'other':
            return plaintext_to_html(s)
        else:
            return ('<span class="py-%s">%s</span>' %
                    (tag, plaintext_to_html(s)))

class LaTeXDoctestColorizer(DoctestColorizer):
    """A subclass of DoctestColorizer that generates LaTeX output."""
    PREFIX = '\\begin{alltt}\n'
    SUFFIX = '\\end{alltt}\n'
    def markup(self, s, tag):
        if tag == 'other':
            return plaintext_to_latex(s)
        else:
            return '\\pysrc%s{%s}' % (tag, plaintext_to_latex(s))

        

########NEW FILE########
__FILENAME__ = epytext
#
# epytext.py: epydoc formatted docstring parsing
# Edward Loper
#
# Created [04/10/01 12:00 AM]
# $Id: epytext.py 1652 2007-09-26 04:45:34Z edloper $
#

"""
Parser for epytext strings.  Epytext is a lightweight markup whose
primary intended application is Python documentation strings.  This
parser converts Epytext strings to a simple DOM-like representation
(encoded as a tree of L{Element} objects and strings).  Epytext
strings can contain the following X{structural blocks}:

    - X{epytext}: The top-level element of the DOM tree.
    - X{para}: A paragraph of text.  Paragraphs contain no newlines, 
      and all spaces are soft.
    - X{section}: A section or subsection.
    - X{field}: A tagged field.  These fields provide information
      about specific aspects of a Python object, such as the
      description of a function's parameter, or the author of a
      module.
    - X{literalblock}: A block of literal text.  This text should be
      displayed as it would be displayed in plaintext.  The
      parser removes the appropriate amount of leading whitespace 
      from each line in the literal block.
    - X{doctestblock}: A block containing sample python code,
      formatted according to the specifications of the C{doctest}
      module.
    - X{ulist}: An unordered list.
    - X{olist}: An ordered list.
    - X{li}: A list item.  This tag is used both for unordered list
      items and for ordered list items.

Additionally, the following X{inline regions} may be used within
C{para} blocks:
    
    - X{code}:   Source code and identifiers.
    - X{math}:   Mathematical expressions.
    - X{index}:  A term which should be included in an index, if one
                 is generated.
    - X{italic}: Italicized text.
    - X{bold}:   Bold-faced text.
    - X{uri}:    A Universal Resource Indicator (URI) or Universal
                 Resource Locator (URL)
    - X{link}:   A Python identifier which should be hyperlinked to
                 the named object's documentation, when possible.

The returned DOM tree will conform to the the following Document Type
Description::

   <!ENTITY % colorized '(code | math | index | italic |
                          bold | uri | link | symbol)*'>

   <!ELEMENT epytext ((para | literalblock | doctestblock |
                      section | ulist | olist)*, fieldlist?)>

   <!ELEMENT para (#PCDATA | %colorized;)*>

   <!ELEMENT section (para | listblock | doctestblock |
                      section | ulist | olist)+>

   <!ELEMENT fieldlist (field+)>
   <!ELEMENT field (tag, arg?, (para | listblock | doctestblock)
                                ulist | olist)+)>
   <!ELEMENT tag (#PCDATA)>
   <!ELEMENT arg (#PCDATA)>
   
   <!ELEMENT literalblock (#PCDATA | %colorized;)*>
   <!ELEMENT doctestblock (#PCDATA)>

   <!ELEMENT ulist (li+)>
   <!ELEMENT olist (li+)>
   <!ELEMENT li (para | literalblock | doctestblock | ulist | olist)+>
   <!ATTLIST li bullet NMTOKEN #IMPLIED>
   <!ATTLIST olist start NMTOKEN #IMPLIED>

   <!ELEMENT uri     (name, target)>
   <!ELEMENT link    (name, target)>
   <!ELEMENT name    (#PCDATA | %colorized;)*>
   <!ELEMENT target  (#PCDATA)>
   
   <!ELEMENT code    (#PCDATA | %colorized;)*>
   <!ELEMENT math    (#PCDATA | %colorized;)*>
   <!ELEMENT italic  (#PCDATA | %colorized;)*>
   <!ELEMENT bold    (#PCDATA | %colorized;)*>
   <!ELEMENT indexed (#PCDATA | %colorized;)>
   <!ATTLIST code style CDATA #IMPLIED>

   <!ELEMENT symbol (#PCDATA)>

@var SYMBOLS: A list of the of escape symbols that are supported
      by epydoc.  Currently the following symbols are supported:
<<<SYMBOLS>>>
"""
# Note: the symbol list is appended to the docstring automatically,
# below.

__docformat__ = 'epytext en'

# Code organization..
#   1. parse()
#   2. tokenize()
#   3. colorize()
#   4. helpers
#   5. testing

import re, string, types, sys, os.path
from epydoc.markup import *
from epydoc.util import wordwrap, plaintext_to_html, plaintext_to_latex
from epydoc.markup.doctest import doctest_to_html, doctest_to_latex

##################################################
## DOM-Like Encoding
##################################################

class Element:
    """
    A very simple DOM-like representation for parsed epytext
    documents.  Each epytext document is encoded as a tree whose nodes
    are L{Element} objects, and whose leaves are C{string}s.  Each
    node is marked by a I{tag} and zero or more I{attributes}.  Each
    attribute is a mapping from a string key to a string value.
    """
    def __init__(self, tag, *children, **attribs):
        self.tag = tag
        """A string tag indicating the type of this element.
        @type: C{string}"""
        
        self.children = list(children)
        """A list of the children of this element.
        @type: C{list} of (C{string} or C{Element})"""
        
        self.attribs = attribs
        """A dictionary mapping attribute names to attribute values
        for this element.
        @type: C{dict} from C{string} to C{string}"""

    def __str__(self):
        """
        Return a string representation of this element, using XML
        notation.
        @bug: Doesn't escape '<' or '&' or '>'.
        """
        attribs = ''.join([' %s=%r' % t for t in self.attribs.items()])
        return ('<%s%s>' % (self.tag, attribs) +
                ''.join([str(child) for child in self.children]) +
                '</%s>' % self.tag)

    def __repr__(self):
        attribs = ''.join([', %s=%r' % t for t in self.attribs.items()])
        args = ''.join([', %r' % c for c in self.children])
        return 'Element(%s%s%s)' % (self.tag, args, attribs)

##################################################
## Constants
##################################################

# The possible heading underline characters, listed in order of
# heading depth. 
_HEADING_CHARS = "=-~"

# Escape codes.  These should be needed very rarely.
_ESCAPES = {'lb':'{', 'rb': '}'}

# Symbols.  These can be generated via S{...} escapes.
SYMBOLS = [
    # Arrows
    '<-', '->', '^', 'v', 

    # Greek letters
    'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'zeta',  
    'eta', 'theta', 'iota', 'kappa', 'lambda', 'mu',  
    'nu', 'xi', 'omicron', 'pi', 'rho', 'sigma',  
    'tau', 'upsilon', 'phi', 'chi', 'psi', 'omega',
    'Alpha', 'Beta', 'Gamma', 'Delta', 'Epsilon', 'Zeta',  
    'Eta', 'Theta', 'Iota', 'Kappa', 'Lambda', 'Mu',  
    'Nu', 'Xi', 'Omicron', 'Pi', 'Rho', 'Sigma',  
    'Tau', 'Upsilon', 'Phi', 'Chi', 'Psi', 'Omega',
    
    # HTML character entities
    'larr', 'rarr', 'uarr', 'darr', 'harr', 'crarr',
    'lArr', 'rArr', 'uArr', 'dArr', 'hArr', 
    'copy', 'times', 'forall', 'exist', 'part',
    'empty', 'isin', 'notin', 'ni', 'prod', 'sum',
    'prop', 'infin', 'ang', 'and', 'or', 'cap', 'cup',
    'int', 'there4', 'sim', 'cong', 'asymp', 'ne',
    'equiv', 'le', 'ge', 'sub', 'sup', 'nsub',
    'sube', 'supe', 'oplus', 'otimes', 'perp',

    # Alternate (long) names
    'infinity', 'integral', 'product',
    '>=', '<=', 
    ]
# Convert to a dictionary, for quick lookup
_SYMBOLS = {}
for symbol in SYMBOLS: _SYMBOLS[symbol] = 1

# Add symbols to the docstring.
symblist = '      '
symblist += ';\n      '.join([' - C{E{S}{%s}}=S{%s}' % (symbol, symbol)
                              for symbol in SYMBOLS])
__doc__ = __doc__.replace('<<<SYMBOLS>>>', symblist)
del symbol, symblist

# Tags for colorizing text.
_COLORIZING_TAGS = {
    'C': 'code',
    'M': 'math',
    'X': 'indexed',
    'I': 'italic', 
    'B': 'bold',
    'U': 'uri',
    'L': 'link',       # A Python identifier that should be linked to 
    'E': 'escape',     # escapes characters or creates symbols
    'S': 'symbol',
    'G': 'graph',
    }

# Which tags can use "link syntax" (e.g., U{Python<www.python.org>})?
_LINK_COLORIZING_TAGS = ['link', 'uri']

##################################################
## Structuring (Top Level)
##################################################

def parse(str, errors = None):
    """
    Return a DOM tree encoding the contents of an epytext string.  Any
    errors generated during parsing will be stored in C{errors}.

    @param str: The epytext string to parse.
    @type str: C{string}
    @param errors: A list where any errors generated during parsing
        will be stored.  If no list is specified, then fatal errors
        will generate exceptions, and non-fatal errors will be
        ignored.
    @type errors: C{list} of L{ParseError}
    @return: a DOM tree encoding the contents of an epytext string.
    @rtype: C{Element}
    @raise ParseError: If C{errors} is C{None} and an error is
        encountered while parsing.
    """
    # Initialize errors list.
    if errors == None:
        errors = []
        raise_on_error = 1
    else:
        raise_on_error = 0

    # Preprocess the string.
    str = re.sub('\015\012', '\012', str)
    str = string.expandtabs(str)

    # Tokenize the input string.
    tokens = _tokenize(str, errors)

    # Have we encountered a field yet?
    encountered_field = 0

    # Create an document to hold the epytext.
    doc = Element('epytext')

    # Maintain two parallel stacks: one contains DOM elements, and
    # gives the ancestors of the current block.  The other contains
    # indentation values, and gives the indentation of the
    # corresponding DOM elements.  An indentation of "None" reflects
    # an unknown indentation.  However, the indentation must be
    # greater than, or greater than or equal to, the indentation of
    # the prior element (depending on what type of DOM element it
    # corresponds to).  No 2 consecutive indent_stack values will be
    # ever be "None."  Use initial dummy elements in the stack, so we
    # don't have to worry about bounds checking.
    stack = [None, doc]
    indent_stack = [-1, None]

    for token in tokens:
        # Uncomment this for debugging:
        #print ('%s: %s\n%s: %s\n' % 
        #       (''.join(['%-11s' % (t and t.tag) for t in stack]),
        #        token.tag, ''.join(['%-11s' % i for i in indent_stack]),
        #        token.indent))
        
        # Pop any completed blocks off the stack.
        _pop_completed_blocks(token, stack, indent_stack)

        # If Token has type PARA, colorize and add the new paragraph
        if token.tag == Token.PARA:
            _add_para(doc, token, stack, indent_stack, errors)
                     
        # If Token has type HEADING, add the new section
        elif token.tag == Token.HEADING:
            _add_section(doc, token, stack, indent_stack, errors)

        # If Token has type LBLOCK, add the new literal block
        elif token.tag == Token.LBLOCK:
            stack[-1].children.append(token.to_dom(doc))

        # If Token has type DTBLOCK, add the new doctest block
        elif token.tag == Token.DTBLOCK:
            stack[-1].children.append(token.to_dom(doc))

        # If Token has type BULLET, add the new list/list item/field
        elif token.tag == Token.BULLET:
            _add_list(doc, token, stack, indent_stack, errors)
        else:
            assert 0, 'Unknown token type: '+token.tag

        # Check if the DOM element we just added was a field..
        if stack[-1].tag == 'field':
            encountered_field = 1
        elif encountered_field == 1:
            if len(stack) <= 3:
                estr = ("Fields must be the final elements in an "+
                        "epytext string.")
                errors.append(StructuringError(estr, token.startline))

    # Graphs use inline markup (G{...}) but are really block-level
    # elements; so "raise" any graphs we generated.  This is a bit of
    # a hack, but the alternative is to define a new markup for
    # block-level elements, which I'd rather not do.  (See sourceforge
    # bug #1673017.)
    for child in doc.children:
        _raise_graphs(child, doc)

    # If there was an error, then signal it!
    if len([e for e in errors if e.is_fatal()]) > 0:
        if raise_on_error:
            raise errors[0]
        else:
            return None
        
    # Return the top-level epytext DOM element.
    return doc

def _raise_graphs(tree, parent):
    # Recurse to children.
    have_graph_child = False
    for elt in tree.children:
        if isinstance(elt, Element):
            _raise_graphs(elt, tree)
            if elt.tag == 'graph': have_graph_child = True

    block = ('section', 'fieldlist', 'field', 'ulist', 'olist', 'li')
    if have_graph_child and tree.tag not in block:
        child_index = 0
        for elt in tree.children:
            if isinstance(elt, Element) and elt.tag == 'graph':
                # We found a graph: splice it into the parent.
                parent_index = parent.children.index(tree)
                left = tree.children[:child_index]
                right = tree.children[child_index+1:]
                parent.children[parent_index:parent_index+1] = [
                    Element(tree.tag, *left, **tree.attribs),
                    elt,
                    Element(tree.tag, *right, **tree.attribs)]
                child_index = 0
                parent_index += 2
            else:
                child_index += 1

def _pop_completed_blocks(token, stack, indent_stack):
    """
    Pop any completed blocks off the stack.  This includes any
    blocks that we have dedented past, as well as any list item
    blocks that we've dedented to.  The top element on the stack 
    should only be a list if we're about to start a new list
    item (i.e., if the next token is a bullet).
    """
    indent = token.indent
    if indent != None:
        while (len(stack) > 2):
            pop = 0
            
            # Dedent past a block
            if indent_stack[-1]!=None and indent<indent_stack[-1]: pop=1
            elif indent_stack[-1]==None and indent<indent_stack[-2]: pop=1

            # Dedent to a list item, if it is follwed by another list
            # item with the same indentation.
            elif (token.tag == 'bullet' and indent==indent_stack[-2] and 
                  stack[-1].tag in ('li', 'field')): pop=1

            # End of a list (no more list items available)
            elif (stack[-1].tag in ('ulist', 'olist') and
                  (token.tag != 'bullet' or token.contents[-1] == ':')):
                pop=1

            # Pop the block, if it's complete.  Otherwise, we're done.
            if pop == 0: return
            stack.pop()
            indent_stack.pop()

def _add_para(doc, para_token, stack, indent_stack, errors):
    """Colorize the given paragraph, and add it to the DOM tree."""
    # Check indentation, and update the parent's indentation
    # when appropriate.
    if indent_stack[-1] == None:
        indent_stack[-1] = para_token.indent
    if para_token.indent == indent_stack[-1]:
        # Colorize the paragraph and add it.
        para = _colorize(doc, para_token, errors)
        if para_token.inline:
            para.attribs['inline'] = True
        stack[-1].children.append(para)
    else:
        estr = "Improper paragraph indentation."
        errors.append(StructuringError(estr, para_token.startline))

def _add_section(doc, heading_token, stack, indent_stack, errors):
    """Add a new section to the DOM tree, with the given heading."""
    if indent_stack[-1] == None:
        indent_stack[-1] = heading_token.indent
    elif indent_stack[-1] != heading_token.indent:
        estr = "Improper heading indentation."
        errors.append(StructuringError(estr, heading_token.startline))

    # Check for errors.
    for tok in stack[2:]:
        if tok.tag != "section":
            estr = "Headings must occur at the top level."
            errors.append(StructuringError(estr, heading_token.startline))
            break
    if (heading_token.level+2) > len(stack):
        estr = "Wrong underline character for heading."
        errors.append(StructuringError(estr, heading_token.startline))

    # Pop the appropriate number of headings so we're at the
    # correct level.
    stack[heading_token.level+2:] = []
    indent_stack[heading_token.level+2:] = []

    # Colorize the heading
    head = _colorize(doc, heading_token, errors, 'heading')

    # Add the section's and heading's DOM elements.
    sec = Element("section")
    stack[-1].children.append(sec)
    stack.append(sec)
    sec.children.append(head)
    indent_stack.append(None)
        
def _add_list(doc, bullet_token, stack, indent_stack, errors):
    """
    Add a new list item or field to the DOM tree, with the given
    bullet or field tag.  When necessary, create the associated
    list.
    """
    # Determine what type of bullet it is.
    if bullet_token.contents[-1] == '-':
        list_type = 'ulist'
    elif bullet_token.contents[-1] == '.':
        list_type = 'olist'
    elif bullet_token.contents[-1] == ':':
        list_type = 'fieldlist'
    else:
        raise AssertionError('Bad Bullet: %r' % bullet_token.contents)

    # Is this a new list?
    newlist = 0
    if stack[-1].tag != list_type:
        newlist = 1
    elif list_type == 'olist' and stack[-1].tag == 'olist':
        old_listitem = stack[-1].children[-1]
        old_bullet = old_listitem.attribs.get("bullet").split('.')[:-1]
        new_bullet = bullet_token.contents.split('.')[:-1]
        if (new_bullet[:-1] != old_bullet[:-1] or
            int(new_bullet[-1]) != int(old_bullet[-1])+1):
            newlist = 1

    # Create the new list.
    if newlist:
        if stack[-1].tag is 'fieldlist':
            # The new list item is not a field list item (since this
            # is a new list); but it's indented the same as the field
            # list.  This either means that they forgot to indent the
            # list, or they are trying to put something after the
            # field list.  The first one seems more likely, so we'll
            # just warn about that (to avoid confusion).
            estr = "Lists must be indented."
            errors.append(StructuringError(estr, bullet_token.startline))
        if stack[-1].tag in ('ulist', 'olist', 'fieldlist'):
            stack.pop()
            indent_stack.pop()

        if (list_type != 'fieldlist' and indent_stack[-1] is not None and
            bullet_token.indent == indent_stack[-1]):
            # Ignore this error if there's text on the same line as
            # the comment-opening quote -- epydoc can't reliably
            # determine the indentation for that line.
            if bullet_token.startline != 1 or bullet_token.indent != 0:
                estr = "Lists must be indented."
                errors.append(StructuringError(estr, bullet_token.startline))

        if list_type == 'fieldlist':
            # Fieldlist should be at the top-level.
            for tok in stack[2:]:
                if tok.tag != "section":
                    estr = "Fields must be at the top level."
                    errors.append(
                        StructuringError(estr, bullet_token.startline))
                    break
            stack[2:] = []
            indent_stack[2:] = []

        # Add the new list.
        lst = Element(list_type)
        stack[-1].children.append(lst)
        stack.append(lst)
        indent_stack.append(bullet_token.indent)
        if list_type == 'olist':
            start = bullet_token.contents.split('.')[:-1]
            if start != '1':
                lst.attribs["start"] = start[-1]

    # Fields are treated somewhat specially: A "fieldlist"
    # node is created to make the parsing simpler, but fields
    # are adjoined directly into the "epytext" node, not into
    # the "fieldlist" node.
    if list_type == 'fieldlist':
        li = Element("field")
        token_words = bullet_token.contents[1:-1].split(None, 1)
        tag_elt = Element("tag")
        tag_elt.children.append(token_words[0])
        li.children.append(tag_elt)

        if len(token_words) > 1:
            arg_elt = Element("arg")
            arg_elt.children.append(token_words[1])
            li.children.append(arg_elt)
    else:
        li = Element("li")
        if list_type == 'olist':
            li.attribs["bullet"] = bullet_token.contents

    # Add the bullet.
    stack[-1].children.append(li)
    stack.append(li)
    indent_stack.append(None)

##################################################
## Tokenization
##################################################

class Token:
    """
    C{Token}s are an intermediate data structure used while
    constructing the structuring DOM tree for a formatted docstring.
    There are five types of C{Token}:
    
        - Paragraphs
        - Literal blocks
        - Doctest blocks
        - Headings
        - Bullets

    The text contained in each C{Token} is stored in the
    C{contents} variable.  The string in this variable has been
    normalized.  For paragraphs, this means that it has been converted 
    into a single line of text, with newline/indentation replaced by
    single spaces.  For literal blocks and doctest blocks, this means
    that the appropriate amount of leading whitespace has been removed 
    from each line.

    Each C{Token} has an indentation level associated with it,
    stored in the C{indent} variable.  This indentation level is used
    by the structuring procedure to assemble hierarchical blocks.

    @type tag: C{string}
    @ivar tag: This C{Token}'s type.  Possible values are C{Token.PARA} 
        (paragraph), C{Token.LBLOCK} (literal block), C{Token.DTBLOCK}
        (doctest block), C{Token.HEADINGC}, and C{Token.BULLETC}.
        
    @type startline: C{int}
    @ivar startline: The line on which this C{Token} begins.  This 
        line number is only used for issuing errors.

    @type contents: C{string}
    @ivar contents: The normalized text contained in this C{Token}.
    
    @type indent: C{int} or C{None}
    @ivar indent: The indentation level of this C{Token} (in
        number of leading spaces).  A value of C{None} indicates an
        unknown indentation; this is used for list items and fields
        that begin with one-line paragraphs.
        
    @type level: C{int} or C{None}
    @ivar level: The heading-level of this C{Token} if it is a
        heading; C{None}, otherwise.  Valid heading levels are 0, 1,
        and 2.

    @type inline: C{bool}
    @ivar inline: If True, the element is an inline level element, comparable
        to an HTML C{<span>} tag. Else, it is a block level element, comparable
        to an HTML C{<div>}.

    @type PARA: C{string}
    @cvar PARA: The C{tag} value for paragraph C{Token}s.
    @type LBLOCK: C{string}
    @cvar LBLOCK: The C{tag} value for literal C{Token}s.
    @type DTBLOCK: C{string}
    @cvar DTBLOCK: The C{tag} value for doctest C{Token}s.
    @type HEADING: C{string}
    @cvar HEADING: The C{tag} value for heading C{Token}s.
    @type BULLET: C{string}
    @cvar BULLET: The C{tag} value for bullet C{Token}s.  This C{tag}
        value is also used for field tag C{Token}s, since fields
        function syntactically the same as list items.
    """
    # The possible token types.
    PARA = "para"
    LBLOCK = "literalblock"
    DTBLOCK = "doctestblock"
    HEADING = "heading"
    BULLET = "bullet"

    def __init__(self, tag, startline, contents, indent, level=None,
                 inline=False):
        """
        Create a new C{Token}.

        @param tag: The type of the new C{Token}.
        @type tag: C{string}
        @param startline: The line on which the new C{Token} begins.
        @type startline: C{int}
        @param contents: The normalized contents of the new C{Token}.
        @type contents: C{string}
        @param indent: The indentation of the new C{Token} (in number
            of leading spaces).  A value of C{None} indicates an
            unknown indentation.
        @type indent: C{int} or C{None}
        @param level: The heading-level of this C{Token} if it is a
            heading; C{None}, otherwise.
        @type level: C{int} or C{None}
        @param inline: Is this C{Token} inline as a C{<span>}?.
        @type inline: C{bool}
        """
        self.tag = tag
        self.startline = startline
        self.contents = contents
        self.indent = indent
        self.level = level
        self.inline = inline

    def __repr__(self):
        """
        @rtype: C{string}
        @return: the formal representation of this C{Token}.
            C{Token}s have formal representaitons of the form:: 
                <Token: para at line 12>
        """
        return '<Token: %s at line %s>' % (self.tag, self.startline)

    def to_dom(self, doc):
        """
        @return: a DOM representation of this C{Token}.
        @rtype: L{Element}
        """
        e = Element(self.tag)
        e.children.append(self.contents)
        return e

# Construct regular expressions for recognizing bullets.  These are
# global so they don't have to be reconstructed each time we tokenize
# a docstring.
_ULIST_BULLET = '[-]( +|$)'
_OLIST_BULLET = '(\d+[.])+( +|$)'
_FIELD_BULLET = '@\w+( [^{}:\n]+)?:'
_BULLET_RE = re.compile(_ULIST_BULLET + '|' +
                        _OLIST_BULLET + '|' +
                        _FIELD_BULLET)
_LIST_BULLET_RE = re.compile(_ULIST_BULLET + '|' + _OLIST_BULLET)
_FIELD_BULLET_RE = re.compile(_FIELD_BULLET)
del _ULIST_BULLET, _OLIST_BULLET, _FIELD_BULLET

def _tokenize_doctest(lines, start, block_indent, tokens, errors):
    """
    Construct a L{Token} containing the doctest block starting at
    C{lines[start]}, and append it to C{tokens}.  C{block_indent}
    should be the indentation of the doctest block.  Any errors
    generated while tokenizing the doctest block will be appended to
    C{errors}.

    @param lines: The list of lines to be tokenized
    @param start: The index into C{lines} of the first line of the
        doctest block to be tokenized.
    @param block_indent: The indentation of C{lines[start]}.  This is
        the indentation of the doctest block.
    @param errors: A list where any errors generated during parsing
        will be stored.  If no list is specified, then errors will 
        generate exceptions.
    @return: The line number of the first line following the doctest
        block.
        
    @type lines: C{list} of C{string}
    @type start: C{int}
    @type block_indent: C{int}
    @type tokens: C{list} of L{Token}
    @type errors: C{list} of L{ParseError}
    @rtype: C{int}
    """
    # If they dedent past block_indent, keep track of the minimum
    # indentation.  This is used when removing leading indentation
    # from the lines of the doctest block.
    min_indent = block_indent

    linenum = start + 1
    while linenum < len(lines):
        # Find the indentation of this line.
        line = lines[linenum]
        indent = len(line) - len(line.lstrip())
        
        # A blank line ends doctest block.
        if indent == len(line): break
        
        # A Dedent past block_indent is an error.
        if indent < block_indent:
            min_indent = min(min_indent, indent)
            estr = 'Improper doctest block indentation.'
            errors.append(TokenizationError(estr, linenum))

        # Go on to the next line.
        linenum += 1

    # Add the token, and return the linenum after the token ends.
    contents = [line[min_indent:] for line in lines[start:linenum]]
    contents = '\n'.join(contents)
    tokens.append(Token(Token.DTBLOCK, start, contents, block_indent))
    return linenum

def _tokenize_literal(lines, start, block_indent, tokens, errors):
    """
    Construct a L{Token} containing the literal block starting at
    C{lines[start]}, and append it to C{tokens}.  C{block_indent}
    should be the indentation of the literal block.  Any errors
    generated while tokenizing the literal block will be appended to
    C{errors}.

    @param lines: The list of lines to be tokenized
    @param start: The index into C{lines} of the first line of the
        literal block to be tokenized.
    @param block_indent: The indentation of C{lines[start]}.  This is
        the indentation of the literal block.
    @param errors: A list of the errors generated by parsing.  Any
        new errors generated while will tokenizing this paragraph
        will be appended to this list.
    @return: The line number of the first line following the literal
        block. 
        
    @type lines: C{list} of C{string}
    @type start: C{int}
    @type block_indent: C{int}
    @type tokens: C{list} of L{Token}
    @type errors: C{list} of L{ParseError}
    @rtype: C{int}
    """
    linenum = start + 1
    while linenum < len(lines):
        # Find the indentation of this line.
        line = lines[linenum]
        indent = len(line) - len(line.lstrip())

        # A Dedent to block_indent ends the literal block.
        # (Ignore blank likes, though)
        if len(line) != indent and indent <= block_indent:
            break
        
        # Go on to the next line.
        linenum += 1

    # Add the token, and return the linenum after the token ends.
    contents = [line[block_indent+1:] for line in lines[start:linenum]]
    contents = '\n'.join(contents)
    contents = re.sub('(\A[ \n]*\n)|(\n[ \n]*\Z)', '', contents)
    tokens.append(Token(Token.LBLOCK, start, contents, block_indent))
    return linenum

def _tokenize_listart(lines, start, bullet_indent, tokens, errors):
    """
    Construct L{Token}s for the bullet and the first paragraph of the
    list item (or field) starting at C{lines[start]}, and append them
    to C{tokens}.  C{bullet_indent} should be the indentation of the
    list item.  Any errors generated while tokenizing will be
    appended to C{errors}.

    @param lines: The list of lines to be tokenized
    @param start: The index into C{lines} of the first line of the
        list item to be tokenized.
    @param bullet_indent: The indentation of C{lines[start]}.  This is
        the indentation of the list item.
    @param errors: A list of the errors generated by parsing.  Any
        new errors generated while will tokenizing this paragraph
        will be appended to this list.
    @return: The line number of the first line following the list
        item's first paragraph.
        
    @type lines: C{list} of C{string}
    @type start: C{int}
    @type bullet_indent: C{int}
    @type tokens: C{list} of L{Token}
    @type errors: C{list} of L{ParseError}
    @rtype: C{int}
    """
    linenum = start + 1
    para_indent = None
    doublecolon = lines[start].rstrip()[-2:] == '::'

    # Get the contents of the bullet.
    para_start = _BULLET_RE.match(lines[start], bullet_indent).end()
    bcontents = lines[start][bullet_indent:para_start].strip()
    
    while linenum < len(lines):
        # Find the indentation of this line.
        line = lines[linenum]
        indent = len(line) - len(line.lstrip())

        # "::" markers end paragraphs.
        if doublecolon: break
        if line.rstrip()[-2:] == '::': doublecolon = 1

        # A blank line ends the token
        if indent == len(line): break

        # Dedenting past bullet_indent ends the list item.
        if indent < bullet_indent: break
        
        # A line beginning with a bullet ends the token.
        if _BULLET_RE.match(line, indent): break
        
        # If this is the second line, set the paragraph indentation, or 
        # end the token, as appropriate.
        if para_indent == None: para_indent = indent

        # A change in indentation ends the token
        if indent != para_indent: break

        # Go on to the next line.
        linenum += 1

    # Add the bullet token.
    tokens.append(Token(Token.BULLET, start, bcontents, bullet_indent,
                        inline=True))

    # Add the paragraph token.
    pcontents = ([lines[start][para_start:].strip()] + 
                 [line.strip() for line in lines[start+1:linenum]])
    pcontents = ' '.join(pcontents).strip()
    if pcontents:
        tokens.append(Token(Token.PARA, start, pcontents, para_indent,
                            inline=True))

    # Return the linenum after the paragraph token ends.
    return linenum

def _tokenize_para(lines, start, para_indent, tokens, errors):
    """
    Construct a L{Token} containing the paragraph starting at
    C{lines[start]}, and append it to C{tokens}.  C{para_indent}
    should be the indentation of the paragraph .  Any errors
    generated while tokenizing the paragraph will be appended to
    C{errors}.

    @param lines: The list of lines to be tokenized
    @param start: The index into C{lines} of the first line of the
        paragraph to be tokenized.
    @param para_indent: The indentation of C{lines[start]}.  This is
        the indentation of the paragraph.
    @param errors: A list of the errors generated by parsing.  Any
        new errors generated while will tokenizing this paragraph
        will be appended to this list.
    @return: The line number of the first line following the
        paragraph. 
        
    @type lines: C{list} of C{string}
    @type start: C{int}
    @type para_indent: C{int}
    @type tokens: C{list} of L{Token}
    @type errors: C{list} of L{ParseError}
    @rtype: C{int}
    """
    linenum = start + 1
    doublecolon = 0
    while linenum < len(lines):
        # Find the indentation of this line.
        line = lines[linenum]
        indent = len(line) - len(line.lstrip())

        # "::" markers end paragraphs.
        if doublecolon: break
        if line.rstrip()[-2:] == '::': doublecolon = 1

        # Blank lines end paragraphs
        if indent == len(line): break

        # Indentation changes end paragraphs
        if indent != para_indent: break

        # List bullets end paragraphs
        if _BULLET_RE.match(line, indent): break

        # Check for mal-formatted field items.
        if line[indent] == '@':
            estr = "Possible mal-formatted field item."
            errors.append(TokenizationError(estr, linenum, is_fatal=0))
            
        # Go on to the next line.
        linenum += 1

    contents = [line.strip() for line in lines[start:linenum]]
    
    # Does this token look like a heading?
    if ((len(contents) < 2) or
        (contents[1][0] not in _HEADING_CHARS) or
        (abs(len(contents[0])-len(contents[1])) > 5)):
        looks_like_heading = 0
    else:
        looks_like_heading = 1
        for char in contents[1]:
            if char != contents[1][0]:
                looks_like_heading = 0
                break

    if looks_like_heading:
        if len(contents[0]) != len(contents[1]):
            estr = ("Possible heading typo: the number of "+
                    "underline characters must match the "+
                    "number of heading characters.")
            errors.append(TokenizationError(estr, start, is_fatal=0))
        else:
            level = _HEADING_CHARS.index(contents[1][0])
            tokens.append(Token(Token.HEADING, start,
                                contents[0], para_indent, level))
            return start+2
                 
    # Add the paragraph token, and return the linenum after it ends.
    contents = ' '.join(contents)
    tokens.append(Token(Token.PARA, start, contents, para_indent))
    return linenum
        
def _tokenize(str, errors):
    """
    Split a given formatted docstring into an ordered list of
    C{Token}s, according to the epytext markup rules.

    @param str: The epytext string
    @type str: C{string}
    @param errors: A list where any errors generated during parsing
        will be stored.  If no list is specified, then errors will 
        generate exceptions.
    @type errors: C{list} of L{ParseError}
    @return: a list of the C{Token}s that make up the given string.
    @rtype: C{list} of L{Token}
    """
    tokens = []
    lines = str.split('\n')

    # Scan through the lines, determining what @type of token we're
    # dealing with, and tokenizing it, as appropriate.
    linenum = 0
    while linenum < len(lines):
        # Get the current line and its indentation.
        line = lines[linenum]
        indent = len(line)-len(line.lstrip())

        if indent == len(line):
            # Ignore blank lines.
            linenum += 1
            continue
        elif line[indent:indent+4] == '>>> ':
            # blocks starting with ">>> " are doctest block tokens.
            linenum = _tokenize_doctest(lines, linenum, indent,
                                        tokens, errors)
        elif _BULLET_RE.match(line, indent):
            # blocks starting with a bullet are LI start tokens.
            linenum = _tokenize_listart(lines, linenum, indent,
                                        tokens, errors)
            if tokens[-1].indent != None:
                indent = tokens[-1].indent
        else:
            # Check for mal-formatted field items.
            if line[indent] == '@':
                estr = "Possible mal-formatted field item."
                errors.append(TokenizationError(estr, linenum, is_fatal=0))
            
            # anything else is either a paragraph or a heading.
            linenum = _tokenize_para(lines, linenum, indent, tokens, errors)

        # Paragraph tokens ending in '::' initiate literal blocks.
        if (tokens[-1].tag == Token.PARA and
            tokens[-1].contents[-2:] == '::'):
            tokens[-1].contents = tokens[-1].contents[:-1]
            linenum = _tokenize_literal(lines, linenum, indent, tokens, errors)

    return tokens


##################################################
## Inline markup ("colorizing")
##################################################

# Assorted regular expressions used for colorizing.
_BRACE_RE = re.compile('{|}')
_TARGET_RE = re.compile('^(.*?)\s*<(?:URI:|URL:)?([^<>]+)>$')

def _colorize(doc, token, errors, tagName='para'):
    """
    Given a string containing the contents of a paragraph, produce a
    DOM C{Element} encoding that paragraph.  Colorized regions are
    represented using DOM C{Element}s, and text is represented using
    DOM C{Text}s.

    @param errors: A list of errors.  Any newly generated errors will
        be appended to this list.
    @type errors: C{list} of C{string}
    
    @param tagName: The element tag for the DOM C{Element} that should
        be generated.
    @type tagName: C{string}
    
    @return: a DOM C{Element} encoding the given paragraph.
    @returntype: C{Element}
    """
    str = token.contents
    linenum = 0
    
    # Maintain a stack of DOM elements, containing the ancestors of
    # the text currently being analyzed.  New elements are pushed when 
    # "{" is encountered, and old elements are popped when "}" is
    # encountered. 
    stack = [Element(tagName)]

    # This is just used to make error-reporting friendlier.  It's a
    # stack parallel to "stack" containing the index of each element's 
    # open brace.
    openbrace_stack = [0]

    # Process the string, scanning for '{' and '}'s.  start is the
    # index of the first unprocessed character.  Each time through the
    # loop, we process the text from the first unprocessed character
    # to the next open or close brace.
    start = 0
    while 1:
        match = _BRACE_RE.search(str, start)
        if match == None: break
        end = match.start()
        
        # Open braces start new colorizing elements.  When preceeded
        # by a capital letter, they specify a colored region, as
        # defined by the _COLORIZING_TAGS dictionary.  Otherwise, 
        # use a special "literal braces" element (with tag "litbrace"),
        # and convert them to literal braces once we find the matching 
        # close-brace.
        if match.group() == '{':
            if (end>0) and 'A' <= str[end-1] <= 'Z':
                if (end-1) > start:
                    stack[-1].children.append(str[start:end-1])
                if str[end-1] not in _COLORIZING_TAGS:
                    estr = "Unknown inline markup tag."
                    errors.append(ColorizingError(estr, token, end-1))
                    stack.append(Element('unknown'))
                else:
                    tag = _COLORIZING_TAGS[str[end-1]]
                    stack.append(Element(tag))
            else:
                if end > start:
                    stack[-1].children.append(str[start:end])
                stack.append(Element('litbrace'))
            openbrace_stack.append(end)
            stack[-2].children.append(stack[-1])
            
        # Close braces end colorizing elements.
        elif match.group() == '}':
            # Check for (and ignore) unbalanced braces.
            if len(stack) <= 1:
                estr = "Unbalanced '}'."
                errors.append(ColorizingError(estr, token, end))
                start = end + 1
                continue

            # Add any remaining text.
            if end > start:
                stack[-1].children.append(str[start:end])

            # Special handling for symbols:
            if stack[-1].tag == 'symbol':
                if (len(stack[-1].children) != 1 or
                    not isinstance(stack[-1].children[0], basestring)):
                    estr = "Invalid symbol code."
                    errors.append(ColorizingError(estr, token, end))
                else:
                    symb = stack[-1].children[0]
                    if symb in _SYMBOLS:
                        # It's a symbol
                        stack[-2].children[-1] = Element('symbol', symb)
                    else:
                        estr = "Invalid symbol code."
                        errors.append(ColorizingError(estr, token, end))
                        
            # Special handling for escape elements:
            if stack[-1].tag == 'escape':
                if (len(stack[-1].children) != 1 or
                    not isinstance(stack[-1].children[0], basestring)):
                    estr = "Invalid escape code."
                    errors.append(ColorizingError(estr, token, end))
                else:
                    escp = stack[-1].children[0]
                    if escp in _ESCAPES:
                        # It's an escape from _ESCPAES
                        stack[-2].children[-1] = _ESCAPES[escp]
                    elif len(escp) == 1:
                        # It's a single-character escape (eg E{.})
                        stack[-2].children[-1] = escp
                    else:
                        estr = "Invalid escape code."
                        errors.append(ColorizingError(estr, token, end))

            # Special handling for literal braces elements:
            if stack[-1].tag == 'litbrace':
                stack[-2].children[-1:] = ['{'] + stack[-1].children + ['}']

            # Special handling for graphs:
            if stack[-1].tag == 'graph':
                _colorize_graph(doc, stack[-1], token, end, errors)

            # Special handling for link-type elements:
            if stack[-1].tag in _LINK_COLORIZING_TAGS:
                _colorize_link(doc, stack[-1], token, end, errors)

            # Pop the completed element.
            openbrace_stack.pop()
            stack.pop()

        start = end+1

    # Add any final text.
    if start < len(str):
        stack[-1].children.append(str[start:])
        
    if len(stack) != 1: 
        estr = "Unbalanced '{'."
        errors.append(ColorizingError(estr, token, openbrace_stack[-1]))

    return stack[0]

GRAPH_TYPES = ['classtree', 'packagetree', 'importgraph', 'callgraph']

def _colorize_graph(doc, graph, token, end, errors):
    """
    Eg::
      G{classtree}
      G{classtree x, y, z}
      G{importgraph}
    """
    bad_graph_spec = False
    
    children = graph.children[:]
    graph.children = []

    if len(children) != 1 or not isinstance(children[0], basestring):
        bad_graph_spec = "Bad graph specification"
    else:
        pieces = children[0].split(None, 1)
        graphtype = pieces[0].replace(':','').strip().lower()
        if graphtype in GRAPH_TYPES:
            if len(pieces) == 2:
                if re.match(r'\s*:?\s*([\w\.]+\s*,?\s*)*', pieces[1]):
                    args = pieces[1].replace(',', ' ').replace(':','').split()
                else:
                    bad_graph_spec = "Bad graph arg list"
            else:
                args = []
        else:
            bad_graph_spec = ("Bad graph type %s -- use one of %s" %
                              (pieces[0], ', '.join(GRAPH_TYPES)))

    if bad_graph_spec:
        errors.append(ColorizingError(bad_graph_spec, token, end))
        graph.children.append('none')
        graph.children.append('')
        return

    graph.children.append(graphtype)
    for arg in args:
        graph.children.append(arg)

def _colorize_link(doc, link, token, end, errors):
    variables = link.children[:]

    # If the last child isn't text, we know it's bad.
    if len(variables)==0 or not isinstance(variables[-1], basestring):
        estr = "Bad %s target." % link.tag
        errors.append(ColorizingError(estr, token, end))
        return
    
    # Did they provide an explicit target?
    match2 = _TARGET_RE.match(variables[-1])
    if match2:
        (text, target) = match2.groups()
        variables[-1] = text
    # Can we extract an implicit target?
    elif len(variables) == 1:
        target = variables[0]
    else:
        estr = "Bad %s target." % link.tag
        errors.append(ColorizingError(estr, token, end))
        return

    # Construct the name element.
    name_elt = Element('name', *variables)

    # Clean up the target.  For URIs, assume http or mailto if they
    # don't specify (no relative urls)
    target = re.sub(r'\s', '', target)
    if link.tag=='uri':
        if not re.match(r'\w+:', target):
            if re.match(r'\w+@(\w+)(\.\w+)*', target):
                target = 'mailto:' + target
            else:
                target = 'http://'+target
    elif link.tag=='link':
        # Remove arg lists for functions (e.g., L{_colorize_link()})
        target = re.sub(r'\(.*\)$', '', target)
        if not re.match(r'^[a-zA-Z_]\w*(\.[a-zA-Z_]\w*)*$', target):
            estr = "Bad link target."
            errors.append(ColorizingError(estr, token, end))
            return

    # Construct the target element.
    target_elt = Element('target', target)

    # Add them to the link element.
    link.children = [name_elt, target_elt]

##################################################
## Formatters
##################################################

def to_epytext(tree, indent=0, seclevel=0):
    """
    Convert a DOM document encoding epytext back to an epytext string.
    This is the inverse operation from L{parse}.  I.e., assuming there
    are no errors, the following is true:
        - C{parse(to_epytext(tree)) == tree}

    The inverse is true, except that whitespace, line wrapping, and
    character escaping may be done differently.
        - C{to_epytext(parse(str)) == str} (approximately)

    @param tree: A DOM document encoding of an epytext string.
    @type tree: C{Element}
    @param indent: The indentation for the string representation of
        C{tree}.  Each line of the returned string will begin with
        C{indent} space characters.
    @type indent: C{int}
    @param seclevel: The section level that C{tree} appears at.  This
        is used to generate section headings.
    @type seclevel: C{int}
    @return: The epytext string corresponding to C{tree}.
    @rtype: C{string}
    """
    if isinstance(tree, basestring):
        str = re.sub(r'\{', '\0', tree)
        str = re.sub(r'\}', '\1', str)
        return str

    if tree.tag == 'epytext': indent -= 2
    if tree.tag == 'section': seclevel += 1
    variables = [to_epytext(c, indent+2, seclevel) for c in tree.children]
    childstr = ''.join(variables)

    # Clean up for literal blocks (add the double "::" back)
    childstr = re.sub(':(\s*)\2', '::\\1', childstr)

    if tree.tag == 'para':
        str = wordwrap(childstr, indent)+'\n'
        str = re.sub(r'((^|\n)\s*\d+)\.', r'\1E{.}', str)
        str = re.sub(r'((^|\n)\s*)-', r'\1E{-}', str)
        str = re.sub(r'((^|\n)\s*)@', r'\1E{@}', str)
        str = re.sub(r'::(\s*($|\n))', r'E{:}E{:}\1', str)
        str = re.sub('\0', 'E{lb}', str)
        str = re.sub('\1', 'E{rb}', str)
        return str
    elif tree.tag == 'li':
        bullet = tree.attribs.get('bullet') or '-'
        return indent*' '+ bullet + ' ' + childstr.lstrip()
    elif tree.tag == 'heading':
        str = re.sub('\0', 'E{lb}',childstr)
        str = re.sub('\1', 'E{rb}', str)
        uline = len(childstr)*_HEADING_CHARS[seclevel-1]
        return (indent-2)*' ' + str + '\n' + (indent-2)*' '+uline+'\n'
    elif tree.tag == 'doctestblock':
        str = re.sub('\0', '{', childstr)
        str = re.sub('\1', '}', str)
        lines = ['  '+indent*' '+line for line in str.split('\n')]
        return '\n'.join(lines) + '\n\n'
    elif tree.tag == 'literalblock':
        str = re.sub('\0', '{', childstr)
        str = re.sub('\1', '}', str)
        lines = [(indent+1)*' '+line for line in str.split('\n')]
        return '\2' + '\n'.join(lines) + '\n\n'
    elif tree.tag == 'field':
        numargs = 0
        while tree.children[numargs+1].tag == 'arg': numargs += 1
        tag = variables[0]
        args = variables[1:1+numargs]
        body = variables[1+numargs:]
        str = (indent)*' '+'@'+variables[0]
        if args: str += '(' + ', '.join(args) + ')'
        return str + ':\n' + ''.join(body)
    elif tree.tag == 'target':
        return '<%s>' % childstr
    elif tree.tag in ('fieldlist', 'tag', 'arg', 'epytext',
                          'section', 'olist', 'ulist', 'name'):
        return childstr
    elif tree.tag == 'symbol':
        return 'E{%s}' % childstr
    elif tree.tag == 'graph':
        return 'G{%s}' % ' '.join(variables)
    else:
        for (tag, name) in _COLORIZING_TAGS.items():
            if name == tree.tag:
                return '%s{%s}' % (tag, childstr)
    raise ValueError('Unknown DOM element %r' % tree.tag)

SYMBOL_TO_PLAINTEXT = {
    'crarr': '\\',
    }

def to_plaintext(tree, indent=0, seclevel=0):
    """    
    Convert a DOM document encoding epytext to a string representation.
    This representation is similar to the string generated by
    C{to_epytext}, but C{to_plaintext} removes inline markup, prints
    escaped characters in unescaped form, etc.

    @param tree: A DOM document encoding of an epytext string.
    @type tree: C{Element}
    @param indent: The indentation for the string representation of
        C{tree}.  Each line of the returned string will begin with
        C{indent} space characters.
    @type indent: C{int}
    @param seclevel: The section level that C{tree} appears at.  This
        is used to generate section headings.
    @type seclevel: C{int}
    @return: The epytext string corresponding to C{tree}.
    @rtype: C{string}
    """
    if isinstance(tree, basestring): return tree

    if tree.tag == 'section': seclevel += 1

    # Figure out the child indent level.
    if tree.tag == 'epytext': cindent = indent
    elif tree.tag == 'li' and tree.attribs.get('bullet'):
        cindent = indent + 1 + len(tree.attribs.get('bullet'))
    else:
        cindent = indent + 2
    variables = [to_plaintext(c, cindent, seclevel) for c in tree.children]
    childstr = ''.join(variables)

    if tree.tag == 'para':
        return wordwrap(childstr, indent)+'\n'
    elif tree.tag == 'li':
        # We should be able to use getAttribute here; but there's no
        # convenient way to test if an element has an attribute..
        bullet = tree.attribs.get('bullet') or '-'
        return indent*' ' + bullet + ' ' + childstr.lstrip()
    elif tree.tag == 'heading':
        uline = len(childstr)*_HEADING_CHARS[seclevel-1]
        return ((indent-2)*' ' + childstr + '\n' +
                (indent-2)*' ' + uline + '\n')
    elif tree.tag == 'doctestblock':
        lines = [(indent+2)*' '+line for line in childstr.split('\n')]
        return '\n'.join(lines) + '\n\n'
    elif tree.tag == 'literalblock':
        lines = [(indent+1)*' '+line for line in childstr.split('\n')]
        return '\n'.join(lines) + '\n\n'
    elif tree.tag == 'fieldlist':
        return childstr
    elif tree.tag == 'field':
        numargs = 0
        while tree.children[numargs+1].tag == 'arg': numargs += 1
        tag = variables[0]
        args = variables[1:1+numargs]
        body = variables[1+numargs:]
        str = (indent)*' '+'@'+variables[0]
        if args: str += '(' + ', '.join(args) + ')'
        return str + ':\n' + ''.join(body)
    elif tree.tag == 'uri':
        if len(variables) != 2: raise ValueError('Bad URI ')
        elif variables[0] == variables[1]: return '<%s>' % variables[1]
        else: return '%r<%s>' % (variables[0], variables[1])
    elif tree.tag == 'link':
        if len(variables) != 2: raise ValueError('Bad Link')
        return '%s' % variables[0]
    elif tree.tag in ('olist', 'ulist'):
        # [xx] always use condensed lists.
        ## Use a condensed list if each list item is 1 line long.
        #for child in variables:
        #    if child.count('\n') > 2: return childstr
        return childstr.replace('\n\n', '\n')+'\n'
    elif tree.tag == 'symbol':
        return '%s' % SYMBOL_TO_PLAINTEXT.get(childstr, childstr)
    elif tree.tag == 'graph':
        return '<<%s graph: %s>>' % (variables[0], ', '.join(variables[1:]))
    else:
        # Assume that anything else can be passed through.
        return childstr

def to_debug(tree, indent=4, seclevel=0):
    """    
    Convert a DOM document encoding epytext back to an epytext string,
    annotated with extra debugging information.  This function is
    similar to L{to_epytext}, but it adds explicit information about
    where different blocks begin, along the left margin.

    @param tree: A DOM document encoding of an epytext string.
    @type tree: C{Element}
    @param indent: The indentation for the string representation of
        C{tree}.  Each line of the returned string will begin with
        C{indent} space characters.
    @type indent: C{int}
    @param seclevel: The section level that C{tree} appears at.  This
        is used to generate section headings.
    @type seclevel: C{int}
    @return: The epytext string corresponding to C{tree}.
    @rtype: C{string}
    """
    if isinstance(tree, basestring):
        str = re.sub(r'\{', '\0', tree)
        str = re.sub(r'\}', '\1', str)
        return str

    if tree.tag == 'section': seclevel += 1
    variables = [to_debug(c, indent+2, seclevel) for c in tree.children]
    childstr = ''.join(variables)

    # Clean up for literal blocks (add the double "::" back)
    childstr = re.sub(':( *\n     \|\n)\2', '::\\1', childstr)

    if tree.tag == 'para':
        str = wordwrap(childstr, indent-6, 69)+'\n'
        str = re.sub(r'((^|\n)\s*\d+)\.', r'\1E{.}', str)
        str = re.sub(r'((^|\n)\s*)-', r'\1E{-}', str)
        str = re.sub(r'((^|\n)\s*)@', r'\1E{@}', str)
        str = re.sub(r'::(\s*($|\n))', r'E{:}E{:}\1', str)
        str = re.sub('\0', 'E{lb}', str)
        str = re.sub('\1', 'E{rb}', str)
        lines = str.rstrip().split('\n')
        lines[0] = '   P>|' + lines[0]
        lines[1:] = ['     |'+l for l in lines[1:]]
        return '\n'.join(lines)+'\n     |\n'
    elif tree.tag == 'li':
        bullet = tree.attribs.get('bullet') or '-'
        return '  LI>|'+ (indent-6)*' '+ bullet + ' ' + childstr[6:].lstrip()
    elif tree.tag in ('olist', 'ulist'):
        return 'LIST>|'+(indent-4)*' '+childstr[indent+2:]
    elif tree.tag == 'heading':
        str = re.sub('\0', 'E{lb}', childstr)
        str = re.sub('\1', 'E{rb}', str)
        uline = len(childstr)*_HEADING_CHARS[seclevel-1]
        return ('SEC'+`seclevel`+'>|'+(indent-8)*' ' + str + '\n' +
                '     |'+(indent-8)*' ' + uline + '\n')
    elif tree.tag == 'doctestblock':
        str = re.sub('\0', '{', childstr)
        str = re.sub('\1', '}', str)
        lines = ['     |'+(indent-4)*' '+line for line in str.split('\n')]
        lines[0] = 'DTST>'+lines[0][5:]
        return '\n'.join(lines) + '\n     |\n'
    elif tree.tag == 'literalblock':
        str = re.sub('\0', '{', childstr)
        str = re.sub('\1', '}', str)
        lines = ['     |'+(indent-5)*' '+line for line in str.split('\n')]
        lines[0] = ' LIT>'+lines[0][5:]
        return '\2' + '\n'.join(lines) + '\n     |\n'
    elif tree.tag == 'field':
        numargs = 0
        while tree.children[numargs+1].tag == 'arg': numargs += 1
        tag = variables[0]
        args = variables[1:1+numargs]
        body = variables[1+numargs:]
        str = ' FLD>|'+(indent-6)*' '+'@'+variables[0]
        if args: str += '(' + ', '.join(args) + ')'
        return str + ':\n' + ''.join(body)
    elif tree.tag == 'target':
        return '<%s>' % childstr
    elif tree.tag in ('fieldlist', 'tag', 'arg', 'epytext',
                          'section', 'olist', 'ulist', 'name'):
        return childstr
    elif tree.tag == 'symbol':
        return 'E{%s}' % childstr
    elif tree.tag == 'graph':
        return 'G{%s}' % ' '.join(variables)
    else:
        for (tag, name) in _COLORIZING_TAGS.items():
            if name == tree.tag:
                return '%s{%s}' % (tag, childstr)
    raise ValueError('Unknown DOM element %r' % tree.tag)

##################################################
## Top-Level Wrapper function
##################################################
def pparse(str, show_warnings=1, show_errors=1, stream=sys.stderr):
    """
    Pretty-parse the string.  This parses the string, and catches any
    warnings or errors produced.  Any warnings and errors are
    displayed, and the resulting DOM parse structure is returned.

    @param str: The string to parse.
    @type str: C{string}
    @param show_warnings: Whether or not to display non-fatal errors
        generated by parsing C{str}.
    @type show_warnings: C{boolean}
    @param show_errors: Whether or not to display fatal errors 
        generated by parsing C{str}.
    @type show_errors: C{boolean}
    @param stream: The stream that warnings and errors should be
        written to.
    @type stream: C{stream}
    @return: a DOM document encoding the contents of C{str}.
    @rtype: C{Element}
    @raise SyntaxError: If any fatal errors were encountered.
    """
    errors = []
    confused = 0
    try:
        val = parse(str, errors)
        warnings = [e for e in errors if not e.is_fatal()]
        errors = [e for e in errors if e.is_fatal()]
    except:
        confused = 1
        
    if not show_warnings: warnings = []
    warnings.sort()
    errors.sort()
    if warnings:
        print >>stream, '='*SCRWIDTH
        print >>stream, "WARNINGS"
        print >>stream, '-'*SCRWIDTH
        for warning in warnings:
            print >>stream, warning.as_warning()
        print >>stream, '='*SCRWIDTH
    if errors and show_errors:
        if not warnings: print >>stream, '='*SCRWIDTH
        print >>stream, "ERRORS"
        print >>stream, '-'*SCRWIDTH
        for error in errors:
            print >>stream, error
        print >>stream, '='*SCRWIDTH

    if confused: raise
    elif errors: raise SyntaxError('Encountered Errors')
    else: return val

##################################################
## Parse Errors
##################################################

class TokenizationError(ParseError):
    """
    An error generated while tokenizing a formatted documentation
    string.
    """

class StructuringError(ParseError):
    """
    An error generated while structuring a formatted documentation
    string.
    """

class ColorizingError(ParseError):
    """
    An error generated while colorizing a paragraph.
    """
    def __init__(self, descr, token, charnum, is_fatal=1):
        """
        Construct a new colorizing exception.
        
        @param descr: A short description of the error.
        @type descr: C{string}
        @param token: The token where the error occured
        @type token: L{Token}
        @param charnum: The character index of the position in
            C{token} where the error occured.
        @type charnum: C{int}
        """
        ParseError.__init__(self, descr, token.startline, is_fatal)
        self.token = token
        self.charnum = charnum

    CONTEXT_RANGE = 20
    def descr(self):
        RANGE = self.CONTEXT_RANGE
        if self.charnum <= RANGE:
            left = self.token.contents[0:self.charnum]
        else:
            left = '...'+self.token.contents[self.charnum-RANGE:self.charnum]
        if (len(self.token.contents)-self.charnum) <= RANGE:
            right = self.token.contents[self.charnum:]
        else:
            right = (self.token.contents[self.charnum:self.charnum+RANGE]
                     + '...')
        return ('%s\n\n%s%s\n%s^' % (self._descr, left, right, ' '*len(left)))
                
##################################################
## Convenience parsers
##################################################

def parse_as_literal(str):
    """
    Return a DOM document matching the epytext DTD, containing a
    single literal block.  That literal block will include the
    contents of the given string.  This method is typically used as a
    fall-back when the parser fails.

    @param str: The string which should be enclosed in a literal
        block.
    @type str: C{string}
    
    @return: A DOM document containing C{str} in a single literal
        block.
    @rtype: C{Element}
    """
    return Element('epytext', Element('literalblock', str))

def parse_as_para(str):
    """
    Return a DOM document matching the epytext DTD, containing a
    single paragraph.  That paragraph will include the contents of the
    given string.  This can be used to wrap some forms of
    automatically generated information (such as type names) in
    paragraphs.

    @param str: The string which should be enclosed in a paragraph.
    @type str: C{string}
    
    @return: A DOM document containing C{str} in a single paragraph.
    @rtype: C{Element}
    """
    return Element('epytext', Element('para', str))

#################################################################
##                    SUPPORT FOR EPYDOC
#################################################################

def parse_docstring(docstring, errors, **options):
    """
    Parse the given docstring, which is formatted using epytext; and
    return a C{ParsedDocstring} representation of its contents.
    @param docstring: The docstring to parse
    @type docstring: C{string}
    @param errors: A list where any errors generated during parsing
        will be stored.
    @type errors: C{list} of L{ParseError}
    @param options: Extra options.  Unknown options are ignored.
        Currently, no extra options are defined.
    @rtype: L{ParsedDocstring}
    """
    return ParsedEpytextDocstring(parse(docstring, errors), **options)
    
class ParsedEpytextDocstring(ParsedDocstring):
    SYMBOL_TO_HTML = {
        # Symbols
        '<-': '&larr;', '->': '&rarr;', '^': '&uarr;', 'v': '&darr;',
    
        # Greek letters
        'alpha': '&alpha;', 'beta': '&beta;', 'gamma': '&gamma;',
        'delta': '&delta;', 'epsilon': '&epsilon;', 'zeta': '&zeta;',  
        'eta': '&eta;', 'theta': '&theta;', 'iota': '&iota;', 
        'kappa': '&kappa;', 'lambda': '&lambda;', 'mu': '&mu;',  
        'nu': '&nu;', 'xi': '&xi;', 'omicron': '&omicron;',  
        'pi': '&pi;', 'rho': '&rho;', 'sigma': '&sigma;',  
        'tau': '&tau;', 'upsilon': '&upsilon;', 'phi': '&phi;',  
        'chi': '&chi;', 'psi': '&psi;', 'omega': '&omega;',
        'Alpha': '&Alpha;', 'Beta': '&Beta;', 'Gamma': '&Gamma;',
        'Delta': '&Delta;', 'Epsilon': '&Epsilon;', 'Zeta': '&Zeta;',  
        'Eta': '&Eta;', 'Theta': '&Theta;', 'Iota': '&Iota;', 
        'Kappa': '&Kappa;', 'Lambda': '&Lambda;', 'Mu': '&Mu;',  
        'Nu': '&Nu;', 'Xi': '&Xi;', 'Omicron': '&Omicron;',  
        'Pi': '&Pi;', 'Rho': '&Rho;', 'Sigma': '&Sigma;',  
        'Tau': '&Tau;', 'Upsilon': '&Upsilon;', 'Phi': '&Phi;',  
        'Chi': '&Chi;', 'Psi': '&Psi;', 'Omega': '&Omega;',
    
        # HTML character entities
        'larr': '&larr;', 'rarr': '&rarr;', 'uarr': '&uarr;',
        'darr': '&darr;', 'harr': '&harr;', 'crarr': '&crarr;',
        'lArr': '&lArr;', 'rArr': '&rArr;', 'uArr': '&uArr;',
        'dArr': '&dArr;', 'hArr': '&hArr;', 
        'copy': '&copy;', 'times': '&times;', 'forall': '&forall;',
        'exist': '&exist;', 'part': '&part;',
        'empty': '&empty;', 'isin': '&isin;', 'notin': '&notin;',
        'ni': '&ni;', 'prod': '&prod;', 'sum': '&sum;',
        'prop': '&prop;', 'infin': '&infin;', 'ang': '&ang;',
        'and': '&and;', 'or': '&or;', 'cap': '&cap;', 'cup': '&cup;',
        'int': '&int;', 'there4': '&there4;', 'sim': '&sim;',
        'cong': '&cong;', 'asymp': '&asymp;', 'ne': '&ne;',
        'equiv': '&equiv;', 'le': '&le;', 'ge': '&ge;',
        'sub': '&sub;', 'sup': '&sup;', 'nsub': '&nsub;',
        'sube': '&sube;', 'supe': '&supe;', 'oplus': '&oplus;',
        'otimes': '&otimes;', 'perp': '&perp;',
    
        # Alternate (long) names
        'infinity': '&infin;', 'integral': '&int;', 'product': '&prod;',
        '<=': '&le;', '>=': '&ge;',
        }
    
    SYMBOL_TO_LATEX = {
        # Symbols
        '<-': r'\(\leftarrow\)', '->': r'\(\rightarrow\)',
        '^': r'\(\uparrow\)', 'v': r'\(\downarrow\)',
    
        # Greek letters (use lower case when upcase not available)

        'alpha': r'\(\alpha\)', 'beta': r'\(\beta\)', 'gamma':
        r'\(\gamma\)', 'delta': r'\(\delta\)', 'epsilon':
        r'\(\epsilon\)', 'zeta': r'\(\zeta\)', 'eta': r'\(\eta\)',
        'theta': r'\(\theta\)', 'iota': r'\(\iota\)', 'kappa':
        r'\(\kappa\)', 'lambda': r'\(\lambda\)', 'mu': r'\(\mu\)',
        'nu': r'\(\nu\)', 'xi': r'\(\xi\)', 'omicron': r'\(o\)', 'pi':
        r'\(\pi\)', 'rho': r'\(\rho\)', 'sigma': r'\(\sigma\)', 'tau':
        r'\(\tau\)', 'upsilon': r'\(\upsilon\)', 'phi': r'\(\phi\)',
        'chi': r'\(\chi\)', 'psi': r'\(\psi\)', 'omega':
        r'\(\omega\)',
        
        'Alpha': r'\(\alpha\)', 'Beta': r'\(\beta\)', 'Gamma':
        r'\(\Gamma\)', 'Delta': r'\(\Delta\)', 'Epsilon':
        r'\(\epsilon\)', 'Zeta': r'\(\zeta\)', 'Eta': r'\(\eta\)',
        'Theta': r'\(\Theta\)', 'Iota': r'\(\iota\)', 'Kappa':
        r'\(\kappa\)', 'Lambda': r'\(\Lambda\)', 'Mu': r'\(\mu\)',
        'Nu': r'\(\nu\)', 'Xi': r'\(\Xi\)', 'Omicron': r'\(o\)', 'Pi':
        r'\(\Pi\)', 'ho': r'\(\rho\)', 'Sigma': r'\(\Sigma\)', 'Tau':
        r'\(\tau\)', 'Upsilon': r'\(\Upsilon\)', 'Phi': r'\(\Phi\)',
        'Chi': r'\(\chi\)', 'Psi': r'\(\Psi\)', 'Omega':
        r'\(\Omega\)',
    
        # HTML character entities
        'larr': r'\(\leftarrow\)', 'rarr': r'\(\rightarrow\)', 'uarr':
        r'\(\uparrow\)', 'darr': r'\(\downarrow\)', 'harr':
        r'\(\leftrightarrow\)', 'crarr': r'\(\hookleftarrow\)',
        'lArr': r'\(\Leftarrow\)', 'rArr': r'\(\Rightarrow\)', 'uArr':
        r'\(\Uparrow\)', 'dArr': r'\(\Downarrow\)', 'hArr':
        r'\(\Leftrightarrow\)', 'copy': r'{\textcopyright}',
        'times': r'\(\times\)', 'forall': r'\(\forall\)', 'exist':
        r'\(\exists\)', 'part': r'\(\partial\)', 'empty':
        r'\(\emptyset\)', 'isin': r'\(\in\)', 'notin': r'\(\notin\)',
        'ni': r'\(\ni\)', 'prod': r'\(\prod\)', 'sum': r'\(\sum\)',
        'prop': r'\(\propto\)', 'infin': r'\(\infty\)', 'ang':
        r'\(\angle\)', 'and': r'\(\wedge\)', 'or': r'\(\vee\)', 'cap':
        r'\(\cap\)', 'cup': r'\(\cup\)', 'int': r'\(\int\)', 'there4':
        r'\(\therefore\)', 'sim': r'\(\sim\)', 'cong': r'\(\cong\)',
        'asymp': r'\(\approx\)', 'ne': r'\(\ne\)', 'equiv':
        r'\(\equiv\)', 'le': r'\(\le\)', 'ge': r'\(\ge\)', 'sub':
        r'\(\subset\)', 'sup': r'\(\supset\)', 'nsub': r'\(\supset\)',
        'sube': r'\(\subseteq\)', 'supe': r'\(\supseteq\)', 'oplus':
        r'\(\oplus\)', 'otimes': r'\(\otimes\)', 'perp': r'\(\perp\)',
    
        # Alternate (long) names
        'infinity': r'\(\infty\)', 'integral': r'\(\int\)', 'product':
        r'\(\prod\)', '<=': r'\(\le\)', '>=': r'\(\ge\)',
        }
    
    def __init__(self, dom_tree, **options):
        self._tree = dom_tree
        # Caching:
        self._html = self._latex = self._plaintext = None
        self._terms = None
        # inline option -- mark top-level children as inline.
        if options.get('inline') and self._tree is not None:
            for elt in self._tree.children:
                elt.attribs['inline'] = True

    def __str__(self):
        return str(self._tree)
        
    def to_html(self, docstring_linker, directory=None, docindex=None,
                context=None, **options):
        if self._html is not None: return self._html
        if self._tree is None: return ''
        indent = options.get('indent', 0)
        self._html = self._to_html(self._tree, docstring_linker, directory, 
                                   docindex, context, indent)
        return self._html

    def to_latex(self, docstring_linker, **options):
        if self._latex is not None: return self._latex
        if self._tree is None: return ''
        indent = options.get('indent', 0)
        self._hyperref = options.get('hyperref', 1)
        self._latex = self._to_latex(self._tree, docstring_linker, indent)
        return self._latex

    def to_plaintext(self, docstring_linker, **options):
        # [XX] don't cache -- different options might be used!!
        #if self._plaintext is not None: return self._plaintext
        if self._tree is None: return ''
        if 'indent' in options:
            self._plaintext = to_plaintext(self._tree,
                                           indent=options['indent'])
        else:
            self._plaintext = to_plaintext(self._tree)
        return self._plaintext

    def _index_term_key(self, tree):
        str = to_plaintext(tree)
        str = re.sub(r'\s\s+', '-', str)
        return "index-"+re.sub("[^a-zA-Z0-9]", "_", str)

    def _to_html(self, tree, linker, directory, docindex, context,
                 indent=0, seclevel=0):
        if isinstance(tree, basestring):
            return plaintext_to_html(tree)

        if tree.tag == 'epytext': indent -= 2
        if tree.tag == 'section': seclevel += 1

        # Process the variables first.
        variables = [self._to_html(c, linker, directory, docindex, context,
                                   indent+2, seclevel)
                    for c in tree.children]
    
        # Construct the HTML string for the variables.
        childstr = ''.join(variables)
    
        # Perform the approriate action for the DOM tree type.
        if tree.tag == 'para':
            return wordwrap(
                (tree.attribs.get('inline') and '%s' or '<p>%s</p>') % childstr,
                indent)
        elif tree.tag == 'code':
            style = tree.attribs.get('style')
            if style:
                return '<code class="%s">%s</code>' % (style, childstr)
            else:
                return '<code>%s</code>' % childstr
        elif tree.tag == 'uri':
            return ('<a href="%s" target="_top">%s</a>' %
                    (variables[1], variables[0]))
        elif tree.tag == 'link':
            return linker.translate_identifier_xref(variables[1], variables[0])
        elif tree.tag == 'italic':
            return '<i>%s</i>' % childstr
        elif tree.tag == 'math':
            return '<i class="math">%s</i>' % childstr
        elif tree.tag == 'indexed':
            term = Element('epytext', *tree.children, **tree.attribs)
            return linker.translate_indexterm(ParsedEpytextDocstring(term))
            #term_key = self._index_term_key(tree)
            #return linker.translate_indexterm(childstr, term_key)
        elif tree.tag == 'bold':
            return '<b>%s</b>' % childstr
        elif tree.tag == 'ulist':
            return '%s<ul>\n%s%s</ul>\n' % (indent*' ', childstr, indent*' ')
        elif tree.tag == 'olist':
            start = tree.attribs.get('start') or ''
            return ('%s<ol start="%s">\n%s%s</ol>\n' %
                    (indent*' ', start, childstr, indent*' '))
        elif tree.tag == 'li':
            return indent*' '+'<li>\n%s%s</li>\n' % (childstr, indent*' ')
        elif tree.tag == 'heading':
            return ('%s<h%s class="heading">%s</h%s>\n' %
                    ((indent-2)*' ', seclevel, childstr, seclevel))
        elif tree.tag == 'literalblock':
            return '<pre class="literalblock">\n%s\n</pre>\n' % childstr
        elif tree.tag == 'doctestblock':
            return doctest_to_html(tree.children[0].strip())
        elif tree.tag == 'fieldlist':
            raise AssertionError("There should not be any field lists left")
        elif tree.tag in ('epytext', 'section', 'tag', 'arg',
                              'name', 'target', 'html'):
            return childstr
        elif tree.tag == 'symbol':
            symbol = tree.children[0]
            return self.SYMBOL_TO_HTML.get(symbol, '[%s]' % symbol)
        elif tree.tag == 'graph':
            # Generate the graph.
            graph = self._build_graph(variables[0], variables[1:], linker,
                                      docindex, context)
            if not graph: return ''
            # Write the graph.
            image_url = '%s.gif' % graph.uid
            image_file = os.path.join(directory, image_url)
            return graph.to_html(image_file, image_url)
        else:
            raise ValueError('Unknown epytext DOM element %r' % tree.tag)

    #GRAPH_TYPES = ['classtree', 'packagetree', 'importgraph']
    def _build_graph(self, graph_type, graph_args, linker, 
                     docindex, context):
        # Generate the graph
        if graph_type == 'classtree':
            from epydoc.apidoc import ClassDoc
            if graph_args:
                bases = [docindex.find(name, context)
                         for name in graph_args]
            elif isinstance(context, ClassDoc):
                bases = [context]
            else:
                log.warning("Could not construct class tree: you must "
                            "specify one or more base classes.")
                return None
            from epydoc.docwriter.dotgraph import class_tree_graph
            return class_tree_graph(bases, linker, context)
        elif graph_type == 'packagetree':
            from epydoc.apidoc import ModuleDoc
            if graph_args:
                packages = [docindex.find(name, context)
                            for name in graph_args]
            elif isinstance(context, ModuleDoc):
                packages = [context]
            else:
                log.warning("Could not construct package tree: you must "
                            "specify one or more root packages.")
                return None
            from epydoc.docwriter.dotgraph import package_tree_graph
            return package_tree_graph(packages, linker, context)
        elif graph_type == 'importgraph':
            from epydoc.apidoc import ModuleDoc
            modules = [d for d in docindex.root if isinstance(d, ModuleDoc)]
            from epydoc.docwriter.dotgraph import import_graph
            return import_graph(modules, docindex, linker, context)

        elif graph_type == 'callgraph':
            if graph_args:
                docs = [docindex.find(name, context) for name in graph_args]
                docs = [doc for doc in docs if doc is not None]
            else:
                docs = [context]
            from epydoc.docwriter.dotgraph import call_graph
            return call_graph(docs, docindex, linker, context)
        else:
            log.warning("Unknown graph type %s" % graph_type)
            
    
    def _to_latex(self, tree, linker, indent=0, seclevel=0, breakany=0):
        if isinstance(tree, basestring):
            return plaintext_to_latex(tree, breakany=breakany)

        if tree.tag == 'section': seclevel += 1
    
        # Figure out the child indent level.
        if tree.tag == 'epytext': cindent = indent
        else: cindent = indent + 2
        variables = [self._to_latex(c, linker, cindent, seclevel, breakany)
                    for c in tree.children]
        childstr = ''.join(variables)
    
        if tree.tag == 'para':
            return wordwrap(childstr, indent)+'\n'
        elif tree.tag == 'code':
            return '\\texttt{%s}' % childstr
        elif tree.tag == 'uri':
            if len(variables) != 2: raise ValueError('Bad URI ')
            if self._hyperref:
                # ~ and # should not be escaped in the URI.
                uri = tree.children[1].children[0]
                uri = uri.replace('{\\textasciitilde}', '~')
                uri = uri.replace('\\#', '#')
                if variables[0] == variables[1]:
                    return '\\href{%s}{\\textit{%s}}' % (uri, variables[1])
                else:
                    return ('%s\\footnote{\\href{%s}{%s}}' %
                            (variables[0], uri, variables[1]))
            else:
                if variables[0] == variables[1]:
                    return '\\textit{%s}' % variables[1]
                else:
                    return '%s\\footnote{%s}' % (variables[0], variables[1])
        elif tree.tag == 'link':
            if len(variables) != 2: raise ValueError('Bad Link')
            return linker.translate_identifier_xref(variables[1], variables[0])
        elif tree.tag == 'italic':
            return '\\textit{%s}' % childstr
        elif tree.tag == 'math':
            return '\\textit{%s}' % childstr
        elif tree.tag == 'indexed':
            term = Element('epytext', *tree.children, **tree.attribs)
            return linker.translate_indexterm(ParsedEpytextDocstring(term))
        elif tree.tag == 'bold':
            return '\\textbf{%s}' % childstr
        elif tree.tag == 'li':
            return indent*' ' + '\\item ' + childstr.lstrip()
        elif tree.tag == 'heading':
            return ' '*(indent-2) + '(section) %s\n\n' % childstr
        elif tree.tag == 'doctestblock':
            return doctest_to_latex(tree.children[0].strip())
        elif tree.tag == 'literalblock':
            return '\\begin{alltt}\n%s\\end{alltt}\n\n' % childstr
        elif tree.tag == 'fieldlist':
            return indent*' '+'{omitted fieldlist}\n'
        elif tree.tag == 'olist':
            return (' '*indent + '\\begin{enumerate}\n\n' + 
                    ' '*indent + '\\setlength{\\parskip}{0.5ex}\n' +
                    childstr +
                    ' '*indent + '\\end{enumerate}\n\n')
        elif tree.tag == 'ulist':
            return (' '*indent + '\\begin{itemize}\n' +
                    ' '*indent + '\\setlength{\\parskip}{0.6ex}\n' +
                    childstr +
                    ' '*indent + '\\end{itemize}\n\n')
        elif tree.tag == 'symbol':
            symbol = tree.children[0]
            return self.SYMBOL_TO_LATEX.get(symbol, '[%s]' % symbol)
        elif tree.tag == 'graph':
            return '(GRAPH)'
            #raise ValueError, 'graph not implemented yet for latex'
        else:
            # Assume that anything else can be passed through.
            return childstr

    _SUMMARY_RE = re.compile(r'(\s*[\w\W]*?\.)(\s|$)')

    def summary(self):
        if self._tree is None: return self, False
        tree = self._tree
        doc = Element('epytext')
    
        # Find the first paragraph.
        variables = tree.children
        while (len(variables) > 0) and (variables[0].tag != 'para'):
            if variables[0].tag in ('section', 'ulist', 'olist', 'li'):
                variables = variables[0].children
            else:
                variables = variables[1:]
    
        # Special case: if the docstring contains a single literal block,
        # then try extracting the summary from it.
        if (len(variables) == 0 and len(tree.children) == 1 and
            tree.children[0].tag == 'literalblock'):
            str = re.split(r'\n\s*(\n|$).*',
                           tree.children[0].children[0], 1)[0]
            variables = [Element('para')]
            variables[0].children.append(str)
    
        # If we didn't find a paragraph, return an empty epytext.
        if len(variables) == 0: return ParsedEpytextDocstring(doc), False
    
        # Is there anything else, excluding tags, after the first variable?
        long_docs = False
        for var in variables[1:]:
            if isinstance(var, Element) and var.tag == 'fieldlist':
                continue
            long_docs = True
            break
        
        # Extract the first sentence.
        parachildren = variables[0].children
        para = Element('para', inline=True)
        doc.children.append(para)
        for parachild in parachildren:
            if isinstance(parachild, basestring):
                m = self._SUMMARY_RE.match(parachild)
                if m:
                    para.children.append(m.group(1))
                    long_docs |= parachild is not parachildren[-1]
                    if not long_docs:
                        other = parachild[m.end():]
                        if other and not other.isspace():
                            long_docs = True
                    return ParsedEpytextDocstring(doc), long_docs
            para.children.append(parachild)

        return ParsedEpytextDocstring(doc), long_docs

    def split_fields(self, errors=None):
        if self._tree is None: return (self, ())
        tree = Element(self._tree.tag, *self._tree.children,
                       **self._tree.attribs)
        fields = []

        if (tree.children and
            tree.children[-1].tag == 'fieldlist' and
            tree.children[-1].children):
            field_nodes = tree.children[-1].children
            del tree.children[-1]

            for field in field_nodes:
                # Get the tag
                tag = field.children[0].children[0].lower()
                del field.children[0]

                # Get the argument.
                if field.children and field.children[0].tag == 'arg':
                    arg = field.children[0].children[0]
                    del field.children[0]
                else:
                    arg = None

                # Process the field.
                field.tag = 'epytext'
                fields.append(Field(tag, arg, ParsedEpytextDocstring(field)))

        # Save the remaining docstring as the description..
        if tree.children and tree.children[0].children:
            return ParsedEpytextDocstring(tree), fields
        else:
            return None, fields

    
    def index_terms(self):
        if self._terms is None:
            self._terms = []
            self._index_terms(self._tree, self._terms)
        return self._terms

    def _index_terms(self, tree, terms):
        if tree is None or isinstance(tree, basestring):
            return
        
        if tree.tag == 'indexed':
            term = Element('epytext', *tree.children, **tree.attribs)
            terms.append(ParsedEpytextDocstring(term))

        # Look for index items in child nodes.
        for child in tree.children:
            self._index_terms(child, terms)

########NEW FILE########
__FILENAME__ = javadoc
#
# javadoc.py: javadoc docstring parsing
# Edward Loper
#
# Created [07/03/03 12:37 PM]
# $Id: javadoc.py 1574 2007-03-07 02:55:14Z dvarrazzo $
#

"""
Epydoc parser for U{Javadoc<http://java.sun.com/j2se/javadoc/>}
docstrings.  Javadoc is an HTML-based markup language that was
developed for documenting Java APIs with inline comments.  It consists
of raw HTML, augmented by Javadoc tags.  There are two types of
Javadoc tag:

  - X{Javadoc block tags} correspond to Epydoc fields.  They are
    marked by starting a line with a string of the form \"C{@M{tag}
    [M{arg}]}\", where C{M{tag}} indicates the type of block, and
    C{M{arg}} is an optional argument.  (For fields that take
    arguments, Javadoc assumes that the single word immediately
    following the tag is an argument; multi-word arguments cannot be
    used with javadoc.)  
  
  - X{inline Javadoc tags} are used for inline markup.  In particular,
    epydoc uses them for crossreference links between documentation.
    Inline tags may appear anywhere in the text, and have the form
    \"C{{@M{tag} M{[args...]}}}\", where C{M{tag}} indicates the
    type of inline markup, and C{M{args}} are optional arguments.

Epydoc supports all Javadoc tags, I{except}:
  - C{{@docRoot}}, which gives the (relative) URL of the generated
    documentation's root.
  - C{{@inheritDoc}}, which copies the documentation of the nearest
    overridden object.  This can be used to combine the documentation
    of the overridden object with the documentation of the
    overridding object.
  - C{@serial}, C{@serialField}, and C{@serialData} which describe the
    serialization (pickling) of an object.
  - C{{@value}}, which copies the value of a constant.

@warning: Epydoc only supports HTML output for Javadoc docstrings.
"""
__docformat__ = 'epytext en'

# Imports
import re
from xml.dom.minidom import *
from epydoc.markup import *

def parse_docstring(docstring, errors, **options):
    """
    Parse the given docstring, which is formatted using Javadoc; and
    return a C{ParsedDocstring} representation of its contents.
    @param docstring: The docstring to parse
    @type docstring: C{string}
    @param errors: A list where any errors generated during parsing
        will be stored.
    @type errors: C{list} of L{ParseError}
    @param options: Extra options.  Unknown options are ignored.
        Currently, no extra options are defined.
    @rtype: L{ParsedDocstring}
    """
    return ParsedJavadocDocstring(docstring, errors)

class ParsedJavadocDocstring(ParsedDocstring):
    """
    An encoded version of a Javadoc docstring.  Since Javadoc is a
    fairly simple markup language, we don't do any processing in
    advance; instead, we wait to split fields or resolve
    crossreference links until we need to.

    @group Field Splitting: split_fields, _ARG_FIELDS, _FIELD_RE
    @cvar _ARG_FIELDS: A list of the fields that take arguments.
        Since Javadoc doesn't mark arguments in any special way, we
        must consult this list to decide whether the first word of a
        field is an argument or not.
    @cvar _FIELD_RE: A regular expression used to search for Javadoc
        block tags.

    @group HTML Output: to_html, _LINK_SPLIT_RE, _LINK_RE
    @cvar _LINK_SPLIT_RE: A regular expression used to search for
        Javadoc inline tags.
    @cvar _LINK_RE: A regular expression used to process Javadoc
        inline tags.
    """
    def __init__(self, docstring, errors=None):
        """
        Create a new C{ParsedJavadocDocstring}.
        
        @param docstring: The docstring that should be used to
            construct this C{ParsedJavadocDocstring}.
        @type docstring: C{string}
        @param errors: A list where any errors generated during
            parsing will be stored.  If no list is given, then
            all errors are ignored.
        @type errors: C{list} of L{ParseError}
        """
        self._docstring = docstring
        if errors is None: errors = []
        self._check_links(errors)

    #////////////////////////////////////////////////////////////
    # Field Splitting
    #////////////////////////////////////////////////////////////

    _ARG_FIELDS = ('group variable var type cvariable cvar ivariable '+
                   'ivar param '+
                   'parameter arg argument raise raises exception '+
                   'except deffield newfield keyword kwarg kwparam').split()
    _FIELD_RE = re.compile(r'(^\s*\@\w+[\s$])', re.MULTILINE)
    
    # Inherit docs from ParsedDocstring.
    def split_fields(self, errors=None):

        # Split the docstring into an alternating list of field tags
        # and text (odd pieces are field tags).
        pieces = self._FIELD_RE.split(self._docstring)

        # The first piece is the description.
        descr = ParsedJavadocDocstring(pieces[0])

        # The remaining pieces are the block fields (alternating tags
        # and bodies; odd pieces are tags).
        fields = []
        for i in range(1, len(pieces)):
            if i%2 == 1:
                # Get the field tag.
                tag = pieces[i].strip()[1:]
            else:
                # Get the field argument (if appropriate).
                if tag in self._ARG_FIELDS:
                    subpieces = pieces[i].strip().split(None, 1)+['','']
                    (arg, body) = subpieces[:2]
                else:
                    (arg, body) = (None, pieces[i])

                # Special processing for @see fields, since Epydoc
                # allows unrestricted text in them, but Javadoc just
                # uses them for xref links:
                if tag == 'see' and body:
                    if body[0] in '"\'':
                        if body[-1] == body[0]: body = body[1:-1]
                    elif body[0] == '<': pass
                    else: body = '{@link %s}' % body

                # Construct the field.
                parsed_body = ParsedJavadocDocstring(body)
                fields.append(Field(tag, arg, parsed_body))

        if pieces[0].strip():
            return (descr, fields)
        else:
            return (None, fields)

    #////////////////////////////////////////////////////////////
    # HTML Output.
    #////////////////////////////////////////////////////////////

    _LINK_SPLIT_RE = re.compile(r'({@link(?:plain)?\s[^}]+})')
    _LINK_RE = re.compile(r'{@link(?:plain)?\s+' + r'([\w#.]+)' +
                          r'(?:\([^\)]*\))?' + r'(\s+.*)?' + r'}')

    # Inherit docs from ParsedDocstring.
    def to_html(self, docstring_linker, **options):
        # Split the docstring into an alternating list of HTML and
        # links (odd pieces are links).
        pieces = self._LINK_SPLIT_RE.split(self._docstring)

        # This function is used to translate {@link ...}s to HTML.
        translate_xref = docstring_linker.translate_identifier_xref
        
        # Build up the HTML string from the pieces.  For HTML pieces
        # (even), just add it to html.  For link pieces (odd), use
        # docstring_linker to translate the crossreference link to
        # HTML for us.
        html = ''
        for i in range(len(pieces)):
            if i%2 == 0:
                html += pieces[i]
            else:
                # Decompose the link into pieces.
                m = self._LINK_RE.match(pieces[i])
                if m is None: continue # Error flagged by _check_links
                (target, name) = m.groups()

                # Normalize the target name.
                if target[0] == '#': target = target[1:]
                target = target.replace('#', '.')
                target = re.sub(r'\(.*\)', '', target)

                # Provide a name, if it wasn't specified.
                if name is None: name = target
                else: name = name.strip()

                # Use docstring_linker to convert the name to html.
                html += translate_xref(target, name)
        return html

    def _check_links(self, errors):
        """
        Make sure that all @{link}s are valid.  We need a separate
        method for ths because we want to do this at parse time, not
        html output time.  Any errors found are appended to C{errors}.
        """
        pieces = self._LINK_SPLIT_RE.split(self._docstring)
        linenum = 0
        for i in range(len(pieces)):
            if i%2 == 1 and not self._LINK_RE.match(pieces[i]):
                estr = 'Bad link %r' % pieces[i]
                errors.append(ParseError(estr, linenum, is_fatal=0))
            linenum += pieces[i].count('\n')

    #////////////////////////////////////////////////////////////
    # Plaintext Output.
    #////////////////////////////////////////////////////////////

    # Inherit docs from ParsedDocstring.  Since we don't define
    # to_latex, this is used when generating latex output.
    def to_plaintext(self, docstring_linker, **options):
        return self._docstring

    _SUMMARY_RE = re.compile(r'(\s*[\w\W]*?\.)(\s|$)')

    # Jeff's hack to get summary working
    def summary(self):
        # Drop tags
        doc = "\n".join([ row for row in self._docstring.split('\n')
                          if not row.lstrip().startswith('@') ])

        m = self._SUMMARY_RE.match(doc)
        if m:
            other = doc[m.end():]
            return (ParsedJavadocDocstring(m.group(1)),
                    other != '' and not other.isspace())
            
        else:
            parts = doc.strip('\n').split('\n', 1)
            if len(parts) == 1:
                summary = parts[0]
                other = False
            else:
                summary = parts[0] + '...'
                other = True
            
            return ParsedJavadocDocstring(summary), other
        
#     def concatenate(self, other):
#         if not isinstance(other, ParsedJavadocDocstring):
#             raise ValueError, 'Could not concatenate docstrings'
#         return ParsedJavadocDocstring(self._docstring+other._docstring)

########NEW FILE########
__FILENAME__ = plaintext
#
# plaintext.py: plaintext docstring parsing
# Edward Loper
#
# Created [04/10/01 12:00 AM]
# $Id: plaintext.py 1574 2007-03-07 02:55:14Z dvarrazzo $
#

"""
Parser for plaintext docstrings.  Plaintext docstrings are rendered as
verbatim output, preserving all whitespace.
"""
__docformat__ = 'epytext en'

from epydoc.markup import *
from epydoc.util import plaintext_to_html, plaintext_to_latex

def parse_docstring(docstring, errors, **options):
    """
    @return: A pair C{(M{d}, M{e})}, where C{M{d}} is a
        C{ParsedDocstring} that encodes the contents of the given
        plaintext docstring; and C{M{e}} is a list of errors that were
        generated while parsing the docstring.
    @rtype: C{L{ParsedPlaintextDocstring}, C{list} of L{ParseError}}
    """
    return ParsedPlaintextDocstring(docstring, **options)

class ParsedPlaintextDocstring(ParsedDocstring):
    def __init__(self, text, **options):
        self._verbatim = options.get('verbatim', 1)
        if text is None: raise ValueError, 'Bad text value (expected a str)'
        self._text = text

    def to_html(self, docstring_linker, **options):
        if options.get('verbatim', self._verbatim) == 0:
            return plaintext_to_html(self.to_plaintext(docstring_linker))
        else:
            return ParsedDocstring.to_html(self, docstring_linker, **options)

    def to_latex(self, docstring_linker, **options):
        if options.get('verbatim', self._verbatim) == 0:
            return plaintext_to_latex(self.to_plaintext(docstring_linker))
        else:
            return ParsedDocstring.to_latex(self, docstring_linker, **options)

    def to_plaintext(self, docstring_linker, **options):
        if 'indent' in options:
            indent = options['indent']
            lines = self._text.split('\n')
            return '\n'.join([' '*indent+l for l in lines])+'\n'
        return self._text+'\n'
    
    _SUMMARY_RE = re.compile(r'(\s*[\w\W]*?(?:\.(\s|$)|[\n][\t ]*[\n]))')

    def summary(self):
        m = self._SUMMARY_RE.match(self._text)
        if m:
            other = self._text[m.end():]
            return (ParsedPlaintextDocstring(m.group(1), verbatim=0),
                    other != '' and not other.isspace())
        else:
            parts = self._text.strip('\n').split('\n', 1)
            if len(parts) == 1:
                summary = parts[0]
                other = False
            else:
                summary = parts[0] + '...'
                other = True
                
            return ParsedPlaintextDocstring(summary, verbatim=0), other
        
#     def concatenate(self, other):
#         if not isinstance(other, ParsedPlaintextDocstring):
#             raise ValueError, 'Could not concatenate docstrings'
#         text = self._text+other._text
#         options = self._options.copy()
#         options.update(other._options)
#         return ParsedPlaintextDocstring(text, options)

########NEW FILE########
__FILENAME__ = pyval_repr
# epydoc -- Marked-up Representations for Python Values
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: apidoc.py 1448 2007-02-11 00:05:34Z dvarrazzo $

"""
Syntax highlighter for Python values.  Currently provides special
colorization support for:

  - lists, tuples, sets, frozensets, dicts
  - numbers
  - strings
  - compiled regexps

The highlighter also takes care of line-wrapping, and automatically
stops generating repr output as soon as it has exceeded the specified
number of lines (which should make it faster than pprint for large
values).  It does I{not} bother to do automatic cycle detection,
because maxlines is typically around 5, so it's really not worth it.

The syntax-highlighted output is encoded using a
L{ParsedEpytextDocstring}, which can then be used to generate output in
a variety of formats.
"""
__docformat__ = 'epytext en'

# Implementation note: we use exact tests for classes (list, etc)
# rather than using isinstance, because subclasses might override
# __repr__.

import types, re
import epydoc.apidoc
from epydoc.util import decode_with_backslashreplace
from epydoc.util import plaintext_to_html, plaintext_to_latex
from epydoc.compat import *
import sre_parse, sre_constants

from epydoc.markup.epytext import Element, ParsedEpytextDocstring

def is_re_pattern(pyval):
    return type(pyval).__name__ == 'SRE_Pattern'

class _ColorizerState:
    """
    An object uesd to keep track of the current state of the pyval
    colorizer.  The L{mark()}/L{restore()} methods can be used to set
    a backup point, and restore back to that backup point.  This is
    used by several colorization methods that first try colorizing
    their object on a single line (setting linebreakok=False); and
    then fall back on a multi-line output if that fails.  The L{score}
    variable is used to keep track of a 'score', reflecting how good
    we think this repr is.  E.g., unhelpful values like '<Foo instance
    at 0x12345>' get low scores.  If the score is too low, we'll use
    the parse-derived repr instead.
    """
    def __init__(self):
        self.result = []
        self.charpos = 0
        self.lineno = 1
        self.linebreakok = True
        
        #: How good this represention is?
        self.score = 0

    def mark(self):
        return (len(self.result), self.charpos,
                self.lineno, self.linebreakok, self.score)

    def restore(self, mark):
        n, self.charpos, self.lineno, self.linebreakok, self.score = mark
        del self.result[n:]

class _Maxlines(Exception):
    """A control-flow exception that is raised when PyvalColorizer
    exeeds the maximum number of allowed lines."""
    
class _Linebreak(Exception):
    """A control-flow exception that is raised when PyvalColorizer
    generates a string containing a newline, but the state object's
    linebreakok variable is False."""

class ColorizedPyvalRepr(ParsedEpytextDocstring):
    """
    @ivar score: A score, evaluating how good this repr is.
    @ivar is_complete: True if this colorized repr completely describes
       the object.
    """
    def __init__(self, tree, score, is_complete):
        ParsedEpytextDocstring.__init__(self, tree)
        self.score = score
        self.is_complete = is_complete

def colorize_pyval(pyval, parse_repr=None, min_score=None,
                   linelen=75, maxlines=5, linebreakok=True, sort=True):
    return PyvalColorizer(linelen, maxlines, linebreakok, sort).colorize(
        pyval, parse_repr, min_score)

class PyvalColorizer:
    """
    Syntax highlighter for Python values.
    """

    def __init__(self, linelen=75, maxlines=5, linebreakok=True, sort=True):
        self.linelen = linelen
        self.maxlines = maxlines
        self.linebreakok = linebreakok
        self.sort = sort

    #////////////////////////////////////////////////////////////
    # Colorization Tags & other constants
    #////////////////////////////////////////////////////////////

    GROUP_TAG = 'variable-group'     # e.g., "[" and "]"
    COMMA_TAG = 'variable-op'        # The "," that separates elements
    COLON_TAG = 'variable-op'        # The ":" in dictionaries
    CONST_TAG = None                 # None, True, False
    NUMBER_TAG = None                # ints, floats, etc
    QUOTE_TAG = 'variable-quote'     # Quotes around strings.
    STRING_TAG = 'variable-string'   # Body of string literals

    RE_CHAR_TAG = None
    RE_GROUP_TAG = 're-group'
    RE_REF_TAG = 're-ref'
    RE_OP_TAG = 're-op'
    RE_FLAGS_TAG = 're-flags'

    ELLIPSIS = Element('code', u'...', style='variable-ellipsis')
    LINEWRAP = Element('symbol', u'crarr')
    UNKNOWN_REPR = Element('code', u'??', style='variable-unknown')
    
    GENERIC_OBJECT_RE = re.compile(r'^<.* at 0x[0-9a-f]+>$', re.IGNORECASE)

    ESCAPE_UNICODE = False # should we escape non-ascii unicode chars?

    #////////////////////////////////////////////////////////////
    # Entry Point
    #////////////////////////////////////////////////////////////

    def colorize(self, pyval, parse_repr=None, min_score=None):
        """
        @return: A L{ColorizedPyvalRepr} describing the given pyval.
        """
        UNKNOWN = epydoc.apidoc.UNKNOWN
        # Create an object to keep track of the colorization.
        state = _ColorizerState()
        state.linebreakok = self.linebreakok
        # Colorize the value.  If we reach maxlines, then add on an
        # ellipsis marker and call it a day.
        try:
            if pyval is not UNKNOWN:
                self._colorize(pyval, state)
            elif parse_repr not in (None, UNKNOWN):
                self._output(parse_repr, None, state)
            else:
                state.result.append(PyvalColorizer.UNKNOWN_REPR)
            is_complete = True
        except (_Maxlines, _Linebreak):
            if self.linebreakok:
                state.result.append('\n')
                state.result.append(self.ELLIPSIS)
            else:
                if state.result[-1] is self.LINEWRAP:
                    state.result.pop()
                self._trim_result(state.result, 3)
                state.result.append(self.ELLIPSIS)
            is_complete = False
        # If we didn't score high enough, then try again.
        if (pyval is not UNKNOWN and parse_repr not in (None, UNKNOWN)
            and min_score is not None and state.score < min_score):
            return self.colorize(UNKNOWN, parse_repr)
        # Put it all together.
        tree = Element('epytext', *state.result)
        return ColorizedPyvalRepr(tree, state.score, is_complete)

    def _colorize(self, pyval, state):
        pyval_type = type(pyval)
        state.score += 1
        
        if pyval is None or pyval is True or pyval is False:
            self._output(unicode(pyval), self.CONST_TAG, state)
        elif pyval_type in (int, float, long, types.ComplexType):
            self._output(unicode(pyval), self.NUMBER_TAG, state)
        elif pyval_type is str:
            self._colorize_str(pyval, state, '', 'string-escape')
        elif pyval_type is unicode:
            if self.ESCAPE_UNICODE:
                self._colorize_str(pyval, state, 'u', 'unicode-escape')
            else:
                self._colorize_str(pyval, state, 'u', None)
        elif pyval_type is list:
            self._multiline(self._colorize_iter, pyval, state, '[', ']')
        elif pyval_type is tuple:
            self._multiline(self._colorize_iter, pyval, state, '(', ')')
        elif pyval_type is set:
            self._multiline(self._colorize_iter, self._sort(pyval),
                            state, 'set([', '])')
        elif pyval_type is frozenset:
            self._multiline(self._colorize_iter, self._sort(pyval),
                            state, 'frozenset([', '])')
        elif pyval_type is dict:
            self._multiline(self._colorize_dict, self._sort(pyval.items()),
                            state, '{', '}')
        elif is_re_pattern(pyval):
            self._colorize_re(pyval, state)
        else:
            try:
                pyval_repr = repr(pyval)
                if not isinstance(pyval_repr, (str, unicode)):
                    pyval_repr = unicode(pyval_repr)
                pyval_repr_ok = True
            except KeyboardInterrupt:
                raise
            except:
                pyval_repr_ok = False
                state.score -= 100

            if pyval_repr_ok:
                if self.GENERIC_OBJECT_RE.match(pyval_repr):
                    state.score -= 5
                self._output(pyval_repr, None, state)
            else:
                state.result.append(self.UNKNOWN_REPR)

    def _sort(self, items):
        if not self.sort: return items
        try: return sorted(items)
        except KeyboardInterrupt: raise
        except: return items
        
    def _trim_result(self, result, num_chars):
        while num_chars > 0:
            if not result: return 
            if isinstance(result[-1], Element):
                assert len(result[-1].children) == 1
                trim = min(num_chars, len(result[-1].children[0]))
                result[-1].children[0] = result[-1].children[0][:-trim]
                if not result[-1].children[0]: result.pop()
                num_chars -= trim
            else:
                trim = min(num_chars, len(result[-1]))
                result[-1] = result[-1][:-trim]
                if not result[-1]: result.pop()
                num_chars -= trim

    #////////////////////////////////////////////////////////////
    # Object Colorization Functions
    #////////////////////////////////////////////////////////////

    def _multiline(self, func, pyval, state, *args):
        """
        Helper for container-type colorizers.  First, try calling
        C{func(pyval, state, *args)} with linebreakok set to false;
        and if that fails, then try again with it set to true.
        """
        linebreakok = state.linebreakok
        mark = state.mark()
        
        try:
            state.linebreakok = False
            func(pyval, state, *args)
            state.linebreakok = linebreakok

        except _Linebreak:
            if not linebreakok:
                raise
            state.restore(mark)
            func(pyval, state, *args)
            
    def _colorize_iter(self, pyval, state, prefix, suffix):
        self._output(prefix, self.GROUP_TAG, state)
        indent = state.charpos
        for i, elt in enumerate(pyval):
            if i>=1:
                if state.linebreakok:
                    self._output(',', self.COMMA_TAG, state)
                    self._output('\n'+' '*indent, None, state)
                else:
                    self._output(', ', self.COMMA_TAG, state)
            self._colorize(elt, state)
        self._output(suffix, self.GROUP_TAG, state)

    def _colorize_dict(self, items, state, prefix, suffix):
        self._output(prefix, self.GROUP_TAG, state)
        indent = state.charpos
        for i, (key, val) in enumerate(items):
            if i>=1:
                if state.linebreakok:
                    self._output(',', self.COMMA_TAG, state)
                    self._output('\n'+' '*indent, None, state)
                else:
                    self._output(', ', self.COMMA_TAG, state)
            self._colorize(key, state)
            self._output(': ', self.COLON_TAG, state)
            self._colorize(val, state)
        self._output(suffix, self.GROUP_TAG, state)

    def _colorize_str(self, pyval, state, prefix, encoding):
        # Decide which quote to use.
        if '\n' in pyval and state.linebreakok: quote = "'''"
        else: quote = "'"
        # Divide the string into lines.
        if state.linebreakok:
            lines = pyval.split('\n')
        else:
            lines = [pyval]
        # Open quote.
        self._output(prefix+quote, self.QUOTE_TAG, state)
        # Body
        for i, line in enumerate(lines):
            if i>0: self._output('\n', None, state)
            if encoding: line = line.encode(encoding)
            self._output(line, self.STRING_TAG, state)
        # Close quote.
        self._output(quote, self.QUOTE_TAG, state)

    def _colorize_re(self, pyval, state):
        # Extract the flag & pattern from the regexp.
        pat, flags = pyval.pattern, pyval.flags
        # If the pattern is a string, decode it to unicode.
        if isinstance(pat, str):
            pat = decode_with_backslashreplace(pat)
        # Parse the regexp pattern.
        tree = sre_parse.parse(pat, flags)
        groups = dict([(num,name) for (name,num) in
                       tree.pattern.groupdict.items()])
        # Colorize it!
        self._output("re.compile(r'", None, state)
        self._colorize_re_flags(tree.pattern.flags, state)
        self._colorize_re_tree(tree, state, True, groups)
        self._output("')", None, state)

    def _colorize_re_flags(self, flags, state):
        if flags:
            flags = [c for (c,n) in sorted(sre_parse.FLAGS.items())
                     if (n&flags)]
            flags = '(?%s)' % ''.join(flags)
            self._output(flags, self.RE_FLAGS_TAG, state)

    def _colorize_re_tree(self, tree, state, noparen, groups):
        assert noparen in (True, False)
        if len(tree) > 1 and not noparen:
            self._output('(', self.RE_GROUP_TAG, state)
        for elt in tree:
            op = elt[0]
            args = elt[1]
    
            if op == sre_constants.LITERAL:
                c = unichr(args)
                # Add any appropriate escaping.
                if c in '.^$\\*+?{}[]|()\'': c = '\\'+c
                elif c == '\t': c = '\\t'
                elif c == '\r': c = '\\r'
                elif c == '\n': c = '\\n'
                elif c == '\f': c = '\\f'
                elif c == '\v': c = '\\v'
                elif ord(c) > 0xffff: c = r'\U%08x' % ord(c)
                elif ord(c) > 0xff: c = r'\u%04x' % ord(c)
                elif ord(c)<32 or ord(c)>=127: c = r'\x%02x' % ord(c)
                self._output(c, self.RE_CHAR_TAG, state)
            
            elif op == sre_constants.ANY:
                self._output('.', self.RE_CHAR_TAG, state)
                
            elif op == sre_constants.BRANCH:
                if args[0] is not None:
                    raise ValueError('Branch expected None arg but got %s'
                                     % args[0])
                for i, item in enumerate(args[1]):
                    if i > 0:
                        self._output('|', self.RE_OP_TAG, state)
                    self._colorize_re_tree(item, state, True, groups)
                
            elif op == sre_constants.IN:
                if (len(args) == 1 and args[0][0] == sre_constants.CATEGORY):
                    self._colorize_re_tree(args, state, False, groups)
                else:
                    self._output('[', self.RE_GROUP_TAG, state)
                    self._colorize_re_tree(args, state, True, groups)
                    self._output(']', self.RE_GROUP_TAG, state)
                    
            elif op == sre_constants.CATEGORY:
                if args == sre_constants.CATEGORY_DIGIT: val = r'\d'
                elif args == sre_constants.CATEGORY_NOT_DIGIT: val = r'\D'
                elif args == sre_constants.CATEGORY_SPACE: val = r'\s'
                elif args == sre_constants.CATEGORY_NOT_SPACE: val = r'\S'
                elif args == sre_constants.CATEGORY_WORD: val = r'\w'
                elif args == sre_constants.CATEGORY_NOT_WORD: val = r'\W'
                else: raise ValueError('Unknown category %s' % args)
                self._output(val, self.RE_CHAR_TAG, state)
                
            elif op == sre_constants.AT:
                if args == sre_constants.AT_BEGINNING_STRING: val = r'\A'
                elif args == sre_constants.AT_BEGINNING: val = r'^'
                elif args == sre_constants.AT_END: val = r'$'
                elif args == sre_constants.AT_BOUNDARY: val = r'\b'
                elif args == sre_constants.AT_NON_BOUNDARY: val = r'\B'
                elif args == sre_constants.AT_END_STRING: val = r'\Z'
                else: raise ValueError('Unknown position %s' % args)
                self._output(val, self.RE_CHAR_TAG, state)
                
            elif op in (sre_constants.MAX_REPEAT, sre_constants.MIN_REPEAT):
                minrpt = args[0]
                maxrpt = args[1]
                if maxrpt == sre_constants.MAXREPEAT:
                    if minrpt == 0:   val = '*'
                    elif minrpt == 1: val = '+'
                    else: val = '{%d,}' % (minrpt)
                elif minrpt == 0:
                    if maxrpt == 1: val = '?'
                    else: val = '{,%d}' % (maxrpt)
                elif minrpt == maxrpt:
                    val = '{%d}' % (maxrpt)
                else:
                    val = '{%d,%d}' % (minrpt, maxrpt)
                if op == sre_constants.MIN_REPEAT:
                    val += '?'
                    
                self._colorize_re_tree(args[2], state, False, groups)
                self._output(val, self.RE_OP_TAG, state)
                
            elif op == sre_constants.SUBPATTERN:
                if args[0] is None:
                    self._output('(?:', self.RE_GROUP_TAG, state)
                elif args[0] in groups:
                    self._output('(?P<', self.RE_GROUP_TAG, state)
                    self._output(groups[args[0]], self.RE_REF_TAG, state)
                    self._output('>', self.RE_GROUP_TAG, state)
                elif isinstance(args[0], (int, long)):
                    # This is cheating:
                    self._output('(', self.RE_GROUP_TAG, state)
                else:
                    self._output('(?P<', self.RE_GROUP_TAG, state)
                    self._output(args[0], self.RE_REF_TAG, state)
                    self._output('>', self.RE_GROUP_TAG, state)
                self._colorize_re_tree(args[1], state, True, groups)
                self._output(')', self.RE_GROUP_TAG, state)
    
            elif op == sre_constants.GROUPREF:
                self._output('\\%d' % args, self.RE_REF_TAG, state)
    
            elif op == sre_constants.RANGE:
                self._colorize_re_tree( ((sre_constants.LITERAL, args[0]),),
                                        state, False, groups )
                self._output('-', self.RE_OP_TAG, state)
                self._colorize_re_tree( ((sre_constants.LITERAL, args[1]),),
                                        state, False, groups )
                
            elif op == sre_constants.NEGATE:
                self._output('^', self.RE_OP_TAG, state)
    
            elif op == sre_constants.ASSERT:
                if args[0] > 0:
                    self._output('(?=', self.RE_GROUP_TAG, state)
                else:
                    self._output('(?<=', self.RE_GROUP_TAG, state)
                self._colorize_re_tree(args[1], state, True, groups)
                self._output(')', self.RE_GROUP_TAG, state)
                               
            elif op == sre_constants.ASSERT_NOT:
                if args[0] > 0:
                    self._output('(?!', self.RE_GROUP_TAG, state)
                else:
                    self._output('(?<!', self.RE_GROUP_TAG, state)
                self._colorize_re_tree(args[1], state, True, groups)
                self._output(')', self.RE_GROUP_TAG, state)
    
            elif op == sre_constants.NOT_LITERAL:
                self._output('[^', self.RE_GROUP_TAG, state)
                self._colorize_re_tree( ((sre_constants.LITERAL, args),),
                                        state, False, groups )
                self._output(']', self.RE_GROUP_TAG, state)
            else:
                log.error("Error colorizing regexp: unknown elt %r" % elt)
        if len(tree) > 1 and not noparen: 
            self._output(')', self.RE_GROUP_TAG, state)
                           
    #////////////////////////////////////////////////////////////
    # Output function
    #////////////////////////////////////////////////////////////

    def _output(self, s, tag, state):
        """
        Add the string `s` to the result list, tagging its contents
        with tag `tag`.  Any lines that go beyond `self.linelen` will
        be line-wrapped.  If the total number of lines exceeds
        `self.maxlines`, then raise a `_Maxlines` exception.
        """
        # Make sure the string is unicode.
        if isinstance(s, str):
            s = decode_with_backslashreplace(s)
        
        # Split the string into segments.  The first segment is the
        # content to add to the current line, and the remaining
        # segments are new lines.
        segments = s.split('\n')

        for i, segment in enumerate(segments):
            # If this isn't the first segment, then add a newline to
            # split it from the previous segment.
            if i > 0:
                if (state.lineno+1) > self.maxlines:
                    raise _Maxlines()
                if not state.linebreakok:
                    raise _Linebreak()
                state.result.append(u'\n')
                state.lineno += 1
                state.charpos = 0

            # If the segment fits on the current line, then just call
            # markup to tag it, and store the result.
            if state.charpos + len(segment) <= self.linelen:
                state.charpos += len(segment)
                if tag:
                    segment = Element('code', segment, style=tag)
                state.result.append(segment)

            # If the segment doesn't fit on the current line, then
            # line-wrap it, and insert the remainder of the line into
            # the segments list that we're iterating over.  (We'll go
            # the the beginning of the next line at the start of the
            # next iteration through the loop.)
            else:
                split = self.linelen-state.charpos
                segments.insert(i+1, segment[split:])
                segment = segment[:split]
                if tag:
                    segment = Element('code', segment, style=tag)
                state.result += [segment, self.LINEWRAP]


########NEW FILE########
__FILENAME__ = restructuredtext
#
# rst.py: ReStructuredText docstring parsing
# Edward Loper
#
# Created [06/28/03 02:52 AM]
# $Id: restructuredtext.py 1661 2007-11-07 12:59:34Z dvarrazzo $
#

"""
Epydoc parser for ReStructuredText strings.  ReStructuredText is the
standard markup language used by the Docutils project.
L{parse_docstring()} provides the primary interface to this module; it
returns a L{ParsedRstDocstring}, which supports all of the methods
defined by L{ParsedDocstring}.

L{ParsedRstDocstring} is basically just a L{ParsedDocstring} wrapper
for the C{docutils.nodes.document} class.

Creating C{ParsedRstDocstring}s
===============================

C{ParsedRstDocstring}s are created by the C{parse_document} function,
using the C{docutils.core.publish_string()} method, with the following
helpers:

  - An L{_EpydocReader} is used to capture all error messages as it
    parses the docstring.
  - A L{_DocumentPseudoWriter} is used to extract the document itself,
    without actually writing any output.  The document is saved for
    further processing.  The settings for the writer are copied from
    C{docutils.writers.html4css1.Writer}, since those settings will
    be used when we actually write the docstring to html.

Using C{ParsedRstDocstring}s
============================

C{ParsedRstDocstring}s support all of the methods defined by
C{ParsedDocstring}; but only the following four methods have
non-default behavior:

  - L{to_html()<ParsedRstDocstring.to_html>} uses an
    L{_EpydocHTMLTranslator} to translate the C{ParsedRstDocstring}'s
    document into an HTML segment.
  - L{split_fields()<ParsedRstDocstring.split_fields>} uses a
    L{_SplitFieldsTranslator} to divide the C{ParsedRstDocstring}'s
    document into its main body and its fields.  Special handling
    is done to account for consolidated fields.
  - L{summary()<ParsedRstDocstring.summary>} uses a
    L{_SummaryExtractor} to extract the first sentence from
    the C{ParsedRstDocstring}'s document.
  - L{to_plaintext()<ParsedRstDocstring.to_plaintext>} uses
    C{document.astext()} to convert the C{ParsedRstDocstring}'s
    document to plaintext.

@todo: Add ParsedRstDocstring.to_latex()
@var CONSOLIDATED_FIELDS: A dictionary encoding the set of
'consolidated fields' that can be used.  Each consolidated field is
marked by a single tag, and contains a single bulleted list, where
each list item starts with an identifier, marked as interpreted text
(C{`...`}).  This module automatically splits these consolidated
fields into individual fields.  The keys of C{CONSOLIDATED_FIELDS} are
the names of possible consolidated fields; and the values are the
names of the field tags that should be used for individual entries in
the list.
"""
__docformat__ = 'epytext en'

# Imports
import re, os, os.path
from xml.dom.minidom import *

from docutils.core import publish_string
from docutils.writers import Writer
from docutils.writers.html4css1 import HTMLTranslator, Writer as HTMLWriter
from docutils.writers.latex2e import LaTeXTranslator, Writer as LaTeXWriter
from docutils.readers.standalone import Reader as StandaloneReader
from docutils.utils import new_document
from docutils.nodes import NodeVisitor, Text, SkipChildren
from docutils.nodes import SkipNode, TreeCopyVisitor
from docutils.frontend import OptionParser
from docutils.parsers.rst import directives, roles
import docutils.nodes
import docutils.transforms.frontmatter
import docutils.transforms
import docutils.utils

from epydoc.compat import * # Backwards compatibility
from epydoc.markup import *
from epydoc.apidoc import ModuleDoc, ClassDoc
from epydoc.docwriter.dotgraph import *
from epydoc.docwriter.xlink import ApiLinkReader
from epydoc.markup.doctest import doctest_to_html, doctest_to_latex, \
                                  HTMLDoctestColorizer

#: A dictionary whose keys are the "consolidated fields" that are
#: recognized by epydoc; and whose values are the corresponding epydoc
#: field names that should be used for the individual fields.
CONSOLIDATED_FIELDS = {
    'parameters': 'param',
    'arguments': 'arg',
    'exceptions': 'except',
    'variables': 'var',
    'ivariables': 'ivar',
    'cvariables': 'cvar',
    'groups': 'group',
    'types': 'type',
    'keywords': 'keyword',
    }

#: A list of consolidated fields whose bodies may be specified using a
#: definition list, rather than a bulleted list.  For these fields, the
#: 'classifier' for each term in the definition list is translated into
#: a @type field.
CONSOLIDATED_DEFLIST_FIELDS = ['param', 'arg', 'var', 'ivar', 'cvar', 'keyword']

def parse_docstring(docstring, errors, **options):
    """
    Parse the given docstring, which is formatted using
    ReStructuredText; and return a L{ParsedDocstring} representation
    of its contents.
    @param docstring: The docstring to parse
    @type docstring: C{string}
    @param errors: A list where any errors generated during parsing
        will be stored.
    @type errors: C{list} of L{ParseError}
    @param options: Extra options.  Unknown options are ignored.
        Currently, no extra options are defined.
    @rtype: L{ParsedDocstring}
    """
    writer = _DocumentPseudoWriter()
    reader = _EpydocReader(errors) # Outputs errors to the list.
    publish_string(docstring, writer=writer, reader=reader,
                   settings_overrides={'report_level':10000,
                                       'halt_level':10000,
                                       'warning_stream':None})
    return ParsedRstDocstring(writer.document)

class OptimizedReporter(docutils.utils.Reporter):
    """A reporter that ignores all debug messages.  This is used to
    shave a couple seconds off of epydoc's run time, since docutils
    isn't very fast about processing its own debug messages."""
    def debug(self, *args, **kwargs): pass

class ParsedRstDocstring(ParsedDocstring):
    """
    An encoded version of a ReStructuredText docstring.  The contents
    of the docstring are encoded in the L{_document} instance
    variable.

    @ivar _document: A ReStructuredText document, encoding the
        docstring.
    @type _document: C{docutils.nodes.document}
    """
    def __init__(self, document):
        """
        @type document: C{docutils.nodes.document}
        """
        self._document = document
        
        # The default document reporter and transformer are not
        # pickle-able; so replace them with stubs that are.
        document.reporter = OptimizedReporter(
            document.reporter.source, 'SEVERE', 'SEVERE', '')
        document.transformer = docutils.transforms.Transformer(document)

    def split_fields(self, errors=None):
        # Inherit docs
        if errors is None: errors = []
        visitor = _SplitFieldsTranslator(self._document, errors)
        self._document.walk(visitor)
        if len(self._document.children) > 0:
            return self, visitor.fields
        else:
            return None, visitor.fields

    def summary(self):
        # Inherit docs
        visitor = _SummaryExtractor(self._document)
        try: self._document.walk(visitor)
        except docutils.nodes.NodeFound: pass
        return visitor.summary, bool(visitor.other_docs)

#     def concatenate(self, other):
#         result = self._document.copy()
#         for child in (self._document.get_children() +
#                       other._document.get_children()):
#             visitor = TreeCopyVisitor(self._document)
#             child.walkabout(visitor)
#             result.append(visitor.get_tree_copy())
#         return ParsedRstDocstring(result)
        
    def to_html(self, docstring_linker, directory=None,
                docindex=None, context=None, **options):
        # Inherit docs
        visitor = _EpydocHTMLTranslator(self._document, docstring_linker,
                                        directory, docindex, context)
        self._document.walkabout(visitor)
        return ''.join(visitor.body)

    def to_latex(self, docstring_linker, **options):
        # Inherit docs
        visitor = _EpydocLaTeXTranslator(self._document, docstring_linker)
        self._document.walkabout(visitor)
        return ''.join(visitor.body)

    def to_plaintext(self, docstring_linker, **options):
        # This is should be replaced by something better:
        return self._document.astext() 

    def __repr__(self): return '<ParsedRstDocstring: ...>'

    def index_terms(self):
        visitor = _TermsExtractor(self._document)
        self._document.walkabout(visitor)
        return visitor.terms

class _EpydocReader(ApiLinkReader):
    """
    A reader that captures all errors that are generated by parsing,
    and appends them to a list.
    """
    # Remove the DocInfo transform, to ensure that :author: fields are
    # correctly handled.  This needs to be handled differently
    # depending on the version of docutils that's being used, because
    # the default_transforms attribute was deprecated & replaced by
    # get_transforms().
    version = [int(v) for v in docutils.__version__.split('.')]
    version += [ 0 ] * (3 - len(version))
    if version < [0,4,0]:
        default_transforms = list(ApiLinkReader.default_transforms)
        try: default_transforms.remove(docutils.transforms.frontmatter.DocInfo)
        except ValueError: pass
    else:
        def get_transforms(self):
            return [t for t in ApiLinkReader.get_transforms(self)
                    if t != docutils.transforms.frontmatter.DocInfo]
    del version

    def __init__(self, errors):
        self._errors = errors
        ApiLinkReader.__init__(self)
        
    def new_document(self):
        document = new_document(self.source.source_path, self.settings)
        # Capture all warning messages.
        document.reporter.attach_observer(self.report)
        # These are used so we know how to encode warning messages:
        self._encoding = document.reporter.encoding
        self._error_handler = document.reporter.error_handler
        # Return the new document.
        return document

    def report(self, error):
        try: is_fatal = int(error['level']) > 2
        except: is_fatal = 1
        try: linenum = int(error['line'])
        except: linenum = None

        msg = ''.join([c.astext().encode(self._encoding, self._error_handler)
                       for c in error])

        self._errors.append(ParseError(msg, linenum, is_fatal))
        
class _DocumentPseudoWriter(Writer):
    """
    A pseudo-writer for the docutils framework, that can be used to
    access the document itself.  The output of C{_DocumentPseudoWriter}
    is just an empty string; but after it has been used, the most
    recently processed document is available as the instance variable
    C{document}

    @type document: C{docutils.nodes.document}
    @ivar document: The most recently processed document.
    """
    def __init__(self):
        self.document = None
        Writer.__init__(self)
        
    def translate(self):
        self.output = ''
        
class _SummaryExtractor(NodeVisitor):
    """
    A docutils node visitor that extracts the first sentence from
    the first paragraph in a document.
    """
    def __init__(self, document):
        NodeVisitor.__init__(self, document)
        self.summary = None
        self.other_docs = None
        
    def visit_document(self, node):
        self.summary = None
        
    _SUMMARY_RE = re.compile(r'(\s*[\w\W]*?\.)(\s|$)')
    def visit_paragraph(self, node):
        if self.summary is not None:
            # found a paragraph after the first one
            self.other_docs = True
            raise docutils.nodes.NodeFound('Found summary')

        summary_pieces = []

        # Extract the first sentence.
        for child in node:
            if isinstance(child, docutils.nodes.Text):
                m = self._SUMMARY_RE.match(child.data)
                if m:
                    summary_pieces.append(docutils.nodes.Text(m.group(1)))
                    other = child.data[m.end():]
                    if other and not other.isspace():
                        self.other_docs = True
                    break
            summary_pieces.append(child)

        summary_doc = self.document.copy() # shallow copy
        summary_para = node.copy() # shallow copy
        summary_doc[:] = [summary_para]
        summary_para[:] = summary_pieces
        self.summary = ParsedRstDocstring(summary_doc)

    def visit_field(self, node):
        raise SkipNode

    def unknown_visit(self, node):
        'Ignore all unknown nodes'

class _TermsExtractor(NodeVisitor):
    """
    A docutils node visitor that extracts the terms from documentation.

    Terms are created using the C{:term:} interpreted text role.
    """
    def __init__(self, document):
        NodeVisitor.__init__(self, document)
        
        self.terms = None
        """
        The terms currently found.
        @type: C{list}
        """
        
    def visit_document(self, node):
        self.terms = []
        self._in_term = False

    def visit_emphasis(self, node):
        if 'term' in node.get('classes'):
            self._in_term = True

    def depart_emphasis(self, node):
        if 'term' in node.get('classes'):
            self._in_term = False

    def visit_Text(self, node):
        if self._in_term:
            doc = self.document.copy()
            doc[:] = [node.copy()]
            self.terms.append(ParsedRstDocstring(doc))

    def unknown_visit(self, node):
        'Ignore all unknown nodes'

    def unknown_departure(self, node):
        'Ignore all unknown nodes'

class _SplitFieldsTranslator(NodeVisitor):
    """
    A docutils translator that removes all fields from a document, and
    collects them into the instance variable C{fields}

    @ivar fields: The fields of the most recently walked document.
    @type fields: C{list} of L{Field<markup.Field>}
    """
    
    ALLOW_UNMARKED_ARG_IN_CONSOLIDATED_FIELD = True
    """If true, then consolidated fields are not required to mark
    arguments with C{`backticks`}.  (This is currently only
    implemented for consolidated fields expressed as definition lists;
    consolidated fields expressed as unordered lists still require
    backticks for now."""
    
    def __init__(self, document, errors):
        NodeVisitor.__init__(self, document)
        self._errors = errors
        self.fields = []
        self._newfields = {}

    def visit_document(self, node):
        self.fields = []

    def visit_field(self, node):
        # Remove the field from the tree.
        node.parent.remove(node)

        # Extract the field name & optional argument
        tag = node[0].astext().split(None, 1)
        tagname = tag[0]
        if len(tag)>1: arg = tag[1]
        else: arg = None

        # Handle special fields:
        fbody = node[1]
        if arg is None:
            for (list_tag, entry_tag) in CONSOLIDATED_FIELDS.items():
                if tagname.lower() == list_tag:
                    try:
                        self.handle_consolidated_field(fbody, entry_tag)
                        return
                    except ValueError, e:
                        estr = 'Unable to split consolidated field '
                        estr += '"%s" - %s' % (tagname, e)
                        self._errors.append(ParseError(estr, node.line,
                                                       is_fatal=0))
                        
                        # Use a @newfield to let it be displayed as-is.
                        if tagname.lower() not in self._newfields:
                            newfield = Field('newfield', tagname.lower(),
                                             parse(tagname, 'plaintext'))
                            self.fields.append(newfield)
                            self._newfields[tagname.lower()] = 1
                        
        self._add_field(tagname, arg, fbody)

    def _add_field(self, tagname, arg, fbody):
        field_doc = self.document.copy()
        for child in fbody: field_doc.append(child)
        field_pdoc = ParsedRstDocstring(field_doc)
        self.fields.append(Field(tagname, arg, field_pdoc))
            
    def visit_field_list(self, node):
        # Remove the field list from the tree.  The visitor will still walk
        # over the node's children.
        node.parent.remove(node)

    def handle_consolidated_field(self, body, tagname):
        """
        Attempt to handle a consolidated section.
        """
        if len(body) != 1:
            raise ValueError('does not contain a single list.')
        elif body[0].tagname == 'bullet_list':
            self.handle_consolidated_bullet_list(body[0], tagname)
        elif (body[0].tagname == 'definition_list' and
              tagname in CONSOLIDATED_DEFLIST_FIELDS):
            self.handle_consolidated_definition_list(body[0], tagname)
        elif tagname in CONSOLIDATED_DEFLIST_FIELDS:
            raise ValueError('does not contain a bulleted list or '
                             'definition list.')
        else:
            raise ValueError('does not contain a bulleted list.')

    def handle_consolidated_bullet_list(self, items, tagname):
        # Check the contents of the list.  In particular, each list
        # item should have the form:
        #   - `arg`: description...
        n = 0
        _BAD_ITEM = ("list item %d is not well formed.  Each item must "
                     "consist of a single marked identifier (e.g., `x`), "
                     "optionally followed by a colon or dash and a "
                     "description.")
        for item in items:
            n += 1
            if item.tagname != 'list_item' or len(item) == 0: 
                raise ValueError('bad bulleted list (bad child %d).' % n)
            if item[0].tagname != 'paragraph':
                if item[0].tagname == 'definition_list':
                    raise ValueError(('list item %d contains a definition '+
                                      'list (it\'s probably indented '+
                                      'wrong).') % n)
                else:
                    raise ValueError(_BAD_ITEM % n)
            if len(item[0]) == 0: 
                raise ValueError(_BAD_ITEM % n)
            if item[0][0].tagname != 'title_reference':
                raise ValueError(_BAD_ITEM % n)

        # Everything looks good; convert to multiple fields.
        for item in items:
            # Extract the arg
            arg = item[0][0].astext()

            # Extract the field body, and remove the arg
            fbody = item[:]
            fbody[0] = fbody[0].copy()
            fbody[0][:] = item[0][1:]

            # Remove the separating ":", if present
            if (len(fbody[0]) > 0 and
                isinstance(fbody[0][0], docutils.nodes.Text)):
                child = fbody[0][0]
                if child.data[:1] in ':-':
                    child.data = child.data[1:].lstrip()
                elif child.data[:2] in (' -', ' :'):
                    child.data = child.data[2:].lstrip()

            # Wrap the field body, and add a new field
            self._add_field(tagname, arg, fbody)
        
    def handle_consolidated_definition_list(self, items, tagname):
        # Check the list contents.
        n = 0
        _BAD_ITEM = ("item %d is not well formed.  Each item's term must "
                     "consist of a single marked identifier (e.g., `x`), "
                     "optionally followed by a space, colon, space, and "
                     "a type description.")
        for item in items:
            n += 1
            if (item.tagname != 'definition_list_item' or len(item) < 2 or
                item[0].tagname != 'term' or
                item[-1].tagname != 'definition'):
                raise ValueError('bad definition list (bad child %d).' % n)
            if len(item) > 3:
                raise ValueError(_BAD_ITEM % n)
            if not ((item[0][0].tagname == 'title_reference') or
                    (self.ALLOW_UNMARKED_ARG_IN_CONSOLIDATED_FIELD and
                     isinstance(item[0][0], docutils.nodes.Text))):
                raise ValueError(_BAD_ITEM % n)
            for child in item[0][1:]:
                if child.astext() != '':
                    raise ValueError(_BAD_ITEM % n)

        # Extract it.
        for item in items:
            # The basic field.
            arg = item[0][0].astext()
            fbody = item[-1]
            self._add_field(tagname, arg, fbody)
            # If there's a classifier, treat it as a type.
            if len(item) == 3:
                type_descr = item[1]
                self._add_field('type', arg, type_descr)

    def unknown_visit(self, node):
        'Ignore all unknown nodes'

def latex_head_prefix():
    document = new_document('<fake>')
    translator = _EpydocLaTeXTranslator(document, None)
    return translator.head_prefix
    
class _EpydocLaTeXTranslator(LaTeXTranslator):
    settings = None
    def __init__(self, document, docstring_linker):
        # Set the document's settings.
        if self.settings is None:
            settings = OptionParser([LaTeXWriter()]).get_default_values()
            settings.output_encoding = 'utf-8'
            self.__class__.settings = settings
        document.settings = self.settings

        LaTeXTranslator.__init__(self, document)
        self._linker = docstring_linker

        # Start at section level 3.  (Unfortunately, we now have to
        # set a private variable to make this work; perhaps the standard
        # latex translator should grow an official way to spell this?)
        self.section_level = 3
        self._section_number = [0]*self.section_level

    # Handle interpreted text (crossreferences)
    def visit_title_reference(self, node):
        target = self.encode(node.astext())
        xref = self._linker.translate_identifier_xref(target, target)
        self.body.append(xref)
        raise SkipNode()

    def visit_document(self, node): pass
    def depart_document(self, node): pass

    # For now, just ignore dotgraphs. [XXX]
    def visit_dotgraph(self, node):
        log.warning("Ignoring dotgraph in latex output (dotgraph "
                    "rendering for latex not implemented yet).")
        raise SkipNode()
    
    def visit_doctest_block(self, node):
        self.body.append(doctest_to_latex(node[0].astext()))
        raise SkipNode()

class _EpydocHTMLTranslator(HTMLTranslator):
    settings = None
    def __init__(self, document, docstring_linker, directory,
                 docindex, context):
        self._linker = docstring_linker
        self._directory = directory
        self._docindex = docindex
        self._context = context
        
        # Set the document's settings.
        if self.settings is None:
            settings = OptionParser([HTMLWriter()]).get_default_values()
            self.__class__.settings = settings
        document.settings = self.settings

        # Call the parent constructor.
        HTMLTranslator.__init__(self, document)

    # Handle interpreted text (crossreferences)
    def visit_title_reference(self, node):
        target = self.encode(node.astext())
        xref = self._linker.translate_identifier_xref(target, target)
        self.body.append(xref)
        raise SkipNode()

    def should_be_compact_paragraph(self, node):
        if self.document.children == [node]:
            return True
        else:
            return HTMLTranslator.should_be_compact_paragraph(self, node)

    def visit_document(self, node): pass
    def depart_document(self, node): pass
        
    def starttag(self, node, tagname, suffix='\n', **attributes):
        """
        This modified version of starttag makes a few changes to HTML
        tags, to prevent them from conflicting with epydoc.  In particular:
          - existing class attributes are prefixed with C{'rst-'}
          - existing names are prefixed with C{'rst-'}
          - hrefs starting with C{'#'} are prefixed with C{'rst-'}
          - hrefs not starting with C{'#'} are given target='_top'
          - all headings (C{<hM{n}>}) are given the css class C{'heading'}
        """
        # Get the list of all attribute dictionaries we need to munge.
        attr_dicts = [attributes]
        if isinstance(node, docutils.nodes.Node):
            attr_dicts.append(node.attributes)
        if isinstance(node, dict):
            attr_dicts.append(node)
        # Munge each attribute dictionary.  Unfortunately, we need to
        # iterate through attributes one at a time because some
        # versions of docutils don't case-normalize attributes.
        for attr_dict in attr_dicts:
            for (key, val) in attr_dict.items():
                # Prefix all CSS classes with "rst-"; and prefix all
                # names with "rst-" to avoid conflicts.
                if key.lower() in ('class', 'id', 'name'):
                    attr_dict[key] = 'rst-%s' % val
                elif key.lower() in ('classes', 'ids', 'names'):
                    attr_dict[key] = ['rst-%s' % cls for cls in val]
                elif key.lower() == 'href':
                    if attr_dict[key][:1]=='#':
                        attr_dict[key] = '#rst-%s' % attr_dict[key][1:]
                    else:
                        # If it's an external link, open it in a new
                        # page.
                        attr_dict['target'] = '_top'

        # For headings, use class="heading"
        if re.match(r'^h\d+$', tagname):
            attributes['class'] = ' '.join([attributes.get('class',''),
                                            'heading']).strip()
        
        return HTMLTranslator.starttag(self, node, tagname, suffix,
                                       **attributes)

    def visit_dotgraph(self, node):
        if self._directory is None: return # [xx] warning?
        
        # Generate the graph.
        graph = node.graph(self._docindex, self._context, self._linker)
        if graph is None: return
        
        # Write the graph.
        image_url = '%s.gif' % graph.uid
        image_file = os.path.join(self._directory, image_url)
        self.body.append(graph.to_html(image_file, image_url))
        raise SkipNode()

    def visit_doctest_block(self, node):
        pysrc = node[0].astext()
        if node.get('codeblock'):
            self.body.append(HTMLDoctestColorizer().colorize_codeblock(pysrc))
        else:
            self.body.append(doctest_to_html(pysrc))
        raise SkipNode()

    def visit_emphasis(self, node):
        # Generate a corrent index term anchor
        if 'term' in node.get('classes') and node.children:
            doc = self.document.copy()
            doc[:] = [node.children[0].copy()]
            self.body.append(
                self._linker.translate_indexterm(ParsedRstDocstring(doc)))
            raise SkipNode()

        HTMLTranslator.visit_emphasis(self, node)

def python_code_directive(name, arguments, options, content, lineno,
                          content_offset, block_text, state, state_machine):
    """
    A custom restructuredtext directive which can be used to display
    syntax-highlighted Python code blocks.  This directive takes no
    arguments, and the body should contain only Python code.  This
    directive can be used instead of doctest blocks when it is
    inconvenient to list prompts on each line, or when you would
    prefer that the output not contain prompts (e.g., to make
    copy/paste easier).
    """
    required_arguments = 0
    optional_arguments = 0

    text = '\n'.join(content)
    node = docutils.nodes.doctest_block(text, text, codeblock=True)
    return [ node ]
    
python_code_directive.arguments = (0, 0, 0)
python_code_directive.content = True

directives.register_directive('python', python_code_directive)

def term_role(name, rawtext, text, lineno, inliner,
            options={}, content=[]):

    text = docutils.utils.unescape(text)
    node = docutils.nodes.emphasis(rawtext, text, **options)
    node.attributes['classes'].append('term')

    return [node], []

roles.register_local_role('term', term_role)

######################################################################
#{ Graph Generation Directives
######################################################################
# See http://docutils.sourceforge.net/docs/howto/rst-directives.html

class dotgraph(docutils.nodes.image):
    """
    A custom docutils node that should be rendered using Graphviz dot.
    This node does not directly store the graph; instead, it stores a
    pointer to a function that can be used to generate the graph.
    This allows the graph to be built based on information that might
    not be available yet at parse time.  This graph generation
    function has the following signature:

        >>> def generate_graph(docindex, context, linker, *args):
        ...     'generates and returns a new DotGraph'

    Where C{docindex} is a docindex containing the documentation that
    epydoc has built; C{context} is the C{APIDoc} whose docstring
    contains this dotgraph node; C{linker} is a L{DocstringLinker}
    that can be used to resolve crossreferences; and C{args} is any
    extra arguments that are passed to the C{dotgraph} constructor.
    """
    def __init__(self, generate_graph_func, *generate_graph_args):
        docutils.nodes.image.__init__(self)
        self.graph_func = generate_graph_func
        self.args = generate_graph_args
    def graph(self, docindex, context, linker):
        return self.graph_func(docindex, context, linker, *self.args)

def _dir_option(argument):
    """A directive option spec for the orientation of a graph."""
    argument = argument.lower().strip()
    if argument == 'right': return 'LR'
    if argument == 'left': return 'RL'
    if argument == 'down': return 'TB'
    if argument == 'up': return 'BT'
    raise ValueError('%r unknown; choose from left, right, up, down' %
                     argument)
 
def digraph_directive(name, arguments, options, content, lineno,
                      content_offset, block_text, state, state_machine):
    """
    A custom restructuredtext directive which can be used to display
    Graphviz dot graphs.  This directive takes a single argument,
    which is used as the graph's name.  The contents of the directive
    are used as the body of the graph.  Any href attributes whose
    value has the form <name> will be replaced by the URL of the object
    with that name.  Here's a simple example::

     .. digraph:: example_digraph
       a -> b -> c
       c -> a [dir=\"none\"]
    """
    if arguments: title = arguments[0]
    else: title = ''
    return [ dotgraph(_construct_digraph, title, options.get('caption'),
                    '\n'.join(content)) ]
digraph_directive.arguments = (0, 1, True)
digraph_directive.options = {'caption': directives.unchanged}
digraph_directive.content = True
directives.register_directive('digraph', digraph_directive)

def _construct_digraph(docindex, context, linker, title, caption,
                       body):
    """Graph generator for L{digraph_directive}"""
    graph = DotGraph(title, body, caption=caption)
    graph.link(linker)
    return graph

def classtree_directive(name, arguments, options, content, lineno,
                        content_offset, block_text, state, state_machine):
    """
    A custom restructuredtext directive which can be used to
    graphically display a class hierarchy.  If one or more arguments
    are given, then those classes and all their descendants will be
    displayed.  If no arguments are given, and the directive is in a
    class's docstring, then that class and all its descendants will be
    displayed.  It is an error to use this directive with no arguments
    in a non-class docstring.

    Options:
      - C{:dir:} -- Specifies the orientation of the graph.  One of
        C{down}, C{right} (default), C{left}, C{up}.
    """
    return [ dotgraph(_construct_classtree, arguments, options) ]
classtree_directive.arguments = (0, 1, True)
classtree_directive.options = {'dir': _dir_option}
classtree_directive.content = False
directives.register_directive('classtree', classtree_directive)

def _construct_classtree(docindex, context, linker, arguments, options):
    """Graph generator for L{classtree_directive}"""
    if len(arguments) == 1:
        bases = [docindex.find(name, context) for name in
                 arguments[0].replace(',',' ').split()]
        bases = [d for d in bases if isinstance(d, ClassDoc)]
    elif isinstance(context, ClassDoc):
        bases = [context]
    else:
        log.warning("Could not construct class tree: you must "
                    "specify one or more base classes.")
        return None
        
    return class_tree_graph(bases, linker, context, **options)

def packagetree_directive(name, arguments, options, content, lineno,
                        content_offset, block_text, state, state_machine):
    """
    A custom restructuredtext directive which can be used to
    graphically display a package hierarchy.  If one or more arguments
    are given, then those packages and all their submodules will be
    displayed.  If no arguments are given, and the directive is in a
    package's docstring, then that package and all its submodules will
    be displayed.  It is an error to use this directive with no
    arguments in a non-package docstring.

    Options:
      - C{:dir:} -- Specifies the orientation of the graph.  One of
        C{down}, C{right} (default), C{left}, C{up}.
    """
    return [ dotgraph(_construct_packagetree, arguments, options) ]
packagetree_directive.arguments = (0, 1, True)
packagetree_directive.options = {
  'dir': _dir_option,
  'style': lambda a:directives.choice(a.lower(), ('uml', 'tree'))}
packagetree_directive.content = False
directives.register_directive('packagetree', packagetree_directive)

def _construct_packagetree(docindex, context, linker, arguments, options):
    """Graph generator for L{packagetree_directive}"""
    if len(arguments) == 1:
        packages = [docindex.find(name, context) for name in
                    arguments[0].replace(',',' ').split()]
        packages = [d for d in packages if isinstance(d, ModuleDoc)]
    elif isinstance(context, ModuleDoc):
        packages = [context]
    else:
        log.warning("Could not construct package tree: you must "
                    "specify one or more root packages.")
        return None

    return package_tree_graph(packages, linker, context, **options)

def importgraph_directive(name, arguments, options, content, lineno,
                        content_offset, block_text, state, state_machine):
    return [ dotgraph(_construct_importgraph, arguments, options) ]
importgraph_directive.arguments = (0, 1, True)
importgraph_directive.options = {'dir': _dir_option}
importgraph_directive.content = False
directives.register_directive('importgraph', importgraph_directive)

def _construct_importgraph(docindex, context, linker, arguments, options):
    """Graph generator for L{importgraph_directive}"""
    if len(arguments) == 1:
        modules = [ docindex.find(name, context)
                    for name in arguments[0].replace(',',' ').split() ]
        modules = [d for d in modules if isinstance(d, ModuleDoc)]
    else:
        modules = [d for d in docindex.root if isinstance(d, ModuleDoc)]

    return import_graph(modules, docindex, linker, context, **options)

def callgraph_directive(name, arguments, options, content, lineno,
                        content_offset, block_text, state, state_machine):
    return [ dotgraph(_construct_callgraph, arguments, options) ]
callgraph_directive.arguments = (0, 1, True)
callgraph_directive.options = {'dir': _dir_option,
                                 'add_callers': directives.flag,
                                 'add_callees': directives.flag}
callgraph_directive.content = False
directives.register_directive('callgraph', callgraph_directive)

def _construct_callgraph(docindex, context, linker, arguments, options):
    """Graph generator for L{callgraph_directive}"""
    if len(arguments) == 1:
        docs = [docindex.find(name, context) for name in
                 arguments[0].replace(',',' ').split()]
        docs = [doc for doc in docs if doc is not None]
    else:
        docs = [context]
    return call_graph(docs, docindex, linker, context, **options)
  

########NEW FILE########
__FILENAME__ = util
#
# epydoc -- Utility functions used by regression tests (*.doctest)
# Edward Loper
#
# Created [01/30/01 05:18 PM]
# $Id: html.py 1420 2007-01-28 14:19:30Z dvarrazzo $
#

"""
Utility functions used by the regression tests (C{*.doctest}).
"""
__docformat__ = 'epytext en'

import tempfile, re, os, os.path, textwrap, sys
from epydoc.docbuilder import build_doc, build_doc_index
from epydoc.docparser import parse_docs
from epydoc.docintrospecter import introspect_docs
from epydoc.apidoc import ClassDoc, RoutineDoc
from epydoc.markup import ParsedDocstring
from epydoc.docwriter.html import HTMLWriter

######################################################################
#{ Test Functions
######################################################################

def buildvaluedoc(s):
    """
    This test function takes a string containing the contents of a
    module.  It writes the string contents to a file, imports the file
    as a module, and uses build_doc to build documentation, and
    returns it as a C{ValueDoc} object.
    """
    tmp_dir = write_pystring_to_tmp_dir(s)
    val_doc = build_doc(os.path.join(tmp_dir, 'epydoc_test.py'))
    cleanup_tmp_dir(tmp_dir)
    return val_doc

def runbuilder(s, attribs='', build=None, exclude=''):
    """
    This test function takes a string containing the contents of a
    module.  It writes the string contents to a file, imports the file
    as a module, and uses build_doc to build documentation, and pretty
    prints the resulting ModuleDoc object.  The C{attribs} argument
    specifies which attributes of the C{APIDoc}s should be displayed.
    The C{build} argument gives the name of a variable in the module
    whose documentation should be built, instead of bilding docs for
    the whole module.
    """
    # Write it to a temp file.
    tmp_dir = write_pystring_to_tmp_dir(s)
    # Build it.
    val_doc = build_doc(os.path.join(tmp_dir, 'epydoc_test.py'))
    if build: val_doc = val_doc.variables[build].value
    # Display it.
    if isinstance(val_doc, ClassDoc):
        for val in val_doc.variables.values():
            if isinstance(val.value, RoutineDoc):
                fun_to_plain(val.value)
    s = val_doc.pp(include=attribs.split(),exclude=exclude.split())
    s = re.sub(r"(filename = ).*", r"\1...", s)
    s = re.sub(r"(<module 'epydoc_test' from ).*", r'\1...', s)
    s = re.sub(r"(<function \w+ at )0x\w+>", r"\1...>", s)
    s = re.sub(r"(<\w+ object at )0x\w+>", r"\1...>", s)
    print s
    # Clean up.
    cleanup_tmp_dir(tmp_dir)

def runparser(s, attribs='', show=None, exclude=''):
    """
    This test function takes a string containing the contents of a
    module, and writes it to a file, uses `parse_docs` to parse it,
    and pretty prints the resulting ModuleDoc object.  The `attribs`
    argument specifies which attributes of the `APIDoc`s should be
    displayed.  The `show` argument, if specifies, gives the name of
    the object in the module that should be displayed (but the whole
    module will always be inspected; this just selects what to
    display).
    """
    # Write it to a temp file.
    tmp_dir = write_pystring_to_tmp_dir(s)
    # Parse it.
    val_doc = parse_docs(os.path.join(tmp_dir, 'epydoc_test.py'))
    if show is not None:
        for name in show.split('.'):
            if isinstance(val_doc, ClassDoc):
                val_doc = val_doc.local_variables[name].value
            else:
                val_doc = val_doc.variables[name].value
    # Display it.
    s = val_doc.pp(include=attribs.split(), exclude=exclude.split())
    s = re.sub(r"filename = .*", "filename = ...", s)
    print s
    # Clean up.
    cleanup_tmp_dir(tmp_dir)

def runintrospecter(s, attribs='', introspect=None, exclude=''):
    """
    This test function takes a string containing the contents of a
    module.  It writes the string contents to a file, imports the file
    as a module, and uses C{introspect_docs} to introspect it, and
    pretty prints the resulting ModuleDoc object.  The C{attribs}
    argument specifies which attributes of the C{APIDoc}s should be
    displayed.  The C{introspect} argument gives the name of a variable
    in the module whose value should be introspected, instead of
    introspecting the whole module.
    """
    # Write it to a temp file.
    tmp_dir = write_pystring_to_tmp_dir(s)
    # Import it.
    sys.path.insert(0, tmp_dir)
    if introspect is None:
        import epydoc_test as val
    else:
        exec("from epydoc_test import %s as val" % introspect)
    del sys.path[0]
    # Introspect it.
    val_doc = introspect_docs(val)
    # Display it.
    s = val_doc.pp(include=attribs.split(),exclude=exclude.split())
    s = re.sub(r"(filename = ).*", r"\1...", s)
    s = re.sub(r"(<module 'epydoc_test' from ).*", r'\1...', s)
    s = re.sub(r"(<function \w+ at )0x\w+>", r"\1...>", s)
    s = re.sub(r"(<\w+ object at )0x\w+>", r"\1...>", s)
    print s
    # Clean up.
    cleanup_tmp_dir(tmp_dir)

def print_warnings():
    """
    Register a logger that will print warnings & errors.
    """
    from epydoc import log
    del log._loggers[:]
    log.register_logger(log.SimpleLogger(log.DOCSTRING_WARNING))

def testencoding(s, introspect=True, parse=True, debug=False):
    """
    An end-to-end test for unicode encodings.  This function takes a
    given string, writes it to a python file, and processes that
    file's documentation.  It then generates HTML output from the
    documentation, extracts all docstrings from the generated HTML
    output, and displays them.  (In order to extract & display all
    docstrings, it monkey-patches the HMTLwriter.docstring_to_html()
    method.)"""
    # Monkey-patch docstring_to_html
    original_docstring_to_html = HTMLWriter.docstring_to_html
    HTMLWriter.docstring_to_html = print_docstring_as_html
    
    # Write s to a temporary file.
    tmp_dir = tempfile.mkdtemp()
    path = os.path.join(tmp_dir, 'enc_test.py')
    out = open(path, 'w')
    out.write(textwrap.dedent(s))
    out.close()
    # Build docs for it
    docindex = build_doc_index([path], introspect, parse)
    if docindex is None: return
    sys.modules.pop('enc_test', None)
    # Write html output.
    writer = HTMLWriter(docindex, mark_docstrings=True)
    writer.write(tmp_dir)
    for file in os.listdir(tmp_dir):
        os.unlink(os.path.join(tmp_dir,file))
    os.rmdir(tmp_dir)

    # Restore the HTMLWriter class to its original state.
    HTMLWriter.docstring_to_html = original_docstring_to_html

######################################################################
#{ Helper Functions
######################################################################

def write_pystring_to_tmp_dir(s):
    tmp_dir = tempfile.mkdtemp()
    out = open(os.path.join(tmp_dir, 'epydoc_test.py'), 'w')
    out.write(textwrap.dedent(s))
    out.close()
    return tmp_dir

def cleanup_tmp_dir(tmp_dir):
    os.unlink(os.path.join(tmp_dir, 'epydoc_test.py'))
    try: os.unlink(os.path.join(tmp_dir, 'epydoc_test.pyc'))
    except OSError: pass
    os.rmdir(tmp_dir)
    sys.modules.pop('epydoc_test', None)

def to_plain(docstring):
    """Conver a parsed docstring into plain text"""
    if isinstance(docstring, ParsedDocstring):
        docstring = docstring.to_plaintext(None)
    return docstring.rstrip()

def fun_to_plain(val_doc):
    """Convert parsed docstrings in text from a RoutineDoc"""
    for k, v in val_doc.arg_types.items():
        val_doc.arg_types[k] = to_plain(v)
    for i, (k, v) in enumerate(val_doc.arg_descrs):
        val_doc.arg_descrs[i] = (k, to_plain(v))

def print_docstring_as_html(self, parsed_docstring, *varargs, **kwargs):
    """
    Convert the given parsed_docstring to HTML and print it.  Ignore
    any other arguments.  This function is used by L{testencoding} to
    monkey-patch the HTMLWriter class's docstring_to_html() method.
    """
    s = parsed_docstring.to_html(None).strip()
    s = s.encode('ascii', 'xmlcharrefreplace')
    s = remove_surrogates(s)
    print s
    return ''

def remove_surrogates(s):
    """
    The following is a helper function, used to convert two-character
    surrogate sequences into single characters.  This is needed
    because some systems create surrogates but others don't.
    """
    pieces = re.split('(&#\d+;)', s)
    for i in range(3, len(pieces)-1, 2):
        if pieces[i-1] != '': continue
        high,low = int(pieces[i-2][2:-1]), int(pieces[i][2:-1])
        if 0xd800 <= high <= 0xdbff and 0xdc00 <= low <= 0xdfff:
            pieces[i-2] = '&#%d;' % (((high&0x3ff)<<10) +
                                     (low&0x3ff) + 0x10000)
            pieces[i] = ''
    return ''.join(pieces)

########NEW FILE########
__FILENAME__ = util
# epydoc -- Utility functions
#
# Copyright (C) 2005 Edward Loper
# Author: Edward Loper <edloper@loper.org>
# URL: <http://epydoc.sf.net>
#
# $Id: util.py 1671 2008-01-29 02:55:49Z edloper $

"""
Miscellaneous utility functions that are used by multiple modules.

@group Python source types: is_module_file, is_package_dir, is_pyname,
    py_src_filename
@group Text processing: wordwrap, decode_with_backslashreplace,
    plaintext_to_html
"""
__docformat__ = 'epytext en'

import os, os.path, re

######################################################################
## Python Source Types
######################################################################

PY_SRC_EXTENSIONS = ['.py', '.pyw']
PY_BIN_EXTENSIONS = ['.pyc', '.so', '.pyd']

def is_module_file(path):
    # Make sure it's a file name.
    if not isinstance(path, basestring):
        return False
    (dir, filename) = os.path.split(path)
    (basename, extension) = os.path.splitext(filename)
    return (os.path.isfile(path) and
            re.match('[a-zA-Z_]\w*$', basename) and
            extension in PY_SRC_EXTENSIONS+PY_BIN_EXTENSIONS)

def is_src_filename(filename):
    if not isinstance(filename, basestring): return False
    if not os.path.exists(filename): return False
    return os.path.splitext(filename)[1] in PY_SRC_EXTENSIONS
    
def is_package_dir(dirname):
    """
    Return true if the given directory is a valid package directory
    (i.e., it names a directory that contains a valid __init__ file,
    and its name is a valid identifier).
    """
    # Make sure it's a directory name.
    if not isinstance(dirname, basestring):
        return False
    if not os.path.isdir(dirname):
        return False
    dirname = os.path.abspath(dirname)
    # Make sure it's a valid identifier.  (Special case for
    # "foo/", where os.path.split -> ("foo", "").)
    (parent, dir) = os.path.split(dirname)
    if dir == '': (parent, dir) = os.path.split(parent)
    
    # The following constraint was removed because of sourceforge
    # bug #1787028 -- in some cases (eg eggs), it's too strict.
    #if not re.match('\w+$', dir):
    #    return False
    
    for name in os.listdir(dirname):
        filename = os.path.join(dirname, name)
        if name.startswith('__init__.') and is_module_file(filename):
            return True
    else:
        return False

def is_pyname(name):
    return re.match(r"\w+(\.\w+)*$", name)

def py_src_filename(filename):
    basefile, extension = os.path.splitext(filename)
    if extension in PY_SRC_EXTENSIONS:
        return filename
    else:
        for ext in PY_SRC_EXTENSIONS:
            if os.path.isfile('%s%s' % (basefile, ext)):
                return '%s%s' % (basefile, ext)
        else:
            raise ValueError('Could not find a corresponding '
                             'Python source file for %r.' % filename)

def munge_script_name(filename):
    name = os.path.split(filename)[1]
    name = re.sub(r'\W', '_', name)
    return 'script-'+name

######################################################################
## Text Processing
######################################################################

def decode_with_backslashreplace(s):
    r"""
    Convert the given 8-bit string into unicode, treating any
    character c such that ord(c)<128 as an ascii character, and
    converting any c such that ord(c)>128 into a backslashed escape
    sequence.

        >>> decode_with_backslashreplace('abc\xff\xe8')
        u'abc\\xff\\xe8'
    """
    # s.encode('string-escape') is not appropriate here, since it
    # also adds backslashes to some ascii chars (eg \ and ').
    assert isinstance(s, str)
    return (s
            .decode('latin1')
            .encode('ascii', 'backslashreplace')
            .decode('ascii'))

def wordwrap(str, indent=0, right=75, startindex=0, splitchars=''):
    """
    Word-wrap the given string.  I.e., add newlines to the string such
    that any lines that are longer than C{right} are broken into
    shorter lines (at the first whitespace sequence that occurs before
    index C{right}).  If the given string contains newlines, they will
    I{not} be removed.  Any lines that begin with whitespace will not
    be wordwrapped.

    @param indent: If specified, then indent each line by this number
        of spaces.
    @type indent: C{int}
    @param right: The right margin for word wrapping.  Lines that are
        longer than C{right} will be broken at the first whitespace
        sequence before the right margin.
    @type right: C{int}
    @param startindex: If specified, then assume that the first line
        is already preceeded by C{startindex} characters.
    @type startindex: C{int}
    @param splitchars: A list of non-whitespace characters which can
        be used to split a line.  (E.g., use '/\\' to allow path names
        to be split over multiple lines.)
    @rtype: C{str}
    """
    if splitchars:
        chunks = re.split(r'( +|\n|[^ \n%s]*[%s])' %
                          (re.escape(splitchars), re.escape(splitchars)),
                          str.expandtabs())
    else:
        chunks = re.split(r'( +|\n)', str.expandtabs())
    result = [' '*(indent-startindex)]
    charindex = max(indent, startindex)
    for chunknum, chunk in enumerate(chunks):
        if (charindex+len(chunk) > right and charindex > 0) or chunk == '\n':
            result.append('\n' + ' '*indent)
            charindex = indent
            if chunk[:1] not in ('\n', ' '):
                result.append(chunk)
                charindex += len(chunk)
        else:
            result.append(chunk)
            charindex += len(chunk)
    return ''.join(result).rstrip()+'\n'

def plaintext_to_html(s):
    """
    @return: An HTML string that encodes the given plaintext string.
    In particular, special characters (such as C{'<'} and C{'&'})
    are escaped.
    @rtype: C{string}
    """
    s = s.replace('&', '&amp;').replace('"', '&quot;')
    s = s.replace('<', '&lt;').replace('>', '&gt;')
    return s
        
def plaintext_to_latex(str, nbsp=0, breakany=0):
    """
    @return: A LaTeX string that encodes the given plaintext string.
    In particular, special characters (such as C{'$'} and C{'_'})
    are escaped, and tabs are expanded.
    @rtype: C{string}
    @param breakany: Insert hyphenation marks, so that LaTeX can
    break the resulting string at any point.  This is useful for
    small boxes (e.g., the type box in the variable list table).
    @param nbsp: Replace every space with a non-breaking space
    (C{'~'}).
    """
    # These get converted to hyphenation points later
    if breakany: str = re.sub('(.)', '\\1\1', str)

    # These get converted to \textbackslash later.
    str = str.replace('\\', '\0')

    # Expand tabs
    str = str.expandtabs()

    # These elements need to be backslashed.
    str = re.sub(r'([#$&%_\${}])', r'\\\1', str)

    # These elements have special names.
    str = str.replace('|', '{\\textbar}')
    str = str.replace('<', '{\\textless}')
    str = str.replace('>', '{\\textgreater}')
    str = str.replace('^', '{\\textasciicircum}')
    str = str.replace('~', '{\\textasciitilde}')
    str = str.replace('\0', r'{\textbackslash}')

    # replace spaces with non-breaking spaces
    if nbsp: str = str.replace(' ', '~')

    # Convert \1's to hyphenation points.
    if breakany: str = str.replace('\1', r'\-')
    
    return str

class RunSubprocessError(OSError):
    def __init__(self, cmd, out, err):
        OSError.__init__(self, '%s failed' % cmd[0])
        self.out = out
        self.err = err

def run_subprocess(cmd, data=None):
    """
    Execute the command C{cmd} in a subprocess.
    
    @param cmd: The command to execute, specified as a list
        of string.
    @param data: A string containing data to send to the
        subprocess.
    @return: A tuple C{(out, err)}.
    @raise OSError: If there is any problem executing the
        command, or if its exitval is not 0.
    """
    if isinstance(cmd, basestring):
        cmd = cmd.split()

    # Under Python 2.4+, use subprocess
    try:
        from subprocess import Popen, PIPE
        pipe = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
        out, err = pipe.communicate(data)
        if hasattr(pipe, 'returncode'):
            if pipe.returncode == 0:
                return out, err
            else:
                raise RunSubprocessError(cmd, out, err)
        else:
            # Assume that there was an error iff anything was written
            # to the child's stderr.
            if err == '':
                return out, err
            else:
                raise RunSubprocessError(cmd, out, err)
    except ImportError:
        pass

    # Under Python 2.3 or earlier, on unix, use popen2.Popen3 so we
    # can access the return value.
    import popen2
    if hasattr(popen2, 'Popen3'):
        pipe = popen2.Popen3(' '.join(cmd), True)
        to_child = pipe.tochild
        from_child = pipe.fromchild
        child_err = pipe.childerr
        if data:
            to_child.write(data)
        to_child.close()
        out = err = ''
        while pipe.poll() is None:
            out += from_child.read()
            err += child_err.read()
        out += from_child.read()
        err += child_err.read()
        if pipe.wait() == 0:
            return out, err
        else:
            raise RunSubprocessError(cmd, out, err)

    # Under Python 2.3 or earlier, on non-unix, use os.popen3
    else:
        to_child, from_child, child_err = os.popen3(' '.join(cmd), 'b')
        if data:
            try:
                to_child.write(data)
            # Guard for a broken pipe error
            except IOError, e:
                raise OSError(e)
        to_child.close()
        out = from_child.read()
        err = child_err.read()
        # Assume that there was an error iff anything was written
        # to the child's stderr.
        if err == '':
            return out, err
        else:
            raise RunSubprocessError(cmd, out, err)

########NEW FILE########
__FILENAME__ = apirst2html
#!/usr/bin/env python
# -*- coding: iso-8859-1 -*-

"""An HTML writer supporting link to external documentation.

This module is a frontend for the Docutils_ HTML writer. It allows a document
to reference objects documented in the API documentation generated by
extraction tools such as Doxygen_ or Epydoc_.

.. _Docutils:   http://docutils.sourceforge.net/
.. _Doxygen:    http://www.doxygen.org/
.. _Epydoc:     http://epydoc.sourceforge.net/
"""

# $Id: apirst2html.py 1531 2007-02-18 23:07:25Z dvarrazzo $
__version__ = "$Revision: 1531 $"[11:-2]
__author__ = "Daniele Varrazzo"
__copyright__ = "Copyright (C) 2007 by Daniele Varrazzo"
__docformat__ = 'reStructuredText en'

try:
    import locale
    locale.setlocale(locale.LC_ALL, '')
except:
    pass

# We have to do some path magic to prevent Python from getting
# confused about the difference between the ``epydoc.py`` script, and the
# real ``epydoc`` package.  So remove ``sys.path[0]``, which contains the
# directory of the script.
import sys, os.path
script_path = os.path.abspath(sys.path[0])
sys.path = [p for p in sys.path if os.path.abspath(p) != script_path]

import epydoc.docwriter.xlink as xlink

from docutils.core import publish_cmdline, default_description
description = ('Generates (X)HTML documents with API documentation links.  '
                + default_description)
publish_cmdline(reader=xlink.ApiLinkReader(), writer_name='html',
                description=description)

########NEW FILE########
__FILENAME__ = epydoc
#!/usr/bin/env python
#
# Call the command line interface for Epydoc.
#

# We have to do some path magic to prevent Python from getting
# confused about the difference between this epydoc module, and the
# real epydoc package.  So remove sys.path[0], which contains the
# directory of the script.
import sys, os.path
script_path = os.path.abspath(sys.path[0])
sys.path = [p for p in sys.path if
            os.path.abspath(p) != script_path]

from epydoc.cli import cli
cli()


########NEW FILE########
__FILENAME__ = AlphaDataModel
'''
Created on Jun 1, 2010

@author: Shreyas Joshi
@summary: The purpose of this module is to make it easy to create hdf5 files with "alpha" values in them
'''
import tables as pt
fileName="defaultAlphaFileName.h5"
h5f=[]
group=[]
table=[]
opened=False    
ctr=float (0.0)    

class AlphaDataModelClass(pt.IsDescription):  
    symbol = pt.StringCol(30)
    exchange = pt.StringCol(10) 
    alphaValue=pt.Float32Col() 
    timestamp= pt.Time64Col() 
    
    
    
    def __init__(self):
        print "In the AlphaDataModelClass constructor"
        
    #constructor done
#class ends!

    
def openFile (newFileName):
        '''
        @param newFileName: Full path to the file and the name of the file.
        @summary: This function creates a new file. If the length of the name passed =0 then a file called "defaultAlphaFileName.h5" will be created.
        @warning: If a file of the same name already exists then that file will be overwritten.
        '''
        global fileName, h5f, group, table, opened, ctr
        ctr=float (0.0)
        
        if newFileName is None:
            print "Using default name for alpha file"
        else:
           if (len(newFileName)>0):
               fileName= str(newFileName)
           else:
             print "Using default name for alpha file"
            
        #Opening the file now...
        if not opened:
         h5f = pt.openFile(str(fileName), mode = "w")
         group = h5f.createGroup("/", 'alphaData')
         table = h5f.createTable(group, 'alphaData', AlphaDataModelClass)   
         opened=True 
        else:
         print "File already opened. Doing nothing"      
    
    # File opened    
    
def addRow (currSymbol, currExchange, currAlphaVal, currTS):
    '''
    @param currSymbol: The symbol of the stock
    @param currExchange: The exchange the stock trades on
    @param currAlphaVal: The alpha value of the stock at the current timestamp
    @param currTS: The current time stamp
    @summary: Adds a row of data to the file- and writes it out do disk...eventually
    @warning: File must be opened before calling this function   
    '''
    global ctr
    
    if opened:
        ctr= ctr + 1
        row = table.row
        row['symbol']= currSymbol
        row['exchange']=currExchange
        row['alphaValue']= currAlphaVal
        row['timestamp']= currTS
        row.append()
        #print "Appending row " + str (currTS)
        if (ctr==10000): #Might cause mem error
          ctr=0
          table.flush() #write to disk
    
    
    
    else:
       print "ERROR: File not open. Can not add row."  
       raise IOError   
#    addRow done


#def readAllData():
##  global h5f
##  table2 = h5f.root.alphaData.alphaData
#  
#  for row in table.iterrows():  #for row in table2.iterrows():
#      print "SYM: "+str(row['symbol'])+", EX: "+ str(row['exchange'])+", ALPHA: "+str(row['alphaValue'])+", TIMESTAMP: "+str(row['timestamp'])
    
    
def closeFile():
        '''
        @summary: closes the file.
        '''
        
        table.flush()
        h5f.close()
        print str(fileName)+ " closed."
        opened= False
########NEW FILE########
__FILENAME__ = alphaGenerator
'''
Created on Jun 1, 2010

@author: Shreyas Joshi
@contact: shreyasj@gatech.edu
@summary: This module is used to generate random alpha values that will then be looked at by the simulator when running. The alpha values have to
          be generated before the simulator starts. 
'''
import tables as pt
import time
import random
#from AlphaDataModel import *

import AlphaDataModel as adm
#Main begins

#alpha val writing begins

adm.openFile("randomAlpha.h5")

#of ("myAlphaFile.h5")

startDate=19840101
endDate=20100101

tsStart= time.mktime(time.strptime(str(startDate),'%Y%m%d'))

tsEnd= time.mktime(time.strptime(str(endDate),'%Y%m%d'))

while (tsStart <= tsEnd):
    adm.addRow("AAPL", "EXCHG", random.random(), tsStart)
    
    tsStart+=86400
    #While ends    

tsStart= time.mktime(time.strptime(str(startDate),'%Y%m%d'))
while (tsStart <= tsEnd):
    adm.addRow("GOOG", "EXCHG", random.random(), tsStart)
    
    tsStart+=86400
    #While ends
tsStart= time.mktime(time.strptime(str(startDate),'%Y%m%d'))
while (tsStart <= tsEnd):
    adm.addRow("MSFT", "EXCHG", random.random(), tsStart)
    
    tsStart+=86400
    #While ends

    
    
tsStart= time.mktime(time.strptime(str(startDate),'%Y%m%d'))
while (tsStart <= tsEnd):
    adm.addRow("YHOO", "EXCHG", random.random(), tsStart)
    
    tsStart+=86400
    #While ends
    
print "Finished adding all data"

#print "Reading it in now..."
#adm.readAllData()
adm.closeFile()
print "All done"
#Main ends
        
########NEW FILE########
__FILENAME__ = Bollingerbands
'''
Created on Jul 30, 2010

@author: sjoshi42
@summary: This module generates alpha values based on bollinger bands
'''

import DataAccess
import dircache
import numpy
import alphaDataModel.AlphaDataModel as adm
#import alphaGenerator.AlphaDataModel as adm
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from matplotlib.figure import Figure

def getStocks(listOfPaths):
        
            listOfStocks=list()
            #Path does not exist
            print "Reading in all stock names..."
            fileExtensionToRemove=".h5"   
            
            for path in listOfPaths:
               stocksAtThisPath=list ()
               
               stocksAtThisPath= dircache.listdir(str(path))
               #Next, throw away everything that is not a .h5 And these are our stocks!
               stocksAtThisPath = filter (lambda x:(str(x).find(str(fileExtensionToRemove)) > -1), stocksAtThisPath)
               #Now, we remove the .h5 to get the name of the stock
               stocksAtThisPath = map(lambda x:(x.partition(str(fileExtensionToRemove))[0]),stocksAtThisPath)
               
               for stock in stocksAtThisPath:
                   listOfStocks.append(stock)
               return listOfStocks
    #readStocksFromFile done



def removeNaNs(numArray):
    ctr=1
    #fill forward
    while (ctr< numArray.size):
        if (numpy.isnan(numArray[ctr])):
            if not (numpy.isnan(numArray[ctr-1])):
                numArray[ctr]= numArray[ctr-1]
                #if not ends
            #if ends
        ctr+=1
        #while ends        
    
    #fill back
    ctr= numArray.size-2
    while (ctr>=0):
        if (numpy.isnan(numArray[ctr])):
            if not (numpy.isnan(numArray[ctr+1])):
                numArray[ctr]= numArray[ctr+1]
                #if not ends
            #if ends
        ctr-=1
        #while ends        
    return numArray
#removeNaNs ends


def main():
    '''
    @summary: Calculates Bollinger bands
    '''
    
    folderList= list()
    folderList.append("C:\\tempoutput\\")
    listOfStocks= list()
#    listOfStocks.append("ACY")
    listOfStocks.append("AAPL")
    #listOfStocks= getStocks(folderList)
    
    
    
    dataAccess= DataAccess.DataAccess (True, folderList, "/StrategyData", "StrategyData", True, listOfStocks)
    timestamps= list(dataAccess.getTimestampArray())
    adm.openFile("AAPLonlybollingerBandsAlphaVals.h5")
    
    period= 10
    stdMultiplier=2
    noOfDays= len (timestamps) #400
    
    
    centerband= numpy.zeros(noOfDays, dtype= float) #len(timestamps)- period + 1 #Just to make it the same length as the adj_close to make it easier to plot
    upperBand= numpy.zeros(noOfDays, dtype= float)
    lowerBand= numpy.zeros(noOfDays, dtype= float)
    x= numpy.zeros(noOfDays, dtype= float)
    
    ctr=0
    while (ctr< noOfDays):
        x[ctr]=ctr
        ctr+=1
        #end while
        
    
    for stock in listOfStocks:
        print "Processing: " + str(stock)
        #adj_close= dataAccess.getStockDataList(str(stock), 'adj_close')
        adj_close= dataAccess.getStockDataList(stock, 'adj_close', timestamps[0], timestamps[noOfDays-1])
        
        adj_close= removeNaNs(adj_close)#nan's removed, unless all are nans
        
        #Now calculating bollinger bands
        for ctr in range (period, noOfDays):
            
            try:
               centerband[ctr]= numpy.average(adj_close[ctr- period:ctr])
               stdDev= numpy.std(adj_close[ctr- period:ctr])
               upperBand[ctr]= centerband[ctr] + (stdMultiplier* stdDev)
               lowerBand[ctr]= centerband[ctr] - (stdMultiplier* stdDev)
            except IndexError:
                print "ctr is: " + str(ctr)
        
        #writing alpha values to file
        for ctr in range (0, noOfDays):
            if (upperBand[ctr]== lowerBand[ctr])or (adj_close[ctr]== centerband[ctr]):
                adm.addRow(str(stock), "blah", 0.0, timestamps[ctr])
            elif (adj_close[ctr] < centerband[ctr]):
                alphaValue=  lowerBand[ctr]/ adj_close[ctr]
                adm.addRow (str(stock), "blah", alphaValue, timestamps[ctr])
            else:
                alphaValue= - adj_close[ctr]/ upperBand[ctr]
                adm.addRow (str(stock), "blah", alphaValue, timestamps[ctr])
            #done writing alpha values of this stock to file
            
                   
                       
            
            #calculating bollinger bands done!
            
#        fig = Figure()
#        canvas = FigureCanvas(fig)
#        ax = fig.add_subplot(111)
#        ax.plot(centerband)
#        ax.plot (lowerBand)
#        ax.plot (upperBand)
#        ax.plot (adj_close)
#
#        ax.set_title(str(stock)+' Bollinger bands')
#        ax.grid(True)
#        ax.set_xlabel('time')
#        ax.set_ylabel('')
#        canvas.print_figure(str(listOfStocks.index(stock)))
        #for stock in listOfStocks: done
        
    adm.closeFile()
        

#Main done        
            
        
if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = CurveFittingAlphaGenerator
'''
Created on Jul 26, 2010

@author: Shreyas Joshi
@contact: shreyasj@gatech.edu
'''

import tables
import DataAccess
import dircache
import numpy
import alphaGenerator.AlphaDataModel as adm

def getStocks(listOfPaths):
        
            listOfStocks=list()
            #Path does not exist
            print "Reading in all stock names..."
            fileExtensionToRemove=".h5"   
            
            for path in listOfPaths:
               stocksAtThisPath=list ()
               
               stocksAtThisPath= dircache.listdir(str(path))
               #Next, throw away everything that is not a .h5 And these are our stocks!
               stocksAtThisPath = filter (lambda x:(str(x).find(str(fileExtensionToRemove)) > -1), stocksAtThisPath)
               #Now, we remove the .h5 to get the name of the stock
               stocksAtThisPath = map(lambda x:(x.partition(str(fileExtensionToRemove))[0]),stocksAtThisPath)
               
               for stock in stocksAtThisPath:
                   listOfStocks.append(stock)
               return listOfStocks
    #readStocksFromFile done



#Main begins
noOfDaysToUse=5
daysAhead=5
folderList=list()
folderList.append("C:\\tempoutput\\")
listOfStocks= getStocks(folderList)

beginTS= 473490000 #2 Jan 1985 0500 hrs GMT
endTS=  1262235600 #31 DEc 2009 0500 hrs GMT

print "list of stocks is: " + str(listOfStocks)
dataAccess= DataAccess.DataAccess(True, folderList, "/StrategyData", "StrategyData", True, listOfStocks, beginTS, endTS)
timestamps= list(dataAccess.getTimestampArray())

print "Printing all timestamps: "
for ts in timestamps:
    print ts
print "Printing ts done"


#alpha= alphaGenerator.AlphaDataModel.AlphaDataModelClass()
adm.openFile("curveFittingAlphaVals_Jan_85_to_2010.h5")
daysArray= numpy.zeros ((noOfDaysToUse+1), dtype=float)

ctr=0
while (ctr<= noOfDaysToUse): #because we get back noOfDaysToUse+1 rows
    daysArray[ctr]= ctr
    ctr+=1
    #while ends
    
try:
  beginIndex= timestamps.index(beginTS)
  endIndex= timestamps.index(endTS) 
except ValueError:
    print "beginTS or endTS not found!"
    raise ValueError


#beginTS+= (noOfDaysToUse*day)
beginIndex+=noOfDaysToUse


while (beginIndex<=endIndex):
    #closeData= dataAccess.getMatrixFromTS(listOfStocks, 'adj_close', beginTS, -noOfDaysToUse)
    closeData= dataAccess.getMatrixBetweenIndex(listOfStocks, 'adj_close', beginIndex- noOfDaysToUse, beginIndex)
    print "At ts: " + str (timestamps[beginIndex])
    
    if (closeData is not None):
        print "closeData is not none"
        stockCtr=0
        while (stockCtr < len (listOfStocks)):
            nanPresent=False
            closeData= numpy.ma.masked_values(closeData, numpy.NaN)
#            ctr=0
#            while (ctr<= noOfDaysToUse):
#              try:  
#                if (numpy.isnan(closeData[ctr][stockCtr])):
#                    nanPresent=True
#                    zeroDueToNaNCtr+=1
#                    adm.addRow(listOfStocks[stockCtr], "blah", 0.0, beginTS)
#                    break
#                ctr+=1
#              
#              
#              except IndexError:
#                    print "stockCtr: " + str(stockCtr)+", ctr: "+str(ctr)
#                    print "Shape is: "+str(closeData.shape)
                
                
                
                #while (ctr<= noOfDaysToUse) ends
            if (nanPresent is False):
                #calculate the best fit 3 degree polynomial
                #print "daysArray: "+str(daysArray.shape) +" closeData: " + str(blah.shape) + "the actual arr: " + str (closeData[:][stockCtr])
                polynomial= numpy.polyfit (daysArray, closeData[:, stockCtr], 3)
                predictedClosingValue= numpy.polyval(polynomial, noOfDaysToUse + daysAhead -1)
                #if (predictedClosingValue <0):
                    #print "predicted closing value negative! But that can't be!"
                    #predictedClosingValue=0
                #print "val: " + str(predictedClosingValue) + ", closingVal: "+ str(closeData[noOfDaysToUse][stockCtr])+", stock: " +str(listOfStocks[stockCtr]+ " ts: "+ str(timestamps[beginIndex]))
                #print "val: " + str(predictedClosingValue) +", stock: " +str(listOfStocks[stockCtr]+ " ts: "+ str(timestamps[beginIndex]))
                
                valueToBeAdded= (predictedClosingValue- closeData[noOfDaysToUse][stockCtr])/ closeData[noOfDaysToUse][stockCtr]
                adm.addRow(listOfStocks[stockCtr], "blah",valueToBeAdded , timestamps[beginIndex])
                #if ends
            
            stockCtr+=1
            #while ends
    else:
        #closeData is None
        print "closeData is None"
#        for stock in listOfStocks:
#            adm.addRow(stock, "blah", 0.0, beginTS)
#            zeroDueToNoneCtr+=1
    
    beginIndex+=1
    #while ends

adm.closeFile()
########NEW FILE########
__FILENAME__ = EventProfiler
# (c) 2011, 2012 Georgia Tech Research Corporation
# This source code is released under the New BSD license.  Please see
# http://wiki.quantsoftware.org/index.php?title=QSTK_License
# for license details.
#
# Created on October <day>, 2011
#
# @author: Vishal Shekhar
# @contact: mailvishalshekhar@gmail.com
# @summary: Event Profiler Application
#

import pandas 
import numpy as np
import Events as ev
import matplotlib.pyplot as plt
from pylab import *
import qstkutil.qsdateutil as du
import qstkutil.tsutil as tsu
import datetime as dt
import qstkutil.DataAccess as da

class EventProfiler():

	def __init__(self,eventMatrix,startday,endday,\
            lookback_days = 20, lookforward_days =20,\
            verbose=False):

	    """ Event Profiler class construtor 
		Parameters : evenMatrix
			   : startday
			   : endday
		(optional) : lookback_days ( default = 20)
		(optional) : lookforward_days( default = 20)

		eventMatrix is a pandas DataMatrix
		eventMatrix must have the following structure:
		    |IBM |GOOG|XOM |MSFT| GS | JP |
		(d1)|nan |nan | 1  |nan |nan | 1  |
		(d2)|nan | 1  |nan |nan |nan |nan |
		(d3)| 1  |nan | 1  |nan | 1  |nan |
		(d4)|nan |  1 |nan | 1  |nan |nan |
		...................................
		...................................
		Also, d1 = start date
		nan = no information about any event.
		 = status bit(positively confirms the event occurence)
	    """

	    self.eventMatrix = eventMatrix
	    self.startday = startday
	    self.endday = endday
	    self.symbols = eventMatrix.columns
	    self.lookback_days = lookback_days
	    self.lookforward_days = lookforward_days
	    self.total_days = lookback_days + lookforward_days + 1
	    self.dataobj = da.DataAccess('Norgate')
	    self.timeofday = dt.timedelta(hours=16)
	    self.timestamps = du.getNYSEdays(startday,endday,self.timeofday)
            self.verbose = verbose
            if verbose:
                print __name__ + " reading historical data"
	    self.close = self.dataobj.get_data(self.timestamps,\
                self.symbols, "close", verbose=self.verbose)
	    self.close = (self.close.fillna()).fillna(method='backfill')
	
	def study(self,filename,method="mean", \
            plotMarketNeutral = True, \
            plotErrorBars = False, \
            plotEvents = False, \
            marketSymbol='SPY'):
	    """ 
	    Creates an event study plot
            the marketSymbol must exist in the data if plotMarketNeutral 
            is True This method plots the average of market neutral 
            cumulative returns, along with error bars The X-axis is the 
            relative time frame from -self.lookback_days to self.lookforward_days
            Size of error bar on each side of the mean value on the i 
            relative day = abs(mean @ i - standard dev @ i)
            parameters : filename. Example filename="MyStudy.pdf"
	    """

            #plt.clf()
            #plt.plot(self.close.values)
            #plt.legend(self.close.columns)
            #plt.ylim(0,2)
            #plt.draw()
            #savefig('test1.pdf',format='pdf')

            # compute 0 centered daily returns
            self.dailyret = self.close.copy() 
            tsu.returnize0(self.dailyret.values)

            # make it market neutral
	    if plotMarketNeutral:
                # assuming beta = 1 for all stocks --this is unrealistic.but easily fixable.
        	self.mktneutDM = self.dailyret - self.dailyret[marketSymbol] 
                # remove the market column from consideration
	    	del(self.mktneutDM[marketSymbol])
	    	del(self.eventMatrix[marketSymbol])
	    else:
		self.mktneutDM = self.dailyret

            # Wipe out events which are on the boundary.
            self.eventMatrix.values[0:self.lookback_days,:] = NaN
            self.eventMatrix.values[-self.lookforward_days:,:] = NaN

            # prepare to build impact matrix
            rets = self.mktneutDM.values
            events = self.eventMatrix.values
            numevents = nansum(events)
            numcols = events.shape[1]
            # create a blank impact matrix
            impact = np.zeros((self.total_days,numevents))
            currcol = 0
            # step through each column in event matrix
            for col in range(0,events.shape[1]):
                if (self.verbose and col%20==0):
                    print __name__ + " study: " + str(col) + " of " + str(numcols)
                # search each column for events
                for row in range(0,events.shape[0]):
                    # when we find an event
                    if events[row,col]==1.0:
                        # copy the daily returns in to the impact matrix
                        impact[:,currcol] = \
                            rets[row-self.lookback_days:\
                            row+self.lookforward_days+1,\
                            col]
                        currcol = currcol+1

            # now compute cumulative daily returns
            impact = cumprod(impact+1,axis=0)
            impact = impact / impact[0,:]
            # normalize everything to the time of the event
            impact = impact / impact[self.lookback_days,:]

            # prepare data for plot
            studystat = mean(impact,axis=1)
            studystd = std(impact,axis=1)
            studyrange = range(-self.lookback_days,self.lookforward_days+1)

            # plot baby
            plt.clf()
            if (plotEvents): # draw a line for each event
                plt.plot(studyrange,\
                    impact,alpha=0.1,color='#FF0000')
            # draw a horizontal line at Y = 1.0
            plt.axhline(y=1.0,xmin=-self.lookback_days,xmax=self.lookforward_days+1,\
                color='#000000')
            if plotErrorBars==True: # draw errorbars if user wants them
                plt.errorbar(studyrange[self.lookback_days:],\
                    studystat[self.lookback_days:],\
                    yerr=studystd[self.lookback_days:],\
                    ecolor='#AAAAFF',\
                    alpha=0.1)
            plt.plot(studyrange,studystat,color='#0000FF',linewidth=3,\
                label='mean')
            # set the limits of the axes to appropriate ranges
            plt.ylim(min(min(studystat),0.5),max(max(studystat),1.5))
            plt.xlim(min(studyrange)-1,max(studyrange)+1)
            # draw titles and axes
            if plotMarketNeutral:
                plt.title(('market relative mean of '+ \
                    str(int(numevents))+ ' events'))
            else:
                plt.title(('mean of '+ str(int(numevents))+ ' events'))
            plt.xlabel('Days')
            plt.ylabel('Cumulative Abnormal Returns')
            plt.draw()
            savefig(filename,format='pdf')

########NEW FILE########
__FILENAME__ = names
import matplotlib.pyplot as plt
from pylab import *
from qstkutil import DataAccess as da
from qstkutil import timeutil as tu
from qstkutil import pseries as ps
import pandas

# Set the list of stocks for us to look at
# symbols= list()
# symtoplot = 'VZ'
# symbols.append(symtoplot)
# symbols.append('IBM')
# symbols.append('GOOG')

symbols = list(np.loadtxt('allsyms.csv',dtype='str',delimiter=',',
        comments='#',skiprows=0))

# Set start and end boundary times.  They must be specified in Unix Epoch
tsstart = tu.ymd2epoch(2008,1,1)
tsend = tu.ymd2epoch(2008,12,31)

# Get the data from the data store
storename = "Norgate" # get data from our daily prices source
fieldname = "close" # adj_open, adj_close, adj_high, adj_low, close, volume
closes = ps.getDataMatrixFromData(storename,fieldname,symbols,tsstart,tsend)

cldata = closes.values
cldata[cldata<=4.0]  = 1.0
cldata[cldata>4.0]   = 0
cldata[isnan(cldata)]= 0
lows = sum(cldata,axis=0)

lownames = array(closes.cols())[lows>0]

f = open('below4.csv', 'w')
for sym in lownames:
	f.write(sym + '\n')
f.close()

########NEW FILE########
__FILENAME__ = OldDataAccess
'''
Created on Jun 2, 2010

@author: Shreyas Joshi
@contact: shreyasj@gatech.edu
'''

import tables as pt
#import sys


class DataAccess:
    '''
    @attention:  Assumption is that the data has symbols and timestamps. Also assumes that data is read in from low timestamp to high timestamp.
                 data is not sorted after reading in.
    @warning: No checks are perform to prevent this function from returning future data. You will get what you ask for! (assuming its there!)
    @summary: The purpose of this class is to be a general way to access any data about stocks- that is in a 2-D array with one dimension
              being stock symbols and the other being time. Each element of data can be an encapsulation of things like opening price, closing
              price, adj_close etc... or it can just be a single value.
    '''

    def __init__(self, fileIterator, noisy, dataItemsList=None, SYMBOL='symbol', TIMESTAMP='timestamp'):
        '''
        @param fileIterator: [filename].[root].[group name].[table name] as needed to read in and hdf5 file 
        @param noisy: is it noisy?  
        @param dataItemList: should be a list of all the data items that need to be read in from the file. If None then all will data will be read
        @param SYMBOL: just in case that the name of the "symbol" data item is not exactly "symbol" this can be changed.
        '''
        
        self.allDataList=[]
        self.dataItemsList=[]
#        self.SYMBOL=[]
#        self.TIMESTAMP=[]    
        self.SYMBOL= SYMBOL
        self.TIMESTAMP= TIMESTAMP
        self.noisy=noisy
        
        
       #Making sure dataItemsList has symbols and timestamps
        if (dataItemsList is not None):
         try:
          dataItemsList.index("symbol")
         except ValueError:
           print "adding SYMBOL"   
           dataItemsList.append(self.SYMBOL)
          
         try:
          dataItemsList.index("timestamp")
         except ValueError:  
           print "adding TIMESTAMP"
           dataItemsList.append(self.TIMESTAMP)  
          
        else:
         #adding all known items to the list- change this list to change default behaviour (ie when dataItemsList is none)
         dataItemsList= list()
         dataItemsList.append('symbol')
         dataItemsList.append('timestamp')
         dataItemsList.append('exchange')
         dataItemsList.append('adj_open')    
         dataItemsList.append('adj_close')
         dataItemsList.append('adj_high')
         dataItemsList.append('adj_low')
         dataItemsList.append('close')
         dataItemsList.append('volume')
         
        self.dataItemsList= dataItemsList
        for row in fileIterator.iterrows():  
#          print "SYM: "+str(row['symbol'])+", EX: "+ str(row['exchange'])+", ALPHA: "+str(row['alphaValue'])+", TIMESTAMP: "+str(row['timestamp'])
          self.allDataList.append(self.cloneRow(row, dataItemsList))
#          print self.allDataList[len(self.allDataList)-1]
        self.allDataList.sort(cmp=None, key=None, reverse=False)  
    # constructor ends
    

    def getData (self, stockList=None, dataList=None, beginTS=None, endTS=None):            
        '''
        @param stockList: If data about only 1 stock is needed then this param can be a string- else a list of strings of names of all the stocks that you want data about. If not specified data about all stocks will be returned
        @param dataList: If only one dataItem is needed (say adj_open only) then this param can be a string- else a list of strings of the names of all the data items you want. If not specified all data items will be returned.
        @param beginTS: If specified- only rows with timestamp greater than or equal to this will be returned
        @param endTS: If specified- only rows with timestamp smaller than or equal to this will be returned
        
        @warning: function does not check if beginTS < endTS- but violating this will result in None being returned.       
        @summary: this function just traverses over the data. It assumes that all the data fits into memory.
                  The real reading from disk is done in the constructor
                  To get data for one timestamp only- set beginTS=endTS= the timestamp you want.
        @return: returns the requested data as a list. NOTE: the returned list will always have symbol and timestamp- 
                 even if they weren't explicitly asked for in the dataItemsList. If no data is found then an empty list is returned.
        '''
        
        #stockList (despite its name) can be a string or a list
        if (stockList is not None):
         if (type(stockList) is not list):
            if (type (stockList) is not str):
                print "Stocks must either be a string (if you want only 1 stock) or a list of strings"
                raise TypeError
            else:
                #its not a list but its a string
                tempStr= str(stockList)
                stockList= list()
                stockList.append(tempStr)
#                print "changed stockList from str to list!"
#                print "Printing first in the list: " + stockList[0]
        
        
        #dataList (despite its name) can be a string or a list
        if (dataList is not None):
         if (type(dataList) is not list):
            if (type (dataList) is not str):
                print "data items you want must either be a string (of you want only one data item) or a list of strings"
                raise TypeError
            else:
                #its not a list but its a string
                tempStr= str(dataList)
                dataList= list()
                dataList.append(tempStr)
#                print "changed dataList from str to list!"
#                print "Printing first in the list: " + dataList[0]
        else:
            #dataList is None
            dataList= self.dataItemsList
                
        #Making sure dataList has symbols and timestamps
        try:
         dataList.index("symbol")
        except ValueError:
#          print "adding SYMBOL"   
          dataList.append(self.SYMBOL)
          
        try:
         dataList.index("timestamp")
        except ValueError:  
#          print "adding TIMESTAMP"
          dataList.append(self.TIMESTAMP)
               
        #Now, filter out the data from allDataList, put it in another list (inefficient?) and return the other list
        tempFilteredList=[]
        for item in self.allDataList:
            if (beginTS is not None):
              # which means we need to reject all rows with timestamp < beginTS
              if item[self.TIMESTAMP]< beginTS: # so = will be included
#                  print "rejecting because of beginning TS"
                  continue #skipping this item
            
            if (endTS is not None):
               # which means we need to reject all rows with timestamp > endTS
               if item[self.TIMESTAMP]> endTS: # so = will be included
#                  print "rejecting because of ending TS" 
                  continue #skipping this item
            
            if (stockList is not None):
             # We need to return this item only if its name is present in the stockList
             nameFound= False
             for item2 in stockList:
                 if (item2== item [self.SYMBOL]):
                    nameFound= True
             #searching done
             if (nameFound== False):
#                 print "rejecting because of stock name not found"
                 continue #skipping this item
                                  
             # if we got till here then the row must be returned. Hence adding to list
            if (dataList is None): 
             tempFilteredList.append(self.cloneRow(item, self.dataItemsList))
            else:
             tempFilteredList.append(self.cloneRow(item, dataList))    
        # for item in self.allDataList done
        
        if (len (tempFilteredList)==0):
            if self.noisy is True:
             print "Warning: no data found"
#            sys.stdout.flush()
        
        
        return tempFilteredList        
        
    # getData ends
    
    def getDataList(self, stockName, dataItemName, beginTS=None, endTS=None):
     '''
     @param stockName: The name of the stock whose data you need. This has to be a string. One stock only
     @param dataItemName:The data item that you need like open, close, volume etc. This has to be a string. Only one can be specified.
     @param beginTS: Optional parameter. If specified only data for timestamp >= beginTS will be considered.
     @param endTS: Optional paramter. If specified only data for timestamp <= endTS will be considered.
      
     @warning: function does not check if beginTS < endTS- but violating this will result in None being returned.
     @summary: Use this function to get a list of values of some dataItem of a particular stock. Unlike the getData function this function 
               does not return a list of dictionaries with the stock symbol and timestamp. 
               To get data for one timestamp only- set beginTS=endTS= the timestamp you want.
     @return: A list of dataItemName values for stockName between beginTS and endTS- if specified - or values for all timestamps if not
              specified. If no data is found then an empty list is returned.          
     '''   
    
     if (type(stockName) is not str):
         print "stock name must be a string"
         raise TypeError
     
     if (type(dataItemName) is not str):
         print "data item must be a string"
         raise TypeError
     
     tempList=[]
     
     for item in self.allDataList:
         if beginTS is not None:
           if item[self.TIMESTAMP]< beginTS:
               continue #skipping this item
         
         if endTS is not None:
             if item[self.TIMESTAMP] > endTS:
                 continue #skipping this item
             
         if item[self.SYMBOL] == stockName:
             tempList.append(item[dataItemName])
     #for loop ends
     
     if (len (tempList)==0):
         if self.noisy is True:
           print "Warning: no data found"
#         sys.stdout.flush()
                 
     return tempList              
    #getDataList ends
    
    
    def getDataItem (self, stockName, dataItemName, timestamp):
        
     '''
     @param stockName: The name of the stock whose data you need. This has to be a string. One stock only
     @param dataItemName:The data item that you need like open, close, volume etc. This has to be a string. Only one can be specified.
     @param timestamp: Required parameter. Only data for this timestamp will be considered.
    
     @summary: Use this function to get one value of some dataItem of a particular stock. Unlike the getData function, this function 
               does not return a list of dictionaries with the stock symbol and timestamp. Unlike the getDataList function, this 
               function does not return an array of values. 
     @return: The value of dataItemName value for stockName at the specified timestamp
     '''   
        
     if (type(stockName) is not str):
         print "stock name must be a string"
         raise TypeError
     
     if (type(dataItemName) is not str):
         print "data item must be a string"
         raise TypeError
     
#       tempStr=str("")
     
     for item in self.allDataList:
           if item[self.SYMBOL]== stockName:
               if item[self.TIMESTAMP]== timestamp:
                   return item[dataItemName]            
       
     if self.noisy is True:
      print "Warning: no data found"
#     sys.stdout.flush()
     return None
    #getDataitem ends    
    
    
    def cloneRow(self, row, itemsList):
        
        dct={}
        for dataItem in itemsList:
            try:
             dct[str(dataItem)]= row[str(dataItem)] 
            except KeyError:
             print "Error: "+str(dataItem)+" not available"
             raise KeyError
        return dct
########NEW FILE########
__FILENAME__ = pseries
'''
Created on Oct 7, 2010

@author: Tucker Balch
@contact: tucker@cc.@gatech.edu
'''

import os
import pandas as pandas
from qstkutil import DataAccess as da
from qstkutil import timeutil as tu

__version__ = "$Revision: 156 $"

def getDataMatrixFromData(dataname,partname,symbols,tsstart,tsend):
	pathpre = os.environ.get('QSDATA') + "/Processed"
	if dataname == "Norgate":
		pathsub = "/Norgate/Equities"
		paths=list()
		paths.append(pathpre + pathsub + "/US_NASDAQ/")
		paths.append(pathpre + pathsub + "/US_NYSE/")
		paths.append(pathpre + pathsub + "/US_NYSE Arca/")
		paths.append(pathpre + pathsub + "/OTC/")
		paths.append(pathpre + pathsub + "/US_AMEX/")
		paths.append(pathpre + pathsub + "/Delisted_US_Recent/")
		paths.append(pathpre + pathsub + "/US_Delisted/")
		datastr1 = "/StrategyData"
		datastr2 = "StrategyData"
	else:
		raise Exception("unknown dataname " + str(dataname))

	data = da.DataAccess(True, paths, datastr1, datastr2,
       		False, symbols, tsstart, tsend)
	tss = list(data.getTimestampArray())
	start_time = tss[0]
	end_time = tss[-1]
	dates = []
	for ts in tss:
		dates.append(tu.epoch2date(ts))
	vals = data.getMatrixBetweenTS(symbols,partname,
		start_time,end_time)
	syms = list(data.getListOfSymbols())
	del data

	return(pandas.DataMatrix(vals,dates,syms))
# end getTSFromData

########NEW FILE########
__FILENAME__ = OrderModel
import tables as pt #@UnresolvedImport
import time

class FillModel(pt.IsDescription):
    timestamp = pt.Time64Col()
    quantity = pt.Int32Col()
    cashChange = pt.Float32Col()
    commission = pt.Float32Col()
    impactCost = pt.Float32Col()
    
class OrderModel(pt.IsDescription):
    task = pt.StringCol(5)
    shares = pt.Int32Col()
    symbol = pt.StringCol(30)
    order_type = pt.StringCol(5)       #moo moc limit vwap
    duration = pt.Time64Col()
    timestamp = pt.Time64Col()
    close_type = pt.StringCol(4)       #lifo or fifo for a sell, none for a buy
    limit_price = pt.Float32Col()
    fill = FillModel()
########NEW FILE########
__FILENAME__ = PortfolioModel
import tables as pt #@UnresolvedImport

class PortfolioModel(pt.IsDescription):
    cash = pt.Float32Col()
    timestamp = pt.Time64Col()
########NEW FILE########
__FILENAME__ = PositionModel
import tables as pt #@UnresolvedImport
import numpy as np

class PositionModel(pt.IsDescription):
    timestamp = pt.Time64Col()
    symbol = pt.StringCol(30) 
    shares = pt.Int32Col()
    purchase_price = pt.Float32Col()
########NEW FILE########
__FILENAME__ = StrategyDataModel
import tables as pt #@UnresolvedImport
import time
    
class StrategyDataModel(pt.IsDescription):
    symbol = pt.StringCol(30)           #30 char string; Ticker
    exchange = pt.StringCol(10)         #10 char string; NYSE, NASDAQ, etc.
    adj_high = pt.Float32Col()
    adj_low = pt.Float32Col()
    adj_open = pt.Float32Col()
    adj_close = pt.Float32Col()
    close = pt.Float32Col()
    volume = pt.Int32Col()
    timestamp = pt.Time64Col()
    date = pt.Int32Col()
    interval = pt.Time64Col()
########NEW FILE########
__FILENAME__ = BollingerOptimizer
'''
Created on Aug 2, 2010

@author: sjoshi42
'''


'''
Created on Jul 27, 2010

@author: Shreyas Joshi
@summary: This module reads in the alpha values generated based on the bollinger bands. It then outputs orders accordingly.
'''
import DataAccess
import numpy
class Optimizer(object):
    
    def __init__(self, listOfStocks):
        self.listOfStocks= listOfStocks
        self.DAY=86400
        dataItemsList=list()
        dataItemsList.append("alphaValue")
        self.alphaData= DataAccess.DataAccess(False, "AAPLonlybollingerBandsAlphaVals.h5", "/alphaData", "alphaData", True, listOfStocks, None, None, None, dataItemsList)
        print "Timestamps are: " 
        for ts in self.alphaData.timestamps:
            print ts
        #__init__ done
        
        
    def execute(self, portfolio,positions,timestamp,stockInfo, dataAccess):
        
        output=[]
        for stock in self.listOfStocks:
            alphaVal= self.alphaData.getStockDataItem(stock, "alphaValue", timestamp)
            
            
            print "alphaVal: "+ str (alphaVal)+ ", stock: "+ str(stock)+", ts: " + str(timestamp)
            
            if not (numpy.isnan(alphaVal)):
                #alphaVal is not Nan
                if (alphaVal > 0.9):
                    #buy
                    order= stockInfo.OutputOrder()
                    order.symbol= stock
                    order.volume= 100 #min(int(500*alphaVal), 100)
                    order.task= 'buy'
                    order.orderType = 'moc'
                    order.closeType = 'fifo'
                    order.duration = self.DAY
                    newOrder = order.getOutput()
                    if newOrder != None:
                         output.append(newOrder)
                    else:
                         print "ERROR! ERROR! ERROR!"                    
                else:
                    pass
                    #print "alhpaVal for "+str(stock)+" is: " + str(alphaVal)
            else:
                pass
                #print "alphaVal is nan"
            
            #for stock in self.listOfStocks done
                     
        for stock in portfolio.getListOfStocks():
            alphaVal= self.alphaData.getStockDataItem(stock, "alphaValue", timestamp)
            if not (numpy.isnan(alphaVal)):
                if (alphaVal < -0.6):
                    order= stockInfo.OutputOrder()
                    order.symbol= stock
                    order.volume=  max( (int (portfolio.getHeldQty(stock) /4)) , 1)
                    order.task= 'sell'
                    order.orderType = 'moc'
                    order.closeType = 'fifo'
                    order.duration = self.DAY
                    newOrder = order.getOutput()
                    if newOrder != None:
                        output.append(newOrder)
                    else:
                        print "ERROR! ERROR! ERROR!"
        return output                
                                            
                
        
    

########NEW FILE########
__FILENAME__ = curveFittingOptimizer
'''
Created on Jul 27, 2010

@author: Shreyas Joshi
'''
import DataAccess
import numpy
class Optimizer(object):
    
    def __init__(self, listOfStocks):
        self.listOfStocks= listOfStocks
        self.DAY=86400
        dataItemsList=list()
        dataItemsList.append("alphaValue")
        self.alphaData= DataAccess.DataAccess(False, "curveFittingAlphaVals_Jan_85_to_2010.h5", "/alphaData", "alphaData", True, listOfStocks, None, None, None, dataItemsList)
        print "Timestamps are: " 
        for ts in self.alphaData.timestamps:
            print ts
        #__init__ done
        
        
    def execute(self, portfolio,positions,timestamp,stockInfo, dataAccess):
        
        output=[]
        for stock in self.listOfStocks:
            alphaVal= self.alphaData.getStockDataItem(stock, "alphaValue", timestamp)
            
            
            print "alphaVal: "+ str (alphaVal)+ ", stock: "+ str(stock)+", ts: " + str(timestamp)
            
            if not (numpy.isnan(alphaVal)):
                #alphaVal is not Nan
                if (alphaVal > 15.0):
                    #buy
                    order= stockInfo.OutputOrder()
                    order.symbol= stock
                    order.volume= 100 #min(int(500*alphaVal), 100)
                    order.task= 'buy'
                    order.orderType = 'moc'
                    order.closeType = 'fifo'
                    order.duration = self.DAY
                    newOrder = order.getOutput()
                    if newOrder != None:
                         output.append(newOrder)
                    else:
                         print "ERROR! ERROR! ERROR!"                    
                else:
                    pass
                    #print "alhpaVal for "+str(stock)+" is: " + str(alphaVal)
            else:
                pass
                #print "alphaVal is nan"
            
            #for stock in self.listOfStocks done
                     
        for stock in portfolio.getListOfStocks():
            alphaVal= self.alphaData.getStockDataItem(stock, "alphaValue", timestamp)
            if not (numpy.isnan(alphaVal)):
                if (alphaVal < 3.0):
                    order= stockInfo.OutputOrder()
                    order.symbol= stock
                    order.volume= portfolio.getHeldQty(stock)
                    order.task= 'sell'
                    order.orderType = 'moc'
                    order.closeType = 'fifo'
                    order.duration = self.DAY
                    newOrder = order.getOutput()
                    if newOrder != None:
                        output.append(newOrder)
                    else:
                        print "ERROR! ERROR! ERROR!"
        return output                
                                            
                
        
    

########NEW FILE########
__FILENAME__ = Optimizer
'''
Created on May 28, 2010

@author: Shreyas Joshi
@contact: shreyasj@gatech.edu
'''

import DataAccess as da
import tables as pt
import math
import numpy as np

class Optimizer(object):
    '''
    @summary: The optimizer class is supposed to get the alpha values and data on the current portfolio to make a decision on what trades to make.
    
    '''

# one day in unix time
    

    def __init__(self, listOfStocks):
        '''
        Constructor
        '''
#        self.alphah5f= pt.openFile("randomAlpha.h5", mode = "a") # if mode ='w' is used here then the file gets overwritten!
        self.listOfLoosingStocks=list()
        self.noOfDaysStockHasBeenLoosingValue=list()
        
        self.listOfStocks= list(listOfStocks)
        self.DAY = 86400
        staticDataItemsList= list()
        staticDataItemsList.append("blah")
        dataItemsList= list()
        dataItemsList.append("alphaValue")
        self.minCom= 5.00
        self.ComPerShare = 0.01
#        self.alphaData= da.DataAccess(False,"randomAlpha.h5","/alphaData", "alphaData", True, listOfStocks, None, dataItemsList)
    #def __init__ ends
        
    def strategyOne (self, portfolio,positions,timestamp,stockInfo, dataAccess):
     output=[]
     adjOpenData= dataAccess.getMatrix (self.listOfStocks, "adj_open", timestamp- 2*self.DAY, timestamp- self.DAY)
     
     if adjOpenData is not None:
         #choose the biggest loser
         ctr=0
         currentBiggestLoss= - float("infinity")
         currentBiggestLoserIndex=-1
         while (ctr<len(self.listOfStocks)):
#             print "adjOpenData shape: " + str(adjOpenData.shape) + ", ctr: " + str(ctr) + ", len(self.listOfStocks): " + str(len(self.listOfStocks))
             if (adjOpenData[0][ctr] > adjOpenData[1][ctr]):
                 #Which means the stock lost value
                 if ((adjOpenData[0][ctr] - adjOpenData[1][ctr])/adjOpenData[0][ctr] > currentBiggestLoss): #biggest % loss
                     currentBiggestLoss= (adjOpenData[0][ctr] - adjOpenData[1][ctr])
                     currentBiggestLoserIndex= ctr
             ctr+=1
             #While loop done
         #Now  we have the stock which lost the most value. We buy it and also put in a sell order for 2 days later
         
         if (currentBiggestLoserIndex != -1):
             order= stockInfo.OutputOrder()
             order.symbol= self.listOfStocks[currentBiggestLoserIndex]
             order.volume= 10
             order.task= 'buy'
             order.orderType = 'moc'
             order.duration = self.DAY
             
             newOrder = order.getOutput()
             
             if newOrder != None:
                output.append(newOrder)
             else:
                 print "ERROR! ERROR! ERROR!"
             #if (currentBiggestLoserIndex != -1): ends
         #if adjOpenData is not None: ends
     else:
         print "adjOpenData is None!"
         #else ends    
     #Now to decide which stocks to sell
     currentPositions= positions.getPositions()
     for stock in portfolio.currStocks:
         for pos in currentPositions:
          if (str(pos['symbol'])== str(stock)):   
           if ((pos['timestamp'] )< timestamp - 2*self.DAY):
               temp=  dataAccess.getStockDataItem(pos['symbol'], 'adj_close', timestamp)
               if not (np.isnan(temp)):
                if ((pos['purchase_price'] + (pos['shares']*self.ComPerShare))< temp):
                 order= stockInfo.OutputOrder()
                 order.symbol= stock
                 order.volume= pos['shares']
                 order.task= 'sell'
                 order.orderType = 'moc'
                 order.closeType = 'fifo'
                 order.duration = self.DAY
                 newOrder = order.getOutput()
                 
                 if newOrder != None:
                   output.append(newOrder)
                 else:
                   print "ERROR! ERROR! ERROR!"
        #for pos in currentPositions: ends         
     return output  
        
    def strategyTwo (self, portfolio,positions,timestamp,stockInfo, dataAccess):
        #Here we track all the stocks that continuously loose value- then buy them when they stop loosing value. Then hold them until they keep
        #gaining value. Then sell them
        
        
        output=[]
        
        #adjOpenData= dataAccess.getMatrix (self.listOfStocks, "adj_open", timestamp- 2*self.DAY, timestamp- self.DAY)
        
        adjOpenData= dataAccess.getMatrixFromTS (self.listOfStocks, "adj_open", timestamp, -1)
        
#        print "list of loosing stocks: "+ str (self.listOfLoosingStocks)
#        print "current positions: " + str(positions.getPositions())
#        print "no of days: " + str (self.noOfDaysStockHasBeenLoosingValue)
        
        if (adjOpenData is not None):
            ctr=0
            while (ctr< len(self.listOfStocks)):
                if (adjOpenData[0][ctr] > adjOpenData[1][ctr]):
                    
                    try:
                        index2= self.listOfLoosingStocks.index(self.listOfStocks[ctr])
                        self.noOfDaysStockHasBeenLoosingValue[index2]+=1
                    except:
                        #stock not found in the list
                        self.listOfLoosingStocks.append(self.listOfStocks[ctr])
                        self.noOfDaysStockHasBeenLoosingValue.append(1)
                
                    currentPositions= positions.getPositions()        
                    for pos in currentPositions:
                        try:
                            index2= self.listOfLoosingStocks.index(pos['symbol']) #if it isn't in this list then we don't have to sell it
                            if (self.noOfDaysStockHasBeenLoosingValue[index2] > 2):
                                # we have this stock and it lost value twice
                                # Ergo- we sell
                                #sell
                                #if ((pos['purchase_price'] + (pos['shares']*self.ComPerShare))< temp): #rig it to make money
                                  print str(pos['symbol'])+" finally lost value for "+ str(self.noOfDaysStockHasBeenLoosingValue[index2])+" days. Selling it"
                                  order= stockInfo.OutputOrder()
                                  order.symbol= pos['symbol']
                                  order.volume= pos['shares']
                                  order.task= 'sell'
                                  order.orderType = 'moc'
                                  order.closeType = 'fifo'
                                  order.duration = self.DAY

                                  newOrder = order.getOutput()
                                  if newOrder != None:
                                    output.append(newOrder)
                                  else:
                                    print "ERROR! ERROR! ERROR!"
                        except ValueError:
                            pass #index not found 
                    #for pos in currentPositions
                             
                else:
                    #this stock did not loose value
                    
                    #Check if had been loosing value
                    #print str(self.listOfStocks[ctr])+ " gained value"
                    
                    try:
                      index1= self.listOfLoosingStocks.index(self.listOfStocks[ctr])
                      if (self.noOfDaysStockHasBeenLoosingValue[index1]>3):
                          
                            #print "This stock has been loosing value for atleast 3 days"
                            
                            order= stockInfo.OutputOrder()
                            order.symbol= self.listOfStocks[ctr]
                            order.volume= min(10* self.noOfDaysStockHasBeenLoosingValue[index1], 100)
                            order.task= 'buy'
                            order.orderType = 'moc'
                            order.closeType = 'fifo'
                            order.duration = self.DAY
                            newOrder = order.getOutput()
                            if newOrder != None:
                                 output.append(newOrder)
                            else:
                                 print "ERROR! ERROR! ERROR!"
            
                          #The stock was loosing value for <=3 days  but now gained value- so off with the head
                      self.listOfLoosingStocks.pop(index1)
                      self.noOfDaysStockHasBeenLoosingValue.pop(index1)                                  

                    except ValueError:
                        pass
                    
                    
#                    try:
#                        index1= self.listOfLoosingStocks.index(self.listOfStocks[ctr])
#                        
#                        print str(self.listOfStocks[ctr])+" lost value for "+ str(self.noOfDaysStockHasBeenLoosingValue[index]+ "..and then gained..")
#                        #Stock found
#                        #if it had lost value for more than 2 days then buy!
#                        if (self.noOfDaysStockHasBeenLoosingValue[index1]>3):
#                            #buy
#                            order= stockInfo.OutputOrder()
#                            order.symbol= self.listOfStocks[ctr]
#                            order.volume= max(10* self.noOfDaysStockHasBeenLoosingValue[index1], 100)
#                            order.task= 'buy'
#                            order.orderType = 'moc'
#                            order.closeType = 'fifo'
#                            order.duration = self.DAY
#                            newOrder = order.getOutput()
#                            if newOrder != None:
#                                 output.append(newOrder)
#                            else:
#                                 print "ERROR! ERROR! ERROR!"
#                        else:
#                            #it was loosing value- but for less than 2 days. So we just remove this entry...
#                            self.listOfLoosingStocks.pop(index1)
#                            self.noOfDaysStockHasBeenLoosingValue.pop(index1)
#                    except:
#                        #Not found- this stock had not lost value
#                        print "could not find index! Possibly a bug"
                ctr+=1
                #while loop ends..hopefully!
        
            
        
        
        
        return output
        #strategyTwo ends
    
    
    
    def execute (self, portfolio,positions,timestamp,stockInfo, dataAccess):
     '''
     @param portfolio: The portfolio object that has symbol and value of currently held stocks.
     @param positions: Detailed info about current stock holdings.
     @param timestamp: Current simulator time stamp
     @param stockInfo: Not used anymore for dataAccess.
     @param dataAccess: a dataAccess object that will henceforth be used to access all data
     '''
     
     output=[]
     #output =  self.strategyOne(portfolio, positions, timestamp, stockInfo, dataAccess)
     output =  self.strategyTwo(portfolio, positions, timestamp, stockInfo, dataAccess)
        #for pos in currentPositions: ends         
     #print "The outout is: " + str(output)
     
     return output           
                       
                 
             
         
         
    
    
    
    
#    
#     #Right now this is stratDemo firstStrategy
#     output = []
#    #This first for loop goes over all of the stock data to determine which stocks to buy
#     for stock in dataAccess.getListOfStocks(): #stockInfo.getStocks(startTime = timestamp - self.DAY,endTime = timestamp):
#        # if close is higher than open and close is closer to high than open is to low, buy
#        
##        print "In Optimizer"
##        print "     timestamp asked for is: " + str(timestamp - self.DAY, timestamp)
##        print "self.DAY: " + str(self.DAY)
#        adj_open= dataAccess.getStockDataList(stock, 'adj_open', timestamp - self.DAY, timestamp)
#        adj_close= dataAccess.getStockDataList(stock, 'adj_close', timestamp - self.DAY, timestamp)
#        adj_high= dataAccess.getStockDataList(stock, 'adj_high', timestamp - self.DAY, timestamp)
##        alphaValue= self.alphaData.getStockDataList (stock, 'alphaValue', timestamp - self.DAY, timestamp)
#        
#        if (adj_open.size > 0):
#            #if alphaValue <= 0.5 and adj_open < adj_close and (adj_high - adj_close) > (adj_open - adj_close): #highly possible bug here?
#         if adj_open < adj_close and (adj_high - adj_close) > (adj_open - adj_close): #highly possible bug here?
#            order = stockInfo.OutputOrder()
#            order.symbol = stock #stock['symbol']
#            order.volume = 20 
#            order.task = 'buy'
#            order.orderType = 'moc'
#            order.duration = self.DAY * 2
#            newOrder = order.getOutput()
#            if newOrder != None:
#                output.append(newOrder)
#            
#    #This for loop goes over all of our current stocks to determine which stocks to sell
#     for stock in portfolio.currStocks:
#        openPrice = list(dataAccess.getStockDataList(stock, 'adj_open',timestamp - self.DAY, timestamp))#dataAccess.getDataList(stock, 'adj_open',timestamp - self.DAY, timestamp)#stockInfo.getPrices(timestamp - self.DAY, timestamp,stock,'adj_open')
#        closePrice = list(dataAccess.getStockDataList(stock, 'adj_close',timestamp - self.DAY, timestamp))#dataAccess.getDataList(stock, 'adj_close',timestamp - self.DAY, timestamp) #stockInfo.getPrices(timestamp - self.DAY, timestamp,stock,'adj_close')
#        highPrice =  list(dataAccess.getStockDataList(stock, 'adj_high',timestamp - self.DAY, timestamp))#dataAccess.getDataList(stock, 'adj_high',timestamp - self.DAY, timestamp) #stockInfo.getPrices(timestamp - self.DAY, timestamp,stock,'adj_high')
#        lowPrice = list(dataAccess.getStockDataList(stock, 'adj_low',timestamp - self.DAY, timestamp))#dataAccess.getDataList(stock, 'adj_low',timestamp - self.DAY, timestamp) #stockInfo.getPrices(timestamp - self.DAY, timestamp,stock,'adj_low')
#        if(len(openPrice) != 0 and len(closePrice) != 0 and len(highPrice) != 0 and len(lowPrice) != 0):
#            # if closeprice is closer to low than openprice is to high, sell
#            if (closePrice[0]-lowPrice[0]) > (highPrice[0]-openPrice[0]):
#                order = stockInfo.OutputOrder()
#                order.symbol = stock
#                order.volume = portfolio.currStocks[stock]/2+1
#                order.task = 'sell'
#                order.orderType = 'moo'
#                order.closeType = 'fifo'
#                order.duration = self.DAY * 2
#                newOrder = order.getOutput()
#                if newOrder != None:
#                    output.append(newOrder)   
#    # return the sell orders and buy orders to the simulator to execute
#     return output     
         
     #return orders    
     #def execute ends
########NEW FILE########
__FILENAME__ = Order
import tables as pt, numpy as np
from models.OrderModel import OrderModel

class Order:
    def __init__(self, isTable):
        self.isTable = isTable
        self.orderFile = pt.openFile('OrderModel.h5', mode = "w")
        self.order = self.orderFile.createTable('/', 'order', OrderModel)
        if isTable == False:
            self.orderArray = np.array([])
    
    def addOrder(self,timestamp,task,shares,symbol,orderType,duration,closeType,limitPrice): 
        ''' 
        @param timestamp: the exact timestamp when the order was submitted
        @param task: buy, sell, short, cover
        @param shares: the number of shares to trade
        @param symbol: the symbol abbreviation of the stock
        @param orderType: they type of order (moo, moc, limit, vwap)
        @param duration: the length of time the order is valid for
        @param closeType: sell first or sell last (lifo,fifo)
        
        @summary: adds a new unfulfilled order to the orders table
        @returns a reference to the row
        '''  
        if self.isTable:
            row = self.order.row
            row['task'] = task
            row['shares'] = shares
            row['symbol'] = symbol
            row['order_type'] = orderType
            row['duration'] = duration
            row['timestamp'] = timestamp
            row['close_type'] = closeType
            row['limit_price'] = limitPrice
            return row
        else:
            return self.addOrderArray(timestamp,task,shares,symbol,orderType,duration,closeType,limitPrice = 0)
        
    def fillOrder(self, timestamp, rowIterator, quantity, price, commission, impactCost):
        ''' 
        @param timestamp: the exact timestamp when the order was fufilled
        @param rowIterator: a pytables iteratable rows object with 1 row, the row to be filled in it
        @param quantity: the number of shares successfully traded
        @param price: the purchase price per share

        @warning: CURRENTLY DONE IN THE SIMULATOR BUY/SELL METHODS THIS METHOD CURRENTLY WILL NOT FUCTION IF USED
        @summary: adds a fill to a given order
        '''  
        for row in rowIterator:
            row['fill/timestamp'] = timestamp
            row['fill/quantity'] = quantity
            row['fill/cashChange'] = price
            row['fill/commission'] = commission
            row['fill/impactCost'] = impactCost
            row.update()
    
    def getOrders(self):
        '''
        @return: Returns all of the orders
        '''
        if self.isTable:
            return self.order.iterrows()
        else:
            return self.orderArray
        
    def addOrderArray(self,timestamp,task,shares,symbol,orderType,duration,closeType,limitPrice):  
        ''' 
        @param timestamp: the exact timestamp when the order was submitted
        @param task: buy, sell, short, cover
        @param shares: the number of shares to trade
        @param symbol: the symbol abbreviation of the stock
        @param orderType: they type of order (moo, moc, limit, vwap)
        @param duration: the length of time the order is valid for
        @param closeType: sell first or sell last (lifo,fifo)
        
        @summary: adds a new unfulfilled order to the orders table and returns the order
        '''  
        row = {}
        row['task'] = task
        row['shares'] = shares
        row['symbol'] = symbol
        row['order_type'] = orderType
        row['duration'] = duration
        row['timestamp'] = timestamp
        row['close_type'] = closeType
        row['limit_price'] = limitPrice
        row['fill/timestamp'] = 0
        row['fill/quantity'] = 0
        row['fill/cashChange'] = 0
        row['fill/commission'] = 0
        row['fill/impactCost'] = 0
        self.orderArray = np.append(self.orderArray,row)
        return row
      
    def fillOrderArray(self, timestamp, row, quantity, price, commission, impactCost):
        '''
        @summary: CURRENTLY DONE IN THE SIMULATOR BUY/SELL METHODS. adds a fill to a given order
        @param timestamp: the exact timestamp when the order was fufilled
        @param row: the dictionary representing the row
        @param quantity: the number of shares successfully traded
        @param price: the purchase price per share
        '''  
        row['fill/timestamp'] = timestamp
        row['fill/quantity'] = quantity
        row['fill/cashChange'] = price
        row['fill/commission'] = commission
        row['fill/impactCost'] = impactCost
        
    def fillTable(self):
        '''
        @summary: converts all orders to HDF5 and outputs the file
        '''
        for arrRow in self.orderArray:
            row = self.order.row
            row['task'] = arrRow['task']
            row['shares'] = arrRow['shares']
            row['symbol'] = arrRow['symbol']
            row['order_type'] = arrRow['order_type']
            row['duration'] = arrRow['duration']
            row['timestamp'] = arrRow['timestamp']
            row['close_type'] = arrRow['close_type']
            row['limit_price'] = arrRow['limit_price']
            row['fill/timestamp'] = arrRow['fill/timestamp']
            row['fill/quantity'] = arrRow['fill/quantity']
            row['fill/cashChange'] = arrRow['fill/cashChange']
            row['fill/commission'] = arrRow['fill/commission']
            row['fill/impactCost'] = arrRow['fill/impactCost']
            row.append()
        self.order.flush() 
        
    def close(self):
        if self.isTable:
            self.orderFile.close()
        else:
            self.fillTable()
            self.orderFile.close()
        
########NEW FILE########
__FILENAME__ = Portfolio
import StrategyData, Simulator, tables as pt
from models.PortfolioModel import PortfolioModel
import numpy as np

class Portfolio:

    def __init__(self, cash, stocks):
        '''
        @param cash: int representing the cash on hand
        @param stocks: dictionary representing all of the stocks a user has {}
        '''
        self.currCash = float(cash)
        self.currStocks = stocks
        self.lastNonNanValue= {} #dict
        
    
    def buyTransaction(self, order):
        '''
        @param order: the order (we know is valid at this point) to execute on the portfolio
        @summary: Updates the portfolio after a stock is purchased
        '''
        # Subtract the impact cost - it cost more because you inflated the price buying so much
        # cashChange is NEGATIVE when passed in 
        self.currCash += float(-order['fill/commission'] + (order['fill/cashChange'] * order['fill/quantity']) - order['fill/impactCost'])
        print "Cash adjusted for buy txn is: " + str(self.currCash)
        if order['symbol'] in self.currStocks.keys():
            self.currStocks[order['symbol']] += order['fill/quantity']
        else:
            self.currStocks[order['symbol']] = order['fill/quantity']

        self.lastNonNanValue[order['symbol']]= order['limit_price']
        
    def sellTransaction(self,order):
        '''
        @param order: the order (we know is valid at this point) to execute on the portfolio        
        @summary: Updates the portfolio after a stock is sold
        '''
        # Subtract effect - gain less money
        # cashChange is POSITIVE when passed in 
        self.currCash += float(-order['fill/commission'] + (order['fill/cashChange'] * order['fill/quantity']) - order['fill/impactCost'])
        print "Cash adjusted for sell  txn is: " + str(self.currCash)
        if order['symbol'] in self.currStocks:
            self.currStocks[order['symbol']] -= order['fill/quantity']
            if self.currStocks[order['symbol']] == 0:
                del self.currStocks[order['symbol']]
        else:
            self.currStocks[order['symbol']] = -order['fill/quantity']
        
        self.lastNonNanValue[order['symbol']]= order['limit_price']    
    
    def hasStock(self,symbol,volume):
        '''
        @summary: Returns a boolean of whether or not the appropriate amount of the given stock exist in the portfolio.
        '''
        if not symbol in self.currStocks:
            return False
        if volume < 0:
            return self.currStocks[symbol] <= volume
        return self.currStocks[symbol] >= volume
    
    
    def calcPortfolioValue(self, timestamp, dataAccess):
        '''
        @attention: includes cash
        '''
        DAY= 86400
        
        portfolioValue= float(0.0)
        for stock in self.currStocks:
            stockPrice= dataAccess.getStockDataItem (str(stock), 'adj_close', timestamp - DAY)
            
            if (np.isnan(stockPrice)):
                portfolioValue+= float (self.lastNonNanValue[str(stock)])
            else:
                #value is not nan
                portfolioValue+= float(stockPrice)
                self.lastNonNanValue[str(stock)]= float(stockPrice)
        
        portfolioValue+= float(self.currCash)
        return portfolioValue            
        #calcPortfolioValue done        
        
    def getHeldQty(self, stock):
        if stock in self.currStocks.keys():
            return self.currStocks[stock]
        else:
            return np.NaN
        
    def getListOfStocks(self):
        '''
        @return: A list of all cureently held stocks (in arbitrary order?)
        '''
        return self.currStocks.keys()    
            
    
    def close(self):
        #no tables or HDF5 output
        pass
        
########NEW FILE########
__FILENAME__ = Position
import tables as pt
import numpy as np
from models.PositionModel import PositionModel

class Position:
    def __init__(self):
        self.position = np.array([])
    
    def getPositions(self):
        '''
        Returns all of the positions
        '''
        return self.position
    
    def addPosition(self,timestamp,symbol,shares,purchase_price):
        '''
        Adds a new position
        timestamp: the time the position was entered
        symbol: the ticker of the stock
        shares: the number of shares
        purchase_price: the price per share (excludes any additional costs such as commission or impact)
        '''
        row = {}
        row['timestamp'] = timestamp
        row['symbol'] = symbol 
        row['shares'] = shares
        row['purchase_price'] = purchase_price
        self.position = np.append(self.position, row)
    
    def removePosition(self, symbol, shares, closeType):
        '''
        Removes/modifies positions until the total number of shares have been removed
        symbol: the ticker of the stock
        shares: the number of shares to remove
        closeType: removal order "lifo" or "fifo"
        NOTE: Method assumes that verification of valid sell has already been completed
        '''
        debug = False
        
        rowIndexes = []
        rows = []
        #check for negative shares, short or not
        if shares<0:
            short = True
        else:
            short = False
        if debug:
            print 'REMOVING POSITIONS'
            print 'REMOVE:',symbol,shares,closeType
            for row in self.position:
                print 'CURRROWS:', row
        #get all rows for the correct stock
        idx = 0
        for row in self.position:
            if(row['symbol']==symbol):
                row["keyIndex"]=idx
                rows.append(row)
            idx+=1
        if debug:
            print 'POSSIBLE ROWS TO REMOVE: ',rows
        if(closeType=='fifo'):
            i = 0
            row = rows[i] #get first row
            if debug:
                print 'FIFO', row
            posShares = row['shares'] #get shares
            posShares = abs(posShares) #account for shorts (make positive) 
            #determines the number of positions to remove
            while(shares>posShares):
                shares-=posShares
                i+=1
                row = rows[i]
                posShares = row['shares']
                posShares = abs(posShares)
            #converts shorts back to negative
            if short:
                shares *= -1
                posShares *= -1
            #change shares in the last (changed) row
            newRow = self.position[ rows[i]['keyIndex'] ]
            newShares = posShares-shares
            newRow['shares'] = newShares
            if debug:
                print 'UPDATEDROW(FIFO):', newRow            
            #removes old rows
            removes = []
            #remove updated row if it has 0 shares now
            if newShares == 0:
                removes.append(rows[i]['keyIndex'])
            #remove the rest of the rows
            cnt = 0
            while cnt<i:
                row = rows[cnt]
                removes.append(row['keyIndex'])
                cnt+=1
            if debug:
                for idx in removes:
                    print 'ROWREMOVED:', self.position[idx]
            self.position = np.delete(self.position,removes)
            #remove the keyIndex field from the data
            for row in rows:
               del row['keyIndex'] 
               
                    
        elif(closeType=='lifo'):
            i = len(rows)-1 #sets i to last row
            row = rows[i]
            if debug:
                print "LIFO",row
            #gets info from last row's position
            posShares = row['shares']
            posShares = abs(posShares) #account for shorts (make positive) 
            #determines number of positions to remove     
            while(shares>posShares):
                shares-=posShares
                i-=1
                row = rows[i]
                posShares = row['shares']
                posShares = abs(posShares)
            #converts shorts back to negative
            if short:
                shares *= -1
                posShares *= -1 
            #modifies changed row             
            newRow = self.position[ rows[i]['keyIndex'] ]
            newShares = posShares-shares
            newRow['shares'] = newShares
            if debug:
                print 'UPDATEDROW(LIFO):', newRow            
            #removes old rows
            removes = []
            #remove updated row if it has 0 shares now
            if newShares == 0:
                removes.append(rows[i]['keyIndex'])
            #remove the rest of the rows
            cnt = len(rows)-1
            while cnt>i:
                row = rows[cnt]
                removes.append(row['keyIndex'])
                cnt-=1
            if debug:
                for idx in removes:
                    print 'ROWREMOVED:', self.position[idx]
            self.position = np.delete(self.position,removes)
            for row in rows:
               del row['keyIndex']
        else:
            #invalid type
            raise TypeError("Not an existing close type '%s'." % str(closeType))
    
    def fillTable(self):
        '''
        Converts the arrays into HDF5 tables for post simulation review
        '''
        self.positionFile = pt.openFile('PositionModel.h5', mode = "w")
        self.position = self.positionFile.createTable('/', 'position', PositionModel)     
        for arrRow in self.position:
            row = self.position.row
            row['timestamp'] = arrRow['timestamp']
            row['symbol'] = arrRow['symbol'] 
            row['shares'] = arrRow['shares']
            row['purchase_price'] = arrRow['purchase_price']
            row.append()
        self.position.flush() 
        self.positionFile.close()
    
    def close(self):
        self.fillTable()


########NEW FILE########
__FILENAME__ = Simulator
#import optimizers.BollingerOptimizer as Optimizer
import optimizers.BollingerOptimizer as Optimizer
import models.PortfolioModel, models.PositionModel, models.OrderModel, models.StrategyDataModel
import tables as pt, numpy as np
from optparse import OptionParser
import sys, time
import Portfolio, Position, Order, DataAccess as da , StrategyData
import os
import dircache
import numpy as np
#import curveFittingOptimizer
#import optimizers.BollingerOptimizer as Optimizer

from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from matplotlib.figure import Figure



class Simulator():
    def __init__(self, cash, stocks, startTime, endTime, interval, minCom, comPerShare, isTable, maxEffect, arrayFile, listOfStocksFile):
        # strategy contains a reference to the strategy method specified in the command line
#        self.strategy = strategy
        # startTime/endTime are the timestamps marking the beginning and end of the time for which the simulation should run
        self.startTime = startTime
        self.currTimestamp = startTime
        self.endTime = endTime
        # interval is the amount of time between iterations of the strategy
        self.interval = interval
        # minCom is the minimum commission per transaction
        self.minCom = minCom
        # comPerShare is the calculated commission per share--if this is greater than the minimum commission, this is what gets used
        self.comPerShare = comPerShare
        # timeStampIndex and currDataTimeIndex are markers to track the current position in the list of timestamps
        self.timeStampIndex = 0
        self.currDataTimeIndex = 0
        # maxEffect is the maximum percentage change in price a single transaction can have on the actual market price
        self.maxEffect = maxEffect
        # times becomes the list of timestamps
        self.times =  []
        # isTable tells the simulator whether to use the table- or array-specific methods
        self.isTable = isTable
        
        #starting portfolio, position, and order initializations
        self.portfolio = Portfolio.Portfolio(cash, stocks)
        self.position = Position.Position()
        self.order = Order.Order(self.isTable)
        #populate the strategyData with the relevant type of data storage
        if isTable:
            
            
#            self.h5f= pt.openFile(pytablesFile, mode = "a") # if mode ='w' is used here then the file gets overwritten!

            listOfPaths=list()
            #listOfPaths.append("C:\\generated data files\\one stock per file\\maintain folder structure\\US_NASDAQ\\")
            #listOfPaths.append("C:\\temp\\")
            #listOfPaths.append("C:\\tempoutput\\")
            listOfPaths.append("/hzr71/research/QSData/tempdata/") #Modification for gekko
            self.listOfStocks= self.getStocks(listOfStocksFile, listOfPaths)
            
            self.dataAccess= da.DataAccess (True, listOfPaths, "/StrategyData", "StrategyData", True, self.listOfStocks, self.startTime, self.endTime)
            self.strategyData = StrategyData.StrategyData("someRandomStringToNotBreakTheCode", self.dataAccess, self.isTable)
            
#        else:
#            self.strategyData = StrategyData.StrategyData(arrayFile,self.isTable) 

    def getStocks(self, pathToFile, listOfPaths):
        
        listOfStocks=list()
        if (os.path.exists(pathToFile)):
           print "Reading in stock names from file..." 
           f= open(pathToFile)
           lines= f.readlines()
           f.close()
           for line1 in lines:
             listOfStocks.append(line1.partition("\n")[0])
             #for done
        else:
            #Path does not exist
            print "Reading in all stock names..."
            fileExtensionToRemove=".h5"   
            
            for path in listOfPaths:
               stocksAtThisPath=list ()
               
               stocksAtThisPath= dircache.listdir(str(path))
               #Next, throw away everything that is not a .h5 And these are our stocks!
               stocksAtThisPath = filter (lambda x:(str(x).find(str(fileExtensionToRemove)) > -1), stocksAtThisPath)
               #Now, we remove the .h5 to get the name of the stock
               stocksAtThisPath = map(lambda x:(x.partition(str(fileExtensionToRemove))[0]),stocksAtThisPath)
               
               for stock in stocksAtThisPath:
                   listOfStocks.append(stock)
        return listOfStocks
    #readStocksFromFile done
    
    
    
    def addTimeStamps(self):
        # generates the list of timestamps
        global timersActive
        temp = []

        if timersActive:
            print 'Generating valid timestamps'
            cnt = 0
            cycTime = time.time()
#        for i in self.strategyData.strategyData.iterrows():
        for ts in self.dataAccess.getTimestampArray():    
            if ts not in temp:
                temp.append(ts)
            if timersActive:
                if(cnt%1000000==0):
                    print '%i rows finished: %i secs elapsed'%(cnt,time.time()-cycTime)
                cnt+=1
        if timersActive:
            print 'all rows added: %i secs elapsed'%(time.time()-cycTime)      
        #Put the list in order, convert it to a NumPy array  
        temp.sort()
        temp = np.array(temp)
        return temp
    
    def calcCommission(self, volume):
        '''
        @summary: returns the commission on a given trade given the volume 
        '''
        return max(minCom,volume * self.comPerShare)
    
    def getCurrentDataTimestamp(self):
        '''
        @summary: returns the timestamp of the most recent data available
        '''
        while self.times[self.currDataTimeIndex+1]<self.currTimestamp:
            self.currDataTimeIndex += 1
        return self.times[self.currDataTimeIndex]
    
    def getExecutionTimestamp(self):
        '''
        @summary: returns the timestamp of the current execution timestamp
        @attention: Orders placed on the last day can not be executed after it..so they will be executed on that day. Possible bug?
        '''
        while self.times[self.timeStampIndex]<self.currTimestamp:
            self.timeStampIndex += 1
        
        if (self.timeStampIndex+1 < len (self.times)):
             idealTime = self.times[self.timeStampIndex+1]
        else:
             idealTime = self.times[self.timeStampIndex] 
        
        return idealTime
        
    def calcEffect(self, maxVol, shares):
        # calculates the effect in the market of a given trade
        return float(shares)/maxVol * self.maxEffect
        
#    def getVolumePerDay(self, symbol, timestamp):
#        '''
#        COMMENT BY SHREYAS JOSHI. THIS FUNCTION IS NOT NECESSARY. 22 JUN 2010. HENCE REMOVING IT.
#        @summary: returns the volume of a given stock for the given day (used in conjunction with calcEffect). Call with startTime = endTime = desired timestamp to get just that timestamp
#        '''  
#
#
##        stocks = self.strategyData.getStocks(timestamp, timestamp+1, symbol)
#        stocks= self.dataAccess.getData(symbol, 'volume', timestamp, timestamp+1) # we need only the volume here
#        if len(stocks) > 0:
#            myStockasDict = stocks[0] #Grab the first dictionary in the list
#            return myStockasDict['volume'] # Get the volume
#        return None   
            
    def buyStock(self, newOrder):
        '''
        @summary: function takes in an instance of OrderDetails, executes the changes to the portfolio and adds the order to the order table
        @param newOrder: an instance of OrderDetails representing the new order
        @warning: The Order should not be added to the order table before calling this function
        '''
        ts = self.getCurrentDataTimestamp()
        maxVol4Day = self.dataAccess.getStockDataItem(newOrder['symbol'], 'volume', ts)#self.getVolumePerDay(newOrder['symbol'], ts)
        if newOrder['order_type'] == 'moo':
            #market order open
#            price = strategyData.getPrice(ts, newOrder['symbol'], 'adj_open')
            price = self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_open', ts) 
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for ts:",ts,'stock:',newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']
                return None
            else:
                print "Checking cash..."
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                # New is cost the original total price (price * shares) + effect*Total Price
                # Basically, you raise the cost as you buy
                cost = (checkAmount * price[0]['adj_open'] + (checkAmount * price[0]['adj_open'] * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash):
                    #Not enough cash to buy stock
                    print "Not enough cash to buy stock."
                    #print "Apparently not enough cash. I don't believe this. Current cash: " + str (self.portfolio.currCash) + " total cost: "+ str (cost)+ ", cost of one share: "+str (self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_open', ts))
                    
                    return None
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                #__execute trade__
                #populate fill field in order
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'BUY') else -newOrder['shares']
                newOrder['fill/cashChange'] = -price
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.buyTransaction(newOrder)
                #add position
                self.position.addPosition(ts,newOrder['symbol'],newOrder['fill/quantity'],price)
        elif newOrder['order_type'] == 'moc':
            #market order close
#            price = self.strategyData.getPrice(ts, newOrder['symbol'], 'adj_close')
#            price = self.dataAccess.getData(newOrder['symbol'], 'adj_close', ts, ts)[0]['adj_close']
            
            
            price = self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_close', ts)
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for ts:",ts,'stock:',newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']
                return None
            else: 
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                # New is cost the original total price (price * shares) + effect*Total Price
                # Basically, you raise the cost as you buy
#                cost = (checkAmount  + (checkAmount  * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                cost = (checkAmount * price + (checkAmount * price * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash):
                    #Not enough cash to buy stock
                    print "Not enough cash. Current cash: " + str (self.portfolio.currCash) + " total cost: "+ str (cost)+ ", cost of one share: "+str (self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_close', ts))
                    
                    return None
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'BUY') else -newOrder['shares']
                newOrder['fill/cashChange'] = -price
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.buyTransaction(newOrder)
                #add position
                self.position.addPosition(ts,newOrder['symbol'],newOrder['fill/quantity'],price)
        elif newOrder['order_type'] == 'limit':
            #limit order
            price = newOrder['limit_price']
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for ts:",ts,'stock:',newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']
                return None
            else:
#                if ((newOrder['limit_price'] > self.strategyData.getPrice(ts, newOrder['symbol'], 'adj_high')) or ( newOrder['limit_price'] < self.strategyData.getPrice(ts, newOrder['symbol'], 'adj_low'))):
                if ((newOrder['limit_price'] > self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_high', ts)) or ( newOrder['limit_price'] < self.dataAccess.getData(newOrder['symbol'], 'adj_low', ts))):   
                    #limit price outside of daily range
                    return None
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                # New is cost the original total price (price * shares) + effect*Total Price
                # Basically, you raise the cost as you buy
                cost = (checkAmount * price + (checkAmount * price * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash):
                    #Not enough cash to buy stock
                    return None
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                #__execute trade__
                #populate fill field in order
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'BUY') else -newOrder['shares']
                newOrder['fill/cashChange'] = -price
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.buyTransaction(newOrder)
                #add position
                self.position.addPosition(ts,newOrder['symbol'],newOrder['fill/quantity'],price)
        elif newOrder['order_type'] == 'vwap':
            #volume weighted average price
#            price = strategyData.getPrice(ts, newOrder['symbol'], 'adj_open')
#            price = self.dataAccess.getData(newOrder['symbol'], 'adj_open', ts, ts)[0]['adj_close']
            price = self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_open', ts)
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for ts:",ts,'stock:',newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']
                return None
            else:
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                # New is cost the original total price (price * shares) + effect*Total Price
                # Basically, you raise the cost as you buy
                price += self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_close', ts)#[0]['adj_close'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_close')
                price += self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_high', ts)#[0]['adj_high'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_high')
                price += self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_low', ts)#[0]['adj_low'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_low')
                price = price / 4.
                cost = (checkAmount * price + (checkAmount * price * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash):
                    #Not enough cash to buy stock
                    return None
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                # New is cost the original total price (price * shares) + effect*Total Price
                # Basically, you raise the cost the more you buy.
                #__execute trade__
                #populate fill field in order
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'BUY') else -newOrder['shares']
                newOrder['fill/cashChange'] = -price
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.buyTransaction(newOrder) 
                #add position
                self.position.addPosition(ts,newOrder['symbol'],newOrder['fill/quantity'],price)
        else:
            #throw invalid type error
            raise TypeError("Not an existing trade type '%s'." % str(newOrder['order_type']))
        newOrder.update()
        self.order.order.flush()
        return price
    
    def sellStock(self,newOrder):
        '''
        @summary: function takes in an instance of OrderDetails, executes the changes to the portfolio and adds the order to the order table
        @param newOrder: an instance of OrderDetails representing the new order
        @warning: The Order should not be added to the order table before calling this function
        '''
        ts = self.getCurrentDataTimestamp() #need a function to get the next available time we can trade
        maxVol4Day = self.dataAccess.getStockDataItem(newOrder['symbol'], 'volume', ts)#self.getVolumePerDay(newOrder['symbol'], ts)    
        if newOrder['order_type'] == 'moo':
            #market order open
            price = self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_open', ts)#[0]['adj_open'] #self.strategyData.getPrice(ts, newOrder['symbol'], 'adj_open')
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for",ts,newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']
                return None
            else:
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                if newOrder['task'].upper() == 'SELL':
                    if not (self.portfolio.hasStock(newOrder['symbol'],checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        print "Not enough shares owned to sell the requested amount"
                        return None
                else:
                    if not (self.portfolio.hasStock(newOrder['symbol'],-checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        print "Not enough shares owned to sell the requested amount"
                        return None
                cost = (checkAmount * price + (checkAmount * price * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash) and (newOrder['shares'] < 0):
                    #Not enough cash to cover stock
                    print "Not enough cash to cover stock"
                    return None
                #__execute trade__
                #populate fill field in order
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares']
                newOrder['fill/cashChange'] = price #NEW
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.sellTransaction(newOrder)
                #remove positions according to lifo/fifo
                self.position.removePosition(newOrder['symbol'],newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares'],newOrder['close_type'])
        elif newOrder['order_type'] == 'moc':
            #market order close
            price = self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_close', ts)#[0]['adj_close'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_close')
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for",ts,newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']        
                return None
            else:
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                if newOrder['shares'] > 0:
                    if not (self.portfolio.hasStock(newOrder['symbol'],checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        print "Not enough shares owned to sell the requested amount"
                        return None
                else:
                    if not (self.portfolio.hasStock(newOrder['symbol'],-checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        print "Not enough shares owned to sell the requested amount"
                        return None
                cost = (checkAmount * price + (checkAmount * price * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash) and (newOrder['shares'] < 0):
                    #Not enough cash to cover stock
                    print "Not enough cash to cover stock"
                    return None
                #__execute trade__
                #populate fill field in order
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares']
                newOrder['fill/cashChange'] = price
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.sellTransaction(newOrder)
                #remove positions according to lifo/fifo
                self.position.removePosition(newOrder['symbol'],newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares'],newOrder['close_type'])            
        elif newOrder['order_type'] == 'limit':
            #limit order
            price = newOrder['limit_price']
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for",ts,newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']
                return None
            else:
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                if newOrder['shares'] > 0:
                    if not (self.portfolio.hasStock(newOrder['symbol'],checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        print "Not enough shares owned to sell the requested amount"
                        return None
                else:
                    if not (self.portfolio.hasStock(newOrder['symbol'],-checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        return None
                cost = (checkAmount * price + (checkAmount * price * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash) and (newOrder['shares'] < 0):
                    #Not enough cash to cover stock
                    print "Not enough cash to cover stock"
                    return None
                #__execute trade__
                #populate fill field in order
#                if ((newOrder['limit_price'] > strategyData.getPrice(ts, newOrder['symbol'], 'adj_high')) or ( newOrder['limit_price'] < strategyData.getPrice(ts, newOrder['symbol'], 'adj_low'))):
                if ((newOrder['limit_price'] > self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_high', ts)) or ( newOrder['limit_price'] < self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_low', ts))):    
                    #limit price outside of daily range
                    return None
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                #__execute trade__
                #populate fill field in order
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares']
                newOrder['fill/cashChange'] = price
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.sellTransaction(newOrder)
                #remove positions according to lifo/fifo
                self.position.removePosition(newOrder['symbol'],newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares'],newOrder['close_type'])
        elif newOrder.order_type == 'vwap':
            #volume weighted average price
            price = self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_open', ts)#[0]['adj_open'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_open')
            if price == None or np.isnan (price):
                if noisy:
                    print "Price data unavailable for",ts,newOrder['symbol']
                return None
            elif maxVol4Day == None or np.isnan(maxVol4Day):
                if noisy:
                    print "Volume Data Not Available for ts:", ts, 'stock:', newOrder['symbol']
                return None
            else:
                checkAmount = min(abs(newOrder['shares']),maxVol4Day)
                if newOrder['shares'] > 0:
                    if not (self.portfolio.hasStock(newOrder['symbol'],checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        print "Not enough shares owned to sell the requested amount"
                        return None
                else:
                    if not (self.portfolio.hasStock(newOrder['symbol'],-checkAmount)): # NEW
                        #Not enough shares owned to sell requested amount
                        print "Not enough shares owned to sell the requested amount"
                        return None
                price += self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_close', ts)#[0]['adj_close'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_close')
                price += self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_high', ts)#[0]['adj_high'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_high')
                price += self.dataAccess.getStockDataItem(newOrder['symbol'], 'adj_low', ts)#[0]['adj_low'] #strategyData.getPrice(ts, newOrder['symbol'], 'adj_low')
                price = price / 4.
                cost = (checkAmount * price + (checkAmount * price * self.calcEffect(maxVol4Day, checkAmount))) + self.calcCommission(checkAmount)
                if(cost>self.portfolio.currCash) and (newOrder['shares'] < 0):
                    #Not enough cash to cover stock
                    print "Not enough cash to cover stock"
                    return None
                #__execute trade__
                #populate fill field in order
                if abs(newOrder['shares']) > maxVol4Day:
                    if newOrder['shares'] < 0:
                        newOrder['shares'] = -maxVol4Day
                    else:
                        newOrder['shares'] = maxVol4Day
                    newOrder.update()
                    self.order.order.flush()
                newOrder['fill/timestamp'] = ts
                newOrder['fill/quantity'] = newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares']
                newOrder['fill/cashChange'] = price
                newOrder['fill/commission'] = self.calcCommission(newOrder['shares'])
                newOrder['fill/impactCost'] = newOrder['shares'] * price * self.calcEffect(maxVol4Day, newOrder['shares']) # This is the CHANGE in the total cost - what effect the volume has
                #add trade to portfolio
                self.portfolio.sellTransaction(newOrder)
                #remove positions according to lifo/fifo
                self.position.removePosition(newOrder['symbol'],newOrder['shares'] if (newOrder['task'].upper() == 'SELL') else -newOrder['shares'],newOrder['close_type'])            
        else:
            #throw invalid type error
            raise TypeError("Not an existing trade type '%s'." % str(newOrder.order_type))
        newOrder.update()
        self.order.order.flush()
        return price
            
    def execute(self):
        '''
        @summary: This function iterates through the orders and attempts to execute all the ones that are still valid and unfilled
        '''
        count = 0
        for order in self.order.getOrders():
            if (order['timestamp'] < self.currTimestamp):
                if (order['duration'] + order['timestamp']) >= self.currTimestamp:
                    if order['fill/timestamp'] == 0:
                        #Have unfilled, valid orders
                        if order['task'].upper() == "BUY":
                            #is a buy
                            if self.portfolio.hasStock(order['symbol'],1):
                                if order['shares']>0:
                                    result = self.buyStock(order)
                                    if noisy:
                                        if result is not None:
                                            print "Succeeded in buying %d shares of %s for %.2f as %s, with close type %s. Placed at: %d.  Current timestamp: %d, order #%d" % (order['shares'], order['symbol'], result, order['order_type'], order['close_type'], order['timestamp'], self.currTimestamp, count)
                                        #else:
                                            #print "THIS IS MOST LIKELY WRONG- Did not succeed in buying %d shares of %s as %s; not enough cash.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                                else:
                                    if noisy:
                                        print "Did not succeed in buying %d shares of %s as %s; negative values are not valid buy amounts.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                            elif self.portfolio.hasStock(order['symbol'],-1):
                                if noisy:
                                    print "Did not succeed in buying %d shares of %s as %s; you must cover your shortsell before you can buy.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                            else:
                                result = self.buyStock(order)
                                if noisy:
                                    if result:
                                        print "Succeeded in buying %d shares of %s for %.2f as %s. Placed at: %d.  Current timestamp: %d, order #%d" % (order['shares'], order['symbol'], result, order['order_type'], order['timestamp'], self.currTimestamp, count)
                                    else:
                                        print "Did not succeed in buying %d shares of %s as %s.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                        elif order['task'].upper() == "SELL":
                            # is a sell
                            if order['shares']>0:
                                result = self.sellStock(order)
                                if noisy:
                                    if result:
                                        print "Succeeded in selling %d shares of %s for %.2f as %s, with close type %s.  Current timestamp: %d" % (order['shares'], order['symbol'], result, order['order_type'], order['close_type'], self.currTimestamp)
                                    #else:
                                        #print "Did not succeed in selling %d shares of %s as %s; not enough owned.  Order valid until %d.  Current timestamp: %d" %(order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], self.currTimestamp)
                            else:
                                if noisy:
                                    print "Did not succeed in selling %d shares of %s as %s; you cannot sell a non-positive amount.  Order valid until %d.  Current timestamp: %d" %(order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], self.currTimestamp)
                        elif order['task'].upper() == "SHORT":
                            #is a short sell
                            if self.portfolio.hasStock(order['symbol'],-1):
                                if order['shares']>0:
                                    result = self.buyStock(order)
                                    if noisy:
                                        if result:
                                            print "Succeeded in short selling %d shares of %s for %.2f as %s, with close type %s. Placed at: %d.  Current timestamp: %d, order #%d" % (-order['shares'], order['symbol'], -result, order['order_type'], order['close_type'], order['timestamp'], self.currTimestamp, count)
                                        else:
                                            print "Did not succeed in short selling %d shares of %s as %s; not enough cash???  How do you not have enough cash for a short sell?.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                                else:
                                    if noisy:
                                        print "Did not succeed in short selling %d shares of %s as %s; negative values are not valid short sell amounts.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(-order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                            elif self.portfolio.hasStock(order['symbol'],1):
                                if noisy:
                                    print "Did not succeed in short selling %d shares of %s as %s; you cannot short sell a stock you already own.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(-order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                            else:
                                result = self.buyStock(order)
                                if noisy:
                                    if result:
                                        print "Succeeded in short selling %d shares of %s for %.2f as %s, with close type %s. Placed at: %d.  Current timestamp: %d, order #%d" % (-order['shares'], order['symbol'], result, order['order_type'], order['close_type'], order['timestamp'], self.currTimestamp, count)
                                    else:
                                        print "Did not succeed in short selling %d shares of %s as %s; not enough cash???  How do you not have enough cash for a short sell?.  Order valid until %d. Placed at: %d.  Current timestamp: %d, order #%d" %(-order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], order['timestamp'], self.currTimestamp, count)
                        elif order['task'].upper() == "COVER":
                            # is a cover
                            if order['shares']>0:
                                result = self.sellStock(order)
                                if noisy:
                                    if result:
                                        print "Succeeded in covering %d shares of %s for %.2f as %s, with close type %s.  Current timestamp: %d" % (-order['shares'], order['symbol'], result, order['order_type'], order['close_type'], self.currTimestamp)
                                    else:
                                        print "Did not succeed in covering %d shares of %s as %s; not short enough or not enough cash.  Order valid until %d.  Current timestamp: %d" %(-order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], self.currTimestamp)
                            else:
                                if noisy:
                                    print "Did not succeed in covering %d shares of %s as %s; you cannot cover a non-positive amount.  Order valid until %d.  Current timestamp: %d" %(-order['shares'], order['symbol'], order['order_type'], order['duration'] + order['timestamp'], self.currTimestamp)
                        else:
                            if noisy:
                                print "'%s' is not a valid task.  Order valid until %d.  Current timestamp: %d" % (order['task'].upper(), order['duration'] + order['timestamp'], self.currTimestamp)
            count += 1
        
        
    def addOrders(self,commands):
        '''
        @summary: takes in commands (return value of strategy), parses it, and adds it in the correct format to the order data storage
        '''
        if self.isTable:
            for stock in commands:
                newOrder = self.order.addOrder(self.getExecutionTimestamp(),stock[0],stock[1],stock[2],stock[3],stock[4],stock[5],stock[6])
                newOrder.append()
                self.order.order.flush()
        else:
            for stock in commands:
                self.order.addOrder(self.getExecutionTimestamp(),stock[0],stock[1],stock[2],stock[3],stock[4],stock[5],stock[6])
                
    def run(self):
        '''
        @summary: Run the simulation
        '''
        
        optimizer= Optimizer.Optimizer(self.listOfStocks)
        #optimizer= curveFittingOptimizer.Optimizer(self.listOfStocks)
        timestamps= list(self.dataAccess.getTimestampArray())
        portfolioValList= list()
      
        ctr=0
        while (timestamps[ctr]< self.startTime):
            ctr+=1
            #while loop done
            
               
        self.currTimestamp = timestamps[ctr] #self.startTime
        
        ctr2= ctr
        
        while (timestamps[ctr2]< self.endTime):
            ctr2+=1
            if (ctr2>= len(timestamps)):
                break
            
            #while loop done
            
        if (ctr2>= len (timestamps)):
            self.endTime= timestamps[ctr2-1]
        else:
            self.endTime= timestamps[ctr2]    
        
        if timersActive:
            print "Simulation timer started at "+ str(self.currTimestamp)
            totalTime = time.time()
            cycTime = time.clock()
            
#        self.strategyData.currTimestamp = self.currTimestamp
        i=1
        while self.currTimestamp < self.endTime and self.currTimestamp < time.time(): # and self.currTimestamp < self.strategyData.timestampIndex[len(self.strategyData.timestampIndex)-2]: ************POSSIBLE BUG***** JUST TRYING OUT
            # While not yet reached the end timestamp AND not yet caught up to present AND not yet reached the end of the data
            # execute the existing orders, then run the strategy and add the new orders
            
            
            
            beforeExec=time.clock()
            self.execute()
            afterExec= time.clock()
#            self.addOrders(self.strategy(self.portfolio,self.position,self.currTimestamp,self.strategyData))
#            self.addOrders(optimizer.execute(self.portfolio,self.position,self.currTimestamp,self.strategyData))
            beforeAddOrders= time.clock()
            self.addOrders(optimizer.execute(self.portfolio,self.position,self.currTimestamp,self.strategyData, self.dataAccess))
            afterAddOrders= time.clock()
            
            if noisy or timersActive:
                print '' #newline                
            if mtm:
                #portValue = self.portfolio.currCash + self.strategyData.calculatePortValue(self.portfolio.currStocks,self.currTimestamp)
                portValue= float (0.0)
                print "| %i %.2f |"%(self.currTimestamp,portValue) + "  Value from portfolio class: " +  str (self.portfolio.calcPortfolioValue(self.currTimestamp, self.dataAccess))
            if timersActive and not noisy:
                print "Strategy at %i took %.4f secs"%(self.currTimestamp,(time.clock()-cycTime))
                i+=1
                cycTime = time.clock()
            if noisy and not timersActive:
                portValue = (self.portfolio.calcPortfolioValue(self.currTimestamp, self.dataAccess)) #self.portfolio.currCash + self.strategyData.calculatePortValue(self.portfolio.currStocks,self.currTimestamp)
                portfolioValList.append(portValue)
                
                print "Strategy at %d completed successfully." % self.currTimestamp
                print "Current cash: " + str(self.portfolio.currCash)
                print "Current stocks: %s."%self.portfolio.currStocks
                print "Current portfolio value: "+ str(portValue)+"\n\n"
                #print "Current portfolio value: %.2f.\n\n"%(portValue)
            if noisy and timersActive:
                portValue =  float (self.portfolio.calcPortfolioValue(self.currTimestamp, self.dataAccess)) #self.portfolio.currCash + self.strategyData.calculatePortValue(self.portfolio.currStocks,self.currTimestamp)
                portfolioValList.append(portValue)
                
                print "Strategy at %i took %.4f secs"%(self.currTimestamp,(time.clock()-cycTime))
                print "Exec function took: " + str(afterExec - beforeExec)
                print "Time for addorders: " + str(afterAddOrders - beforeAddOrders)
                
                print "Strategy at %d completed successfully." % self.currTimestamp
                #print "Current cash: %.2f."%(self.portfolio.currCash)
                print "Current cash: " + str(self.portfolio.currCash)
                print "Current stocks: %s."%self.portfolio.currStocks
                #print "Current portfolio value: %.2f.\n\n"%(portValue)
                print "Current portfolio value: "+ str(portValue)+"\n\n"
                i+=1
                cycTime = time.clock() 

 
                        
            #self.currTimestamp += self.interval   -- Unfortunately this does not work becuase of daylight saving time complications
            ctr+=1
            self.currTimestamp= timestamps[ctr] 
            #self.strategyData.currTimestamp = self.currTimestamp
        if noisy:
            print "Simulation complete."
        if timersActive:
            print "Simulation complete in %i seconds."%(time.time() - totalTime)
        
        self.portfolio.close()
        self.position.close()
        self.order.close()
        #self.strategyData.close()
        
        
        #plotting the portfolio value
        fig = Figure()
        canvas = FigureCanvas(fig)
        ax = fig.add_subplot(111)
        ax.plot (portfolioValList)
        ax.set_title('Portfolio value')
        ax.grid(True)
        ax.set_xlabel('time')
        ax.set_ylabel('$')
        canvas.print_figure('portfolio')
        
        
        
        #def run ends

      
cash = 0; comPerShare = 0.0; minCom = 0.; startTime = 0; endTime = 0; timeStep = 0; maxEffect = 0.; decayCycles = 0
noisy = False; timersActive = False; mtm = False; isTable = False; arrayFile = 'datafiles/defaultArrayFile.pk'; listOfStocksFile="someRandomString"
def main():
    global cash,comPerShare,minCom,startTime,endTime,timeStep,maxEffect,decayCycles,noisy,timersActive,mtm,isTable,arrayFile,listOfStocksFile
    # NOTE: the OptionParser class is currently not necessary, as we can just access sys.argv[1:], but if we
    # want to implement optional arguments, this will make it considerably easier.
    parser = OptionParser()
    
    # parser.parse_args() returns a tuple of (options, args)
    # As of right now, we don't have any options for our program, so we only care about the three arguments:
    # config file, strategy module name, strategy main function name
    args = parser.parse_args()[1]
    
#    if len(args) != 3 and len(args) != 2:
#        print "FAILURE TO INCLUDE THE CORRECT NUMBER OF ARGUMENTS; TERMINATING."
#        return
    if len(args) != 1:
        print "FAILURE TO INCLUDE THE CORRECT NUMBER OF ARGUMENTS; TERMINATING."
        return


    configFile = 'configfiles/'+args[0]
#    if len(args) == 3:
#        stratName = args[2]
#    else:
#        stratName = "strategyMain"
    if noisy:
        print "About to parse configuration files.  Any invalid fields found in the user-specified file will use the relevant value from the default file instead."
    for fileName in ["configfiles/default.ini",configFile]:
        if noisy:
            print "Parsing %s now..." % filename[12:]
        thisFile = open(fileName,'r')
        for line in thisFile.readlines():
            # Separate the command in the config file from the arguments
            if not ('#' in line):
                line = line.strip().split('=')
                command = line[0].strip().upper()
                if(command == 'ARRAYFILE' or command =='PYTABLESFILE'):
                    if len(line)>1:
                        vals = line[1].split()
                    else:
                        vals = []  
                else:
                    if len(line)>1:
                        vals = line[1].upper().split()
                    else:
                        vals = []  
                # Parse commands, look for correct number of arguments, do rudimentary error checking, apply to simulator as appropriate
                if command == 'CASH':
                    if len(vals) != 1:
                        print "WRONG NUMBER OF ARGUMENTS FOR CASH!"
                    else:
                        try:
                            cash = float(vals[0])
                        except ValueError:
                            print "ARGUMENT FOR CASH IS NOT A FLOAT!"
                
                # Code for handling stocks in a starting portfolio.  Implementation not correct; removing for the time being.
#                elif command == "STOCK":
#                    if len(vals) != 2:
#                        print "WRONG NUMBER OF ARGUMENTS FOR STOCK!!  RAAAAWR!  ALSO, I NEED TO LEARN TO THROW ERRORS!"
#                    else:
#                        try:
#                            stocks.append([vals[0],int(vals[1])])
#                        except:
#                            print "STOCK TAKES IN A STOCK NAME AND AN INT!  AND DON'T YOU FORGET IT!"
                elif command == "COMPERSHARE":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR COMMISSIONS PER SHARE."
                    else:
                        try:
                            comPerShare = float(vals[0])
                        except ValueError:
                            print "COMMISSIONS PER SHARE REQUIRES A FLOAT INPUT"
                elif command == "MINCOM":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR MINIMUM COMMISSION."
                    else:
                        try:
                            minCom = float(vals[0])
                        except ValueError:
                            print "MINIMUM COMMISSIONS REQUIRES A FLOAT INPUT"
                elif command == "STARTTIME":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR START TIME."
                    else:
                        try:
                            startTime = long(vals[0])
                        except ValueError:
                            print "START TIME REQUIRES A LONG INPUT"
                elif command == "ENDTIME":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR END TIME."
                    else:
                        try:
                            endTime = long(vals[0])
                        except ValueError:
                            print "END TIME REQUIRES A LONG INPUT"
                elif command == "TIMESTEP":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR TIME STEP."
                    else:
                        try:
                            timeStep = long(vals[0])
                        except ValueError:
                            print "TIME STEP REQUIRES A LONG INPUT"
                elif command == "MAXMARKETEFFECT":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR MAX MARKET EFFECT."
                    else:
                        try:
                            maxEffect = float(vals[0])
                        except ValueError:
                            print "MAX MARKET EFFECT REQUIRES A FLOAT INPUT"
                elif command == "DECAYCYCLES":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR DECAY CYCLES."
                    else:
                        try:
                            decayCycles = int(vals[0])
                        except ValueError:
                            print "DECAY CYCLES REQUIRES AN INTEGER INPUT"
                elif command == "DATATYPE":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR DATATYPE."
                    else:
                        if vals[0] == "TABLE":
                            isTable = True
                        elif vals[0] == "ARRAY":
                            isTable = False
                        else:
                            print "%s IS NOT A VALID PARAMETER FOR DATATYPE." % vals[0]  
                elif command == "ARRAYFILE":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR ARRAYFILE."
                    else:
                        try:
                            arrayFile = str(vals[0])
                        except ValueError:
                            print "ARRAYFILE REQUIRES A STRING INPUT"
                elif command == "PYTABLESFILE":
                    if len(vals) != 1:
                        print "NEED EXACTLY ONE PARAMETER FOR PYTABLESFILE."
                    else:
                        try:
                            pytablesFile = str(vals[0])
                        except ValueError:
                            print "PYTABLESFILE REQUIRES A STRING INPUT"
                elif command == "NOISY":
                    noisy = True
                elif command == "TIMER":
                    timersActive = True
                elif command == "MTM":
                    mtm = True
                elif command == "LISTOFSTOCKSFILE":
                    listOfStocksFile= str (vals[0])
                    if not (os.path.exists(listOfStocksFile)):
                       print "File containing list of stocks does not exist. Will read in all files at specified paths."
#                       raise ValueError
                   
                elif command != '':
                        print "Unrecognized command '%s'." % command
        thisFile.close()
    if noisy:
        print "Config files finished parsing.  Starting simulation."
    
    
    # Add the strategies subdirectory to the system path so Python can find the module
    sys.path.append(sys.path[0] + '/strategies')
#    myStrategy = eval("__import__('%s').%s" % (args[1],stratName) )
    mySim = Simulator(cash,{}, startTime, endTime, timeStep, minCom, comPerShare, isTable, maxEffect, arrayFile, listOfStocksFile)
    # Add the timestamps
    if isTable:
        mySim.times = mySim.addTimeStamps()
        #mySim.strategyData.timestampIndex = mySim.times
    else:
        pass
        #mySim.times = mySim.strategyData.timestampIndex
    mySim.run()

# This ensures the main function runs automatically when the program is run from the command line, but 
# not if the file somehow gets imported from something else.  Nifty, eh?
if __name__ == "__main__":
    main()
########NEW FILE########
__FILENAME__ = StrategyData
import tables as pt, numpy as np, pickle
from models.StrategyDataModel import StrategyDataModel
import numpy as np
            
class StrategyData: 
    def __init__(self,dataFile, dataAccess, isTable = False):
        '''
        @param dataFile: The filename of the data file (array or pytables)
        @param isTable: The runtype, true for table false for array
        @param dataAcess: is a DataAccess object that is used to access the stockData 
        '''
        #for pytables
        self.isTable = isTable
        self.currTimestamp = 0
        self.dataAccess=dataAccess
        if(isTable):
            isTable #do nothing.. just so that I don't have to remove the if/else
#            self.strategyDataFile = pt.openFile(dataFile, mode = "r")
#            self.strategyData = self.strategyDataFile.root.StrategyData.StrategyData
#            self.timestampIndex = None
#            self.stocksIndex = self.findStocks()
        else:
            self.prevTsIdx = 0
            f = open(dataFile,'r')
            ts = pickle.load(f)
            st = pickle.load(f)
            pA = pickle.load(f)
            f.close()
            self.symbolIndex = st
            self.timestampIndex = ts
            self.priceArray = pA
                    
    def findStocks(self):
        '''
        @summary: Populates the symbolIndex for table run
        '''
        temp = []
        for i in self.strategyData.iterrows():
            if i['symbol'] not in temp:
                temp.append(self.cloneRow(i)['symbol'])
        temp.sort()
        return temp
    
    def calculatePortValue(self,stocks,timestamp):
        '''
        @param stocks: the current stocks you hold as represented by currStocks in portfolio
        @param timestamp: the timestamp used to calculate the present value of stocks
        @summary: Calculates the current portfolio value: cash + stocks. If the value of a stock on a particular day is NaN then it keeps going back in time (upto 20 days) to find the first nonNan stock value. If a non NaN value is not found then the value of the portfolio is NaN
        
        '''
        
        total=0
        DAY=86400
        for stock in stocks:
            priceOfStock= self.dataAccess.getStockDataItem (stock, 'adj_close', timestamp- DAY) #close of previous day
            if not(np.isnan(priceOfStock)):
                total+= priceOfStock
            else:
                
                #Keep looking back in time till we get a non NaN closing value
                ctr=2
                while (np.isnan(priceOfStock) and ctr < 20):
                    priceOfStock= self.dataAccess.getStockDataItem (stock, 'adj_close', timestamp- (ctr*DAY))
                    ctr+=1
                    
                if np.isnan(priceOfStock):
                    return np.NaN
                     
                total+= priceOfStock
        
        return total            
                     
        
        
#        total = 0
#        for stock in stocks:
#            prices = self.dataAccess.getStockDataList(stock, 'adj_close', timestamp- 86400, timestamp)  #self.getPrices(timestamp - 86400, timestamp, stock, 'adj_close')
#            i = 86400
#            count = 0
#            while(len(prices)==0 and count<10):
#                prices = self.dataAccess.getStockDataList(stock, 'adj_close',timestamp - i - 86400, timestamp - i) #self.getPrices(timestamp - i, timestamp - i - 86400, stock, 'adj_close')
#                i += 86400
#                count+=1
#            if(len(prices) != 0):
#                total += prices[len(prices)-1] * stocks[stock]
#        return total
        #calculatePortValue
#    def getStocks(self, startTime=None, endTime=None, ticker=None):
#        '''
#        Returns a list of dictionaries that contain all of the valid stock data as keys
#        or an empty list if no results are found
#        Can be called independently or used as part of the getPrices function
#        startTime: checks stocks >= startTime
#        endTime: checks stocks <= endTime
#        ticker: the ticker/symbol of the stock or a list of tickers
#        '''
#        if self.isTable:
#            if endTime == None:
#                endTime = self.currTimestamp
#            if endTime > self.currTimestamp:
#                print 'Tried to access a future time %i, endTime set to %i' %(endTime, self.currTimestamp)
#                endTime = self.currTimestamp
#            tempList = []
#            if(ticker!=None):    
#                if(type(ticker)==str):
#                    for row in self.strategyData.where('symbol=="%s"'%ticker):
#                        if(startTime!=None and endTime!=None):
#                            if(row['timestamp']>=startTime and row['timestamp']<endTime):
#                                tempList.append(self.cloneRow(row))
#                        elif(startTime!=None):
#                            if(row['timestamp']>=startTime and row['timestamp']<self.currTimestamp):
#                                tempList.append(self.cloneRow(row))
#                        elif(endTime!=None):
#                            if(row['timestamp']<endTime):
#                                tempList.append(self.cloneRow(row))
#                        else: #no time given
#                            tempList.append(self.cloneRow(row))
#                elif(type(ticker)==list):
#                    for tick in ticker:
#                        for row in self.strategyData.where('symbol=="%s"'%tick):
#                            if(startTime!=None and endTime!=None):
#                                if(row['timestamp']>=startTime and row['timestamp']<endTime):
#                                    tempList.append(self.cloneRow(row))
#                            elif(startTime!=None):
#                                if(row['timestamp']>=startTime and row['timestamp']<self.currTimestamp):
#                                    tempList.append(self.cloneRow(row))
#                            elif(endTime!=None):
#                                if(row['timestamp']<endTime):
#                                    tempList.append(self.cloneRow(row))
#                            else: #no time given
#                                tempList.append(self.cloneRow(row))
#                     
#            else:
#                for row in self.strategyData.iterrows():
#                    if(startTime!=None and endTime!=None):
#                        if(row['timestamp']>=startTime and row['timestamp']<endTime):
#                            tempList.append(self.cloneRow(row))
#                    elif(startTime!=None):
#                        if(row['timestamp']>=startTime and row['timestamp']<self.currTimestamp):
#                            tempList.append(self.cloneRow(row))
#                    elif(endTime!=None):
#                        if(row['timestamp']<endTime):
#                            tempList.append(self.cloneRow(row))
#                    else: #no time given
#                        tempList.append(self.cloneRow(row))                    
#            return tempList
#        else:
#            return self.getStocksArray(startTime, endTime, ticker)
    
#    def getPrice(self, timestamp, ticker, description):
#        '''
#        Returns a single price based on the parameters
#        timestamp: the exact timestamp of the desired stock data
#        ticker: the ticker/symbol of the stock
#        description: the field from data that is desired IE. adj_high
#        NOTE: If the data is incorrect or invalid, the function will return None  
#        '''
#        if self.isTable:
#            result = None
#            for row in self.strategyData.where('symbol=="%s"'%ticker):
#                if row['timestamp']==timestamp:
#                    result = row[description]
#            return result
#        else:
#            return self.getPriceArray(timestamp, ticker, description)
        
#    def getPrices(self, startTime=None, endTime=None, ticker=None, description=None):
#        '''
#        Returns a list of prices for the given description: [adj_high1, adj_high2, adj_high3...]
#        or a tuple if no description is given: [ (adj_high1, adj_low1, adj_open1, adj_close1, close1), (adj_high2, adj_low2...), .... ]
#        startTime: checks stocks >= startTime
#        endTime: checks stocks <= endTime
#        ticker: the ticker/symbol of the stock or a list of tickers
#        description: the field from data that is desired IE. adj_high
#        '''
#        if self.isTable:
#            rows = self.getStocks(startTime, endTime, ticker)
#            result = []
#            if(description==None):
#                for row in rows:
#                    row = self.cloneRow(row)
#                    result.append((row['adj_high'],row['adj_low'],row['adj_open'],row['adj_close'],row['close']))
#            else:
#                for row in rows:
#                    result.append(self.cloneRow(row)[description])
#            return result
#        else:
#            return self.getPricesArray(startTime, endTime, ticker, description)
    
    def cloneRow(self,row):
        ''' 
        @summary: Makes a copy of the row so that the correct information will be appended to the list
        '''
        dct = {}  
        dct['symbol'] = row['symbol']
        dct['exchange'] = row['exchange']
        dct['adj_high'] = row['adj_high']
        dct['adj_low'] = row['adj_low']
        dct['adj_open'] = row['adj_open']
        dct['adj_close'] = row['adj_close']
        dct['close'] = row['close']
        dct['volume'] = row['volume']
        dct['timestamp'] = row['timestamp']
        dct['date'] = row['date']
        dct['interval'] = row['interval']            
        return dct
 
#    def getStocksArray(self, startTime=None, endTime=None, ticker=None):
#        '''
#        Returns a list of dictionaries that contain all of the valid stock data as keys
#        or an empty list if no results are found
#        Can be called independently or used as part of the getPrices function
#        startTime: checks stocks >= startTime
#        endTime: checks stocks <= endTime
#        ticker: the ticker/symbol of the stock or a list of tickers
#        '''
#        if endTime == None:
#            endTime = self.currTimestamp
#        if endTime > self.currTimestamp:
#            print 'Tried to access a future time %i, endTime set to %i' %(endTime, self.currTimestamp)
#            endTime = self.currTimestamp
#        if ticker != None:
#            if type(ticker)==str:
#                tickIdxList = []
#                tickerIdx = self.symbolIndex.searchsorted(ticker)
#                if tickerIdx < self.symbolIndex.size and self.symbolIndex[tickerIdx] == ticker:
#                    tickIdxList.append(tickerIdx)
#            elif type(ticker)==list:
#                for tick in tickerIdx:
#                    tickerIdx = self.symbolIndex.searchsorted(ticker)
#                    if tickerIdx < self.symbolIndex.size and self.symbolIndex[tickerIdx] == ticker:
#                        tickIdxList.append(tickerIdx)
#        else:
#            tickerIdx = None      
#        if startTime != None:
#            startIdx = self.timestampIndex.searchsorted(startTime, 'left')
#        else:
#            startIdx = None
#        if endTime != None:
#            endIdx = self.timestampIndex.searchsorted(endTime, 'left')
#        else:
#            endIdx = None  
#        if tickerIdx != None:
#            result = np.array([])
#            for tickerIdx in tickIdxList:
#                result = np.append(result,self.priceArray[startIdx:endIdx,tickerIdx])
#            return result
#        else:
#            result = self.priceArray[startIdx:endIdx,:]
#            if len(result) ==0:
#                return []
#            else:
#                return result[0]
        
        
#    def getPriceArray(self, timestamp, ticker, description):
#        '''
#        timestamp: the exact timestamp of the desired stock data
#        ticker: the ticker/symbol of the stock
#        description: the field from data that is desired IE. adj_high
#        NOTE: If the data is incorrect or invalid, the function will return None  
#        '''
#        tsIdx = self.timestampIndex.searchsorted(timestamp)
#        if tsIdx >= self.timestampIndex.size or self.timestampIndex[tsIdx] != timestamp:
#            return None #NaN  
#        tickerIdx = self.symbolIndex.searchsorted(ticker)
#        if tickerIdx >= self.symbolIndex.size or self.symbolIndex[tickerIdx] != ticker:
#            return None #NaN
#        return self.priceArray[tsIdx,tickerIdx][description]
 
#    def getPricesArray(self, startTime=None, endTime=None, ticker=None, description=None):
#        '''
#        Returns a list of prices for the given description: [adj_high0, adj_high1, adj_high2...]
#        or a tuple if no description is given: [ (adj_high0, adj_low0, adj_open0, adj_close0, close0), (adj_high1, adj_low1...), .... ]
#        startTime: checks stocks >= startTime
#        endTime: checks stocks <= endTime
#        ticker: the ticker/symbol of the stock or a list of tickers
#        description: the field from data that is desired IE. adj_high 
#        description: 
#        '''
#        rows = self.getStocksArray(startTime, endTime, ticker)
#        result = []
#        if(description==None):
#            for row in rows:
#                result.append((row['adj_high'],row['adj_low'],row['adj_open'],row['adj_close'],row['close']))
#        else:
#            for row in rows:
#                result.append(row[description])
#        return result 

    def close(self):
        if self.isTable:
            self.isTable
#            self.strategyDataFile.close()
           
    class OutputOrder:
        '''
        @summary: Subclass to make adding strategies easier
        '''
        def __init__(self,symbol = "",volume = 0,task = "",duration = 0,closeType = "",orderType = "",limitPrice = 0):
            self.symbol = symbol
            self.volume = volume
            self.task = task
            self.duration = duration
            self.closeType = closeType
            self.orderType = orderType
            self.limitPrice = limitPrice
            
        def getOutput(self):
            if self.symbol == "" or type(self.symbol) != str:
                print "Invalid symbol %s in output." % str(self.symbol)
                return None
            if self.volume == 0 or type(self.volume) != int:
                print "Invalid volume %s in output." % str(self.volume)
                return None
            if self.task == "" or type(self.task) != str:
                print "Invalid task %s in output." % str(self.task)
                return None
            if self.duration <= 0 or type(self.duration) != int:
                print "Invalid duration %s in output." % str(self.duration)
                return None
            if self.orderType == "" or type(self.orderType) != str:
                print "Invalid orderType %s in output." % str(self.orderType)
                return None
            if type(self.task) != str:
                print "Invalid closeType %s specified." % str(self.task)
                return None
            if self.task.upper() == "SELL" or self.task.upper() == "COVER":
                if self.closeType == "" or type(self.closeType) != str:
                    print "Invalid closeType %s specified for %s." % (str(self.closeType),self.task)
                    return None
            if type(self.orderType) != str:
                print "Invalid orderType %s specified." % str(self.orderType)
            if self.orderType.upper() == "LIMIT":
                if self.limitPrice == 0 or type(self.limitPrice) != int:
                    print "Invalid limitPrice specified."
                    return None
            if self.task.upper() not in ["BUY","SELL","SHORT","COVER"]:
                print "Invalid task %s specified." %self.task
                return None
            if self.orderType.upper() not in ["LIMIT","MOC","MOO","VWAP"]:
                print "Invalid orderType %s specified." % self.orderType
                return None
            return (self.task,self.volume,self.symbol,self.orderType,self.duration,self.closeType,self.limitPrice)
        
        #END OutputOrder SUBLCLASS
        

def generateKnownArray():
    timestamps = np.array([])
    stocks = np.array([])
    for i in range(10,100):
        timestamps = np.append(timestamps, i*86400)
    for i in range(3):
        stocks = np.append(stocks,'stock%i'%i)
    priceArray = np.ndarray(shape=(timestamps.size,stocks.size),dtype=np.object)
    for i in range(timestamps.size):
        for j in range(stocks.size):
            row = {}
            row['exchange'] = 'NYSE'
            row['symbol'] = stocks[j]
            row['adj_open'] = (timestamps[i]/86400)  * (j+1)
            row['adj_close'] = (timestamps[i]/86400)  * (j+1)
            row['adj_high'] = (timestamps[i]/86400)  * (j+1)
            row['adj_low'] = (timestamps[i]/86400)  * (j+1)
            row['close'] = (timestamps[i]/86400)  * (j+1)
            row['volume'] = 200
            row['timestamp'] = timestamps[i]
            row['when_available'] = timestamps[i]
            row['interval'] = 86400
            priceArray[i,j] = row
    return (timestamps, stocks, priceArray)
def generateRandomArray():
    import random
    random.seed(1)    
    #86400 seconds in a day
    timestamps = np.array([])
    stocks = np.array([])
    for i in range(10,100): #timestamps
        timestamps = np.append(timestamps,i*86400)
    for i in range(30): #stocks
        stocks = np.append(stocks,'stock%.6i'%i)
        
    priceArray = np.ndarray( shape=(timestamps.size, stocks.size), dtype=np.object)
    for i in range(timestamps.size):    
        for j in range(stocks.size):
            
            row = {}
            if j ==0:
                row['exchange'] = 'NYSE'
                row['symbol'] = stocks[j]
                row['adj_open'] = 10 
                row['adj_close'] = 20
                row['adj_high'] = 22
                row['adj_low'] = 7
                row['close'] = 20
                row['volume'] = 200
                row['timestamp'] = timestamps[i]
                row['when_available'] = timestamps[i]
                row['interval'] = 86400
            else:
                adjOpen = random.random() * random.randint(1,100)   
                adjClose = random.random() * random.randint(1,100) 
                row['exchange'] = 'NYSE'
                row['symbol'] = stocks[j]
                row['adj_open'] = adjOpen 
                row['adj_close'] = adjClose
                row['adj_high'] = max(adjOpen,adjClose) * random.randint(1,5)
                row['adj_low'] = min(adjOpen,adjClose) / random.randint(1,5)
                row['close'] = adjClose
                row['volume'] = random.randint(1000,10000)
                row['timestamp'] = timestamps[i]
                row['when_available'] = timestamps[i]
                row['interval'] = 86400
            priceArray[i,j] = row 
        if i%10==0:
            print i,
        if i%100==0:
            print ''
    print ''
    '''
    pickle_output = open('randomArrayFile.pkl','w')
    pickler = pickle.dump(timestamps,pickle_output)
    pickler = pickle.dump(stocks,pickle_output)
    pickler = pickle.dump(priceArray,pickle_output)
    pickle_output.close()
    '''
    return (timestamps, stocks, priceArray)

def methodTest():
    strat = StrategyData('models/PriceTestData.h5')
    print strat.getStocks(startTime=0, ticker='KO')
    
def classTest():
    '''
    Needs to be updated to reflect move from data class to interpreter class
    '''
    rows = getStocks(ticker = 'KO')
    rows = getStocks(1020, 1050)
    for row in rows:
        print row['symbol'], row['exchange'], row['timestamp'],\
            row['when_available'], row['interval'], row['data']
    
    price = getPrice('adj_high', 1020, 'KO')
    print price
    prices = getPrices('adj_high',ticker='KO')
    print prices
########NEW FILE########
__FILENAME__ = stratDemo
import random
#THIS FILE IS NOW NOT NEEDED- SHREYAS JOSHI- 1- JUNE- 2010


# This file contains the demo strategies put together for testing purposes.
# note: all the parameters for strategy functions MUST BE THE SAME, otherwise will not work

# one day in unix time
DAY = 86400

def shortStrategy(portfolio,positions,timestamp,stockInfo):
    # written to test shortsells
    # shorts 20 shares a day
    output = []
    for stock in stockInfo.getStocks(startTime = timestamp-DAY,endTime = timestamp):
        order = stockInfo.OutputOrder()
        order.symbol = stock['symbol']
        order.volume = 20
        order.task = 'short'
        order.orderType = 'moc'
        order.duration = DAY * 2
        newOrder = order.getOutput()
        if newOrder != None:
            output.append(newOrder)
    # covers roughly half the owned volume
    for stock in portfolio.currStocks:
        order = stockInfo.OutputOrder()
        order.symbol = stock
        order.volume = -(portfolio.currStocks[stock]/2+1)
        order.task = 'cover'
        order.orderType = 'moo'
        order.closeType = 'fifo'
        order.duration = DAY * 2
        newOrder = order.getOutput()
        if newOrder != None:
            output.append(newOrder)
    return output

def dumbStrategy(portfolio,positions,timestamp,stockInfo):
    # rudimentary strategy to verify price data--built to run on fabricated stock data where volume/price were functions of the timestamp
    # buys an amount based on the timestamp, then sells half of what is owned
    output = []
    for stock in stockInfo.getStocks(startTime = timestamp-DAY,endTime = timestamp):
        order = stockInfo.OutputOrder()
        order.symbol = stock['symbol']
        order.volume = timestamp / DAY / 50
        order.task = 'buy'
        order.orderType = 'moc'
        order.duration = DAY * 2
        newOrder = order.getOutput()
        if newOrder != None:
            output.append(newOrder)
    for stock in portfolio.currStocks:
        order = stockInfo.OutputOrder()
        order.symbol = stock
        order.volume = portfolio.currStocks[stock]/2+1
        order.task = 'sell'
        order.orderType = 'moo'
        order.closeType = 'fifo'
        order.duration = DAY * 2
        newOrder = order.getOutput()
        if newOrder != None:
            output.append(newOrder)
    return output
        
def firstStrategy(portfolio,positions,timestamp,stockInfo):
    '''
    Decides what to do based on current and past stock data.
    The portfolio is a portfolio object that has your currently held stocks (currStocks) and your current cash (currCash)
    The timestamp is the current timestamp that the simulator is running on
    stockInfo is the StrategyData that the strategy can use to find out information about the stocks.  See below.
    '''
    output = []
    #This first for loop goes over all of the stock data to determine which stocks to buy
    for stock in stockInfo.getStocks(startTime = timestamp - DAY,endTime = timestamp):
        # if close is higher than open and close is closer to high than open is to low, buy
        if stock['adj_open'] < stock['adj_close'] and (stock['adj_high'] - stock['adj_close']) > (stock['adj_open'] - stock['adj_close']):
            order = stockInfo.OutputOrder()
            order.symbol = stock['symbol']
            order.volume = 20
            order.task = 'buy'
            order.orderType = 'moc'
            order.duration = DAY * 2
            newOrder = order.getOutput()
            if newOrder != None:
                output.append(newOrder)
            
    #This for loop goes over all of our current stocks to determine which stocks to sell
    for stock in portfolio.currStocks:
        openPrice = stockInfo.getPrices(timestamp - DAY, timestamp,stock,'adj_open')
        closePrice = stockInfo.getPrices(timestamp - DAY, timestamp,stock,'adj_close')
        highPrice = stockInfo.getPrices(timestamp - DAY, timestamp,stock,'adj_high')
        lowPrice = stockInfo.getPrices(timestamp - DAY, timestamp,stock,'adj_low')
        if(len(openPrice) != 0 and len(closePrice) != 0 and len(highPrice) != 0 and len(lowPrice) != 0):
            # if closeprice is closer to low than openprice is to high, sell
            if (closePrice[0]-lowPrice[0]) > (highPrice[0]-openPrice[0]):
                order = stockInfo.OutputOrder()
                order.symbol = stock
                order.volume = portfolio.currStocks[stock]/2+1
                order.task = 'sell'
                order.orderType = 'moo'
                order.closeType = 'fifo'
                order.duration = DAY * 2
                newOrder = order.getOutput()
                if newOrder != None:
                    output.append(newOrder)   
    # return the sell orders and buy orders to the simulator to execute
    return output

def dollarStrategy(portfolio,positions,timestamp,stockInfo):
    '''
    First semi-real strategy (specified by Tucker).
    Buys a stock if yesterday's close price was above $1 and todays is below $1, sells 20 days later no matter what.
    '''
    output = []
    for yesterday in stockInfo.getStocksArray(timestamp - DAY * 2, timestamp - DAY):
        if yesterday['close'] > 1:
            for today in stockInfo.getStocksArray(timestamp-DAY,timestamp,yesterday['symbol']):
                if today['close'] < 1:
                    order = stockInfo.OutputOrder()
                    order.symbol = today['symbol']
                    order.volume = 10000./today['close']
                    order.task = 'buy'
                    order.orderType = 'limit'
                    order.limitPrice = today['close']
                    order.duration = DAY
                    newOrder = order.getOutput()
                    if newOrder != None:
                        output.append(newOrder)  
    for position in positions.getPositions():
        if (position['timestamp'] <= (timestamp - DAY * 20)) and (position['shares'] > 0):
            order = stockInfo.OutputOrder()
            order.symbol = position['symbol']
            order.volume = position['shares']
            order.task = 'sell'
            order.orderType = 'moo'
            order.closeType = 'fifo'
            order.duration = DAY
            newOrder = order.getOutput()
            if newOrder != None:
                output.append(newOrder)  
    return output

def sortBySignal(listOfSymbols, timestamp,stockInfo):
    # helper function for analystStrategy - sorts a list of symbols according to the value of the signal field
    return sorted(listOfSymbols, lambda x: stockInfo.getStocks(timestamp-DAY,timestamp,x)[0]['signal'])

def analystStrategy(portfolio,positions,timestamp,stockInfo):
    # demonstrates how to use an additional field (such as analyst data) to base your transactions on
    # In this case, the additional field is a float from -1.0 to 1.0 which tells how good it is to sell/buy at that time
    
    # This strategy shorts the n most negative stocks, buys the n most positive, sells/covers everything after ten days
    n = 10
    nMostNegative = []
    nMostPositive = []
    output = []
    for stock in stockInfo.getStocks(timestamp - DAY, timestamp):
        if len(nMostNegative) < n:
            # if not n found yet, add to the list
            nMostNegative.append(stock['symbol'])
            nMostNegative = sortBySignal(nMostNegative,timestamp,stockInfo)
        elif stock['signal'] < getStocks(timestamp-DAY,timestamp,nMostNegative[len(nMostNegative)-1])[0]['signal']:
            # if better than something in the n found, add it, resort, and remove the worst
            nMostNegative.append(stock['symbol'])
            nMostNegative = sortBySignal(nMostNegative,timestamp,stockInfo)
            nMostNegative = nMostNegative[:-1]
        if len(nMostPositive) < n:
            # if not n found yet, add to the list
            nMostPositive.append(stock['symbol'])
            nMostPositive = sortBySignal(nMostPositive,timestamp,stockInfo)
        elif stock['signal'] > getStocks(timestamp-DAY,timestamp,nMostPositive[0])[0]['signal']:
            # if better than something in the n found, add it, resort, and remove the worst
            nMostPositive.append(stock['symbol'])
            nMostPositive = sortBySignal(nMostPositive,timestamp,stockInfo)
            nMostPositive = nMostPositive[1:]
    # add everything in the lists to the output
    for symbol in nMostNegative:
        order = stockInfo.OutputOrder()
        order.symbol = symbol
        order.volume = 50
        order.task = 'short'
        order.orderType = 'moo'
        order.duration = DAY
        newOrder = order.getOutput()
        if newOrder != None:
            output.append(newOrder)
    for symbol in nMostPositive:         
        order = stockInfo.OutputOrder()
        order.symbol = symbol
        order.volume = 50
        order.task = 'buy'
        order.orderType = 'moo'
        order.duration = DAY
        newOrder = order.getOutput()
        if newOrder != None:
            output.append(newOrder)
    # sell/cover ten days later
    for position in positions.getPositions():
        if (position['timestamp'] <= (timestamp - DAY * 10)) and (position['shares'] > 0):
            order = stockInfo.OutputOrder()
            order.symbol = position['symbol']
            order.volume = position['shares']
            order.task = 'sell'
            order.orderType = 'moo'
            order.closeType = 'fifo'
            order.duration = DAY
            newOrder = order.getOutput()
            if newOrder != None:
                output.append(newOrder)
        if (position['timestamp'] <= (timestamp - DAY * 10)) and (position['shares'] < 0):
            order = stockInfo.OutputOrder()
            order.symbol = position['symbol']
            order.volume = -position['shares']
            order.task = 'cover'
            order.orderType = 'moo'
            order.closeType = 'fifo'
            order.duration = DAY
            newOrder = order.getOutput()
            if newOrder != None:
                output.append(newOrder)  
    return output
    
########NEW FILE########
__FILENAME__ = timeseries
'''
Created on Oct 7, 2010

@author: Tucker Balch
@contact: tucker@cc.@gatech.edu
'''

import os
from qstkutil import DataAccess as da

__version__ = "$Revision: 156 $"

class TimeSeries:
	"""A class for processing time series information"""
	
	timestamps = list()
	symbols = list()
	values = []
	
	def __init__(self, tss, syms, vals):
		self.timestamps = list(tss)
		self.symbols = list(syms)
		self.values = vals
		
def getTSFromData(dataname,partname,symbols,tsstart,tsend):
	pathpre = os.environ.get('QSDATA') + "/Processed"
	if dataname == "Norgate":
		pathsub = "/Norgate/Equities"
		paths=list()
		paths.append(pathpre + pathsub + "/US_NASDAQ/")
		paths.append(pathpre + pathsub + "/US_NYSE/")
		paths.append(pathpre + pathsub + "/US_NYSE Arca/")
		paths.append(pathpre + pathsub + "/OTC/")
		datastr1 = "/StrategyData"
		datastr2 = "StrategyData"
	else:
		raise Exception("unknown dataname " + str(dataname))

	data = da.DataAccess(True, paths, datastr1, datastr2,
       		False, symbols, tsstart, tsend)
	tss = list(data.getTimestampArray())
	start_time = tss[0]
	end_time = tss[-1]
	vals = data.getMatrixBetweenTS(symbols,partname,
		start_time,end_time)
	syms = list(data.getListOfSymbols())
	del data

	return(TimeSeries(tss,syms,vals))
# end getTSFromData

########NEW FILE########
__FILENAME__ = quickSim
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''


# Python imports
import os
import cPickle
import sys
import datetime as dt

# 3rd Party Imports
import numpy.core.multiarray
import pandas as pand

# QSTK imports
import qstksim as qs
from qstkutil import qsdateutil as du
from qstkutil import DataAccess as da


def quickSim( alloc, historic, start_cash ):
    """
    @summary Quickly back tests an allocation for certain historical data, 
             using a starting fund value
    @param alloc: DataMatrix containing timestamps to test as indices and 
                 Symbols to test as columns, with _CASH symbol as the last 
                 column
    @param historic: Historic dataframe of equity prices
    @param start_cash: integer specifing initial fund value
    @return funds: TimeSeries with fund values for each day in the back test
    @rtype TimeSeries
    """
    
    from inspect import currentframe, getframeinfo
    frameinfo = getframeinfo(currentframe())
    raise DeprecationWarning('Please use qstksim.tradesim instead, or' +
                              ' comment me out in %s, line %i'%(frameinfo.filename, 
                                                              frameinfo.lineno))
    
    #original quick simulator
    #not designed to handle shorts
    
    #check each row in alloc
    for row in range( 0, len(alloc.values[:, 0]) ):
        if( abs(alloc.values[row, :].sum() - 1) > .0001 ):
            print "warning, alloc row " + str(row) + \
            "does not sum to one, rebalancing"
            #if no allocation, all in cash
            if(alloc.values[row, :].sum()==0):
                alloc.values[row, -1] = 1
            else:
                alloc.values[row, :] = alloc.values[row, :]  \
                / alloc.values[row, :].sum()
    
    # add cash column
    historic['_CASH'] = 1



    closest = historic[historic.index <= alloc.index[0]].ix[:]


    # start shares/fund out as 100% cash
    fund_ts = pand.Series( [start_cash], index = [closest.index[0]] )
    
    shares = (alloc.ix[0:1] * 0.0)
    shares['_CASH'] = start_cash
    
    #compute all trades in the allocation frame
    for row_index, row in alloc.iterrows():
        
        trade_price = historic.ix[row_index:].ix[0:1]
        trade_date = trade_price.index[0]
        
        # get stock prices on all the days up until this trade
        to_calculate = historic[ (historic.index <= trade_date) &
                                 (historic.index > fund_ts.index[-1]) ]
        
        # multiply prices by our current shares
        values_by_stock = to_calculate * shares.ix[-1]
        
        # calculate total value and append to our fund history
        fund_ts = fund_ts.append( values_by_stock.sum(axis=1) )

        # Get new shares values
        shares = (row * fund_ts.ix[-1]) / trade_price

    return fund_ts

def _compute_short(arr):
    ''' Computes total value of negative positions '''
    tally = 0
    for i in range(0, len(arr) - 1):
        if arr[i] < 0:
            tally = tally+arr[i]
    return abs(tally)

def _compute_long(arr):
    ''' Computes total value of positive positions '''
    tally = 0
    for i in range(0, len(arr) - 1):
        if arr[i] > 0:
            tally = tally+arr[i]
    return tally
    
def _compute_leverage(arr, fundval):
    ''' Computes percent leverage '''
    if fundval == 0:
        return 0
    return (_compute_long(arr) - _compute_short(arr))/fundval
    
def shortingQuickSim(alloc, historic, start_cash, leverage):
    ''' shortingQuickSim
        designed to handle shorts, keeps track of leverage keeping it within
        paramaterized value, ignore alloc cash column '''

    del alloc['_CASH']
    #fix invalid days
    historic = historic.fillna(method='backfill') 
    
    #compute first trade
    closest = historic[historic.index <= alloc.index[0]]
    fund_ts = pand.Series( [start_cash], index = [closest.index[-1]] )
    shares = alloc.values[0, :] * fund_ts.values[-1] / closest.values[-1, :]
    cash_values = pand.DataMatrix( [shares * closest.values[-1, :]], 
                                   index=[closest.index[-1]] )
    
    #compute all trades
    for i in range(1, len(alloc.values[:, 0])):
        #check leverage
        #TODO Find out what to use for fundvall below...
        this_leverage = _compute_leverage( alloc.values[0, :], start_cash )
        if this_leverage > leverage:
            print 'Warning, leverage of ', this_leverage, \
                  ' reached, exceeds leverage limit of ', leverage, '\n'
        #get closest date(previous date)
        closest = historic[ historic.index <= alloc.index[i] ]
        #for loop to calculate fund daily (without rebalancing)
        for date in closest[ closest.index > fund_ts.index[-1] ].index:
            #compute and record total fund value (Sum(closest close * stocks))
            fund_ts = fund_ts.append( pand.Series(
                      [(closest.xs(date) * shares).sum()], index=[date]) )
            cash_values = cash_values.append( pand.DataMatrix(
                                   [shares*closest.xs(date)], index=[date]) )
        
        #distribute fund in accordance with alloc
        shares = alloc.values[i, :] * (
                 fund_ts.values[-1] / closest.xs(closest.index[-1]) )

    #compute fund value for rest of historic data with final share distribution
    for date in historic[ historic.index > alloc.index[-1] ].index:
        if date in closest.index:
            fund_ts = fund_ts.append( 
                      pand.Series( [(closest.xs(date) * shares).sum()], 
                                   index=[date]) )  

    #return fund record
    return fund_ts

def alloc_backtest(alloc, start):
    """
    @summary: Back tests an allocation from a pickle file. Uses a starting 
              portfolio value of start.
    @param alloc: Name of allocation pickle file. Pickle file contains a 
                  DataMatrix with timestamps as indexes and stock symbols as
                  columns, with the last column being the _CASH symbol, 
                  indicating how much
    of the allocation is in cash.
    @param start: integer specifying the starting value of the portfolio
    @return funds: List of fund values indicating the value of the portfolio 
                   throughout the back test.
    @rtype timeSeries
    """
    
    #read in alloc table from command line arguements
    alloc_input_file = open(alloc, "r")
    alloc = cPickle.load(alloc_input_file)
    
    # Get the data from the data store
    dataobj = da.DataAccess('Norgate')
    startday=alloc.index[0]-dt.timedelta(days=10)
    endday = alloc.index[-1]

    # Get desired timestamps
    timeofday=dt.timedelta(hours=16)
    timestamps = du.getNYSEdays(startday,endday,timeofday)
    historic = dataobj.get_data( timestamps, list(alloc.columns[0:-1]),
                                 "close" )
    #backtestx
    [fund, leverage, commissions, slippage]= qs.tradesim(alloc, historic, int(start), 1, True, 0.02, 5, 0.02 )

    return [fund, leverage, commissions, slippage]

def strat_backtest1(strat, start, end, num, diff, startval):
    """
    @summary: Back tests a strategy defined in a python script that takes
              in a start
    and end date along with a starting value a set number of times. 
    @param strat: filename of python script strategy
    @param start: starting date in a datetime object
    @param end: ending date in a datetime object
    @param num: number of tests to perform
    @param diff: offset in days of the tests
    @param startval: starting value of fund during back tests
    @return fundsmatrix: Datamatrix of fund values returned from each test
    @rtype datamatrix
    """
    fundsmatrix = []
    startdates = du.getNextNNYSEdays(start, num * diff, dt.timedelta(hours=16))
    enddates = du.getNextNNYSEdays(end, num * diff, dt.timedelta(hours=16))
    for i in range(0, num):
        os.system( 'python ' + strat + ' ' + startdates[i].strftime("%m-%d-%Y")\
                    + ' ' + enddates[i].strftime("%m-%d-%Y") + \
                    ' temp_alloc.pkl')
        
        return alloc_backtest('temp_alloc.pkl', startval)
    return fundsmatrix
    
def strat_backtest2(strat, start, end, diff, dur, startval):
    """
    @summary: Back tests a strategy defined in a python script that takes in a
             start and end date along with a starting value over a given 
             period.
    @param strat: filename of python script strategy
    @param start: starting date in a datetime object
    @param end: ending date in a datetime object
    @param diff: offset in days of the tests
    @param dur: length of a test
    @param startval: starting value of fund during back tests
    @return fundsmatrix: Datamatrix of fund values returned from each test
    @rtype datamatrix
    """
    fundsmatrix = []
    startdates = du.getNYSEdays( start, end, dt.timedelta(hours=16) )
    for i in range(0, len(startdates), diff):
        if( i + dur>=len(startdates) ):
            enddate = startdates[-1]
        else:
            enddate = startdates[i+dur]
        os.system('python ' + strat + ' ' + startdates[i].strftime("%m-%d-%Y")\
                   + ' ' + enddate.strftime("%m-%d-%Y") + ' temp_alloc.pkl')
        funds = alloc_backtest('temp_alloc.pkl', startval)
        fundsmatrix.append(funds)
    return fundsmatrix

def run_main():
    ''' Main program '''
    
    #
    # CmdlnQuickSim
    #
    # A function which runs a quick sim on an allocation provided via 
    # command line,
    # along with a starting cash value
    # 
    # Allocation Backtest:
    # python quickSim.py -a allocation_fiel start_value output_file
    # python quickSim.py -a 'alloc_file.pkl' 1000 'fund_output.pkl'
    #
    # Strategy backtest:
    # python quickSim.py -s strategy start end start_value output_file
    # python quickSim.py -s 'strategy.py' '2/2/2007' '2/2/2009' 1000 
    # 'fund_output.pkl' 
    #
    # Robust backtest:
    # python quickSim.py -r strategy start end days_between duration
    # start_value output
    # python quickSim.py -r 'strategy.py' '1-1-2004' '1-1-2007' 7 28 10000
    # 'out.pkl'
    # Drew Bratcher
    #

    if(sys.argv[1] == '-a'):
        funds = alloc_backtest(sys.argv[2], sys.argv[3])
        output = open(sys.argv[4], "w")
        cPickle.dump(funds, output)
    elif(sys.argv[1] == '-s'):
        t = map(int,  sys.argv[3].split('-'))
        startday = dt.datetime(t[2], t[0], t[1])
        t = map(int,  sys.argv[4].split('-'))
        endday = dt.datetime(t[2], t[0], t[1])  
        fundsmatrix = strat_backtest1(sys.argv[2], startday, endday, 1, 0, 
                                    int(sys.argv[5]))
        output = open(sys.argv[6], "w")
        cPickle.dump(fundsmatrix, output) 
    elif(sys.argv[1] == '-r'):
        t = map(int, sys.argv[3].split('-'))
        startday = dt.datetime(t[2], t[0], t[1])
        t = map(int, sys.argv[4].split('-'))
        endday = dt.datetime(t[2], t[0], t[1])
        fundsmatrix = strat_backtest2(sys.argv[2], startday, endday,
                                    int(sys.argv[5]), int(sys.argv[6]),
                                    int(sys.argv[7]))
        output = open(sys.argv[8], "w")
        cPickle.dump(fundsmatrix, output)
    else:
        print 'invalid command line call'
        print 'use python quickSim.py -a alloc_pkl start_value output_pkl'
        print 'or python quickSim.py -s strategy start_date end_date' + \
              'start_value output_pkl'
        print 'or python quickSim.py -r strategy start_date end_date' + \
              ' test_offset_in_days duration start_value output_pkl'

if __name__ == "__main__":
    run_main()

########NEW FILE########
__FILENAME__ = bollinger
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''



# bollinger.py
#
# A module which contains a bollinger strategy.
#
#

#python imports
import cPickle
from pylab import *
from pandas import *
import matplotlib.pyplot as plt
import datetime as dt
import os

#qstk imports
from qstkutil import DataAccess as da
import qstkutil.qsdateutil as du
import qstkutil.bollinger as boil

#simple versions
#stateful
def createStatefulStrat(adjclose, timestamps, lookback, highthresh, lowthresh):
	alloc=DataMatrix(index=[timestamps[0]],columns=adjclose.columns, data=[zeros(len(adjclose.columns))])
	bs=boil.calcbvals(adjclose, timestamps, adjclose.columns, lookback)
	hold=[]
 	for i in bs.index[1:]:
		for stock in range(0,len(bs.columns)):
			if(bs.xs(i)[stock]<lowthresh and len(hold)<10):
				hold.append(stock)
			elif(bs.xs(i)[stock]>highthresh):
				if stock in hold:
					hold.remove(stock)
		vals=zeros(len(adjclose.columns))
		for j in range(0,len(hold)):
			vals[hold[j]]=.1
		alloc=alloc.append(DataMatrix(index=[i],columns=adjclose.columns,data=[vals]))
	return alloc
	
#stateless
def createStatelessStrat(adjclose, timestamps, lookback, highthresh, lowthresh):
	return create(adjclose,timestamps,lookback,len(adjclose.columns),highthresh,lowthresh,.1,len(adjclose.index))

#creates an allocation DataMatrix based on bollinger strategy and paramaters
def create(adjclose, timestamps, lookback, spread, high, low, bet, duration):
	alloc=DataMatrix(index=[timestamps[0]],columns=adjclose.columns, data=[zeros(len(adjclose.columns))])
	bs=boil.calcbvals(adjclose, timestamps, adjclose.columns, lookback)
	hold=[]
	time=[]
 	for i in bs.index[1:]:
		for stock in range(0,len(bs.columns)):
			if(bs.xs(i)[stock]<low and len(hold)<spread):
				hold.append(stock)
				time.append(duration)
			elif(bs.xs(i)[stock]>high):
				if stock in hold:
					del time[hold.index(stock)]
					hold.remove(stock)
		for j in range(0,len(time)):
			time[j]-=1
			if(time[j]<=0):
				del hold[j]
				del time[j]
		
		vals=zeros(len(adjclose.columns))
		for j in range(0,len(hold)):
			vals[hold[j]]=bet
		alloc=alloc.append(DataMatrix(index=[i],columns=adjclose.columns,data=[vals]))
	return alloc

if __name__ == "__main__":
	#Usage: python bollinger.py '1-1-2004' '1-1-2009' 'alloc.pkl'
	print "Running Bollinger strategy starting "+sys.argv[1]+" and ending "+sys.argv[2]+"."
	
	#Run S&P500 for thresholds 1 and -1 in simple version for lookback of 10 days
	symbols = list(np.loadtxt(os.environ['QS']+'/quicksim/strategies/S&P500.csv',dtype='str',delimiter=',',comments='#',skiprows=0))
	
	t=map(int,sys.argv[1].split('-'))
	startday = dt.datetime(t[2],t[0],t[1])
	t=map(int,sys.argv[2].split('-'))
	endday = dt.datetime(t[2],t[0],t[1])
	
	timeofday=dt.timedelta(hours=16)
	timestamps=du.getNYSEdays(startday,endday,timeofday)
	
	
	dataobj=da.DataAccess(da.DataSource.NORGATE)
	intersectsyms=list(set(dataobj.get_all_symbols())&set(symbols))
	badsyms=[]
	if size(intersectsyms)<size(symbols):
		badsyms=list(set(symbols)-set(intersectsyms))
		print "bad symms:"
		print badsyms
	for i in badsyms:
		index=symbols.index(i)
		symbols.pop(index)
	historic = dataobj.get_data(timestamps,symbols,"close")
	
	alloc=createStatelessStrat(historic,timestamps,10,1,-1)
	
	output=open(sys.argv[3],"wb")
	cPickle.dump(alloc,output)
	output.close()

########NEW FILE########
__FILENAME__ = MonthlyRebalancing
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''


#
# MonthlyRebalancingExample.py
#
# Usage: python MonthlyRebalancingExample.py 1-1-2004' '1-1-2009' 'alloc.pkl'
#
# A strategy script which creates a monthly allocation table using 
# start date and end date along with the first 20 symbols of S&P500.
# It then dumps the allocation table to a pickle file.
#
#

#python imports
import cPickle
from pylab import *
from pandas import *
import matplotlib.pyplot as plt
import datetime as dt
import os

#qstk imports
import qstkutil.DataAccess as da
import qstkutil.qsdateutil as du

if __name__ == "__main__":
	print "Running Monthly Rebalancing strategy starting "+sys.argv[1]+" and ending "+sys.argv[2]+"."

	#Get first 20 S&P Symbols 
	symbols = list(np.loadtxt(os.environ['QS']+'/quicksim/strategies/S&P500.csv',dtype='str',delimiter=',',comments='#',skiprows=0))
	symbols = symbols[0:19]
	
	#Set start and end boundary times
	t = map(int,sys.argv[1].split('-'))
	startday = dt.datetime(t[2],t[0],t[1])
	t = map(int,sys.argv[2].split('-'))
	endday = dt.datetime(t[2],t[0],t[1])
	
	#Get desired timestamps
	timeofday=dt.timedelta(hours=16)
	timestamps = du.getNYSEdays(startday,endday,timeofday)
	
	# Get the data from the data store
	dataobj = da.DataAccess('Norgate')
	historic = dataobj.get_data(timestamps, symbols, "close")

    # Setup the allocation table
	alloc_vals=.8/(len(historic.values[0,:])-1)*ones((1,len(historic.values[0,:])))
	alloc=DataMatrix(index=[historic.index[0]], data=alloc_vals, columns=symbols)
	for date in range(1, len(historic.index)):
		if(historic.index[date].day==1):
			alloc=alloc.append(DataMatrix(index=[historic.index[date]], data=alloc_vals, columns=symbols))
	alloc[symbols[0]] = .1
	alloc['_CASH'] = .1
	
	#Dump to a pkl file
	output=open(sys.argv[3],"wb")
	cPickle.dump(alloc, output)

########NEW FILE########
__FILENAME__ = OneStock
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''


# OneStock.py
#
# Usage: python OneStock.py '1-1-2004' '1-1-2009' 'alloc.pkl'
#
# A strategy script which creates a daily allocation table using one stock (GOOG)
# and the start and end dates provided by the user.
# It then dumps the allocation table to a pickle file.
#
#

# python imports
import cPickle
import sys
from pandas import DataMatrix
import datetime as dt
import random

# qstk imports
import qstkutil.DataAccess as da
import qstkutil.qsdateutil as du

if __name__ == "__main__":
    print "Running One Stock strategy from "+sys.argv[1] +" to "+sys.argv[2]

    # Use google symbol
    symbols = list(['SPY'])

    # Set start and end dates
    t = map(int,sys.argv[1].split('-'))
    startday = dt.datetime(t[2],t[0],t[1])
    t = map(int,sys.argv[2].split('-'))
    endday = dt.datetime(t[2],t[0],t[1])

    # Get desired timestamps
    timeofday=dt.timedelta(hours=16)
    timestamps = du.getNYSEdays(startday,endday,timeofday)

    # Get the data from the data store
    dataobj = da.DataAccess('Norgate')
    historic = dataobj.get_data(timestamps, symbols, "close")

    # Setup the allocation table
    alloc_val= random.random()
    alloc=DataMatrix(index=[historic.index[0]], data=[alloc_val], columns=symbols)
    for date in range(1, len(historic.index)):
        alloc_val=1 #random.random()
        alloc=alloc.append(DataMatrix(index=[historic.index[date]], data=[alloc_val], columns=[symbols[0]]))
    alloc['_CASH']=1-alloc[symbols[0]]

    # Dump to pkl file
    output=open(sys.argv[3],"wb")
    cPickle.dump(alloc, output)

########NEW FILE########
__FILENAME__ = AccessData
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on April, 20, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Visualizer - Data Access files 

'''

#import libraries
import numpy as np
import time
import qstkutil.tsutil as tsu
import datetime as dt
import pickle
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from pylab import *
from pandas import *
import os
from PyQt4 import QtGui, QtCore, Qt


# Changes made in the featutil file are : Edited the getFeatureFuncs()


def ReadData(DataFile, TimeTag, IDTag, FactorTag):
	#Reading the timestamps from a text file.
	timestamps=[]
	file = open(TimeTag, 'r')
	for onedate in file.readlines():
		timestamps.append(dt.datetime.strptime(onedate, "%Y-%m-%d\n"))
	file.close()

	symbols=[]
	file = open(IDTag, 'r')
	for f in file.readlines():
		j = f[:-1]
		symbols.append(j)
	file.close()

	# Reading the Data Values
	Numpyarray=pickle.load(open( DataFile, 'rb' ))
	
	for i in range(0,len(Numpyarray)):
		tsu.fillforward(Numpyarray[i])
		tsu.fillbackward(Numpyarray[i])

	featureslist=[]
	file = open(FactorTag, 'r')
	for f in file.readlines():
		j = f[:-1]
		featureslist.append(j)
	file.close()

	PandasObject= Panel(Numpyarray, items=featureslist, major_axis=timestamps, minor_axis=symbols)
	featureslist.sort()
	return (PandasObject, featureslist, symbols, timestamps)

def DataParameter(PandasObject, featureslist, symbols, timestamps):
	
	startday=timestamps[0]
	endday=timestamps[-1]
	MinFeat=[]
	MaxFeat=[]

	for feature in featureslist:
		MinFeat.append(np.amin(np.min(PandasObject[feature], axis=0)))
		MaxFeat.append(np.amax(np.max(PandasObject[feature], axis=0)))

	dMinFeat=dict(zip(featureslist, MinFeat))
	dMaxFeat=dict(zip(featureslist, MaxFeat))
	
	return(dMinFeat, dMaxFeat, startday, endday)


def GetData(directorylocation):

	DataFile = directorylocation + 'ALLDATA.pkl'
	TimeTag=directorylocation + 'TimeStamps.txt'
	IDTag=directorylocation + 'Symbols.txt'
	FactorTag=directorylocation + 'Features.txt'	

	(PandasObject, featureslist, symbols, timestamps)=ReadData(DataFile, TimeTag, IDTag, FactorTag)
	(dMinFeat, dMaxFeat, startday, endday)=DataParameter(PandasObject, featureslist, symbols, timestamps)
	return (PandasObject, featureslist, symbols, timestamps,dMinFeat, dMaxFeat, startday, endday)


if __name__ == '__main__':
	directorylocation = os.environ['QS']+'/Tools/Visualizer/Data/Dow_2009-01-01_2010-12-31/'
	GetData(directorylocation)
	print "The access functions are working"

########NEW FILE########
__FILENAME__ = CsvData
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on April, 20, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Visualizer - Data reading from a CSV

'''

#import libraries
import numpy as np
import datetime as dt
import pickle
import os
import matplotlib.pyplot as plt
from pylab import *
from pandas import *
import csv

def csv_Dataconverter(datadirectory, ip_path):
	op_folderpath = os.environ['QS'] + 'Tools/Visualizer/Data/' + datadirectory
	if not os.path.exists(op_folderpath):
		os.mkdir(op_folderpath)
	op_folderpath = op_folderpath + '/'

	f = open(ip_path)
	data = csv.reader(f)
	fields = data.next()

	featureslist = fields[2:]
	timestamps=[]
	seen_timestamps = set()
	symbols=[]
	seen_symbols =set()

	for row in data:
		timestamp = dt.datetime.strptime(row[0], "%Y-%m-%d")
		tag = row[1]
		if timestamp not in seen_timestamps:
			timestamps.append(timestamp)
			seen_timestamps.add(timestamp)
		if tag not in seen_symbols:
			seen_symbols.add(tag)
			symbols.append(tag)

	numpyarray = np.empty([len(featureslist),len(timestamps),len(symbols)])
	numpyarray[:] = np.NAN

	PandasObject= Panel(numpyarray, items=featureslist, major_axis=timestamps, minor_axis=symbols)

	f.close()	

	data = csv.reader(open(ip_path))
	data.next()

	for row in data:
		timestamp = dt.datetime.strptime(row[0], "%Y-%m-%d")
		tag = row[1] 
		row1 = row[2:]
		for feat, val  in zip(featureslist, row1):
			try:
				PandasObject[feat][tag][timestamp] = float(val)
			except: 
				continue

	#Creating a txt file of timestamps
	file = open(op_folderpath +'TimeStamps.txt', 'w')
	for onedate in timestamps:
		stringdate=dt.date.isoformat(onedate)
		file.write(stringdate+'\n')
	file.close()

	#Creating a txt file of symbols
	file = open(op_folderpath +'Symbols.txt', 'w')
	for sym in symbols:
		file.write(str(sym)+'\n')
	file.close()

	#Creating a txt file of Features
	file = open(op_folderpath +'Features.txt', 'w')
	for f in featureslist:
		file.write(f+'\n')
	file.close()
	
	Numpyarray_Final = PandasObject.values
	for i,feat in enumerate(featureslist):
		for j,tag in enumerate(symbols):
			for k in range(len(timestamps)-1):
				if np.isnan(Numpyarray_Final[i][k+1][j]):
					Numpyarray_Final[i][k+1][j] = Numpyarray_Final[i][k][j]

	for i,feat in enumerate(featureslist):
		for j,tag in enumerate(symbols):
			for z in range(1,len(timestamps)):
				k = len(timestamps) - z
				if np.isnan(Numpyarray_Final[i][k-1][j]):
					Numpyarray_Final[i][k-1][j] = Numpyarray_Final[i][k][j]

	pickle.dump(Numpyarray_Final,open(op_folderpath +'ALLDATA.pkl', 'wb' ),-1)

	print 'All data has been converted'

def main():
	datadirectory = 'TestCSV'
	ip_path = os.environ['QS'] + '/Tools/Visualizer/Data/Raw/Test.csv'
	csv_Dataconverter(datadirectory, ip_path)

if __name__ == '__main__':
	main()

########NEW FILE########
__FILENAME__ = FormatData
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on April, 20, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Visualizer - Random Data Source - Weather Data

'''

#import libraries
import numpy as np
import datetime as dt
import pickle
import dircache
import os
import string
import datetime as dt
from pylab import *
from pandas import *
import qstkutil.tsutil as tsu

def genData():
	
	op_folderpath = os.environ['QS'] + 'Tools/Visualizer/Data/Norway'
	ip_folderpath = os.environ['QS'] + 'Tools/Visualizer/Data/Norway/Raw/'
	
	if not os.path.exists(op_folderpath):
		os.mkdir(op_folderpath)
		print "Data was missing"
		return
	op_folderpath = op_folderpath + '/'	

	files_at_this_path = dircache.listdir(ip_folderpath)
	ip_folderpath = ip_folderpath +'/'
	
	stationnames = []
	startyears = []
	endyears=[]

	for file1 in files_at_this_path:
		file = open(ip_folderpath + file1, 'r')
		for f in file.readlines():
			if string.find(f, 'Name')!=-1:
				n= string.lstrip(f, 'Name= ')
				stationnames.append(string.rstrip(n))
			if string.find(f, 'Start year')!=-1:
				n= string.lstrip(f, 'Start year= ')
				startyears.append(int(string.rstrip(n)))
			if string.find(f, 'End year')!=-1:
				n= string.lstrip(f, 'End year= ')
				endyears.append(int(string.rstrip(n)))
		file.close()

	timestamps = [ dt.datetime(year,1,1) for year in range(min(startyears),max(endyears)+1)]

	months = ['January','February','March','April','May','June','July','August','September','October','November','December']

	numpyarray = np.empty([len(months),len(timestamps),len(stationnames)])
	numpyarray[:] = np.NAN

	PandasObject= Panel(numpyarray, items=months, major_axis=timestamps, minor_axis=stationnames)

	for i, file1 in enumerate(files_at_this_path):
		flag=0
		station=stationnames[i]
		file = open(ip_folderpath + file1, 'r')
		for f in file.readlines():
			if flag==1:
				data=string.split(f)
				year = int(data.pop(0))
				time = dt.datetime(year,1,1)
				for month,val in zip(months,data):
					PandasObject[month][station][time] = float(val)
			if string.find(f, 'Obs')!=-1:
				flag=1
		file.close()

	#Creating a txt file of timestamps
	file = open(op_folderpath +'TimeStamps.txt', 'w')
	for onedate in timestamps:
		stringdate=dt.date.isoformat(onedate)
		file.write(stringdate+'\n')
	file.close()

	#Creating a txt file of symbols
	file = open(op_folderpath +'Symbols.txt', 'w')
	for sym in stationnames:
		file.write(str(sym)+'\n')
	file.close()

	#Creating a txt file of Features
	file = open(op_folderpath +'Features.txt', 'w')
	for f in months:
		file.write(f+'\n')
	file.close()
	
	Numpyarray_Final = PandasObject.values
	for i,month in enumerate(months):
		for j,station in enumerate(stationnames):
			for k in range(len(timestamps)-1):
				if np.isnan(Numpyarray_Final[i][k+1][j]):
					Numpyarray_Final[i][k+1][j] = Numpyarray_Final[i][k][j]

	for i,month in enumerate(months):
		for j,station in enumerate(stationnames):
			for z in range(1,len(timestamps)):
				k = len(timestamps) - z
				if np.isnan(Numpyarray_Final[i][k-1][j]):
					Numpyarray_Final[i][k-1][j] = Numpyarray_Final[i][k][j]

	pickle.dump(Numpyarray_Final,open(op_folderpath +'ALLDATA.pkl', 'wb' ),-1)


def main():
	genData()

if __name__ == '__main__':
	main()

########NEW FILE########
__FILENAME__ = GenerateData
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on April, 20, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Visualizer - Generating Data from QSTK

'''

#import libraries
import numpy as np
import qstkutil.qsdateutil as du
import qstkutil.tsutil as tsu
import qstkutil.DataAccess as da
import qstkfeat.sb_featutil as feat
import datetime as dt
import pickle
import os
import matplotlib.pyplot as plt
from pylab import *
from pandas import *


# Changes made in the featutil file are : Edited the getFeatureFuncs()


def genData(startday, endday, datadirectory, symbols):

	coredirectory = os.environ['QS']+'Tools/Visualizer/Data/'

	directorylocation= coredirectory+datadirectory+'_'+startday.date().isoformat() +'_'+endday.date().isoformat()

	if not os.path.exists(directorylocation):
		os.mkdir(directorylocation)

	directorylocation = directorylocation +'/'

	timeofday = dt.timedelta(hours=16)
	timestamps = du.getNYSEdays(startday,endday,timeofday)
	
	#Creating a txt file of timestamps
	file = open(directorylocation +'TimeStamps.txt', 'w')
	for onedate in timestamps:
		stringdate=dt.date.isoformat(onedate)
		file.write(stringdate+'\n')
	file.close()

	# Reading the Stock Price Data
	dataobj = da.DataAccess('Norgate')
	all_symbols = dataobj.get_all_symbols()
	badsymbols=set(symbols)-set(all_symbols)
	if len(list(badsymbols))>0:
		print "Some Symbols are not valid" + str(badsymbols)
	symbols=list(set(symbols)-badsymbols)

	lsKeys = ['open', 'high', 'low', 'close', 'volume']

	ldfData = dataobj.get_data( timestamps, symbols, lsKeys )
	dData = dict(zip(lsKeys, ldfData))
	
	
	# Creating the 3D Matrix

	(lfcFeatures, ldArgs, lsNames)= feat.getFeatureFuncs22()	

	FinalData = feat.applyFeatures( dData, lfcFeatures, ldArgs, sMarketRel='SPY')
	
	#Creating a txt file of symbols
	file = open(directorylocation +'Symbols.txt', 'w')
	for sym in symbols:
		file.write(str(sym)+'\n')
	file.close()

	#Creating a txt file of Features
	file = open(directorylocation +'Features.txt', 'w')
	for f in lsNames:
		file.write(f+'\n')
	file.close()
	
	Numpyarray=[]
	for IndicatorData in FinalData:
		Numpyarray.append(IndicatorData.values)

	pickle.dump(Numpyarray,open(directorylocation +'ALLDATA.pkl', 'wb' ),-1)
	
def main():
	startday=dt.datetime(2009,1,1)
	endday=dt.datetime(2010,12,31)
	datadirectory = 'Dow'
#	datadirectory = 'SP500'
#	symbols=np.loadtxt('Symbols.csv',dtype='S5',comments='#',skiprows=1,)
	symbols=['SPY','MMM','AA','AXP','T','BAC','BA','CAT','CVX','CSCO','KO','DD','XOM','GE','HPQ','HD','INTC','IBM','JNJ','JPM','KFT', 'MCD','MRK','MSFT','PFE','PG','TRV','UTX','VZ','WMT','DIS']
	genData(startday,endday, datadirectory, symbols)

if __name__ == '__main__':
	main()

########NEW FILE########
__FILENAME__ = Visualizer
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on April, 20, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Visualizer Main Code

'''
import sys
import numpy as np
import math 
import os
import dircache

import AccessData as AD
import pickle
from PyQt4 import QtGui, QtCore, Qt
from matplotlib.figure import Figure
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas


# SLIDER range variable for all the range slider. Increasing this will increase precision
SLIDER_RANGE=100
LOOKBACK_DAYS=20


# Range Slider Class, to implement the custom range slider.
class RangeSlider(QtGui.QSlider):
    """ A slider for ranges.
    
        This class provides a dual-slider for ranges, where there is a defined
        maximum and minimum, as is a normal slider, but instead of having a
        single slider value, there are 2 slider values.
        
        This class emits the same signals as the QSlider base class, with the 
        exception of valueChanged
    """
    def __init__(self, *args):
        super(RangeSlider, self).__init__(*args)
        
        self._low = self.minimum()
        self._high = self.maximum()
        
        self.pressed_control = QtGui.QStyle.SC_None
        self.hover_control = QtGui.QStyle.SC_None
        self.click_offset = 0
        
        # 0 for the low, 1 for the high, -1 for both
        self.active_slider = 0

    def low(self):
        return self._low

    def setLow(self, low):
        self._low = low
        self.update()

    def high(self):
        return self._high

    def setHigh(self, high):
        self._high = high
        self.update()
        
        
    def paintEvent(self, event):
        # based on http://qt.gitorious.org/qt/qt/blobs/master/src/gui/widgets/qslider.cpp

        painter = QtGui.QPainter(self)
        style = QtGui.QApplication.style() 
        
        for i, value in enumerate([self._low, self._high]):
            opt = QtGui.QStyleOptionSlider()
            self.initStyleOption(opt)

            # Only draw the groove for the first slider so it doesn't get drawn
            # on top of the existing ones every time
            if i == 0:
                opt.subControls = QtGui.QStyle.SC_SliderHandle#QtGui.QStyle.SC_SliderGroove | QtGui.QStyle.SC_SliderHandle
            else:
                opt.subControls = QtGui.QStyle.SC_SliderHandle

            if self.tickPosition() != self.NoTicks:
                opt.subControls |= QtGui.QStyle.SC_SliderTickmarks

            if self.pressed_control:
                opt.activeSubControls = self.pressed_control
                opt.state |= QtGui.QStyle.State_Sunken
            else:
                opt.activeSubControls = self.hover_control

            opt.sliderPosition = value
            opt.sliderValue = value                                  
            style.drawComplexControl(QtGui.QStyle.CC_Slider, opt, painter, self)
            
        
    def mousePressEvent(self, event):
        event.accept()
        
        style = QtGui.QApplication.style()
        button = event.button()
        
        # In a normal slider control, when the user clicks on a point in the 
        # slider's total range, but not on the slider part of the control the
        # control would jump the slider value to where the user clicked.
        # For this control, clicks which are not direct hits will slide both
        # slider parts
                
        if button:
            opt = QtGui.QStyleOptionSlider()
            self.initStyleOption(opt)

            self.active_slider = -1
            
            for i, value in enumerate([self._low, self._high]):
                opt.sliderPosition = value                
                hit = style.hitTestComplexControl(style.CC_Slider, opt, event.pos(), self)
                if hit == style.SC_SliderHandle:
                    self.active_slider = i
                    self.pressed_control = hit
                    
                    self.triggerAction(self.SliderMove)
                    self.setRepeatAction(self.SliderNoAction)
                    self.setSliderDown(True)
                    break

            if self.active_slider < 0:
                self.pressed_control = QtGui.QStyle.SC_SliderHandle
                self.click_offset = self.__pixelPosToRangeValue(self.__pick(event.pos()))
                self.triggerAction(self.SliderMove)
                self.setRepeatAction(self.SliderNoAction)
        else:
            event.ignore()
                                
    def mouseMoveEvent(self, event):
        if self.pressed_control != QtGui.QStyle.SC_SliderHandle:
            event.ignore()
            return
        
        event.accept()
        new_pos = self.__pixelPosToRangeValue(self.__pick(event.pos()))
        opt = QtGui.QStyleOptionSlider()
        self.initStyleOption(opt)
        
        if self.active_slider < 0:
            offset = new_pos - self.click_offset
            self._high += offset
            self._low += offset
            if self._low < self.minimum():
                diff = self.minimum() - self._low
                self._low += diff
                self._high += diff
            if self._high > self.maximum():
                diff = self.maximum() - self._high
                self._low += diff
                self._high += diff            
        elif self.active_slider == 0:
            if new_pos >= self._high:
                new_pos = self._high - 1
            self._low = new_pos
        else:
            if new_pos <= self._low:
                new_pos = self._low + 1
            self._high = new_pos

        self.click_offset = new_pos

        self.update()

        self.emit(QtCore.SIGNAL('sliderMoved'), self._low, self._high)
            
    def __pick(self, pt):
        if self.orientation() == QtCore.Qt.Horizontal:
            return pt.x()
        else:
            return pt.y()
           
    def __pixelPosToRangeValue(self, pos):
        opt = QtGui.QStyleOptionSlider()
        self.initStyleOption(opt)
        style = QtGui.QApplication.style()
        
        gr = style.subControlRect(style.CC_Slider, opt, style.SC_SliderGroove, self)
        sr = style.subControlRect(style.CC_Slider, opt, style.SC_SliderHandle, self)
        
        if self.orientation() == QtCore.Qt.Horizontal:
            slider_length = sr.width()
            slider_min = gr.x()
            slider_max = gr.right() - slider_length + 1
        else:
            slider_length = sr.height()
            slider_min = gr.y()
            slider_max = gr.bottom() - slider_length + 1
            
        return style.sliderValueFromPosition(self.minimum(), self.maximum(),
                                             pos-slider_min, slider_max-slider_min,
                                             opt.upsideDown)


###########################
##  Visualizer Class     ##
###########################

# Main class that contains the Visualizer Qt and all functions
class Visualizer(QtGui.QMainWindow):
    
	def __init__(self):
		super(Visualizer, self).__init__()
		# Initialization is a 3 phase process : Loading Data, Declaring Variables and Creating the GUI
		self.LoadData()
		self.Reset()
		self.create_main_frame()
		self.ResetFunc()
        
	def create_main_frame(self):
		# Setting Up the Main Frame of the GUI

		self.main_frame = QtGui.QWidget()
		self.statusBar().showMessage('Loading')

		# Declaring the matplotlib canvas for plotting graphs
		self.dpi=100
		self.fig = Figure((6.0, 5.5), dpi=self.dpi)
		self.canvas = FigureCanvas(self.fig)
		self.canvas.setParent(self.main_frame)
		self.ax = self.fig.gca(projection='3d')

		self.fig2 = Figure((6.0, 6.0), dpi=self.dpi*2)
		self.canvas2 = FigureCanvas(self.fig2)
		self.ax2 = self.fig2.gca(projection='3d')
		self.datetext2 = self.ax2.text2D(0, 1, 'Date : ', transform=self.ax2.transAxes)

		self.datetext = self.ax.text2D(0, 1, 'Date : ', transform=self.ax.transAxes)
		
		# Declaring the Texts in the GUI, fonts and Spacers to control the size of sliders	
		self.FactorLable=QtGui.QLabel('Factors', self)
		self.font = QtGui.QFont("Times", 16, QtGui.QFont.Bold, True)
		self.font1 = QtGui.QFont("Times", 12)
		self.font2 = QtGui.QFont("Times", 14, QtGui.QFont.Bold, True)
		self.font3 = QtGui.QFont("Times", 20, QtGui.QFont.Bold, True)
		self.VisLable = QtGui.QLabel('QuantViz', self)
		self.SpacerItem1 = Qt.QSpacerItem(450,0,Qt.QSizePolicy.Fixed,Qt.QSizePolicy.Expanding)		
		self.SpacerItem2 = Qt.QSpacerItem(300,0,Qt.QSizePolicy.Fixed,Qt.QSizePolicy.Expanding)	
		self.SpacerItem3 = Qt.QSpacerItem(300,0,Qt.QSizePolicy.Fixed,Qt.QSizePolicy.Expanding)	
		self.SpacerItem4 = Qt.QSpacerItem(1,500,Qt.QSizePolicy.Fixed)	
		self.VisLable.setFont(self.font3)
		self.FactorLable.setFont(self.font)


########### Region for declaring the varibles associated with X Axis ########################

		self.XLable=QtGui.QLabel('X', self)
		self.XLable.setFont(self.font2)

		self.XCombo = QtGui.QComboBox(self)
		self.XCombo.activated[str].connect(self.XComboActivated)

		self.XMinTag=QtGui.QLabel('Min :', self)
		self.XMaxTag=QtGui.QLabel('Max :', self)
		self.XLimitTag=QtGui.QLabel('Scale:', self)
		self.XSliceTag=QtGui.QLabel('Slice :', self)

		self.XMinLable=QtGui.QLabel(str(self.XMin), self)
		self.XMinLable.setFont(self.font1)
		
		self.XRange=RangeSlider(Qt.Qt.Horizontal)
		self.XRangeSlice=RangeSlider(Qt.Qt.Horizontal)

		self.XMaxLable=QtGui.QLabel(str(self.XMax), self)
		self.XMaxLable.setFont(self.font1)
		
		self.XMin_Box= QtGui.QLineEdit()	
		self.XMax_Box= QtGui.QLineEdit()	
		self.XMinSlice_Box= QtGui.QLineEdit()	
		self.XMaxSlice_Box= QtGui.QLineEdit()

		self.XMin_Box.setMaxLength(4)
		self.XMax_Box.setMaxLength(4)
		self.XMinSlice_Box.setMaxLength(4)	
		self.XMaxSlice_Box.setMaxLength(4)
		
		self.XMin_Box.setFixedSize(50,27)
		self.XMax_Box.setFixedSize(50,27)
		self.XMinSlice_Box.setFixedSize(50,27)	
		self.XMaxSlice_Box.setFixedSize(50,27)

		self.connect(self.XMin_Box, QtCore.SIGNAL('editingFinished()'), self.XMin_BoxInput)
		self.connect(self.XMax_Box, QtCore.SIGNAL('editingFinished()'), self.XMax_BoxInput)
		self.connect(self.XMinSlice_Box, QtCore.SIGNAL('editingFinished()'), self.XMinSlice_BoxInput)
		self.connect(self.XMaxSlice_Box, QtCore.SIGNAL('editingFinished()'), self.XMaxSlice_BoxInput)

############# GUI Box - Related to X ###################

		Xhbox1 = QtGui.QHBoxLayout()
        
		for w in [ self.XLable, self.XCombo]:
			Xhbox1.addWidget(w)
			Xhbox1.setAlignment(w, QtCore.Qt.AlignVCenter)
		Xhbox1.addStretch(1)

		Xhbox2 = QtGui.QHBoxLayout()
        
		for w in [self.XMinTag, self.XMinLable]:
			Xhbox2.addWidget(w)
			Xhbox2.setAlignment(w, QtCore.Qt.AlignVCenter)
		Xhbox2.addStretch(1)
		for w in [self.XMaxTag, self.XMaxLable]:
			Xhbox2.addWidget(w)
			Xhbox2.setAlignment(w, QtCore.Qt.AlignVCenter)

		Xhbox3 = QtGui.QHBoxLayout()
        
		for w in [  self.XLimitTag ,self.XMin_Box, self.XRange, self.XMax_Box]:
			Xhbox3.addWidget(w)
			Xhbox3.setAlignment(w, QtCore.Qt.AlignVCenter)

		Xhbox4 = QtGui.QHBoxLayout()
        
		for w in [  self.XSliceTag, self.XMinSlice_Box, self.XRangeSlice, self.XMaxSlice_Box]:
			Xhbox4.addWidget(w)
			Xhbox4.setAlignment(w, QtCore.Qt.AlignVCenter)

		Xvbox1 = QtGui.QVBoxLayout()
		Xvbox1.addLayout(Xhbox1)
		Xvbox1.addLayout(Xhbox2)
		Xvbox1.addLayout(Xhbox3)
		Xvbox1.addLayout(Xhbox4)

########### Region for declaring the varibles associated with Y Axis ########################

		self.YLable=QtGui.QLabel('Y', self)
		self.YLable.setFont(self.font2)

		self.YCombo = QtGui.QComboBox(self)
		self.YCombo.activated[str].connect(self.YComboActivated)

		self.YMinTag=QtGui.QLabel('Min :', self)
		self.YMaxTag=QtGui.QLabel('Max :', self)
		self.YLimitTag=QtGui.QLabel('Scale:', self)
		self.YSliceTag=QtGui.QLabel('Slice :', self)

		self.YMinLable=QtGui.QLabel(str(self.YMin), self)
		self.YMinLable.setFont(self.font1)
		
		self.YRange=RangeSlider(Qt.Qt.Horizontal)
		self.YRangeSlice=RangeSlider(Qt.Qt.Horizontal)

		self.YMaxLable=QtGui.QLabel(str(self.YMax), self)
		self.YMaxLable.setFont(self.font1)
		
		self.YMin_Box= QtGui.QLineEdit()	
		self.YMax_Box= QtGui.QLineEdit()	
		self.YMinSlice_Box= QtGui.QLineEdit()	
		self.YMaxSlice_Box= QtGui.QLineEdit()

		self.YMin_Box.setMaxLength(4)
		self.YMax_Box.setMaxLength(4)
		self.YMinSlice_Box.setMaxLength(4)	
		self.YMaxSlice_Box.setMaxLength(4)
		
		self.YMin_Box.setFixedSize(50,27)
		self.YMax_Box.setFixedSize(50,27)
		self.YMinSlice_Box.setFixedSize(50,27)	
		self.YMaxSlice_Box.setFixedSize(50,27)

		self.connect(self.YMin_Box, QtCore.SIGNAL('editingFinished()'), self.YMin_BoxInput)
		self.connect(self.YMax_Box, QtCore.SIGNAL('editingFinished()'), self.YMax_BoxInput)
		self.connect(self.YMinSlice_Box, QtCore.SIGNAL('editingFinished()'), self.YMinSlice_BoxInput)
		self.connect(self.YMaxSlice_Box, QtCore.SIGNAL('editingFinished()'), self.YMaxSlice_BoxInput)

############# GUI Box - Related to Y ###################

		Yhbox1 = QtGui.QHBoxLayout()
        
		for w in [ self.YLable, self.YCombo]:
			Yhbox1.addWidget(w)
			Yhbox1.setAlignment(w, QtCore.Qt.AlignVCenter)
		Yhbox1.addStretch(1)

		Yhbox2 = QtGui.QHBoxLayout()
        
		for w in [self.YMinTag, self.YMinLable]:
			Yhbox2.addWidget(w)
			Yhbox2.setAlignment(w, QtCore.Qt.AlignVCenter)
		Yhbox2.addStretch(1)
		for w in [self.YMaxTag, self.YMaxLable]:
			Yhbox2.addWidget(w)
			Yhbox2.setAlignment(w, QtCore.Qt.AlignVCenter)

		Yhbox3 = QtGui.QHBoxLayout()
        
		for w in [ self.YLimitTag, self.YMin_Box, self.YRange, self.YMax_Box]:
			Yhbox3.addWidget(w)
			Yhbox3.setAlignment(w, QtCore.Qt.AlignVCenter)

		Yhbox4 = QtGui.QHBoxLayout()
        
		for w in [  self.YSliceTag,self.YMinSlice_Box, self.YRangeSlice, self.YMaxSlice_Box]:
			Yhbox4.addWidget(w)
			Yhbox4.setAlignment(w, QtCore.Qt.AlignVCenter)

		Yvbox1 = QtGui.QVBoxLayout()
		Yvbox1.addLayout(Yhbox1)
		Yvbox1.addLayout(Yhbox2)
		Yvbox1.addLayout(Yhbox3)
		Yvbox1.addLayout(Yhbox4)

########### Region for declaring the varibles associated with Z Axis ########################

		self.ZLable=QtGui.QLabel('Z', self)
		self.ZLable.setFont(self.font2)		
	
		self.ZCombo = QtGui.QComboBox(self)
		self.ZCombo.activated[str].connect(self.ZComboActivated)
	
		self.ZMinTag=QtGui.QLabel('Min :', self)
		self.ZMaxTag=QtGui.QLabel('Max :', self)
		self.ZLimitTag=QtGui.QLabel('Scale:', self)
		self.ZSliceTag=QtGui.QLabel('Slice :', self)

		self.ZMinLable=QtGui.QLabel(str(self.ZMin), self)
		self.ZMinLable.setFont(self.font1)
		
		self.ZRange=RangeSlider(Qt.Qt.Horizontal)
		self.ZRangeSlice=RangeSlider(Qt.Qt.Horizontal)

		self.ZMaxLable=QtGui.QLabel(str(self.ZMax), self)
		self.ZMaxLable.setFont(self.font1)
		
		self.ZMin_Box= QtGui.QLineEdit()	
		self.ZMax_Box= QtGui.QLineEdit()	
		self.ZMinSlice_Box= QtGui.QLineEdit()	
		self.ZMaxSlice_Box= QtGui.QLineEdit()

		self.ZMin_Box.setMaxLength(4)
		self.ZMax_Box.setMaxLength(4)
		self.ZMinSlice_Box.setMaxLength(4)	
		self.ZMaxSlice_Box.setMaxLength(4)
		
		self.ZMin_Box.setFixedSize(50,27)
		self.ZMax_Box.setFixedSize(50,27)
		self.ZMinSlice_Box.setFixedSize(50,27)	
		self.ZMaxSlice_Box.setFixedSize(50,27)

		self.connect(self.ZMin_Box, QtCore.SIGNAL('editingFinished()'), self.ZMin_BoxInput)
		self.connect(self.ZMax_Box, QtCore.SIGNAL('editingFinished()'), self.ZMax_BoxInput)
		self.connect(self.ZMinSlice_Box, QtCore.SIGNAL('editingFinished()'), self.ZMinSlice_BoxInput)
		self.connect(self.ZMaxSlice_Box, QtCore.SIGNAL('editingFinished()'), self.ZMaxSlice_BoxInput)

############# GUI Box - Related to Z ###################

		Zhbox1 = QtGui.QHBoxLayout()
        
		for w in [ self.ZLable, self.ZCombo]:
			Zhbox1.addWidget(w)
			Zhbox1.setAlignment(w, QtCore.Qt.AlignVCenter)
		Zhbox1.addStretch(1)

		Zhbox2 = QtGui.QHBoxLayout()
        
		for w in [self.ZMinTag, self.ZMinLable]:
			Zhbox2.addWidget(w)
			Zhbox2.setAlignment(w, QtCore.Qt.AlignVCenter)
		Zhbox2.addStretch(1)
		for w in [self.ZMaxTag, self.ZMaxLable]:
			Zhbox2.addWidget(w)
			Zhbox2.setAlignment(w, QtCore.Qt.AlignVCenter)

		Zhbox3 = QtGui.QHBoxLayout()
        
		for w in [ self.ZLimitTag ,self.ZMin_Box, self.ZRange, self.ZMax_Box]:
			Zhbox3.addWidget(w)
			Zhbox3.setAlignment(w, QtCore.Qt.AlignVCenter)

		Zhbox4 = QtGui.QHBoxLayout()
        
		for w in [  self.ZSliceTag,self.ZMinSlice_Box, self.ZRangeSlice, self.ZMaxSlice_Box]:
			Zhbox4.addWidget(w)
			Zhbox4.setAlignment(w, QtCore.Qt.AlignVCenter)

		Zvbox1 = QtGui.QVBoxLayout()
		Zvbox1.addLayout(Zhbox1)
		Zvbox1.addLayout(Zhbox2)
		Zvbox1.addLayout(Zhbox3)
		Zvbox1.addLayout(Zhbox4)

########### Region for declaring the varibles associated with Size ########################

		self.SizeLable=QtGui.QLabel('Size   ', self)
		self.SizeLable.setFont(self.font2)

		self.SizeCombo = QtGui.QComboBox(self)
		self.SizeCombo.activated[str].connect(self.SComboActivated)

		self.SMinTag=QtGui.QLabel('Min :', self)
		self.SMaxTag=QtGui.QLabel('Max :', self)
		self.SLimitTag=QtGui.QLabel('Scale:', self)
		self.SSliceTag=QtGui.QLabel('Slice :', self)

		self.SMinLable=QtGui.QLabel(str(self.SMin), self)
		self.SMinLable.setFont(self.font1)
		
		self.SRange=RangeSlider(Qt.Qt.Horizontal)
		self.SRangeSlice=RangeSlider(Qt.Qt.Horizontal)

		self.SMaxLable=QtGui.QLabel(str(self.SMax), self)
		self.SMaxLable.setFont(self.font1)

		self.SMin_Box= QtGui.QLineEdit()	
		self.SMax_Box= QtGui.QLineEdit()	
		self.SMin_Box.setMaxLength(4)
		self.SMax_Box.setMaxLength(4)
		self.SMin_Box.setFixedSize(50,27)
		self.SMax_Box.setFixedSize(50,27)

		self.connect(self.SMin_Box, QtCore.SIGNAL('editingFinished()'), self.SMin_BoxInput)
		self.connect(self.SMax_Box, QtCore.SIGNAL('editingFinished()'), self.SMax_BoxInput)

		self.SMinSlice_Box= QtGui.QLineEdit()	
		self.SMaxSlice_Box= QtGui.QLineEdit()
		self.SMinSlice_Box.setMaxLength(4)	
		self.SMaxSlice_Box.setMaxLength(4)
		self.SMinSlice_Box.setFixedSize(50,27)	
		self.SMaxSlice_Box.setFixedSize(50,27)

		self.connect(self.SMinSlice_Box, QtCore.SIGNAL('editingFinished()'), self.SMinSlice_BoxInput)
		self.connect(self.SMaxSlice_Box, QtCore.SIGNAL('editingFinished()'), self.SMaxSlice_BoxInput)

############# GUI Box - Related to Size ###################

		Shbox1 = QtGui.QHBoxLayout()

		for w in [  self.SizeLable, self.SizeCombo]:
			Shbox1.addWidget(w)
			Shbox1.setAlignment(w, QtCore.Qt.AlignVCenter)
		
		Shbox1.addStretch(1)

		Shbox2 = QtGui.QHBoxLayout()
        
		for w in [self.SMinTag, self.SMinLable]:
			Shbox2.addWidget(w)
			Shbox2.setAlignment(w, QtCore.Qt.AlignVCenter)
		Shbox2.addStretch(1)
		for w in [self.SMaxTag, self.SMaxLable]:
			Shbox2.addWidget(w)
			Shbox2.setAlignment(w, QtCore.Qt.AlignVCenter)

		Shbox3 = QtGui.QHBoxLayout()
        
		for w in [ self.SLimitTag ,self.SMin_Box, self.SRange, self.SMax_Box]:
			Shbox3.addWidget(w)
			Shbox3.setAlignment(w, QtCore.Qt.AlignVCenter)

		Shbox4 = QtGui.QHBoxLayout()
        
		for w in [  self.SSliceTag, self.SMinSlice_Box, self.SRangeSlice, self.SMaxSlice_Box]:
			Shbox4.addWidget(w)
			Shbox4.setAlignment(w, QtCore.Qt.AlignVCenter)

		Svbox1 = QtGui.QVBoxLayout()
		Svbox1.addLayout(Shbox1)
		Svbox1.addLayout(Shbox2)
		Svbox1.addLayout(Shbox3)
		Svbox1.addLayout(Shbox4)

########### Region for declaring the varibles associated with Color ########################

		self.ColorLable=QtGui.QLabel('Color', self)
		self.ColorLable.setFont(self.font2)

		self.ColorCombo = QtGui.QComboBox(self)
		self.ColorCombo.activated[str].connect(self.CComboActivated)

		self.CMinTag=QtGui.QLabel('Min :', self)
		self.CMaxTag=QtGui.QLabel('Max :', self)
		self.CLimitTag=QtGui.QLabel('Scale:', self)
		self.CSliceTag=QtGui.QLabel('Slice :', self)

		self.CMinLable=QtGui.QLabel(str(self.CMin), self)
		self.CMinLable.setFont(self.font1)
		
		self.CRange=RangeSlider(Qt.Qt.Horizontal)
		self.CRangeSlice=RangeSlider(Qt.Qt.Horizontal)

		self.CMaxLable=QtGui.QLabel(str(self.CMax), self)
		self.CMaxLable.setFont(self.font1)

		self.CMin_Box= QtGui.QLineEdit()	
		self.CMax_Box= QtGui.QLineEdit()	
		self.CMin_Box.setMaxLength(4)
		self.CMax_Box.setMaxLength(4)
		self.CMin_Box.setFixedSize(50,27)
		self.CMax_Box.setFixedSize(50,27)

		self.connect(self.CMin_Box, QtCore.SIGNAL('editingFinished()'), self.CMin_BoxInput)
		self.connect(self.CMax_Box, QtCore.SIGNAL('editingFinished()'), self.CMax_BoxInput)

		self.CMinSlice_Box= QtGui.QLineEdit()	
		self.CMaxSlice_Box= QtGui.QLineEdit()
		self.CMinSlice_Box.setMaxLength(4)	
		self.CMaxSlice_Box.setMaxLength(4)
		self.CMinSlice_Box.setFixedSize(50,27)	
		self.CMaxSlice_Box.setFixedSize(50,27)

		self.connect(self.CMinSlice_Box, QtCore.SIGNAL('editingFinished()'), self.CMinSlice_BoxInput)
		self.connect(self.CMaxSlice_Box, QtCore.SIGNAL('editingFinished()'), self.CMaxSlice_BoxInput)


############# GUI Box - Related to Color ###################

		Chbox1 = QtGui.QHBoxLayout()

		for w in [self.ColorLable, self.ColorCombo]:
			Chbox1.addWidget(w)
			Chbox1.setAlignment(w, QtCore.Qt.AlignVCenter)

		Chbox1.addStretch(1)
		Chbox2 = QtGui.QHBoxLayout()
        
		for w in [self.CMinTag, self.CMinLable]:
			Chbox2.addWidget(w)
			Chbox2.setAlignment(w, QtCore.Qt.AlignVCenter)
		Chbox2.addStretch(2)
		for w in [self.CMaxTag, self.CMaxLable]:
			Chbox2.addWidget(w)
			Chbox2.setAlignment(w, QtCore.Qt.AlignVCenter)

		Chbox3 = QtGui.QHBoxLayout()
        
		for w in [ self.CLimitTag ,self.CMin_Box, self.CRange, self.CMax_Box]:
			Chbox3.addWidget(w)
			Chbox3.setAlignment(w, QtCore.Qt.AlignVCenter)

		Chbox4 = QtGui.QHBoxLayout()
        
		for w in [  self.CSliceTag, self.CMinSlice_Box, self.CRangeSlice, self.CMaxSlice_Box]:
			Chbox4.addWidget(w)
			Chbox4.setAlignment(w, QtCore.Qt.AlignVCenter)

		Cvbox1 = QtGui.QVBoxLayout()
		Cvbox1.addLayout(Chbox1)
		Cvbox1.addLayout(Chbox2)
		Cvbox1.addLayout(Chbox3)
		Cvbox1.addLayout(Chbox4)

############# GUI Lines - All horizontal and Vertical lines are declared here ###################

		self.Frame1= QtGui.QFrame()
		self.Frame1.setFrameShape(4)

		self.Frame2= QtGui.QFrame()
		self.Frame2.setFrameShape(4)

		self.Frame3= QtGui.QFrame()
		self.Frame3.setFrameShape(4)

		self.Frame4= QtGui.QFrame()
		self.Frame4.setFrameShape(4)

		self.Frame5= QtGui.QFrame()
		self.Frame5.setFrameShape(4)

		self.Frame6= QtGui.QFrame()
		self.Frame6.setFrameShape(4)

		self.Frame7= QtGui.QFrame()
		self.Frame7.setFrameShape(4)

		self.Frame8= QtGui.QFrame()
		self.Frame8.setFrameShape(4)

		self.VFrame= QtGui.QFrame()
		self.VFrame.setFrameShape(5)

		self.VFrame1= QtGui.QFrame()
		self.VFrame1.setFrameShape(5)

############# Region to declare variables related to Date ###################

		self.DLable=QtGui.QLabel('Time ', self)
		self.DLable.setFont(self.font2)

		self.DateLable=QtGui.QLabel(self.dayofplot.date().isoformat(), self)
		self.DateLable.setFont(self.font1)

		self.DateMinLable=QtGui.QLabel(self.startday.date().isoformat(), self)
		self.DateMinLable.setFont(self.font1)

		self.DateSlider= QtGui.QSlider(QtCore.Qt.Horizontal, self)
		self.DateSlider.setRange(0,len(self.timestamps)-1)
		self.DateSlider.valueChanged.connect(self.DateActivated)

		self.DateMaxLable=QtGui.QLabel(self.endday.date().isoformat(), self)
		self.DateMaxLable.setFont(self.font1)

############# GUI Box - Related to Date ###################
		
		Datehbox1 = QtGui.QHBoxLayout()
		Datehbox1.addWidget(self.DLable)
		Datehbox1.addWidget(self.DateLable)
		Datehbox1.addStretch(1)

		Datehbox2 = QtGui.QHBoxLayout()
        
		for w in [  self.DateMinLable, self.DateSlider, self.DateMaxLable]:
			Datehbox2.addWidget(w)
			Datehbox2.setAlignment(w, QtCore.Qt.AlignVCenter)

		Datevbox1 = QtGui.QVBoxLayout()
		Datevbox1.addLayout(Datehbox1)
		Datevbox1.addLayout(Datehbox2)

		Datehbox3 = QtGui.QHBoxLayout()
		Datehbox3.addLayout(Datevbox1)
		Datehbox3.addWidget(self.VFrame1)	

############# Region for declaring all the Checkboxes for GUI ###################

		self.TextCheck = QtGui.QCheckBox('Show Label', self)
		self.Day5Check = QtGui.QCheckBox(str(LOOKBACK_DAYS)+' Days', self)
		self.SizeCheck = QtGui.QCheckBox('Fix Size', self)
		self.ColorCheck = QtGui.QCheckBox('Fix Color', self)
		self.MovieCheck = QtGui.QCheckBox('Smooth for Movie', self)

		Checkhbox1 = QtGui.QVBoxLayout()
		Checkhbox1.addWidget(self.TextCheck)
		Checkhbox1.addWidget(self.Day5Check)
		Checkhbox1.addWidget(self.SizeCheck)
		Checkhbox1.addWidget(self.ColorCheck)
		Checkhbox1.addWidget(self.MovieCheck)

############# Region for Declaring all the buttons in the GUI ###################

		self.UpdateButton =QtGui.QPushButton('Plot',self)
		self.UpdateButton.setToolTip('Update the plot')
		self.UpdateButton.resize(self.UpdateButton.sizeHint())
		self.UpdateButton.clicked.connect(self.PlotCanvas)

		self.SaveButton =QtGui.QPushButton('Save Plot',self)
		self.SaveButton.setToolTip('Save the plot')
		self.SaveButton.resize(self.SaveButton.sizeHint())
		self.SaveButton.clicked.connect(self.save_plot)

		self.MovieButton =QtGui.QPushButton('Movie',self)
		self.MovieButton.setToolTip('Make a movie over time')
		self.MovieButton.resize(self.MovieButton.sizeHint())
		self.MovieButton.clicked.connect(self.make_movie)

		self.AboutButton =QtGui.QPushButton('About',self)
		self.AboutButton.setToolTip('About the Visualizer')
		self.AboutButton.resize(self.AboutButton.sizeHint())
		self.AboutButton.clicked.connect(self.on_about)

		self.DataButton =QtGui.QPushButton('Change Data',self)
		self.DataButton.setToolTip('Load new Data')
		self.DataButton.resize(self.DataButton.sizeHint())
		self.DataButton.clicked.connect(self.ChangeDataset)

		self.ResetButton =QtGui.QPushButton('Reset',self)
		self.ResetButton.setToolTip('Reset Settings')
		self.ResetButton.resize(self.ResetButton.sizeHint())
		self.ResetButton.clicked.connect(self.ResetSettings)

		self.ExitButton =QtGui.QPushButton('Exit',self)
		self.ExitButton.setToolTip('Exit')
		self.ExitButton.resize(self.ExitButton.sizeHint())
		self.ExitButton.clicked.connect(QtGui.qApp.quit)

		self.SaveSettingsButton =QtGui.QPushButton('Save Settings',self)
		self.SaveSettingsButton.setToolTip('Save Settings')
		self.SaveSettingsButton.resize(self.SaveSettingsButton.sizeHint())
		self.SaveSettingsButton.clicked.connect(self.SaveSettings)

		self.LoadSettingsButton =QtGui.QPushButton('Load Settings',self)
		self.LoadSettingsButton.setToolTip('Load Settings')
		self.LoadSettingsButton.resize(self.LoadSettingsButton.sizeHint())
		self.LoadSettingsButton.clicked.connect(self.LoadSettings)

############# GUI Box - Related to Button Bar on the top ###################

		Buttonbox = QtGui.QHBoxLayout()
		Buttonbox.addLayout(Checkhbox1)
		Buttonbox.addWidget(self.UpdateButton)

		Buttonbox2 = QtGui.QHBoxLayout()
		Buttonbox2.addWidget(self.ExitButton)
		Buttonbox2.addWidget(self.AboutButton)
		Buttonbox2.addWidget(self.SaveButton)		
		Buttonbox2.addWidget(self.MovieButton)
		Buttonbox2.addWidget(self.DataButton)
		Buttonbox2.addWidget(self.ResetButton)
		Buttonbox2.addWidget(self.SaveSettingsButton)
		Buttonbox2.addWidget(self.LoadSettingsButton)
		Buttonbox2.addStretch()


############# Layout settings in the GUI - Arrangement of Everything ###################

		Vbox1 = QtGui.QVBoxLayout()
		Vbox1.addWidget(self.VisLable)
		Vbox1.addWidget(self.canvas)
		Vbox1.addWidget(self.Frame7)
		Vbox1.addLayout(Datehbox3)
		Vbox1.addItem(self.SpacerItem1)
		Vbox1.addStretch(1)

		Vbox2= QtGui.QVBoxLayout()
		Vbox2.addWidget(self.Frame1)
		Vbox2.addLayout(Xvbox1)
		Vbox2.addWidget(self.Frame2)
		Vbox2.addLayout(Yvbox1)
		Vbox2.addWidget(self.Frame3)
		Vbox2.addLayout(Zvbox1)
		Vbox2.addStretch(1)
		Vbox2.addItem(self.SpacerItem2)

		Vbox3 = QtGui.QVBoxLayout()
		Vbox3.addWidget(self.Frame4)
		Vbox3.addLayout(Svbox1)
		Vbox3.addWidget(self.Frame5)
		Vbox3.addLayout(Cvbox1)
		Vbox3.addWidget(self.Frame6)
		Vbox3.addLayout(Buttonbox)
		Vbox3.addStretch(1)
		Vbox3.addItem(self.SpacerItem3)

		HBox1 = QtGui.QHBoxLayout()
		HBox1.addLayout(Vbox2)
		HBox1.addWidget(self.VFrame)
		HBox1.addLayout(Vbox3)

		Vbox4 = QtGui.QVBoxLayout()
		Vbox4.addStretch(1)
		Vbox4.addWidget(self.FactorLable)
		Vbox4.addLayout(HBox1)
		Vbox4.addStretch(1)

		HBox2= QtGui.QHBoxLayout()
		HBox2.addLayout(Vbox1)
		HBox2.addLayout(Vbox4)
		HBox2.addItem(self.SpacerItem4)
		HBox2.addStretch(1)

		FinalBox = QtGui.QVBoxLayout()
		FinalBox.addLayout(Buttonbox2)
		FinalBox.addLayout(HBox2)
		FinalBox.addStretch(1)

		self.setWindowTitle('QuantViz')
		self.setWindowIcon(QtGui.QIcon('V.png'))
		self.main_frame.setLayout(FinalBox)
		self.setCentralWidget(self.main_frame)
		self.statusBar().showMessage('Ready')

		'''
		All functions of the class start here.
		'''

############### Function to Load Data from the Dataset ####################

	def LoadData(self):
		fname = str(QtGui.QFileDialog.getExistingDirectory(None, 'Data Directory', os.environ['QS']+'/Tools/Visualizer/Data/', options=QtGui.QFileDialog.DontUseNativeDialog))
		fname= fname+'/'
		(self.PandasObject, self.featureslist, self.symbols, self.timestamps, self.dMinFeat, self.dMaxFeat, self.startday, self.endday) = AD.GetData(fname)

############### Function to Reset all the variables ####################

	def Reset(self):
		self.scatterpts=[]
		self.textpts=[]
		self.scatterpts2=[]
		self.textpts2=[]
		self.Xfeature=self.featureslist[0]
		self.Yfeature=self.featureslist[0]
		self.Zfeature=self.featureslist[0]
		self.Sfeature=self.featureslist[0]
		self.Cfeature=self.featureslist[0]
		self.XMin=0.0
		self.XMax=1.0
		self.XLow=self.XMin
		self.XHigh=self.XMax
		self.XLowSlice=self.XMin
		self.XHighSlice=self.XMax
		self.YMin=0.0
		self.YMax=1.0
		self.YLow=self.YMin
		self.YHigh=self.YMax
		self.YLowSlice=self.YMin
		self.YHighSlice=self.YMax
		self.ZMin=0.0
		self.ZMax=1.0
		self.ZLow=self.ZMin
		self.ZHigh=self.ZMax
		self.ZLowSlice=self.ZMin
		self.ZHighSlice=self.ZMax
		self.SMin=0.0
		self.SMax=1.0
		self.SLow=self.SMin
		self.SHigh=self.SMax
		self.SLowSlice=self.SMin
		self.SHighSlice=self.SMax
		self.CMin=0.0
		self.CMax=1.0
		self.CLow=self.CMin
		self.CHigh=self.CMax
		self.CLowSlice=self.CMin
		self.CHighSlice=self.CMax
		self.dayofplot=self.timestamps[0]

############### Function to Reset all the GUI sliders and Labels ####################

	def ResetFunc(self):
		self.FeatureComboBox(self.XCombo)
		self.XInitRangeSlider(self.XRange)		
		self.XInitRangeSliderSlice(self.XRangeSlice)
		self.XMaxLable.setText(str(self.XMax))
		self.XMinLable.setText(str(self.XMin))
		self.XMin_Box.setText(str(self.XLow))
		self.XMax_Box.setText(str(self.XHigh))
		self.XMinSlice_Box.setText(str(self.XLowSlice))	
		self.XMaxSlice_Box.setText(str(self.XHighSlice))

		self.FeatureComboBox(self.YCombo)
		self.YInitRangeSlider(self.YRange)
		self.YInitRangeSliderSlice(self.YRangeSlice)
		self.YMaxLable.setText(str(self.YMax))
		self.YMinLable.setText(str(self.YMin))
		self.YMin_Box.setText(str(self.YLow))
		self.YMax_Box.setText(str(self.YHigh))
		self.YMinSlice_Box.setText(str(self.YLowSlice))	
		self.YMaxSlice_Box.setText(str(self.YHighSlice))

		self.FeatureComboBox(self.ZCombo)
		self.ZInitRangeSlider(self.ZRange)
		self.ZInitRangeSliderSlice(self.ZRangeSlice)
		self.ZMaxLable.setText(str(self.ZMax))
		self.ZMinLable.setText(str(self.ZMin))
		self.ZMin_Box.setText(str(self.ZLow))
		self.ZMax_Box.setText(str(self.ZHigh))
		self.ZMinSlice_Box.setText(str(self.ZLowSlice))	
		self.ZMaxSlice_Box.setText(str(self.ZHighSlice))

		self.FeatureComboBox(self.SizeCombo)
		self.SInitRangeSlider(self.SRange)
		self.SInitRangeSliderSlice(self.SRangeSlice)
		self.SMaxLable.setText(str(self.SMax))
		self.SMinLable.setText(str(self.SMin))
		self.SMin_Box.setText(str(self.SLow))
		self.SMax_Box.setText(str(self.SHigh))
		self.SMinSlice_Box.setText(str(self.SLowSlice))	
		self.SMaxSlice_Box.setText(str(self.SHighSlice))

		self.FeatureComboBox(self.ColorCombo)
		self.CInitRangeSlider(self.CRange)
		self.CInitRangeSliderSlice(self.CRangeSlice)
		self.CMaxLable.setText(str(self.CMax))
		self.CMinLable.setText(str(self.CMin))
		self.CMin_Box.setText(str(self.CLow))
		self.CMax_Box.setText(str(self.CHigh))
		self.CMinSlice_Box.setText(str(self.CLowSlice))	
		self.CMaxSlice_Box.setText(str(self.CHighSlice))

		self.DateLable.setText(self.dayofplot.date().isoformat())
		self.DateSlider.setSliderPosition(self.timestamps.index(self.dayofplot))

############### Function to Load Data a new Dataset ####################

	def ChangeDataset(self):
		self.ClearCanvas()
		self.LoadData()
		self.Reset()
		self.ResetFunc()
		self.statusBar().showMessage('Loading data set complete')

############### Function to Reset Dataset ####################

	def ResetSettings(self):
		self.ClearCanvas()
		self.Reset()
		self.ResetFunc()
		self.statusBar().showMessage('Reset Complete')

############### Function to Save current Settings ####################

	def SaveSettings(self):
		fname = str(QtGui.QFileDialog.getSaveFileName(self, 'Save file', os.environ['QS']+'/Tools/Visualizer/Settings/settings.pkl', options=QtGui.QFileDialog.DontUseNativeDialog))
		if fname=='':
			return
		SettingArray=(self.Xfeature,self.Yfeature,self.Zfeature,self.Sfeature,self.Cfeature, self.XMin,self.XMax,self.XLow,self.XHigh,self.XLowSlice,self.XHighSlice,self.YMin,self.YMax,self.YLow,self.YHigh, self.YLowSlice,self.YHighSlice,self.ZMin,self.ZMax,self.ZLow,self.ZHigh,self.ZLowSlice,self.ZHighSlice,self.SMin, self.SMax,self.SLow,self.SHigh,self.SLowSlice,self.SHighSlice,self.CMin,self.CMax,self.CLow,self.CHigh, self.CLowSlice,self.CHighSlice,self.dayofplot)
		
		pickle.dump(SettingArray,open(fname, 'wb' ),-1)
		self.statusBar().showMessage('Saved Settings')

############### Function to Load Settings previously stored ####################

	def LoadSettings(self):
		fname = str(QtGui.QFileDialog.getOpenFileName(self, 'Open file', os.environ['QS']+'/Tools/Visualizer/Settings/', options=QtGui.QFileDialog.DontUseNativeDialog))
		if fname=='':
			return
		(self.Xfeature,self.Yfeature,self.Zfeature,self.Sfeature,self.Cfeature,self.XMin, self.XMax,self.XLow,self.XHigh,self.XLowSlice,self.XHighSlice,self.YMin,self.YMax,self.YLow,self.YHigh,self.YLowSlice,self.YHighSlice, self.ZMin,self.ZMax,self.ZLow,self.ZHigh,self.ZLowSlice,self.ZHighSlice,self.SMin,self.SMax,self.SLow,self.SHigh,self.SLowSlice, self.SHighSlice,self.CMin,self.CMax,self.CLow,self.CHigh,self.CLowSlice,self.CHighSlice,self.dayofplot)=pickle.load(open( fname, 'rb' ))

		self.XCombo.setCurrentIndex(self.XCombo.findText(self.Xfeature))
		self.XMaxLable.setText(str(self.XMax))
		self.XMinLable.setText(str(self.XMin))
		self.XMin_Box.setText(str(self.XLow))
		self.XMax_Box.setText(str(self.XHigh))
		self.XMinSlice_Box.setText(str(self.XLowSlice))	
		self.XMaxSlice_Box.setText(str(self.XHighSlice))

		self.YCombo.setCurrentIndex(self.YCombo.findText(self.Yfeature))
		self.YMaxLable.setText(str(self.YMax))
		self.YMinLable.setText(str(self.YMin))
		self.YMin_Box.setText(str(self.YLow))
		self.YMax_Box.setText(str(self.YHigh))
		self.YMinSlice_Box.setText(str(self.YLowSlice))	
		self.YMaxSlice_Box.setText(str(self.YHighSlice))

		self.ZCombo.setCurrentIndex(self.ZCombo.findText(self.Zfeature))
		self.ZMaxLable.setText(str(self.ZMax))
		self.ZMinLable.setText(str(self.ZMin))
		self.ZMin_Box.setText(str(self.ZLow))
		self.ZMax_Box.setText(str(self.ZHigh))
		self.ZMinSlice_Box.setText(str(self.ZLowSlice))	
		self.ZMaxSlice_Box.setText(str(self.ZHighSlice))

		self.SizeCombo.setCurrentIndex(self.SizeCombo.findText(self.Sfeature))
		self.SMaxLable.setText(str(self.SMax))
		self.SMinLable.setText(str(self.SMin))
		self.SMin_Box.setText(str(self.SLow))
		self.SMax_Box.setText(str(self.SHigh))
		self.SMinSlice_Box.setText(str(self.SLowSlice))	
		self.SMaxSlice_Box.setText(str(self.SHighSlice))

		self.ColorCombo.setCurrentIndex(self.ColorCombo.findText(self.Cfeature))
		self.CMaxLable.setText(str(self.CMax))
		self.CMinLable.setText(str(self.CMin))
		self.CMin_Box.setText(str(self.CLow))
		self.CMax_Box.setText(str(self.CHigh))
		self.CMinSlice_Box.setText(str(self.CLowSlice))	
		self.CMaxSlice_Box.setText(str(self.CHighSlice))

		self.DateLable.setText(self.dayofplot.date().isoformat())
		self.DateSlider.setSliderPosition(self.timestamps.index(self.dayofplot))
		self.ClearCanvas()
		self.statusBar().showMessage('Loaded Settings')

############### Function for add Features to the Boxes ####################

	def FeatureComboBox(self, combo):
		combo.clear()
		for feat in self.featureslist:
			combo.addItem(feat)
        
############### Function to Check validity of the input into the textbox of sliders ####################

	def correctInput(self, valueEntered, Min, Max, boundcase):
		low = math.floor(((valueEntered- Min)*SLIDER_RANGE)/(Max-Min))
		high= low+1
		lowval = (low*(Max-Min))/SLIDER_RANGE+Min
		highval = (high*(Max-Min))/SLIDER_RANGE+Min
		stepsize = highval - lowval
		if boundcase=='f':
			if (highval-valueEntered)<0.2*stepsize: return high
			else: return low
		if boundcase=='c':
			if (valueEntered-lowval)<0.2*stepsize: return low
			else: return high
		else: return low

############### Region for Functions associated with X Axis ####################

	def XComboActivated(self,text):
		self.Xfeature=text
		self.XMax=self.dMaxFeat[str(self.Xfeature)]
		self.XMin=self.dMinFeat[str(self.Xfeature)]
		self.XMinLable.setText(str(round(self.XMin,1)))
		self.XMaxLable.setText(str(round(self.XMax,1)))
		self.XChangeValues(self.XRange.low(), self.XRange.high())
		self.XChangeValuesSlice(self.XRangeSlice.low(), self.XRangeSlice.high())

	def XChangeValues(self, low, high):
		self.XLow=(low*(self.XMax-self.XMin))/SLIDER_RANGE+self.XMin
		self.XHigh=(high*(self.XMax-self.XMin))/SLIDER_RANGE+self.XMin
		self.XMin_Box.setText(str(round(self.XLow,2)))
		self.XMax_Box.setText(str(round(self.XHigh,2)))

	def XChangeValuesSlice(self, low, high):
		self.XLowSlice=(low*(self.XMax-self.XMin))/SLIDER_RANGE+self.XMin
		self.XHighSlice=(high*(self.XMax-self.XMin))/SLIDER_RANGE+self.XMin	
		self.XMinSlice_Box.setText(str(self.XLowSlice))
		self.XMaxSlice_Box.setText(str(self.XHighSlice))
		
	def XInitRangeSlider(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.XChangeValues)

	def XInitRangeSliderSlice(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.XChangeValuesSlice)

	def XMin_BoxInput(self):
		valueEntered=float(self.XMin_Box.text())
		if valueEntered>self.XHigh or valueEntered<self.XMin:
			self.XMin_Box.setText(str(self.XLow))
			return
		slide = self.correctInput(valueEntered, self.XMin, self.XMax, 'f')
		self.XRange.setLow(slide)
		self.XChangeValues(slide, self.XRange.high())

	def XMinSlice_BoxInput(self):
		valueEntered=float(self.XMinSlice_Box.text())
		if valueEntered>self.XHighSlice or valueEntered<self.XMin:
			self.XMinSlice_Box.setText(str(self.XLowSlice))
			return
		slide = self.correctInput(valueEntered, self.XMin, self.XMax, 'f')
		self.XRangeSlice.setLow(slide)
		self.XChangeValuesSlice(slide, self.XRangeSlice.high())

	def XMax_BoxInput(self):
		valueEntered=float(self.XMax_Box.text())
		if valueEntered<self.XLow or valueEntered>self.XMax:
			self.XMax_Box.setText(str(self.XHigh))
			return
		slide = self.correctInput(valueEntered, self.XMin, self.XMax, 'c')
		self.XRange.setHigh(slide)
		self.XChangeValues(self.XRange.low(),slide)

	def XMaxSlice_BoxInput(self):
		valueEntered=float(self.XMaxSlice_Box.text())
		if valueEntered<self.XLowSlice or valueEntered>self.XMax:
			self.XMaxSlice_Box.setText(str(self.XHighSlice))
			return
		slide = self.correctInput(valueEntered, self.XMin, self.XMax, 'c')
		self.XRangeSlice.setHigh(slide)
		self.XChangeValuesSlice(self.XRangeSlice.low(), slide)

############### Region for Functions associated with Y Axis ####################

	def YComboActivated(self,text):
		self.Yfeature=text
		self.YMax=self.dMaxFeat[str(self.Yfeature)]
		self.YMin=self.dMinFeat[str(self.Yfeature)]
		self.YMinLable.setText(str(round(self.YMin,1)))
		self.YMaxLable.setText(str(round(self.YMax,1)))
		self.YChangeValues(self.YRange.low(), self.YRange.high())
		self.YChangeValuesSlice(self.YRangeSlice.low(), self.YRangeSlice.high())

	def YChangeValues(self, low, high):
		self.YLow=(low*(self.YMax-self.YMin))/SLIDER_RANGE+self.YMin
		self.YHigh=(high*(self.YMax-self.YMin))/SLIDER_RANGE+self.YMin
		self.YMin_Box.setText(str(round(self.YLow,2)))
		self.YMax_Box.setText(str(round(self.YHigh,2)))	

	def YChangeValuesSlice(self, low, high):
		self.YLowSlice=(low*(self.YMax-self.YMin))/SLIDER_RANGE+self.YMin
		self.YHighSlice=(high*(self.YMax-self.YMin))/SLIDER_RANGE+self.YMin		
		self.YMinSlice_Box.setText(str(self.YLowSlice))
		self.YMaxSlice_Box.setText(str(self.YHighSlice))

	def YInitRangeSlider(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.YChangeValues)

	def YInitRangeSliderSlice(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.YChangeValuesSlice)

	def YMin_BoxInput(self):
		valueEntered=float(self.YMin_Box.text())
		if valueEntered>self.YHigh or valueEntered<self.YMin:
			self.YMin_Box.setText(str(self.YLow))
			return
		slide = self.correctInput(valueEntered, self.YMin, self.YMax, 'f')
		self.YRange.setLow(slide)
		self.YChangeValues(slide, self.YRange.high())

	def YMinSlice_BoxInput(self):
		valueEntered=float(self.YMinSlice_Box.text())
		if valueEntered>self.YHighSlice or valueEntered<self.YMin:
			self.YMinSlice_Box.setText(str(self.YLowSlice))
			return
		slide = self.correctInput(valueEntered, self.YMin, self.YMax, 'f')
		self.YRangeSlice.setLow(slide)
		self.YChangeValuesSlice(slide, self.YRangeSlice.high())

	def YMax_BoxInput(self):
		valueEntered=float(self.YMax_Box.text())
		if valueEntered<self.YLow or valueEntered>self.YMax:
			self.YMax_Box.setText(str(self.YHigh))
			return
		slide = self.correctInput(valueEntered, self.YMin, self.YMax, 'c')
		self.YRange.setHigh(slide)
		self.YChangeValues(self.YRange.low(),slide)

	def YMaxSlice_BoxInput(self):
		valueEntered=float(self.YMaxSlice_Box.text())
		if valueEntered<self.YLowSlice or valueEntered>self.YMax:
			self.YMaxSlice_Box.setText(str(self.YHighSlice))
			return
		slide = self.correctInput(valueEntered, self.YMin, self.YMax, 'c')
		self.YRangeSlice.setHigh(slide)
		self.YChangeValuesSlice(self.YRangeSlice.low(), slide)

############### Region for Functions associated with Z Axis ####################

	def ZComboActivated(self,text):
		self.Zfeature=text
		self.ZMax=self.dMaxFeat[str(self.Zfeature)]
		self.ZMin=self.dMinFeat[str(self.Zfeature)]
		self.ZMinLable.setText(str(round(self.ZMin,1)))
		self.ZMaxLable.setText(str(round(self.ZMax,1)))
		self.ZChangeValues(self.ZRange.low(), self.ZRange.high())
		self.ZChangeValuesSlice(self.ZRangeSlice.low(), self.ZRangeSlice.high())

	def ZChangeValues(self, low, high):
		self.ZLow=(low*(self.ZMax-self.ZMin))/SLIDER_RANGE+self.ZMin
		self.ZHigh=(high*(self.ZMax-self.ZMin))/SLIDER_RANGE+self.ZMin	
		self.ZMin_Box.setText(str(round(self.ZLow,2)))
		self.ZMax_Box.setText(str(round(self.ZHigh,2)))		

	def ZChangeValuesSlice(self, low, high):
		self.ZLowSlice=(low*(self.ZMax-self.ZMin))/SLIDER_RANGE+self.ZMin
		self.ZHighSlice=(high*(self.ZMax-self.ZMin))/SLIDER_RANGE+self.ZMin		
		self.ZMinSlice_Box.setText(str(self.ZLowSlice))
		self.ZMaxSlice_Box.setText(str(self.ZHighSlice))

	def ZInitRangeSlider(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.ZChangeValues)

	def ZInitRangeSliderSlice(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.ZChangeValuesSlice)

	def ZMin_BoxInput(self):
		valueEntered=float(self.ZMin_Box.text())
		if valueEntered>self.ZHigh or valueEntered<self.ZMin:
			self.ZMin_Box.setText(str(self.ZLow))
			return
		slide = self.correctInput(valueEntered, self.ZMin, self.ZMax, 'f')
		self.ZRange.setLow(slide)
		self.ZChangeValues(slide, self.ZRange.high())

	def ZMinSlice_BoxInput(self):
		valueEntered=float(self.ZMinSlice_Box.text())
		if valueEntered>self.ZHighSlice or valueEntered<self.ZMin:
			self.ZMinSlice_Box.setText(str(self.ZLowSlice))
			return
		slide = self.correctInput(valueEntered, self.ZMin, self.ZMax, 'f')
		self.ZRangeSlice.setLow(slide)
		self.ZChangeValuesSlice(slide, self.ZRangeSlice.high())

	def ZMax_BoxInput(self):
		valueEntered=float(self.ZMax_Box.text())
		if valueEntered<self.ZLow or valueEntered>self.ZMax:
			self.ZMax_Box.setText(str(self.ZHigh))
			return
		slide = self.correctInput(valueEntered, self.ZMin, self.ZMax, 'c')
		self.ZRange.setHigh(slide)
		self.ZChangeValues(self.ZRange.low(),slide)

	def ZMaxSlice_BoxInput(self):
		valueEntered=float(self.ZMaxSlice_Box.text())
		if valueEntered<self.ZLowSlice or valueEntered>self.ZMax:
			self.ZMaxSlice_Box.setText(str(self.ZHighSlice))
			return
		slide = self.correctInput(valueEntered, self.ZMin, self.ZMax, 'c')
		self.ZRangeSlice.setHigh(slide)
		self.ZChangeValuesSlice(self.ZRangeSlice.low(), slide)

############### Region for Functions associated with Size ####################

	def SComboActivated(self,text):
		self.Sfeature=text
		self.SMax=self.dMaxFeat[str(self.Sfeature)]
		self.SMin=self.dMinFeat[str(self.Sfeature)]
		self.SMinLable.setText(str(round(self.SMin,1)))
		self.SMaxLable.setText(str(round(self.SMax,1)))
		self.SChangeValues(self.SRange.low(), self.SRange.high())
		self.SChangeValuesSlice(self.SRangeSlice.low(), self.SRangeSlice.high())

	def SChangeValues(self, low, high):
		self.SLow=(low*(self.SMax-self.SMin))/SLIDER_RANGE+self.SMin
		self.SHigh=(high*(self.SMax-self.SMin))/SLIDER_RANGE+self.SMin	
		self.SMin_Box.setText(str(round(self.SLow,2)))
		self.SMax_Box.setText(str(round(self.SHigh,2)))			

	def SInitRangeSlider(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.SChangeValues)

	def SMin_BoxInput(self):
		valueEntered=float(self.SMin_Box.text())
		if valueEntered>self.SHigh or valueEntered<self.SMin:
			self.SMin_Box.setText(str(self.SLow))
			return
		slide = self.correctInput(valueEntered, self.SMin, self.SMax, 'f')
		self.SRange.setLow(slide)
		self.SChangeValues(slide, self.SRange.high())

	def SMax_BoxInput(self):
		valueEntered=float(self.SMax_Box.text())
		if valueEntered<self.SLow or valueEntered>self.SMax:
			self.SMax_Box.setText(str(self.SHigh))
			return
		slide = self.correctInput(valueEntered, self.SMin, self.SMax, 'c')
		self.SRange.setHigh(slide)
		self.SChangeValues(self.SRange.low(),slide)

	def SChangeValuesSlice(self, low, high):
		self.SLowSlice=(low*(self.SMax-self.SMin))/SLIDER_RANGE+self.SMin
		self.SHighSlice=(high*(self.SMax-self.SMin))/SLIDER_RANGE+self.SMin	
		self.SMinSlice_Box.setText(str(self.SLowSlice))
		self.SMaxSlice_Box.setText(str(self.SHighSlice))

	def SInitRangeSliderSlice(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.SChangeValuesSlice)

	def SMinSlice_BoxInput(self):
		valueEntered=float(self.SMinSlice_Box.text())
		if valueEntered>self.SHighSlice or valueEntered<self.SMin:
			self.SMinSlice_Box.setText(str(self.SLowSlice))
			return
		slide = self.correctInput(valueEntered, self.SMin, self.SMax, 'f')
		self.SRangeSlice.setLow(slide)
		self.SChangeValuesSlice(slide, self.SRangeSlice.high())

	def SMaxSlice_BoxInput(self):
		valueEntered=float(self.SMaxSlice_Box.text())
		if valueEntered<self.SLowSlice or valueEntered>self.SMax:
			self.SMaxSlice_Box.setText(str(self.SHighSlice))
			return
		slide = self.correctInput(valueEntered, self.SMin, self.SMax, 'c')
		self.SRangeSlice.setHigh(slide)
		self.SChangeValuesSlice(self.SRangeSlice.low(), slide)

############### Region for Functions associated with Color ####################

	def CComboActivated(self,text):
		self.Cfeature=text
		self.CMax=self.dMaxFeat[str(self.Cfeature)]
		self.CMin=self.dMinFeat[str(self.Cfeature)]
		self.CMinLable.setText(str(round(self.CMin,1)))
		self.CMaxLable.setText(str(round(self.CMax,1)))
		self.CChangeValues(self.CRange.low(), self.CRange.high())
		self.CChangeValuesSlice(self.CRangeSlice.low(), self.CRangeSlice.high())

	def CChangeValues(self, low, high):
		self.CLow=(low*(self.CMax-self.CMin))/SLIDER_RANGE+self.CMin
		self.CHigh=(high*(self.CMax-self.CMin))/SLIDER_RANGE+self.CMin	
		self.CMin_Box.setText(str(round(self.CLow,2)))
		self.CMax_Box.setText(str(round(self.CHigh,2)))				

	def CInitRangeSlider(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.CChangeValues)

	def CMin_BoxInput(self):
		valueEntered=float(self.CMin_Box.text())
		if valueEntered>self.CHigh or valueEntered<self.CMin:
			self.CMin_Box.setText(str(self.CLow))
			return
		slide = self.correctInput(valueEntered, self.CMin, self.CMax, 'f')
		self.CRange.setLow(slide)
		self.CChangeValues(slide, self.CRange.high())

	def CMax_BoxInput(self):
		valueEntered=float(self.CMax_Box.text())
		if valueEntered<self.CLow or valueEntered>self.CMax:
			self.CMax_Box.setText(str(self.CHigh))
			return
		slide = self.correctInput(valueEntered, self.CMin, self.CMax, 'c')
		self.CRange.setHigh(slide)
		self.CChangeValues(self.CRange.low(),slide)

	def CChangeValuesSlice(self, low, high):
		self.CLowSlice=(low*(self.CMax-self.CMin))/SLIDER_RANGE+self.CMin
		self.CHighSlice=(high*(self.CMax-self.CMin))/SLIDER_RANGE+self.CMin	
		self.CMinSlice_Box.setText(str(self.CLowSlice))
		self.CMaxSlice_Box.setText(str(self.CHighSlice))

	def CInitRangeSliderSlice(self, slider):
		slider.setMinimum(0)
		slider.setMaximum(SLIDER_RANGE)
		slider.setLow(0)
		slider.setHigh(SLIDER_RANGE)
		QtCore.QObject.connect(slider, QtCore.SIGNAL('sliderMoved'), self.CChangeValuesSlice)

	def CMinSlice_BoxInput(self):
		valueEntered=float(self.CMinSlice_Box.text())
		if valueEntered>self.CHighSlice or valueEntered<self.CMin:
			self.CMinSlice_Box.setText(str(self.CLowSlice))
			return
		slide = self.correctInput(valueEntered, self.CMin, self.CMax, 'f')
		self.CRangeSlice.setLow(slide)
		self.CChangeValuesSlice(slide, self.CRangeSlice.high())

	def CMaxSlice_BoxInput(self):
		valueEntered=float(self.CMaxSlice_Box.text())
		if valueEntered<self.CLowSlice or valueEntered>self.CMax:
			self.CMaxSlice_Box.setText(str(self.CHighSlice))
			return
		slide = self.correctInput(valueEntered, self.CMin, self.CMax, 'c')
		self.CRangeSlice.setHigh(slide)
		self.CChangeValuesSlice(self.CRangeSlice.low(), slide)

############### Function which is called when the date slider is changed ####################

	def DateActivated(self, index):
		self.dayofplot=self.timestamps[index]
		dateval=self.timestamps[index]
		dateval=dateval.date().isoformat()
		self.DateLable.setText(dateval)
		self.DateLable.adjustSize() 

############### Region for Plotting related fuctions ####################

	# Checkes if the value lies between the min and max, otherwise returns a NAN
	def inrange(self, x, minx, maxx):
		if x>=minx and x<=maxx: return x
		else: return np.NAN
	
	# Returns a value to be used in size array based on the scale and slice values
	def inrangeS(self, s, low, lowslice, high, highslice ,absmin, absmax):
		if s<=low: s=low
		elif s>=high: s=high
	
		if s<=lowslice: return 0
		elif s>=highslice: return 0
		else: return (199*((s-absmin)/(absmax-absmin))+10)

	# Returns a value to be used in color array based on the scale and slice values
	def inrangeC(self, c, low, lowslice, high, highslice ,absmin, absmax):
		if c<=low: c=low
		elif c>=high: c=high

		if c<=lowslice: return np.NAN
		elif c>=highslice: return np.NAN
		else: return (501*((c-absmin)/(absmax-absmin))+10)

	#Remove points from the plot - Clear the plot - No actual clear function yet in matplotlib
	def clean(self):
		if self.scatterpts:
			for pt in self.scatterpts :
				pt.remove()
		self.scatterpts = []
		if self.textpts:
			for pt in self.textpts :
				pt.remove()
		self.textpts = []

	# Reading data from the pandas object 
	def readdata(self, day):
		xs=self.PandasObject[str(self.Xfeature)].xs(day)
		ys=self.PandasObject[str(self.Yfeature)].xs(day)
		zs=self.PandasObject[str(self.Zfeature)].xs(day)		
		size=self.PandasObject[str(self.Sfeature)].xs(day)
		color=self.PandasObject[str(self.Cfeature)].xs(day)
		return (xs,ys,zs,size,color)

	# 5 Day Average for smoothning in the movie
	def avg(self,x1,x2,x3,x4,x5):
		return (x1+x2+x3+x4+x5)/5.0

	# Plotting Points - Just draws points - has a lot of work around techniques for bugs in matplotlib
	def PlotPoints(self, day):
		index = self.timestamps.index(day)
		# Check whether smoothning is required or not
		if self.MovieCheck.isChecked() and index<(len(self.timestamps)-2) and index>=2:
			(x1,y1,z1,s1,c1)= self.readdata(self.timestamps[index-2])
			(x2,y2,z2,s2,c2)= self.readdata(self.timestamps[index-1])
			(x3,y3,z3,s3,c3)= self.readdata(self.timestamps[index])
			(x4,y4,z4,s4,c4)= self.readdata(self.timestamps[index+1])
			(x5,y5,z5,s5,c5)= self.readdata(self.timestamps[index+2])
						
			xs= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(x1,x2,x3,x4,x5)]
			ys= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(y1,y2,y3,y4,y5)]
			zs= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(z1,z2,z3,z4,z5)]
			size= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(s1,s2,s3,s4,s5)]
			color= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(c1,c2,c3,c4,c5)]	

		else:
			(xs,ys,zs,size,color) = self.readdata(day)
		
		# Whether the points lie between size and scale values
		xs1 = [self.inrange(x, max(self.XLow, self.XLowSlice), min(self.XHigh, self.XHighSlice)) for x in xs]
		ys1 = [self.inrange(y, max(self.YLow, self.YLowSlice), min(self.YHigh, self.YHighSlice)) for y in ys]
		zs1 = [self.inrange(z, max(self.ZLow, self.ZLowSlice), min(self.ZHigh, self.ZHighSlice)) for z in zs]

		# Check if size is fixed or not
		if self.SizeCheck.isChecked():
			size1 =20
		else: size1 = [self.inrangeS(s, self.SLow, self.SLowSlice, self.SHigh, self.SHighSlice, self.SMin, self.SMax) for s in size]

		# Check if color is fixed or not
		if self.ColorCheck.isChecked():
			color1 = 'g'
		else: color1 = [self.inrangeC(c, self.CLow, self.CLowSlice, self.CHigh, self.CHighSlice, self.CMin, self.CMax) for c in color]

		# Scatter Plot
		pt=self.ax.scatter(xs1,ys1,zs1,marker='o', alpha=1, c=color1, s=size1, edgecolor='none')
		self.scatterpts.append(pt)
		
		# Check if labels need to be put in
		if self.TextCheck.isChecked():
			for x,y,z,l in zip(xs1,ys1,zs1,self.symbols):
				pt=self.ax.text(x,y,z,l)
				self.textpts.append(pt)
		self.datetext.set_text('Date : ' + day.date().isoformat())

	#Function to plot the figure, use the above and set limits and lables. To avoid redoing everything in the 5 day case
	def PlotFigure(self, day):
		self.PlotPoints(day)
		if self.Day5Check.isChecked():
			index = self.timestamps.index(day)
			for i in range(LOOKBACK_DAYS-1):
				if (index-i-1)>=0:
					self.PlotPoints(self.timestamps[index-i-1])

		self.ax.set_xlim(self.XLow, self.XHigh)
		self.ax.set_ylim(self.YLow, self.YHigh)
		self.ax.set_zlim(self.ZLow, self.ZHigh)		
		self.ax.set_xlabel(self.Xfeature)
		self.ax.set_ylabel(self.Yfeature)
		self.ax.set_zlabel(self.Zfeature)

############### Function to clear the canvas ####################

	def ClearCanvas(self):
		self.clean()
		xs=self.PandasObject[str(self.Xfeature)].xs(self.dayofplot)
		pt=self.ax.scatter(xs,xs,xs,marker='o', alpha=0, c='g', s=0)
		self.scatterpts.append(pt)
		self.ax.set_xlim(0, 1)
		self.ax.set_ylim(0, 1)
		self.ax.set_zlim(0, 1)		
		self.ax.set_xlabel(' ')
		self.ax.set_ylabel(' ')
		self.ax.set_zlabel(' ')
		self.canvas.draw()
		self.clean()
		self.statusBar().showMessage('Cleared the Plot')
		
############### Function called when the plot button is pressed ####################

	def PlotCanvas(self):
		self.clean()
		self.PlotFigure(self.dayofplot)
		self.canvas.draw()
		self.statusBar().showMessage('Update the Plot')

############### Functions to plot on the canvas2 - High resolution images for saved image and movie #############33

	#Remove points from the plot - Clear the plot - No actual clear function yet in matplotlib
	def clean2(self):
		if self.scatterpts2:
			for pt in self.scatterpts2 :
				pt.remove()
		self.scatterpts2 = []
		if self.textpts2:
			for pt in self.textpts2 :
				pt.remove()
		self.textpts2 = []

	# Plotting Points - Just draws points - has a lot of work around techniques for bugs in matplotlib
	def PlotPoints2(self, day):
		index = self.timestamps.index(day)
		# Check whether smoothning is required or not
		if self.MovieCheck.isChecked() and index<(len(self.timestamps)-2) and index>=2:
			(x1,y1,z1,s1,c1)= self.readdata(self.timestamps[index-2])
			(x2,y2,z2,s2,c2)= self.readdata(self.timestamps[index-1])
			(x3,y3,z3,s3,c3)= self.readdata(self.timestamps[index])
			(x4,y4,z4,s4,c4)= self.readdata(self.timestamps[index+1])
			(x5,y5,z5,s5,c5)= self.readdata(self.timestamps[index+2])
						
			xs= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(x1,x2,x3,x4,x5)]
			ys= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(y1,y2,y3,y4,y5)]
			zs= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(z1,z2,z3,z4,z5)]
			size= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(s1,s2,s3,s4,s5)]
			color= [ self.avg(a1,a2,a3,a4,a5) for a1,a2,a3,a4,a5 in zip(c1,c2,c3,c4,c5)]	

		else:
			(xs,ys,zs,size,color) = self.readdata(day)
		
		# Whether the points lie between size and scale values
		xs1 = [self.inrange(x, max(self.XLow, self.XLowSlice), min(self.XHigh, self.XHighSlice)) for x in xs]
		ys1 = [self.inrange(y, max(self.YLow, self.YLowSlice), min(self.YHigh, self.YHighSlice)) for y in ys]
		zs1 = [self.inrange(z, max(self.ZLow, self.ZLowSlice), min(self.ZHigh, self.ZHighSlice)) for z in zs]

		# Check if size is fixed or not
		if self.SizeCheck.isChecked():
			size1 =20
		else: size1 = [self.inrangeS(s, self.SLow, self.SLowSlice, self.SHigh, self.SHighSlice, self.SMin, self.SMax) for s in size]

		# Check if color is fixed or not
		if self.ColorCheck.isChecked():
			color1 = 'g'
		else: color1 = [self.inrangeC(c, self.CLow, self.CLowSlice, self.CHigh, self.CHighSlice, self.CMin, self.CMax) for c in color]

		# Scatter Plot
		pt=self.ax2.scatter(xs1,ys1,zs1,marker='o', alpha=1, c=color1, s=size1, edgecolor='none')
		self.scatterpts2.append(pt)
		
		# Check if labels need to be put in
		if self.TextCheck.isChecked():
			for x,y,z,l in zip(xs1,ys1,zs1,self.symbols):
				pt=self.ax2.text(x,y,z,l)
				self.textpts2.append(pt)
		self.datetext2.set_text('Date : ' + day.date().isoformat())

	#Function to plot the figure, use the above and set limits and lables. To avoid redoing everything in the 5 day case
	def PlotFigure2(self, day):
		self.PlotPoints2(day)
		if self.Day5Check.isChecked():
			index = self.timestamps.index(day)
			for i in range(LOOKBACK_DAYS-1):
				if (index-i-1)>=0:
					self.PlotPoints2(self.timestamps[index-i-1])

		self.ax2.set_xlim(self.XLow, self.XHigh)
		self.ax2.set_ylim(self.YLow, self.YHigh)
		self.ax2.set_zlim(self.ZLow, self.ZHigh)		
		self.ax2.set_xlabel(self.Xfeature)
		self.ax2.set_ylabel(self.Yfeature)
		self.ax2.set_zlabel(self.Zfeature)

############### Function to save the current plot ####################

	# Redraw of the Image is to account for the bug in matplotlib which loses the color -> If you have made the bug fix, No need to redraw

	def save_plot(self):
		fname = str(QtGui.QFileDialog.getSaveFileName(self, 'Save file', os.environ['QS']+'/Tools/Visualizer/untitled.png', 'Images (*.png *.xpm *.jpg)', options=QtGui.QFileDialog.DontUseNativeDialog))
		if fname=='':
			return
		self.clean2()
		self.PlotFigure2(self.dayofplot)
		self.canvas2.print_png(fname, dpi=self.dpi*2, facecolor='gray', edgecolor='gray')
		self.statusBar().showMessage('Saved the File')

############### Function to create the movie over time with current settings ####################

	def make_movie(self):
		folderpath = os.environ['QS']+'/Tools/Visualizer/Movie/'
		text, ok = QtGui.QInputDialog.getText(self, 'Input Dialog', 'Enter name of movie:')
		if ok:
			if len(text)<1:
				print "Movie name Invalid"
				return 
			folderpath= folderpath + str(text)
		else:
			return
		if not os.path.exists(folderpath):
			os.mkdir(folderpath)
		
		folderpath=folderpath + '/'

		files_at_this_path = dircache.listdir(folderpath)
		for _file in files_at_this_path:
			if (os.path.isfile(folderpath + _file)):
				os.remove(folderpath + _file)

		for i in range(0, len(self.timestamps)):
			self.clean2()
			self.PlotFigure2(self.timestamps[i])
			fname=folderpath + str(i) +'.png'
			self.canvas2.print_png(fname, dpi=self.dpi*2, facecolor='gray', edgecolor='gray')
		self.statusBar().showMessage('Movie Complete')
	
############### Function which is called when about is pressed ####################

	def on_about(self):
		msg = """ A Data Visualizer for QSTK:
        
         * Use the matplotlib for plotting 
         * Plotting various features of financial data
         * Integrated with Quant software toolkit
         * Save the plot to a file using the File menu
         * For more information visit http://wiki.quantsoftware.org/
		"""
		QtGui.QMessageBox.about(self, "About QuantViz", msg.strip())

#####################################################
''' END OF THE CLASS - THAT WAS AWESOME !! '''

# This is main (The easiest one to write in python)
def main():

	app = QtGui.QApplication(sys.argv)
	ex = Visualizer()
	ex.show()
	sys.exit(app.exec_())


if __name__ == '__main__':
    main()   

########NEW FILE########
__FILENAME__ = classes
'''

(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Nov 7, 2011

@author: John Cornwell
@contact: JohnWCornwellV@gmail.com
@summary: File containing various classification functions

'''

# 3rd Party Imports 
import pandas as pand
import numpy as np

def class_fut_ret( d_data, i_lookforward=21, s_rel=None, b_use_open=False ):
    '''
    @summary: Calculate classification, uses future returns 
    @param d_data: Dictionary of data to use
    @param i_lookforward: Number of days to look in the future
    @param s_rel: Stock symbol that this should be relative to, ususally $SPX.
    @param b_use_open: If True, stock will be purchased at T+1 open, sold at 
        T+i_lookforward close
    @return: DataFrame containing values
    '''
    
    if b_use_open:
        df_val = d_data['open'].copy()
    else:
        df_val = d_data['close'].copy()
    
    na_val = df_val.values

    if b_use_open:
        na_val[:-(i_lookforward + 1), :] = ((na_val[i_lookforward + 1:, :] -
                                       na_val[1:-(i_lookforward), :]) /
                                       na_val[1:-(i_lookforward), :])
        na_val[-(i_lookforward+1):, :] = np.nan
        
    else:
        na_val[:-i_lookforward, :] = ((na_val[i_lookforward:, :] -
                                       na_val[:-i_lookforward, :]) /
                                       na_val[:-i_lookforward, :])
        na_val[-i_lookforward:, :] = np.nan

    return df_val


if __name__ == '__main__':
    pass

########NEW FILE########
__FILENAME__ = features
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.
Created on Nov 7, 2011

@author: John Cornwell
@contact: JohnWCornwellV@gmail.com
@summary: File containing various feature functions

'''

#''' Python imports '''
import random

#''' 3rd Party Imports '''
import pandas as pand
import numpy as np
import datetime as dt

#''' QSTK Imports '''
import QSTK.qstkutil.tsutil as tsu
from QSTK.qstkutil import DataAccess as da
import QSTK.qstkutil.qsdateutil as du

def featMomentum(dData, lLookback=20, b_human=False ):
    '''
    @summary: N day cumulative return (based on 1) indicator
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    dfPrice = dData['close'].copy()
    
    #Calculate Returns
    tsu.returnize0(dfPrice.values)
    
    #Calculate rolling sum
    dfRet = pand.rolling_sum(dfPrice, lLookback)
    
    
    return dfRet

def featHiLow(dData, lLookback=20, b_human=False ):
    '''
    @summary: 1 represents a high for the lookback -1 represents a low
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    dfPrice = dData['close']
    
    #Find Max for each price for lookback
    maxes = pand.rolling_max(dfPrice, lLookback, 1)
    
    #Find Min
    mins = pand.rolling_min(dfPrice, lLookback, 1)
    
    #Find Range
    ranges = maxes - mins
    
    #Calculate (price - min) * 2 / range -1
    dfRet = (((dfPrice-mins)*2)/ranges)-1
    
    return dfRet

def featDate(dData, b_human=False ):
    '''
    @summary: Returns -1 for jan 1st 1 for dec 31st
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    
    dfPrice = dData['close']
    dfRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=np.zeros(dfPrice.shape) )
    
    for sStock in dfPrice.columns:
        tsPrice = dfPrice[sStock]
        tsRet = dfRet[sStock]
        #'' Loop over time '''
        for i in range(len(tsPrice.index)):
            #get current date
            today = tsPrice.index[i]
            
            #get days since January 1st
            days = today - dt.datetime(today.year, 1, 1)
            
            # multiply by 2, divide by 365, subtract 1
            tsRet[i] = float(days.days * 2) / 365 - 1
            
    return dfRet


def featOption(dData, b_human=False ):
    '''
    @summary: Returns 1 if option close is today, -1 if it was yesterday
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    dfPrice = dData['close']
    dfRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=np.zeros(dfPrice.shape) )
    
    for sStock in dfPrice.columns:
        tsPrice = dfPrice[sStock]
        tsRet = dfRet[sStock]
        #'' Loop over time '''
        for i in range(len(tsPrice.index)):
            #get current date
            today = tsPrice.index[i]
            
            #get last option close
            last_close = du.getLastOptionClose(today, tsPrice.index)
            
            #get next option close
            next_close = du.getNextOptionClose(today, tsPrice.index)
            
            #get days between
            days_between = next_close - last_close
            
            #get days since last close
            days = today - last_close
            
            # multiply by 2, divide by 365, subtract 1
            tsRet[i] = float(days.days * 2) / days_between.days - 1
            
    return dfRet

def featMA( dData, lLookback=30, bRel=True, b_human=False ):
    '''
    @summary: Calculate moving average
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    
    dfPrice = dData['close']
    
    dfRet = pand.rolling_mean(dfPrice, lLookback)
    
    if bRel:
        dfRet = dfRet / dfPrice
    if b_human:  
        data2 = dfRet * dData['close']
        data3 = pand.DataFrame({"Raw":data2[data2.columns[0]]})
        for sym in dfRet.columns:
            if sym != '$SPX' and sym != '$VIX':
                data3[sym + " Moving Average"] = data2[sym]
                data3[sym] = dData['close'][sym]
        del data3['Raw']
        return data3
    return dfRet


def featEMA( dData, lLookback=20, bRel=True,  b_human=False ):
    '''
    @summary: Calculate exponential moving average
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    
    dfPrice = dData['close']
    
    dfRet = pand.ewma(dfPrice, span=lLookback)
    
    if bRel:
        dfRet = dfRet / dfPrice;
    if b_human:  
        data2 = dfRet*dData['close']
        data3 = pand.DataFrame({"Raw":data2[data2.columns[0]]})
        for sym in dfRet.columns:
            if sym != '$SPX' and sym != '$VIX':
                data3[sym + " Moving Average"] = data2[sym]
                data3[sym] = dData['close'][sym]
        del data3['Raw']
        return data3          
    return dfRet

def featSTD( dData, lLookback=20, bRel=True,  b_human=False ):
    '''
    @summary: Calculate standard deviation
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    
    dfPrice = dData['close'].copy()
    
    tsu.returnize1(dfPrice.values)
    dfRet = pand.rolling_std(dfPrice, lLookback)
    
    if bRel:
        dfRet = dfRet / dfPrice
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dfRet

def featRSI( dData, lLookback=14,  b_human=False):
    '''
    @summary: Calculate RSI
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to look in the past, 14 is standard
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''

    # create deltas per day
    dfDelta = dData['close'].copy()
    dfDelta.ix[1:,:] -= dfDelta.ix[:-1,:].values
    dfDelta.ix[0,:] = np.NAN

    dfDeltaUp = dfDelta
    dfDeltaDown = dfDelta.copy()
    
    # seperate data into positive and negative for easy calculations
    for sColumn in dfDeltaUp.columns:
        tsColDown = dfDeltaDown[sColumn]
        tsColDown[tsColDown >= 0] = 0 
        
        tsColUp = dfDeltaUp[sColumn]
        tsColUp[tsColUp <= 0] = 0
    
    # Note we take abs() of negative values, all should be positive now
    dfRolUp = pand.rolling_mean(dfDeltaUp, lLookback, min_periods=1)
    dfRolDown = pand.rolling_mean(dfDeltaDown, lLookback, min_periods=1).abs()
    
    # relative strength
    dfRS = dfRolUp / dfRolDown
    dfRSI = 100.0 - (100.0 / (1.0 + dfRS))
    
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    
    return dfRSI


def featDrawDown( dData, lLookback=30,  b_human=False):
    '''
    @summary: Calculate Drawdown for the stock
    @param dData: Dictionary of data to use
    @param lLookback: Days to look back
    @return: DataFrame array containing values
    @param b_human: if true return dataframe to plot
    @warning: Drawdown and RunUp can depend heavily on sample period
    '''
    
    dfPrice = dData['close']
    
    #''' Feature DataFrame will be 1:1, we can use the price as a template '''
    dfRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=np.zeros(dfPrice.shape) )
    
    dfMax = pand.rolling_max(dfPrice, lLookback)
    return (dfMax - dfPrice) / dfMax;
    
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dfRet

def featRunUp( dData, lLookback=30, b_human=False ):
    '''
    @summary: CalculateRunup for the stock
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to calculate min over 
    @return: DataFrame array containing feature values
    @param b_human: if true return dataframe to plot
    @warning: Drawdown and RunUp can depend heavily on when the sample starts
    '''
    
    dfPrice = dData['close']
    
    dfMax = pand.rolling_min(dfPrice, lLookback)
    return dfPrice / dfMax;
            
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dfRet


def featVolumeDelta( dData, lLookback=30, b_human=False ):
    '''
    @summary: Calculate moving average
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to use for MA
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    
    dfVolume = dData['volume']
    
    dfRet = pand.rolling_mean(dfVolume, lLookback)
    dfRet /= dfVolume
        
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']    
    return dfRet

def featAroon( dData, bDown=False, lLookback=25, b_human=False ):
    '''
    @summary: Calculate Aroon - indicator indicating days since a 25-day 
              high/low, weighted between 0 and 100
    @param dData: Dictionary of data to use
    @param bDown: If false, calculates aroonUp (high), else aroonDown (lows)
    @param lLookback: Days to lookback to calculate high/low from
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing feature values
    '''
    
    dfPrice = dData['close']

    #Feature DataFrame will be 1:1, we can use the price as a template
    dfRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, 
                            data=np.zeros(dfPrice.shape) )
    
    #Loop through time
    for i in range(dfPrice.shape[0]):
        if( (i-lLookback) < 0 ):
            dfRet.ix[i,:] = np.NAN
        else:
            if bDown:
                dfRet.ix[i,:] = dfPrice.values[i:(i-lLookback):-1,:].argmin(
                                axis=0)
            else:
                dfRet.ix[i,:] = dfPrice.values[i:(i-lLookback):-1,:].argmax(
                                axis=0)
    
    dfRet = ((lLookback - 1.) - dfRet) / (lLookback - 1.) * 100.

    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dfRet


def featAroonDown( dData, lLookback=25, b_human=False ):
    '''
    @summary: Wrapper to call aroon with flag = true
    '''
    return featAroon(dData, bDown=True, lLookback=lLookback, b_human=b_human)


def featStochastic( dData, lLookback=14, bFast=True, lMA=3, b_human=False ):
    '''
    @summary: Calculate stochastic oscillator - indicates what range of recent low-high spread we are in.
    @param dData: Dictionary of data to use
    @param bFast: If false, do slow stochastics, 3 day MA, if not use fast, no MA
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing feature values
    '''

    dfLow = dData['low']
    dfHigh = dData['high']
    dfPrice = dData['close']

    
    #''' Loop through stocks '''
    dfLows = pand.rolling_min(dfLow, lLookback)
    dfHighs = pand.rolling_max(dfHigh, lLookback)
    
    dfStoch = (dfPrice - dfLows) / (dfHighs - dfLows)
            
    #''' For fast we just take the stochastic value, slow we need 3 day MA '''
    if not bFast:
       dfStoch = pand.rolling_mean(dfStoch, lMA)
                 
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    
    return dfStoch

def featBeta( dData, lLookback=14, sMarket='$SPX', b_human=False ):
    '''
    @summary: Calculate beta relative to a given stock/index.
    @param dData: Dictionary of data to use
    @param sStock: Stock to calculate beta relative to
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing feature values
    '''

    dfPrice = dData['close']

    #''' Calculate returns '''
    dfRets = dfPrice.copy()
    tsu.returnize1(dfRets.values)

    tsMarket = dfRets[sMarket]

    dfRet = pand.rolling_cov(tsMarket, dfRets, lLookback)
    dfRet /= dfRet[sMarket]
   
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dfRet

def featBollinger( dData, lLookback=20, b_human=False ):
    '''
    @summary: Calculate bollinger position as a function of std deviations.
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to calculate moving average over
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing feature values
    '''
    if b_human:
        dfPrice = dData['close']
        nstdsRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=np.zeros(dfPrice.shape) )
        #average minus standard deviation
        pstdsRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=np.zeros(dfPrice.shape) )      
        data3 = pand.DataFrame({"Raw":dfPrice[dfPrice.columns[0]]})
        for sym in dfPrice.columns:
            if sym != '$SPX' and sym != '$VIX':
                tsPrice = dfPrice[sym]
                nstdRet = nstdsRet[sym]
                pstdRet = pstdsRet[sym]
                for i in range(len(tsPrice.index)):
                    if i < lLookback - 1:
                        nstdRet[i] = float('nan')
                        pstdRet[i] = float('nan')
                        continue    
                    fAvg = np.average( tsPrice[ i-(lLookback-1):i+1 ] )
                    fStd = np.std( tsPrice[ i-(lLookback-1):i+1 ] )
                    pstdRet[i] = fAvg+2.0*fStd
                    nstdRet[i] = fAvg-2.0*fStd  
                data3[sym] = dfPrice[sym]
                data3[sym + " Lower"] = nstdsRet[sym]
                data3[sym + " Upper"] = pstdsRet[sym]
        del data3['Raw']
        return data3
    else:
        dfPrice = dData['close']
        #''' Feature DataFrame will be 1:1, we can use the price as a template '''
        dfRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=np.zeros(dfPrice.shape) )
        
        #''' Loop through stocks '''
        dfAvg = pand.rolling_mean(dfPrice, lLookback)
        dfStd = pand.rolling_std(dfPrice, lLookback)
        return (dfPrice - dfAvg) / (2.0*dfStd)


def featCorrelation( dData, lLookback=20, sRel='$SPX', b_human=False ):
    '''
    @summary: Calculate correlation of two stocks.
    @param dData: Dictionary of data to use
    @param lLookback: Number of days to calculate moving average over
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing feature values
    '''

    dfPrice = dData['close']
    
    if sRel not in dfPrice.columns:
        raise KeyError( "%s not found in data provided to featCorrelation"%sRel )
       
    #''' Calculate returns '''
    naRets = dfPrice.values.copy()
    tsu.returnize1(naRets)
    dfHistReturns = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=naRets )

    #''' Feature DataFrame will be 1:1, we can use the price as a template '''
    dfRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, data=np.zeros(dfPrice.shape) )
    
    #''' Loop through stocks '''
    for sStock in dfHistReturns.columns:   
        tsHistReturns = dfHistReturns[sStock]
        tsRelativeReturns = dfHistReturns[sRel]
        tsRet = dfRet[sStock]
        
        #''' Loop over time '''
        for i in range(len(tsHistReturns.index)):
            
            #''' NaN if not enough data to do lookback '''
            if i < lLookback - 1:
                tsRet[i] = float('nan')
                continue    
            
            naCorr = np.corrcoef( tsHistReturns[ i-(lLookback-1):i+1 ], tsRelativeReturns[ i-(lLookback-1):i+1 ] )
            
            tsRet[i] = naCorr[0,1]

    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dfRet

def featPrice(dData, b_human=False):
    '''
    @summary: Price feature
    @param dData: Dictionary of data to use
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dData['close']

def featVolume(dData, b_human=False):
    '''
    @summary: Volume feature
    @param dData: Dictionary of data to use
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dData['volume']


def featRand( dData, b_human=False ):
    '''
    @summary: Random feature - used for robustness testing
    @param dData: Dictionary of data to use
    @param b_human: if true return dataframe to plot
    @return: DataFrame array containing values
    '''
    
    dfPrice = dData['close']
    
    #''' Feature DataFrame will be 1:1, we can use the price as a template '''
    dfRet = pand.DataFrame( index=dfPrice.index, columns=dfPrice.columns, 
                            data=np.random.randn(*dfPrice.shape) )
    
    if b_human:
        for sym in dData['close']:
            x=1000/dData['close'][sym][0]
            dData['close'][sym]=dData['close'][sym]*x
        return dData['close']
    return dfRet


if __name__ == '__main__':
    pass

########NEW FILE########
__FILENAME__ = featutil
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Nov 7, 2011

@author: John Cornwell
@contact: JohnWCornwellV@gmail.com
@summary: Contains utility functions to interact with feature functions in features.py
'''

''' Python imports '''
import math
import pickle
import inspect
import datetime as dt
from dateutil.relativedelta import relativedelta

''' 3rd Party Imports '''
import numpy as np
import matplotlib.pyplot as plt


''' Our Imports '''
import QSTK.qstklearn.kdtknn as kdt
from QSTK.qstkutil import DataAccess as da
from QSTK.qstkutil import qsdateutil as du
from QSTK.qstkutil import tsutil as tsu

from QSTK.qstkfeat.features import *
from QSTK.qstkfeat.classes import class_fut_ret



def getMarketRel( dData, sRel='$SPX' ):
    '''
    @summary: Calculates market relative data.
    @param dData - Dictionary containing data to be used, requires specific naming: open/high/low/close/volume
    @param sRel - Stock ticker to make the data relative to, $SPX is default.
    @return: Dictionary of market relative values
    '''
    
    # the close dataframe is assumed to be in the dictionary data
    # otherwise the function will NOT WORK!
    if sRel not in dData['close'].columns:
        raise KeyError( 'Market relative stock %s not found in getMR()'%sRel )
    
    
    dRet = {}

    dfClose = dData['close'].copy()
    
    dfCloseMark = dfClose.copy()
    tsu.returnize0(  dfCloseMark.values )
    dfCloseMark = (dfCloseMark - dfCloseMark[sRel]) + 1.
    dfCloseMark.ix[0, :] = 100.
    dfCloseMark = dfCloseMark.cumprod(axis=0)
    
    #print dfCloseMark
    #Make all data market relative, except for volume
    for sKey in dData.keys():
        
        # Don't calculate market relative volume, but still copy it over 
        if sKey == 'volume':
            dRet['volume'] = dData['volume']
            continue

        dfKey = dData[sKey]
        dfRatio = dfKey/dfClose
        
        
        #Add dataFrame to dictionary to return, move to next key 
        dRet[sKey] = dfCloseMark * dfRatio
        
        #Comment the line below to convert the sRel as well, uncomment it
        #to keep the relative symbol's raw data
        dRet[sKey][sRel] = dData[sKey][sRel]

    #print dRet 
    return dRet



def applyFeatures( dData, lfcFeatures, ldArgs, sMarketRel=None, sLog=None, bMin=False ):
    '''
    @summary: Calculates the feature values using a list of feature functions and arguments.
    @param dData - Dictionary containing data to be used, requires specific naming: open/high/low/close/volume
    @param lfcFeatures: List of feature functions, most likely coming from features.py
    @param ldArgs: List of dictionaries containing arguments, passed as **kwargs
                   There is a special argument 'MR', if it exists, the data will be made market relative
    @param sMarketRel: If not none, the data will all be made relative to the symbol provided
    @param sLog: If not None, will be filename to log all of the features to 
    @param bMin: If true, only run for the last day
    @return: list of dataframes containing values
    '''



        
    ldfRet = []
    
    ''' Calculate market relative data '''
    if sMarketRel != None:
        dDataRelative = getMarketRel( dData, sRel=sMarketRel )
    
    
    ''' Loop though feature functions, pass each data dictionary and arguments '''
    for i, fcFeature in enumerate(lfcFeatures):
        #dt_start = dt.datetime.now()
        #print fcFeature, ldArgs[i], ' in:',
        ''' Check for special arguments '''
        if 'MR' in ldArgs[i]:
            
            if ldArgs[i]['MR'] == False:
                print 'Warning, setting MR to false will still be Market Relative',\
                      'simply do not include MR key in args'
        
            if sMarketRel == None:
                raise AssertionError('Functions require market relative stock but sMarketRel=None')
            del ldArgs[i]['MR']
            if bMin:
                # bMin means only calculate the LAST row of the stock
                dTmp = {}
                for sKey in dDataRelative:
                    if 'i_bars' in ldArgs[i]:
                        dTmp[sKey] = dDataRelative[sKey].ix[ 
                                     -(ldArgs[i]['lLookback'] + 
                                     ldArgs[i]['i_bars']+1):]
                    else:  
                        if 'lLookback' not in ldArgs[i]:
                            d_defaults = inspect.getargspec(fcFeature).defaults
                            d_args = inspect.getargspec(fcFeature).args
                            i_diff = len(d_args) - len(d_defaults)
                            i_index = d_args.index('lLookback') - i_diff
                            i_cut = -(d_defaults[i_index]+1)
                            dTmp[sKey] = dDataRelative[sKey].ix[i_cut:]
                            #print fcFeature.__name__ + ":" + str(i_cut)
                            
                        else:   
                            dTmp[sKey] = dDataRelative[sKey].ix[ 
                                         -(ldArgs[i]['lLookback'] + 1):]  
                ldfRet.append( fcFeature( dTmp, **ldArgs[i] ).ix[-1:] )
            else:
                ldfRet.append( fcFeature( dDataRelative, **ldArgs[i] ) )
        

                
        else:
            if bMin:
                # bMin means only calculate the LAST row of the stock
                dTmp = {}
                for sKey in dData:
                    if 'i_bars' in ldArgs[i]:
                        dTmp[sKey] = dData[sKey].ix[ 
                                     -(ldArgs[i]['lLookback'] + 
                                     ldArgs[i]['i_bars']+1):]
                       
                    else:    
                        if 'lLookback' not in ldArgs[i]:
                            d_defaults = inspect.getargspec(fcFeature).defaults
                            d_args = inspect.getargspec(fcFeature).args
                            i_diff = len(d_args) - len(d_defaults)
                            i_index = d_args.index('lLookback') - i_diff
                            i_cut = -(d_defaults[i_index]+1)
                            dTmp[sKey] = dData[sKey].ix[i_cut:]
                            #print fcFeature.__name__ + ":" + str(i_cut)
                        else:   
                            dTmp[sKey] = dData[sKey].ix[ 
                                     -(ldArgs[i]['lLookback'] + 1):]
                   
                ldfRet.append( fcFeature( dTmp, **ldArgs[i] ).ix[-1:] )
            else:
                ldfRet.append( fcFeature( dData, **ldArgs[i] ) )
        #print  dt.datetime.now() - dt_start

    
    if not sLog == None:
        with open( sLog, 'wb' ) as fFile:
            pickle.dump( ldfRet, fFile, -1 )
        
    return ldfRet

def loadFeatures( sLog ):
    '''
    @summary: Loads cached features.
    @param sLog: Filename of features.
    @return: Numpy array containing values
    '''
    
    ldfRet = []
    
    if not sLog == None:
        with open( sLog, 'rb' ) as fFile:
            ldfRet = pickle.load( fFile )
        
    return ldfRet


def stackSyms( ldfFeatures, dtStart=None, dtEnd=None, lsSym=None, sDelNan='ALL', bShowRemoved=False ):
    '''
    @summary: Remove symbols from the dataframes, effectively stacking all stocks on top of each other.
    @param ldfFeatures: List of data frames of features.
    @param dtStart: Start time, if None, uses all
    @param dtEnd: End time, if None uses all
    @param lsSym: List of symbols to use, if None, all are used.
    @param sDelNan: Optional, default is ALL: delete any rows with a NaN in it 
                    FEAT: Delete if any of the feature points are NaN, allow NaN classification
                    None: Do not delete any NaN rows
    @return: Numpy array containing all features as columns and all 
    '''
    
    if dtStart == None:
        dtStart = ldfFeatures[0].index[0]
    if dtEnd == None:
        dtEnd = ldfFeatures[0].index[-1]
    
    naRet = None
    ''' Stack stocks vertically '''
    for sStock in ldfFeatures[0].columns:
        
        if lsSym != None and sStock not in lsSym:
            continue

        naStkData = None
        ''' Loop through all features, stacking columns horizontally '''
        for dfFeat in ldfFeatures:
            
            dfFeat = dfFeat.ix[dtStart:dtEnd]
            
            if naStkData == None:
                naStkData = np.array( dfFeat[sStock].values.reshape(-1,1) )
            else:
                naStkData = np.hstack( (naStkData, dfFeat[sStock].values.reshape(-1,1)) )
   
        ''' Remove nan rows possibly'''
        if 'ALL' == sDelNan or 'FEAT' == sDelNan:
            llValidRows = []
            for i in range(naStkData.shape[0]):
                
                
                if 'ALL' == sDelNan and not math.isnan( np.sum(naStkData[i,:]) ) or\
                  'FEAT' == sDelNan and not math.isnan( np.sum(naStkData[i,:-1]) ):
                    llValidRows.append(i)
                elif  bShowRemoved:
                    print 'Removed', sStock, naStkData[i,:]
                        
            naStkData = naStkData[llValidRows,:]
            
    
        ''' Now stack each block of stock data vertically '''
        if naRet == None:
            naRet = naStkData
        else:
            naRet = np.vstack( (naRet, naStkData) )

    return naRet


def normFeatures( naFeatures, fMin, fMax, bAbsolute, bIgnoreLast=True ):
    '''
    @summary: Normalizes the featurespace.
    @param naFeatures:  Numpy array of features,  
    @param fMin: Data frame containing the price information for all of the stocks.
    @param fMax: List of feature functions, most likely coming from features.py
    @param bAbsolute: If true, min value will be scaled to fMin, max to fMax, if false,
                      +-1 standard deviations will be scaled to fit between fMin and fMax, i.e. ~69% of the values
    @param bIgnoreLast: If true, last column is ignored (assumed to be classification)
    @return: list of (weights, shifts) to be used to normalize the query points
    '''
    
    fNewRange = fMax - fMin
    
    lUseCols = naFeatures.shape[1]
    if bIgnoreLast:
        lUseCols -= 1
    
    ltRet = []
    ''' Loop through all features '''
    for i in range(lUseCols):
        
        ''' If absolutely scaled use exact min and max '''
        if bAbsolute:
            fFeatMin = np.min( naFeatures[:,i] )
            fFeatMax = np.max( naFeatures[:,i] )
        else:
            ''' Otherwise use mean +-1 std deviations for min/max (~94% of data) '''
            fMean = np.average( naFeatures[:,i] )
            fStd = np.std( naFeatures[:,i] ) 
            fFeatMin = fMean - fStd
            fFeatMax = fMean + fStd
            
        ''' Calculate multiplier and shift variable so that new data fits in specified range '''
        fRange = fFeatMax - fFeatMin
        
        if fRange == 0:
            print 'Warning, bad query data range'
            fMult = 1.
            fShigt = 0.
        else:
            fMult = fNewRange / fRange
            fShift = fMin - (fFeatMin * fMult)
        
        ''' scale and shift, save in return array '''
        naFeatures[:,i] *= fMult
        naFeatures[:,i] += fShift
        ltRet.append( (fMult, fShift) )
    
    return ltRet

def normQuery( naQueries, ltWeightShift ):
    '''
    @summary: Normalizes the queries using the given normalization parameters generated from training data.
    @param naQueries:  Numpy array of queries  
    @param ltWeightShift: List of weights and shift amounts to be applied to each query.
    @return: None, modifies naQueries
    '''
    
    assert naQueries.shape[1] == len(ltWeightShift)
    
    for i in range(naQueries.shape[1]):
        
        ''' scale and shift, save in return array '''
        naQueries[:,i] *= ltWeightShift[i][0]
        naQueries[:,i] += ltWeightShift[i][1]
        
def createKnnLearner( naFeatures, lKnn=30, leafsize=10, method='mean' ):
    '''
    @summary: Creates a quick KNN learner 
    @param naFeatures:  Numpy array of features,  
    @param fMin: Data frame containing the price information for all of the stocks.
    @param fMax: List of feature functions, most likely coming from features.py
    @param bAbsolute: If true, min value will be scaled to fMin, max to fMax, if false,
                      +-1 standard deviations will be scaled to fit between fMin and fMax, i.e. ~69% of the values
    @param bIgnoreLast: If true, last column is ignored (assumed to be classification)
    @return: None, data is modified in place
    '''
    cLearner = kdt.kdtknn( k=lKnn, method=method, leafsize=leafsize)

    cLearner.addEvidence( naFeatures )

    return cLearner

def log500( sLog ):
    '''
    @summary: Loads cached features.
    @param sLog: Filename of features.
    @return: Nothing, logs features to desired location
    '''
    
    
    lsSym = ['A', 'AA', 'AAPL', 'ABC', 'ABT', 'ACE', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES', 'AET', 'AFL', 'AGN', 'AIG', 'AIV', 'AIZ', 'AKAM', 'AKS', 'ALL', 'ALTR', 'AMAT', 'AMD', 'AMGN', 'AMP', 'AMT', 'AMZN', 'AN', 'ANF', 'ANR', 'AON', 'APA', 'APC', 'APD', 'APH', 'APOL', 'ARG', 'ATI', 'AVB', 'AVP', 'AVY', 'AXP', 'AZO', 'BA', 'BAC', 'BAX', 'BBBY', 'BBT', 'BBY', 'BCR', 'BDX', 'BEN', 'BF.B', 'BHI', 'BIG', 'BIIB', 'BK', 'BLK', 'BLL', 'BMC', 'BMS', 'BMY', 'BRCM', 'BRK.B', 'BSX', 'BTU', 'BXP', 'C', 'CA', 'CAG', 'CAH', 'CAM', 'CAT', 'CB', 'CBG', 'CBS', 'CCE', 'CCL', 'CEG', 'CELG', 'CERN', 'CF', 'CFN', 'CHK', 'CHRW', 'CI', 'CINF', 'CL', 'CLF', 'CLX', 'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNP', 'CNX', 'COF', 'COG', 'COH', 'COL', 'COP', 'COST', 'COV', 'CPB', 'CPWR', 'CRM', 'CSC', 'CSCO', 'CSX', 'CTAS', 'CTL', 'CTSH', 'CTXS', 'CVC', 'CVH', 'CVS', 'CVX', 'D', 'DD', 'DE', 'DELL', 'DF', 'DFS', 'DGX', 'DHI', 'DHR', 'DIS', 'DISCA', 'DNB', 'DNR', 'DO', 'DOV', 'DOW', 'DPS', 'DRI', 'DTE', 'DTV', 'DUK', 'DV', 'DVA', 'DVN', 'EBAY', 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'EMC', 'EMN', 'EMR', 'EOG', 'EP', 'EQR', 'EQT', 'ERTS', 'ESRX', 'ETFC', 'ETN', 'ETR', 'EW', 'EXC', 'EXPD', 'EXPE', 'F', 'FAST', 'FCX', 'FDO', 'FDX', 'FE', 'FFIV', 'FHN', 'FII', 'FIS', 'FISV', 'FITB', 'FLIR', 'FLR', 'FLS', 'FMC', 'FO', 'FRX', 'FSLR', 'FTI', 'FTR', 'GAS', 'GCI', 'GD', 'GE', 'GILD', 'GIS', 'GLW', 'GME', 'GNW', 'GOOG', 'GPC', 'GPS', 'GR', 'GS', 'GT', 'GWW', 'HAL', 'HAR', 'HAS', 'HBAN', 'HCBK', 'HCN', 'HCP', 'HD', 'HES', 'HIG', 'HNZ', 'HOG', 'HON', 'HOT', 'HP', 'HPQ', 'HRB', 'HRL', 'HRS', 'HSP', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IFF', 'IGT', 'INTC', 'INTU', 'IP', 'IPG', 'IR', 'IRM', 'ISRG', 'ITT', 'ITW', 'IVZ', 'JBL', 'JCI', 'JCP', 'JDSU', 'JEC', 'JNJ', 'JNPR', 'JNS', 'JOYG', 'JPM', 'JWN', 'K', 'KEY', 'KFT', 'KIM', 'KLAC', 'KMB', 'KMX', 'KO', 'KR', 'KSS', 'L', 'LEG', 'LEN', 'LH', 'LIFE', 'LLL', 'LLTC', 'LLY', 'LM', 'LMT', 'LNC', 'LO', 'LOW', 'LSI', 'LTD', 'LUK', 'LUV', 'LXK', 'M', 'MA', 'MAR', 'MAS', 'MAT', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDT', 'MET', 'MHP', 'MHS', 'MJN', 'MKC', 'MMC', 'MMI', 'MMM', 'MO', 'MOLX', 'MON', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', 'MSI', 'MTB', 'MU', 'MUR', 'MWV', 'MWW', 'MYL', 'NBL', 'NBR', 'NDAQ', 'NE', 'NEE', 'NEM', 'NFLX', 'NFX', 'NI', 'NKE', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NU', 'NUE', 'NVDA', 'NVLS', 'NWL', 'NWSA', 'NYX', 'OI', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PBI', 'PCAR', 'PCG', 'PCL', 'PCLN', 'PCP', 'PCS', 'PDCO', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGN', 'PGR', 'PH', 'PHM', 'PKI', 'PLD', 'PLL', 'PM', 'PNC', 'PNW', 'POM', 'PPG', 'PPL', 'PRU', 'PSA', 'PWR', 'PX', 'PXD', 'QCOM', 'QEP', 'R', 'RAI', 'RDC', 'RF', 'RHI', 'RHT', 'RL', 'ROK', 'ROP', 'ROST', 'RRC', 'RRD', 'RSG', 'RTN', 'S', 'SAI', 'SBUX', 'SCG', 'SCHW', 'SE', 'SEE', 'SHLD', 'SHW', 'SIAL', 'SJM', 'SLB', 'SLE', 'SLM', 'SNA', 'SNDK', 'SNI', 'SO', 'SPG', 'SPLS', 'SRCL', 'SRE', 'STI', 'STJ', 'STT', 'STZ', 'SUN', 'SVU', 'SWK', 'SWN', 'SWY', 'SYK', 'SYMC', 'SYY', 'T', 'TAP', 'TDC', 'TE', 'TEG', 'TEL', 'TER', 'TGT', 'THC', 'TIE', 'TIF', 'TJX', 'TLAB', 'TMK', 'TMO', 'TROW', 'TRV', 'TSN', 'TSO', 'TSS', 'TWC', 'TWX', 'TXN', 'TXT', 'TYC', 'UNH', 'UNM', 'UNP', 'UPS', 'URBN', 'USB', 'UTX', 'V', 'VAR', 'VFC', 'VIA.B', 'VLO', 'VMC', 'VNO', 'VRSN', 'VTR', 'VZ', 'WAG', 'WAT', 'WDC', 'WEC', 'WFC', 'WFM', 'WFR', 'WHR', 'WIN', 'WLP', 'WM', 'WMB', 'WMT', 'WPI', 'WPO', 'WU', 'WY', 'WYN', 'WYNN', 'X', 'XEL', 'XL', 'XLNX', 'XOM', 'XRAY', 'XRX', 'YHOO', 'YUM', 'ZION', 'ZMH']
    lsSym.append('$SPX')
    lsSym.sort()
    
    
    ''' Max lookback is 6 months '''
    dtEnd = dt.datetime.now()
    dtEnd = dtEnd.replace(hour=16, minute=0, second=0, microsecond=0)
    dtStart = dtEnd - relativedelta(months=6)
    
    
    ''' Pull in current data '''
    norObj = da.DataAccess('Norgate')
    ''' Get 2 extra months for moving averages and future returns '''
    ldtTimestamps = du.getNYSEdays( dtStart - relativedelta(months=2), \
                                    dtEnd   + relativedelta(months=2), dt.timedelta(hours=16) )
    
    dfPrice = norObj.get_data( ldtTimestamps, lsSym, 'close' )
    dfVolume = norObj.get_data( ldtTimestamps, lsSym, 'volume' )

    ''' Imported functions from qstkfeat.features, NOTE: last function is classification '''
    lfcFeatures, ldArgs, lsNames = getFeatureFuncs()                
    
    ''' Generate a list of DataFrames, one for each feature, with the same index/column structure as price data '''
    applyFeatures( dfPrice, dfVolume, lfcFeatures, ldArgs, sLog=sLog )


def getFeatureFuncs():
    '''
    @summary: Gets feature functions supported by the website.
    @return: Tuple containing (list of functions, list of arguments, list of names)
    '''
    
    lfcFeatures = [ featMA, featMA, featRSI, featDrawDown, featRunUp, featVolumeDelta, featAroon, featAroon, featStochastic , featBeta, featBollinger, featCorrelation, featPrice, class_fut_ret]
    lsNames = ['MovingAverage', 'RelativeMovingAverage', 'RSI', 'DrawDown', 'RunUp', 'VolumeDelta', 'AroonUp', 'AroonLow', 'Stochastic', 'Beta', 'Bollinger', 'Correlation', 'Price', 'FutureReturn']
      
    ''' Custom Arguments '''
    ldArgs = [ {'lLookback':30, 'bRel':False},\
               {'lLookback':30, 'bRel':True},\
               {'lLookback':14},\
               {'lLookback':30},\
               {'lLookback':30},\
               {'lLookback':30},\
               {'bDown':False, 'lLookback':25},\
               {'bDown':True, 'lLookback':25},\
               {'lLookback':14},\
               {'lLookback':14, 'sMarket':'SPY'},\
               {'lLookback':20},\
               {'lLookback':20, 'sRel':'SPY'},\
               {},\
               {'lLookforward':5, 'sRel':None, 'bUseOpen':False}]
    
    return lfcFeatures, ldArgs, lsNames
      

def testFeature( fcFeature, dArgs ):
    '''
    @summary: Quick function to run a feature on some data and plot it to see if it works.
    @param fcFeature: Feature function to test
    @param dArgs: Arguments to pass into feature function 
    @return: Void
    '''
    
    ''' Get Train data for 2009-2010 '''
    dtStart = dt.datetime(2011, 7, 1)
    dtEnd = dt.datetime(2011, 12, 31)
         
    ''' Pull in current training data and test data '''
    norObj = de.DataAccess('mysql')
    ''' Get 2 extra months for moving averages and future returns '''
    ldtTimestamps = du.getNYSEdays( dtStart, dtEnd, dt.timedelta(hours=16) )
    
    lsSym = ['GOOG']
    lsSym.append('WMT')
    lsSym.append('$SPX')
    lsSym.append('$VIX')
    lsSym.sort()
    
    lsKeys = ['open', 'high', 'low', 'close', 'volume', 'actual_close']
    ldfData = norObj.get_data( ldtTimestamps, lsSym, lsKeys )
    dData = dict(zip(lsKeys, ldfData))
    dfPrice = dData['close']


    #print dfPrice.values
    
    ''' Generate a list of DataFrames, one for each feature, with the same index/column structure as price data '''
    dtStart = dt.datetime.now()
    ldfFeatures = applyFeatures( dData, [fcFeature], [dArgs], sMarketRel='$SPX' )
    print 'Runtime:', dt.datetime.now() - dtStart
    
    ''' Use last 3 months of index, to avoid lookback nans '''

    dfPrint = ldfFeatures[0]['GOOG']
    print 'GOOG values:', dfPrint.values
    print 'GOOG Sum:', dfPrint.ix[dfPrint.notnull()].sum()
    
    for sSym in lsSym:
        plt.subplot( 211 )
        plt.plot( ldfFeatures[0].index[-60:], dfPrice[sSym].values[-60:] )
        plt.plot( ldfFeatures[0].index[-60:], dfPrice['$SPX'].values[-60:] * dfPrice[sSym].values[-60] / dfPrice['$SPX'].values[-60] )
        plt.legend((sSym, '$SPX'))
        plt.title(sSym)
        plt.subplot( 212 )
        plt.plot( ldfFeatures[0].index[-60:], ldfFeatures[0][sSym].values[-60:] )
        plt.title( '%s-%s'%(fcFeature.__name__, str(dArgs)) )
        plt.show()


    
    
def speedTest(lfcFeature,ldArgs):
    '''
    @Author: Tingyu Zhu
    @summary: Function to test the runtime for a list of features, and output them by speed
    @param lfcFeature: a list of features that will be sorted by runtime
    @param dArgs: Arguments to pass into feature function
    @return: A list of sorted tuples of format (time, function name/param string)
    '''     

    '''pulling out 2 years data to run test'''
    daData = de.DataAccess('mysql')
    dtStart = dt.datetime(2010, 1, 1)
    dtEnd = dt.datetime(2011, 12, 31)
    dtTimeofday = dt.timedelta(hours=16)
    lsSym = ['AAPL', 'GOOG', 'XOM', 'AMZN', 'BA', 'GILD', '$SPX']

    #print lsSym

    '''set up variables for applyFeatures'''
    lsKeys = ['open', 'high', 'low', 'close', 'volume', 'actual_close']
    ldtTimestamps = du.getNYSEdays( dtStart, dtEnd, dtTimeofday)
    ldfData = daData.get_data( ldtTimestamps, lsSym, lsKeys)
    dData = dict(zip(lsKeys, ldfData))
    
    '''loop through features'''
    ltResults = []
    for i in range(len(lfcFeature)):
        dtFuncStart = dt.datetime.now()
        ldfFeatures = applyFeatures( dData, [lfcFeature[i]], [ldArgs[i]], 
                                     sMarketRel='$SPX')
        ltResults.append((dt.datetime.now() - dtFuncStart, 
                         lfcFeature[i].__name__ + ' : ' + str(ldArgs[i])))
    ltResults.sort()
    
    '''print out result'''
    for tResult in ltResults:
        print tResult[1], ':', tResult[0]
    
    return ltResults

if __name__ == '__main__':
   
   # speedTest([featMA, featRSI, featAroon, featBeta, featCorrelation, 
   #            featBollinger, featStochastic], [{'lLookback':30}] * 7) 
   testFeature( featHiLow, {})
   pass

########NEW FILE########
__FILENAME__ = 1knn
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Feb 20, 2011
@author: John Cornwell
@organization: Georgia Institute of Technology
@contact: JohnWCornwellV@gmail.com
@summary: This is an implementation of the 1-KNN algorithm for ranking features quickly.
          It uses the knn implementation.
@status: oneKNN functions correctly, optimized to use n^2/2 algorithm.
'''

import matplotlib.pyplot as plt
from pylab import gca

import itertools
import string
import numpy as np
import math
import knn

from time import clock


'''
@summary: Query function for 1KNN, return value is a double between 0 and 1.

@param naData: A 2D numpy array. Each row is a data point with the final column containing the classification.
'''
def oneKnn( naData ):
    
    
    if naData.ndim != 2:
        raise Exception( "Data should have two dimensions" )
    
    lLen = naData.shape[0]
    ''' # of dimensions, subtract one for classification '''
    lDim = naData.shape[1] - 1
    
    ''' Start best distances as very large '''
    ldDistances = [1E300] * lLen
    llIndexes = [-1] * lLen
        
    dDistance = 0.0;

    ''' Loop through finding closest neighbors '''
    for i in range( lLen ):
        for j in range( i+1, lLen ):
             
            dDistance = 0.0
            for k in range( 0, lDim ):
                dDistance += (naData[i][k] - naData[j][k])**2
            dDistance = math.sqrt( dDistance )
            
            ''' Two distances to check, for i's best, and j's best '''
            if dDistance < ldDistances[i]:
                ldDistances[i] = dDistance
                llIndexes[i] = j
                
            if dDistance < ldDistances[j]:
                ldDistances[j] = dDistance
                llIndexes[j] = i
                
    lCount = 0
    ''' Now count # of matching pairs '''
    for i in range( lLen ):
        if naData[i][-1] == naData[ llIndexes[i] ][-1]:
            lCount = lCount + 1

    return float(lCount) / lLen
            

''' Test function to plot  results '''
def _plotResults( naDist1, naDist2, lfOneKnn, lf5Knn ):
    plt.clf()
    
    plt.subplot(311)
    plt.scatter( naDist1[:,0], naDist1[:,1] )

    plt.scatter( naDist2[:,0], naDist2[:,1], color='r' )
    

    #plt.ylabel( 'Feature 2' )
    #plt.xlabel( 'Feature 1' )
    #gca().annotate( '', xy=( .8, 0 ), xytext=( -.3 , 0 ), arrowprops=dict(facecolor='red', shrink=0.05) )    
    gca().annotate( '', xy=( .7, 0 ), xytext=( 1.5 , 0 ), arrowprops=dict(facecolor='black', shrink=0.05) )    
    plt.title( 'Data Distribution' )
    
    plt.subplot(312)
    
    plt.plot( range( len(lfOneKnn) ), lfOneKnn )

    plt.ylabel( '1-KNN Value' )    
    #plt.xlabel( 'Distribution Merge' )

    plt.title( '1-KNN Performance' )

    plt.subplot(313)
    
    plt.plot( range( len(lf5Knn) ), lf5Knn )

    plt.ylabel( '% Correct Classification' )    
    #plt.xlabel( 'Distribution Merge' )

    plt.title( '5-KNN Performance' )
    
    plt.subplots_adjust()
    
    plt.show() 
    
''' Function to plot 2 distributions '''
def _plotDist( naDist1, naDist2, i ):
    plt.clf()

    plt.scatter( naDist1[:,0], naDist1[:,1] )

    plt.scatter( naDist2[:,0], naDist2[:,1], color='r' )
    

    plt.ylabel( 'Feature 2' )
    plt.xlabel( 'Feature 1' )

    plt.title( 'Iteration ' + str(i) )
    
    plt.show()
    
''' Function to test KNN performance '''
def _knnResult( naData ):
    

    ''' Split up data into training/testing '''
    lSplit = naData.shape[0] * .7
    naTrain = naData[:lSplit, :]
    naTest  = naData[lSplit:, :]
    
    knn.addEvidence( naTrain.astype(float), 1 );
    
    ''' Query with last column omitted and 5 nearest neighbors '''
    naResults = knn.query( naTest[:,:-1], 5, 'mode') 
    
    ''' Count returns which are correct '''
    lCount = 0
    for i, dVal in enumerate(naResults):
        if dVal == naTest[i,-1]:
            lCount = lCount + 1
            
    dResult = float(lCount) / naResults.size

    return dResult

''' Tests performance of 1-KNN '''
def _test1():
        
    ''' Generate three random samples to show the value of 1-KNN compared to 5KNN learner performance '''
    
    for i in range(3):
        
        ''' Select one of three distributions '''
        if i == 0:
            naTest1 = np.random.normal( loc=[0,0],scale=.25,size=[500,2] )
            naTest1 = np.hstack( (naTest1, np.zeros(500).reshape(-1,1) ) )
            
            naTest2 = np.random.normal( loc=[1.5,0],scale=.25,size=[500,2] )
            naTest2 = np.hstack( (naTest2, np.ones(500).reshape(-1,1) ) )
        elif i == 1:
            naTest1 = np.random.normal( loc=[0,0],scale=.25,size=[500,2] )
            naTest1 = np.hstack( (naTest1, np.zeros(500).reshape(-1,1) ) )
            
            naTest2 = np.random.normal( loc=[1.5,0],scale=.1,size=[500,2] )
            naTest2 = np.hstack( (naTest2, np.ones(500).reshape(-1,1) ) )
        else:
            naTest1 = np.random.normal( loc=[0,0],scale=.25,size=[500,2] )
            naTest1 = np.hstack( (naTest1, np.zeros(500).reshape(-1,1) ) )
            
            naTest2 = np.random.normal( loc=[1.5,0],scale=.25,size=[250,2] )
            naTest2 = np.hstack( (naTest2, np.ones(250).reshape(-1,1) ) )
        
        naOrig = np.vstack( (naTest1, naTest2) )
        naBoth = np.vstack( (naTest1, naTest2) )
        
        ''' Keep track of runtimes '''
        t = clock()
        cOneRuntime = t-t;
        cKnnRuntime = t-t;
                                  
        lfResults = []
        lfKnnResults = []
        for i in range( 15 ):
            #_plotDist( naTest1, naBoth[100:,:], i )
            
            t = clock()
            lfResults.append( oneKnn( naBoth ) )
            cOneRuntime = cOneRuntime + (clock() - t)
            
            t = clock()
            lfKnnResults.append( _knnResult( np.random.permutation(naBoth) ) )
            cKnnRuntime = cKnnRuntime + (clock() - t)
            
            naBoth[500:,0] = naBoth[500:,0] - .1

        print 'Runtime OneKnn:', cOneRuntime
        print 'Runtime 5-KNN:', cKnnRuntime   
        _plotResults( naTest1, naTest2, lfResults, lfKnnResults )

''' Tests performance of 1-KNN '''
def _test2():
    ''' Generate three random samples to show the value of 1-KNN compared to 5KNN learner performance '''
    
    np.random.seed( 12345 )

    ''' Create 5 distributions for each of the 5 attributes '''
    dist1 = np.random.uniform( -1, 1, 1000 ).reshape( -1, 1 )
    dist2 = np.random.uniform( -1, 1, 1000 ).reshape( -1, 1 )   
    dist3 = np.random.uniform( -1, 1, 1000 ).reshape( -1, 1 )
    dist4 = np.random.uniform( -1, 1, 1000 ).reshape( -1, 1 )
    dist5 = np.random.uniform( -1, 1, 1000 ).reshape( -1, 1 )
    
    lDists = [ dist1, dist2, dist3, dist4, dist5 ]

    ''' All features used except for distribution 4 '''
    distY = np.sin( dist1 ) + np.sin( dist2 ) + np.sin( dist3 ) + np.sin( dist5 )
    distY = distY.reshape( -1, 1 )
    
    for i, fVal  in enumerate( distY ):
        if fVal >= 0:
            distY[i] = 1
        else:
            distY[i] = 0
    
    for i in range( 1, 6 ):
        
        lsNames = []
        lf1Vals = []
        lfVals = []   
             
        for perm in itertools.combinations( '12345', i ):
            
            ''' set test distribution to first element '''
            naTest = lDists[ int(perm[0]) - 1 ]
            sPerm = perm[0]
            
            ''' stack other distributions on '''
            for j in range( 1, len(perm) ):
                sPerm = sPerm + str(perm[j])
                naTest = np.hstack( (naTest, lDists[ int(perm[j]) - 1 ] ) )
            
            ''' finally stack y values '''
            naTest = np.hstack( (naTest, distY) )
            
            lf1Vals.append( oneKnn( naTest ) )
            lfVals.append( _knnResult( np.random.permutation(naTest) ) )
            lsNames.append( sPerm )

        ''' Plot results '''
        plt1 = plt.bar( np.arange(len(lf1Vals)), lf1Vals, .2, color='r' )
        plt2 = plt.bar( np.arange(len(lfVals)) + 0.2, lfVals, .2, color='b' )
        
        plt.legend( (plt1[0], plt2[0]), ('1-KNN', 'KNN, K=5') )
    
        plt.ylabel('1-KNN Value/KNN Classification')
        plt.xlabel('Feature Set')
        plt.title('Combinations of ' + str(i) + ' Features')

        plt.ylim( (0,1) )
        if len(lf1Vals) < 2:
            plt.xlim( (-1,1) )

        gca().xaxis.set_ticks( np.arange(len(lf1Vals)) + .2 )
        gca().xaxis.set_ticklabels( lsNames )
        
        plt.show()

          
    
if __name__ == '__main__':
    
    _test1()
    #_test2()
    
    
    
    

########NEW FILE########
__FILENAME__ = fastknn
"""
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

This package is an implementation of a novel improvement to KNN which
speeds up query times
"""
import math,random,sys,bisect,time
import numpy,scipy.spatial.distance,scipy.spatial.kdtree
import cProfile,pstats,gendata

def adistfun(u,v):
	#assuming 1xN ndarrays
	#return math.sqrt(((u-v)**2).sum())
	#return ((u-v)**2).sum()
	tmp = (u-v)
	return math.sqrt(numpy.dot(tmp,tmp))

class FastKNN:
	"""
	A class which implements the KNN learning algorithm with sped up
	query times.

	This class follows the conventions of other classes in the qstklearn
	module, with a constructor that initializes basic parameters and
	bookkeeping variables, an 'addEvidence' method for adding labeled
	training data individually or as a batch, and a 'query' method
	that returns an estimated class for an unlabled point.  Training
	and testing data are in the form of numpy arrays, and classes are
	discrete.
	
	In order to speed up query times, this class keeps a number of lists which
	sort the training data by distance to 'anchor' points.  The lists aren't
	sorted until the first call to the 'query' method, after which, the lists
	are kept in sorted order. Initial sort is done using pythons 'sort'
	(samplesort), and sorted insertions with 'insort' from the bisect module.
	"""
	def __init__(self, num_anchors, k):
		"""
		Creates a new FastKNN object that will use the given number of 
		anchors.
		"""
		self.num_anchors = num_anchors
		self.training_data = list()
		self.anchors = list()
		self.data_by_anchors = dict()
		self.data_classes = dict()
		self.is_sorted = False
		#self.distfun = scipy.spatial.distance.euclidean
		self.distfun = adistfun
		self.num_checks = 0
		self.kdt = None
		self.k = k
	
	def resetAnchors(self,selection_type='random'):
		"""
		Picks a new set of anchors.  The anchor lists will be re-sorted upon
		the next call to 'query'.
		
		selection_type - the method to use when selecting new anchor points.
		'random' performs a random permutation of the training points and
		picks the first 'num_anchors' as new anchors.
		"""
		if selection_type == 'random':
			self.anchors = range(len(self.training_data))
			random.shuffle(self.anchors)
			self.anchors = self.anchors[0:self.num_anchors]
			self.kdt = scipy.spatial.kdtree.KDTree(numpy.array(self.training_data)[self.anchors,:])
		self.is_sorted = False
				
	
	def addEvidence(self,data,label):
		"""
		Adds to the set of training data. If the anchor lists were sorted
		before the call to this method, the new data will be inserted into
		the anchor lists using 'bisect.insort'
		
		data - a numpy array, either a single point (1D) or a set of
		points (2D)
		
		label - the label for data. A single value, or a list of values
		in the same order as the points in data.
		"""
		if len(data.shape)==1:
			new_idx = len(self.training_data)
			self.training_data.append(data)
			self.data_classes[new_idx] = label
			if self.is_sorted:
				for a in self.anchors:
					dist = self.distfun(data,self.training_data[a])
					bisect.insort(self.data_by_anchors[a],(dist,new_idx))
		elif len(data.shape)>1:
			for i in xrange(len(data)):
				thing = data[i]
				new_idx = len(self.training_data)
				self.training_data.append(thing)
				self.data_classes[new_idx] = label[i]
				if self.is_sorted:
					for a in self.anchors:
						dist = self.distfun(thing,self.training_data[a])
						bisect.insort(self.data_by_anchors[a],(dist,new_idx))
	
	def query(self,point,k=None,method='mode',slow=False,dumdumcheck=False):
		"""
		Returns class value for an unlabled point by examining its k nearest
		neighbors. 'method' determines how the class of the unlabled point is
		determined.
		"""
		if k is None:
			k = self.k
		#stime = time.time()
		if len(self.anchors) < self.num_anchors:
			self.resetAnchors()
		if not self.is_sorted:
			for a in self.anchors:
				self.data_by_anchors[a] = [ ( self.distfun(self.training_data[datai],self.training_data[a]), datai) for datai in range(len(self.training_data))]
				self.data_by_anchors[a].sort(key=lambda pnt: pnt[0])
		#select the anchor to search from
		#right now pick the anchor closest to the query point
		
		#anchor = self.anchors[0]
		#anchor_dist = self.distfun(point,self.training_data[anchor]) 
		#for i in xrange(1,len(self.anchors)):
		#	new_anchor = self.anchors[i]
		#	new_anchor_dist = self.distfun(point,self.training_data[new_anchor])
		#	if new_anchor_dist < anchor_dist:
		#		anchor = new_anchor
		#		anchor_dist = new_anchor_dist
		res = self.kdt.query(numpy.array([point,]))
		anchor = self.anchors[res[1][0]]
		anchor_dist = res[0][0]
		#print "Found the anchor",anchor,anchor_dist
		
		#now search through the list
		anchor_list = self.data_by_anchors[anchor]
		neighbors = list()
		maxd = None
		maxd_idx = 0
		for i in xrange(0,len(anchor_list)):
			nextpnt_dist = self.distfun(point,self.training_data[anchor_list[i][1]])
			self.num_checks += 1
			nextthing = (nextpnt_dist,anchor_list[i][1])
			
			#ins_idx = bisect.bisect(neighbors,nextthing)
			#if ins_idx <= k:
			#	neighbors.insert(ins_idx,nextthing)
			#	neighbors = neighbors[:k]
			#if not(slow) and len(neighbors) >= k:
			#	if anchor_dist + neighbors[k-1][0] < anchor_list[i][0]:
			#		break
			
			if len(neighbors)<k:
				if (maxd is None) or (maxd < nextpnt_dist):
					maxd = nextpnt_dist
					maxd_idx = len(neighbors)
				neighbors.append(nextthing)
			elif nextpnt_dist < maxd:
				neighbors[maxd_idx] = nextthing
				maxthing = max(neighbors)
				maxd_idx = neighbors.index(maxthing)
				maxd = maxthing[0]
			if not(slow) and len(neighbors) >= k:
				if anchor_dist + maxd < anchor_list[i][0]:
					break

		#we have the k neighbors, report the class
		#of the query point via method
		if method == 'mode':
			class_count = dict()
			for n in neighbors:
				nid = n[1]
				clss = self.data_classes[nid]
				if clss in class_count:
					tmp = class_count[clss]
				else:
					tmp = 0
				class_count[clss] = tmp+1
			bleh = max(class_count.iteritems(),key=lambda item:item[1])
			if dumdumcheck and bleh[1] == 1:
				print "aHAH!"
				print point
			rv = bleh[0]
		elif method == 'mean':
			return sum([self.data_classes[n[1]] for n in neighbors])/float(k)
		#etime = time.time()
		#print "Query time:", etime-stime
		return rv

def dataifywine(fname):
	foo = open(fname)
	bar = [line for line in foo]
	foo.close()
	#first line is the name of the attributes, strip it off
	bar = bar[1:]
	#trim, split, and cast the data. seperator is ';'
	return [map(float,thing.strip().split(';')) for thing in bar]

def testwine():
	wqred = dataifywine('wine/winequality-red.csv') + dataifywine('wine/winequality-white.csv')
	leftoutperc = 0.1
	leftout = int(len(wqred)*leftoutperc)
	testing = wqred[:leftout]
	training = wqred[leftout:]
	print "Training:",len(training)
	print "Testing:",len(testing)
	foo = FastKNN(10)
	foo.addEvidence(numpy.array([thing[:-1] for thing in training]), [thing[-1] for thing in training])
	knn.addEvidence(numpy.array(training))
	total = 0
	correct = 0
	for x in xrange(len(testing)):
		thing = testing[x]
		guess = foo.query(numpy.array(thing[:-1]),3)
		#realknn = knn.query(numpy.array([thing[:-1],]),3,method='mean')
		#guess = realknn[0]
		#print realknn
		#print guess, thing[-1]
		if guess == thing[-1]:
			correct += 1
		total += 1
		if total % 50 == 0:
			print total,'/',len(testing)
	print correct,"/",total,":",float(correct)/float(total)
	print "Average checks per query:", float(foo.num_checks)/float(total)
	
def testspiral():
	for leftout in xrange(1,11):
		print "Fold",leftout
		foo = FastKNN(10)
		for x in xrange(1,11):
			if x != leftout:
				somedata = open("spiral/spiralfold%d.txt" % x)
				pnts = list()
				clss = list()
				for line in somedata:
					pbbbt,x,y = line.split()
					x,y = float(x),float(y)
					pnts.append((x,y))
					clss.append(line.split()[0])
				somedata.close()
				pnts = numpy.array(pnts)
				foo.addEvidence(pnts,clss)
		somedata = open("spiral/spiralfold%d.txt" % leftout)
		correct = total = 0
		for line in somedata:
			pbbbt,x,y = line.split()
			x,y = float(x),float(y)
			guess=foo.query((x,y),10)
			#guess2 = foo.query((x,y),10,slow=True)
			#if guess != guess2:
			#	print "Crap!"
			#	print guess,guess2,(x,y,pbbbt)
			#print guess, pbbbt
			if guess == pbbbt:
				correct += 1
			total += 1
		print correct,"/",total,":",float(correct)/float(total)
		print "Average number of checks per query:", 
		print float(foo.num_checks)/float(total)

def getflatcsv(fname):
	inf = open(fname)
	return numpy.array([map(float,s.strip().split(',')) for s in inf.readlines()])

def testgendata():
	anchors = 200
	fname = 'test2.dat'
	querys = 1000
	d = 2
	k = 3
	bnds = ((-10,10),)*d
	clsses = (0,1)
	foo = FastKNN(anchors,k)
	data = getflatcsv(fname)
	foo.addEvidence(data[:,:-1],data[:,-1])
	foo.num_checks = 0
	for x in xrange(querys):
		pnt = numpy.array(gendata.gensingle(d,bnds,clsses))
		foo.query(pnt[:-1])
		if x % 50 == 0:
			print float(foo.num_checks)/float(x+1),
			print x,"/",querys
	print "Average # queries:", float(foo.num_checks)/float(querys)
	
	

def test():
	testgendata()

if __name__=="__main__":
	test()
	#prof= cProfile.Profile()
	#prof.run('test()')
	#stats = pstats.Stats(prof)
	#stats.sort_stats("cumulative").print_stats()

########NEW FILE########
__FILENAME__ = gendata
import random
def gendata(N,d,bounds,clsses,fname):
	outf = open(fname,'w')
	for i in xrange(N):
		pnt = [None,]*(d+1)
		for x in xrange(d):
			pnt[x] = random.uniform(bounds[x][0],bounds[x][1])
		pnt[d] = random.choice(clsses)
		outf.write(", ".join(map(str,pnt))+"\n")
	outf.close()
def gensingle(d,bounds,clsses):
	pnt = [None,]*(d+1)
	for x in xrange(d):
		pnt[x] = random.uniform(bounds[x][0],bounds[x][1])
	pnt[d] = random.choice(clsses)
	return pnt

########NEW FILE########
__FILENAME__ = hmm
"""
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

This package includes code for representing and learning HMM's.

Most of the code in this package was derived from the descriptions provided in
'A Tutorial on Hidden Markov Models and Selected Applications in Speach
Recognition' by Lawence Rabiner.

Conventions:
The keyword argument elem_size will be passed in when creating numpy array 
objects.
"""
import math,random,sys
import numpy

def calcalpha(stateprior,transition,emission,observations,numstates,elem_size=numpy.longdouble):
	"""
	Calculates 'alpha' the forward variable.
	
	The alpha variable is a numpy array indexed by time, then state (TxN).
	alpha[t][i] = the probability of being in state 'i' after observing the 
	first t symbols.
	"""
	alpha = numpy.zeros((len(observations),numstates),dtype=elem_size)
	for x in xrange(numstates):
		alpha[0][x] = stateprior[x]*emission[x][observations[0]]
	for t in xrange(1,len(observations)):
		for j in xrange(numstates):
			for i in xrange(numstates):
				alpha[t][j] += alpha[t-1][i]*transition[i][j]
			alpha[t][j] *= emission[j][observations[t]]
	return alpha

def forwardbackward(stateprior,transition,emission,observations,numstates,elem_size=numpy.longdouble):
	"""
	Calculates the probability of a sequence given the HMM.
	"""
	alpha = calcalpha(stateprior,transition,emission,observations,numstates,elem_size)
	return sum(alpha[-1])

def calcbeta(transition,emission,observations,numstates,elem_size=numpy.longdouble):
	"""
	Calculates 'beta' the backward variable.
	
	The beta variable is a numpy array indexed by time, then state (TxN).
	beta[t][i] = the probability of being in state 'i' and then observing the
	symbols from t+1 to the end (T).
	"""
	beta = numpy.zeros((len(observations),numstates),dtype=elem_size)
	for s in xrange(numstates):
		beta[len(observations)-1][s] = 1.
	for t in xrange(len(observations)-2,-1,-1):
		for i in xrange(numstates):
			for j in xrange(numstates):
				beta[t][i] += transition[i][j]*emission[j][observations[t+1]]*beta[t+1][j]
	return beta

def calcxi(stateprior,transition,emission,observations,numstates,alpha=None,beta=None,elem_size=numpy.longdouble):
	"""
	Calculates 'xi', a joint probability from the 'alpha' and 'beta' variables.
	
	The xi variable is a numpy array indexed by time, state, and state (TxNxN).
	xi[t][i][j] = the probability of being in state 'i' at time 't', and 'j' at
	time 't+1' given the entire observation sequence.
	"""
	if alpha is None:
		alpha = calcalpha(stateprior,transition,emission,observations,numstates,elem_size)
	if beta is None:
		beta = calcbeta(transition,emission,observations,numstates,elem_size)
	xi = numpy.zeros((len(observations),numstates,numstates),dtype=elem_size)
	for t in xrange(len(observations)-1):
		denom = 0.0
		for i in xrange(numstates):
			for j in xrange(numstates):
				thing = 1.0
				thing *= alpha[t][i]
				thing *= transition[i][j]
				thing *= emission[j][observations[t+1]]
				thing *= beta[t+1][j]
				denom += thing
		for i in xrange(numstates):
			for j in xrange(numstates):
				numer = 1.0
				numer *= alpha[t][i]
				numer *= transition[i][j]
				numer *= emission[j][observations[t+1]]
				numer *= beta[t+1][j]
				xi[t][i][j] = numer/denom
	return xi

def calcgamma(xi,seqlen,numstates, elem_size=numpy.longdouble):
	"""
	Calculates 'gamma' from xi.
	
	Gamma is a (TxN) numpy array, where gamma[t][i] = the probability of being
	in state 'i' at time 't' given the full observation sequence.
	"""
	gamma = numpy.zeros((seqlen,numstates),dtype=elem_size)
	for t in xrange(seqlen):
		for i in xrange(numstates):
			gamma[t][i] = sum(xi[t][i])
	return gamma

def baumwelchstep(stateprior,transition,emission,observations,numstates,numsym,elem_size=numpy.longdouble):
	"""
	Given an HMM model and a sequence of observations, computes the Baum-Welch 
	update to the parameters using gamma and xi. 
	"""
	xi = calcxi(stateprior,transition,emission,observations,numstates,elem_size=elem_size)
	gamma = calcgamma(xi,len(observations),numstates,elem_size)
	newprior = gamma[0]
	newtrans = numpy.zeros((numstates,numstates),dtype=elem_size)
	for i in xrange(numstates):
		for j in xrange(numstates):
			numer = 0.0
			denom = 0.0
			for t in xrange(len(observations)-1):
				numer += xi[t][i][j]
				denom += gamma[t][i]
			newtrans[i][j] = numer/denom
	newemiss = numpy.zeros( (numstates,numsym) ,dtype=elem_size)
	for j in xrange(numstates):
		for k in xrange(numsym):
			numer = 0.0
			denom = 0.0
			for t in xrange(len(observations)):
				if observations[t] == k:
					numer += gamma[t][j]
				denom += gamma[t][j]
			newemiss[j][k] = numer/denom
	return newprior,newtrans,newemiss

class HMMLearner:
	"""
	A class for modeling and learning HMMs.
	
	This class conveniently wraps the module level functions. Class objects hold 6
	data members:
	- num_states       		number of hidden states in the HMM
	- num_symbols      		number of possible symbols in the observation 
	                   		sequence
	- precision        		precision of the numpy.array elements (defaults to
	                   		longdouble)
	- prior            		The prior probability of starting in each state
	                   		(Nx1 array)
	- transition_matrix		The probability of transitioning between each state
	                  		(NxN matrix)
	- emission_matrix  		The probability of each symbol in each state
	                   		(NxO matrix)
	You can set the 3 matrix parameters as you wish, but make sure the shape of
	the arrays matches num_states and num_symbols, as these are used internally
	
	Typical usage of this class is to create an HMM with a set number of states
	and external symbols, train the HMM using addEvidence(...), and then use
	the sequenceProb(...) method to see how well a specific sequence matches
	the trained HMM.
	"""
	def __init__(self,num_states,num_symbols,init_type='uniform',precision=numpy.longdouble):
		"""
		Creates a new HMMLearner object with the given number of internal
		states, and external symbols.
		
		calls self.reset(init_type=init_type)
		"""
		self.num_states = num_states
		self.num_symbols = num_symbols
		self.precision = precision
		self.reset(init_type=init_type)
	
	def reset(self, init_type='uniform'):
		"""
		Resets the 3 arrays using the given initialization method.
		
		Wipes out the old arrays. You can use this method to change the shape
		of the arrays by first changing num_states and/or num_symbols, and then
		calling this method.
		
		Currently supported initialization methods:
		uniform		prior, transition, and emission probabilities are all 
					uniform (default)
		"""
		if init_type == 'uniform':
			self.prior = numpy.ones( (self.num_states), dtype=self.precision) *(1.0/self.num_states)
			self.transition_matrix = numpy.ones( (self.num_states,self.num_states), dtype=self.precision)*(1.0/self.num_states)
			self.emission_matrix = numpy.ones( (self.num_states,self.num_symbols), dtype=self.precision)*(1.0/self.num_symbols)
	
	def sequenceProb(self, newData):
		"""
		Returns the probability that this HMM generated the given sequence.
		
		Uses the forward-backward algorithm.  If given an array of
		sequences, returns a 1D array of probabilities.
		"""
		if len(newData.shape) == 1:
			return forwardbackward(	self.prior,\
									self.transition_matrix,\
									self.emission_matrix,\
									newData,\
									self.num_states,\
									self.precision)
		elif len(newData.shape) == 2:
			return numpy.array([forwardbackward(self.prior,self.transition_matrix,self.emission_matrix,newSeq,self.num_states,self.precision) for newSeq in newData])

	def addEvidence(self, newData, iterations=1,epsilon=0.0):
		"""
		Updates this HMMs parameters given a new set of observed sequences
		using the Baum-Welch algorithm.
		
		newData can either be a single (1D) array of observed symbols, or a 2D
		matrix, each row of which is a seperate sequence. The Baum-Welch update
		is repeated 'iterations' times, or until the sum absolute change in
		each matrix is less than the given epsilon.  If given multiple
		sequences, each sequence is used to update the parameters in order, and
		the sum absolute change is calculated once after all the sequences are
		processed.
		"""
		if len(newData.shape) == 1:
			for i in xrange(iterations):
				newp,newt,newe = baumwelchstep(	self.prior, \
												self.transition_matrix, \
												self.emission_matrix, \
												newData, \
												self.num_states, \
												self.num_symbols,\
												self.precision)
				pdiff = sum([abs(np-op) for np in newp for op in self.prior])
				tdiff = sum([sum([abs(nt-ot) for nt in newti for ot in oldt]) for newti in newt for oldt in self.transition_matrix])
				ediff = sum([sum([abs(ne-oe) for ne in newei for oe in olde]) for newei in newe for olde in self.emission_matrix])
				if(pdiff < epsilon) and (tdiff < epsilon) and (ediff < epsilon):
					break
				self.prior = newp
				self.transition_matrix = newt
				self.emission_matrix = newe
		else:
			for i in xrange(iterations):
				for sequence in newData:
					newp,newt,newe = baumwelchstep(	self.prior, \
													self.transition_matrix, \
													self.emission_matrix, \
													sequence, \
													self.num_states, \
													self.num_symbols,\
													self.precision)
					self.prior = newp
					self.transition_matrix = newt
					self.emission_matrix = newe
				pdiff = sum([abs(np-op) for np in newp for op in self.prior])
				tdiff = sum([sum([abs(nt-ot) for nt in newti for ot in oldt]) for newti in newt for oldt in self.transition_matrix])
				ediff = sum([sum([abs(ne-oe) for ne in newei for oe in olde]) for newei in newe for olde in self.emission_matrix])
				if(pdiff < eps) and (tdiff < eps) and (ediff < eps):
					break
				self.prior = newp
				self.transition_matrix = newt
				self.emission_matrix = newe

#def test():
#	#an unfair coinflip, simpler than unfair casino
#	#symbols: 0 = heads, 1 = tails
#	#states: 0 = fair coin, 1 = heads weighted coin
#	stateprior = (0.8,0.2)
#	transition = ((0.9,0.1),(0.3,0.7))
#	emission = ((0.5,0.5),(0.9,0.1))
#	ob1 = (0,1,0,1,0,1,0,1,0,1,0,1)
#	ob2 = (0,0,0,0,0,0,1,1,1,1,1,1)
#	ob3 = (1,1,1,1,1,1,1,1,1,1,1,1)
#	ob4 = (0,0,0,0,0,0,0,0,0,0,0,0)
#	print "Pr(",ob1,") = ",forwardbackward(stateprior,transition,emission,ob1,2)
#	print "Pr(",ob2,") = ",forwardbackward(stateprior,transition,emission,ob2,2)
#	print "Pr(",ob3,") = ",forwardbackward(stateprior,transition,emission,ob3,2)
#	print "Pr(",ob4,") = ",forwardbackward(stateprior,transition,emission,ob4,2)
#	print
#	xi = calcxi(stateprior,transition,emission,ob2,2)
#	gamma = calcgamma(xi,len(ob2),2)
#	for t in xrange(len(ob2)):
#		print gamma[t]
#	print
#	ob5 = (0,1,2,3,0,1,2,3,0,1,2,3)
#	print "Doing Baum-welch"
#	my_hmm = HMMLearner(2,2)
#	my_hmm.addEvidence(numpy.array(ob3*10),100)
#	print "Prior",my_hmm.prior
#	print "Transition",my_hmm.transition_matrix
#	print "Emission", my_hmm.emission_matrix
#	print "Probability that the above HMM generated", (ob3*10)
#	print my_hmm.sequenceProb(numpy.array(ob3*10))
#	print "Probability that the above HMM generated", (ob1*10)
#	print my_hmm.sequenceProb(numpy.array(ob1*10))
#	
#if __name__=="__main__":
#	test()

########NEW FILE########
__FILENAME__ = kdtknn
"""
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

A simple wrapper for scipy.spatial.kdtree.KDTree for doing KNN
"""
import math,random,sys,bisect,time
import numpy,scipy.spatial.distance
from scipy.spatial import cKDTree
import cProfile,pstats,gendata
import numpy as np

class kdtknn(object):
    """
    A simple wrapper of scipy.spatial.kdtree.KDTree
    
    Since the scipy KDTree implementation does not allow for incrementally adding
    data points, the entire KD-tree is rebuilt on the first call to 'query' after a
    call to 'addEvidence'. For this reason it is more efficient to add training data
    in batches.
    """
    def __init__(self,k=3,method='mean',leafsize=10):
        """
        Basic setup.
        """
        self.leafsize = leafsize
        self.data = None
        self.kdt = None
        self.rebuild_tree = True
        self.k = k
        self.method = method

    def addEvidence(self,dataX,dataY=None):
        """
        @summary: Add training data
        @param dataX: Data to add, either entire set with classification as last column, or not if
                      the Y data is provided explicitly.  Must be same width as previously appended data.
        @param dataY: Optional, can be used 
        
        'data' should be a numpy array matching the same dimensions as any data 
        provided in previous calls to addEvidence, with dataY as the 
        training label.
        """
        
        ''' Slap on Y column if it is provided, if not assume it is there '''
        if not dataY == None:
            data = numpy.zeros([dataX.shape[0],dataX.shape[1]+1])
            data[:,0:dataX.shape[1]]=dataX
            data[:,(dataX.shape[1])]=dataY
        else:
            data = dataX
        
        self.rebuild_tree = True
        if self.data is None:
            self.data = data
        else:
            self.data = numpy.append(self.data,data,axis=0)

    def rebuildKDT(self):
        """
        Force the internal KDTree to be rebuilt.
        """
        self.kdt = cKDTree(self.data[:,:-1],leafsize=self.leafsize)
        self.rebuild_tree = False
    
    def query(self,points,k=None,method=None):
        """
        Classify a set of test points given their k nearest neighbors.
        
        'points' should be a numpy array with each row corresponding to a specific query.
        Returns the estimated class according to supplied method (currently only 'mode'
        and 'mean' are supported)
        """
        if k is None:
            k = self.k
        if method is None:
            method = self.method
        if self.rebuild_tree is True:
            if self.data is None:
                return None
            self.rebuildKDT()
        #kdt.query returns a list of distances and a list of indexes into the
        #data array
        if k == 1:
            tmp = self.kdt.query(points,k)
            #in the case of k==1, numpy fudges an array of 1 dimension into
            #a scalar, so we handle it seperately. tmp[1] is the list of
            #indecies, tmp[1][0] is the first one (we only need one),
            #self.data[tmp[1][0]] is the data point corresponding to the
            #first neighbor, and self.data[tmp[1][0]][-1] is the last column
            #which is the class of the neighbor.
            return self.data[tmp[1][0]][-1]
        #for all the neighbors returned by kdt.query, get their class and stick that into a list
        
        na_dist, na_neighbors =  self.kdt.query(points,k)
        
        n_clsses = map(lambda rslt: map(lambda p: p[-1], self.data[rslt]), na_neighbors)
        #print n_clsses

        if method=='mode':
            return map(lambda x: scipy.stats.stats.mode(x)[0],n_clsses)[0]
        elif method=='mean':
            return numpy.array(map(lambda x: numpy.mean(x),n_clsses))
        elif method=='median':
            return numpy.array(map(lambda x: numpy.median(x),n_clsses))
        elif method=='raw':
            return numpy.array(n_clsses)
        elif method=='all':
            return numpy.array(n_clsses), na_dist

def getflatcsv(fname):
    inf = open(fname)
    return numpy.array([map(float,s.strip().split(',')) for s in inf.readlines()])

def testgendata():
    fname = 'test2.dat'
    querys = 1000
    d = 2
    k=3
    bnds = ((-10,10),)*d
    clsses = (0,1)
    data = getflatcsv(fname)
    kdt = kdtknn(k,method='mode')
    kdt.addEvidence(data)
    kdt.rebuildKDT()
    stime = time.time()
    for x in xrange(querys):
        pnt = numpy.array(gendata.gensingle(d,bnds,clsses))
        reslt = kdt.query(numpy.array([pnt[:-1]]))
        print pnt,"->",reslt
    etime = time.time()
    print etime-stime,'/',querys,'=',(etime-stime)/float(querys),'avg wallclock time per query'
    #foo.addEvidence(data[:,:-1],data[:,-1])
    #foo.num_checks = 0
    #for x in xrange(querys):
    #    pnt = numpy.array(gendata.gensingle(d,bnds,clsses))
    #    foo.query(pnt[:-1],3)
    #    if x % 50 == 0:
    #        print float(foo.num_checks)/float(x+1),
    #        print x,"/",querys
    #print "Average # queries:", float(foo.num_checks)/float(querys)
    
def test():
    testgendata()

if __name__=="__main__":
    test()
    #prof= cProfile.Profile()
    #prof.run('test()')
    #stats = pstats.Stats(prof)
    #stats.sort_stats("cumulative").print_stats()

########NEW FILE########
__FILENAME__ = mldiagnostics
# (c) 2011, 2012 Georgia Tech Research Corporation
# This source code is released under the New BSD license.  Please see
# http://wiki.quantsoftware.org/index.php?title=QSTK_License
# for license details.
#
# Created on Month day, Year
#
# @author: Vishal Shekhar
# @contact: mailvishalshekhar@gmail.com
# @summary: ML Algo Diagnostic Utility (plots performance of the Algo on Train Vs CV sets)
#

import copy
import numpy as np
import matplotlib.pyplot as plt
from pylab import *

class MLDiagnostics:
	"""
	This class can be used to produce learning curves.
	These are plots of evolution of Training Error and Cross Validation Error across lambda(in general a control param for model complexity).
	This plot can help diagnose if the ML algorithmic model has high bias or a high variance problem and can
	thus help decide the next course of action.
	In general, ML Algorithm is of the form,
		Y=f(t,X) + lambdaVal*|t|
		where Y is the output, t is the model parameter vector, lambdaVal is the regularization parameter.
		|t| is the size of model parameter vector.
	"""
	def __init__(self,learner,Xtrain,Ytrain,Xcv,Ycv,lambdaArray):
		self.learner = learner
		self.Xtrain = Xtrain
		self.Ytrain = Ytrain
		self.Xcv = Xcv
		self.Ycv = Ycv
		self.lambdaArray = lambdaArray
		self.ErrTrain = np.zeros((len(lambdaArray),1))
		self.ErrCV = copy.copy(self.ErrTrain)

	def avgsqerror(self,Y,Ypred):
		return np.sum((Y-Ypred)**2)/len(Y)
	
	def plotCurves(self,filename):
		Xrange = [i*self.step for i in range(1,len(self.ErrTrain)+1)]
		plt.plot(Xrange,self.ErrTrain,label = "Train Error")
		plt.plot(Xrange,self.ErrCV,label="CV Error")
		plt.title('Learning Curves')
		plt.xlabel('# of Training Examples')
		plt.ylabel('Average Error')
		plt.draw()
		savefig(filename,format='pdf')

	def runDiagnostics(self,filename):
		for i,lambdaVal in zip(range(len(self.lambdaArray)),self.lambdaArray):
			learner = copy.copy(self.learner())# is deep copy required
			# setLambda needs to be a supported function for all ML strategies.
			learner.setLambda(lambdaVal)
			learner.addEvidence(self.Xtrain,self.Ytrain)
			YtrPred = learner.query(self.Xtrain)
			self.ErrTrain[i] = self.avgsqerror(self.Ytrain,YtrPred)
			YcvPred = learner.query(self.Xcv)
			self.ErrCV[i] = self.avgsqerror(self.Ycv,YcvPred)
		self.plotCurves(filename)

########NEW FILE########
__FILENAME__ = parallelknn
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Feb 1, 2011
@author: Shreyas Joshi
@organization: Georgia Institute of Technology
@contact: shreyasj@gatech.edu
@summary: This is an implementation of the K nearest neighbor learning algorithm. The implementation is trivial in that the near neighbors are 
          calculated naively- without any smart tricks. Euclidean distance is used to calculate the distance between two points. The implementation
          also provides some coarse parallelism. If the par_query function is used then the query points are split up equally amongst threads and their
          near neighbors are calculated in parallel. If the number of threads to use is not specified then no of threads  = no of cores as returned by
          the cpu_count function. This may not be ideal.    
@status: complete. "mode" untested
'''

import numpy as np
import scipy as sc
import math
import sys
import time
from multiprocessing import Pool
from multiprocessing import cpu_count
data = np.zeros (0)#this is the global data
import scipy.stats

def par_query (allQueries, k, method='mean', noOfThreads=None):
    '''
    @summary: Finds the k- nearest nrighbors in parallel. Based on function "query"
    @param allQueries: is another 2D numpy array. Each row here is one query point. It has no 'y' values. These have to be calculated.
    @param k: no. of neighbors to consider
    @param method: method of combining the 'y' values of the nearest neighbors. Default is mean.
    @param noOfThreads: optional parameter that specifies how many threads to create. Default value: no. of threads = value returned by cpu_count
    @return: A numpy array with the predicted 'y' values for the query points. The ith element in the array is the 'y' value for the ith query point.
    '''
    
    #Here we basically start 'noOfThreads' threads. Each thread calculates the neighbors for (noOfQueryPoints / noOfThreads) query points.
    
    if (noOfThreads == None):
        noOfThreads = cpu_count()
        #if ends
    
    #print "No of threads: " + str (noOfThreads)
    pool = Pool (processes=noOfThreads)
    
    resultList = []
    query_per_thread = allQueries.shape[0] / noOfThreads
    
    #time_start = time.time();
    for thread_ctr in range (0, noOfThreads - 1):
        resultList.append(pool.apply_async(query, (allQueries[math.floor(query_per_thread * thread_ctr): (math.floor(query_per_thread * (thread_ctr + 1 ))),:], k,)))
        #NOTE: we may need a -1 in above. Possible bug
        #for ends
    #the "remaining" query points go to the last thread
    resultList.append (pool.apply_async(query , (allQueries[(math.floor(query_per_thread* (noOfThreads - 1))):, :] ,k ,)))    
    
    pool.close()
    pool.join()
    
    #time_finish = time.time()
    #print "Time taken (secs): " + str (time_finish - time_start)
    
    answer = resultList[0].get()
    
    for thread_ctr  in range (1, noOfThreads):
        answer = np.hstack((answer, resultList[thread_ctr].get()))
        #for ends

    #print "par_query done"
    return answer
    #par_query ends


def query(allQueries, k, method='mean'):
    '''
    @summary: A serial implementation of k-nearest neighbors.
    @param allQueries: is another 2D numpy array. Each row here is one query point. It has no 'y' values. These have to be calculated.
    @param k: no. of neighbors to consider
    @param method: method of combining the 'y' values of the nearest neighbors. Default is mean.
    @return: A numpy array with the predicted 'y' values for the query points. The ith element in the array is the 'y' value for the ith query point. If there is more than one mode then only the first mode is returned.
    '''
    
    limit = allQueries.shape [0]
    data_limit = data.shape[0]
    omitLastCol = data.shape [1] - 1; #It must have two columns at least. Possibly add a check for this?
    answer = np.zeros (limit) #initialize the answer array to all zeros
    temp1 = np.zeros ((data.shape[0], (data.shape[1] -1)))
    temp2= np.zeros (data.shape[0])
    
    if (allQueries.shape[1] != (data.shape[1] -1) ):
        print "ERROR: Data and query points are not of the same dimension"
        raise ValueError
        #if ends
    if (k < 1):
        print "ERROR: K should be >= 1"
        raise ValueError
        #if ends    
    if (k > data.shape[0]):
        print "ERROR: K is greater than the total number of data points."
        raise ValueError
        #if ends
    
    for ctr in range (0, limit): #for every query point...  
        #if (ctr % 10 == 0):
         #   print ctr
            #if ends
        
        #for i in range (0 , data_limit): #for every data point
        temp1[0:data_limit, :] = data [0:data_limit ,0:omitLastCol] - allQueries[ctr, :]
        #for loop done
        
        temp1 = temp1*temp1; #square each element in temp1
        for ctr2 in range (0, data_limit):
            temp2[ctr2] = math.sqrt(sum (temp1[ctr2,:]))
            #for ends
                  
        index = temp2.argsort () #This is actually overkill because we need to sort only the top 'k' terms- but this sorts all of them     
        #following loop for debugging only  
        #for j in range (0, k):
        #   print str(data [index[j], :])
            #for ends
        
        if (method == 'mean'):
            #Now we need to find the average of the k top most 'y' values
            answer[ctr] = sum (data[ index [0:k], -1]) / k  #0 to (k-1)th index will be k values. But for this we have to give [0:k] because it stops one short of the last index
            #if method == mean ends
        if (method == 'median'):
            answer [ctr] = np.median (data[index [0:k], -1])
            # if median ends
        if (method == 'mode'):
            answer [ctr] = sc.stats.mode (data[index[0:k],-1])[0][0]; #The first mode. If there is more than one mode then the only the first mode is returned
            #endif mode    
        #for ctr in range (0, limit) ends
    return answer
    #getAnswer ends


def addEvidence (newData):
    '''
    @summary: This is the funtion to be called to add data. This function can be called multiple times- to add data whenever you like.
    @note: Any dimensional data can be added the first time. After that- the data must have the same number of columns as the data that was added the first time.
    @param newData: A 2D numpy array. Each row is a data point and each column is a dimension. The last dimension corresponds to 'y' values.
    '''
    
    global data
    if (data.shape[0] == 0):
        data = newData
    else:
        try:
            data= np.vstack ((data, newData))
        except Exception as ex:
            print "Type of exception: "+ str(type (ex))
            print "args: " + str(ex.args)
            #except ends   
    #addEvidence ends


def main(args):
    '''
    @summary: This function is just for testing. Will not be used as such...
    '''
    
    #Below code just for testing
    a = np.loadtxt ("/nethome/sjoshi42/knn_naive/data/3_D_1000_diskQueryPoints.txt")
    b= np.loadtxt ("/nethome/sjoshi42/knn_naive/data/3_D_128000_diskDataPoints.txt")
    addEvidence(b)

    answer = par_query(a, 5 ,'mode')
    #answer = query(a, 5, 'mean')
    
    for i in range (0, answer.shape[0]):
        print answer[i]
    #end for    
    
    
    print "The answer is: "

if __name__ == '__main__':
    main (sys.argv)

########NEW FILE########
__FILENAME__ = test_tradesim
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on May 14, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Test cases for tradeSim
'''

# Python imports
import datetime as dt
import unittest

# 3rd Party Imports
import pandas as pand
import numpy as np

# QSTK imports
import QSTK.qstksim


class Test(unittest.TestCase):
    df_close = None
    df_alloc = None
    i_open_result = None

    def _generate_data(self):
        
        ldt_timestamps = []
        na_close = np.ones( (16, 3) )

        for i in range(8):
            ldt_timestamps.append( dt.datetime(2012, 3, i+1, 9) )
            ldt_timestamps.append( dt.datetime(2012, 3, i+1, 16) )

        for i in range(16):
            if i == 0:
                na_close[i, :] = 1
            else:
                na_close[i, 0] = na_close[i-1, 0]+1
                na_close[i, 1] = na_close[i-1, 1]-0.02
			
                if (i % 3 == 0):
                    na_close[i, 2] = na_close[i-1, 2] + 0.2
                else:
                    na_close[i, 2] = na_close[i-1, 2]

        self.df_close = pand.DataFrame( index = ldt_timestamps, \
                        data = na_close, columns = ['A', 'B', 'C'] )

        # Create first allocation
        na_alloc = np.array( [[1, 0, 0], [0.5, 0.5, 0], [1.0, -1, 0], \
                       [1.0, 0, 1], [1.0, 0, 1], [.5, .5, 0], [0, .5, 0.5]] )

        ldt_timestamps = [dt.datetime( 2012, 3, 2, 13 )]
        ldt_timestamps.append( dt.datetime( 2012, 3, 3, 13 )  )
        ldt_timestamps.append( dt.datetime( 2012, 3, 3, 20 )  )
        ldt_timestamps.append( dt.datetime( 2012, 3, 4, 20 )  )
        ldt_timestamps.append( dt.datetime( 2012, 3, 5, 13 )  )
        ldt_timestamps.append( dt.datetime( 2012, 3, 6, 13 )  )
        ldt_timestamps.append( dt.datetime( 2012, 3, 8, 4 )  )

        self.df_alloc = pand.DataFrame( index=ldt_timestamps, \
                        data=na_alloc, columns=['A', 'B', 'C'] )
        self.df_alloc['_CASH'] = 0.0

        #Based on hand calculation using the transaction costs and slippage.
        self.i_open_result = 1.35011891

    def setUp(self):
        ''' Unittest setup function '''
        self._generate_data()

    def test_buy_close(self):
        ''' Tests tradesim buy-on-open functionality '''
        (df_funds, ts_leverage, f_commision, f_slippage, f_borrow) = \
              qstksim.tradesim( self.df_alloc, self.df_close, 10000, 1, True, 0.02,
                           5, 0.02 )

        print 'Commision Costs : ' + str(f_commision)
        print 'Slippage : ' + str(f_slippage)
        print 'Short Borrowing Cost : ' + str(f_borrow)
        print 'Leverage : '	
        print ts_leverage
        np.testing.assert_approx_equal(df_funds[-1], \
             10000 * self.i_open_result, significant = 3)
        self.assertTrue(True)
        #self.assertTrue(abs(df_funds[-1] - 10000 * self.i_open_result)<=0.01)


if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


########NEW FILE########
__FILENAME__ = test_tradesim_SPY
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on May 19, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Test cases for tradeSim - Monthly Rebalancing of $SPX
'''

# Python imports
import datetime as dt
import unittest

# 3rd Party Imports
import pandas as pand
import numpy as np

# QSTK imports
import QSTK.qstksim
import QSTK.qstkutil.DataAccess as da
import QSTK.qstkutil.qsdateutil as du


class Test(unittest.TestCase):
    df_close = None
    df_alloc = None
    i_open_result = None

    def _generate_data(self):

        year = 2009        
        startday = dt.datetime(year-1, 12, 1)
        endday = dt.datetime(year+1, 1, 31)

        l_symbols = ['$SPX']

        #Get desired timestamps
        timeofday = dt.timedelta(hours = 16)
        ldt_timestamps = du.getNYSEdays(startday, endday, timeofday)

        dataobj = da.DataAccess('Norgate')
        self.df_close = dataobj.get_data( \
                        ldt_timestamps, l_symbols, "close", verbose=True)

        self.df_alloc = pand.DataFrame( \
                        index=[dt.datetime(year, 1, 1)], \
                                data=[1], columns=l_symbols)

        for i in range(11):
            self.df_alloc = self.df_alloc.append( \
                     pand.DataFrame(index=[dt.datetime(year, i+2, 1)], \
                                      data=[1], columns=l_symbols))

        self.df_alloc['_CASH'] = 0.0

        #Based on hand calculation using the transaction costs and slippage.
        self.i_open_result = 1.15921341122
	

    def setUp(self):
        ''' Unittest setup function '''
        self._generate_data()

    def test_buy_close(self):
        ''' Tests tradesim buy-on-open functionality '''
        (df_funds, ts_leverage, f_commision, f_slippage, f_borrow) = \
              qstksim.tradesim( self.df_alloc, self.df_close, 10000, 1, True, 0.02,
                           5, 0.02)

        print 'Commision Costs : ' + str(f_commision)
        print 'Slippage : ' + str(f_slippage)
        print 'Short Borrowing Cost : ' + str(f_borrow)
        print 'Leverage : '	
        print ts_leverage
        np.testing.assert_approx_equal(df_funds[-1], \
             10000 * self.i_open_result, significant = 3)
        self.assertTrue(True)
        #self.assertTrue(abs(df_funds[-1] - 10000 * self.i_open_result)<=0.01)


if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


########NEW FILE########
__FILENAME__ = test_tradesim_SPY_Short
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on May 19, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Test cases for tradeSim - Monthly Rebalancing of $SPX
'''

# Python imports
import datetime as dt
import unittest

# 3rd Party Imports
import pandas as pand
import numpy as np

# QSTK imports
import QSTK.qstksim
import QSTK.qstkutil.DataAccess as da
import QSTK.qstkutil.qsdateutil as du


class Test(unittest.TestCase):
    df_close = None
    df_alloc = None
    i_open_result = None

    def _generate_data(self):

        year = 2009        
        startday = dt.datetime(year-1, 12, 1)
        endday = dt.datetime(year+1, 1, 31)

        l_symbols = ['$SPX']

        #Get desired timestamps
        timeofday = dt.timedelta(hours = 16)
        ldt_timestamps = du.getNYSEdays(startday, endday, timeofday)

        dataobj = da.DataAccess('Norgate')
        self.df_close = dataobj.get_data( \
                        ldt_timestamps, l_symbols, "close", verbose=True)

        self.df_alloc = pand.DataFrame( \
                        index=[dt.datetime(year, 1, 1)], \
                                data=[-1], columns=l_symbols)

        for i in range(11):
            self.df_alloc = self.df_alloc.append( \
                     pand.DataFrame(index=[dt.datetime(year, i+2, 1)], \
                                      data=[-1], columns=l_symbols))

        self.df_alloc['_CASH'] = 0.0

        #Based on hand calculation using the transaction costs and slippage.
        self.i_open_result = 0.7541428779600005
	

    def setUp(self):
        ''' Unittest setup function '''
        self._generate_data()

    def test_buy_close(self):
        ''' Tests tradesim buy-on-open functionality '''
        (df_funds, ts_leverage, f_commision, f_slippage, f_borrow) = \
              qstksim.tradesim( self.df_alloc, self.df_close, 10000, 1, True,
                                0.02, 5, 0.02)

        print 'Commision Costs : ' + str(f_commision)
        print 'Slippage : ' + str(f_slippage)
        print 'Short Borrowing Cost : ' + str(f_borrow)
        print 'Leverage : '	
        print ts_leverage
        np.testing.assert_approx_equal(df_funds[-1], \
             10000 * self.i_open_result, significant = 3)
        self.assertTrue(True)
        #self.assertTrue(abs(df_funds[-1] - 10000 * self.i_open_result)<=0.01)


if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


########NEW FILE########
__FILENAME__ = tradesim
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on May 14, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: Backtester

'''


# Python imports
from datetime import timedelta

# 3rd Party Imports
import pandas as pand
import numpy as np
from copy import deepcopy

# QSTK imports
from QSTK.qstkutil import tsutil as tsu

def _calculate_leverage(values_by_stock, ts_leverage, ts_long_exposure, ts_short_exposure, ts_net_exposure):
    """
    @summary calculates leverage based on the dataframe values_by_stock
             and returns the updated timeseries of leverage
    @param values_by_stock: Dataframe containing the values held in
             in each stock in the portfolio
    @param ts_leverage: time series of leverage values
    @return ts_leverage : updated time series of leverage values
    """

    for r_index, r_val in values_by_stock.iterrows():
        f_long = 0
        f_short = 0
        for val in r_val.values[:-1]:
            if np.isnan(val) == False:
                if val >= 0:
                    f_long = f_long + val
                else:
                    f_short = f_short + val
                
        f_lev = (f_long + abs(f_short)) \
                /(f_long + r_val.values[-1] + f_short)
        f_net = (f_long - abs(f_short)) \
                /(f_long + r_val.values[-1] + f_short)
        f_long_ex = (f_long) \
                /(f_long + r_val.values[-1] + f_short)
        f_short_ex = (abs(f_short)) \
                /(f_long + r_val.values[-1] + f_short)                

        if np.isnan(f_lev): f_lev = 0
        if np.isnan(f_net): f_net = 0
        if np.isnan(f_long): f_long = 0
        if np.isnan(f_short): f_short = 0

        ts_leverage = ts_leverage.append(pand.Series(f_lev, index = [r_index] ))
        ts_long_exposure = ts_long_exposure.append(pand.Series(f_long_ex, index = [r_index] ))
        ts_short_exposure = ts_short_exposure.append(pand.Series(f_short_ex, index = [r_index] ))
        ts_net_exposure = ts_net_exposure.append(pand.Series(f_net, index = [r_index] ))

    return ts_leverage, ts_long_exposure, ts_short_exposure, ts_net_exposure
    
    
def _monthly_turnover(ts_orders, ts_fund):
    
    order_val_month = 0
    last_date = ts_orders.index[0]
    b_first_month = True
    ts_turnover = "None"
    for date in ts_orders.index:
        if last_date.month == date.month:
            order_val_month += ts_orders.ix[date]
        else:
            if b_first_month == True:
                ts_turnover = pand.Series(order_val_month, index=[last_date])
                b_first_month = False
            else:
                ts_turnover = ts_turnover.append(pand.Series(order_val_month, index=[last_date]))
            order_val_month = ts_orders.ix[date]
            last_date = date
            
    if type(ts_turnover) != type("None"):
        ts_turnover = ts_turnover.append(pand.Series(order_val_month, index=[last_date]))
    else:
        ts_turnover = pand.Series(order_val_month, index=[last_date])
    
    order_month = 0
    len_orders = len(ts_turnover.index)
    last_date = ts_fund.index[0]
    
    for date in ts_fund.index:
        if order_month < len_orders:
            if (ts_turnover.index[order_month]).month != date.month:
                ts_turnover.ix[ts_turnover.index[order_month]] = ts_turnover.ix[ts_turnover.index[order_month]]/(2*ts_fund.ix[last_date])
                order_month += 1 
            else:
                last_date = date
        else:
            if date.month == last_date.month:
                last_date = date
            else:
                break
    
    ts_turnover.ix[ts_turnover.index[-1]] = ts_turnover.ix[ts_turnover.index[-1]]/(2*ts_fund.ix[last_date])           

    return ts_turnover


def _normalize(row):

    """
    @summary Normalize an allocation row based on sum(abs(alloc))
    @param row: single row of the allocation dataframe
    @param proportion: normalized proportion of the row
    """
    total = row.abs().sum()
    proportion = row/total
    return proportion

def _nearest_interger(f_x):

    """
    @summary Return the nearest integer to the float number
    @param x: single float number
    @return: nearest integer to x
    """
    if f_x >= 0:
        return np.floor(f_x)
    else :
        return np.ceil(f_x)

def tradesim( alloc, df_historic, f_start_cash, i_leastcount=1,
            b_followleastcount=False, f_slippage=0.0,
            f_minimumcommision=0.0, f_commision_share=0.0,
            i_target_leverage=1, f_rate_borrow = 0.0, log="false", b_exposure=False):

    """
    @summary Quickly back tests an allocation for certain df_historical data,
             using a starting fund value
    @param alloc: DataMatrix containing timestamps to test as indices and
                 Symbols to test as columns, with _CASH symbol as the last
                 column
    @param df_historic: df_historic dataframe of equity prices
    @param f_start_cash: integer specifing initial fund value
    @param i_leastcount: Minimum no. of shares per transaction, ie: 1, 10, 20
    @param f_slippage: slippage per share (0.02)
    @param f_minimumcommision: Minimum commision cost per transaction
    @param f_commision_share: Commision per share
    @param b_followleastcount: False will allow fractional shares
    @param log: CSV file to log transactions to
    @return funds: TimeSeries with fund values for each day in the back test
    @return leverage: TimeSeries with Leverage values for each day in the back test
    @return Commision costs : Total commision costs in the whole backtester
    @return Slippage costs : Total slippage costs in the whole backtester
    @rtype TimeSeries
    """

    if alloc.index[-1] > df_historic.index[-1]:
        print "Historical Data not sufficient"
        indices, = np.where(alloc.index <= df_historic.index[-1])
        alloc = alloc.reindex(index = alloc.index[indices])

    if alloc.index[0] < df_historic.index[0]:
        print "Historical Data not sufficient"
        indices, = np.where(alloc.index >= df_historic.index[0])
        alloc = alloc.reindex(index = alloc.index[indices])

    #open log file
    if log!="false":
        log_file=open(log,"w")

    #write column headings
    if log!="false":
        print "writing transaction log to "+log
        log_file.write("Symbol,Company Name,Txn Type,Txn Date/Time, Gross Leverage, Net Leverage,# Shares,Price,Txn Value,Portfolio # Shares,Portfolio Value,Commission,Slippage(10BPS),Comments\n")

    #a dollar is always worth a dollar
    df_historic['_CASH'] = 1.0

    # Shares -> Variable holds the shares to be traded on the next timestamp
    # prediction_shares -> Variable holds the shares that were calculated
                            #for trading based on previous timestamp
    shares = (alloc.ix[0:1] * 0.0)
    shares['_CASH'] = f_start_cash
    prediction_shares = deepcopy(shares)

    # Total commision and Slippage costs
    f_total_commision = 0.0
    f_total_slippage = 0.0
    f_total_borrow = 0.0

    #remember last change in cash due to transaction costs and slippage
    cashleft = 0.0

    #value of fund ignoring cashleft
    no_trans_fund = 0.0

    dt_last_date = None
    f_last_borrow = 0.0
    b_first_iter = True
    b_order_flag = True

    for row_index, row in alloc.iterrows():

        # Trade Date and Price (Next timestamp)
        # Prediction Date and Price (Previous timestamp)

        trade_price = df_historic.ix[row_index:].ix[0:1]
        trade_index = df_historic.index.searchsorted(trade_price.index[0])
        pred_index = trade_index - 1
        prediction_price = \
                  df_historic.ix[df_historic.index[pred_index]:].ix[0:1]

        prediction_date = prediction_price.index[0]

        #trade_date is unused right now, but holds the next timestamp
        trade_date = trade_price.index[0]

        if b_first_iter == True:
            #log initial cash value
            if log!="false":
                log_file.write("_CASH,_CASH,Cash Deposit,"+str(prediction_date)+",,,,,"+str(f_start_cash)+",,\n")


            # Fund Value on start
            ts_fund = pand.Series( f_start_cash, index = [prediction_date] )

            # Ignoring cash delta fund value on start
            no_trans_fund=pand.Series( f_start_cash, index = [prediction_date])

            # Leverage at the start
            ts_leverage = pand.Series( 0, index = [prediction_date] )
            ts_long_exposure = pand.Series(0, index=[prediction_date])
            ts_short_exposure = pand.Series(0, index=[prediction_date])
            ts_net_exposure = pand.Series(0, index=[prediction_date])

            ts_orders = None

            days_since_alloc = 0
            f_borrow_cost = 0.0

            # Flag for first iteration is False now
            b_first_iter = False

        else :
            # get stock prices on all the days up until this trade
            to_calculate = df_historic[ (df_historic.index <= prediction_date) \
                        & (df_historic.index > ts_fund.index[-1]) ]

            # multiply prices by our current shares
            values_by_stock = to_calculate * shares.ix[-1]

            # calculate total value and append to our fund history
            ts_fund = ts_fund.append( values_by_stock.sum(axis=1) )
            # remember what value would be without cash delta as well
            no_trans_fund = no_trans_fund.append(values_by_stock.sum(axis=1) - cashleft)

            #Leverage
            ts_leverage, ts_long_exposure, ts_short_exposure, ts_net_exposure = _calculate_leverage(
                                        values_by_stock, ts_leverage, ts_long_exposure, ts_short_exposure, ts_net_exposure)

            days_since_alloc = (trade_date - dt_last_date).days
            f_borrow_cost = abs((days_since_alloc*f_last_borrow*f_rate_borrow)/(100*365))
            f_total_borrow = f_total_borrow + f_borrow_cost

        #Normalizing the allocations
        proportion = _normalize(row)
        '''
        indices_short = np.where(proportion.values < 0)
        short_val = abs(sum(proportion.values[indices_short]))

        indices_long = np.where(proportion.values >= 0)
        long_val = abs(sum(proportion.values[indices_long]))

        sl_ratio = short_val/long_val
        '''
        # Allocation to be scaled upto the allowed Leverage
        proportion = proportion*i_target_leverage

        #Amount allotted to each equity
        value_allotted = proportion*ts_fund.ix[-1]

        value_before_trade = ts_fund.ix[-1]

        # Get shares to be purchased
        prediction_shares = value_allotted/ prediction_price

        #ignoring cash delta calculate value allotted per share and cacluclate number of shares for each
        ntva=proportion*no_trans_fund.ix[-1]
        no_trans_shares = ntva/prediction_price


        #Adjusting the amount of shares to be purchased based on the leastcount
        # default is 1 : whole number of shares

        if b_followleastcount == True:
            prediction_shares /= i_leastcount
            prediction_shares = prediction_shares.apply(_nearest_interger)
            prediction_shares *= i_leastcount
            #handle shares ignoring last cash delta similarly
            no_trans_shares /= i_leastcount
            no_trans_shares = no_trans_shares.apply(_nearest_interger)
            no_trans_shares *= i_leastcount

        #remove cashleft from current holding
        cash_delta_less_shares=deepcopy(shares)
        cash_delta_less_shares["_CASH"]=cash_delta_less_shares["_CASH"]-cashleft


        #compare current holding to future holding (both ignoring the last round of transmission cost and slippage)
        same=1
        for sym in shares:
            if str(cash_delta_less_shares[sym].values[0])!=str(no_trans_shares[sym].values[0]):
                same=0

        #if same:
        #perform a transaction
        if same==0:
            #print "transaction"
            #print tampered_shares
            #print no_trans_shares
            #Order to be executed
            order = pand.Series(((prediction_shares.values \
                    - shares.values)[0])[:-1], index = row.index[:-1])

            # Transaction costs
            f_transaction_cost = 0
            for index in order.index:
                val = abs(order[index])
                if (val != 0):
                    t_cost = max(f_minimumcommision, f_commision_share*val)
                    f_transaction_cost = f_transaction_cost + t_cost

            f_total_commision = f_total_commision + f_transaction_cost

            value_before_trade = ((trade_price*shares.ix[-1]).sum(axis = 1)).ix[-1]

            # Shares that were actually purchased
            shares = prediction_shares

            # Value after the purchase (change in price at execution)
            value_after_trade = ((trade_price*shares.ix[-1]).sum(axis = 1)).ix[-1]

            #Slippage Cost
            f_slippage_cost = f_slippage*(trade_price.values[0][:-1])*order.values
            f_slippage_cost = abs(f_slippage_cost)
            f_slippage_cost[np.isnan(f_slippage_cost)] = 0.0
            f_slippage_cost = f_slippage_cost.sum()

            #Orders
            f_order = (1+f_slippage)*(trade_price.values[0][:-1])*order.values
            f_order = abs(f_order)
            f_order[np.isnan(f_order)] = 0.0
            f_order = f_order.sum()
            
            if b_order_flag == True:
                ts_orders = pand.Series(f_order, index=[trade_date])
                b_order_flag = False
            else:
                ts_orders = ts_orders.append(pand.Series(f_order, index=[trade_date]))

            if np.isnan(f_slippage_cost) == False:
                f_total_slippage = f_total_slippage + f_slippage_cost
                # Rebalancing the cash left
                cashleft = value_before_trade - value_after_trade - f_transaction_cost - f_slippage_cost - f_borrow_cost
            else:
                cashleft = value_before_trade - value_after_trade - f_transaction_cost - f_borrow_cost


            dt_last_date = trade_date
            f_last_holding = ((trade_price*shares.ix[-1]).ix[-1]).values
            indices = np.where(f_last_holding < 0)
            f_last_borrow = abs(sum(f_last_holding[indices]))

            shares['_CASH'] = shares['_CASH'] + cashleft

            money_short = f_last_borrow
            indices_long = np.where(f_last_holding >= 0)
            money_long = abs(sum(f_last_holding[indices_long]))
            money_cash = cashleft

            GL = (money_long + money_short) / (money_long - money_short + money_cash)
            NL = (money_long - money_short) / (money_long - money_short + money_cash)


            #for all symbols, print required transaction to log
            for sym in shares:
                if sym != "_CASH":
                    f_stock_commission=max(f_minimumcommision, f_commision_share*abs(order[sym]))
                    order_type="Buy"
                    if(order[sym]<0):
                        if(shares[sym]<0):
                            order_type="Sell Short"
                        else:
                            order_type="Sell"
                    elif shares[sym]<0:
                        order_type="Buy to Cover"


                    if log!="false":
                        if(abs(order[sym])!=0):
                            log_file.write(str(sym) + ","+str(sym)+","+order_type+","+str(trade_date)+","+str(GL)+","+str(NL)+\
                                       ","+str(order[sym])+","+str(trade_price[sym].values[0])+","+\
                                        str(trade_price[sym].values[0]*order[sym])+","\
                                       +str(shares[sym].ix[-1])+","+str(value_after_trade)+","+str(f_stock_commission)+","+\
                                        str(round(f_slippage_cost,2))+",")
                            log_file.write("\n")

        # End of Loop


    #close log
    if log!="false":
        #deposit nothing at end so that if we reload the transaction history the whole period gets shown
        log_file.write("_CASH,_CASH,Cash Deposit,"+str(prediction_date)+",,,,,"+str(0)+",,")
        log_file.close()
    #print ts_fund
    #print ts_leverage
    #print f_total_commision
    #print f_total_slippage
    #print f_total_borrow

    ts_turnover = _monthly_turnover(ts_orders, ts_fund)
    
    if b_exposure:
        return (ts_fund, ts_leverage, f_total_commision, f_total_slippage, f_total_borrow, 
                        ts_long_exposure, ts_short_exposure, ts_net_exposure, ts_turnover)
    return (ts_fund, ts_leverage, f_total_commision, f_total_slippage, f_total_borrow)


def tradesim_comb( df_alloc, d_data, f_start_cash, i_leastcount=1,
                   b_followleastcount=False, f_slippage=0.0,
                   f_minimumcommision=0.0, f_commision_share=0.0,
                   i_target_leverage=1, f_rate_borrow = 0.0, log="false", b_exposure=False):

    """
    @summary Same as tradesim, but combines open and close data into one.
    @param alloc: DataMatrix containing timestamps to test as indices and
                 Symbols to test as columns, with _CASH symbol as the last
                 column
    @param d_data: Historic dictionary of dataframes containing 'open' and
                   'close'
    @param f_start_cash: integer specifing initial fund value
    @param i_leastcount: Minimum no. of shares per transaction, ie: 1, 10, 20
    @param f_slippage: slippage per share (0.02)
    @param f_minimumcommision: Minimum commision cost per transaction
    @param f_commision_share: Commision per share
    @param b_followleastcount: False will allow fractional shares
    @return funds: TimeSeries with fund values for each day in the back test
    @return leverage: TimeSeries with Leverage values for each day in the back test
    @return Commision costs : Total commision costs in the whole backtester
    @return Slippage costs : Total slippage costs in the whole backtester
    @rtype TimeSeries
    """

    df_close = d_data['close']
    df_open = d_data['open']

    f_shift_close = 16. - df_close.index[0].hour
    f_shift_open = 9.5 - df_open.index[0].hour

    df_new_close = df_close.shift( 1, timedelta(hours=f_shift_close) )
    df_new_open = df_open.shift( 1, timedelta(hours=f_shift_open) )

    df_combined = df_new_close.append( df_new_open ).sort()

    return tradesim(df_alloc, df_combined, f_start_cash, i_leastcount,
                   b_followleastcount, f_slippage, f_minimumcommision,
                   f_commision_share, i_target_leverage, f_rate_borrow, log, b_exposure)

if __name__ == '__main__':
    print "Done"

########NEW FILE########
__FILENAME__ = strategies
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Sep 27, 2011

@author: John Cornwell
@contact: JohnWCornwellV@gmail.com
@summary: Various simple trading strategies to generate allocations.
'''

''' Python imports '''
import datetime as dt
from math import sqrt


''' 3rd party imports '''
import numpy as np
import pandas as pand

''' QSTK imports '''
import QSTK.qstkutil.tsutil as tsu


def stratGiven( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Simplest strategy, weights are provided through args.
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """    
    if not dFuncArgs.has_key('dmPrice'):
        print 'Error: Strategy requires dmPrice information'
        return
    
    if not dFuncArgs.has_key('lfWeights'):
        print 'Error: Strategy requires weight information'
        return
    
    dmPrice = dFuncArgs['dmPrice']
    lfWeights = dFuncArgs['lfWeights']
    
    ''' Generate two allocations, one for the start day, one for the end '''
    naAlloc = np.array( lfWeights ).reshape(1,-1)

    dfAlloc = pand.DataFrame( index=[dtStart], data=naAlloc, columns=(dmPrice.columns) )
    dfAlloc = dfAlloc.append( pand.DataMatrix(index=[dtEnd], data=naAlloc, columns=dmPrice.columns))
    dfAlloc['_CASH'] = 0.0
    
    return dfAlloc

def strat1OverN( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Evenly distributed strategy.
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """        
    if not dFuncArgs.has_key('dmPrice'):
        print 'Error: Strategy requires dmPrice information'
        return
    
    dmPrice = dFuncArgs['dmPrice']
    
    lNumSym = len(dmPrice.columns)
    
    ''' Generate two allocations, one for the start day, one for the end '''
    naAlloc = (np.array( np.ones(lNumSym) ) * (1.0 / lNumSym)).reshape(1,-1)
    dfAlloc = pand.DataMatrix( index=[dtStart], data=naAlloc, columns=(dmPrice.columns) )
    dfAlloc = dfAlloc.append( pand.DataMatrix(index=[dtEnd], data=naAlloc, columns=dmPrice.columns))
    dfAlloc['_CASH'] = 0.0
    
    return dfAlloc
    

def stratMark( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Markovitz strategy, generates a curve and then chooses a point on it.
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """         
    if not dFuncArgs.has_key('dmPrice'):
        print 'Error:', stratMark.__name__, 'requires dmPrice information'
        return
    
    if not dFuncArgs.has_key('sPeriod'):
        print 'Error:', stratMark.__name__, 'requires rebalancing period'
        return

    if not dFuncArgs.has_key('lLookback'):
        print 'Error:', stratMark.__name__, 'requires lookback'
        return

    if not dFuncArgs.has_key('sMarkPoint'):
        print 'Error:', stratMark.__name__, 'requires markowitz point to choose'
        return 

    ''' Optional variables '''
    if not dFuncArgs.has_key('bAddAlpha'):
        bAddAlpha = False
    else:
        bAddAlpha = dFuncArgs['bAddAlpha']
    
    dmPrice = dFuncArgs['dmPrice']
    sPeriod = dFuncArgs['sPeriod']
    lLookback = dFuncArgs['lLookback']
    sMarkPoint = dFuncArgs['sMarkPoint']

    ''' Select rebalancing dates '''
    drNewRange = pand.DateRange(dtStart, dtEnd, timeRule=sPeriod) + pand.DateOffset(hours=16)
    
    dfAlloc = pand.DataMatrix()
    ''' Go through each rebalance date and calculate an efficient frontier for each '''
    for i, dtDate in enumerate(drNewRange):
        dtStart = dtDate - pand.DateOffset(days=lLookback)
        
        if( dtStart < dmPrice.index[0] ):
            print 'Error, not enough data to rebalance'
            continue  
       
        naRets = dmPrice.ix[ dtStart:dtDate ].values.copy()
        tsu.returnize1(naRets)
        tsu.fillforward(naRets)
        tsu.fillbackward(naRets)
        
        ''' Add alpha to returns '''
        if bAddAlpha:
            if i < len(drNewRange) - 1:
                naFutureRets = dmPrice.ix[ dtDate:drNewRange[i+1] ].values.copy()
                tsu.returnize1(naFutureRets)
                tsu.fillforward(naFutureRets)
                tsu.fillbackward(naFutureRets)
                
                naAvg = np.mean( naFutureRets, axis=0 )
                
                ''' make a mix of past/future rets '''
                for i in range( naRets.shape[0] ):
                    naRets[i,:] = (naRets[i,:] + (naAvg*0.05)) / 1.05
                

        ''' Generate the efficient frontier '''
        (lfReturn, lfStd, lnaPortfolios) = getFrontier( naRets, fUpper=0.2, fLower=0.01 )
        
        lInd = 0
        
        '''
        plt.clf()
        plt.plot( lfStd, lfReturn)'''
        
        if( sMarkPoint == 'Sharpe'):
            ''' Find portfolio with max sharpe '''
            fMax = -1E300
            for i in range( len(lfReturn) ):
                fShrp = (lfReturn[i]-1) / (lfStd[i])
                if fShrp > fMax:
                    fMax = fShrp
                    lInd = i
            '''     
            plt.plot( [lfStd[lInd]], [lfReturn[lInd]], 'ro')
            plt.draw()
            time.sleep(2)
            plt.show()'''
            
        elif( sMarkPoint == 'MinVar'):
            ''' use portfolio with minimum variance '''
            fMin = 1E300
            for i in range( len(lfReturn) ):
                if lfStd[i] < fMin:
                    fMin = lfStd[i]
                    lInd = i
        
        elif( sMarkPoint == 'MaxRet'):
            ''' use Portfolio with max returns (not really markovitz) '''
            lInd = len(lfReturn)-1
        
        elif( sMarkPoint == 'MinRet'):
            ''' use Portfolio with min returns (not really markovitz) '''
            lInd = 0    
                
        else:
            print 'Warning: invalid sMarkPoint'''
            return
    
        
    
        ''' Generate allocation based on selected portfolio '''
        naAlloc = (np.array( lnaPortfolios[lInd] ).reshape(1,-1) )
        dmNew = pand.DataMatrix( index=[dtDate], data=naAlloc, columns=(dmPrice.columns) )
        dfAlloc = dfAlloc.append( dmNew )
    
    dfAlloc['_CASH'] = 0.0
    return dfAlloc

def stratMarkSharpe( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Calls stratMark with sharpe ratio point.
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """        
    dFuncArgs['sMarkPoint'] = 'Sharpe'
    return stratMark( dtStart, dtEnd, dFuncArgs )
    
def stratMarkLowVar( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Calls stratMark and uses lowest variance ratio point.
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """    
    dFuncArgs['sMarkPoint'] = 'MinVar'
    return stratMark( dtStart, dtEnd, dFuncArgs )

def stratMarkMaxRet( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Calls stratMark and uses maximum returns.
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """    
    dFuncArgs['sMarkPoint'] = 'MaxRet'
    return stratMark( dtStart, dtEnd, dFuncArgs )
    
def stratMarkMinRet( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Calls stratMark and uses minimum returns.
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """    
    dFuncArgs['sMarkPoint'] = 'MinRet'
    return stratMark( dtStart, dtEnd, dFuncArgs )
      
def stratMarkSharpeAlpha( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Calls stratMark and chooses the highest share point, uses future knowlege (alpha).
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """    
    dFuncArgs['sMarkPoint'] = 'Sharpe'
    dFuncArgs['bAddAlpha'] = True
    return stratMark( dtStart, dtEnd, dFuncArgs )

def stratMarkMaxRetAlpha( dtStart, dtEnd, dFuncArgs ):
    """
    @summary Calls stratMark chooses the highest returns point, uses future knowlege (alpha).
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param dFuncArgs: Dict of function args passed to the function
    @return DataFrame corresponding to the portfolio allocations
    """    
    dFuncArgs['sMarkPoint'] = 'MaxRet'
    dFuncArgs['bAddAlpha'] = True
    return stratMark( dtStart, dtEnd, dFuncArgs )



########NEW FILE########
__FILENAME__ = EventProfiler
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 16, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj90@gmail.com
@summary: EventProfiler

'''

import numpy as np
import matplotlib.pyplot as plt

import QSTK.qstkutil.DataAccess as da
import QSTK.qstkutil.tsutil as tsu
import QSTK.qstkutil.qsdateutil as du


def eventprofiler(df_events_arg, d_data, i_lookback=20, i_lookforward=20,
                s_filename='study', b_market_neutral=True, b_errorbars=True,
                s_market_sym='SPY'):
    ''' Event Profiler for an event matix'''
    df_close = d_data['close'].copy()
    df_rets = df_close.copy()

    # Do not modify the original event dataframe.
    df_events = df_events_arg.copy()
    tsu.returnize0(df_rets.values)

    if b_market_neutral == True:
        df_rets = df_rets - df_rets[s_market_sym]
        del df_rets[s_market_sym]
        del df_events[s_market_sym]

    df_close = df_close.reindex(columns=df_events.columns)

    # Removing the starting and the end events
    df_events.values[0:i_lookback, :] = np.NaN
    df_events.values[-i_lookforward:, :] = np.NaN

    # Number of events
    i_no_events = int(np.logical_not(np.isnan(df_events.values)).sum())
    assert i_no_events > 0, "Zero events in the event matrix"
    na_event_rets = "False"

    # Looking for the events and pushing them to a matrix
    for i, s_sym in enumerate(df_events.columns):
        for j, dt_date in enumerate(df_events.index):
            if df_events[s_sym][dt_date] == 1:
                na_ret = df_rets[s_sym][j - i_lookback:j + 1 + i_lookforward]
                if type(na_event_rets) == type(""):
                    na_event_rets = na_ret
                else:
                    na_event_rets = np.vstack((na_event_rets, na_ret))

    if len(na_event_rets.shape) == 1:
        na_event_rets = np.expand_dims(na_event_rets, axis=0)

    # Computing daily rets and retuns
    na_event_rets = np.cumprod(na_event_rets + 1, axis=1)
    na_event_rets = (na_event_rets.T / na_event_rets[:, i_lookback]).T

    # Study Params
    na_mean = np.mean(na_event_rets, axis=0)
    na_std = np.std(na_event_rets, axis=0)
    li_time = range(-i_lookback, i_lookforward + 1)

    # Plotting the chart
    plt.clf()
    plt.axhline(y=1.0, xmin=-i_lookback, xmax=i_lookforward, color='k')
    if b_errorbars == True:
        plt.errorbar(li_time[i_lookback:], na_mean[i_lookback:],
                    yerr=na_std[i_lookback:], ecolor='#AAAAFF',
                    alpha=0.1)
    plt.plot(li_time, na_mean, linewidth=3, label='mean', color='b')
    plt.xlim(-i_lookback - 1, i_lookforward + 1)
    if b_market_neutral == True:
        plt.title('Market Relative mean return of ' +\
                str(i_no_events) + ' events')
    else:
        plt.title('Mean return of ' + str(i_no_events) + ' events')
    plt.xlabel('Days')
    plt.ylabel('Cumulative Returns')
    plt.savefig(s_filename, format='pdf')

########NEW FILE########
__FILENAME__ = Events
# (c) 2011, 2012 Georgia Tech Research Corporation
# This source code is released under the New BSD license.  Please see
# http://wiki.quantsoftware.org/index.php?title=QSTK_License
# for license details.

#Created on October <day>, 2011
#
#@author: Vishal Shekhar
#@contact: mailvishalshekhar@gmail.com
#@summary: Example Event Datamatrix acceptable to EventProfiler App
#

import pandas 
from QSTK.qstkutil import DataAccess as da
import numpy as np
import math
import QSTK.qstkutil.qsdateutil as du
import datetime as dt
import QSTK.qstkutil.DataAccess as da

"""
Accepts a list of symbols along with start and end date
Returns the Event Matrix which is a pandas Datamatrix
Event matrix has the following structure :
    |IBM |GOOG|XOM |MSFT| GS | JP |
(d1)|nan |nan | 1  |nan |nan | 1  |
(d2)|nan | 1  |nan |nan |nan |nan |
(d3)| 1  |nan | 1  |nan | 1  |nan |
(d4)|nan |  1 |nan | 1  |nan |nan |
...................................
...................................
Also, d1 = start date
nan = no information about any event.
1 = status bit(positively confirms the event occurence)
"""

def find_events(symbols, d_data, verbose=False):
	# Get the data from the data store
	storename = "Yahoo" # get data from our daily prices source
	# Available field names: open, close, high, low, close, actual_close, volume
	closefield = "close"
	volumefield = "volume"
	window = 10

	if verbose:
            print __name__ + " reading data"
	close = d_data[closefield]
	if verbose:
            print __name__ + " finding events"
	for symbol in symbols:
	    close[symbol][close[symbol]>= 1.0] = np.NAN
	    for i in range(1,len(close[symbol])):
	        if np.isnan(close[symbol][i-1]) and close[symbol][i] < 1.0 :#(i-1)th was > $1, and (i)th is <$1
             		close[symbol][i] = 1.0 #overwriting the price by the bit
	    close[symbol][close[symbol]< 1.0] = np.NAN
	return close

########NEW FILE########
__FILENAME__ = study
#
# Example use of the event profiler
#
import QSTK.qstkstudy.Events as ev
import datetime as dt
import QSTK.qstkstudy.EventProfiler as ep
import numpy as np

if __name__ == '__main__':

    ls_symbols = np.loadtxt('symbol-set1.txt',dtype='S10',comments='#')
    dt_start = dt.datetime(2008,1,1)
    dt_end = dt.datetime(2009,12,31)
    ldt_timestamps = du.getNYSEdays( dt_start, dt_end, dt.timedelta(hours=16) )

    dataobj = da.DataAccess('Yahoo')
    ls_keys = ['open', 'high', 'low', 'close', 'volume', 'actual_close']
    ldf_data = dataobj.get_data(ldt_timestamps, ls_symbols, ls_keys)
    d_data = dict(zip(ls_keys, ldf_data))

    eventMatrix = ev.find_events(ls_symbols,d_data,verbose=True)
    ep.eventprofiler(eventMatrix, d_data,
            i_lookback=20,i_lookforward=20,
            s_filename="MyEventStudy")

########NEW FILE########
__FILENAME__ = testDataAccess
'''
Created on Jun 1, 2010

@author: Shreyas Joshi
@summary: Just a quick way to test the DataAccess class... nothing more "I dare do all that may become a DataAccessTester. Who dares do more is none"
'''

#Due to the momentary lack of a HDF viewer that installs/works without hassle- I decided to write a little something to check if the alpha 
#values were being written properly

#Main begins
#from DataAccess import *
#import DataAccessNew as da
import QSTK.qstkutil.DataAccess as da
import tables as pt
import numpy as np
from itertools import izip 
import time
import dircache

def getStocks(listOfPaths):
        
            listOfStocks=list()
            #Path does not exist
            print "Reading in all stock names..."
            fileExtensionToRemove=".h5"   
            
            for path in listOfPaths:
               stocksAtThisPath=list ()
               
               stocksAtThisPath= dircache.listdir(str(path))
               #Next, throw away everything that is not a .h5 And these are our stocks!
               stocksAtThisPath = filter (lambda x:(str(x).find(str(fileExtensionToRemove)) > -1), stocksAtThisPath)
               #Now, we remove the .h5 to get the name of the stock
               stocksAtThisPath = map(lambda x:(x.partition(str(fileExtensionToRemove))[0]),stocksAtThisPath)
               
               for stock in stocksAtThisPath:
                   listOfStocks.append(stock)
               return listOfStocks
    #readStocksFromFile done




if __name__ == '__main__':
	
	print "Starting..."
	dataItemsList=[]
	
	dataItemsList.append('alphaValue')
	
	
	
	
	
	#for gekko
	#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NASDAQ/")
	#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/Delisted_US_Recent/")
	#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/OTC/")
	#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_AMEX/")
	#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_Delisted/")
	#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE/")
	#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE Arca/")
	#gekko paths end
	
	
	
	listOfStocks= list()
	#listOfStocks.append("AAPL")
	#listOfStocks.append("YHOO")
	#listOfStocks.append("AMZN")
	
	listOfPaths=list()
	listOfPaths.append("C:\\test\\temp\\")
	#listOfPaths.append("C:\\test\\hdf\\")
	
	listOfStocks= getStocks(listOfPaths)
	
	
	
	
	alpha= da.DataAccess (True, listOfPaths, "/StrategyData", "StrategyData", True, listOfStocks) # , 946702800 , 1262322000 
	
	#alpha= da.DataAccess (False, "C:\\test\\temp\\AAPL.h5", "/StrategyData", "StrategyData", True, None) # reading a single hdf5 file
	
	tslist= list(alpha.getTimestampArray())
	
	#for ts in tslist:
	#    for stock in listOfStocks:
	#        print str(stock)+"  "+ str(ts)+"   "+str(alpha.getStockDataItem(str(stock), 'volume', ts)) 
	
	
	
	
	#alpha= da.DataAccess (False, "curveFittingAlphaVals.h5", "/alphaData", "alphaData", True, listOfStocks, None, None, None, dataItemsList)
	
	
	listOfTS= alpha.getTimestampArray()
	for stock in ["AAPL"]:
	            alphaList= alpha.getStockDataList(stock, 'volume')
	            ctr=0
	            for val in alphaList:
	                print "stock: " + str(stock) + ", val: "+str(val) + ", ts: " + str(listOfTS[ctr])
	                ctr+=1
	                
	print "DONE!"                

########NEW FILE########
__FILENAME__ = testLearner
#
# How to plot 2D data for learning
#
# by Tucker Balch
# September 2010
#

#
# imports
#
import numpy as np
import matplotlib.pyplot as plt
import QSTK.qstklearn.kdtknn as kdt
from mpl_toolkits.mplot3d import Axes3D
from pylab import *
import datetime as dt

#
# Choose colors
#
def findcolors(Y):
	miny = min(Y)
	maxy = max(Y)
	tY = (Y-miny)/(maxy-miny)
	colors =[]
	for i in tY:
		if (i>0.66):
			j = min(1,(i-.66)*3)
			colors.append([1,(1-j),0])
		elif (i<=.66) and (i>.33):
			j = (i-.33)*3
			colors.append([j,1,0])
		else:
			j = i*3
			colors.append([0,j,1])
		#print i,j
	return colors

def main():
	#
	# read in and slice up the data
	#
	#data = np.loadtxt('data-classification-prob.csv',delimiter=',',skiprows=1)
	data = np.loadtxt('data-ripple-prob.csv',delimiter=',',skiprows=1)
	X1 = data[:,0]
	X2 = data[:,1]
	Y  = data[:,2]
	colors = findcolors(Y)
	
	#
	# scatter plot X1 vs X2 and colors are Y
	#
	plt.clf()
	fig = plt.figure()
	fig1 = fig.add_subplot(221)
	plt.scatter(X1,X2,c=colors,edgecolors='none')
	plt.xlabel('X1')
	plt.ylabel('X2')
	plt.xlim(-1,1)	# set x scale
	plt.ylim(-1,1)	# set y scale
	plt.title('Training Data 2D View',fontsize=12)
	
	# plot the 3d view
	ax = fig.add_subplot(222,projection='3d')
	ax.scatter(X1,X2,Y,c=colors,edgecolors='none')
	#ax.scatter(X1,X2,Y,c=colors)
	ax.set_xlabel('X1')
	ax.set_ylabel('X2')
	ax.set_zlabel('Y')
	ax.set_xlim3d(-1,1)
	ax.set_ylim3d(-1,1)
	ax.set_zlim3d(-1,1)
	plt.title('Training Data 3D View',fontsize=12)
	
	##########
	# OK, now create and train a learner
	#
	learner = kdt.kdtknn(k=30,method='mean')
	numpoints = X1.shape[0]
	dataX = np.zeros([numpoints,2])
	dataX[:,0] = X1
	dataX[:,1] = X2
	
	trainsize = floor(dataX.shape[0] * .6)
	learner.addEvidence(dataX[0:trainsize],dataY=Y[0:trainsize])
	steps = 50.0
	stepsize = 2.0/steps
	
	Xtest = np.zeros([steps*steps,2])
	count = 0
	for i in np.arange(-1,1,stepsize):
		for j in np.arange(-1,1,stepsize):
			Xtest[count,0] = i + stepsize/2
			Xtest[count,1] = j + stepsize/2
			count = count+1
	Ytest = learner.query(Xtest) # to check every point
	
	#
	# Choose colors
	#
	colors = findcolors(Ytest)
	
	#
	# scatter plot X1 vs X2 and colors are Y
	#
	fig1 = fig.add_subplot(223)
	plt.scatter(Xtest[:,0],Xtest[:,1],c=colors,edgecolors='none')
	plt.xlabel('X1')
	plt.ylabel('X2')
	plt.xlim(-1,1)	# set x scale
	plt.ylim(-1,1)	# set y scale
	plt.title('Learned Model 2D',fontsize=12)
	
	# plot the 3d view
	ax = fig.add_subplot(224,projection='3d')
	ax.scatter(Xtest[:,0],Xtest[:,1],Ytest,c=colors,edgecolors='none')
	#X1 = Xtest[:,0]
	#X2 = Xtest[:,1]
	#X1 = np.reshape(X1,(steps,steps))
	#X2 = np.reshape(X2,(steps,steps))
	#Ytest = np.reshape(Ytest,(steps,steps))
	ax.set_xlabel('X1')
	ax.set_ylabel('X2')
	ax.set_zlabel('Y')
	ax.set_xlim3d(-1,1)
	ax.set_ylim3d(-1,1)
	ax.set_zlim3d(-1,1)
	plt.title('Learned Model 3D',fontsize=12)
	savefig("scatterdata3D.png", format='png')
	plt.close()
	
	#
	# Compare to ground truth
	#
	print 'trainsize ' + str(trainsize)
	Ytruth = Y[-trainsize:]
	print 'Ytruth.shape ' + str(Ytruth.shape)
	Xtest = dataX[-trainsize:,:]
	print 'Xtest.shape ' + str(Xtest.shape)
	Ytest = learner.query(Xtest) # to check every point
	print 'Ytest.shape ' + str(Ytest.shape)
	
	plt.clf()
	plt.scatter(Ytruth,Ytest,edgecolors='none')
	plt.xlim(-1.2,1.2)	# set x scale
	plt.ylim(-1.2,1.2)	# set y scale
	plt.xlabel('Ground Truth')
	plt.ylabel('Estimated')
	savefig("scatterdata.png", format='png')
	
	print corrcoef(Ytruth,Ytest)

if __name__ == '__main__':
	main()

########NEW FILE########
__FILENAME__ = csv2fund
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.


Created on October, 2, 2012

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Contains converter for csv files to fund values. Also analyzes transactions.

'''

# import sys
import pandas
import csv
# from ofxparse import OfxParser
import numpy
import pickle
import datetime as dt
import dateutil.parser as dp
from Bin import report
from QSTK.qstksim import _calculate_leverage
from QSTK.qstkutil import qsdateutil as du
from QSTK.qstkutil import DataEvolved as de


def calculate_efficiency(dt_start_date, dt_end_date, s_stock):
    """
    @summary calculates the exit-entry/high-low trade efficiency of a stock from historical data
    @param start_date: entry point for the trade
    @param end_date: exit point for the trade
    @param stock: stock to compute efficiency for
    @return: float representing efficiency
    """
    # Get the data from the data store
    dataobj = de.DataAccess('mysql')

    # Get desired timestamps
    timeofday=dt.timedelta(hours=16)
    timestamps = du.getNYSEdays(dt_start_date,dt_end_date+dt.timedelta(days=1),timeofday)
    historic = dataobj.get_data( timestamps, [s_stock] ,["close"] )[0]
    # print "######"
    # print historic
    hi=numpy.max(historic.values)
    low=numpy.min(historic.values)
    entry=historic.values[0]
    exit_price=historic.values[-1]
    return (((exit_price-entry)/(hi-low))[0])

def _ignore_zeros_average(array):
    """
    @summary internal function computes average ignoring zero elements
    @param array: array to compute average for
    @return: float average
    """
    i_num=0
    f_sum=0
    for var in array:
        if var!=0:
            i_num=i_num+1
            f_sum=f_sum+var
    if i_num==0:
        i_num=1
    return f_sum/i_num

def analyze_transactions(filename, plot_name, share_table, show_transactions=False):
    """
    @summary computes various statistics for given filename and appends to report assumed via plot_name
    @param filename: file of past transactions
    @param plot_name: name of report
    """
    html_file  =  open("./"+plot_name+"/report-"+plot_name+".html","a")
    html_file.write("<pre>\n\nTransaction Statistics\n")
    #calc stats

    #first pass
    reader=csv.reader(open(filename,'rU'), delimiter=',')
    reader.next()
    prev=0
    first=1
    diffs=[]
    volume=0
    start=0
    sold=0
    bought=0
    end=0
    rets=[]
    efficiencies=[]
    holds=[]
    commissions=[]
    # slippage=[]
    buy_dates=[] #matrix of when stocks were bought (used for matches)
    for row in reader:
        weighted_ret=0
        weighted_e=0
        weighted_hold=0
        num_stocks=0
        volume+=1
        if(row[7]!=''):
            commissions.append(float(row[7]))
            # slippage.append(float(row[8]))
        if first:
            #add na for first trade efficiency
            start=dp.parse(row[3])
            first=0
            prev=dp.parse(row[3])
        else:
            if row[2] == "Buy":
                bought=bought+float(row[5])
                buy_dates.append({"date":dp.parse(row[3]),"stock":row[0],"amount":row[4],"price":float(row[5])})
            elif row[2] == "Sell":
                #sold at price
                sold=sold+float(row[5])
                #get number of stocks for this sell
                stocks=float(row[4])
                #try and match trade (grab first date of stocks)
                for date in buy_dates:
                    #while stocks are left
                    if(stocks>0):
                        #match a date
                        if(date["stock"]==row[1]):
                            stocks_sold=0
                            #use as many stocks from date as necessary
                            leftover=float(date["amount"])-stocks
                            if(leftover>0):
                                date["amount"]=leftover
                                stocks_sold=stocks
                                #compute stats
                                temp_e=calculate_efficiency(date["date"], dp.parse(row[3]), row[0])
                                weighted_ret=(weighted_ret*num_stocks+(float(row[5])/date["price"])*stocks_sold)/(num_stocks+stocks_sold)
                                weighted_hold=(weighted_hold*num_stocks+(dp.parse(row[3])-date["date"]).days*stocks_sold)/(num_stocks+stocks_sold)
                                weighted_e=(weighted_e*num_stocks+temp_e*stocks_sold)/(num_stocks+stocks_sold)
                                num_stocks=num_stocks+stocks_sold
                                break
                            else:
                                stocks_sold=float(date["amount"])
                                stocks=stocks-stocks_sold
                                #compute stats
                                temp_e=calculate_efficiency(date["date"], dp.parse(row[3]), row[0])
                                weighted_ret=(weighted_ret*num_stocks+(float(row[5])/date["price"])*stocks_sold)/(num_stocks+stocks_sold)
                                weighted_hold=(weighted_hold*num_stocks+(dp.parse(row[3])-date["date"]).days*stocks_sold)/(num_stocks+stocks_sold)
                                weighted_e=(weighted_e*num_stocks+temp_e*stocks_sold)/(num_stocks+stocks_sold)
                                num_stocks=num_stocks+stocks_sold
                                date["stock"]="DONE"
                                #buy_dates.remove(date)
            #elif row[2] == "Sell Short":
                #do nothing
            #elif row[2] == "Buy to Cover":
                #do nothing
            # elif row[2] == "Deposit Cash":

            if(prev!=dp.parse(row[3])):
                diffs.append(dp.parse(row[3])-prev)
                prev=dp.parse(row[3])
                end=prev
        holds.append(weighted_hold)
        efficiencies.append(weighted_e)
        rets.append(weighted_ret*100)

    avg_period=sum(diffs, dt.timedelta(0))/len(diffs)
    avg_hold=_ignore_zeros_average(holds)
    t=sold/(bought+sold)
    turnover=t/(end-start).days
    efficiency=_ignore_zeros_average(efficiencies)
    avg_com=_ignore_zeros_average(commissions)
    # avg_slip=_ignore_zeros_average(slippage)
    avg_ret=_ignore_zeros_average(rets)

    #print stats
    html_file.write("\nNumber of trades:         %10d" % volume)
    html_file.write("\nAverage Trading Period:   %10s" % str(avg_period).split(",")[0])
    html_file.write("\nAverage Position Hold:    %5d days" % avg_hold)
    html_file.write("\nAverage Daily Turnover:   %%%9.4f" % (turnover*100))
    html_file.write("\nAverage Trade Efficiency: %%%9.4f" % (efficiency*100))
    html_file.write("\nAverage Commissions:      %10d" % avg_com)
    # html_file.write("\nAverage Slippage:         %10d" % avg_slip)
    html_file.write("\nAverage Return:           %%%9.4f\n\n" % avg_ret)

    html_file.write("Positions by Date\n")
    for date in share_table.index:
        html_file.write("\nPosition for day: "+str(date).split()[0])
        html_file.write("\n")
        i=0
        for item in share_table.ix[date]:
            if(item>0):
                html_file.write("%15s" % str(share_table.columns[i]))
            i=i+1
        html_file.write("\n")
        for item in share_table.ix[date]:
            if(item>0):
                html_file.write("%15s" % str(item))
        html_file.write("\n\nTransactions for day:\n")
        reader=csv.reader(open(filename,'rU'), delimiter=',')
        a=0
        cash=0
        if(show_transactions):
            for row in reader:
                if a==0:
                    html_file.write("   Date      | ")
                    html_file.write("   Name    | ")
                    html_file.write("   Type    | ")
                    html_file.write("  Price    | ")
                    html_file.write("  Shares   | ")
                    html_file.write("Commission | ")
                    # html_file.write(" Slippage  | ")
                    html_file.write("OnHand Cash| ")
                    html_file.write("Efficiency  | ")
                    html_file.write(" Returns    ")
                    html_file.write(" | ")
                    html_file.write("\n")
                    a=1
                else:
                    compute=False
                    if(len(str(row[3]).split())>2):
                        compute=(dt.datetime.strptime(str(row[3]),"%b %d, %Y")==date)
                    else:
                        compute=(str(row[3])==str(date))
                    if(compute):
                        var=row[2]
                        if var == "Cash Deposit":
                            cash=cash+float(row[6])
                            var="Deposit"
                        elif var == "Cash Withdraw":
                            cash=cash-float(row[6])
                            var="Withdraw"
                        else:
                            cash=cash-float(row[6])
                        var=var.split(" ")[0]
                        html_file.write("%12s | " % str(row[3].split(':')[0]))
                        html_file.write("%10s | " % str(row[0]))
                        html_file.write("%10s | " % str(var))
                        html_file.write("%10s | " % str(row[5]))
                        html_file.write("%10s | " % str(row[4]))
                        html_file.write("%10s | " % str(row[7]))
                        html_file.write("%10s | " % str(row[8]))
                        html_file.write("%10s | " % str(round(cash,2)))
                        html_file.write(" %%%9.2f | " % (efficiencies[a-1]*100))
                        html_file.write(" %%%9.2f " % (rets[a-1]))
                        a=a+1
                        html_file.write(" | ")
                        html_file.write("\n")

    html_file.close()

def csv2fund(filename):
    """
    @summary converts a csv file to a fund with the given starting value
    @param filename: csv file to open and convert
    @param start_val: starting value for the portfolio
    @return fund : time series containing fund value over time
    @return leverage : time series containing fund value over time
    @return slippage : value of slippage over the csv time
    @return commissions : value of slippage over the csv time
    """
    reader=csv.reader(open(filename,'rU'), delimiter=',')
    reader.next()
    symbols=[]
    dates=[]
    for row in reader:
        if not(row[0] in symbols):
            if not(row[0]=="cash" or row[0] == ''):
                symbols.append(row[0])
        if not(dp.parse(row[3]) in dates):
            dates.append(dp.parse(row[3]))
    print symbols
    reader=csv.reader(open(filename,'rU'), delimiter=',')
    reader.next()
    if not("_CASH" in symbols):
        symbols.append("_CASH")
    vals=numpy.zeros([len(dates),len(symbols)])
    share_table=pandas.DataFrame(index=dates, columns=symbols, data=vals)
    share_table["_CASH"]=0
    # share_table["_CASH"].ix[0]=start_val
    commissions=0

    for row in reader:
        date=dp.parse(row[3])
        order_type=row[2]
        sym = row[0]
        if order_type != 'Deposit Cash':
            price = float(row[5])
            shares=float(row[4])
            commission=float(row[7])

        if order_type=="Deposit Cash":
            cash = float(row[6])
            share_table["_CASH"].ix[date]=share_table.ix[date]["_CASH"]+ cash
        if order_type=="Buy":
            share_table.ix[date][sym]+=shares
            commissions=commissions+float(commission)
            share_table["_CASH"].ix[date]=share_table.ix[date]["_CASH"]-float(price)*float(shares)-float(commission)
        if order_type=="Sell":
            share_table[sym].ix[date]+=-1*shares
            commissions=commissions+float(commission)
            share_table["_CASH"].ix[date]=share_table.ix[date]["_CASH"]+float(price)*float(shares)-float(commission)
        if order_type=="Sell Short":
            share_table[sym].ix[date]+=-1*shares
            commissions=commissions+float(commission)
            share_table["_CASH"].ix[date]=share_table.ix[date]["_CASH"]+float(price)*float(shares)-float(commission)
        if order_type=="Buy to Cover":
            share_table.ix[date][sym]+=shares
            commissions=commissions+float(commission)
            share_table["_CASH"].ix[date] = share_table.ix[date]["_CASH"]-float(price)*float(shares)-float(commission)
    share_table = share_table.cumsum()
    time_index = sorted(share_table.index)
    column_index = sorted(share_table.columns)
    share_table = share_table.reindex(index=time_index, columns=column_index)
    i_start_cash = share_table["_CASH"].ix[0]
    print i_start_cash
    return [share_table, commissions, i_start_cash]

# def ofx2fund(filename, start_val):
#     """
#     @summary converts a ofx file to a fund with the given starting value
#     @param filename: ofx file to open and convert
#     @param start_val: starting value for the portfolio
#     @return fund : time series containing fund value over time
#     @return leverage : time series containing fund value over time
#     @return slippage : value of slippage over the ofx time
#     @return commissions : value of slippage over the ofx time
#     """
#     try:
#         from ofxparse import OfxParser
#     except:
#         print "ofxparse is required to use ofx2fund"
#         exit()
#     ofx = OfxParser.parse(file(filename))
#     symbols=[]
#     dates=[]
#     for order in ofx.account.statement.transactions:
#         sym=order.security.split(":")[1]
#         if not(sym in symbols):
#             symbols.append(sym)
#         date=order.tradeDate
#         if not(date in dates):
#             dates.append(date)
#     dates.sort()
#     vals=numpy.zeros([len(dates),len(symbols)+1])
#     symbols.append("_CASH")
#     share_table=pandas.DataFrame(index=dates, columns=symbols, data=vals)
#     share_table.ix[0]["_CASH"]=start_val
#     for order in ofx.account.statement.transactions:
#         sym=order.security.split(":")[1]
#         share_table.ix[order.tradeDate][sym]=order.units
#         share_table.ix[order.tradeDate]["_CASH"]=share_table.ix[order.tradeDate]["_CASH"]-float(order.unit_price)*float(order.units)
#     share_table=share_table.cumsum()
#     slippage=0
#     commissions=0
#     return [share_table, slippage, commissions]


def share_table2fund(share_table):
    """
    @summary converts data frame of shares into fund values
    @param share_table: data frame containing shares on days transactions occured
    @return fund : time series containing fund value over time
    @return leverage : time series containing fund value over time
    """
    # Get the data from the data store
    dataobj = de.DataAccess('mysql')
    startday = share_table.index[0]
    endday = share_table.index[-1]

    symbols = list(share_table.columns)
    symbols.remove('_CASH')

    # print symbols

    # Get desired timestamps
    timeofday = dt.timedelta(hours=16)
    timestamps = du.getNYSEdays(startday - dt.timedelta(days=5), endday + dt.timedelta(days=1), timeofday)
    historic = dataobj.get_data(timestamps, symbols, ["close"])[0]
    historic.fillna(method='ffill', inplace=True)
    historic["_CASH"] = 1
    closest = historic[historic.index <= share_table.index[0]].ix[:]
    ts_leverage = pandas.Series(0, index=[closest.index[-1]])

    # start shares/fund out as 100% cash
    first_val = closest.ix[-1] * share_table.ix[0]
    fund_ts = pandas.Series([first_val.sum(axis=1)], index=[closest.index[-1]])
    prev_row = share_table.ix[0]
    for row_index, row in share_table.iterrows():
        # print row_index
        trade_price = historic.ix[row_index:].ix[0:1]
        trade_date = trade_price.index[0]

        # print trade_date

        # get stock prices on all the days up until this trade
        to_calculate = historic[(historic.index <= trade_date) & (historic.index > fund_ts.index[-1])]
        # multiply prices by our current shares
        values_by_stock = to_calculate * prev_row

        # for date, sym in values_by_stock.iteritems():
        #     print date,sym
        # print values_by_stock
        prev_row = row
        #update leverage
        ts_leverage = _calculate_leverage(values_by_stock, ts_leverage)

        # calculate total value and append to our fund history
        fund_ts = fund_ts.append([values_by_stock.sum(axis=1)])
    return [fund_ts, ts_leverage]

if __name__ == "__main__":
    filename = "Strat.csv"
    plot_name = "Strategy"
    print "load csv"
    [share_table, commissions, i_start_cash] = csv2fund(filename)
    print share_table
    [fund_ts, ts_leverage] = share_table2fund(share_table)
    print "print report"
    print fund_ts
    report.print_stats(fund_ts, ["SPY"], plot_name, directory="./" + plot_name, commissions=commissions, i_start_cash=i_start_cash)
    print "analyze transactions"
    #Generate new plot based off transactions alone
    analyze_transactions(filename, plot_name, share_table, True)
    print "done"

########NEW FILE########
__FILENAME__ = report
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''

from os import path, makedirs
from os import sys
from QSTK.qstkutil import DataAccess as de
from QSTK.qstkutil import qsdateutil as du
from QSTK.qstkutil import tsutil as tsu
from QSTK.qstkutil import fundutil as fu
from dateutil.relativedelta import relativedelta
import numpy as np
from math import log10
import locale
from pylab import savefig
from matplotlib import pyplot
from matplotlib import gridspec
import matplotlib.dates as mdates
import cPickle
import datetime as dt
import pandas
import numpy as np
from copy import deepcopy
import scipy.stats as scst

def _dividend_rets_funds(df_funds, f_dividend_rets):

    df_funds_copy = deepcopy(df_funds)
    f_price = deepcopy(df_funds_copy[0])

    df_funds_copy.values[1:] = (df_funds_copy.values[1:]/df_funds_copy.values[0:-1])
    df_funds_copy.values[0] = 1

    df_funds_copy = df_funds_copy + f_dividend_rets

    na_funds_copy = np.cumprod(df_funds_copy.values)
    na_funds_copy = na_funds_copy*f_price

    df_funds = pandas.Series(na_funds_copy, index = df_funds_copy.index)

    return df_funds

def print_header(html_file, name):
    """
    @summary prints header of report html file
    """
    html_file.write("<HTML>\n")
    html_file.write("<HEAD>\n")
    html_file.write("<TITLE>QSTK Generated Report:" + name + "</TITLE>\n")
    html_file.write("</HEAD>\n\n")
    html_file.write("<BODY>\n\n")

def print_footer(html_file):
    """
    @summary prints footer of report html file
    """
    html_file.write("</BODY>\n\n")
    html_file.write("</HTML>")

def get_annual_return(fund_ts, years):
    """
    @summary prints annual return for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    lf_ret=[]
    for year in years:
        year_vals = []
        for date in fund_ts.index:
            if(date.year ==year):
                year_vals.append([fund_ts.ix[date]])
        day_rets = tsu.daily1(year_vals)
        ret = tsu.get_ror_annual(day_rets)
        ret=float(ret)
        lf_ret.append(ret*100) #" %+8.2f%%" % (ret*100)
    return lf_ret

def get_winning_days(fund_ts, years):
    """
    @summary prints winning days for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    s_ret=""
    for year in years:
        year_vals = []
        for date in fund_ts.index:
            if(date.year==year):
                year_vals.append([fund_ts.ix[date]])
        ret = fu.get_winning_days(year_vals)
        s_ret+=" % + 8.2f%%" % ret
    return s_ret

def get_max_draw_down(fund_ts, years):
    """
    @summary prints max draw down for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    s_ret=""
    for year in years:
        year_vals = []
        for date in fund_ts.index:
            if(date.year==year):
                year_vals.append(fund_ts.ix[date])
        ret = fu.get_max_draw_down(year_vals)
        s_ret+=" % + 8.2f%%" % (ret*100)
    return s_ret

def get_daily_sharpe(fund_ts, years):
    """
    @summary prints sharpe ratio for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    s_ret=""
    for year in years:
        year_vals = []
        for date in fund_ts.index:
            if(date.year==year):
                year_vals.append([fund_ts.ix[date]])
        ret = fu.get_sharpe_ratio(year_vals)
        s_ret+=" % + 8.2f " % ret
    return s_ret

def get_daily_sortino(fund_ts, years):
    """
    @summary prints sortino ratio for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    s_ret=""
    for year in years:
        year_vals = []
        for date in fund_ts.index:
            if(date.year==year):
                year_vals.append([fund_ts.ix[date]])
        ret = fu.get_sortino_ratio(year_vals)
        s_ret+=" % + 8.2f " % ret
    return s_ret

def get_std_dev(fund_ts):
    """
    @summary gets standard deviation of returns for a fund as a string
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    fund_ts=fund_ts.fillna(method='pad')
    fund_ts=fund_ts.fillna(method='bfill')
    ret=np.std(tsu.daily(fund_ts.values))*10000
    return ("%+7.2f bps " % ret)


def ks_statistic(fund_ts):
    fund_ts = deepcopy(fund_ts)
    if len(fund_ts.values) > 60:
        seq1 = fund_ts.values[0:-60]
        seq2 = fund_ts.values[-60:]
        tsu.returnize0(seq1)
        tsu.returnize0(seq2)
        (ks, p) = scst.ks_2samp(seq1, seq2)
        return ks, p
    # elif len(fund_ts.values) > 5:
    #     seq1 = fund_ts.values[0:-5]
    #     seq2 = fund_ts.values[-5:]
    #     (ks, p) = scst.ks_2samp(seq1, seq2)
    #     return ks, p

    ks = -1
    p = -1
    return ks, p

def ks_statistic_calc(fund_ts_past, fund_ts_month):
    try:
        seq1 = deepcopy(fund_ts_past.values)
        seq2 = deepcopy(fund_ts_month.values)
        tsu.returnize0(seq1)
        tsu.returnize0(seq2)
        (ks, p) = scst.ks_2samp(seq1, seq2)
        return ks, p
    except:
        return -1,-1

def print_industry_coer(fund_ts, ostream):
    """
    @summary prints standard deviation of returns for a fund
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    industries = [['$DJUSBM', 'Materials'],
    ['$DJUSNC', 'Goods'],
    ['$DJUSCY', 'Services'],
    ['$DJUSFN', 'Financials'],
    ['$DJUSHC', 'Health'],
    ['$DJUSIN', 'Industrial'],
    ['$DJUSEN', 'Oil & Gas'],
    ['$DJUSTC', 'Technology'],
    ['$DJUSTL', 'TeleComm'],
    ['$DJUSUT', 'Utilities']]
    for i in range(0, len(industries) ):
        if(i%2==0):
            ostream.write("\n")
        #load data
        norObj = de.DataAccess('Yahoo')
        ldtTimestamps = du.getNYSEdays( fund_ts.index[0], fund_ts.index[-1], dt.timedelta(hours=16) )
        ldfData = norObj.get_data( ldtTimestamps, [industries[i][0]], ['close'] )
        #get corelation
        ldfData[0]=ldfData[0].fillna(method='pad')
        ldfData[0]=ldfData[0].fillna(method='bfill')
        a=np.corrcoef(np.ravel(tsu.daily(ldfData[0][industries[i][0]])),np.ravel(tsu.daily(fund_ts.values)))
        b=np.ravel(tsu.daily(ldfData[0][industries[i][0]]))
        f=np.ravel(tsu.daily(fund_ts))
        fBeta, unused = np.polyfit(b,f,1)
        ostream.write("%10s(%s):%+6.2f,   %+6.2f   " % (industries[i][1], industries[i][0], a[0,1], fBeta))

def print_other_coer(fund_ts, ostream):
    """
    @summary prints standard deviation of returns for a fund
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    industries = [['$SPX', '    S&P Index'],
    ['$DJI', '    Dow Jones'],
    ['$DJUSEN', 'Oil & Gas'],
    ['$DJGSP', '     Metals']]
    for i in range(0, len(industries) ):
        if(i%2==0):
            ostream.write("\n")
        #load data
        norObj =de.DataAccess('Yahoo')
        ldtTimestamps = du.getNYSEdays( fund_ts.index[0], fund_ts.index[-1], dt.timedelta(hours=16) )
        ldfData = norObj.get_data( ldtTimestamps, [industries[i][0]], ['close'] )
        #get corelation
        ldfData[0]=ldfData[0].fillna(method='pad')
        ldfData[0]=ldfData[0].fillna(method='bfill')
        a=np.corrcoef(np.ravel(tsu.daily(ldfData[0][industries[i][0]])),np.ravel(tsu.daily(fund_ts.values)))
        b=np.ravel(tsu.daily(ldfData[0][industries[i][0]]))
        f=np.ravel(tsu.daily(fund_ts))
        fBeta, unused = np.polyfit(b,f,1)
        ostream.write("%10s(%s):%+6.2f,   %+6.2f   " % (industries[i][1], industries[i][0], a[0,1], fBeta))


def print_benchmark_coer(fund_ts, benchmark_close, sym,  ostream):
    """
    @summary prints standard deviation of returns for a fund
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    fund_ts=fund_ts.fillna(method='pad')
    fund_ts=fund_ts.fillna(method='bfill')
    benchmark_close=benchmark_close.fillna(method='pad')
    benchmark_close=benchmark_close.fillna(method='bfill')
    faCorr=np.corrcoef(np.ravel(tsu.daily(fund_ts.values)),np.ravel(tsu.daily(benchmark_close)));
    b=np.ravel(tsu.daily(benchmark_close))
    f=np.ravel(tsu.daily(fund_ts))
    fBeta, unused = np.polyfit(b,f, 1);
    print_line(sym+"Correlattion","%+6.2f" % faCorr[0,1],i_spacing=3,ostream=ostream)
    print_line(sym+"Beta","%+6.2f" % fBeta,i_spacing=3,ostream=ostream)

def print_monthly_returns(fund_ts, years, ostream):
    """
    @summary prints monthly returns for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    ostream.write("    ")
    month_names = du.getMonthNames()
    for name in month_names:
        ostream.write("    " + str(name))
    ostream.write("\n")
    i = 0
    mrets = tsu.monthly(fund_ts)
    for year in years:
        ostream.write(str(year))
        months = du.getMonths(fund_ts, year)
        for k in range(1, months[0]):
            ostream.write("       ")
        for month in months:
            ostream.write(" % + 6.2f" % (mrets[i]*100))
            i += 1
        ostream.write("\n")



def print_monthly_turnover(fund_ts, years, ts_turnover, ostream):
    """
    @summary prints monthly returns for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    ostream.write("    ")
    month_names = du.getMonthNames()
    for name in month_names:
        ostream.write("    " + str(name))
    ostream.write("\n")
    i = 0
    # mrets = tsu.monthly(fund_ts)
    for year in years:
        ostream.write(str(year))
        months = du.getMonths(ts_turnover, year)
        if months != []:
            for k in range(1, months[0]):
                ostream.write("       ")
        for month in months:
            ostream.write(" % + 6.2f" % (ts_turnover[i]*100))
            i += 1
        ostream.write("\n")
        
def print_monthly_ks(fund_ts, years, ostream):
    """
    @summary prints monthly returns for given fund and years to the given stream
    @param fund_ts: pandas fund time series
    @param years: list of years to print out
    @param ostream: stream to print to
    """
    ostream.write("    ")
    month_names = du.getMonthNames()
    for name in month_names:
        ostream.write("    " + str(name))
    ostream.write("\n")

    # mrets = tsu.monthly(fund_ts)
    m_str = []

    for i, year in enumerate(years):
        months = du.getMonths(fund_ts, year)
        for j, month in enumerate(months):
            if i == 0 and j < 3:
                m_str.append('    ')
            else:
                # dt_st = max(fund_ts.index[0], dt.datetime(year, month, 1)-relativedelta(months=6))
                dt_st = fund_ts.index[0]
                dt_today = dt.datetime(year, month, 1) - relativedelta(months=2)
                dt_end = min(dt.datetime(year, month, 1) + relativedelta(months=1) + dt.timedelta(hours=-5), fund_ts.index[-1])
                fund_ts_past = fund_ts.ix[dt_st: dt_today]
                fund_ts_month = fund_ts.ix[dt_today: dt_end]
                ks, p = ks_statistic_calc(fund_ts_past, fund_ts_month)
                if not(ks == -1 or p == -1):
                    if ks < p:
                        m_str.append('PASS')
                    else:
                        m_str.append('FAIL')
                else:
                    m_str.append('    ')

    i = 0
    for year in years:
        ostream.write(str(year))
        months = du.getMonths(fund_ts, year)
        for k in range(1, months[0]):
            ostream.write("       ")
        for month in months:
            ostream.write("%7s" % (m_str[i]))
            i = i + 1
        ostream.write("\n")


def print_years(years, ostream):
    ostream.write("\n")
    s_line=""
    s_line2=""
    for f_token in years:
        s_line+="%9d " % f_token
        s_line2+="%10s" % '------'

    ostream.write("%35s %s%30s\n" % ("                  ", " "*4, s_line))
    ostream.write("%35s %s%30s\n" % ("                  ", " "*4, s_line2))


def print_line(s_left_side, s_right_side, i_spacing=0, ostream="stdout"):
    ostream.write("%35s:%s%30s\n" % (s_left_side, " "*i_spacing, s_right_side))

def print_stats(fund_ts, benchmark, name, lf_dividend_rets=0.0, original="",s_fund_name="Fund",
    s_original_name="Original", d_trading_params="", d_hedge_params="", s_comments="", directory = False,
    leverage = False, s_leverage_name="Leverage", commissions = 0, slippage = 0, borrowcost = 0, ostream = sys.stdout, 
    i_start_cash=1000000, ts_turnover="False"):
    """
    @summary prints stats of a provided fund and benchmark
    @param fund_ts: fund value in pandas timeseries
    @param benchmark: benchmark symbol to compare fund to
    @param name: name to associate with the fund in the report
    @param directory: parameter to specify printing to a directory
    @param leverage: time series to plot with report
    @param commissions: value to print with report
    @param slippage: value to print with report
    @param ostream: stream to print stats to, defaults to stdout
    """

    #Set locale for currency conversions
    locale.setlocale(locale.LC_ALL, '')

    if original != "" and type(original) != type([]):
        original = [original]
        if type(s_original_name) != type([]):
            s_original_name = [s_original_name]

    #make names length independent for alignment
    s_formatted_original_name = []
    for name_temp in s_original_name:
        s_formatted_original_name.append("%15s" % name_temp)
    s_formatted_fund_name = "%15s" % s_fund_name

    fund_ts=fund_ts.fillna(method='pad')
    fund_ts=fund_ts.fillna(method='bfill')
    fund_ts=fund_ts.fillna(1.0)
    if directory != False :
        if not path.exists(directory):
            makedirs(directory)

        sfile = path.join(directory, "report-%s.html" % name )
        splot = "plot-%s.png" % name
        splot_dir =  path.join(directory, splot)
        ostream = open(sfile, "wb")
        ostream.write("<pre>")
        print "writing to ", sfile

        if type(original)==type("str"):
            if type(leverage)!=type(False):
                print_plot(fund_ts, benchmark, name, splot_dir, lf_dividend_rets, leverage=leverage, i_start_cash = i_start_cash, s_leverage_name=s_leverage_name)
            else:
                print_plot(fund_ts, benchmark, name, splot_dir, lf_dividend_rets, i_start_cash = i_start_cash)
        else:
            if type(leverage)!=type(False):
                print_plot([fund_ts, original], benchmark, name, splot_dir, s_original_name, lf_dividend_rets,
                             leverage=leverage, i_start_cash = i_start_cash, s_leverage_name=s_leverage_name)
            else:
                print_plot([fund_ts, original], benchmark, name, splot_dir, s_original_name, lf_dividend_rets, i_start_cash = i_start_cash)

    start_date = fund_ts.index[0].strftime("%m/%d/%Y")
    end_date = fund_ts.index[-1].strftime("%m/%d/%Y")
    ostream.write("Performance Summary for "\
	 + str(path.basename(name)) + " Backtest\n")
    ostream.write("For the dates " + str(start_date) + " to "\
                                       + str(end_date) + "")

    #paramater section
    if d_trading_params!="":
        ostream.write("\n\nTrading Paramaters\n\n")
        for var in d_trading_params:
            print_line(var, d_trading_params[var],ostream=ostream)
    if d_hedge_params!="":
        ostream.write("\nHedging Paramaters\n\n")
        if type(d_hedge_params['Weight of Hedge']) == type(float):
            d_hedge_params['Weight of Hedge'] = str(int(d_hedge_params['Weight of Hedge']*100)) + '%'
        for var in d_hedge_params:
            print_line(var, d_hedge_params[var],ostream=ostream)

    #comment section
    if s_comments!="":
        ostream.write("\nComments\n\n%s" % s_comments)


    if directory != False :
        ostream.write("\n\n<img src="+splot+" width=700 />\n\n")

    mult = i_start_cash/fund_ts.values[0]


    timeofday = dt.timedelta(hours = 16)
    timestamps = du.getNYSEdays(fund_ts.index[0], fund_ts.index[-1], timeofday)
    dataobj =de.DataAccess('Yahoo')
    years = du.getYears(fund_ts)
    benchmark_close = dataobj.get_data(timestamps, benchmark, ["close"], \
                                                     verbose = False)[0]
    for bench_sym in benchmark:
        benchmark_close[bench_sym]=benchmark_close[bench_sym].fillna(method='pad')
        benchmark_close[bench_sym]=benchmark_close[bench_sym].fillna(method='bfill')
        benchmark_close[bench_sym]=benchmark_close[bench_sym].fillna(1.0)

    if type(lf_dividend_rets) != type(0.0):
        for i,sym in enumerate(benchmark):
            benchmark_close[sym] = _dividend_rets_funds(benchmark_close[sym], lf_dividend_rets[i])

    ostream.write("Resulting Values in $ with an initial investment of "+ locale.currency(int(round(i_start_cash)), grouping=True) + "\n")

    print_line(s_formatted_fund_name+" Resulting Value"," %15s, %10.2f%%" % (locale.currency(int(round(fund_ts.values[-1]*mult)), grouping=True), \
                                                     float(100*((fund_ts.values[-1]/fund_ts.values[0])-1))), i_spacing=4, ostream=ostream)

    # if type(original)!=type("str"):
    #     mult3 = i_start_cash / original.values[0]
    #     # print_line(s_formatted_original_name +" Resulting Value",(locale.currency(int(round(original.values[-1]*mult3)), grouping=True)),i_spacing=3, ostream=ostream)
    #     print_line(s_formatted_original_name+" Resulting Value"," %15s, %10.2f%%" % (locale.currency(int(round(original.values[-1]*mult3)), grouping=True), \
    #                                                  float(100*((original.values[-1]/original.values[0])-1))), i_spacing=4, ostream=ostream)

    if type(original)!=type("str"):
        for i in range(len(original)):
            mult3 = i_start_cash / original[i].values[0]
            # print_line(s_formatted_original_name +" Resulting Value",(locale.currency(int(round(original[i].values[-1]*mult3)), grouping=True)),i_spacing=3, ostream=ostream)
            print_line(s_formatted_original_name[i]+" Resulting Value"," %15s, %10.2f%%" % (locale.currency(int(round(original[i].values[-1]*mult3)), grouping=True), \
                                                     float(100*((original[i].values[-1]/original[i].values[0])-1))), i_spacing=4, ostream=ostream)

    for bench_sym in benchmark:
        mult2= i_start_cash / benchmark_close[bench_sym].values[0]
        # print_line(bench_sym+" Resulting Value",locale.currency(int(round(benchmark_close[bench_sym].values[-1]*mult2)), grouping=True),i_spacing=3, ostream=ostream)
        print_line(bench_sym+" Resulting Value"," %15s, %10.2f%%" % (locale.currency(int(round(benchmark_close[bench_sym].values[-1]*mult2)), grouping=True), \
                                                     float(100*((benchmark_close[bench_sym].values[-1]/benchmark_close[bench_sym].values[0])-1))), i_spacing=4, ostream=ostream)

    ostream.write("\n")

    # if len(years) > 1:
    print_line(s_formatted_fund_name+" Sharpe Ratio","%10.3f" % fu.get_sharpe_ratio(fund_ts.values)[0],i_spacing=4, ostream=ostream)
    if type(original)!=type("str"):
        for i in range(len(original)):
            print_line(s_formatted_original_name[i]+" Sharpe Ratio","%10.3f" % fu.get_sharpe_ratio(original[i].values)[0],i_spacing=4, ostream=ostream)

    for bench_sym in benchmark:
        print_line(bench_sym+" Sharpe Ratio","%10.3f" % fu.get_sharpe_ratio(benchmark_close[bench_sym].values)[0],i_spacing=4,ostream=ostream)
    ostream.write("\n")


    # KS - Similarity
    # ks, p = ks_statistic(fund_ts);
    # if ks!= -1 and p!= -1:
    #     if ks < p:
    #         ostream.write("\nThe last three month's returns are consistent with previous performance (KS = %2.5f, p = %2.5f) \n\n"% (ks, p))
    #     else:
    #         ostream.write("\nThe last three month's returns are NOT CONSISTENT with previous performance (KS = %2.5f, p = %2.5f) \n\n"% (ks, p))


    ostream.write("Transaction Costs\n")
    print_line("Total Commissions"," %15s, %10.2f%%" % (locale.currency(int(round(commissions)), grouping=True), \
                                                  float((round(commissions)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    print_line("Total Slippage"," %15s, %10.2f%%" % (locale.currency(int(round(slippage)), grouping=True), \
                                                     float((round(slippage)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    print_line("Total Short Borrowing Cost"," %15s, %10.2f%%" % (locale.currency(int(round(borrowcost)), grouping=True), \
                                                     float((round(borrowcost)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    print_line("Total Costs"," %15s, %10.2f%%" % (locale.currency(int(round(borrowcost+slippage+commissions)), grouping=True), \
                                  float((round(borrowcost+slippage+commissions)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    ostream.write("\n")

    print_line(s_formatted_fund_name+" Std Dev of Returns",get_std_dev(fund_ts),i_spacing=8, ostream=ostream)

    if type(original)!=type("str"):
        for i in range(len(original)):
            print_line(s_formatted_original_name[i]+" Std Dev of Returns", get_std_dev(original[i]), i_spacing=8, ostream=ostream)

    for bench_sym in benchmark:
        print_line(bench_sym+" Std Dev of Returns", get_std_dev(benchmark_close[bench_sym]), i_spacing=8, ostream=ostream)

    ostream.write("\n")


    for bench_sym in benchmark:
        print_benchmark_coer(fund_ts, benchmark_close[bench_sym], str(bench_sym), ostream)
    ostream.write("\n")

    ostream.write("\nYearly Performance Metrics")
    print_years(years, ostream)


    s_line=""
    for f_token in get_annual_return(fund_ts, years):
        s_line+=" %+8.2f%%" % f_token
    print_line(s_formatted_fund_name+" Annualized Return",s_line, i_spacing=4, ostream=ostream)


    if type(original)!=type("str"):
        for i in range(len(original)):
            s_line=""
            for f_token in get_annual_return(original[i], years):
                s_line+=" %+8.2f%%" % f_token
            print_line(s_formatted_original_name[i]+" Annualized Return", s_line, i_spacing=4, ostream=ostream)

    for bench_sym in benchmark:
        s_line=""
        for f_token in get_annual_return(benchmark_close[bench_sym], years):
            s_line+=" %+8.2f%%" % f_token
        print_line(bench_sym+" Annualized Return", s_line, i_spacing=4, ostream=ostream)

    print_years(years, ostream)

    print_line(s_formatted_fund_name+" Winning Days",get_winning_days(fund_ts, years), i_spacing=4, ostream=ostream)


    if type(original)!=type("str"):
        for i in range(len(original)):
            print_line(s_formatted_original_name[i]+" Winning Days",get_winning_days(original[i], years), i_spacing=4, ostream=ostream)


    for bench_sym in benchmark:
        print_line(bench_sym+" Winning Days",get_winning_days(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    print_years(years, ostream)

    print_line(s_formatted_fund_name+" Max Draw Down",get_max_draw_down(fund_ts, years), i_spacing=4, ostream=ostream)

    if type(original)!=type("str"):
        for i in range(len(original)):
            print_line(s_formatted_original_name[i]+" Max Draw Down",get_max_draw_down(original[i], years), i_spacing=4, ostream=ostream)


    for bench_sym in benchmark:
        print_line(bench_sym+" Max Draw Down",get_max_draw_down(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    print_years(years, ostream)


    print_line(s_formatted_fund_name+" Daily Sharpe Ratio",get_daily_sharpe(fund_ts, years), i_spacing=4, ostream=ostream)


    if type(original)!=type("str"):
        for i in range(len(original)):
            print_line(s_formatted_original_name[i]+" Daily Sharpe Ratio",get_daily_sharpe(original[i], years), i_spacing=4, ostream=ostream)

    for bench_sym in benchmark:
        print_line(bench_sym+" Daily Sharpe Ratio",get_daily_sharpe(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    print_years(years, ostream)

    print_line(s_formatted_fund_name+" Daily Sortino Ratio",get_daily_sortino(fund_ts, years), i_spacing=4, ostream=ostream)

    if type(original)!=type("str"):
        for i in range(len(original)):
            print_line(s_formatted_original_name[i]+" Daily Sortino Ratio",get_daily_sortino(original[i], years), i_spacing=4, ostream=ostream)


    for bench_sym in benchmark:
        print_line(bench_sym+" Daily Sortino Ratio",get_daily_sortino(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    ostream.write("\n\n\nCorrelation and Beta with DJ Industries for the Fund ")

    print_industry_coer(fund_ts,ostream)

    ostream.write("\n\nCorrelation and Beta with Other Indices for the Fund ")

    print_other_coer(fund_ts,ostream)

    ostream.write("\n\n\nMonthly Returns for the Fund %\n")

    print_monthly_returns(fund_ts, years, ostream)

    if type(ts_turnover) != type("False"):
        ostream.write("\n\nMonthly Turnover for the fund\n")
        print_monthly_turnover(fund_ts, years, ts_turnover, ostream)

    ostream.write("\n\n3 Month Kolmogorov-Smirnov 2-Sample Similarity Test\n")

    print_monthly_ks(fund_ts, years, ostream)

    ks, p = ks_statistic(fund_ts);
    if ks!= -1 and p!= -1:
        ostream.write("\nResults for the Similarity Test over last 3 months : (KS = %2.5f, p = %2.5f) \n\n"% (ks, p))

    if directory != False:
        ostream.write("</pre>")


def print_html(fund_ts, benchmark, name, lf_dividend_rets=0.0, original="",
    s_fund_name="Fund", s_original_name="Original", d_trading_params="", d_hedge_params="",
    s_comments="", directory=False, leverage=False, s_leverage_name="Leverage",commissions=0, slippage=0,
    borrowcost=0, ostream=sys.stdout, i_start_cash=1000000):
    """
    @summary prints stats of a provided fund and benchmark
    @param fund_ts: fund value in pandas timeseries
    @param benchmark: benchmark symbol to compare fund to
    @param name: name to associate with the fund in the report
    @param directory: parameter to specify printing to a directory
    @param leverage: time series to plot with report
    @param commissions: value to print with report
    @param slippage: value to print with report
    @param ostream: stream to print stats to, defaults to stdout
    """

    #Set locale for currency conversions
    locale.setlocale(locale.LC_ALL, '')

    #make names length independent for alignment
    s_formatted_original_name="%15s" % s_original_name
    s_formatted_fund_name = "%15s" % s_fund_name

    fund_ts=fund_ts.fillna(method='pad')
    if directory != False :
        if not path.exists(directory):
            makedirs(directory)

        sfile = path.join(directory, "report-%s.html" % name )
        splot = "plot-%s.png" % name
        splot_dir =  path.join(directory, splot)
        ostream = open(sfile, "wb")
        print "writing to ", sfile

        if type(original)==type("str"):
            if type(leverage)!=type(False):
                print_plot(fund_ts, benchmark, name, splot_dir, lf_dividend_rets, leverage=leverage, i_start_cash = i_start_cash, s_leverage_name=s_leverage_name)
            else:
                print_plot(fund_ts, benchmark, name, splot_dir, lf_dividend_rets, i_start_cash = i_start_cash)
        else:
            if type(leverage)!=type(False):
                print_plot([fund_ts, original], benchmark, name, splot_dir, s_original_name, lf_dividend_rets, leverage=leverage, i_start_cash = i_start_cash, s_leverage_name=s_leverage_name)
            else:
                print_plot([fund_ts, original], benchmark, name, splot_dir, s_original_name, lf_dividend_rets, i_start_cash = i_start_cash)

    print_header(ostream,name)
    start_date = fund_ts.index[0].strftime("%m/%d/%Y")
    end_date = fund_ts.index[-1].strftime("%m/%d/%Y")
    ostream.write("Performance Summary for "\
     + str(path.basename(name)) + " Backtest\n")
    ostream.write("For the dates " + str(start_date) + " to "\
                                       + str(end_date) + "")

    #paramater section
    if d_trading_params!="":
        ostream.write("\n\nTrading Paramaters\n\n")
        for var in d_trading_params:
            print_line(var, d_trading_params[var],ostream=ostream)
    if d_hedge_params!="":
        ostream.write("\nHedging Paramaters\n\n")
        if type(d_hedge_params['Weight of Hedge']) == type(float):
            d_hedge_params['Weight of Hedge'] = str(int(d_hedge_params['Weight of Hedge']*100)) + '%'
        for var in d_hedge_params:
            print_line(var, d_hedge_params[var],ostream=ostream)

    #comment section
    if s_comments!="":
        ostream.write("\nComments\n\n%s" % s_comments)


    if directory != False :
        ostream.write("\n\n<img src="+splot+" width=600 />\n\n")

    mult = i_start_cash/fund_ts.values[0]


    timeofday = dt.timedelta(hours = 16)
    timestamps = du.getNYSEdays(fund_ts.index[0], fund_ts.index[-1], timeofday)
    dataobj =de.DataAccess('Yahoo')
    years = du.getYears(fund_ts)
    benchmark_close = dataobj.get_data(timestamps, benchmark, ["close"])
    benchmark_close=benchmark_close[0]
    for bench_sym in benchmark:
        benchmark_close[bench_sym]=benchmark_close[bench_sym].fillna(method='pad')

    if type(lf_dividend_rets) != type(0.0):
        for i,sym in enumerate(benchmark):
            benchmark_close[sym] = _dividend_rets_funds(benchmark_close[sym], lf_dividend_rets[i])

    ostream.write("Resulting Values in $ with an initial investment of "+ locale.currency(int(round(i_start_cash)), grouping=True) + "\n")

    print_line(s_formatted_fund_name+" Resulting Value",(locale.currency(int(round(fund_ts.values[-1]*mult)), grouping=True)),i_spacing=3, ostream=ostream)

    if type(original)!=type("str"):
        mult3 = i_start_cash / original.values[0]
        print_line(s_formatted_original_name +" Resulting Value",(locale.currency(int(round(original.values[-1]*mult3)), grouping=True)),i_spacing=3, ostream=ostream)

    for bench_sym in benchmark:
        mult2=i_start_cash/benchmark_close[bench_sym].values[0]
        print_line(bench_sym+" Resulting Value",locale.currency(int(round(benchmark_close[bench_sym].values[-1]*mult2)), grouping=True),i_spacing=3, ostream=ostream)

    ostream.write("\n")

    if len(years) > 1:
        print_line(s_formatted_fund_name+" Sharpe Ratio","%10.3f" % fu.get_sharpe_ratio(fund_ts.values)[0],i_spacing=4, ostream=ostream)
        if type(original)!=type("str"):
            print_line(s_formatted_original_name+" Sharpe Ratio","%10.3f" % fu.get_sharpe_ratio(original.values)[0],i_spacing=4, ostream=ostream)

        for bench_sym in benchmark:
            print_line(bench_sym+" Sharpe Ratio","%10.3f" % fu.get_sharpe_ratio(benchmark_close[bench_sym].values)[0],i_spacing=4,ostream=ostream)
        ostream.write("\n")

    ostream.write("Transaction Costs\n")
    print_line("Total Commissions"," %15s, %10.2f%%" % (locale.currency(int(round(commissions)), grouping=True), \
                                                  float((round(commissions)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    print_line("Total Slippage"," %15s, %10.2f%%" % (locale.currency(int(round(slippage)), grouping=True), \
                                                     float((round(slippage)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    print_line("Total Short Borrowing Cost"," %15s, %10.2f%%" % (locale.currency(int(round(borrowcost)), grouping=True), \
                                                     float((round(borrowcost)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    print_line("Total Costs"," %15s, %10.2f%%" % (locale.currency(int(round(borrowcost+slippage+commissions)), grouping=True), \
                                  float((round(borrowcost+slippage+commissions)*100)/(fund_ts.values[-1]*mult))), i_spacing=4, ostream=ostream)

    ostream.write("\n")

    print_line(s_formatted_fund_name+" Std Dev of Returns",get_std_dev(fund_ts),i_spacing=8, ostream=ostream)

    if type(original)!=type("str"):
        print_line(s_formatted_original_name+" Std Dev of Returns", get_std_dev(original), i_spacing=8, ostream=ostream)

    for bench_sym in benchmark:
        print_line(bench_sym+" Std Dev of Returns", get_std_dev(benchmark_close[bench_sym]), i_spacing=8, ostream=ostream)

    ostream.write("\n")


    for bench_sym in benchmark:
        print_benchmark_coer(fund_ts, benchmark_close[bench_sym], str(bench_sym), ostream)
    ostream.write("\n")

    ostream.write("\nYearly Performance Metrics")
    print_years(years, ostream)

    s_line=""
    for f_token in get_annual_return(fund_ts, years):
        s_line+=" %+8.2f%%" % f_token
    print_line(s_formatted_fund_name+" Annualized Return", s_line, i_spacing=4, ostream=ostream)
    lf_vals=[get_annual_return(fund_ts, years)]
    ls_labels=[name]

    if type(original)!=type("str"):
        s_line=""
        for f_token in get_annual_return(original, years):
            s_line+=" %+8.2f%%" % f_token
        print_line(s_formatted_original_name+" Annualized Return", s_line, i_spacing=4, ostream=ostream)
        lf_vals.append(get_annual_return(original, years))
        ls_labels.append(s_original_name)

    for bench_sym in benchmark:
        s_line=""
        for f_token in get_annual_return(benchmark_close[bench_sym], years):
            s_line+=" %+8.2f%%" % f_token
        print_line(bench_sym+" Annualized Return", s_line, i_spacing=4, ostream=ostream)
        lf_vals.append(get_annual_return(benchmark_close[bench_sym], years))
        ls_labels.append(bench_sym)

    print lf_vals
    print ls_labels
    ls_year_labels=[]
    for i in range(0,len(years)):
        ls_year_labels.append(str(years[i]))
    print_bar_chart(lf_vals, ls_labels, ls_year_labels, directory+"/annual_rets.png")

    print_years(years, ostream)

    print_line(s_formatted_fund_name+" Winning Days",get_winning_days(fund_ts, years), i_spacing=4, ostream=ostream)


    if type(original)!=type("str"):
        print_line(s_formatted_original_name+" Winning Days",get_winning_days(original, years), i_spacing=4, ostream=ostream)


    for bench_sym in benchmark:
        print_line(bench_sym+" Winning Days",get_winning_days(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    print_years(years, ostream)

    print_line(s_formatted_fund_name+" Max Draw Down",get_max_draw_down(fund_ts, years), i_spacing=4, ostream=ostream)

    if type(original)!=type("str"):
        print_line(s_formatted_original_name+" Max Draw Down",get_max_draw_down(original, years), i_spacing=4, ostream=ostream)


    for bench_sym in benchmark:
        print_line(bench_sym+" Max Draw Down",get_max_draw_down(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    print_years(years, ostream)


    print_line(s_formatted_fund_name+" Daily Sharpe Ratio",get_daily_sharpe(fund_ts, years), i_spacing=4, ostream=ostream)


    if type(original)!=type("str"):
        print_line(s_formatted_original_name+" Daily Sharpe Ratio",get_daily_sharpe(original, years), i_spacing=4, ostream=ostream)

    for bench_sym in benchmark:
        print_line(bench_sym+" Daily Sharpe Ratio",get_daily_sharpe(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    print_years(years, ostream)

    print_line(s_formatted_fund_name+" Daily Sortino Ratio",get_daily_sortino(fund_ts, years), i_spacing=4, ostream=ostream)

    if type(original)!=type("str"):
        print_line(s_formatted_original_name+" Daily Sortino Ratio",get_daily_sortino(original, years), i_spacing=4, ostream=ostream)


    for bench_sym in benchmark:
        print_line(bench_sym+" Daily Sortino Ratio",get_daily_sortino(benchmark_close[bench_sym], years), i_spacing=4, ostream=ostream)


    ostream.write("\n\n\nCorrelation and Beta with DJ Industries for the Fund ")

    print_industry_coer(fund_ts,ostream)

    ostream.write("\n\nCorrelation and Beta with Other Indices for the Fund ")

    print_other_coer(fund_ts,ostream)

    ostream.write("\n\n\nMonthly Returns for the Fund %\n")

    print_monthly_returns(fund_ts, years, ostream)
    print_footer(ostream)

def print_bar_chart(llf_vals, ls_fund_labels, ls_year_labels, s_filename):
    llf_vals=((1,2,3),(3,2,1),(2,2,2))
    amin=min(min(llf_vals))
    print amin
    min_lim=0
    if amin<0:
        min_lim = amin
    ls_fund_labels=("Fund 1","Benchmark","Original")
    ls_year_labels=("2000","2001","2002")
    pyplot.clf()
    ind = np.arange(len(ls_year_labels))
    ind=ind*2
    width = 0.35
    fig = pyplot.figure()
    ax = fig.add_subplot(111)
    colors=('r','g','b')
    rects=[]
    for i in range(0,len(llf_vals)):
        rects.append( ax.bar(ind+width*i, llf_vals[i], width, color=colors[i]))
    ax.set_ylabel('Annual Return')
    ax.set_ylim(min_lim, 5)
    ax.set_title('Annual Return by Fund and Year')
    ax.set_xticks(ind+width*len(llf_vals)/2)
    ax.set_xticklabels(ls_year_labels)
    plots=[]
    for i in range(0,len(llf_vals)):
        plots.append(rects[i][0])
    ax.legend(plots,ls_fund_labels)

    def autolabel(rects):
        # attach some text labels
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x()+rect.get_width()/2., 1.05*height, '%d'%int(height),
                    ha='center', va='bottom')
    for i in range(0,len(llf_vals)):
        autolabel(rects[i])
    savefig(s_filename, format = 'png')

def print_plot(fund, benchmark, graph_name, filename, s_original_name="", lf_dividend_rets=0.0, leverage=False, i_start_cash = 1000000, s_leverage_name="Leverage"):
    """
    @summary prints a plot of a provided fund and benchmark
    @param fund: fund value in pandas timeseries
    @param benchmark: benchmark symbol to compare fund to
    @param graph_name: name to associate with the fund in the report
    @param filename: file location to store plot1
    """
    pyplot.clf()
    fig = pyplot.figure()
    from matplotlib.font_manager import FontProperties
    fontP = FontProperties()
    fontP.set_size('small')

    if type(leverage)==type(False):
        ax = pyplot.subplot(111)
    else:
        gs = gridspec.GridSpec(2, 1, height_ratios=[4, 1])
        ax = pyplot.subplot(gs[0])

    start_date = 0
    end_date = 0
    if(type(fund)!= type(list())):
        if(start_date == 0 or start_date>fund.index[0]):
            start_date = fund.index[0]
        if(end_date == 0 or end_date<fund.index[-1]):
            end_date = fund.index[-1]
        mult = i_start_cash/fund.values[0]
        pyplot.plot(fund.index, fund.values * mult,'b', label = \
                                 path.basename(graph_name))
    else:
        i=0
        for entity in fund:
            if i == 1 and len(fund)!=1:
                for j in range(len(entity)):
                    if(start_date == 0 or start_date>entity[j].index[0]):
                        start_date = entity[j].index[0]
                    if(end_date == 0 or end_date<entity[j].index[-1]):
                        end_date = entity[j].index[-1]
                    mult = i_start_cash/entity[j].values[0]
                    if j ==0:
                        pyplot.plot(entity[j].index, entity[j].values * mult, 'k', label = \
                                      s_original_name[j])
                    else:
                        pyplot.plot(entity[j].index, entity[j].values * mult, 'g', label = \
                                      s_original_name[j])
            else:
                if(start_date == 0 or start_date>entity.index[0]):
                    start_date = entity.index[0]
                if(end_date == 0 or end_date<entity.index[-1]):
                    end_date = entity.index[-1]
                mult = i_start_cash/entity.values[0]
                pyplot.plot(entity.index, entity.values * mult, 'b', label = \
                                  path.basename(graph_name))
            i=i+1
    timeofday = dt.timedelta(hours = 16)
    timestamps = du.getNYSEdays(start_date, end_date, timeofday)
    dataobj = de.DataAccess('Yahoo')
    benchmark_close = dataobj.get_data(timestamps, benchmark, ["close"])
    benchmark_close = benchmark_close[0]
    benchmark_close = benchmark_close.fillna(method='pad')
    benchmark_close = benchmark_close.fillna(method='bfill')
    benchmark_close = benchmark_close.fillna(1.0)

    if type(lf_dividend_rets) != type(0.0):
        for i,sym in enumerate(benchmark):
            benchmark_close[sym] = _dividend_rets_funds(benchmark_close[sym], lf_dividend_rets[i])

    for i,sym in enumerate(benchmark):
        mult = i_start_cash / benchmark_close[sym].values[0]
        pyplot.plot(benchmark_close[sym].index, \
                benchmark_close[sym].values*mult, 'r', label = sym)

    # pyplot.gcf().autofmt_xdate()
    # pyplot.gca().fmt_xdata = mdates.DateFormatter('%m-%d-%Y')
    # pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %d %Y'))
    pyplot.xlabel('Date', size='xx-small')
    pyplot.ylabel('Fund Value', size='xx-small')
    pyplot.xticks(size='xx-small')
    pyplot.yticks(size='xx-small')

    # Shink current axis's height by 10% on the bottom
    box = ax.get_position()
    ax.set_position([box.x0, box.y0 + box.height * 0.1,
                     box.width, box.height * 0.9])

    # Put a legend below current axis
    ax.legend(prop=fontP, loc='upper center', bbox_to_anchor=(0.5, -0.05),
               ncol=3)


    if type(leverage)!=type(False):
        ax1 = pyplot.subplot(gs[1])
        if type(leverage) == type([]):
            for i in range(len(leverage)):
                pyplot.plot(leverage[i].index, leverage[i].values, label=s_leverage_name[i])
        else:
            pyplot.plot(leverage.index, leverage.values, label=s_leverage_name)
        # pyplot.gcf().autofmt_xdate()
        # pyplot.gca().fmt_xdata = mdates.DateFormatter('%m-%d-%Y')
        # pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %d %Y'))
        if type(leverage) != type([]):
            labels=[]
            max_label=max(leverage.values)
            min_label=min(leverage.values)
            rounder= -1*(round(log10(max_label))-1)
            labels.append(round(min_label*0.9, int(rounder)))
            labels.append(round((max_label+min_label)/2, int(rounder)))
            labels.append(round(max_label*1.1, int(rounder)))
            pyplot.yticks(labels, size='xx-small')
        # pyplot.title(graph_name + " Leverage")
        pyplot.xlabel('Date', size='xx-small')
        pyplot.ylabel('Exposure', size='xx-small')
        pyplot.xticks(size='xx-small')
        pyplot.yticks(size='xx-small')

        # Shink current axis's height by 10% on the bottom
        box = ax1.get_position()
        ax1.set_position([box.x0, box.y0 + box.height * 0.1,
                         box.width, box.height * 0.9])

        # Put a legend below current axis
        ax1.legend(prop=fontP, loc='upper center', bbox_to_anchor=(0.5, -0.15),
                   ncol=3)

    pyplot.savefig(filename, format = 'png')


def generate_report(funds_list, graph_names, out_file, i_start_cash = 10000):
    """
    @summary generates a report given a list of fund time series
    """
    html_file  =  open("report.html","w")
    print_header(html_file, out_file)
    html_file.write("<IMG SRC = \'./funds.png\' width = 400/>\n")
    html_file.write("<BR/>\n\n")
    i = 0
    pyplot.clf()
    #load spx for time frame
    symbol = ["$SPX"]
    start_date = 0
    end_date = 0
    for fund in funds_list:
        if(type(fund)!= type(list())):
            if(start_date == 0 or start_date>fund.index[0]):
                start_date = fund.index[0]
            if(end_date == 0 or end_date<fund.index[-1]):
                end_date = fund.index[-1]
            mult = i_start_cash/fund.values[0]
            pyplot.plot(fund.index, fund.values * mult, label = \
                                 path.basename(graph_names[i]))
        else:
            if(start_date == 0 or start_date>fund[0].index[0]):
                start_date = fund[0].index[0]
            if(end_date == 0 or end_date<fund[0].index[-1]):
                end_date = fund[0].index[-1]
            mult = i_start_cash/fund[0].values[0]
            pyplot.plot(fund[0].index, fund[0].values * mult, label = \
                                      path.basename(graph_names[i]))
        i += 1
    timeofday = dt.timedelta(hours = 16)
    timestamps = du.getNYSEdays(start_date, end_date, timeofday)
    dataobj = de.DataAccess('Yahoo')
    benchmark_close = dataobj.get_data(timestamps, symbol, ["close"], \
                                            verbose = False)[0]
    mult = i_start_cash/benchmark_close.values[0]
    i = 0
    for fund in funds_list:
        if(type(fund)!= type(list())):
            print_stats(fund, ["$SPX"], graph_names[i])
        else:
            print_stats( fund[0], ["$SPX"], graph_names[i])
        i += 1
    pyplot.plot(benchmark_close.index, \
                 benchmark_close.values*mult, label = "SSPX")
    pyplot.ylabel('Fund Value')
    pyplot.xlabel('Date')
    pyplot.legend()
    savefig('funds.png', format = 'png')
    print_footer(html_file)

def generate_robust_report(fund_matrix, out_file):
    """
    @summary generates a report using robust backtesting
    @param fund_matrix: a pandas matrix of fund time series
    @param out_file: filename where to print report
    """
    html_file  =  open(out_file,"w")
    print_header(html_file, out_file)
    # converter.fundsToPNG(fund_matrix,'funds.png')
    html_file.write("<H2>QSTK Generated Report:" + out_file + "</H2>\n")
    # html_file.write("<IMG SRC = \'./funds.png\'/>\n")
    html_file.write("<IMG SRC = \'./analysis.png\'/>\n")
    html_file.write("<BR/>\n\n")
    print_stats(fund_matrix, "robust funds", html_file)
    print_footer(html_file)

if __name__  ==  '__main__':
    # Usage
    #
    # Normal:
    # python report.py 'out.pkl' ['out2.pkl' ...]
    #
    # Robust:
    # python report.py -r 'out.pkl'
    #

    ROBUST = 0

    if(sys.argv[1] == '-r'):
        ROBUST = 1

    FILENAME  =  "report.html"

    if(ROBUST == 1):
        ANINPUT = open(sys.argv[2],"r")
        FUNDS = cPickle.load(ANINPUT)
        generate_robust_report(FUNDS, FILENAME)
    else:
        FILES = sys.argv
        FILES.remove(FILES[0])
        FUNDS = []
        for AFILE in FILES:
            ANINPUT = open(AFILE,"r")
            FUND = cPickle.load(ANINPUT)
            FUNDS.append(FUND)
        generate_report(FUNDS, FILES, FILENAME)



########NEW FILE########
__FILENAME__ = YahooDataPull
'''
Pulling Yahoo CSV Data
'''

import urllib2
import urllib
import datetime
import os
import QSTK.qstkutil.DataAccess as da


def get_yahoo_data(data_path, ls_symbols):
    '''Read data from Yahoo
    @data_path : string for where to place the output files
    @ls_symbols: list of symbols to read from yahoo
    '''
    # Create path if it doesn't exist
    if not (os.access(data_path, os.F_OK)):
        os.makedirs(data_path)

    ls_missed_syms = []
    # utils.clean_paths(data_path)   

    _now = datetime.datetime.now()
    # Counts how many symbols we could not get
    miss_ctr = 0
    for symbol in ls_symbols:
        # Preserve original symbol since it might
        # get manipulated if it starts with a "$"
        symbol_name = symbol
        if symbol[0] == '$':
            symbol = '^' + symbol[1:]

        symbol_data = list()
        # print "Getting {0}".format(symbol)

        try:
            params = urllib.urlencode ({'a':0, 'b':1, 'c':2000, 'd':_now.month-1, 'e':_now.day, 'f':_now.year, 's': symbol})
            url = "http://ichart.finance.yahoo.com/table.csv?%s" % params
            url_get = urllib2.urlopen(url)
            
            header = url_get.readline()
            symbol_data.append (url_get.readline())
            while (len(symbol_data[-1]) > 0):
                symbol_data.append(url_get.readline())

            # The last element is going to be the string of length zero. 
            # We don't want to write that to file.
            symbol_data.pop(-1)
            #now writing data to file
            f = open (data_path + symbol_name + ".csv", 'w')

            #Writing the header
            f.write (header)

            while (len(symbol_data) > 0):
                f.write (symbol_data.pop(0))

            f.close()

        except urllib2.HTTPError:
            miss_ctr += 1
            ls_missed_syms.append(symbol_name)
            print "Unable to fetch data for stock: {0} at {1}".format(symbol_name, url)
        except urllib2.URLError:
            miss_ctr += 1
            ls_missed_syms.append(symbol_name)
            print "URL Error for stock: {0} at {1}".format(symbol_name, url)

    print "All done. Got {0} stocks. Could not get {1}".format(len(ls_symbols) - miss_ctr, miss_ctr)
    return ls_missed_syms


def read_symbols(s_symbols_file):
    '''Read a list of symbols'''
    ls_symbols = []
    ffile = open(s_symbols_file, 'r')
    for line in ffile.readlines():
        str_line = str(line)
        if str_line.strip(): 
            ls_symbols.append(str_line.strip())
    ffile.close()
    return ls_symbols 


def update_my_data():
    '''Update the data in the root dir'''
    c_dataobj = da.DataAccess('Yahoo', verbose=True)
    s_path = c_dataobj.rootdir
    ls_symbols = c_dataobj.get_all_symbols()
    ls_missed_syms = get_yahoo_data(s_path, ls_symbols)
    # Making a second call for symbols that failed to double check
    get_yahoo_data(s_path, ls_missed_syms)
    return


def main():
    '''Main Function'''
    path = './'
    ls_symbols = read_symbols('symbols.txt')
    get_yahoo_data(path, ls_symbols)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = DataAccess
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 15, 2013

@author: Sourabh Bajaj
@contact: sourabhbajaj@gatech.edu
@summary: Data Access python library.

'''

import numpy as np
import pandas as pa
import os
import re
import csv
import pickle as pkl
import time
import datetime as dt
import dircache
import tempfile
import copy

class Exchange (object):
    AMEX = 1
    NYSE = 2
    NYSE_ARCA = 3
    OTC = 4
    DELISTED = 5
    NASDAQ = 6


class DataItem (object):
    OPEN = "open"
    HIGH = "high"
    LOW = "low"
    CLOSE = "close"
    VOL = "volume"
    VOLUME = "volume"
    ACTUAL_CLOSE = "actual_close"
    ADJUSTED_CLOSE = "adj_close"
    # Compustat label list pulled from _analyze() in compustat_csv_to_pkl.py
    COMPUSTAT = ['gvkey', 'fyearq', 'fqtr', 'fyr', 'ACCTSTDQ', 'ADRRQ', 'AJEXQ', 'AJPQ', 'CURRTRQ', 'CURUSCNQ', 'PDQ', 'PDSA', 'PDYTD', 'SCFQ', 'SRCQ', 'UPDQ', 'ACCDQ', 'ACCHGQ', 'ACCOQ', 'ACOMINCQ', 'ACOQ', 'ACOXQ', 'ACTQ', 'ADPACQ', 'ALTOQ', 'AMQ', 'ANCQ', 'ANOQ', 'AOCIDERGLQ', 'AOCIOTHERQ', 'AOCIPENQ', 'AOCISECGLQ', 'AOL2Q', 'AOQ', 'AOTQ', 'APOQ', 'APQ', 'AQAQ', 'AQDQ', 'AQEPSQ', 'AQPL1Q', 'AQPQ', 'ARCED12', 'ARCEDQ', 'ARCEEPS12', 'ARCEEPSQ', 'ARCEQ', 'ARTFSQ', 'ATQ', 'AUL3Q', 'AUTXRQ', 'BCEFQ', 'BCTQ', 'BDIQ', 'CAPCSTQ', 'CAPR1Q', 'CAPR2Q', 'CAPR3Q', 'CAPRTQ', 'CAPSQ', 'CAQ', 'CEQQ', 'CFBDQ', 'CFEREQ', 'CFOQ', 'CFPDOQ', 'CHEQ', 'CHQ', 'CHSQ', 'CIBEGNIQ', 'CICURRQ', 'CIDERGLQ', 'CIMIIQ', 'CIOTHERQ', 'CIPENQ', 'CIQ', 'CISECGLQ', 'CITOTALQ', 'CLTQ', 'COGSQ', 'CSH12Q', 'CSHFDQ', 'CSHIQ', 'CSHOPQ', 'CSHOQ', 'CSHPRQ', 'CSTKEQ', 'CSTKQ', 'DCOMQ', 'DFPACQ', 'DFXAQ', 'DILADQ', 'DILAVQ', 'DITQ', 'DLCQ', 'DLTTQ', 'DOQ', 'DPACREQ', 'DPACTQ', 'DPQ', 'DPRETQ', 'DPTBQ', 'DPTCQ', 'DRCQ', 'DRLTQ', 'DTEAQ', 'DTEDQ', 'DTEEPSQ', 'DTEPQ', 'DVPDPQ', 'DVPQ', 'DVRREQ', 'DVTQ', 'EPSF12', 'EPSFIQ', 'EPSFXQ', 'EPSPIQ', 'EPSPXQ', 'EPSX12', 'EQRTQ', 'EROQ', 'ESOPCTQ', 'ESOPNRQ', 'ESOPRQ', 'ESOPTQ', 'ESUBQ', 'FCAQ', 'FEAQ', 'FELQ', 'FFOQ', 'GDWLAMQ', 'GDWLIA12', 'GDWLIAQ', 'GDWLID12', 'GDWLIDQ', 'GDWLIEPS12', 'GDWLIEPSQ', 'GDWLIPQ', 'GDWLQ', 'GLAQ', 'GLCEA12', 'GLCEAQ', 'GLCED12', 'GLCEDQ', 'GLCEEPS12', 'GLCEEPSQ', 'GLCEPQ', 'GLDQ', 'GLEPSQ', 'GLPQ', 'GPQ', 'HEDGEGLQ', 'IATIQ', 'IBADJ12', 'IBADJQ', 'IBCOMQ', 'IBKIQ', 'IBMIIQ', 'IBQ', 'ICAPTQ', 'IDITQ', 'IIREQ', 'IITQ', 'INTACCQ', 'INTANOQ', 'INTANQ', 'INTCQ', 'INVFGQ', 'INVOQ', 'INVRMQ', 'INVTQ', 'INVWIPQ', 'IOBDQ', 'IOIQ', 'IOREQ', 'IPQ', 'IPTIQ', 'ISGTQ', 'ISTQ', 'IVAEQQ', 'IVAOQ', 'IVIQ', 'IVLTQ', 'IVPTQ', 'IVSTQ', 'IVTFSQ', 'LCABGQ', 'LCACUQ', 'LCOQ', 'LCOXQ', 'LCTQ', 'LLTQ', 'LNOQ', 'LOL2Q', 'LOQ', 'LOXDRQ', 'LQPL1Q', 'LSEQ', 'LSQ', 'LTMIBQ', 'LTQ', 'LUL3Q', 'MIBNQ', 'MIBQ', 'MIBTQ', 'MIIQ', 'MSAQ', 'MTLQ', 'NCOQ', 'NIITQ', 'NIMQ', 'NIQ', 'NITQ', 'NOPIOQ', 'NOPIQ', 'NPATQ', 'NRTXTDQ', 'NRTXTEPSQ', 'NRTXTQ', 'OEPF12', 'OEPS12', 'OEPSXQ', 'OIADPQ', 'OIBDPQ', 'OPEPSQ', 'OPROQ', 'OPTDRQ', 'OPTFVGRQ', 'OPTLIFEQ', 'OPTRFRQ', 'OPTVOLQ', 'PCLQ', 'PIQ', 'PLLQ', 'PNC12', 'PNCD12', 'PNCDQ', 'PNCEPS12', 'PNCEPSQ', 'PNCIAPQ', 'PNCIAQ', 'PNCIDPQ', 'PNCIDQ', 'PNCIEPSPQ', 'PNCIEPSQ', 'PNCIPPQ', 'PNCIPQ', 'PNCPD12', 'PNCPDQ', 'PNCPEPS12', 'PNCPEPSQ', 'PNCPQ', 'PNCQ', 'PNCWIAPQ', 'PNCWIAQ', 'PNCWIDPQ', 'PNCWIDQ', 'PNCWIEPQ', 'PNCWIEPSQ', 'PNCWIPPQ', 'PNCWIPQ', 'PNRSHOQ', 'PPEGTQ', 'PPENTQ', 'PRCAQ', 'PRCD12', 'PRCDQ', 'PRCE12', 'PRCEPS12', 'PRCEPSQ', 'PRCPD12', 'PRCPDQ', 'PRCPEPS12', 'PRCPEPSQ', 'PRCPQ', 'PRCQ', 'PRCRAQ', 'PRSHOQ', 'PSTKNQ', 'PSTKQ', 'PSTKRQ', 'PTRANQ', 'PVOQ', 'PVTQ', 'RATIQ', 'RAWMSMQ', 'RCAQ', 'RCDQ', 'RCEPSQ', 'RCPQ', 'RDIPAQ', 'RDIPDQ', 'RDIPEPSQ', 'RDIPQ', 'RECCOQ', 'RECDQ', 'RECTAQ', 'RECTOQ', 'RECTQ', 'RECTRQ', 'RECUBQ', 'REITQ', 'REQ', 'RETQ', 'REUNAQ', 'REVTQ', 'RISQ', 'RLLQ', 'RLTQ', 'RRA12', 'RRAQ', 'RRD12', 'RRDQ', 'RREPS12', 'RREPSQ', 'RRPQ', 'RVLRVQ', 'RVTIQ', 'RVUTXQ', 'SAAQ', 'SALEQ', 'SALQ', 'SBDCQ', 'SCOQ', 'SCQ', 'SCTQ', 'SEQOQ', 'SEQQ', 'SETA12', 'SETAQ', 'SETD12', 'SETDQ', 'SETEPS12', 'SETEPSQ', 'SETPQ', 'SPCE12', 'SPCED12', 'SPCEDPQ', 'SPCEDQ', 'SPCEEPS12', 'SPCEEPSP12', 'SPCEEPSPQ', 'SPCEEPSQ', 'SPCEP12', 'SPCEPD12', 'SPCEPQ', 'SPCEQ', 'SPIDQ', 'SPIEPSQ', 'SPIOAQ', 'SPIOPQ', 'SPIQ', 'SRETQ', 'SSNPQ', 'STKCHQ', 'STKCOQ', 'STKCPAQ', 'TDSGQ', 'TDSTQ', 'TEQQ', 'TFVAQ', 'TFVCEQ', 'TFVLQ', 'TIEQ', 'TIIQ', 'TRANSAQ', 'TSTKNQ', 'TSTKQ', 'TXDBAQ', 'TXDBQ', 'TXDIQ', 'TXDITCQ', 'TXPQ', 'TXTQ', 'TXWQ', 'UACOQ', 'UAOQ', 'UAPTQ', 'UCAPSQ', 'UCCONSQ', 'UCEQQ', 'UDDQ', 'UDMBQ', 'UDOLTQ', 'UDPCOQ', 'UDVPQ', 'UGIQ', 'UINVQ', 'ULCOQ', 'UNIAMIQ', 'UNNPQ', 'UNOPINCQ', 'UOPIQ', 'UPDVPQ', 'UPMCSTKQ', 'UPMPFQ', 'UPMPFSQ', 'UPMSUBPQ', 'UPSTKCQ', 'UPSTKQ', 'URECTQ', 'USPIQ', 'USUBDVPQ', 'USUBPCVQ', 'UTEMQ', 'WCAPQ', 'WDAQ', 'WDDQ', 'WDEPSQ', 'WDPQ', 'XAGTQ', 'XBDTQ', 'XCOMIQ', 'XCOMQ', 'XDVREQ', 'XIDOQ', 'XINTQ', 'XIOQ', 'XIQ', 'XIVIQ', 'XIVREQ', 'XOBDQ', 'XOIQ', 'XOPROQ', 'XOPRQ', 'XOPT12', 'XOPTD12', 'XOPTD12P', 'XOPTDQ', 'XOPTDQP', 'XOPTEPS12', 'XOPTEPSP12', 'XOPTEPSQ', 'XOPTEPSQP', 'XOPTQ', 'XOPTQP', 'XOREQ', 'XPPQ', 'XRDQ', 'XRETQ', 'XSGAQ', 'XSQ', 'XSTOQ', 'XSTQ', 'XTQ', 'ACCHGY', 'ACCLIY', 'ACQDISNY', 'ACQDISOY', 'ADPACY', 'AFUDCCY', 'AFUDCIY', 'AMCY', 'AMY', 'AOLOCHY', 'APALCHY', 'APCHY', 'AQAY', 'AQCY', 'AQDY', 'AQEPSY', 'AQPY', 'ARCEDY', 'ARCEEPSY', 'ARCEY', 'ASDISY', 'ASINVY', 'ATOCHY', 'AUTXRY', 'BCEFY', 'BCTY', 'BDIY', 'CAPCSTY', 'CAPFLY', 'CAPXFIY', 'CAPXY', 'CDVCY', 'CFBDY', 'CFEREY', 'CFLAOTHY', 'CFOY', 'CFPDOY', 'CHECHY', 'CHENFDY', 'CIBEGNIY', 'CICURRY', 'CIDERGLY', 'CIMIIY', 'CIOTHERY', 'CIPENY', 'CISECGLY', 'CITOTALY', 'CIY', 'COGSY', 'CSHFDY', 'CSHPRY', 'CSTKEY', 'DCSFDY', 'DCUFDY', 'DEPCY', 'DFXAY', 'DILADY', 'DILAVY', 'DISPOCHY', 'DITY', 'DLCCHY', 'DLTISY', 'DLTRY', 'DOCY', 'DOY', 'DPCY', 'DPRETY', 'DPY', 'DTEAY', 'DTEDY', 'DTEEPSY', 'DTEPY', 'DVPDPY', 'DVPY', 'DVRECY', 'DVRREY', 'DVTY', 'DVY', 'EIEACY', 'EPSFIY', 'EPSFXY', 'EPSPIY', 'EPSPXY', 'EQDIVPY', 'ESUBCY', 'ESUBY', 'EXRESY', 'EXREUY', 'EXREY', 'FCAY', 'FFOY', 'FIAOY', 'FINCFY', 'FININCY', 'FINLEY', 'FINREY', 'FINVAOY', 'FOPOXY', 'FOPOY', 'FOPTY', 'FSRCOPOY', 'FSRCOPTY', 'FSRCOY', 'FSRCTY', 'FUSEOY', 'FUSETY', 'GDWLAMY', 'GDWLIAY', 'GDWLIDY', 'GDWLIEPSY', 'GDWLIPY', 'GLAY', 'GLCEAY', 'GLCEDY', 'GLCEEPSY', 'GLCEPY', 'GLDY', 'GLEPSY', 'GLPY', 'GPY', 'HEDGEGLY', 'IBADJY', 'IBCOMY', 'IBCY', 'IBKIY', 'IBMIIY', 'IBY', 'IDITY', 'IIREY', 'IITY', 'INTANDY', 'INTANPY', 'INTCY', 'INTFACTY', 'INTFLY', 'INTIACTY', 'INTOACTY', 'INTPDY', 'INTPNY', 'INTRCY', 'INVCHY', 'INVDSPY', 'INVSVCY', 'IOBDY', 'IOIY', 'IOREY', 'IPTIY', 'ISGTY', 'ITCCY', 'IVACOY', 'IVCHY', 'IVIY', 'IVNCFY', 'IVSTCHY', 'LIQRESNY', 'LIQRESOY', 'LNDEPY', 'LNINCY', 'LNMDY', 'LNREPY', 'LTDCHY', 'LTDLCHY', 'LTLOY', 'MICY', 'MIIY', 'MISEQY', 'NCFLIQY', 'NCOY', 'NEQMIY', 'NIITY', 'NIMY', 'NITY', 'NIY', 'NOASUBY', 'NOPIOY', 'NOPIY', 'NRTXTDY', 'NRTXTEPSY', 'NRTXTY', 'OANCFCY', 'OANCFDY', 'OANCFY', 'OEPSXY', 'OIADPY', 'OIBDPY', 'OPEPSY', 'OPPRFTY', 'OPROY', 'OPTDRY', 'OPTFVGRY', 'OPTLIFEY', 'OPTRFRY', 'OPTVOLY', 'PCLY', 'PDVCY', 'PIY', 'PLIACHY', 'PLLY', 'PNCDY', 'PNCEPSY', 'PNCIAPY', 'PNCIAY', 'PNCIDPY', 'PNCIDY', 'PNCIEPSPY', 'PNCIEPSY', 'PNCIPPY', 'PNCIPY', 'PNCPDY', 'PNCPEPSY', 'PNCPY', 'PNCWIAPY', 'PNCWIAY', 'PNCWIDPY', 'PNCWIDY', 'PNCWIEPSY', 'PNCWIEPY', 'PNCWIPPY', 'PNCWIPY', 'PNCY', 'PRCAY', 'PRCDY', 'PRCEPSY', 'PRCPDY', 'PRCPEPSY', 'PRCPY', 'PROSAIY', 'PRSTKCCY', 'PRSTKCY', 'PRSTKPCY', 'PRVY', 'PSFIXY', 'PTRANY', 'PURTSHRY', 'PVOY', 'RAWMSMY', 'RCAY', 'RCDY', 'RCEPSY', 'RCPY', 'RDIPAY', 'RDIPDY', 'RDIPEPSY', 'RDIPY', 'RECCHY', 'REITY', 'REVTY', 'RISY', 'RRAY', 'RRDY', 'RREPSY', 'RRPY', 'RVY', 'SALEY', 'SCSTKCY', 'SETAY', 'SETDY', 'SETEPSY', 'SETPY', 'SHRCAPY', 'SIVY', 'SPCEDPY', 'SPCEDY', 'SPCEEPSPY', 'SPCEEPSY', 'SPCEPY', 'SPCEY', 'SPIDY', 'SPIEPSY', 'SPIOAY', 'SPIOPY', 'SPIY', 'SPPCHY', 'SPPEY', 'SPPIVY', 'SPSTKCY', 'SRETY', 'SSTKY', 'STFIXAY', 'STINVY', 'STKCHY', 'STKCOY', 'STKCPAY', 'SUBDISY', 'SUBPURY', 'TDCY', 'TDSGY', 'TFVCEY', 'TIEY', 'TIIY', 'TSAFCY', 'TXACHY', 'TXBCOFY', 'TXBCOY', 'TXDCY', 'TXDIY', 'TXOPY', 'TXPDY', 'TXTY', 'TXWY', 'TXY', 'UAOLOCHY', 'UDFCCY', 'UDVPY', 'UFRETSDY', 'UGIY', 'UNIAMIY', 'UNOPINCY', 'UNWCCY', 'UOISY', 'UPDVPY', 'UPTACY', 'USPIY', 'USTDNCY', 'USUBDVPY', 'UTFDOCY', 'UTFOSCY', 'UTMEY', 'UWKCAPCY', 'WCAPCHCY', 'WCAPCHY', 'WCAPCY', 'WCAPOPCY', 'WCAPSAY', 'WCAPSUY', 'WCAPSY', 'WCAPTY', 'WCAPUY', 'WDAY', 'WDDY', 'WDEPSY', 'WDPY', 'XAGTY', 'XBDTY', 'XCOMIY', 'XCOMY', 'XDVREY', 'XIDOCY', 'XIDOY', 'XINTY', 'XIOY', 'XIVIY', 'XIVREY', 'XIY', 'XOBDY', 'XOIY', 'XOPROY', 'XOPRY', 'XOPTDQPY', 'XOPTDY', 'XOPTEPSQPY', 'XOPTEPSY', 'XOPTQPY', 'XOPTY', 'XOREY', 'XRDY', 'XRETY', 'XSGAY', 'XSTOY', 'XSTY', 'XSY', 'XTY', 'DLRSN', 'FYRC', 'GGROUP', 'GIND', 'GSECTOR', 'GSUBIND', 'NAICS', 'PRIUSA', 'SIC', 'SPCINDCD', 'SPCSECCD', 'STKO']


class DataSource(object):
    NORGATE = "Norgate"
    YAHOO = "Yahoo"
    YAHOOold = "YahooOld"
    COMPUSTAT = "Compustat"
    CUSTOM = "Custom"
    MLT = "ML4Trading"
    #class DataSource ends


class DataAccess(object):
    '''
    @summary: This class is used to access all the symbol data. It readin in pickled numpy arrays converts them into appropriate pandas objects
    and returns that object. The {main} function currently demonstrates use.
    @note: The earliest time for which this works is platform dependent because the python date functionality is platform dependent.
    '''
    def __init__(self, sourcein=DataSource.YAHOO, s_datapath=None,
                 s_scratchpath=None, cachestalltime=12, verbose=False):
        '''
        @param sourcestr: Specifies the source of the data. Initializes paths based on source.
        @note: No data is actually read in the constructor. Only paths for the source are initialized
        @param: Scratch defaults to a directory in /tmp/QSScratch
        '''

        self.folderList = list()
        self.folderSubList = list()
        self.cachestalltime = cachestalltime
        self.fileExtensionToRemove = ".pkl"

        try:
            self.rootdir = os.environ['QSDATA']
            try:
                self.scratchdir = os.environ['QSSCRATCH']
            except:
                self.scratchdir = os.path.join(tempfile.gettempdir(), 'QSScratch')
        except:
            if s_datapath != None:
                self.rootdir = s_datapath
                if s_scratchpath != None:
                    self.scratchdir = s_scratchpath
                else:
                    self.scratchdir = os.path.join(tempfile.gettempdir(), 'QSScratch')
            else:
                self.rootdir = os.path.join(os.path.dirname(__file__), '..', 'QSData')
                self.scratchdir = os.path.join(tempfile.gettempdir(), 'QSScratch')

        if verbose:
            print "Scratch Directory: ", self.scratchdir
            print "Data Directory: ", self.rootdir

        if not os.path.isdir(self.rootdir):
            print "Data path provided is invalid"
            raise

        if not os.path.exists(self.scratchdir):
            os.mkdir(self.scratchdir)

        if (sourcein == DataSource.NORGATE):

            self.source = DataSource.NORGATE
            self.midPath = "/Processed/Norgate/Stocks/"

            self.folderSubList.append("/US/AMEX/")
            self.folderSubList.append("/US/NASDAQ/")
            self.folderSubList.append("/US/NYSE/")
            self.folderSubList.append("/US/NYSE Arca/")
            self.folderSubList.append("/US/OTC/")
            self.folderSubList.append("/US/Delisted Securities/")
            self.folderSubList.append("/US/Indices/")

            for i in self.folderSubList:
                self.folderList.append(self.rootdir + self.midPath + i)

        elif (sourcein == DataSource.CUSTOM):
            self.source = DataSource.CUSTOM
            self.folderList.append(self.rootdir + "/Processed/Custom/")

        elif (sourcein == DataSource.MLT):
            self.source = DataSource.MLT
            self.folderList.append(self.rootdir + "/ML4Trading/")

        elif (sourcein == DataSource.YAHOO):
            self.source = DataSource.YAHOO
            self.folderList.append(self.rootdir + "/Yahoo/")
            self.fileExtensionToRemove = ".csv"

        elif (sourcein == DataSource.COMPUSTAT):
            self.source = DataSource.COMPUSTAT
            self.midPath = "/Processed/Compustat"
            #What if these paths don't exist?
            self.folderSubList.append("/US/NASDAQ/")
            self.folderSubList.append("/US/NYSE/")
            self.folderSubList.append("/US/AMEX/")

            for i in self.folderSubList:
                self.folderList.append(self.rootdir + self.midPath + i)
            #if DataSource.Compustat ends

        else:
            raise ValueError("Incorrect data source requested.")

        #__init__ ends

    def get_data_hardread(self, ts_list, symbol_list, data_item, verbose=False, bIncDelist=False):
        '''
        Read data into a DataFrame no matter what.
        @param ts_list: List of timestamps for which the data values are needed. Timestamps must be sorted.
        @param symbol_list: The list of symbols for which the data values are needed
        @param data_item: The data_item needed. Like open, close, volume etc.  May be a list, in which case a list of DataFrame is returned.
        @param bIncDelist: If true, delisted securities will be included.
        @note: If a symbol is not found then a message is printed. All the values in the column for that stock will be NaN. Execution then
        continues as usual. No errors are raised at the moment.
        '''

        ''' Now support lists of items, still support old string behaviour '''
        bStr = False
        if( isinstance( data_item, str) ):
            data_item = [data_item]
            bStr = True

        # init data struct - list of arrays, each member is an array corresponding do a different data type
        # arrays contain n rows for the timestamps and m columns for each stock
        all_stocks_data = []
        for i in range( len(data_item) ):
            all_stocks_data.append( np.zeros ((len(ts_list), len(symbol_list))) );
            all_stocks_data[i][:][:] = np.NAN
        
        list_index= []
        
        ''' For each item in the list, add to list_index (later used to delete non-used items) '''
        for sItem in data_item:
            if( self.source == DataSource.CUSTOM ) :
                ''' If custom just load what you can '''
                if (sItem == DataItem.CLOSE):
                    list_index.append(1)
                elif (sItem == DataItem.ACTUAL_CLOSE):
                    list_index.append(2)
            if( self.source == DataSource.COMPUSTAT ):
                ''' If compustat, look through list of features '''
                for i, sLabel in enumerate(DataItem.COMPUSTAT):
                    if sItem == sLabel:
                        ''' First item is date index, labels start at 1 index '''
                        list_index.append(i+1)
                        break
                else:
                    raise ValueError ("Incorrect value for data_item %s"%sItem)
            
            if( self.source == DataSource.NORGATE ):
                if (sItem == DataItem.OPEN):
                    list_index.append(1)
                elif (sItem == DataItem.HIGH):
                    list_index.append (2)
                elif (sItem ==DataItem.LOW):
                    list_index.append(3)
                elif (sItem == DataItem.CLOSE):
                    list_index.append(4)
                elif(sItem == DataItem.VOL):
                    list_index.append(5)
                elif (sItem == DataItem.ACTUAL_CLOSE):
                    list_index.append(6)
                else:
                    #incorrect value
                    raise ValueError ("Incorrect value for data_item %s"%sItem)

            if( self.source == DataSource.MLT or self.source == DataSource.YAHOO):
                if (sItem == DataItem.OPEN):
                    list_index.append(1)
                elif (sItem == DataItem.HIGH):
                    list_index.append (2)
                elif (sItem ==DataItem.LOW):
                    list_index.append(3)
                elif (sItem == DataItem.ACTUAL_CLOSE):
                    list_index.append(4)
                elif(sItem == DataItem.VOL):
                    list_index.append(5)
                elif (sItem == DataItem.CLOSE):
                    list_index.append(6)
                else:
                    #incorrect value
                    raise ValueError ("Incorrect value for data_item %s"%sItem)
                #end elif
        #end data_item loop

        #read in data for a stock
        symbol_ctr=-1
        for symbol in symbol_list:
            _file = None
            symbol_ctr = symbol_ctr + 1
            #print self.getPathOfFile(symbol)
            try:
                if (self.source == DataSource.CUSTOM) or (self.source == DataSource.MLT)or (self.source == DataSource.YAHOO):
                    file_path= self.getPathOfCSVFile(symbol);
                else:
                    file_path= self.getPathOfFile(symbol);
                
                ''' Get list of other files if we also want to include delisted '''
                if bIncDelist:
                    lsDelPaths = self.getPathOfFile( symbol, True )
                    if file_path == None and len(lsDelPaths) > 0:
                        print 'Found delisted paths:', lsDelPaths
                
                ''' If we don't have a file path continue... unless we have delisted paths '''
                if (type (file_path) != type ("random string")):
                    if bIncDelist == False or len(lsDelPaths) == 0:
                        continue; #File not found
                
                if not file_path == None: 
                    _file = open(file_path, "rb")
            except IOError:
                # If unable to read then continue. The value for this stock will be nan
                print _file
                continue;
                
            assert( not _file == None or bIncDelist == True )
            ''' Open the file only if we have a valid name, otherwise we need delisted data '''
            if _file != None:
                if (self.source==DataSource.CUSTOM) or (self.source==DataSource.YAHOO)or (self.source==DataSource.MLT):
                    creader = csv.reader(_file)
                    row=creader.next()
                    row=creader.next()
                    #row.pop(0)
                    for i, item in enumerate(row):
                        if i==0:
                            try:
                                date = dt.datetime.strptime(item, '%Y-%m-%d')
                                date = date.strftime('%Y%m%d')
                                row[i] = float(date)
                            except:
                                date = dt.datetime.strptime(item, '%m/%d/%y')
                                date = date.strftime('%Y%m%d')
                                row[i] = float(date)
                        else:
                            row[i]=float(item)
                    naData=np.array(row)
                    for row in creader:
                        for i, item in enumerate(row):
                            if i==0:
                                try:
                                    date = dt.datetime.strptime(item, '%Y-%m-%d')
                                    date = date.strftime('%Y%m%d')
                                    row[i] = float(date)
                                except:
                                    date = dt.datetime.strptime(item, '%m/%d/%y')
                                    date = date.strftime('%Y%m%d')
                                    row[i] = float(date)
                            else: 
                                row[i]=float(item)
                        naData=np.vstack([np.array(row),naData])
                else:
                    naData = pkl.load (_file)
                _file.close()
            else:
                naData = None
                
            ''' If we have delisted data, prepend to the current data '''
            if bIncDelist == True and len(lsDelPaths) > 0 and naData == None:
                for sFile in lsDelPaths[-1:]:
                    ''' Changed to only use NEWEST data since sometimes there is overlap (JAVA) '''
                    inFile = open( sFile, "rb" )
                    naPrepend = pkl.load( inFile )
                    inFile.close()
                    
                    if naData == None:
                        naData = naPrepend
                    else:
                        naData = np.vstack( (naPrepend, naData) )
                        
            #now remove all the columns except the timestamps and one data column
            if verbose:
                print self.getPathOfFile(symbol)
            
            ''' Fix 1 row case by reshaping '''
            if( naData.ndim == 1 ):
                naData = naData.reshape(1,-1)
                
            #print naData
            #print list_index
            ''' We open the file once, for each data item we need, fill out the array in all_stocks_data '''
            for lLabelNum, lLabelIndex in enumerate(list_index):
                
                ts_ctr = 0
                b_skip = True
                
                ''' select timestamps and the data column we want '''
                temp_np = naData[:,(0,lLabelIndex)]
                
                #print temp_np
                
                num_rows= temp_np.shape[0]

                
                symbol_ts_list = range(num_rows) # preallocate
                for i in range (0, num_rows):

                    timebase = temp_np[i][0]
                    timeyear = int(timebase/10000)
                    
                    # Quick hack to skip most of the data
                    # Note if we skip ALL the data, we still need to calculate
                    # last time, so we know nothing is valid later in the code
                    if timeyear < ts_list[0].year and i != num_rows - 1:
                        continue
                    elif b_skip == True:
                        ts_ctr = i
                        b_skip = False
                    
                    
                    timemonth = int((timebase-timeyear*10000)/100)
                    timeday = int((timebase-timeyear*10000-timemonth*100))
                    timehour = 16
    
                    #The earliest time it can generate a time for is platform dependent
                    symbol_ts_list[i]=dt.datetime(timeyear,timemonth,timeday,timehour) # To make the time 1600 hrs on the day previous to this midnight
                    
                #for ends
    
    
                #now we have only timestamps and one data column
                
                
                #Skip data from file which is before the first timestamp in ts_list
    
                while (ts_ctr < temp_np.shape[0]) and (symbol_ts_list[ts_ctr] < ts_list[0]):
                    ts_ctr=  ts_ctr+1
                    
                    #print "skipping initial data"
                    #while ends
                
                for time_stamp in ts_list:
                    
                    if (symbol_ts_list[-1] < time_stamp):
                        #The timestamp is after the last timestamp for which we have data. So we give up. Note that we don't have to fill in NaNs because that is 
                        #the default value.
                        break;
                    else:
                        while ((ts_ctr < temp_np.shape[0]) and (symbol_ts_list[ts_ctr]< time_stamp)):
                            ts_ctr = ts_ctr+1
                            #while ends
                        #else ends
                                            
                    #print "at time_stamp: " + str(time_stamp) + " and symbol_ts "  + str(symbol_ts_list[ts_ctr])
                    
                    if (time_stamp == symbol_ts_list[ts_ctr]):
                        #Data is present for this timestamp. So add to numpy array.
                        #print "    adding to numpy array"
                        if (temp_np.ndim > 1): #This if is needed because if a stock has data for 1 day only then the numpy array is 1-D rather than 2-D
                            all_stocks_data[lLabelNum][ts_list.index(time_stamp)][symbol_ctr] = temp_np [ts_ctr][1]
                        else:
                            all_stocks_data[lLabelNum][ts_list.index(time_stamp)][symbol_ctr] = temp_np [1]
                        #if ends
                        
                        ts_ctr = ts_ctr +1
                    
                #inner for ends
            #outer for ends
        #print all_stocks_data
        
        ldmReturn = [] # List of data matrixes to return
        for naDataLabel in all_stocks_data:
            ldmReturn.append( pa.DataFrame( naDataLabel, ts_list, symbol_list) )            

        
        ''' Contine to support single return type as a non-list '''
        if bStr:
            return ldmReturn[0]
        else:
            return ldmReturn            
        
        #get_data_hardread ends

    def get_data (self, ts_list, symbol_list, data_item, verbose=False, bIncDelist=False):
        '''
        Read data into a DataFrame, but check to see if it is in a cache first.
        @param ts_list: List of timestamps for which the data values are needed. Timestamps must be sorted.
        @param symbol_list: The list of symbols for which the data values are needed
        @param data_item: The data_item needed. Like open, close, volume etc.  May be a list, in which case a list of DataFrame is returned.
        @param bIncDelist: If true, delisted securities will be included.
        @note: If a symbol is not found then a message is printed. All the values in the column for that stock will be NaN. Execution then 
        continues as usual. No errors are raised at the moment.
        '''

        # Construct hash -- filename where data may be already
        #
        # The idea here is to create a filename from the arguments provided.
        # We then check to see if the filename exists already, meaning that
        # the data has already been created and we can just read that file.

        ls_syms_copy = copy.deepcopy(symbol_list)

        # Create the hash for the symbols
        hashsyms = 0
        for i in symbol_list:
            hashsyms = (hashsyms + hash(i)) % 10000000

        # Create the hash for the timestamps
        hashts = 0

        # print "test point 1: " + str(len(ts_list))
        # spyfile=os.environ['QSDATA'] + '/Processed/Norgate/Stocks/US/NYSE Arca/SPY.pkl'
        for i in ts_list:
            hashts = (hashts + hash(i)) % 10000000
        hashstr = 'qstk-' + str (self.source)+'-' +str(abs(hashsyms)) + '-' + str(abs(hashts)) \
            + '-' + str(hash(str(data_item))) #  + '-' + str(hash(str(os.path.getctime(spyfile))))

        # get the directory for scratch files from environment
        # try:
        #     scratchdir = os.environ['QSSCRATCH']
        # except KeyError:
        #     #self.rootdir = "/hzr71/research/QSData"
        #     raise KeyError("Please be sure to set the value for QSSCRATCH in config.sh or local.sh")

        # final complete filename
        cachefilename = self.scratchdir + '/' + hashstr + '.pkl'
        if verbose:
            print "cachefilename is: " + cachefilename

        # now eather read the pkl file, or do a hardread
        readfile = False  # indicate that we have not yet read the file

        #check if the cachestall variable is defined.
        # try:
        #     catchstall=dt.timedelta(hours=int(os.environ['CACHESTALLTIME']))
        # except:
        #     catchstall=dt.timedelta(hours=1)
        cachestall = dt.timedelta(hours=self.cachestalltime)

        # Check if the file is older than the cachestalltime
        if os.path.exists(cachefilename):
            if ((dt.datetime.now() - dt.datetime.fromtimestamp(os.path.getmtime(cachefilename))) < cachestall):
                if verbose:
                    print "cache hit"
                try:
                    cachefile = open(cachefilename, "rb")
                    start = time.time() # start timer
                    retval = pkl.load(cachefile)
                    elapsed = time.time() - start # end timer
                    readfile = True # remember success
                    cachefile.close()
                except IOError:
                    if verbose:
                        print "error reading cache: " + cachefilename
                        print "recovering..."
                except EOFError:
                    if verbose:
                        print "error reading cache: " + cachefilename
                        print "recovering..."
        if (readfile!=True):
            if verbose:
                print "cache miss"
                print "beginning hardread"
            start = time.time() # start timer
            if verbose:
                print "data_item(s): " + str(data_item)
                print "symbols to read: " + str(symbol_list)
            retval = self.get_data_hardread(ts_list, 
                symbol_list, data_item, verbose, bIncDelist)
            elapsed = time.time() - start # end timer
            if verbose:
                print "end hardread"
                print "saving to cache"
            try:
                cachefile = open(cachefilename,"wb")
                pkl.dump(retval, cachefile, -1)
                os.chmod(cachefilename,0666)
            except IOError:
                print "error writing cache: " + cachefilename
            if verbose:
                print "end saving to cache"
            if verbose:
                print "reading took " + str(elapsed) + " seconds"

        if type(retval) == type([]):
            for i, df_single in enumerate(retval):
                retval[i] = df_single.reindex(columns=ls_syms_copy)
        else:
            retval = retval.reindex(columns=ls_syms_copy)
        return retval

    def getPathOfFile(self, symbol_name, bDelisted=False):
        '''
        @summary: Since a given pkl file can exist in any of the folders- we need to look for it in each one until we find it. Thats what this function does.
        @return: Complete path to the pkl file including the file name and extension
        '''

        if not bDelisted:
            for path1 in self.folderList:
                if (os.path.exists(str(path1) + str(symbol_name + ".pkl"))):
                    # Yay! We found it!
                    return (str(str(path1) + str(symbol_name) + ".pkl"))
                    #if ends
                elif (os.path.exists(str(path1) + str(symbol_name + ".csv"))):
                    # Yay! We found it!
                    return (str(str(path1) + str(symbol_name) + ".csv"))
                #for ends

        else:
            ''' Special case for delisted securities '''
            lsPaths = []
            for sPath in self.folderList:
                if re.search('Delisted Securities', sPath) == None:
                    continue

                for sFile in dircache.listdir(sPath):
                    if not re.match( '%s-\d*.pkl'%symbol_name, sFile ) == None:
                        lsPaths.append(sPath + sFile)

            lsPaths.sort()
            return lsPaths

        print "Did not find path to " + str(symbol_name) + ". Looks like this file is missing"

    def getPathOfCSVFile(self, symbol_name):

        for path1 in self.folderList:
                if (os.path.exists(str(path1)+str(symbol_name+".csv"))):
                    # Yay! We found it!
                    return (str(str(path1)+str(symbol_name)+".csv"))
                    #if ends
                #for ends
        print "Did not find path to " + str (symbol_name)+". Looks like this file is missing"    

    def get_all_symbols (self):
        '''
        @summary: Returns a list of all the symbols located at any of the paths for this source. @see: {__init__}
        @attention: This will discard all files that are not of type pkl. ie. Only the files with an extension pkl will be reported.
        '''

        listOfStocks = list()
        #Path does not exist

        if (len(self.folderList) == 0):
            raise ValueError("DataAccess source not set")

        for path in self.folderList:
            stocksAtThisPath = list()
            #print str(path)
            stocksAtThisPath = dircache.listdir(str(path))
            #Next, throw away everything that is not a .pkl And these are our stocks!
            stocksAtThisPath = filter (lambda x:(str(x).find(str(self.fileExtensionToRemove)) > -1), stocksAtThisPath)
            #Now, we remove the .pkl to get the name of the stock
            stocksAtThisPath = map(lambda x:(x.partition(str(self.fileExtensionToRemove))[0]),stocksAtThisPath)

            listOfStocks.extend(stocksAtThisPath)
            #for stock in stocksAtThisPath:
                #listOfStocks.append(stock)
        return listOfStocks
        #get_all_symbols ends

    def check_symbol(self, symbol, s_list=None):
        '''
        @summary: Returns True if given symbol is present in the s_list.
        @param symbol: Symbol to be checked for.
        @param s_list: Optionally symbol sub-set listing can be given.
                        if not provided, all listings are searched.
        @return:  True if symbol is present in specified list, else False.
        '''
        
        all_symbols = list()
        
        # Create a super-set of symbols.
        if s_list is not None:
            all_symbols = self.get_symbols_from_list(s_list)
        else:
            all_symbols = self.get_all_symbols()
        
        # Check if the symbols is present.
        if ( symbol in all_symbols ):
            return True
        else:
            return False

    def get_symbols_from_list(self, s_list):
        ''' Reads all symbols from a list '''
        ls_symbols = []
        if (len(self.folderList) == 0):
            raise ValueError("DataAccess source not set")

        for path in self.folderList:
            path_to_look = path + 'Lists/' + s_list + '.txt'
            ffile = open(path_to_look, 'r')
            for f in ffile.readlines():
                j = f[:-1]
                ls_symbols.append(j)
            ffile.close()

        return ls_symbols

    def get_symbols_in_sublist (self, subdir):
        '''
        @summary: Returns all the symbols belonging to that subdir of the data store.
        @param subdir: Specifies which subdir you want.
        @return: A list of symbols belonging to that subdir
        '''

        pathtolook = self.rootdir + self.midPath + subdir
        stocksAtThisPath = dircache.listdir(pathtolook)

        #Next, throw away everything that is not a .pkl And these are our stocks!
        try:
            stocksAtThisPath = filter (lambda x:(str(x).find(str(self.fileExtensionToRemove)) > -1), stocksAtThisPath)
            #Now, we remove the .pkl to get the name of the stock
            stocksAtThisPath = map(lambda x:(x.partition(str(self.fileExtensionToRemove))[0]),stocksAtThisPath)
        except:
            print "error: no path to " + subdir
            stocksAtThisPath = list()

        return stocksAtThisPath
        #get_all_symbols_on_exchange ends

    def get_sublists(self):
        '''
        @summary: Returns a list of all the sublists for a data store.
        @return: A list of the valid sublists for the data store.
        '''

        return self.folderSubList
        #get_sublists

    def get_data_labels(self):
        '''
        @summary: Returns a list of all the data labels available for this type of data access object.
        @return: A list of label strings.
        '''

        if (self.source != DataSource.COMPUSTAT):
            print 'Function only valid for Compustat objects!'
            return []

        return DataItem.COMPUSTAT

        #get_data_labels

    def get_info(self):
        '''
        @summary: Returns and prints a string that describes the datastore.
        @return: A string.
        '''

        if (self.source == DataSource.NORGATE):
            retstr = "Norgate:\n"
            retstr = retstr + "Daily price and volume data from Norgate (premiumdata.net)\n"
            retstr = retstr + "that is valid at the time of NYSE close each trading day.\n"
            retstr = retstr + "\n"
            retstr = retstr + "Valid data items include: \n"
            retstr = retstr + "\topen, high, low, close, volume, actual_close\n"
            retstr = retstr + "\n"
            retstr = retstr + "Valid subdirs include: \n"
            for i in self.folderSubList:
                retstr = retstr + "\t" + i + "\n"

        elif (self.source == DataSource.YAHOO):
            retstr = "Yahoo:\n"
            retstr = retstr + "Attempts to load a custom data set, assuming each stock has\n"
            retstr = retstr + "a csv file with the name and first column as the stock ticker,\ date in second column, and data in following columns.\n"
            retstr = retstr + "everything should be located in QSDATA/Yahoo\n"
            for i in self.folderSubList:
                retstr = retstr + "\t" + i + "\n"

        elif (self.source == DataSource.COMPUSTAT):
            retstr = "Compustat:\n"
            retstr = retstr + "Compilation of (almost) all data items provided by Compustat\n"
            retstr = retstr + "Valid data items can be retrieved by calling get_data_labels(): \n"
            retstr = retstr + "\n"
            retstr = retstr + "Valid subdirs include: \n"
            for i in self.folderSubList:
                retstr = retstr + "\t" + i + "\n"
        elif (self.source == DataSource.CUSTOM):
            retstr = "Custom:\n"
            retstr = retstr + "Attempts to load a custom data set, assuming each stock has\n"
            retstr = retstr + "a csv file with the name and first column as the stock ticker, date in second column, and data in following columns.\n"
            retstr = retstr + "everything should be located in QSDATA/Processed/Custom\n"
        elif (self.source == DataSource.MLT):
            retstr = "ML4Trading:\n"
            retstr = retstr + "Attempts to load a custom data set, assuming each stock has\n"
            retstr = retstr + "a csv file with the name and first column as the stock ticker,\ date in second column, and data in following columns.\n"
            retstr = retstr + "everything should be located in QSDATA/Processed/ML4Trading\n"
        else:
            retstr = "DataAccess internal error\n"

        print retstr
        return retstr
        #get_sublists


    #class DataAccess ends
if __name__ == '__main__':
    # Setup DataAccess object
    c_dataobj = DataAccess('Yahoo')
    
    # Check if GOOG is a valid symbol.
    val = c_dataobj.check_symbol('GOOG')
    print "Is GOOG a valid symbol? :" , val
    
    # Check if QWERTY is a valid symbol.
    val = c_dataobj.check_symbol('QWERTY')
    print "Is QWERTY a valid symbol? :" , val

    # Check if EBAY is part of SP5002012 list.
    val = c_dataobj.check_symbol('EBAY', s_list='sp5002012')
    print "Is EBAY a valid symbol in SP5002012 list? :", val

    # Check if GLD is part of SP5002012 after checking if GLD is a valid symbol.
    val = c_dataobj.check_symbol('GLD')
    print "Is GLD a valid symbol? : ", val
    val = c_dataobj.check_symbol('GLD', 'sp5002012')
    print "Is GLD a valid symbol in sp5002012 list? :", val

########NEW FILE########
__FILENAME__ = fundutil
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''

import QSTK.qstkutil.tsutil as tsu

def get_winning_days(fund_ts):
    """
    @summary Returns percentage of winning days in fund time series
    @param fund_ts: pandas time series of daily fund values
    @return Percentage of winning days over fund time series
    """
    return tsu.get_winning_days(tsu.daily(fund_ts))

def get_max_draw_down(fund_ts):
    """
    @summary Returns max draw down of fund time series (in percentage)
    @param fund_ts: pandas time series of daily fund values
    @return Max draw down of fund time series
    """
    MDD = 0
    DD = 0
    peak = -99999
    for value in fund_ts:
        if (value > peak):
            peak = value
        else:
            DD = (peak - value) / peak
        if (DD > MDD):
            MDD = DD
    return -1*MDD

def get_sortino_ratio(fund_ts):
    """
    @summary Returns daily computed Sortino ratio of fund time series
    @param fund_ts: pandas time series of daily fund values
    @return Sortino ratio of fund time series
    """
    return tsu.get_sortino_ratio(tsu.daily(fund_ts))

def get_sharpe_ratio(fund_ts):
    """
    @summary Returns daily computed Sharpe ratio of fund time series
    @param fund_ts: pandas time series of daily fund values
    @return  Sharpe ratio of  fund time series
    """
    return tsu.get_sharpe_ratio(tsu.daily(fund_ts))




########NEW FILE########
__FILENAME__ = qsdateutil
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''


import datetime as dt
from datetime import timedelta
import time as t
import numpy as np
import os
import pandas as pd


def _cache_dates():
    ''' Caches dates '''
    try:
        # filename = os.environ['QS'] + "/qstkutil/NYSE_dates.txt"
        filename = os.path.join(os.path.dirname(__file__), 'NYSE_dates.txt')
    except KeyError:
        print "Please be sure you have NYSE_dates.txt in the qstkutil directory"

    datestxt = np.loadtxt(filename, dtype=str)
    dates = []
    for i in datestxt:
        dates.append(dt.datetime.strptime(i, "%m/%d/%Y"))
    return pd.TimeSeries(index=dates, data=dates)

GTS_DATES = _cache_dates()



def getMonthNames():
    return(['JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC'])

def getYears(funds):
    years=[]
    for date in funds.index:
        if(not(date.year in years)):
            years.append(date.year)
    return(years)

def getMonths(funds,year):
    months=[]
    for date in funds.index:
        if((date.year==year) and not(date.month in months)):
            months.append(date.month)
    return(months)

def getDays(funds,year,month):
    days=[]
    for date in funds.index:
        if((date.year==year) and (date.month==month)):
            days.append(date)
    return(days)

def getDaysBetween(ts_start, ts_end):
    days=[]
    for i in range(0,(ts_end-ts_start).days):
        days.append(ts_start+timedelta(days=1)*i)
    return(days)

def getFirstDay(funds,year,month):
    for date in funds.index:
        if((date.year==year) and (date.month==month)):
            return(date)
    return('ERROR')

def getLastDay(funds,year,month):
    return_date = 'ERROR'
    for date in funds.index:
        if((date.year==year) and (date.month==month)):
            return_date = date
    return(return_date)

def getNextOptionClose(day, trade_days, offset=0):
    #get third friday in month of day
    #get first of month
    year_off=0
    if day.month+offset > 12:
        year_off = 1
        offset = offset - 12
    first = dt.datetime(day.year+year_off, day.month+offset, 1, hour=16)
    #get weekday
    day_num = first.weekday()
    #get first friday (friday - weekday) add 7 if less than 1
    dif = 5 - day_num
    if dif < 1:
        dif = dif+7
    #move to third friday
    dif = dif + 14
    friday = first+dt.timedelta(days=(dif-1))
    #if friday is a holiday, options expire then
    if friday in trade_days:
        month_close = first + dt.timedelta(days=dif)
    else:
        month_close = friday
    #if day is past the day after that
    if month_close < day:
        return_date = getNextOptionClose(day, trade_days, offset=1)
    else:
        return_date = month_close
    return(return_date)

def getLastOptionClose(day, trade_days):
    start = day
    while getNextOptionClose(day, trade_days)>=start:
        day= day - dt.timedelta(days=1)
    return(getNextOptionClose(day, trade_days))


def getNYSEoffset(mark, offset):
    ''' Returns NYSE date offset by number of days '''
    mark = mark.replace(hour=0, minute=0, second=0, microsecond=0)
    
    i = GTS_DATES.index.searchsorted(mark, side='right')
    # If there is no exact match, take first date in past
    if GTS_DATES[i] != mark:
        i -= 1
        
    ret = GTS_DATES[i + offset]

    ret = ret.replace(hour=16)

    return ret


def getNYSEdays(startday = dt.datetime(1964,7,5), endday = dt.datetime(2020,12,31),
    timeofday = dt.timedelta(0)):
    """
    @summary: Create a list of timestamps between startday and endday (inclusive)
    that correspond to the days there was trading at the NYSE. This function
    depends on a separately created a file that lists all days since July 4,
    1962 that the NYSE has been open, going forward to 2020 (based
    on the holidays that NYSE recognizes).

    @param startday: First timestamp to consider (inclusive)
    @param endday: Last day to consider (inclusive)
    @return list: of timestamps between startday and endday on which NYSE traded
    @rtype datetime
    """
    start = startday - timeofday
    end = endday - timeofday

    dates = GTS_DATES[start:end]

    ret = [x + timeofday for x in dates]

    return(ret)

def getNextNNYSEdays(startday, days, timeofday):
    """
    @summary: Create a list of timestamps from startday that is days days long
    that correspond to the days there was trading at  NYSE. This function
    depends on the file used in getNYSEdays and assumes the dates within are
    in order.
    @param startday: First timestamp to consider (inclusive)
    @param days: Number of timestamps to return
    @return list: List of timestamps starting at startday on which NYSE traded
    @rtype datetime
    """
    try:
        # filename = os.environ['QS'] + "/qstkutil/NYSE_dates.txt"
        filename = os.path.join(os.path.dirname(__file__), 'NYSE_dates.txt')
    except KeyError:
        print "Please be sure to set the value for QS in config.sh or\n"
        print "in local.sh and then \'source local.sh\'.\n"

    datestxt = np.loadtxt(filename,dtype=str)
    dates=[]
    for i in datestxt:
        if(len(dates)<days):
            if((dt.datetime.strptime(i,"%m/%d/%Y")+timeofday)>=startday):
                dates.append(dt.datetime.strptime(i,"%m/%d/%Y")+timeofday)
    return(dates)

def getPrevNNYSEday(startday, timeofday):
    """
    @summary: This function returns the last valid trading day before the start
    day, or returns the start day if it is a valid trading day. This function
    depends on the file used in getNYSEdays and assumes the dates within are
    in order.
    @param startday: First timestamp to consider (inclusive)
    @param days: Number of timestamps to return
    @return list: List of timestamps starting at startday on which NYSE traded
    @rtype datetime
    """
    try:
        # filename = os.environ['QS'] + "/qstkutil/NYSE_dates.txt"
        filename = os.path.join(os.path.dirname(__file__), 'NYSE_dates.txt')
    except KeyError:
        print "Please be sure to set the value for QS in config.sh or\n"
        print "in local.sh and then \'source local.sh\'.\n"

    datestxt = np.loadtxt(filename,dtype=str)

    #''' Set return to first day '''
    dtReturn = dt.datetime.strptime( datestxt[0],"%m/%d/%Y")+timeofday

    #''' Loop through all but first '''
    for i in datestxt[1:]:
        dtNext = dt.datetime.strptime(i,"%m/%d/%Y")

        #''' If we are > startday, then use previous valid day '''
        if( dtNext > startday ):
            break

        dtReturn = dtNext + timeofday

    return(dtReturn)

def ymd2epoch(year, month, day):
    """
    @summary: Convert YMD info into a unix epoch value.
    @param year: The year
    @param month: The month
    @param day: The day
    @return epoch: number of seconds since epoch
    """
    return(t.mktime(dt.date(year,month,day).timetuple()))

def epoch2date(ts):
    """
    @summary Convert seconds since epoch into date
    @param ts: Seconds since epoch
    @return thedate: A date object
    """
    tm = t.gmtime(ts)
    return(dt.date(tm.tm_year,tm.tm_mon,tm.tm_mday))


def _trade_dates(dt_start, dt_end, s_period):
    '''
    @summary: Generate dates on which we need to trade
    @param c_strat: Strategy config class
    @param dt_start: Start date
    @param dt_end: End date
    '''

    ldt_timestamps = getNYSEdays(dt_start,
                dt_end, dt.timedelta(hours=16) )


    # Use pandas reindex method instead
    # Note, dates are index as well as values, we select based on index
    # but return values since it is a numpy array of datetimes instead of
    # pandas specific.
    ts_dates = pd.TimeSeries(index=ldt_timestamps, data=ldt_timestamps)

    # These are the dates we want
    if s_period[:2] == 'BW':
        # special case for biweekly

        dr_range = pd.DateRange(dt_start, dt_end,
                                timeRule=s_period[1:])
        dr_range = np.asarray(dr_range)
        li_even = np.array(range(len(dr_range)))
        dr_range = dr_range[li_even[li_even % 2 == 0]]
    else:
        dr_range = pd.DateRange(dt_start, dt_end,
                                timeRule=s_period)
        dr_range = np.asarray(dr_range)


    # Warning, we MUST copy the date range, if we modify it it will be returned
    # in it's modified form the next time we use it.
    dr_range = np.copy(dr_range)
    dr_range += pd.DateOffset(hours=16)
    ts_dates = ts_dates.reindex( dr_range, method='bfill' )
    ldt_dates = ts_dates[ts_dates.notnull()].values

    #Make unique
    sdt_unique = set()
    ldt_dates = [x for x in ldt_dates
                 if x not in sdt_unique and not sdt_unique.add(x)]

    return ldt_dates

########NEW FILE########
__FILENAME__ = test_utils
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on May 14, 2012

@author: John Cornwell
@contact:  John@lucenaresearch.com
@summary: 

'''

# Python imports
import unittest

# 3rd party imports

# QSTK imports



class Test(unittest.TestCase):


    def setUp(self):
        pass


    def tearDown(self):
        pass


    def test_import(self):
        # Silly example to test current error in loading utils
        import qstkutil.utils as utils
        self.assertTrue(True)
        


if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()
########NEW FILE########
__FILENAME__ = tsutil
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license.  Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Jan 1, 2011

@author:Drew Bratcher
@contact: dbratcher@gatech.edu
@summary: Contains tutorial for backtester and report.

'''


import math
import datetime as dt
import numpy as np
from QSTK.qstkutil import qsdateutil
from math import sqrt
import pandas as pd
from copy import deepcopy


import random as rand

from QSTK.qstkutil import DataAccess as da
from QSTK.qstkutil import qsdateutil as du
import numpy as np

def daily(lfFunds):
    """
    @summary Computes daily returns centered around 0
    @param funds: A time series containing daily fund values
    @return an array of daily returns
    """
    if type(lfFunds) == type(pd.Series()):
        ldt_timestamps = du.getNYSEdays(lfFunds.index[0], lfFunds.index[-1], dt.timedelta(hours=16))
        lfFunds = lfFunds.reindex(index=ldt_timestamps, method='ffill')
    nds = np.asarray(deepcopy(lfFunds))
    s= np.shape(nds)
    if len(s)==1:
        nds=np.expand_dims(nds,1)
    returnize0(nds)
    return(nds)

def daily1(lfFunds):
    """
    @summary Computes daily returns centered around 1
    @param funds: A time series containing daily fund values
    @return an array of daily returns
    """
    nds = np.asarray(deepcopy(lfFunds))
    s= np.shape(nds)
    if len(s)==1:
        nds=np.expand_dims(nds,1)
    returnize1(nds)
    return(nds)

def monthly(funds):
    """
    @summary Computes monthly returns centered around 0
    @param funds: A time series containing daily fund values
    @return an array of monthly returns
    """
    funds2 = []
    last_last_month = -1
    years = qsdateutil.getYears(funds)
    for year in years:
        months = qsdateutil.getMonths(funds, year)
        for month in months:
            last_this_month = qsdateutil.getLastDay(funds, year, month)
            if last_last_month == -1 :
                last_last_month=qsdateutil.getFirstDay(funds, year, month)
            if type(funds).__name__=='TimeSeries':
                funds2.append(funds[last_this_month]/funds[last_last_month]-1)
            else:
                funds2.append(funds.xs(last_this_month)/funds.xs(last_last_month)-1)
            last_last_month = last_this_month
    return(funds2)

def average_monthly(funds):
    """
    @summary Computes average monthly returns centered around 0
    @param funds: A time series containing daily fund values
    @return an array of average monthly returns
    """
    rets = daily(funds)
    ret_i = 0
    years = qsdateutil.getYears(funds)
    averages = []
    for year in years:
        months = qsdateutil.getMonths(funds, year)
        for month in months:
            avg = 0
            count = 0
            days = qsdateutil.getDays(funds, year, month)
            for day in days:
                avg += rets[ret_i]
                ret_i += 1
                count += 1
            averages.append(float(avg) / count)
    return(averages)    

def fillforward(nds):
    """
    @summary Removes NaNs from a 2D array by scanning forward in the 
    1st dimension.  If a cell is NaN, the value above it is carried forward.
    @param nds: the array to fill forward
    @return the array is revised in place
    """
    for col in range(nds.shape[1]):
        for row in range(1, nds.shape[0]):
            if math.isnan(nds[row, col]):
                nds[row, col] = nds[row-1, col]

def fillbackward(nds):
    """
    @summary Removes NaNs from a 2D array by scanning backward in the 
    1st dimension.  If a cell is NaN, the value above it is carried backward.
    @param nds: the array to fill backward
    @return the array is revised in place
    """
    for col in range(nds.shape[1]):
        for row in range(nds.shape[0] - 2, -1, -1):
            if math.isnan(nds[row, col]):
                nds[row, col] = nds[row+1, col]


def returnize0(nds):
    """
    @summary Computes stepwise (usually daily) returns relative to 0, where
    0 implies no change in value.
    @return the array is revised in place
    """
    if type(nds) == type(pd.DataFrame()):
        nds = (nds / nds.shift(1)) - 1.0
        nds = nds.fillna(0.0)
        return nds

    s= np.shape(nds)
    if len(s)==1:
        nds=np.expand_dims(nds,1)
    nds[1:, :] = (nds[1:, :] / nds[0:-1]) - 1
    nds[0, :] = np.zeros(nds.shape[1])
    return nds


def returnize1(nds):
    """
    @summary Computes stepwise (usually daily) returns relative to 1, where
    1 implies no change in value.
    @param nds: the array to fill backward
    @return the array is revised in place
    """
    if type(nds) == type(pd.DataFrame()):
        nds = nds / nds.shift(1)
        nds = nds.fillna(1.0)
        return nds

    s= np.shape(nds)
    if len(s)==1:
        nds=np.expand_dims(nds,1)
    nds[1:, :] = (nds[1:, :]/nds[0:-1])
    nds[0, :] = np.ones(nds.shape[1])
    return nds


def priceize1(nds):
    """
    @summary Computes stepwise (usually daily) returns relative to 1, where
    1 implies no change in value.
    @param nds: the array to fill backward
    @return the array is revised in place
    """
    
    nds[0, :] = 100 
    for i in range(1, nds.shape[0]):
        nds[i, :] = nds[i-1, :] * nds[i, :]
    
    
def logreturnize(nds):
    """
    @summary Computes stepwise (usually daily) logarithmic returns.
    @param nds: the array to fill backward
    @return the array is revised in place
    """
    returnize1(nds)
    nds = np.log(nds)
    return nds

def get_winning_days( rets):
    """
    @summary Returns the percentage of winning days of the returns.
    @param rets: 1d numpy array or fund list of daily returns (centered on 0)
    @return Percentage of winning days
    """
    negative_rets = []
    for i in rets:
        if(i<0):
            negative_rets.append(i)
    return 100 * (1 - float(len(negative_rets)) / float(len(rets)))

def get_max_draw_down(ts_vals):
    """
    @summary Returns the max draw down of the returns.
    @param ts_vals: 1d numpy array or fund list
    @return Max draw down
    """
    MDD = 0
    DD = 0
    peak = -99999
    for value in ts_vals:
        if (value > peak):
            peak = value
        else:
            DD = (peak - value) / peak
        if (DD > MDD):
            MDD = DD
    return -1*MDD

def get_sortino_ratio( rets, risk_free=0.00 ):
    """
    @summary Returns the daily Sortino ratio of the returns.
    @param rets: 1d numpy array or fund list of daily returns (centered on 0)
    @param risk_free: risk free return, default is 0%
    @return Sortino Ratio, computed off daily returns
    """
    rets = np.asarray(rets)
    f_mean = np.mean( rets, axis=0 )
    negative_rets = rets[rets < 0]
    f_dev = np.std( negative_rets, axis=0 )
    f_sortino = (f_mean*252 - risk_free) / (f_dev * np.sqrt(252))
    return f_sortino

def get_sharpe_ratio( rets, risk_free=0.00 ):
    """
    @summary Returns the daily Sharpe ratio of the returns.
    @param rets: 1d numpy array or fund list of daily returns (centered on 0)
    @param risk_free: risk free returns, default is 0%
    @return Annualized rate of return, not converted to percent
    """
    f_dev = np.std( rets, axis=0 )
    f_mean = np.mean( rets, axis=0 )
    
    f_sharpe = (f_mean *252 - risk_free) / ( f_dev * np.sqrt(252) )
    
    return f_sharpe

def get_ror_annual( rets ):
    """
    @summary Returns the rate of return annualized.  Assumes len(rets) is number of days.
    @param rets: 1d numpy array or list of daily returns
    @return Annualized rate of return, not converted to percent
    """

    f_inv = 1.0
    for f_ret in rets:
        f_inv = f_inv * f_ret
    
    f_ror_ytd = f_inv - 1.0    
    
    #print ' RorYTD =', f_inv, 'Over days:', len(rets)
    
    return ( (1.0 + f_ror_ytd)**( 1.0/(len(rets)/252.0) ) ) - 1.0

def getPeriodicRets( dmPrice, sOffset ):
    """
    @summary Reindexes a DataMatrix price array and returns the new periodic returns.
    @param dmPrice: DataMatrix of stock prices
    @param sOffset: Offset string to use, choose from _offsetMap in pandas/core/datetools.py
                    e.g. 'EOM', 'WEEKDAY', 'W@FRI', 'A@JAN'.  Or use a pandas DateOffset.
    """    
    
    # Could possibly use DataMatrix.asfreq here """
    # Use pandas DateRange to create the dates we want, use 4:00 """
    drNewRange = DateRange(dmPrice.index[0], dmPrice.index[-1], timeRule=sOffset)
    drNewRange += DateOffset(hours=16)
    
    dmPrice = dmPrice.reindex( drNewRange, method='ffill' )  

    returnize1( dmPrice.values )
    
    # Do not leave return of 1.0 for first time period: not accurate """
    return dmPrice[1:]

def getReindexedRets( rets, l_period ):
    """
    @summary Reindexes returns using the cumulative product. E.g. if returns are 1.5 and 1.5, a period of 2 will
             produce a 2-day return of 2.25.  Note, these must be returns centered around 1.
    @param rets: Daily returns of the various stocks (using returnize1)
    @param l_period: New target period.
    @note: Note that this function does not track actual weeks or months, it only approximates with trading days.
           You can use 5 for week, or 21 for month, etc.
    """    
    naCumData = np.cumprod(rets, axis=0)

    lNewRows =(rets.shape[0]-1) / (l_period)
    # We compress data into height / l_period + 1 new rows """
    for i in range( lNewRows ):
        lCurInd = -1 - i*l_period
        # Just hold new data in same array"""
        # new return is cumprod on day x / cumprod on day x-l_period """
        start=naCumData[lCurInd - l_period, :]
        naCumData[-1 - i, :] = naCumData[lCurInd, :] / start 
        # Select new returns from end of cumulative array """
    
    return naCumData[-lNewRows:, ]

        
def getOptPort(rets, f_target, l_period=1, naLower=None, naUpper=None, lNagDebug=0):
    """
    @summary Returns the Markowitz optimum portfolio for a specific return.
    @param rets: Daily returns of the various stocks (using returnize1)
    @param f_target: Target return, i.e. 0.04 = 4% per period
    @param l_period: Period to compress the returns to, e.g. 7 = weekly
    @param naLower: List of floats which corresponds to lower portfolio% for each stock
    @param naUpper: List of floats which corresponds to upper portfolio% for each stock 
    @return tuple: (weights of portfolio, min possible return, max possible return)
    """
    
    # Attempt to import library """
    try:
        pass
        import nagint as nag
    except ImportError:
        print 'Could not import NAG library'
        print 'make sure nagint.so is in your python path'
        return ([], 0, 0)
    
    # Get number of stocks """
    lStocks = rets.shape[1]
    
    # If period != 1 we need to restructure the data """
    if( l_period != 1 ):
        rets = getReindexedRets( rets, l_period)
    
    # Calculate means and covariance """
    naAvgRets = np.average( rets, axis=0 )
    naCov = np.cov( rets, rowvar=False )
    
    # Special case for None == f_target"""
    # simply return average returns and cov """
    if( f_target is None ):
        return naAvgRets, np.std(rets, axis=0)
    
    # Calculate upper and lower limits of variables as well as constraints """
    if( naUpper is None ): 
        naUpper = np.ones( lStocks )  # max portfolio % is 1
    
    if( naLower is None ): 
        naLower = np.zeros( lStocks ) # min is 0, set negative for shorting
    # Two extra constraints for linear conditions"""
    # result = desired return, and sum of weights = 1 """
    naUpper = np.append( naUpper, [f_target, 1.0] )
    naLower = np.append( naLower, [f_target, 1.0] )
    
    # Initial estimate of portfolio """
    naInitial = np.array([1.0/lStocks]*lStocks)
    
    # Set up constraints matrix"""
    # composed of expected returns in row one, unity row in row two """
    naConstraints = np.vstack( (naAvgRets, np.ones(lStocks)) )

    # Get portfolio weights, last entry in array is actually variance """
    try:
        naReturn = nag.optPort( naConstraints, naLower, naUpper, \
                                      naCov, naInitial, lNagDebug )
    except RuntimeError:
        print 'NAG Runtime error with target: %.02lf'%(f_target)
        return ( naInitial, sqrt( naCov[0][0] ) )  
    #return semi-junk to not mess up the rest of the plot

    # Calculate stdev of entire portfolio to return"""
    # what NAG returns is slightly different """
    fPortDev = np.std( np.dot(rets, naReturn[0,0:-1]) )
    
    # Show difference between above stdev and sqrt NAG covariance"""
    # possibly not taking correlation into account """
    #print fPortDev / sqrt(naReturn[0, -1]) 

    # Return weights and stdDev of portfolio."""
    #  note again the last value of naReturn is NAG's reported variance """
    return (naReturn[0, 0:-1], fPortDev)


def OptPort( naData, fTarget, naLower=None, naUpper=None, naExpected=None, s_type = "long"):
    """
    @summary Returns the Markowitz optimum portfolio for a specific return.
    @param naData: Daily returns of the various stocks (using returnize1)
    @param fTarget: Target return, i.e. 0.04 = 4% per period
    @param lPeriod: Period to compress the returns to, e.g. 7 = weekly
    @param naLower: List of floats which corresponds to lower portfolio% for each stock
    @param naUpper: List of floats which corresponds to upper portfolio% for each stock 
    @return tuple: (weights of portfolio, min possible return, max possible return)
    """
    ''' Attempt to import library '''
    try:
        pass
        from cvxopt import matrix
        from cvxopt.blas import dot
        from cvxopt.solvers import qp, options

    except ImportError:
        print 'Could not import CVX library'
        raise
    
    ''' Get number of stocks '''
    length = naData.shape[1]
    b_error = False

    naLower = deepcopy(naLower)
    naUpper = deepcopy(naUpper)
    naExpected = deepcopy(naExpected)
    
    # Assuming AvgReturns as the expected returns if parameter is not specified
    if (naExpected==None):
        naExpected = np.average( naData, axis=0 )

    na_signs = np.sign(naExpected)
    indices,  = np.where(na_signs == 0)
    na_signs[indices] = 1
    if s_type == "long":
        na_signs = np.ones(len(na_signs))
    elif s_type == "short":
        na_signs = np.ones(len(na_signs))*(-1)
    
    naData = na_signs*naData
    naExpected = na_signs*naExpected

    # Covariance matrix of the Data Set
    naCov=np.cov(naData, rowvar=False)
    
    # If length is one, just return 100% single symbol
    if length == 1:
        return (list(na_signs), np.std(naData, axis=0)[0], False)
    if length == 0:
        return ([], [0], False)
    # If we have 0/1 "free" equity we can't optimize
    # We just use     limits since we are stuck with 0 degrees of freedom
    
    ''' Special case for None == fTarget, simply return average returns and cov '''
    if( fTarget is None ):
        return (naExpected, np.std(naData, axis=0), b_error)
    
    # Upper bound of the Weights of a equity, If not specified, assumed to be 1.
    if(naUpper is None):
        naUpper= np.ones(length)
    
    # Lower bound of the Weights of a equity, If not specified assumed to be 0 (No shorting case)
    if(naLower is None):
        naLower= np.zeros(length)

    if sum(naLower) == 1:
        fPortDev = np.std(np.dot(naData, naLower))
        return (naLower, fPortDev, False)

    if sum(naUpper) == 1:
        fPortDev = np.std(np.dot(naData, naUpper))
        return (naUpper, fPortDev, False)
    
    naFree = naUpper != naLower
    if naFree.sum() <= 1:
        lnaPortfolios = naUpper.copy()
        
        # If there is 1 free we need to modify it to make the total
        # Add up to 1
        if naFree.sum() == 1:
            f_rest = naUpper[~naFree].sum()
            lnaPortfolios[naFree] = 1.0 - f_rest
            
        lnaPortfolios = na_signs * lnaPortfolios
        fPortDev = np.std(np.dot(naData, lnaPortfolios))
        return (lnaPortfolios, fPortDev, False)

    # Double the covariance of the diagonal elements for calculating risk.
    for i in range(length):
        naCov[i][i]=2*naCov[i][i]

    # Note, returns are modified to all be long from here on out
    (fMin, fMax) = getRetRange(False, naLower, naUpper, naExpected, "long") 
    #print (fTarget, fMin, fMax)
    if fTarget<fMin or fTarget>fMax:
        print "Target not possible", fTarget, fMin, fMax
        b_error = True

    naLower = naLower*(-1)
 
    # Setting up the parameters for the CVXOPT Library, it takes inputs in Matrix format.
    '''
    The Risk minimization problem is a standard Quadratic Programming problem according to the Markowitz Theory.
    '''
    S=matrix(naCov)
    #pbar=matrix(naExpected)
    naLower.shape=(length,1)
    naUpper.shape=(length,1)
    naExpected.shape = (1,length)
    zeo=matrix(0.0,(length,1))
    I = np.eye(length)
    minusI=-1*I
    G=matrix(np.vstack((I, minusI)))
    h=matrix(np.vstack((naUpper, naLower)))
    ones=matrix(1.0,(1,length)) 
    A=matrix(np.vstack((naExpected, ones)))
    b=matrix([float(fTarget),1.0])

    # Optional Settings for CVXOPT
    options['show_progress'] = False
    options['abstol']=1e-25
    options['reltol']=1e-24
    options['feastol']=1e-25
    

    # Optimization Calls
    # Optimal Portfolio
    try:
            lnaPortfolios = qp(S, -zeo, G, h, A, b)['x']
    except:
        b_error = True

    if b_error == True:
        print "Optimization not Possible"
        na_port = naLower*-1
        if sum(na_port) < 1:
            if sum(naUpper) == 1:
                na_port = naUpper
            else:
                i=0
                while(sum(na_port)<1 and i<25):
                    naOrder = naUpper - na_port
                    i = i+1
                    indices = np.where(naOrder > 0)
                    na_port[indices]= na_port[indices] + (1-sum(na_port))/len(indices[0]) 
                    naOrder = naUpper - na_port
                    indices = np.where(naOrder < 0)
                    na_port[indices]= naUpper[indices]
            
        lnaPortfolios = matrix(na_port)

    lnaPortfolios = (na_signs.reshape(-1,1) * lnaPortfolios).reshape(-1)
    # Expected Return of the Portfolio
    # lfReturn = dot(pbar, lnaPortfolios)
    
    # Risk of the portfolio
    fPortDev = np.std(np.dot(naData, lnaPortfolios))
    return (lnaPortfolios, fPortDev, b_error)


def getRetRange( rets, naLower, naUpper, naExpected = "False", s_type = "long"):
    """
    @summary Returns the range of possible returns with upper and lower bounds on the portfolio participation
    @param rets: Expected returns
    @param naLower: List of lower percentages by stock
    @param naUpper: List of upper percentages by stock
    @return tuple containing (fMin, fMax)
    """    
    
    # Calculate theoretical minimum and maximum theoretical returns """
    fMin = 0
    fMax = 0

    rets = deepcopy(rets)
    
    if naExpected == "False":
        naExpected = np.average( rets, axis=0 )
        
    na_signs = np.sign(naExpected)
    indices,  = np.where(na_signs == 0)
    na_signs[indices] = 1
    if s_type == "long":
        na_signs = np.ones(len(na_signs))
    elif s_type == "short":
        na_signs = np.ones(len(na_signs))*(-1)
    
    rets = na_signs*rets
    naExpected = na_signs*naExpected

    naSortInd = naExpected.argsort()
    
    # First add the lower bounds on portfolio participation """ 
    for i, fRet in enumerate(naExpected):
        fMin = fMin + fRet*naLower[i]
        fMax = fMax + fRet*naLower[i]


    # Now calculate minimum returns"""
    # allocate the max possible in worst performing equities """
    # Subtract min since we have already counted it """
    naUpperAdd = naUpper - naLower
    fTotalPercent = np.sum(naLower[:])
    for i, lInd in enumerate(naSortInd):
        fRetAdd = naUpperAdd[lInd] * naExpected[lInd]
        fTotalPercent = fTotalPercent + naUpperAdd[lInd]
        fMin = fMin + fRetAdd
        # Check if this additional percent puts us over the limit """
        if fTotalPercent > 1.0:
            fMin = fMin - naExpected[lInd] * (fTotalPercent - 1.0)
            break
    
    # Repeat for max, just reverse the sort, i.e. high to low """
    naUpperAdd = naUpper - naLower
    fTotalPercent = np.sum(naLower[:])
    for i, lInd in enumerate(naSortInd[::-1]):
        fRetAdd = naUpperAdd[lInd] * naExpected[lInd]
        fTotalPercent = fTotalPercent + naUpperAdd[lInd]
        fMax = fMax + fRetAdd
        
        # Check if this additional percent puts us over the limit """
        if fTotalPercent > 1.0:
            fMax = fMax - naExpected[lInd] * (fTotalPercent - 1.0)
            break

    return (fMin, fMax)


def _create_dict(df_rets, lnaPortfolios):

    allocations = {}
    for i, sym in enumerate(df_rets.columns):
        allocations[sym] = lnaPortfolios[i]

    return allocations

def optimizePortfolio(df_rets, list_min, list_max, list_price_target, 
                      target_risk, direction="long"):
    
    naLower = np.array(list_min)
    naUpper = np.array(list_max)
    naExpected = np.array(list_price_target)      

    b_same_flag = np.all( naExpected == naExpected[0])
    if b_same_flag and (naExpected[0] == 0):
        naExpected = naExpected + 0.1
    if b_same_flag:
        na_randomness = np.ones(naExpected.shape)
        target_risk = 0
        for i in range(len(na_randomness)):
            if i%2 ==0:
                na_randomness[i] = -1
        naExpected = naExpected + naExpected*0.0000001*na_randomness

    (fMin, fMax) = getRetRange( df_rets.values, naLower, naUpper, 
                                naExpected, direction)
    
    # Try to avoid intractible endpoints due to rounding errors """
    fMin += abs(fMin) * 0.00000000001 
    fMax -= abs(fMax) * 0.00000000001
    
    if target_risk == 1:
        (naPortWeights, fPortDev, b_error) = OptPort( df_rets.values, fMax, naLower, naUpper, naExpected, direction)
        allocations = _create_dict(df_rets, naPortWeights)
        return {'allocations': allocations, 'std_dev': fPortDev, 'expected_return': fMax, 'error': b_error}

    fStep = (fMax - fMin) / 50.0

    lfReturn =  [fMin + x * fStep for x in range(51)]
    lfStd = []
    lnaPortfolios = []
    
    for fTarget in lfReturn: 
        (naWeights, fStd, b_error) = OptPort( df_rets.values, fTarget, naLower, naUpper, naExpected, direction)
        if b_error == False:
            lfStd.append(fStd)
            lnaPortfolios.append( naWeights )
        else:
            # Return error on ANY failed optimization
            allocations = _create_dict(df_rets, np.zeros(df_rets.shape[1]))
            return {'allocations': allocations, 'std_dev': 0.0, 
                    'expected_return': fMax, 'error': True}

    if len(lfStd) == 0:
        (naPortWeights, fPortDev, b_error) = OptPort( df_rets.values, fMax, naLower, naUpper, naExpected, direction)
        allocations = _create_dict(df_rets, naPortWeights)
        return {'allocations': allocations, 'std_dev': fPortDev, 'expected_return': fMax, 'error': True}

    f_return = lfReturn[lfStd.index(min(lfStd))]

    if target_risk == 0:
        naPortWeights=lnaPortfolios[lfStd.index(min(lfStd))]    
        allocations = _create_dict(df_rets, naPortWeights)
        return {'allocations': allocations, 'std_dev': min(lfStd), 'expected_return': f_return, 'error': False}

    # If target_risk = 0.5, then return the one with maximum sharpe
    if target_risk == 0.5:
        lf_return_new = np.array(lfReturn)
        lf_std_new = np.array(lfStd)
        lf_std_new = lf_std_new[lf_return_new >= f_return]
        lf_return_new = lf_return_new[lf_return_new >= f_return]
        na_sharpe = lf_return_new / lf_std_new

        i_index_max_sharpe, = np.where(na_sharpe == max(na_sharpe))
        i_index_max_sharpe = i_index_max_sharpe[0]
        fTarget = lf_return_new[i_index_max_sharpe]
        (naPortWeights, fPortDev, b_error) = OptPort(df_rets.values, fTarget, naLower, naUpper, naExpected, direction)
        allocations = _create_dict(df_rets, naPortWeights)
        return {'allocations': allocations, 'std_dev': fPortDev, 'expected_return': fTarget, 'error': b_error}

    # Otherwise try to hit custom target between 0-1 min-max return
    fTarget = f_return + ((fMax - f_return) * target_risk)

    (naPortWeights, fPortDev, b_error) = OptPort( df_rets.values, fTarget, naLower, naUpper, naExpected, direction)
    allocations = _create_dict(df_rets, naPortWeights)
    return {'allocations': allocations, 'std_dev': fPortDev, 'expected_return': fTarget, 'error': b_error}
    

def getFrontier( rets, lRes=100, fUpper=0.2, fLower=0.00):
    """
    @summary Generates an efficient frontier based on average returns.
    @param rets: Array of returns to use
    @param lRes: Resolution of the curve, default=100
    @param fUpper: Upper bound on portfolio percentage
    @param fLower: Lower bound on portfolio percentage
    @return tuple containing (lf_ret, lfStd, lnaPortfolios)
            lf_ret: List of returns provided by each point
            lfStd: list of standard deviations provided by each point
            lnaPortfolios: list of numpy arrays containing weights for each portfolio
    """    
    
    # Limit/enforce percent participation """
    naUpper = np.ones(rets.shape[1]) * fUpper
    naLower = np.ones(rets.shape[1]) * fLower
    
    (fMin, fMax) = getRetRange( rets, naLower, naUpper )
    
    # Try to avoid intractible endpoints due to rounding errors """
    fMin *= 1.0000001 
    fMax *= 0.9999999

    # Calculate target returns from min and max """
    lf_ret = []
    for i in range(lRes):
        lf_ret.append( (fMax - fMin) * i / (lRes - 1) + fMin )
    
    
    lfStd = []
    lnaPortfolios = []
    
    # Call the function lRes times for the given range, use 1 for period """
    for f_target in lf_ret: 
        (naWeights, fStd) = getOptPort( rets, f_target, 1, \
                               naUpper=naUpper, naLower=naLower )
        lfStd.append(fStd)
        lnaPortfolios.append( naWeights )
    
    # plot frontier """
    #plt.plot( lfStd, lf_ret )
    plt.plot( np.std( rets, axis=0 ), np.average( rets, axis=0 ), \
                                                  'g+', markersize=10 ) 
    #plt.show()"""
    
    return (lf_ret, lfStd, lnaPortfolios)

        
def stockFilter( dmPrice, dmVolume, fNonNan=0.95, fPriceVolume=100*1000 ):
    """
    @summary Returns the list of stocks filtered based on various criteria.
    @param dmPrice: DataMatrix of stock prices
    @param dmVolume: DataMatrix of stock volumes
    @param fNonNan: Optional non-nan percent, default is .95
    @param fPriceVolume: Optional price*volume, default is 100,000
    @return list of stocks which meet the criteria
    """
    
    lsRetStocks = list( dmPrice.columns )

    for sStock in dmPrice.columns:
        fValid = 0.0
        print sStock
        # loop through all dates """
        for dtDate in dmPrice.index:
            # Count null (nan/inf/etc) values """
            fPrice = dmPrice[sStock][dtDate]
            if( not isnull(fPrice) ):
                fValid = fValid + 1
                # else test price volume """
                fVol = dmVolume[sStock][dtDate]
                if( not isnull(fVol) and fVol * fPrice < fPriceVolume ):
                    lsRetStocks.remove( sStock )
                    break

        # Remove if too many nan values """
        if( fValid / len(dmPrice.index) < fNonNan and sStock in lsRetStocks ):
            lsRetStocks.remove( sStock )

    return lsRetStocks


def getRandPort( lNum, dtStart=None, dtEnd=None, lsStocks=None,\
 dmPrice=None, dmVolume=None, bFilter=True, fNonNan=0.95,\
 fPriceVolume=100*1000, lSeed=None ):
    """
    @summary Returns a random portfolio based on certain criteria.
    @param lNum: Number of stocks to be included
    @param dtStart: Start date for portfolio
    @param dtEnd: End date for portfolio
    @param lsStocks: Optional list of ticker symbols, if not provided all symbols will be used
    @param bFilter: If False, stocks are not filtered by price or volume data, simply return random Portfolio.
    @param dmPrice: Optional price data, if not provided, data access will be queried
    @param dmVolume: Optional volume data, if not provided, data access will be queried
    @param fNonNan: Optional non-nan percent for filter, default is .95
    @param fPriceVolume: Optional price*volume for filter, default is 100,000
    @warning: Does not work for all sets of optional inputs, e.g. if you don't include dtStart, dtEnd, you need 
              to include dmPrice/dmVolume
    @return list of stocks which meet the criteria
    """
    
    if( lsStocks is None ):
        if( dmPrice is None and dmVolume is None ):
            norObj = da.DataAccess('Norgate') 
            lsStocks = norObj.get_all_symbols()
        elif( not dmPrice is None ):
            lsStocks = list(dmPrice.columns)
        else:
            lsStocks = list(dmVolume.columns)
    
    if( dmPrice is None and dmVolume is None and bFilter == True ):
        norObj = da.DataAccess('Norgate')  
        ldtTimestamps = du.getNYSEdays( dtStart, dtEnd, dt.timedelta(hours=16) )

    # if dmPrice and dmVol are provided then we don't query it every time """
    bPullPrice = False
    bPullVol = False
    if( dmPrice is None ):
        bPullPrice = True
    if( dmVolume is None ):
        bPullVol = True
            
    # Default seed (none) uses system clock """    
    rand.seed(lSeed)     
    lsRetStocks = []

    # Loop until we have enough randomly selected stocks """
    llRemainingIndexes = range(0,len(lsStocks))
    lsValid = None
    while( len(lsRetStocks) != lNum ):

        lsCheckStocks = []
        for i in range( lNum - len(lsRetStocks) ):
            lRemaining = len(llRemainingIndexes)
            if( lRemaining == 0 ):
                print 'Error in getRandPort: ran out of stocks'
                return lsRetStocks
            
            # Pick a stock and remove it from the list of remaining stocks """
            lPicked =  rand.randint(0, lRemaining-1)
            lsCheckStocks.append( lsStocks[ llRemainingIndexes.pop(lPicked) ] )

        # If bFilter is false"""
        # simply return our first list of stocks, don't check prive/vol """
        if( not bFilter ):
            return sorted(lsCheckStocks)
            

        # Get data if needed """
        if( bPullPrice ):
            dmPrice = norObj.get_data( ldtTimestamps, lsCheckStocks, 'close' )

        # Get data if needed """
        if( bPullVol ):
            dmVolume = norObj.get_data(ldtTimestamps, lsCheckStocks, 'volume' )

        # Only query this once if data is provided"""
        # else query every time with new data """
        if( lsValid is None or bPullVol or bPullPrice ):
            lsValid = stockFilter(dmPrice, dmVolume, fNonNan, fPriceVolume)
        
        for sAdd in lsValid:
            if sAdd in lsCheckStocks:
                lsRetStocks.append( sAdd )

    return sorted(lsRetStocks)

########NEW FILE########
__FILENAME__ = utils
'''
(c) 2011, 2012 Georgia Tech Research Corporation
This source code is released under the New BSD license. Please see
http://wiki.quantsoftware.org/index.php?title=QSTK_License
for license details.

Created on Apr 22, 2011

@author: shreyas
@contact: dbratcher@gatech.edu
@summary: This is intended to be a collection of helper routines used by different QSTK modules
'''


import dircache
import os

def clean_paths (paths_to_clean):
 '''
 @summary: Removes any previous files in the list of paths.
 '''

 if (type(paths_to_clean) is str):
    temp= paths_to_clean
    paths_to_clean= list()
    paths_to_clean.append(temp)
    #endif
    
 
 for path in paths_to_clean:
    files_at_this_path = dircache.listdir(str(path))
    for _file in files_at_this_path:
        if (os.path.isfile(path + _file)):
            os.remove(path + _file)
            #if ends 
    #for ends
 #outer for ends   
    
#clean_output_paths  ends

########NEW FILE########
