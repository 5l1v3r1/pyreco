


## 

<ul class="incremental" style="list-style: none" >
<li>

```r
    input = to.dfs(1:input.size)
```

<li>

```r
    from.dfs(input)
```

</ul>

## 

```r
    mapreduce(
      input, 
      map = function(k,v) keyval(k,v))
```

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  predicate = 
    function(.,v) unlist(v)%%2 == 0
```

<li> 

```r
    mapreduce(
      input, 
      map = 
        function(k,v) {
          filter = predicate(k,v); 
          keyval(k[filter], v[filter])})
```

</ul>
## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  input.select = 
    to.dfs(
      data.frame(
        a = rnorm(input.size),
        b = 1:input.size,
        c = sample(as.character(1:10),
                   input.size, 
                   replace=TRUE)))
```

<li>

```r
    mapreduce(input.select,
              map = function(.,v) v$b)
```

</ul>
## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  set.seed(0)
  big.sample = rnorm(input.size)
  input.bigsum = to.dfs(big.sample)
```

<li>

```r
    mapreduce(
      input.bigsum, 
      map  = 
        function(.,v) keyval(1, sum(v)), 
      reduce = 
        function(., v) keyval(1, sum(v)),
      combine = TRUE)
```

</ul>
## 

```r
  input.ga = 
    to.dfs(
      keyval(
        1:input.size,
        rnorm(input.size)))
```

## 

```r
  group = function(k,v) k%%100
  aggregate = function(x) sum(x)
```

## 

```r
    mapreduce(
      input.ga, 
      map = 
        function(k,v) 
          keyval(group(k,v), v),
      reduce = 
        function(k, vv) 
          keyval(k, aggregate(vv)),
      combine = TRUE)
```


# Compatibility testing for rmr 3.1.1
Please contribute with additional reports. To claim compatibility you need to run `R CMD  check path-to-rmr` successfully.
If you build your own Hadoop, see [Which Hadoop for rmr](https://github.com/RevolutionAnalytics/RHadoop/wiki/Which-Hadoop-for-rmr).
If you are interested in the compatibility chart for other releases, choose one from the drop down menu on the top left, under tags and find this document again (under docs). Not every release gets a complete round of testing, so typically a bug fix release (change in the third number only) is equally or more compatible than the previous release, even if we don't have the resources to test it directly. 

<table>
<thead>
<tr><th>Hadoop</th><th>R</th><th>OS</th><th>Notes</th><th>Reporter</th></tr>
</thead>
<tbody>
<tr><td>CDH4.6</td><td>R 3.0.2 (Revolution R 7.0)</td><td>CentOS 6.3</td><td>mr2</td><td><a href=mailto:rhadoop@revolutionanalytics.com>Revolution</a></td></tr>
</tbody>
</table>








* This document responds to several inquiries on data formats and how to get data in and out of the rmr system
* Still more a collection of snippets than anything organized
* Thanks Damien  and @ryangarner for the examples and Koert for conversations on the subject

Internally `rmr` uses R's own serialization in most cases and its own typedbytes extension for some atomic vectors. The goal is to make you forget about representation issues most of the time. But what happens at the boundary of the
system, when you need to get non-rmr data in and out of it? Of course `rmr` has to be able to read and write a variety of formats to be of any use. This is what is available and how to extend it.

## Built in formats

The complete list is:

```
[1] "text"                "json"                "csv"                
[4] "native"              "sequence.typedbytes" "hbase"              
[7] "pig.hive"           
```


1. `text`: for english text. key is `NULL` and value is a string, one per line. Please don't use it for anything else.
1. `json`-ish: it is actually `<JSON\tJSON\n>` so that streaming can tell key and value. This implies you have to escape all newlines and tabs in the JSON part. Your data may not be in this form, but almost any
language has decent JSON libraries. It was the default in `rmr` 1.0, but we'll keep because it is almost standard. Parsed in C for efficiency, should handle large objects.
1. `csv`: A family of concrete formats modeled after R's own `read.table`. See examples below.
1. `native`: based on R's own serialization, it is the default and supports everything that R's `serialize` supports. If you want to know the gory details, it is implemented as an application specific type for the typedbytes format, which is further encapsulated in the sequence file format when writing to HDFS, which ... Dont't worry about it, it just works. Unfortunately, it is written and read by only one package, `rmr` itself.
1. `sequence.typedbytes`: based on specs in HADOOP-1722 it has emerged as the standard for non Java hadoop application talking to the rest of Hadoop. Also implemented in C for efficiency, its underlying data model is different from R's and we tried to map the two systems the best we could.
1. `hbase`: read directly from an Hbase table. Experimental and input only for now. Experimental means that testing has not been as thourough as it should and that the feature could be withdrawn in a minor release.

## The easy way

Specify one of those format strings directly as arguments to `mapreduce`, `from.dfs`, `to.dfs`.

```
mapreduce(input, input.format = "json")
```

## Custom formats light
Use `make.input.format` with a string format argument and additional arguments to specify some variants to that format. Typical example is `csv` which is actually a family of character separated formats with lots of variation in the details. In this case you can call something like
```
mapreduce(input, input.format = make.input.format("csv", sep = "\t"))
```
which says to use a CSV format with a tab separator. For this format the arguments are, with few exceptions, the same as `read.table`. The same is true on the output side with `make.output.format` and the model for the additional arguments is `write.table`.

## Custom formats 
A format is a triple. You can create one with `make.input.format`, for instance:

```r
make.input.format("csv")
```

```
$mode
[1] "text"

$format
function (con, keyval.length) 
{
    df = tryCatch(read.table(file = con, nrows = keyval.length, 
        header = FALSE, ...), error = function(e) {
        if (e$message != "no lines available in input") 
            stop(e$message)
        NULL
    })
    if (is.null(df) || dim(df)[[1]] == 0) 
        NULL
    else keyval(NULL, df)
}
<bytecode: 0x104b20b20>
<environment: 0x104b227f0>

$streaming.format
NULL

$backend.parameters
NULL
```


The `mode` element can be `text` or `binary`. The `format` element is a function that takes a connection, reads `nrows` records and creates a key-value object. The `streaming.format` element is a fully qualified Java class (as a string) that writes to the connection the format function reads from. The default is `TextInputFormat` and also useful is `org.apache.hadoop.streaming.AutoInputFormat`. Once you have these three elements you can pass them to `make.input.format` and get something out that can be used as the `input.format` option to `mapreduce` and the `format`  option to `from.dfs`. On the output side the situation is reversed with the R function acting first and then the Java class doing its thing.


```r
make.output.format("csv")
```

```
$mode
[1] "text"

$format
function (kv, con) 
{
    kv = recycle.keyval(kv)
    k = keys(kv)
    v = values(kv)
    write.table(file = con, x = if (is.null(k)) 
        v
    else cbind(k, v), ..., row.names = FALSE, col.names = FALSE)
}
<bytecode: 0x1049cb118>
<environment: 0x1049cb720>

$streaming.format
NULL

$backend.parameters
NULL
```


R data types natively work without additional effort.


```r
my.data = list(TRUE, list("nested list", 7.2), seq(1:3), letters[1:4], matrix(1:25, nrow = 5,ncol = 5))
```


Put into HDFS:

```r
hdfs.data = to.dfs(my.data)
```

`my.data` needs to be one of: vector, data frame, list or matrix. 
Compute a frequency of object lengths.  Only require input, mapper, and reducer. Note that `my.data` is passed into the mapper, record by
record, as `key = NULL, value = subrange`. 


```r
result = mapreduce(
  input = hdfs.data,
  map = function(k, v) keyval(lapply(v, length), 1),
  reduce = function(k, vv) keyval(k, sum(vv)))

from.dfs(result)
```


However, if using data which was not generated with `rmr` (txt, csv, tsv, JSON, log files, etc) it is necessary to specify an input format. 





To define your own `input.format` (e.g. to handle tsv):



```r
tsv.reader = function(con, nrecs){
  lines = readLines(con, 1)
  if(length(lines) == 0)
    NULL
  else {
    delim = strsplit(lines, split = "\t")
    keyval(
      sapply(delim, 
             function(x) x[1]), 
      sapply(delim, 
             function(x) x[-1]))}} 
## first column is the key, note that column indexes moved by 1
```


Frequency count on input column two of the tsv data, data comes into map already delimited


```r
freq.counts = 
  mapreduce(
    input = tsv.data,
    input.format = tsv.format,
    map = function(k, v) keyval(v[1,], 1),
    reduce = function(k, vv) keyval(k, sum(vv)))
```


Or if you want named columns, this would be specific to your data file


```r
tsv.reader = 
  function(con, nrecs){
    lines = readLines(con, 1)
    if(length(lines) == 0)
      NULL
    else {
      delim = strsplit(lines, split = "\t")
      keyval(
        sapply(delim, function(x) x[1]), 
        data.frame(
          location = sapply(delim, function(x) x[2]),
          name = sapply(delim, function(x) x[3]),
          value = sapply(delim, function(x) x[4])))}}
```


You can then use the list names to directly access your column of interest for manipulations

```r
freq.counts = 
  mapreduce(
    input = tsv.data,
    input.format = tsv.format,
    map = 
      function(k, v) { 
        filter = (v$name == "blarg")
        keyval(k[filter], log(as.numeric(v$value[filter])))},
    reduce = function(k, vv) keyval(k, mean(vv)))                      
```


Another common `input.format` is fixed width formatted data:

```r
fwf.reader <- function(con, nrecs) {  
  lines <- readLines(con, nrecs)  
  if (length(lines) == 0) {
    NULL}
  else {
    split.lines = unlist(strsplit(lines, ""))
    df = 
      as.data.frame(
        matrix(
          sapply(
            split(
              split.lines, 
              ceiling(1:length(split.lines)/field.size)), 
            paste, collapse = ""), 
          ncol = length(fields), byrow = T))
    names(df) = fields
    keyval(NULL, df)}} 
fwf.input.format = make.input.format(mode = "text", format = fwf.reader)
```


Using the text `output.format` as a template, we modify it slightly to write fixed width data without tab seperation:

```r
fwf.writer <- function(kv, con, keyval.size) {
  ser = 
    function(df) 
      paste(
          apply(
            df,
            1, 
            function(x) 
              paste(
                format(
                  x, 
                  width = field.size), 
                collapse = "")), 
          collapse = "\n")
  out = ser(values(kv))
  writeLines(out, con = con)}
fwf.output.format = make.output.format(mode = "text", format = fwf.writer)
```


Writing the `mtcars` dataset to a fixed width file with column widths of 6 bytes and putting into hdfs:

```r
fwf.data <- to.dfs(mtcars, format = fwf.output.format)
```


The key thing to note about `fwf.reader` is the global variable `fields`. In `fields`, we define the start and
end byte locations for each field in the data:

```r
fields <- rmr2:::qw(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) 
field.size = 8
```


Sending 1 line at a time to the map function:

```r
out <- from.dfs(mapreduce(input = fwf.data,
                          input.format = fwf.input.format))
out$val
```


Frequency count on `cyl`:

```r
out <- from.dfs(mapreduce(input = fwf.data,
                          input.format = fwf.input.format,
                          map = function(key, value) keyval(value[,"cyl"], 1),
                          reduce = function(key, value) keyval(key, sum(unlist(value))),
                          combine = TRUE))
df <- data.frame(out$key, out$val)
names(df) <- c("cyl","count")
df
```



To get your data out - say you input file, apply column transformations, add columns, and want to output a new csv file
Just like input.format, one must define an output format function:


```r
csv.writer = function(kv, con){
  cat(
    paste(
      apply(cbind(1:32, mtcars), 
            1, 
            paste, collapse = ","), 
      collapse = "\n"),
    file = con)}
```


And then use that as an argument to `make.output.format`, but why sweat it since the devs have already done the work?


```r
csv.format = make.output.format("csv", sep = ",")
```


This time providing output argument so one can extract from hdfs (cannot hdfs.get from a Rhadoop big data object)


```r
mapreduce(
  input = hdfs.data,
  output = tempfile(),
  output.format = csv.format,
  map = function(k, v){
    # complicated function here
    keyval(1, v)},
  reduce = function(k, vv) {
    #complicated function here
    keyval(k, vv[[1]])})
```


### HBase

Reading from an HBase table (an experimental feature) requires specifying yet another format. We rely on the system to be alredy configured to run java MR jobs on HBase tables, see `help(make.input.format)` for a reference. As you can see in the following, non self-contained snippet, one specifies the name of the table as a the input argument to mapreduce (this is not strictly part of the input format but it seemed a good moment to explain it). Then the `input.format` argument is created with a, as usual `make.input.format` with a first argument equal to `"hbase"`. The second argument, `family.colums`, allows  to select certain columns within certain families of columns. It is represented as a  list of lists, with the labels in the outer lists being the name of the families and the element of the lists being the column names. Then we have two deserialiation-related arguments, one for the keys and one for the cell contents. HBase is agnostic to cell content nature. Lacking a source of metadata or a default encoding, keys and cells are passed to R as raw bytes and the user has to fill in the missing information. We support three deserialization formats: "raw" means that the raw bytes are converted to characters, "native" is the built-in R serialization format and "typedbytes" is a format shared with other Hadoop libraries. It's unlikely that these three options are going to cover all applications, therefore both arguments accept also functions for maximum flexibility. For the keys, the function should take a list of raw vectors, deserialize them and return a list with the deserilizaed object. For the cells, the idea is the same but two additional arguments, which will contain the familiy and column being deserialized, allow for additional flexibility. The last two arguments affect the "shape" of the data frame created with the cell contents and the specific types of the columns. `dense` means that a data frame column will be created for each selected HBase column; missing cells will take the value `NA`. When `dense` is `FALSE` the data frame passed to the map function will have always 4 columns, one for the key, one for the family, one for the column and one for the cell contents. That way we can handle the case of sparse HBase tables that can have even millions of columns. Finally `atomic` means that columns can be `unlist`-ed before being added to the data frame, otherwise they are wrapped in a `I` call and added as they are, to support complex cell types.



```r
from.dfs(
  mapreduce(
    input="blogposts", 
    input.format = 
      make.input.format(
        "hbase", 
        family.columns = 
          list(
            image= list("bodyimage"), 
            post = list("author", "body")), 
        key.deserialize = "raw", 
        cell.deserialize = "raw", 
        dense = T, 
        atomic = T)))
```


This is another example run on a freebase data dump loaded into HBase, we have a family named "name" with a single, unnamed column (I suspect this is not recommended, but it works) and another named freebase with a column named "types", which is itself a comma separated list. One could think of a better deserialization that also parses this into a character vector, one element per type, but it's not what is done here.


```r
freebase.input.format = 
  make.input.format(
    "hbase", 
    family.columns = 
      list(
        name = "", 
        freebase = "types"), 
    key.deserialize = "raw", 
    cell.deserialize = "raw", 
    dense = F, 
    atomic = F)
```


The input format thus defined can be used as any other inputp format, on a table called "freebase"


```r
from.dfs(
  mapreduce(
    input = "freebase",
    input.format = freebase.input.format,
    map = function(k,v) keyval(k[1,], v[1,])))
```


Knit document for some timing results:


```r
zz = rmr2:::interleave(1:10^6, 1:10^6)
con = file("/tmp/n-test", "wb")
system.time({
    rmr2:::typedbytes.writer(zz, con, TRUE)
})
```

```
##    user  system elapsed 
##   0.582   0.033   0.615
```

```r
close(con)
con = file("/tmp/tb-test", "wb")
system.time({
    rmr2:::typedbytes.writer(zz, con, FALSE)
})
```

```
##    user  system elapsed 
##   0.295   0.023   0.317
```

```r
close(con)
system.time({
    save(zz, file = "/tmp/save-test")
})
```

```
##    user  system elapsed 
##   2.365   0.022   2.390
```

```r
system.time({
    rmr2:::make.typedbytes.input.format()(file("/tmp/n-test", "rb"), 10^6)
})
```

```
##    user  system elapsed 
##   9.229   0.374   9.603
```

```r
system.time({
    rmr2:::make.typedbytes.input.format()(file("/tmp/tb-test", "rb"), 10^6)
})
```

```
## Warning: closing unused connection 4 (/tmp/n-test)
```

```
##    user  system elapsed 
##   7.387   0.328   7.716
```

```r
system.time({
    load(file = "/tmp/save-test")
})
```

```
## Warning: closing unused connection 4 (/tmp/tb-test)
```

```
##    user  system elapsed 
##   0.652   0.001   0.653
```


# What's new in 3.1.1

## Bugs Fixed

* support for `Date` columns in data frames
* support for logical NAs
* uses Imports when possible

## Enhancement

* drop use of deprecated `-file` streaming option
* switch to the new and improved quickcheck 2.0.0 for testing










## R and Hadoop
### Revolution Analytics
##### Antonio Piccolboni


# <img src="../resources/hadoop-logo.gif">

<details>
operating system of the cloud --
focus is scalability --
different from HPC --
storage, fault tolerance built in, programming model
</details>

# <img src="../resources/R.png">

## <img src="https://r4stats.files.wordpress.com/2012/04/fig_10_cran.png" width=75%>

<details>[r4stats](http://r4stats.com/popularity) --
Google from niche to main language --
oreilly hot language --
SAS and mathematica compatibility
</details>

## <img src="../resources/revo-home.png" width=75%>

## <img src="../resources/rhadoop.png" width=50%>

<details>
hadoop brings horizontal scalability --
r sophisticated analytics --
combination could be powerful  
</details>

##

* rhdfs
* rhbase
* <em>rmr2</em>



## <img src = "../resources/Mapreduce.png">


##


```r
    library(rmr2)
  
    mapreduce(input, ...)
```

<details>
why RHadoop for the R dev --
The data is in Hadoop --
The biggest cluster is Hadoop
Just a library
Not a special run-time
Not a different language
Not a special purpose language
Incrementally port your code
Use all packages
</details>

##

```r
    sapply(data, function)

    mapreduce(big.data, map = function)
```

<details>
Very R-like, building on the functional characteristics of R
Upholds scope rules
</details>


## 
<table width=75% align=center>
<thead>
<th>Direct MR</th><th>Indirect MR</th>
</thead> 
<tr><td>&nbsp;</td></tr>
<tr>
<td></td><td><em>Hive, Pig</em></td>
</tr>
<tr><td>&nbsp;</td></tr>
<tr>
<td><strong>Rmr</strong>, Rhipe, Dumbo, Pydoop, Hadoopy</td><td>Cascalog, Scalding, Scrunch</td>
</tr>
<tr><td>&nbsp;</td></tr>
<tr> 
<td>Java, C++</td><td>Cascading, Crunch</td>
</tr>
</table>

<details>
Much simpler than writing Java
Not as simple as Hive, Pig at what they do, but more general, a real language
Great for prototyping, can transition to production -- optimize instead of rewriting! Lower risk, always executable.
</details>

##

```{.python style="font-size:12px"}
#!/usr/bin/python
import sys
from math import fabs
from org.apache.pig.scripting import Pig

filename = "student.txt"
k = 4
tolerance = 0.01

MAX_SCORE = 4
MIN_SCORE = 0
MAX_ITERATION = 100

# initial centroid, equally divide the space
initial_centroids = ""
last_centroids = [None] * k
for i in range(k):
  last_centroids[i] = MIN_SCORE + float(i)/k*(MAX_SCORE-MIN_SCORE)
  initial_centroids = initial_centroids + str(last_centroids[i])
  if i!=k-1:
    initial_centroids = initial_centroids + ":"

P = Pig.compile("""register udf.jar
          DEFINE find_centroid FindCentroid('$centroids');
          raw = load 'student.txt' as (name:chararray, age:int, gpa:double);
          centroided = foreach raw generate gpa, find_centroid(gpa) as centroid;
          grouped = group centroided by centroid;
          result = foreach grouped generate group, AVG(centroided.gpa);
          store result into 'output';
        """)

converged = False
iter_num = 0
while iter_num<MAX_ITERATION:
  Q = P.bind({'centroids':initial_centroids})
  results = Q.runSingle()
```


##

```{.python style="font-size:12px"}
  if results.isSuccessful() == "FAILED":
    raise "Pig job failed"
  iter = results.result("result").iterator()
  centroids = [None] * k
  distance_move = 0
  # get new centroid of this iteration, caculate the moving distance with last iteration
  for i in range(k):
    tuple = iter.next()
    centroids[i] = float(str(tuple.get(1)))
    distance_move = distance_move + fabs(last_centroids[i]-centroids[i])
  distance_move = distance_move / k;
  Pig.fs("rmr output")
  print("iteration " + str(iter_num))
  print("average distance moved: " + str(distance_move))
  if distance_move<tolerance:
    sys.stdout.write("k-means converged at centroids: [")
    sys.stdout.write(",".join(str(v) for v in centroids))
    sys.stdout.write("]\n")
    converged = True
    break
  last_centroids = centroids[:]
  initial_centroids = ""
  for i in range(k):
    initial_centroids = initial_centroids + str(last_centroids[i])
    if i!=k-1:
      initial_centroids = initial_centroids + ":"
  iter_num += 1

if not converged:
  print("not converge after " + str(iter_num) + " iterations")
  sys.stdout.write("last centroids: [")
  sys.stdout.write(",".join(str(v) for v in last_centroids))
  sys.stdout.write("]\n")
```

##

```{.java style="font-size:12px"}
import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;


public class FindCentroid extends EvalFunc<Double> {
  double[] centroids;
  public FindCentroid(String initialCentroid) {
    String[] centroidStrings = initialCentroid.split(":");
    centroids = new double[centroidStrings.length];
    for (int i=0;i<centroidStrings.length;i++)
      centroids[i] = Double.parseDouble(centroidStrings[i]);
  }
  @Override
  public Double exec(Tuple input) throws IOException {
    double min_distance = Double.MAX_VALUE;
    double closest_centroid = 0;
    for (double centroid : centroids) {
      double distance = Math.abs(centroid - (Double)input.get(0));
      if (distance < min_distance) {
        min_distance = distance;
        closest_centroid = centroid;
      }
    }
    return closest_centroid;
  }

}
```

# A brief history of rmr


# Read and Write

## 

<ul class="incremental" style="list-style: none" >
<li>

```r
  input = to.dfs(1:input.size)
```

<li>

```r
  from.dfs(input)
```

</ul>

# Identity

## 

```r
  mapreduce(
    input, 
    map = function(k, v) keyval(k, v))
```


# Filter

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  predicate = 
    function(., v) v%%2 == 0
```

<li> 

```r
  mapreduce(
    input, 
    map = 
      function(k, v) {
        filter = predicate(k, v)
        keyval(k[filter], v[filter])}
```

</ul>

# Select

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  input.select = 
    to.dfs(
      data.frame(
        a = rnorm(input.size),
        b = 1:input.size,
        c = sample(as.character(1:10),
                   input.size, 
                   replace=TRUE)))
```

<li>

```r
  mapreduce(input.select,
            map = function(., v) v$b)
```

</ul>

# Sum

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  set.seed(0)
  big.sample = rnorm(input.size)
  input.bigsum = to.dfs(big.sample)
```

<li>

```r
  mapreduce(
    input.bigsum, 
    map  = 
      function(., v) keyval(1, sum(v)), 
    reduce = 
      function(., v) keyval(1, sum(v)),
    combine = TRUE)
```

</ul>

# Group and Aggregate

## 


```r
  input.ga = 
    to.dfs(
      cbind(
        1:input.size,
        rnorm(input.size)))
```


## 


```r
  group = function(x) x%%10
  aggregate = function(x) sum(x)
```


## 


```r
  mapreduce(
    input.ga, 
      map = 
        function(k, v) 
          keyval(group(v[,1]), v[,2]),
      reduce = 
        function(k, vv) 
          keyval(k, aggregate(vv)),
      combine = TRUE)
```


# Live demo

```
shrink = function(ct) {
  ct = ddply(ct,c("x", "y"), .fun= function(x) c(Freq = sum(x$Freq)))
  ct[ct$Freq > 0, ]}
```

```
from.dfs(mapreduce(DS.big, map = function(k,v) keyval(1,as.data.frame(table(v))), reduce = function(k,vv) keyval(1, shrink(vv)), combine=T))
```

# K-means

##


```r
    dist.fun = 
      function(C, P) {
        apply(
          C,
          1, 
          function(x) 
            colSums((t(P) - x)^2))}
```

 
##

```r
    kmeans.map = 
      function(., P) {
        nearest = {
          if(is.null(C)) 
            sample(
              1:num.clusters, 
              nrow(P), 
              replace = T)
          else {
            D = dist.fun(C, P)
            nearest = max.col(-D)}}
        if(!(combine || in.memory.combine))
          keyval(nearest, P) 
        else 
          keyval(nearest, cbind(1, P))}
```


##

```r
    kmeans.reduce = {
      if (!(combine || in.memory.combine) ) 
        function(., P) 
          t(as.matrix(apply(P, 2, mean)))
      else 
        function(k, P) 
          keyval(
            k, 
            t(as.matrix(apply(P, 2, sum))))}
```


##

```r
kmeans.mr = 
  function(
    P, 
    num.clusters, 
    num.iter, 
    combine, 
    in.memory.combine) {
```

##

```r
    C = NULL
    for(i in 1:num.iter ) {
      C = 
        values(
          from.dfs(
            mapreduce(
              P, 
              map = kmeans.map,
              reduce = kmeans.reduce)))
      if(combine || in.memory.combine)
        C = C[, -1]/C[, 1]
```

##

```r
      if(nrow(C) < num.clusters) {
        C = 
          rbind(
            C,
            matrix(
              rnorm(
                (num.clusters - 
                   nrow(C)) * nrow(C)), 
              ncol = nrow(C)) %*% C) }}
        C}
```

##


```r
  P = 
    do.call(
      rbind, 
      rep(
        list(
          matrix(
            rnorm(10, sd = 10), 
            ncol=2)), 
        20)) + 
    matrix(rnorm(200), ncol =2)
```


##


```r
    kmeans.mr(
      to.dfs(P),
      num.clusters = 12, 
      num.iter = 5,
      combine = FALSE,
      in.memory.combine = FALSE)
```


<details>
Other features: easy composition of jobs, joins, local modes, combine 
</details>

##

#### Revolution Analytics
### rhadoop@revolutionanalytics.com
#### Antonio Piccolboni
### antonio@piccolboni.info

<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title></title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>





</head>

<body>
<p>Each document is present in three formats. Markdown, extension <strong>.md, is the one you want to click on</strong>. R Markdown, extension Rmd, is the original format, see the package <code>knitr</code> for details, and is the only one that should be edited. html is not used at this time.</p>

</body>

</html>


Each document is present in three formats. Markdown, extension **.md, is the one you want to click on**. R Markdown, extension Rmd, is the original format, see the package `knitr` for details, and is the only one that should be edited. html is not used at this time.

Each document is present in three formats. Markdown, extension **.md, is the one you want to click on**. R Markdown, extension Rmd, is the original format, see the package `knitr` for details, and is the only one that should be edited. html is not used at this time.










## Scalable Analytics in R with rmr
### Revolution Analytics
##### Antonio Piccolboni


# RHadoop

# <img src="../resources/hadoop-logo.gif">

## <img src = "../resources/Mapreduce.png">

<details>
operating system of the cloud --
focus is scalability --
different from HPC --
storage, fault tolerance built in, programming model
</details>

# <img src="../resources/R.png">

## <img src="https://r4stats.files.wordpress.com/2012/04/fig_10_cran.png" width=75%>

<details>[r4stats](http://r4stats.com/popularity) --
Google from niche to main language --
oreilly hot language --
SAS and mathematica compatibility
</details>

## <img src="../resources/revo-home.png" width=75%>

## <img src="../resources/rhadoop.png" width=50%>

<details>
hadoop brings horizontal scalability --
r sophisticated analytics --
combination could be powerful  
</details>

##

* rhdfs
* rhbase
* <em>rmr2</em>

<details>
why RHadoop for the R dev --
The data is in Hadoop --
The biggest cluster is Hadoop
</details>

##

```r
    library(rmr2)
  
    mapreduce(input, ...)
```

<details>
Just a library
Not a special run-time
Not a different language
Not a special purpose language
Incrementally port your code
Use all packages
</details>

##

```r
    sapply(data, function)

    mapreduce(big.data, map = function)
```

<details>
Very R-like, building on the functional characteristics of R
Upholds scope rules
</details>


## 
<table width=75% align=center>
<thead>
<th>Direct MR</th><th>Indirect MR</th>
</thead> 
<tr><td>&nbsp;</td></tr>
<tr>
<td></td><td><em>Hive, Pig</em></td>
</tr>
<tr><td>&nbsp;</td></tr>
<tr>
<td><strong>Rmr</strong>, Rhipe, Dumbo, Pydoop, Hadoopy</td><td>Cascalog, Scalding, Scrunch</td>
</tr>
<tr><td>&nbsp;</td></tr>
<tr> 
<td>Java, C++</td><td>Cascading, Crunch</td>
</tr>
</table>

<details>
Much simpler than writing Java
Not as simple as Hive, Pig at what they do, but more general, a real language
Great for prototyping, can transition to production -- optimize instead of rewriting! Lower risk, always executable.
</details>

##

```{.python style="font-size:12px"}
#!/usr/bin/python
import sys
from math import fabs
from org.apache.pig.scripting import Pig

filename = "student.txt"
k = 4
tolerance = 0.01

MAX_SCORE = 4
MIN_SCORE = 0
MAX_ITERATION = 100

# initial centroid, equally divide the space
initial_centroids = ""
last_centroids = [None] * k
for i in range(k):
  last_centroids[i] = MIN_SCORE + float(i)/k*(MAX_SCORE-MIN_SCORE)
  initial_centroids = initial_centroids + str(last_centroids[i])
  if i!=k-1:
    initial_centroids = initial_centroids + ":"

P = Pig.compile("""register udf.jar
          DEFINE find_centroid FindCentroid('$centroids');
          raw = load 'student.txt' as (name:chararray, age:int, gpa:double);
          centroided = foreach raw generate gpa, find_centroid(gpa) as centroid;
          grouped = group centroided by centroid;
          result = foreach grouped generate group, AVG(centroided.gpa);
          store result into 'output';
        """)

converged = False
iter_num = 0
while iter_num<MAX_ITERATION:
  Q = P.bind({'centroids':initial_centroids})
  results = Q.runSingle()
```


##

```{.python style="font-size:12px"}
  if results.isSuccessful() == "FAILED":
    raise "Pig job failed"
  iter = results.result("result").iterator()
  centroids = [None] * k
  distance_move = 0
  # get new centroid of this iteration, caculate the moving distance with last iteration
  for i in range(k):
    tuple = iter.next()
    centroids[i] = float(str(tuple.get(1)))
    distance_move = distance_move + fabs(last_centroids[i]-centroids[i])
  distance_move = distance_move / k;
  Pig.fs("rmr output")
  print("iteration " + str(iter_num))
  print("average distance moved: " + str(distance_move))
  if distance_move<tolerance:
    sys.stdout.write("k-means converged at centroids: [")
    sys.stdout.write(",".join(str(v) for v in centroids))
    sys.stdout.write("]\n")
    converged = True
    break
  last_centroids = centroids[:]
  initial_centroids = ""
  for i in range(k):
    initial_centroids = initial_centroids + str(last_centroids[i])
    if i!=k-1:
      initial_centroids = initial_centroids + ":"
  iter_num += 1

if not converged:
  print("not converge after " + str(iter_num) + " iterations")
  sys.stdout.write("last centroids: [")
  sys.stdout.write(",".join(str(v) for v in last_centroids))
  sys.stdout.write("]\n")
```

##

```{.java style="font-size:12px"}
import java.io.IOException;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;


public class FindCentroid extends EvalFunc<Double> {
  double[] centroids;
  public FindCentroid(String initialCentroid) {
    String[] centroidStrings = initialCentroid.split(":");
    centroids = new double[centroidStrings.length];
    for (int i=0;i<centroidStrings.length;i++)
      centroids[i] = Double.parseDouble(centroidStrings[i]);
  }
  @Override
  public Double exec(Tuple input) throws IOException {
    double min_distance = Double.MAX_VALUE;
    double closest_centroid = 0;
    for (double centroid : centroids) {
      double distance = Math.abs(centroid - (Double)input.get(0));
      if (distance < min_distance) {
        min_distance = distance;
        closest_centroid = centroid;
      }
    }
    return closest_centroid;
  }

}
```

# Read and Write

## 

<ul class="incremental" style="list-style: none" >
<li>

```r
  input = to.dfs(1:input.size)
```

<li>

```r
  from.dfs(input)
```

</ul>

# Identity

## 

```r
  mapreduce(
    input, 
    map = function(k, v) keyval(k, v))
```


# Filter

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  predicate = 
    function(., v) v%%2 == 0
```

<li> 

```r
  mapreduce(
    input, 
    map = 
      function(k, v) {
        filter = predicate(k, v)
        keyval(k[filter], v[filter])}
```

</ul>

# Select

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  input.select = 
    to.dfs(
      data.frame(
        a = rnorm(input.size),
        b = 1:input.size,
        c = sample(as.character(1:10),
                   input.size, 
                   replace=TRUE)))
```

<li>

```r
  mapreduce(input.select,
            map = function(., v) v$b)
```

</ul>

# Sum

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  set.seed(0)
  big.sample = rnorm(input.size)
  input.bigsum = to.dfs(big.sample)
```

<li>

```r
  mapreduce(
    input.bigsum, 
    map  = 
      function(., v) keyval(1, sum(v)), 
    reduce = 
      function(., v) keyval(1, sum(v)),
    combine = TRUE)
```

</ul>

# Group and Aggregate

## 


```r
  input.ga = 
    to.dfs(
      cbind(
        1:input.size,
        rnorm(input.size)))
```


## 


```r
  group = function(x) x%%10
  aggregate = function(x) sum(x)
```


## 


```r
  mapreduce(
    input.ga, 
      map = 
        function(k, v) 
          keyval(group(v[,1]), v[,2]),
      reduce = 
        function(k, vv) 
          keyval(k, aggregate(vv)),
      combine = TRUE)
```


# Wordcount

##

<ul class="incremental" style="list-style: none" >
<li>

```r
wordcount = 
  function(
    input, 
    output = NULL, 
    pattern = " "){
```

<li>

```r
    mapreduce(
      input = input ,
      output = output,
      input.format = "text",
      map = wc.map,
      reduce = wc.reduce,
      combine = T)}
```

</ul>

##

<ul class="incremental" style="list-style: none" >
<li>

```r
    wc.map = 
      function(., lines) {
        keyval(
          unlist(
            strsplit(
              x = lines,
              split = pattern)),
          1)}
```

<li>

```r
    wc.reduce =
      function(word, counts ) {
        keyval(word, sum(counts))}
```

</ul>

# K-means

##


```r
    dist.fun = 
      function(C, P) {
        apply(
          C,
          1, 
          function(x) 
            colSums((t(P) - x)^2))}
```

 
##

```r
    kmeans.map = 
      function(., P) {
        nearest = {
          if(is.null(C)) 
            sample(
              1:num.clusters, 
              nrow(P), 
              replace = T)
          else {
            D = dist.fun(C, P)
            nearest = max.col(-D)}}
        if(!(combine || in.mem.combine))
          keyval(nearest, P) 
        else 
          keyval(nearest, cbind(1, P))}
```


##

```r
    kmeans.reduce = {
      if (!(combine || in.mem.combine) ) 
        function(., P) 
          t(as.matrix(apply(P, 2, mean)))
      else 
        function(k, P) 
          keyval(
            k, 
            t(as.matrix(apply(P, 2, sum))))}
```


##

```r
kmeans.mr = 
  function(
    P, 
    num.clusters, 
    num.iter, 
    combine, 
    in.mem.combine) {
```

##

```r
    C = NULL
    for(i in 1:num.iter ) {
      C = 
        values(
          from.dfs(
            mapreduce(
              P, 
              map = kmeans.map,
              reduce = kmeans.reduce)))
      if(combine || in.mem.combine)
        C = C[, -1]/C[, 1]
      points(C, col = i + 1, pch = 19)
```

##

```r
      if(nrow(C) < num.clusters) {
        C = 
          rbind(
            C,
            matrix(
              rnorm(
                (num.clusters - 
                   nrow(C)) * nrow(C)), 
              ncol = nrow(C)) %*% C) }}
        C}
```

##


```r
  P = 
    do.call(
      rbind, 
      rep(
        list(
          matrix(
            rnorm(10, sd = 10), 
            ncol=2)), 
        20)) + 
    matrix(rnorm(200), ncol =2)
  x11()
  plot(P)
  points(P)
```


##


```r
    kmeans.mr(
      to.dfs(P),
      num.clusters = 12, 
      num.iter = 5,
      combine = FALSE,
      in.mem.combine = FALSE)
```


<details>
Other features: easy composition of jobs, joins, local modes, combine 
</details>

##

#### Revolution Analytics
### rhadoop@revolutionanalytics.com
#### Antonio Piccolboni
### antonio@piccolboni.info

# Scalable Analytics in R with rmr

*RHadoop* is an open source project started by Revolution Analytics to provide data scientists using R access to Hadoop’s scalability without giving up their favorite language flexibility and convenience.

So far it has three main packages:

* rhdfs provides file level manipulation for HDFS, the Hadoop file system
* rhbase provides access to HBASE, the hadoop database
* rmr allows to write mapreduce programs in R. This will be the focus of this presentation.

rmr allows R developers to program in the mapreduce framework, and to all developers provides an alternative way to implement mapreduce programs that strikes a delicate compromise betwen power and usability. It allows to write general mapreduce programs, offering the full power and ecosystem of an existing, established programming language. It doesn’t force you to replace the R interpreter with a special run-time $mdash;it is just a library. You can write logistic regression in half a page and even understand it. It feels and behaves almost like the usual R iteration and aggregation primitives. It is comprised of a handful of functions with a modest number of arguments and sensible defaults that combine in many useful ways. But there is no way to prove that an API works: one can only show examples of what it allows to do and we will do that covering a few from machine learning and statistics. Finally, we will discuss how to get involved.










## RHadoop Tutorial
### Revolution Analytics
#### Antonio Piccolboni
#### rhadoop@revolutionanalytics.com
#### antonio@piccolboni.info

#RHadoop 

##

- R + Hadoop
- OSS
- <img src="../resources/revolution.jpeg" width="20%">
- <img src="../resources/rhadoop.png" width="20%">
- rhdfs
- rhbase
- rmr2

# Mapreduce 

##

<ul class="incremental" style="list-style: none" >
<li>

```r
  small.ints = 1:1000
  sapply(small.ints, function(x) x^2)
```

<li>

```r
  small.ints = to.dfs(1:1000)
  mapreduce(
    input = small.ints, 
    map = function(k, v) cbind(v, v^2))
```

</ul>

## 

<ul class="incremental" style="list-style: none" >
<li>

```r
  groups = rbinom(32, n = 50, prob = 0.4)
  tapply(groups, groups, length)
```

<li>

```r
  groups = to.dfs(groups)
  from.dfs(
    mapreduce(
      input = groups, 
      map = function(., v) keyval(v, 1), 
      reduce = 
        function(k, vv) 
          keyval(k, length(vv))))
```

</ul>

# rmr-ABC

## 

<ul class="incremental" style="list-style: none" >
<li>

```r
  input = to.dfs(1:input.size)
```

<li>

```r
  from.dfs(input)
```

</ul>

## 

```r
  mapreduce(
    input, 
    map = function(k, v) keyval(k, v))
```

## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  predicate = 
    function(., v) v%%2 == 0
```

<li> 

```r
  mapreduce(
    input, 
    map = 
      function(k, v) {
        filter = predicate(k, v)
        keyval(k[filter], v[filter])}
```

</ul>
## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  input.select = 
    to.dfs(
      data.frame(
        a = rnorm(input.size),
        b = 1:input.size,
        c = sample(as.character(1:10),
                   input.size, 
                   replace=TRUE)))
```

<li>

```r
  mapreduce(input.select,
            map = function(., v) v$b)
```

</ul>
## 
<ul class="incremental" style="list-style: none" >
<li>

```r
  set.seed(0)
  big.sample = rnorm(input.size)
  input.bigsum = to.dfs(big.sample)
```

<li>

```r
  mapreduce(
    input.bigsum, 
    map  = 
      function(., v) keyval(1, sum(v)), 
    reduce = 
      function(., v) keyval(1, sum(v)),
    combine = TRUE)
```

</ul>
## 

```r
  input.ga = 
    to.dfs(
      cbind(
        1:input.size,
        rnorm(input.size)))
```

## 

```r
  group = function(x) x%%10
  aggregate = function(x) sum(x)
```

## 

```r
  mapreduce(
    input.ga, 
      map = 
        function(k, v) 
          keyval(group(v[,1]), v[,2]),
      reduce = 
        function(k, vv) 
          keyval(k, aggregate(vv)),
      combine = TRUE)
```


# Wordcount
##

<ul class="incremental" style="list-style: none" >
<li>

```r
wordcount = 
  function(
    input, 
    output = NULL, 
    pattern = " "){
```

<li>

```r
    mapreduce(
      input = input ,
      output = output,
      map = wc.map,
      reduce = wc.reduce,
      combine = T)}
```

</ul>

##

<ul class="incremental" style="list-style: none" >
<li>

```r
    wc.map = 
      function(., lines) {
        keyval(
          unlist(
            strsplit(
              x = lines,
              split = pattern)),
          1)}
```

<li>

```r
    wc.reduce =
      function(word, counts ) {
        keyval(word, sum(counts))}
```

</ul>



# Logistic Regression

##
<ul class="incremental" style="list-style: none" >

<li>

```r
logistic.regression = 
  function(input, iterations, dims, alpha){
```

<li>

```r
  plane = t(rep(0, dims))
  g = function(z) 1/(1 + exp(-z))
  for (i in 1:iterations) {
    gradient = 
      values(
        from.dfs(
          mapreduce(
            input,
            map = lr.map,     
            reduce = lr.reduce,
            combine = T)))
    plane = plane + alpha * gradient }
  plane }
```

</ul>

##
<ul class="incremental" style="list-style: none" >

<li>

```r
  lr.map =          
    function(., M) {
      Y = M[,1] 
      X = M[,-1]
      keyval(
        1,
        Y * X * 
          g(-Y * as.numeric(X %*% t(plane))))}
```

</li>

```r
  lr.reduce =
    function(k, Z) 
      keyval(k, t(as.matrix(apply(Z,2,sum))))
```

</ul>

##

<ul class="incremental" style="list-style: none" >

<li>

```r
  eps = rnorm(test.size)
  testdata = 
    to.dfs(
      as.matrix(
        data.frame(
          y = 2 * (eps > 0) - 1,
          x1 = 1:test.size, 
          x2 = 1:test.size + eps)))
```

</li>

```r
    logistic.regression(
      testdata, 3, 2, 0.05)
```

</ul>


# K-means

##


```r
    dist.fun = 
      function(C, P) {
        apply(
          C,
          1, 
          function(x) 
            colSums((t(P) - x)^2))}
```

 
##


```r
    kmeans.map = 
      function(., P) {
        nearest = {
          if(is.null(C)) 
            sample(
              1:num.clusters, 
              nrow(P), 
              replace = T)
          else {
            D = dist.fun(C, P)
            nearest = max.col(-D)}}
        if(!(combine || in.memory.combine))
          keyval(nearest, P) 
        else 
          keyval(nearest, cbind(1, P))}
```


##


```r
    kmeans.reduce = {
      if (!(combine || in.memory.combine) ) 
        function(., P) 
          t(as.matrix(apply(P, 2, mean)))
      else 
        function(k, P) 
          keyval(
            k, 
            t(as.matrix(apply(P, 2, sum))))}
```


##


```r
kmeans.mr = 
  function(
    P, 
    num.clusters, 
    num.iter, 
    combine, 
    in.memory.combine) {
```


##


```r
    C = NULL
    for(i in 1:num.iter ) {
      C = 
        values(
          from.dfs(
            mapreduce(
              P, 
              map = kmeans.map,
              reduce = kmeans.reduce)))
      if(combine || in.memory.combine)
        C = C[, -1]/C[, 1]
```


##


```r
      if(nrow(C) < num.clusters) {
        C = 
          rbind(
            C,
            matrix(
              rnorm(
                (num.clusters - 
                   nrow(C)) * nrow(C)), 
              ncol = nrow(C)) %*% C) }}
        C}
```


##

<ul class="incremental" style="list-style: none" >
<li>

```r
  P = 
    do.call(
      rbind, 
      rep(
        list(
          matrix(
            rnorm(10, sd = 10), 
            ncol=2)), 
        20)) + 
    matrix(rnorm(200), ncol =2)
```

<li>

```r
    kmeans.mr(
      to.dfs(P),
      num.clusters = 12, 
      num.iter = 5,
      combine = FALSE,
      in.memory.combine = FALSE)
```

</ul>

# Linear Least Squares

##
  
  $$ \mathbf{X b = y}$$  

```
  solve(t(X)%*%X, t(X)%*%y)
```
  
##


```r
Sum = 
  function(., YY) 
    keyval(1, list(Reduce('+', YY)))
```


##

```r
XtX = 
  values(
    from.dfs(
      mapreduce(
        input = X.index,
        map = 
          function(., Xi) {
            yi = y[Xi[,1],]
            Xi = Xi[,-1]
            keyval(1, list(t(Xi) %*% Xi))},
        reduce = Sum,
        combine = TRUE)))[[1]]
```


##

```r
Xty = 
  values(
    from.dfs(
      mapreduce(
        input = X.index,
        map = function(., Xi) {
          yi = y[Xi[,1],]
          Xi = Xi[,-1]
          keyval(1, list(t(Xi) %*% yi))},
        reduce = Sum,
        combine = TRUE)))[[1]]
```


##
<ul class="incremental" style="list-style: none" >

<li>

```r
solve(XtX, Xty)
```

<li>   


```r
X = matrix(rnorm(2000), ncol = 10)
X.index = to.dfs(cbind(1:nrow(X), X))
y = as.matrix(rnorm(200))
```

</ul>








# Mapreduce in R

## My first mapreduce job
  
  Conceptually, mapreduce is not very different than a combination of `lapply`s and a `tapply`: transform elements of a list, compute an index &mdash; key in mapreduce jargon &mdash; and process the groups thus defined. Let's start with a simple `lapply` example:
 

```r
  small.ints = 1:1000
  sapply(small.ints, function(x) x^2)
```


The example is trivial, just computing the first thousand squares, but we just want to get the basics here, there are interesting examples later on. Now to the mapreduce equivalent:


```r
  small.ints = to.dfs(1:1000)
  mapreduce(
    input = small.ints, 
    map = function(k, v) cbind(v, v^2))
```



This is all it takes to write your first mapreduce job in `rmr`. There are some differences that we will review, but the first thing to notice is that it isn't all that different, and just two lines of code. The first line puts the data into HDFS, where the bulk of the data has to reside for mapreduce to operate on. It is not possible to write out big data with `to.dfs`, not in a scalable way. `to.dfs` is nonetheless very useful for a variety of uses like writing test cases, learning and debugging. `to.dfs` can put the data in a file of your own choosing, but if you don't specify one it will create temp files and clean them up when done. The return value is something we call a *big data object*. You can assign it to variables, pass it to other `rmr` functions, mapreduce jobs or read it back in. It is a stub, that is the data is not in memory, only some information that helps finding and managing the data. This way you can refer to very large data sets whose size exceeds memory limits. 

Now onto the second line. It has `mapreduce` replace `lapply`. We prefer named arguments with `mapreduce` because there's quite a few possible arguments, but it's not mandatory. The input is the variable `small.ints` which contains the output of `to.dfs`, that is a stub for our small number data set in its HDFS version, but it could be a file path or a list containing a mix of both. The function to apply, which is called a map function as opposed to the reduce function, which we are not using here, is a regular R function with a few constraints:
  
1. It's a function of two arguments, a collection of keys and one of values
1. It returns key value pairs using the function `keyval`, which can have vectors, lists, matrices or data.frames as arguments; you can also return `NULL`. You can avoid calling `keyval` explicitly but the return value `x` will be converted with a call to `keyval(NULL,x)`. This is not allowed in the map function when the reduce function is specified and under no circumstance in the combine function, since specifying the key is necessary for the shuffle phase.

In this example, we are not using the keys at all, only the values, but we still need both to support the general mapreduce case. The return value is big data object, and you can pass it as input to other jobs or read it into memory (watch out, it will fail for big data!) with `from.dfs`. `from.dfs` is complementary to `to.dfs` and returns a key-value pair collection. `from.dfs` is useful in defining map reduce algorithms whenever a mapreduce job produces something of reasonable size, like a summary, that can fit in memory and needs to be inspected to decide on the next steps, or to visualize it. It is much more important than `to.dfs` in production work.

## My second mapreduce job

We've just created a simple job that was logically equivalent to a `lapply` but can run on big data. That job had only a map. Now to the reduce part. The closest equivalent in R is arguably a `tapply`. So here is the example from the R docs:


```r
  groups = rbinom(32, n = 50, prob = 0.4)
  tapply(groups, groups, length)
```

This creates a sample from the binomial and counts how many times each outcome occurred. Now onto the mapreduce  equivalent:
  

```r
  groups = to.dfs(groups)
  from.dfs(
    mapreduce(
      input = groups, 
      map = function(., v) keyval(v, 1), 
      reduce = 
        function(k, vv) 
          keyval(k, length(vv))))
```


First we move the data into HDFS with `to.dfs`. As we said earlier, this is not the normal way in which big data will enter HDFS; it is normally the responsibility of scalable data collection systems such as Flume or Sqoop. In that case we would just specify the HDFS path to the data as input to `mapreduce`. But in this case the input is the variable `groups` which contains a big data object, which keeps track of where the data is and does the clean up when the data is no longer needed. Since a map function is not specified it is set to the default, which is like an identity but consistent with the map requirements, that is `function(k,v) keyval(k,v)`. The reduce function takes two arguments, one is a key and the other is a collection of all the values associated with that key. It could be one of vector, list, data frame or matrix depending on what was returned by the map function. The idea is that if the user returned values of one class, we should preserve that through the shuffle phase. Like in the map case, the reduce function can return `NULL`, a key-value pair as generated by the function `keyval` or any other object `x` which is equivalent to `keyval(NULL, x)`. The default is no reduce, that is the output of the map is the output of mapreduce. In this case the keys are realizations of the binomial and the values are all `1` (please note recycling in action) and the only important  thing is how many there are, so `length` gets the job done. Looking back at this second example, there are some small differences with `tapply` but the overall complexity is very similar.

## Word count

The word count program has become a sort of "hello world" of the mapreduce world. For a review of how the same task can be accomplished in several languages, but always for mapreduce, see this [blog entry](http://blog.piccolboni.info/2011/04/looking-for-map-reduce-language.html).

We are defining a function, `wordcount`, that encapsulates this job. This may not look like a big deal but it is important. Our main goal was not simply to make it easy to run a mapreduce job but to make mapreduce jobs first class citizens of the R environment and to make it easy to create abstractions based on them. For instance, we wanted to be able to assign the result of a mapreduce job to a variable &mdash; and I mean *the result*, not some error code or diagnostics &mdash; and to create complex expressions including mapreduce jobs. We take the first step here by creating a function that is itself a job, can be chained with other jobs, executed in a loop etc.  

Let's now look at the signature. 


```r
wordcount = 
  function(
    input, 
    output = NULL, 
    pattern = " "){
```


There is an input and optional output and a pattern that defines what a word is. 


```r
    wc.map = 
      function(., lines) {
        keyval(
          unlist(
            strsplit(
              x = lines,
              split = pattern)),
          1)}
```


The map function, as we know already, takes two arguments, a key and a value. The key here is not important, indeed always `NULL`. The value contains several lines of text, which gets split according to a pattern. Here you can see that `pattern` is accessible in the mapper without any particular work on the programmer side and according to normal R scope rules. This apparent simplicity hides the fact that the map function is executed in a different interpreter and on a different machine than the `mapreduce` function. Behind the scenes the R environment is serialized, broadcast to the cluster and restored on each interpreter running on the nodes. For each word, a key value pair (*w*, 1) is generated with `keyval` and their collection is the return value of the mapper. 


```r
    wc.reduce =
      function(word, counts ) {
        keyval(word, sum(counts))}
```


The reduce function takes a key and a collection of values, in this case a numeric vector, as input and simply sums up all the counts and returns the pair word, count using the same helper function, `keyval`. Finally, specifying the use of a combiner is necessary to guarantee the scalability of this algorithm. Now on to the mapreduce call.


```r
    mapreduce(
      input = input ,
      output = output,
      input.format = "text",
      map = wc.map,
      reduce = wc.reduce,
      combine = T)}
```


The implementation defines map and reduce functions and then makes a single call to `mapreduce`. The map and reduce functions could be as well anonymous functions as they are used only once, but there is one advantage in naming them. `rmr` offers alternate backends, in particular one can switch off Hadoop altogether with `rmr.options(backend = "local")`. While this is of no use for production work, as it offers no scalability, it is an amazing resource for learning and debugging as we are dealing with a local, run of the mill R program with the same semantics as when run on top of Hadoop. This, in combination with using named map and reduce functions, allows us to use `debug` to debug mapper and reducer the familiar way. 

The input can be an HDFS path, the return value of `to.dfs` or another job or a list thereof &mdash; potentially, a mix of all three cases, as in `list("a/long/path", to.dfs(...), mapreduce(...), ...)`. The output can be an HDFS path but if it is `NULL` some temporary file will be generated and wrapped in a big data object, like the ones generated by `to.dfs`. In either event, the job will return the information about the output, either the path or the big data object. Therefore, we simply pass along the input and output of the`wordcount` function to the `mapreduce` call and return whatever its return value. That way the new function also behaves like a proper mapreduce job &mdash; more details [here](Writing-composable-mapreduce-jobs). The `input.format` argument allows us to specify the format of the input. The default is based on R's own serialization functions and supports all R data types. In this case we just want to read a text file, so the `"text"` format will create key value pairs with a `NULL`key and a line of text as value. You can easily specify your own input and output formats and even accept and produce binary formats with the functions `make.input.format` and `make.output.format`. 



## Logistic Regression

Now on to an example from supervised learning, specifically logistic regression by gradient descent. Again we are going to create a function that encapsulates this algorithm. 


```r
logistic.regression = 
  function(input, iterations, dims, alpha){
```


As you can see we have an input representing the training data. For simplicity we ask to specify a fixed number of iterations, but it would be only slightly more difficult to implement a convergence criterion. Then we need to specify the dimension of the problem, which is redundant because it can be inferred after seeing the first line of input, but we didn't want to put additional logic in the map function, and then we have the learning rate `alpha`. Now we are going to make a slight departure from the actual order in which the code is written. The source code goes on to define the map and reduce functions, but we are going to delay their presentation slightly.


```r
  plane = t(rep(0, dims))
  g = function(z) 1/(1 + exp(-z))
  for (i in 1:iterations) {
    gradient = 
      values(
        from.dfs(
          mapreduce(
            input,
            map = lr.map,     
            reduce = lr.reduce,
            combine = T)))
    plane = plane + alpha * gradient }
  plane }
```


We start by initializing the separating plane and defining the logistic function. As before, those variables will be used inside the map function, that is they will travel across interpreter and processor and network barriers to be available where the developer needs them and where a traditional, meaning sequential, R developer expects them to be available according to scope rules &mdash; no boilerplate code and familiar, powerful behavior.

Then we have the main loop where computing the gradient of the loss function is the duty of a map reduce job, whose output is brought into main memory with a call to `from.dfs` &mdash; any intermediate result files are managed by the system, not you. The only important thing you need to know is that the gradient is going to fit in memory so we can call `from.dfs` to get it without exceeding available  resources.


```r
  lr.map =          
    function(., M) {
      Y = M[,1] 
      X = M[,-1]
      keyval(
        1,
        Y * X * 
          g(-Y * as.numeric(X %*% t(plane))))}
```


The map function simply computes the contribution of a subset of points to the gradient. Please note the variables `g` and `plane` making their necessary appearance here without any work on the developer's part. The access here is read only but you could even modify them if you wanted &mdash; the semantics is copy on assign, which is consistent with how R works and easily supported by Hadoop. Since in the next step we just want to add everything together, we return a dummy, constant key for each value. Note the use of recycling in `keyval`.


```r
  lr.reduce =
    function(k, Z) 
      keyval(k, t(as.matrix(apply(Z,2,sum))))
```


The reduce function is just a big sum. Since we have only one key, all the work will fall on one reducer and that's not scalable. Therefore, in this example, it's important to activate the combiner, in this case set to TRUE, which means same as the reducer. Since sums are associative and commutative that's all we need. `mapreduce` also accepts a distinct combiner function. Remember that a combiner's arguments can come from a map or a combine function and its return value can go to a combine or reduce function. Finally, a reminder that both map and reduce functions need to be defined inside the logistic regression function to have access to the `g` function and the `plane` vector, so cutting and pasting this code in this order won't work.

To make this example production-level there are several things one needs to do, like having a convergence criterion instead of a fixed iterations number an an adaptive learning rate,  but probably gradient descent just requires too many iterations to be the right approach in a big data context. But this example should give you all the elements to be able to implement, say, conjugate gradient instead. In general, when each iteration requires I/O of a large data set, the number of iterations needs to be modest and algorithms with O(log(N)) number of iterations are natural candidates, even if the work in each iteration may be more substantial.

## K-means

We are now going to cover a simple but significant clustering algorithm and the complexity will go up just a little bit. To cheer yourself up, you can take a look at [this alternative implementation](http://www.hortonworks.com/new-apache-pig-features-part-2-embedding/) which requires three languages, python, pig and java, to get the job done and is hailed as a model of simplicity.

We are talking about k-means. This is not a production ready implementation, but should be illustrative of the power of this package. You simply can not do this in pig or hive alone and it would take hundreds of lines of code in java. Please note the code has been slightly rearranged for presentation purposes and won't work if you cut and paste it into the R console. See [kmeans.R](../pkg/tests/kmeans.R) for working code.


```r
    dist.fun = 
      function(C, P) {
        apply(
          C,
          1, 
          function(x) 
            colSums((t(P) - x)^2))}
```


This is simply a distance function, the only noteworthy property of which is that it can compute all the distance between a matrix of centers `C` and a matrix of points `P` very efficiently, on my laptop it can do 10^6 points and 10^2 centers in 5 dimensions in approx. 16s. The only explicit iteration is over the dimension, but all the other operations are vectorized (e.g. loops are pushed to the C library), hence the speed.


```r
    kmeans.map = 
      function(., P) {
        nearest = {
          if(is.null(C)) 
            sample(
              1:num.clusters, 
              nrow(P), 
              replace = T)
          else {
            D = dist.fun(C, P)
            nearest = max.col(-D)}}
        if(!(combine || in.memory.combine))
          keyval(nearest, P) 
        else 
          keyval(nearest, cbind(1, P))}
```

The role of the map function is to compute distances between some points and all centers and return for each point the closest center. It has two flavors controlled by the main `if`: the first iteration when no candidate centers are available and all the following ones. Please note that while the points are stored in HDFS and provided to the map function as its second argument, the centers are simply stored in a matrix and available in the map function because of normal scope rules. In the first iteration, each point is randomly assigned to a center, whereas in the following ones a min distance criterion is used. Finally notice the vectorized use of keyval whereby all the center-point pairs are returned in one statement (the correspondence is positional, with the second dimension used when present).


```r
    kmeans.reduce = {
      if (!(combine || in.memory.combine) ) 
        function(., P) 
          t(as.matrix(apply(P, 2, mean)))
      else 
        function(k, P) 
          keyval(
            k, 
            t(as.matrix(apply(P, 2, sum))))}
```


The reduce function couldn't be simpler as it just computes column averages of a matrix of points sharing a center, which is the key.


```r
kmeans.mr = 
  function(
    P, 
    num.clusters, 
    num.iter, 
    combine, 
    in.memory.combine) {
```



```r
    C = NULL
    for(i in 1:num.iter ) {
      C = 
        values(
          from.dfs(
            mapreduce(
              P, 
              map = kmeans.map,
              reduce = kmeans.reduce)))
      if(combine || in.memory.combine)
        C = C[, -1]/C[, 1]
```



```r
      if(nrow(C) < num.clusters) {
        C = 
          rbind(
            C,
            matrix(
              rnorm(
                (num.clusters - 
                   nrow(C)) * nrow(C)), 
              ncol = nrow(C)) %*% C) }}
        C}
```


The main loop does nothing but bring into memory the result of a mapreduce job with the two above functions as mapper and reducer and the big data object with the points as input. Once the keys are discarded, the values form a matrix which become the new centers. The last if before the return value implements a heuristic to keep the number of centers the desired one (when centers are nearest to no points, they are lost). To run this function we need some data:


```r
  P = 
    do.call(
      rbind, 
      rep(
        list(
          matrix(
            rnorm(10, sd = 10), 
            ncol=2)), 
        20)) + 
    matrix(rnorm(200), ncol =2)
```


  That is, create a large matrix with a few rows  repeated many times and then add some noise. Finally, the function call:
  

```r
    kmeans.mr(
      to.dfs(P),
      num.clusters = 12, 
      num.iter = 5,
      combine = FALSE,
      in.memory.combine = FALSE)
```


With a little extra work you can even get pretty visualizations like [this one](kmeans.gif).

## Linear Least Squares
  
This is an example of a hybrid mapreduce-conventional solution to a well known problem. We will start with a mapreduce job that results in a smaller data set that can be brought into main memory and processed in a single R instance. This is straightforward in rmr because of the simple primitive that transfers data into memory, `from.dfs`, and the R-native data model. This is in contrast with hybrid pig-java-python solutions where mapping data types from one language to the other is a time-consuming and error-prone chore the developer has to deal with.

Specifically, we want to solve LLS under the assumption that we have too many data points to fit in memory but not such a huge number of variables that we need to implement the whole process as map reduce job. This is the basic equation we want to solve in the least square sense: 
  
  **X** **&beta;** = **y**
  
  We are going to do it by using the function solve as in the following expression, that is solving the normal equations.

<pre>
  solve(t(X)%*%X, t(X)%*%y)
</pre>
  
  But let's assume that X is too big to fit in memory, so we have to compute the transpose and matrix products using map reduce, then we can do the solve as usual on the results. This is our general plan.

Let's get some data first, potentially big data matrix `X` and a regular vector `y`:


```r
X = matrix(rnorm(2000), ncol = 10)
X.index = to.dfs(cbind(1:nrow(X), X))
y = as.matrix(rnorm(200))
```


The next is a reusable reduce function that just sums a list of matrices, ignores the key.


```r
Sum = 
  function(., YY) 
    keyval(1, list(Reduce('+', YY)))
```


The big matrix is passed to the mapper in chunks of complete rows. Smaller cross-products are computed for these submatrices and passed on to a single reducer, which sums them together. Since we have a single key a combiner is mandatory and since matrix sum is associative and commutative we certainly can use it here.


```r
XtX = 
  values(
    from.dfs(
      mapreduce(
        input = X.index,
        map = 
          function(., Xi) {
            yi = y[Xi[,1],]
            Xi = Xi[,-1]
            keyval(1, list(t(Xi) %*% Xi))},
        reduce = Sum,
        combine = TRUE)))[[1]]
```


The same pretty much goes on also for vector y, which is made available to the nodes according to normal scope rules.


```r
Xty = 
  values(
    from.dfs(
      mapreduce(
        input = X.index,
        map = function(., Xi) {
          yi = y[Xi[,1],]
          Xi = Xi[,-1]
          keyval(1, list(t(Xi) %*% yi))},
        reduce = Sum,
        combine = TRUE)))[[1]]
```


And finally we just need to call `solve`.


```r
solve(XtX, Xty)
```

   
### Related Links
  
  * [Comparison of high level languages for mapreduce: k means](https://github.com/RevolutionAnalytics/RHadoop/wiki/Comparison-of-high-level-languages-for-mapreduce%3A-k-means)
  * [Changelog](https://github.com/RevolutionAnalytics/RHadoop/wiki/Changelog)
  * [Design philosophy](https://github.com/RevolutionAnalytics/RHadoop/wiki/Design-philosophy)
  * [Efficient rmr techniques](https://github.com/RevolutionAnalytics/RHadoop/wiki/Efficient-rmr-techniques)
  * [Writing composable mapreduce jobs](https://github.com/RevolutionAnalytics/RHadoop/wiki/Writing-composable-mapreduce-jobs)
  * [Use cases](https://github.com/RevolutionAnalytics/RHadoop/wiki/Use-cases)
  * [Getting data in and out](https://github.com/RevolutionAnalytics/RHadoop/wiki/Getting-data-in-and-out)
  * [FAQ](https://github.com/RevolutionAnalytics/RHadoop/wiki/FAQ)

   

Hadoopy HBase
License: Apache V2 (http://www.apache.org/licenses/LICENSE-2.0)


How to Run Tests
sudo /usr/lib/hbase/bin/start-hbase.sh
sudo /etc/init.d/hadoop-hbase-thrift start
cd java
sudo bash build.sh
cd ../tests
python thrift_example.py
python hbase_test.py




Acknowledgements: Lasthbase (https://github.com/tims/lasthbase) inspired this module but is incompatible with new HBase versions, necessitating starting from scratch.  The build scripts and general layout were used as a starting point.

 

Package: rmr2
Type: Package
Title: R and Hadoop Streaming Connector
Version: 3.2.0
Date: 2014-05-19
Author: Revolution Analytics
Depends: R (>= 2.6.0)
Imports: Rcpp, RJSONIO (>= 0.8-2),  bitops, digest, functional, reshape2, stringr, plyr, caTools (>= 1.16)
Suggests: quickcheck (>= 2.0.0)
Collate: basic.R extras.R hdfs.R keyval.R IO.R local.R mapreduce.R parse-url.R quickcheck-rmr.R streaming.R  
Maintainer: Revolution Analytics <rhadoop@revolutionanalytics.com>
Description: Supports the map reduce programming model on top of hadoop streaming
License: Apache License (== 2.0)

# Copyright 2011 Revolution Analytics
#    
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#      http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Newly-released Whirr 0.7.1 fixes problems with the Java installation which 
# were caused by changes in licensing of Java by Oracle

# To set up a hadoop/rmr cluster first launch the cluster

$WHIRR_HOME/bin/whirr launch-cluster --config hadoop-ec2.properties
#this config slightly tweaked from whirr distro, starts 5 large nodes

# then install R and rmr
$WHIRR_HOME/bin/whirr run-script --script rmr.sh --config hadoop-ec2.properties

# remember to destroy when done. You are responsible for any AWS charges

$WHIRR_HOME/bin/whirr destroy-cluster --config hadoop-ec2.properties

# 'hadoop-ec2-centos.properties' and 'rmr-master-centos.sh' can be used with 
# the above steps to create a CentOS 4.6-based cluster using a RightScale AMI

rmr2
====

A package that allows R developer to use Hadoop MapReduce, developed as part of the RHadoop project. Please see the [RHadoop wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki) for information. 

